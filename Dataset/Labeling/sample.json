[
    {
        "Question_id":null,
        "Question_title":"Azure machine learning studio for designer function connected with excel?",
        "Question_body":"Hello, I am using Azure machine learning studio, which has been changed since last year.\n\nPreviously, the Azure Machine Learning designer function of the Classic version could be applied to Excel by importing the App function to Excel and downloading it. Like the picture below!\n\nHas the function that can be linked to Excel be lost in this Azure Machine Learning Studio? it's very difficult....\n\nIf there is a function, can you tell me how to do it?\n\nAnd I wonder if there are any lectures that explain the new azure machine learning designer features.",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1645970050413,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/752248\/azure-machine-learning-studio-for-designer-functio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-28T10:37:23.35Z",
                "Answer_score":0,
                "Answer_body":"@RobinJang-2932 The designer studio does not have an add-in for excel. This is only available with the classic version of Azure Machine Learning.\nIf you are new to Azure machine learning designer I would recommend to start with the tutorials from Microsoft Learn available here.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-28T08:06:31.203Z",
                "Answer_score":0,
                "Answer_body":"Hi @RobinJang-2932,\nWhat version of Office did you use?\nI tested in Excel 365, I can add the add-in and run it:\n\n\nThe add-in is only supported:\nExcel 2013 or later on Windows\nExcel 2013 SP1 or later on Windows\nExcel on Windows (Microsoft 365)\nExcel on the web\n\nHere is the link about Azue Machine Learning documents:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/\n\nJust checking in to see if the information was helpful. Please let us know if you would like further assistance.\n\nIf the response is helpful, please click \"Accept Answer\" and upvote it.\nNote: Please follow the steps in our documentation to enable e-mail notifications if you want to receive the related email notification for this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: studio for designer function connected with excel?; Content: hello, i am using studio, which has been changed since last year. previously, the designer function of the classic version could be applied to excel by importing the app function to excel and downloading it. like the picture below! has the function that can be linked to excel be lost in this studio? it's very difficult.... if there is a function, can you tell me how to do it? and i wonder if there are any lectures that explain the new designer features.",
        "Question_original_content_gpt_summary":"The user is struggling to understand how to use the new Studio version to connect the Designer function to Excel.",
        "Question_preprocessed_content":"Title: studio for designer function connected with excel?; Content: hello, i am using studio, which has been changed since last year. previously, the designer function of the classic version could be applied to excel by importing the app function to excel and downloading it. like the picture below! has the function that can be linked to excel be lost in this studio? it's very if there is a function, can you tell me how to do it? and i wonder if there are any lectures that explain the new designer features.",
        "Answer_original_content":"@robinjang-2932 the designer studio does not have an add-in for excel. this is only available with the classic version of . if you are new to designer i would recommend to start with the tutorials from microsoft learn available here. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer to the user's question. The answer suggests that the new Studio version does not have an add-in for Excel and recommends the user to start with tutorials from Microsoft Learn if they are new to Designer.",
        "Answer_preprocessed_content":"the designer studio does not have an for excel. this is only available with the classic version of . if you are new to designer i would recommend to start with the tutorials from microsoft learn available here. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":null,
        "Question_title":"How to organize intents\/pages for a non-service\/support application",
        "Question_body":"I am a new Dialogflow user and I need to create an agent for an application that is not service or support oriented.  The application is educational and has a large collection of questions and answers (1,000s) with no concrete conclusion (for example,  to renew a driver's license).  For the POC I did in Watson I was able to use folders to organize sub-topics.  What is the best way to group intents and responses by topic and sub-topic (for example, President Lincoln's early life President Lincoln's career)?  I expect it would be difficult to manage a list of 1,000s of pages in the left pane.  Thank you.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1669003380000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":44.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-organize-intents-pages-for-a-non-service-support\/td-p\/491306\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-22T09:48:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi\u00a0@NoankMary, are you using Dialogflow CX or ES?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to organize intents\/pages for a non-service\/support application; Content: i am a new dialogflow user and i need to create an agent for an application that is not service or support oriented. the application is educational and has a large collection of questions and answers (1,000s) with no concrete conclusion (for example, to renew a driver's license). for the poc i did in watson i was able to use folders to organize sub-topics. what is the best way to group intents and responses by topic and sub-topic (for example, president lincoln's early life president lincoln's career)? i expect it would be difficult to manage a list of 1,000s of pages in the left pane. thank you.",
        "Question_original_content_gpt_summary":"The user is looking for the best way to organize intents and responses by topic and sub-topic in a non-service\/support application with a large collection of questions and answers.",
        "Question_preprocessed_content":"Title: how to organize for a application; Content: i am a new dialogflow user and i need to create an agent for an application that is not service or support oriented. the application is educational and has a large collection of questions and answers with no concrete conclusion . for the poc i did in watson i was able to use folders to organize what is the best way to group intents and responses by topic and ? i expect it would be difficult to manage a list of , s of pages in the left pane. thank you.",
        "Answer_original_content":"hi@noankmary, are you using dialogflow cx or es?",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer to the question about organizing intents and responses by topic and sub-topic in a non-service\/support application with a large collection of questions and answers. The answer is simply asking for clarification on whether the user is using Dialogflow CX or ES.",
        "Answer_preprocessed_content":"hi are you using dialogflow cx or es?"
    },
    {
        "Question_id":null,
        "Question_title":"Dvc and S3 permissions management",
        "Question_body":"<p>Hello dvc people!<\/p>\n<p>At my team we are experimenting with dvc as an interface to work with trained models and datasets. So far it has been working nicely, but we found a mayor roadblock when it comes to permissions management.<\/p>\n<p>In our ideal scenario, when a teammate gets access to one of our repositories in GitHub that person will only have access to the data from that particular repository. This permission should be given automatically by the fact that the person has access to the repository, with no need to modify roles in AWS.<\/p>\n<p>However, this seems challenging to achieve through S3 + DVC. Our considered solution right now is to have a single S3 bucket with all our DVC repositories. If we grant access to all teammates to the whole bucket it would mean that they will have access to all the company\u2019s data, which is less than ideal. So we were considering having a \u201cJunior\u201d role which we will have to give access manually to whatever specific S3 folders they need and a \u201cSenior\u201d role with access to everything.<\/p>\n<p>This solution is suboptimal as we will need to handle different permissions for GitHub and for S3, which adds overhead.<\/p>\n<p>Is there any other pattern we are missing here? Is there any easy way for a teammate to be given access to ONLY the dvc remote from the GitHub repository automatically?<\/p>\n<p>Thanks for the help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/relaxed.png?v=10\" title=\":relaxed:\" class=\"emoji\" alt=\":relaxed:\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1631267338282,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":484.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-s3-permissions-management\/887",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-10T17:48:06.234Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/javi\">@Javi<\/a>, thank you for asking! This is a very good question and two possible solutions:<\/p>\n<ol>\n<li>\n<strong>Workflow level<\/strong>. You can allow <code>write<\/code>\/<code>push<\/code> access to the bucket only for bots\/virtual-users that will push data to the bucket through CI\/CD only after approved pull requests in GitHub. While all the users will have <code>read<\/code>\/<code>pull<\/code> access from the bucket.<\/li>\n<li>\n<strong>User level<\/strong>. The way you describe it. We are in the design stage for this feature. It would be great if you can help us with the design and requirements.<\/li>\n<\/ol>\n<p>I\u2019d be happy to help you and go deeper into the solutions. Please let me know if you are open to a chat - please shoot me a Hi email to my-first-name at iterative.ai<\/p>",
                "Answer_score":62.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: and s3 permissions management; Content: hello people! at my team we are experimenting with as an interface to work with trained models and datasets. so far it has been working nicely, but we found a mayor roadblock when it comes to permissions management. in our ideal scenario, when a teammate gets access to one of our repositories in github that person will only have access to the data from that particular repository. this permission should be given automatically by the fact that the person has access to the repository, with no need to modify roles in aws. however, this seems challenging to achieve through s3 + . our considered solution right now is to have a single s3 bucket with all our repositories. if we grant access to all teammates to the whole bucket it would mean that they will have access to all the company\u2019s data, which is less than ideal. so we were considering having a \u201cjunior\u201d role which we will have to give access manually to whatever specific s3 folders they need and a \u201csenior\u201d role with access to everything. this solution is suboptimal as we will need to handle different permissions for github and for s3, which adds overhead. is there any other pattern we are missing here? is there any easy way for a teammate to be given access to only the remote from the github repository automatically? thanks for the help",
        "Question_original_content_gpt_summary":"The user is facing a challenge in managing permissions for their team's use of S3 in conjunction with GitHub, as they need to find a way to automatically grant access to only the data from a particular repository when a teammate is given access to it.",
        "Question_preprocessed_content":"Title: and s permissions management; Content: hello people! at my team we are experimenting with as an interface to work with trained models and datasets. so far it has been working nicely, but we found a mayor roadblock when it comes to permissions management. in our ideal scenario, when a teammate gets access to one of our repositories in github that person will only have access to the data from that particular repository. this permission should be given automatically by the fact that the person has access to the repository, with no need to modify roles in aws. however, this seems challenging to achieve through s + . our considered solution right now is to have a single s bucket with all our repositories. if we grant access to all teammates to the whole bucket it would mean that they will have access to all the companys data, which is less than ideal. so we were considering having a junior role which we will have to give access manually to whatever specific s folders they need and a senior role with access to everything. this solution is suboptimal as we will need to handle different permissions for github and for s , which adds overhead. is there any other pattern we are missing here? is there any easy way for a teammate to be given access to only the remote from the github repository automatically? thanks for the help",
        "Answer_original_content":"hi @javi, thank you for asking! this is a very good question and two possible solutions: workflow level. you can allow write\/push access to the bucket only for bots\/virtual-users that will push data to the bucket through ci\/cd only after approved pull requests in github. while all the users will have read\/pull access from the bucket. user level. the way you describe it. we are in the design stage for this feature. it would be great if you can help us with the design and requirements. id be happy to help you and go deeper into the solutions. please let me know if you are open to a chat - please shoot me a hi email to my-first-name at iterative.ai",
        "Answer_original_content_gpt_summary":"The answer suggests two possible solutions to the challenge of managing permissions for S3 and GitHub integration. The first solution involves allowing write\/push access to the bucket only for bots\/virtual-users that will push data to the bucket through ci\/cd only after approved pull requests in GitHub. The second solution is still in the design stage, and the user is invited to help with the design and requirements.",
        "Answer_preprocessed_content":"hi thank you for asking! this is a very good question and two possible solutions workflow level. you can allow \/ access to the bucket only for that will push data to the bucket through only after approved pull requests in github. while all the users will have \/ access from the bucket. user level. the way you describe it. we are in the design stage for this feature. it would be great if you can help us with the design and requirements. id be happy to help you and go deeper into the solutions. please let me know if you are open to a chat please shoot me a hi email to at"
    },
    {
        "Question_id":57544237.0,
        "Question_title":"Inside lambda function - Blazing text algorithm invoke endpoint doesn't support the input content type",
        "Question_body":"<p>Im working on sentence classification using in-build  blazing text algorithm, while invoking endpoint inside lambda function it throughs the content type mismatching error. <\/p>\n\n<p>-- For blazing text it support only application\/jsonlines or application\/json but while invoking , it throughs the error like , it accepts only byte or bytearray<\/p>\n\n<pre><code>input format . application\/json\nevent={\n  \"features\": [\n    \"sensor_subtype Thermostats Thermal Switches product_features Hermetically sealed n Tight tolerances n Tight differentials n Logic level contacts n applications Computers n Medical electronics n Power supplies n Industrial controls n Test equipment n Infotech n description Technical Specifications technical_specs CloseTolerance 2 8 C 5 F DielectricStrength MIL STD 202 Method 301 1250 Vac 60 Hz Terminal to Case ContactResistance MIL STD\"\n  ]\n}\n<\/code><\/pre>\n\n<p>and also i tried application\/jsonlines<\/p>\n\n<p>My code looks like this>>>>>>>>>>>>>>>>>>>>>>>><\/p>\n\n<pre><code>def transform_data(data):\n    try:\n        features = data.copy()\n\n        return features\n\n    except Exception as err:\n        print('Error when transforming: {0},{1}'.format(data,err))\n        raise Exception('Error when transforming: {0},{1}'.format(data,err))\n\n\ndef lambda_handler(event, context):\n    try:    \n        print(\"Received event: \" + json.dumps(event, indent=2))\n\n        request = json.loads(json.dumps(event))\n\n        transformed_data = str(transform_data(request['features'])) #for instance in request['features'])\n        print(ENDPOINT_NAME, \"-------&gt;&gt;&gt;&gt;\")\n        payload=transformed_data\n        result = client.invoke_endpoint(EndpointName=ENDPOINT_NAME, \n                              Body=(payload.encode('utf-8')),\n                              ContentType='application\/json')\n        return result\n<\/code><\/pre>\n\n<pre><code>  \"statusCode\": 400,\n  \"isBase64Encoded\": false,\n  \"body\": \"Call Failed An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (406) from model with message \\\"Invalid payload format\\\".\n_______________LOGS__________________________________\n\uf141\n11:35:22\n[08\/18\/2019 11:35:22 ERROR 140074862942016] Customer Error: Unable to decode payload: Incorrect data format. (caused by ValueError)\n\uf141\n11:35:22\nCaused by: No JSON object could be decoded\n\uf141\n11:35:22\nTraceback (most recent call last): File \"\/opt\/amazon\/lib\/python2.7\/site-packages\/blazingtext\/serve.py\", line 317, in invocations data = json.loads(payload.decode(\"utf-8\")) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/__init__.py\", line 339, in loads return _default_decoder.decode(s) File \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/decoder.py\", line 364, in decode obj, end = self.\n\uf141\n11:35:22\nValueError: No JSON object could be decoded\n<\/code><\/pre>\n\n<p>I need to predict the sentence in realtime using invoke_endpoint option but it shows invalid payload format <\/p>\n\n<p>I tried with byte format and apllication\/jsonlines format.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1.0,
        "Question_creation_time":1566128809927,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":493.0,
        "Owner_creation_time":1541220234400,
        "Owner_last_access_time":1664010357623,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":5.0,
        "Answer_body":"<p>I encountered the same problem when trying to predict on text classification with a BlazingText container. What worked for me was simply changing the key in the payload while keeping the ContentType as application\/json:<\/p>\n<pre><code>sentence = &quot;I'm selling my PS4, practically brand new&quot;\n\npayload = {&quot;instances&quot;: [sentence]}\n\nresponse = client.invoke_endpoint(\n        EndpointName=&quot;text_classification&quot;,\n        Body=json.dumps(payload),\n        ContentType='application\/json'\n        \n    )\n<\/code><\/pre>\n<p>After playing around a little with the payload it seems that blazing text models only accept payloads as a dictionary with &quot;instances&quot; as its key and a list containing your data you want to predict on as its value.<\/p>\n<p>To get to your predictions simply :<\/p>\n<pre><code>print(&quot;ResponseMetadata:&quot;, response[&quot;ResponseMetadata&quot;])\nprint()\nprint(&quot;Body:&quot;, response['Body'].read())\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1595256539003,
        "Answer_score":0.0,
        "Owner_location":"Singapore",
        "Question_last_edit_time":1566133069936,
        "Answer_last_edit_time":1595256858790,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57544237",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: inside lambda function - blazing text algorithm invoke endpoint doesn't support the input content type; Content: im working on sentence classification using in-build blazing text algorithm, while invoking endpoint inside lambda function it throughs the content type mismatching error. -- for blazing text it support only application\/jsonlines or application\/json but while invoking , it throughs the error like , it accepts only byte or bytearray input format . application\/json event={ \"features\": [ \"sensor_subtype thermostats thermal switches product_features hermetically sealed n tight tolerances n tight differentials n logic level contacts n applications computers n medical electronics n power supplies n industrial controls n test equipment n infotech n description technical specifications technical_specs closetolerance 2 8 c 5 f dielectricstrength mil std 202 method 301 1250 vac 60 hz terminal to case contactresistance mil std\" ] } and also i tried application\/jsonlines my code looks like this>>>>>>>>>>>>>>>>>>>>>>>> def transform_data(data): try: features = data.copy() return features except exception as err: print('error when transforming: {0},{1}'.format(data,err)) raise exception('error when transforming: {0},{1}'.format(data,err)) def lambda_handler(event, context): try: print(\"received event: \" + json.dumps(event, indent=2)) request = json.loads(json.dumps(event)) transformed_data = str(transform_data(request['features'])) #for instance in request['features']) print(endpoint_name, \"------->>>>\") payload=transformed_data result = client.invoke_endpoint(endpointname=endpoint_name, body=(payload.encode('utf-8')), contenttype='application\/json') return result \"statuscode\": 400, \"isbase64encoded\": false, \"body\": \"call failed an error occurred (modelerror) when calling the invokeendpoint operation: received client error (406) from model with message \\\"invalid payload format\\\". _______________logs__________________________________ \uf141 11:35:22 [08\/18\/2019 11:35:22 error 140074862942016] customer error: unable to decode payload: incorrect data format. (caused by valueerror) \uf141 11:35:22 caused by: no json object could be decoded \uf141 11:35:22 traceback (most recent call last): file \"\/opt\/amazon\/lib\/python2.7\/site-packages\/blazingtext\/serve.py\", line 317, in invocations data = json.loads(payload.decode(\"utf-8\")) file \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/__init__.py\", line 339, in loads return _default_decoder.decode(s) file \"\/opt\/amazon\/python2.7\/lib\/python2.7\/json\/decoder.py\", line 364, in decode obj, end = self. \uf141 11:35:22 valueerror: no json object could be decoded i need to predict the sentence in realtime using invoke_endpoint option but it shows invalid payload format i tried with byte format and apllication\/jsonlines format.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with invoking an endpoint inside a lambda function for sentence classification using the Blazing Text algorithm, as the endpoint does not support the input content type.",
        "Question_preprocessed_content":"Title: inside lambda function blazing text algorithm invoke endpoint doesn't support the input content type; Content: im working on sentence classification using blazing text algorithm, while invoking endpoint inside lambda function it throughs the content type mismatching error. for blazing text it support only or but while invoking , it throughs the error like , it accepts only byte or bytearray and also i tried my code looks like this i need to predict the sentence in realtime using option but it shows invalid payload format i tried with byte format and format.",
        "Answer_original_content":"i encountered the same problem when trying to predict on text classification with a blazingtext container. what worked for me was simply changing the key in the payload while keeping the contenttype as application\/json: sentence = \"i'm selling my ps4, practically brand new\" payload = {\"instances\": [sentence]} response = client.invoke_endpoint( endpointname=\"text_classification\", body=json.dumps(payload), contenttype='application\/json' ) after playing around a little with the payload it seems that blazing text models only accept payloads as a dictionary with \"instances\" as its key and a list containing your data you want to predict on as its value. to get to your predictions simply : print(\"responsemetadata:\", response[\"responsemetadata\"]) print() print(\"body:\", response['body'].read())",
        "Answer_original_content_gpt_summary":"The solution to the challenge of invoking an endpoint inside a lambda function for sentence classification using the Blazing Text algorithm is to change the key in the payload while keeping the content type as application\/json. Blazing Text models only accept payloads as a dictionary with \"instances\" as its key and a list containing the data to predict on as its value. To get predictions, print the response metadata and body.",
        "Answer_preprocessed_content":"i encountered the same problem when trying to predict on text classification with a blazingtext container. what worked for me was simply changing the key in the payload while keeping the contenttype as after playing around a little with the payload it seems that blazing text models only accept payloads as a dictionary with instances as its key and a list containing your data you want to predict on as its value. to get to your predictions simply"
    },
    {
        "Question_id":null,
        "Question_title":"Create and run multiple experiments from python",
        "Question_body":"<p>Hi,<br>\nI see that the python API is read-only.<br>\nhow can I have python iterate over parameters and run an experiment for each,<br>\ni.e. doing gridsearch or something similar?<\/p>\n<p>os.system(\u201cdvc exp run -n exp_name\u201d) doesn\u2019t seem very \u2018integrated\u2019 and it has generated issues with git index lock and dvc locks.<br>\nthanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1675278304799,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":29.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/create-and-run-multiple-experiments-from-python\/1490",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-01T19:36:24.413Z",
                "Answer_body":"<p>I would like to see a feature like this. I am doing what you described. You can use the <code>Repo<\/code> class from <code>dvc.repo<\/code> to check for experiment names and stuff like that. Hope this helps<\/p>\n<pre><code class=\"lang-auto\">def run_experiment(name, params):\n    command = f\"dvc exp run --queue -n {name}\"\n    for key, val in params.items():\n        command += f\" -S {key}={val}\"\n    result = subprocess.run(command, capture_output=True, shell=True)\n    print(result)\n\n\ndef main():\n    # dvc repo, determine current branch\n    repo = Repo(\".\")\n    branch = repo.scm.active_branch()\n    # get experiments on this commit, including queue\n    taken_names = repo.experiments.ls()[branch]\n    taken_names += [a.name for a in repo.experiments.celery_queue.iter_queued()]\n    experiment_params = get_random_search_params()  # list of dictionaries\n    print(f\"Setting up {len(experiment_params)} experiments...\")\n    for params in experiment_params:\n        # get new name\n        for i in range(9999):\n            name = f\"exp_{i}\"\n            if name not in taken_names:\n                taken_names.append(name)\n                break\n        # run experiment\n        print(name, params)\n        run_experiment(name, params)\n<\/code><\/pre>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-01T22:06:35.343Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/marc_wagner\">@marc_wagner<\/a>: Hi! Indeed, the Python API has no convenient way to do that, but I guess using the Hydra integration on the CLI would solve your issue. Have a look at <a href=\"https:\/\/iterative.ai\/blog\/dvc-hydra-integration\/\" rel=\"noopener nofollow ugc\">our blog<\/a> for a quick intro, or the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\/hydra-composition\" rel=\"noopener nofollow ugc\">full docs<\/a>.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: create and run multiple experiments from python; Content: hi, i see that the python api is read-only. how can i have python iterate over parameters and run an experiment for each, i.e. doing gridsearch or something similar? os.system( exp run -n exp_name) doesnt seem very integrated and it has generated issues with git index lock and locks. thanks!",
        "Question_original_content_gpt_summary":"The user is looking for a way to use Python to iterate over parameters and run multiple experiments, but is having issues with os.system() and git index lock.",
        "Question_preprocessed_content":"Title: create and run multiple experiments from python; Content: hi, i see that the python api is how can i have python iterate over parameters and run an experiment for each, doing gridsearch or something similar? exp run n doesnt seem very integrated and it has generated issues with git index lock and locks. thanks!",
        "Answer_original_content":"i would like to see a feature like this. i am doing what you described. you can use the repo class from .repo to check for experiment names and stuff like that. hope this helps def run_experiment(name, params): command = f\" exp run --queue -n {name}\" for key, val in params.items(): command += f\" -s {key}={val}\" result = subprocess.run(command, capture_output=true, shell=true) print(result) def main(): # repo, determine current branch repo = repo(\".\") branch = repo.scm.active_branch() # get experiments on this commit, including queue taken_names = repo.experiments.ls()[branch] taken_names += [a.name for a in repo.experiments.celery_queue.iter_queued()] experiment_params = get_random_search_params() # list of dictionaries print(f\"setting up {len(experiment_params)} experiments...\") for params in experiment_params: # get new name for i in range(9999): name = f\"exp_{i}\" if name not in taken_names: taken_names.append(name) break # run experiment print(name, params) run_experiment(name, params) @marc_wagner: hi! indeed, the python api has no convenient way to do that, but i guess using the hydra integration on the cli would solve your issue. have a look at our blog for a quick intro, or the full docs.",
        "Answer_original_content_gpt_summary":"The answer does not provide a direct solution to the user's issue with os.system() and git index lock. However, it suggests using the repo class from .repo to check for experiment names and provides a code snippet for running experiments with parameters. Additionally, the answer suggests using the hydra integration on the CLI as a possible solution.",
        "Answer_preprocessed_content":"i would like to see a feature like this. i am doing what you described. you can use the class from to check for experiment names and stuff like that. hope this helps hi! indeed, the python api has no convenient way to do that, but i guess using the hydra integration on the cli would solve your issue. have a look at our blog for a quick intro, or the full docs."
    },
    {
        "Question_id":null,
        "Question_title":"Storing the guild artifacts on s3 using `guild run -r s3-dev`",
        "Question_body":"<p>I wanted to store all my runs in s3 bucket directly instead of the local guildai home directory. So,  I configured my <code>~\/.guild\/config.yml<\/code> in the following way which I thought will setup my s3 remote location and I can automatically save the sourcecode and artifacts that I save locally into s3 bucket.<\/p>\n<pre><code>remotes:\n  s3-dev:\n    type: s3\n    description: Production runs\n    bucket: cortex-model-data\n    region: eu-central-1\n<\/code><\/pre>\n<p>and I have my <code>guild.yml<\/code>as follows<\/p>\n<pre><code>- model: AlwaysPredictMean\n  description: A dummy model which always predicts mean\n  operations:\n    train:\n      description: Training Pipeline Sample Code\n      main: training\/train\n      flags-import: all\n      output-scalars: '(\\key): (\\value)'\n<\/code><\/pre>\n<p>when I run the script using <code>guild run --remote s3-dev<\/code> , I get the following error message<\/p>\n<pre><code>\u00b1 |feature\/guildai U:1 \u2717| \u2192 guild run -r s3-dev\nYou are about to run AlwaysPredictMean:train on s3-dev\n  comment: Description for a given training run\n  config: training\/config\/example.yml\n  data: tests\/test_df.csv\n  epochs: 10\n  model_class: training.example_model::AlwaysPredictMean\n  use_case: example_use_case\nContinue? (Y\/n) y\nguild: remote 's3-dev' does not support this operation\n<\/code><\/pre>\n<p>Can someone let me know what exactly is the problem and why can\u2019t I use guild.ai to store in the specified s3. Thanks<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1605200901861,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":528.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/storing-the-guild-artifacts-on-s3-using-guild-run-r-s3-dev\/449",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-12T17:57:29.291Z",
                "Answer_body":"<p>S3 remotes are only for file storage. You can\u2019t run anything in S3. If you want to run on a remote server, which is what you\u2019re asking for with the <code>--remote<\/code> option, you need either an <a href=\"\/reference\/remotes#ssh\"><code>ssh<\/code><\/a> or <a href=\"https:\/\/my.guild.ai\/reference\/remotes#ec2\"><code>ec2<\/code><\/a> remote type.<\/p>\n<p>If you want to run your operation locally, omit the <code>--remote<\/code> option \u2014 you\u2019ll get runs in your current Guild environment. Then use <a href=\"\/commands\/push\"><code>guild push s3-dev<\/code><\/a> to copy those runs to S3.<\/p>",
                "Answer_score":16.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-13T08:52:27.693Z",
                "Answer_body":"<p>Thanks a lot. Now it is more clear. I misunderstood that if I run a script with a remote s3 config, my runs (including sourcecode and artifacts) are automatically saved to s3. I will do it manually by using <code>guild push s3-dev<\/code><\/p>\n<p>Thanks also for building this nice tool.<\/p>",
                "Answer_score":56.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-13T15:50:12.734Z",
                "Answer_body":"<p>I could see the value of automatically sync\u2019ing with a remote env during and especially after a run. Guild does not currently support this. I could see an enhancement to Guild along the line of <code>--push-to-remote<\/code> or <code>--push-on-success<\/code> that does this. That\u2019s a good idea. Though that poor run command already has quite a few options - it\u2019s going to evolve into it\u2019s own language grammar <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> Still, I like the thinking!<\/p>\n<p>In defense of Guild\u2019s \u201cseparation of concerns\u201d Tao, you can accomplish what you\u2019re looking for this way:<\/p>\n<pre><code class=\"lang-command\">guild run &lt;options&gt; &amp;&amp; guild push s3-dev\n<\/code><\/pre>\n<p>This works on shells that support <code>&amp;&amp;<\/code>, which will execute the second command only when the first command succeeds (exit code of 0).<\/p>\n<p>If you wanted to push regardless of the result, use:<\/p>\n<pre><code class=\"lang-command\">guild run &lt;options&gt;; guild push s3-dev\n<\/code><\/pre>\n<p>Note this will push all runs, not just the latest. As Guild uses rsync (or similar) protocols, this is efficient. But you may want to just push the latest. In that case add the <code>1<\/code> argument to the push command. This tells Guild to only push the latest run (i.e. the run with index <code>1<\/code>).<\/p>\n<p>Wait, there\u2019s more!<\/p>\n<p>If you would like to always sync runs to your S3 bucket, you could create a repeating command <code>guild push<\/code> that runs, say, every 10 minutes, 30 minutes, etc. Most POSIX systems offer cron for this, but there are myriad ways to run scheduled commands. Now this is moving you into \u201csys op\u201d territory, which you might not want enter - beware there be dragons <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> But such is life. If this logic was moved into the <code>run<\/code> command you\u2019d need to worry about command failure (e.g. SIGKILLs) or system failure (e.g. batter\/power loss, etc.) Using something like cron is a nice separation of concerns because cron can back-fill on various failures to complete your backups even when Guild or the system unexpected crashes. E.g. your system loses power during a long training run where you have various interim checkpoints. You restart, cron runs automatically to backup your partial runs. You can then restart the run using <code>guild run --restart &lt;run ID&gt;<\/code> and be on your merry way, letting cron run every so often to refresh the backup. I don\u2019t think this scheme is terribly complicated yet it\u2019s quite robust.<\/p>",
                "Answer_score":26.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-16T06:18:10.999Z",
                "Answer_body":"<p>Thanks garrett, I understand the concern. Coming from mlflow where you can set_tracking_uri to s3 remote, I thought guild also has this option. Having the artifacts stored in a remote s3 bucket makes it easier to collaborate in a small team and track the progress. But after looking at the number of options available for <code>guild run<\/code> and <code>guild push<\/code>, it makes sense to have these two functions separate.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-16T13:26:30.056Z",
                "Answer_body":"<p>I think it\u2019s a great idea and one more option to <code>run<\/code> isn\u2019t a problem. I opened an issue on GitHub to track progress this.<\/p>\n<aside class=\"onebox githubissue\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/252\" target=\"_blank\" rel=\"noopener\">github.com\/guildai\/guildai<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewBox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/252\" target=\"_blank\" rel=\"noopener\">Ability to sync with remote during run without explicit push command<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2020-11-16\" data-time=\"13:25:18\" data-timezone=\"UTC\">01:25PM - 16 Nov 20 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/gar1t\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"gar1t\" src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/84ac3354a76fe15593cedb56fe486a0ed93d5440.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          gar1t\n        <\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n<\/div>\n\n<div class=\"github-row\">\n  <p class=\"github-content\">It's convenient to be able to sync run artifacts with a remote (e.g. S3 bucket, etc.) as a function of the...<\/p>\n<\/div>\n\n<div class=\"labels\">\n    <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">enhancement<\/span>\n<\/div>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: storing the guild artifacts on s3 using `guild run -r s3-dev`; Content: i wanted to store all my runs in s3 bucket directly instead of the local guildai home directory. so, i configured my ~\/.guild\/config.yml in the following way which i thought will setup my s3 remote location and i can automatically save the sourcecode and artifacts that i save locally into s3 bucket. remotes: s3-dev: type: s3 description: production runs bucket: cortex-model-data region: eu-central-1 and i have my guild.ymlas follows - model: alwayspredictmean description: a dummy model which always predicts mean operations: train: description: training pipeline sample code main: training\/train flags-import: all output-scalars: '(\\key): (\\value)' when i run the script using guild run --remote s3-dev , i get the following error message \u00b1 |feature\/guildai u:1 \u2717| \u2192 guild run -r s3-dev you are about to run alwayspredictmean:train on s3-dev comment: description for a given training run config: training\/config\/example.yml data: tests\/test_df.csv epochs: 10 model_class: training.example_model::alwayspredictmean use_case: example_use_case continue? (y\/n) y guild: remote 's3-dev' does not support this operation can someone let me know what exactly is the problem and why can\u2019t i use guild.ai to store in the specified s3. thanks",
        "Question_original_content_gpt_summary":"The user is encountering challenges with storing guild artifacts on s3 using `guild run -r s3-dev`, resulting in an error message indicating that the remote 's3-dev' does not support the operation.",
        "Question_preprocessed_content":"Title: storing the guild artifacts on s using ; Content: i wanted to store all my runs in s bucket directly instead of the local guildai home directory. so, i configured my in the following way which i thought will setup my s remote location and i can automatically save the sourcecode and artifacts that i save locally into s bucket. and i have my as follows when i run the script using , i get the following error message can someone let me know what exactly is the problem and why cant i use to store in the specified s . thanks",
        "Answer_original_content":"s3 remotes are only for file storage. you cant run anything in s3. if you want to run on a remote server, which is what youre asking for with the --remote option, you need either an ssh or ec2 remote type. if you want to run your operation locally, omit the --remote option youll get runs in your current guild environment. then use guild push s3-dev to copy those runs to s3. thanks a lot. now it is more clear. i misunderstood that if i run a script with a remote s3 config, my runs (including sourcecode and artifacts) are automatically saved to s3. i will do it manually by using guild push s3-dev thanks also for building this nice tool. i could see the value of automatically syncing with a remote env during and especially after a run. guild does not currently support this. i could see an enhancement to guild along the line of --push-to-remote or --push-on-success that does this. thats a good idea. though that poor run command already has quite a few options - its going to evolve into its own language grammar still, i like the thinking! in defense of guilds separation of concerns tao, you can accomplish what youre looking for this way: guild run <options> && guild push s3-dev this works on shells that support &&, which will execute the second command only when the first command succeeds (exit code of 0). if you wanted to push regardless of the result, use: guild run <options>; guild push s3-dev note this will push all runs, not just the latest. as guild uses rsync (or similar) protocols, this is efficient. but you may want to just push the latest. in that case add the 1 argument to the push command. this tells guild to only push the latest run (i.e. the run with index 1). wait, theres more! if you would like to always sync runs to your s3 bucket, you could create a repeating command guild push that runs, say, every 10 minutes, 30 minutes, etc. most posix systems offer cron for this, but there are myriad ways to run scheduled commands. now this is moving you into sys op territory, which you might not want enter - beware there be dragons but such is life. if this logic was moved into the run command youd need to worry about command failure (e.g. sigkills) or system failure (e.g. batter\/power loss, etc.) using something like cron is a nice separation of concerns because cron can back-fill on various failures to complete your backups even when guild or the system unexpected crashes. e.g. your system loses power during a long training run where you have various interim checkpoints. you restart, cron runs automatically to backup your partial runs. you can then restart the run using guild run --restart <run id> and be on your merry way, letting cron run every so often to refresh the backup. i dont think this scheme is terribly complicated yet its quite robust. thanks garrett, i understand the concern. coming from mlflow where you can set_tracking_uri to s3 remote, i thought guild also has this option. having the artifacts stored in a remote s3 bucket makes it easier to collaborate in a small team and track the progress. but after looking at the number of options available for guild run and guild push, it makes sense to have these two functions separate. i think its a great idea and one more option to run isnt a problem. i opened an issue on github to track progress this. github.com\/guildai\/guildai ability to sync with remote during run without explicit push command opened 01:25pm - 16 nov 20 utc gar1t it's convenient to be able to sync run artifacts with a remote (e.g. s3 bucket, etc.) as a function of the... enhancement",
        "Answer_original_content_gpt_summary":"Possible solutions extracted from the answer are:\n\n- Use ssh or ec2 remote type to run on a remote server.\n- Omit the --remote option to run the operation locally and use guild push s3-dev to copy those runs to s3.\n- Create a repeating command guild push that runs every 10 minutes, 30 minutes, etc. to always sync runs to your s3 bucket.\n- Use cron to run scheduled commands for syncing runs to your s3 bucket.\n- Open an issue on Github to track progress on the enhancement of syncing with remote during run without explicit push command.\n\nIn summary, the answer provides various solutions to the user's challenge of storing guild artifacts on s3 using `guild run -r s3-dev`.",
        "Answer_preprocessed_content":"s remotes are only for file storage. you cant run anything in s . if you want to run on a remote server, which is what youre asking for with the option, you need either an or remote type. if you want to run your operation locally, omit the option youll get runs in your current guild environment. then use to copy those runs to s . thanks a lot. now it is more clear. i misunderstood that if i run a script with a remote s config, my runs are automatically saved to s . i will do it manually by using thanks also for building this nice tool. i could see the value of automatically syncing with a remote env during and especially after a run. guild does not currently support this. i could see an enhancement to guild along the line of or that does this. thats a good idea. though that poor run command already has quite a few options its going to evolve into its own language grammar still, i like the thinking! in defense of guilds separation of concerns tao, you can accomplish what youre looking for this way this works on shells that support , which will execute the second command only when the first command succeeds . if you wanted to push regardless of the result, use note this will push all runs, not just the latest. as guild uses rsync protocols, this is efficient. but you may want to just push the latest. in that case add the argument to the push command. this tells guild to only push the latest run . wait, theres more! if you would like to always sync runs to your s bucket, you could create a repeating command that runs, say, every minutes, minutes, etc. most posix systems offer cron for this, but there are myriad ways to run scheduled commands. now this is moving you into sys op territory, which you might not want enter beware there be dragons but such is life. if this logic was moved into the command youd need to worry about command failure or system failure using something like cron is a nice separation of concerns because cron can on various failures to complete your backups even when guild or the system unexpected crashes. your system loses power during a long training run where you have various interim checkpoints. you restart, cron runs automatically to backup your partial runs. you can then restart the run using and be on your merry way, letting cron run every so often to refresh the backup. i dont think this scheme is terribly complicated yet its quite robust. thanks garrett, i understand the concern. coming from where you can to s remote, i thought guild also has this option. having the artifacts stored in a remote s bucket makes it easier to collaborate in a small team and track the progress. but after looking at the number of options available for and , it makes sense to have these two functions separate. i think its a great idea and one more option to isnt a problem. i opened an issue on github to track progress this. ability to sync with remote during run without explicit push command opened pm nov utc gar t it's convenient to be able to sync run artifacts with a remote as a function of enhancement"
    },
    {
        "Question_id":null,
        "Question_title":"Closing up a Sagemaker user profile - intended behavior?",
        "Question_body":"I had a Sagemaker user I wasn't using, so I tried to delete it and initially came across this tutorial: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html . As part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. The steps are:\n\nChoose the user.\nOn the User Details page, for each non-failed app in the Apps list, choose Delete app.\nOn the Delete app dialog, choose Yes, delete app, type delete in the confirmation field, and then choose Delete.\nWhen the Status for all apps show as Deleted, choose Delete user.\n\nThe problem comes on the final step: I wasn't able to find a \"Delete user\" button. This feels like a bug, because without such a button the only way to stop charges on a Sagemaker user is to use the CLI, which I eventually did. You can only delete the domain if you have deleted all users, meaning it only works using the CLI for that as well. For every other AWS service I've used, there is an easy way to delete everything from the GUI.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1640092026826,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":529.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmSJa7T1nRm6PQkiXDxZJqA\/closing-up-a-sagemaker-user-profile-intended-behavior",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-21T13:51:27.751Z",
                "Answer_score":0,
                "Answer_body":"Try clicking on the user, then Edit, and then Delete? I don't remember if that is the exact flow, but I do know that you can do it in the GUI. I've done it a few times.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: closing up a user profile - intended behavior?; Content: i had a user i wasn't using, so i tried to delete it and initially came across this tutorial: https:\/\/docs.aws.amazon.com\/\/latest\/dg\/gs-studio-delete-domain.html . as part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. the steps are: choose the user. on the user details page, for each non-failed app in the apps list, choose delete app. on the delete app dialog, choose yes, delete app, type delete in the confirmation field, and then choose delete. when the status for all apps show as deleted, choose delete user. the problem comes on the final step: i wasn't able to find a \"delete user\" button. this feels like a bug, because without such a button the only way to stop charges on a user is to use the cli, which i eventually did. you can only delete the domain if you have deleted all users, meaning it only works using the cli for that as well. for every other aws service i've used, there is an easy way to delete everything from the gui.",
        "Question_original_content_gpt_summary":"The user encountered challenges in closing up a user profile, as they were unable to find a \"delete user\" button and had to use the CLI to delete the domain and stop charges on the user.",
        "Question_preprocessed_content":"Title: closing up a user profile intended behavior?; Content: i had a user i wasn't using, so i tried to delete it and initially came across this tutorial . as part of the larger process for how to delete a domain, it shows how to delete any user profiles within that domain. the steps are choose the user. on the user details page, for each app in the apps list, choose delete app. on the delete app dialog, choose yes, delete app, type delete in the confirmation field, and then choose delete. when the status for all apps show as deleted, choose delete user. the problem comes on the final step i wasn't able to find a delete user button. this feels like a bug, because without such a button the only way to stop charges on a user is to use the cli, which i eventually did. you can only delete the domain if you have deleted all users, meaning it only works using the cli for that as well. for every other aws service i've used, there is an easy way to delete everything from the gui.",
        "Answer_original_content":"try clicking on the user, then edit, and then delete? i don't remember if that is the exact flow, but i do know that you can do it in the gui. i've done it a few times.",
        "Answer_original_content_gpt_summary":"Possible solutions to closing a user profile include clicking on the user, then editing, and then deleting in the GUI. It is unclear if this is the exact flow, but the answerer has done it a few times.",
        "Answer_preprocessed_content":"try clicking on the user, then edit, and then delete? i don't remember if that is the exact flow, but i do know that you can do it in the gui. i've done it a few times."
    },
    {
        "Question_id":null,
        "Question_title":"Dvc push to public S3 bucket",
        "Question_body":"<p>Hello there. I\u2019m excited to try DVC\u2019s features but I\u2019m stuck with a basic problem in S3. Ideally, I\u2019d like to push to a private S3 bucket, which I could do with <code>aws-vault<\/code>. Before doing that, I wanted to check how that would work in a public bucket. But when I try to push to a public S3, I get:<\/p>\n<pre><code>$ dvc remote list\nstorage\ts3:\/\/ml-ci\nhttps3\thttps:\/\/ml-ci.s3.amazonaws.com\/\n\n$ dvc push -r storage\nERROR: unexpected error - An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>So I thought I\u2019d try a suggestion from another thread on this forum and add a HTTPS remote endpoint instead (though I know it was suggested as a read-only solution):<\/p>\n<pre><code>$ dvc push -r https3\nERROR: failed to upload '.dvc\/cache\/a3\/04afb96060aad9017xxxx' to 'https:\/\/ml-ci.s3.amazonaws.com\/a3\/04afb96060aad9017xxxx' - could not perform a POST request\nERROR: failed to push data to the cloud - 1 files failed to upload\n<\/code><\/pre>\n<p>What am I doing incorrectly here with remote setup? Is private + <code>aws-vault<\/code> the only option? I\u2019d really appreciate some help, thank you!<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":null,
        "Question_creation_time":1595914363359,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":2830.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-to-public-s3-bucket\/457",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-28T13:47:42.512Z",
                "Answer_body":"<p>I\u2019m having a similar issue with pushing to a private bucket. It might be related to IAM roles.<\/p>\n<p>In my case I also get a 404 when I do <code>dvc push<\/code>. I get the same error if I just run <code>aws s3 ls s3:\/\/bucket-name<\/code>.<\/p>\n<p>However, if I add my work profile name it works: <code>aws s3 ls s3:\/\/bucket-name --profile work-profile<\/code><\/p>\n<p>I\u2019ve tried setting the AWS_PROFILE env variable to <code>work-profile<\/code>, but nothing changes. Any idea how I force it to see the right profile?<\/p>",
                "Answer_score":93.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-28T13:55:08.128Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/sumita\">@sumita<\/a> Are you able to use <code>aws<\/code> CLI utility with it?<\/p>\n<p><a class=\"mention\" href=\"\/u\/braaannigan\">@braaannigan<\/a> How do you set AWS_PROFILE env var? If awscli with <code>--profile<\/code> works, then dvc also should. We have a <code>profile<\/code> config option <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify<\/a> , e.g.:<\/p>\n<pre><code class=\"lang-auto\">dvc remote modify myremote profile work-profile\n<\/code><\/pre>",
                "Answer_score":93.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-28T14:02:21.072Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a><\/p>\n<p>I\u2019ve found the answer on an old thread (with a slight modification of the credential path):<br>\ndvc remote modify myremote credentialpath ~\/.aws\/credentials<br>\ndvc remote modify myremote profile profile-name<\/p>\n<p>Thanks<\/p>",
                "Answer_score":393.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-09T16:56:07.055Z",
                "Answer_body":"<p>I eventually solved this with aws-vault, but I missed that there\u2019s documentation about how to use dvc remote to add credentials here: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type<\/a>.<\/p>\n<p>Thank you!<\/p>",
                "Answer_score":87.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-15T03:18:05.533Z",
                "Answer_body":"<p>Hi!<\/p>\n<p>I\u2019ve added this comment about improving the S3 remote error:<\/p>\n<aside class=\"onebox githubissue\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3050#issuecomment-674340656\" target=\"_blank\" rel=\"noopener\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewBox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3050#issuecomment-674340656\" target=\"_blank\" rel=\"noopener\">exceptions: general refactor<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2020-01-03\" data-time=\"21:04:37\" data-timezone=\"UTC\">09:04PM - 03 Jan 20 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/mroutis\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"mroutis\" src=\"https:\/\/avatars0.githubusercontent.com\/u\/7363250?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          mroutis\n        <\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n<\/div>\n\n<div class=\"github-row\">\n  <p class=\"github-content\">Improving exceptions would bring a better experience to DVC users and developers:\n\nMove exceptions to a single module (dvc\/exceptions.py), there would be...<\/p>\n<\/div>\n\n<div class=\"labels\">\n    <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">discussion<\/span>\n    <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">p3-nice-to-have<\/span>\n    <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">refactoring<\/span>\n<\/div>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Thanks<\/p>",
                "Answer_score":277.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-15T04:01:55.065Z",
                "Answer_body":"<p>p.s. <a class=\"mention\" href=\"\/u\/sumita\">@sumita<\/a> you might want to take a look at <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/1699\">https:\/\/github.com\/iterative\/dvc.org\/issues\/1699<\/a> and <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/1700\">https:\/\/github.com\/iterative\/dvc.org\/issues\/1700<\/a>.<\/p>\n<p><a href=\"https:\/\/dvc.org\/doc\/user-guide\/contributing\/docs\">Docs contributors<\/a> are welcome! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_score":56.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-17T16:49:20.896Z",
                "Answer_body":"<p>Thanks so much <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a>!  I looked at both the issues and the comments you left on the post (which I\u2019ll respond to there). The issues make sense and I look forward to those enhancements - thanks again.<\/p>",
                "Answer_score":51.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: push to public s3 bucket; Content: hello there. i\u2019m excited to try \u2019s features but i\u2019m stuck with a basic problem in s3. ideally, i\u2019d like to push to a private s3 bucket, which i could do with aws-vault. before doing that, i wanted to check how that would work in a public bucket. but when i try to push to a public s3, i get: $ remote list storage s3:\/\/ml-ci https3 https:\/\/ml-ci.s3.amazonaws.com\/ $ push -r storage error: unexpected error - an error occurred (403) when calling the headobject operation: forbidden so i thought i\u2019d try a suggestion from another thread on this forum and add a https remote endpoint instead (though i know it was suggested as a read-only solution): $ push -r https3 error: failed to upload '.\/cache\/a3\/04afb96060aad9017xxxx' to 'https:\/\/ml-ci.s3.amazonaws.com\/a3\/04afb96060aad9017xxxx' - could not perform a post request error: failed to push data to the cloud - 1 files failed to upload what am i doing incorrectly here with remote setup? is private + aws-vault the only option? i\u2019d really appreciate some help, thank you!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with pushing to a public S3 bucket, receiving errors when attempting to do so, and is unsure if private + aws-vault is the only option.",
        "Question_preprocessed_content":"Title: push to public s bucket; Content: hello there. im excited to try s features but im stuck with a basic problem in s . ideally, id like to push to a private s bucket, which i could do with . before doing that, i wanted to check how that would work in a public bucket. but when i try to push to a public s , i get so i thought id try a suggestion from another thread on this forum and add a https remote endpoint instead what am i doing incorrectly here with remote setup? is private + the only option? id really appreciate some help, thank you!",
        "Answer_original_content":"im having a similar issue with pushing to a private bucket. it might be related to iam roles. in my case i also get a 404 when i do push. i get the same error if i just run aws s3 ls s3:\/\/bucket-name. however, if i add my work profile name it works: aws s3 ls s3:\/\/bucket-name --profile work-profile ive tried setting the aws_profile env variable to work-profile, but nothing changes. any idea how i force it to see the right profile? @sumita are you able to use aws cli utility with it? @braaannigan how do you set aws_profile env var? if awscli with --profile works, then also should. we have a profile config option https:\/\/.org\/doc\/command-reference\/remote\/modify , e.g.: remote modify myremote profile work-profile hi @kupruser ive found the answer on an old thread (with a slight modification of the credential path): remote modify myremote credentialpath ~\/.aws\/credentials remote modify myremote profile profile-name thanks i eventually solved this with aws-vault, but i missed that theres documentation about how to use remote to add credentials here: https:\/\/.org\/doc\/command-reference\/remote\/modify#available-parameters-per-storage-type. thank you! hi! ive added this comment about improving the s3 remote error: github.com\/iterative\/ exceptions: general refactor opened 09:04pm - 03 jan 20 utc mroutis improving exceptions would bring a better experience to users and developers: move exceptions to a single module (\/exceptions.py), there would be... discussion p3-nice-to-have refactoring thanks p.s. @sumita you might want to take a look at https:\/\/github.com\/iterative\/.org\/issues\/1699 and https:\/\/github.com\/iterative\/.org\/issues\/1700. docs contributors are welcome! thanks so much @jorgeorpinel! i looked at both the issues and the comments you left on the post (which ill respond to there). the issues make sense and i look forward to those enhancements - thanks again.",
        "Answer_original_content_gpt_summary":"The answer provides several possible solutions to the user's issue with pushing to a public S3 bucket. One solution is to add the work profile name to the AWS CLI command. Another solution is to modify the remote profile using the remote modify command. The answer also suggests using aws-vault and improving the S3 remote error. Additionally, the answer provides links to relevant documentation and issues for further assistance.",
        "Answer_preprocessed_content":"im having a similar issue with pushing to a private bucket. it might be related to iam roles. in my case i also get a when i do . i get the same error if i just run . however, if i add my work profile name it works ive tried setting the env variable to , but nothing changes. any idea how i force it to see the right profile? are you able to use cli utility with it? how do you set env var? if awscli with works, then also should. we have a config option , hi ive found the answer on an old thread remote modify myremote credentialpath remote modify myremote profile thanks i eventually solved this with but i missed that theres documentation about how to use remote to add credentials here thank you! hi! ive added this comment about improving the s remote error exceptions general refactor opened pm jan utc mroutis improving exceptions would bring a better experience to users and developers move exceptions to a single module , there would discussion refactoring thanks you might want to take a look at and docs contributors are welcome! thanks so much i looked at both the issues and the comments you left on the post . the issues make sense and i look forward to those enhancements thanks again."
    },
    {
        "Question_id":null,
        "Question_title":"MLOps using Azure Databricks & Azure ML - question on data prep for model inference and retraining.",
        "Question_body":"I am using this blog (https:\/\/databricks.com\/blog\/2020\/10\/13\/using-mlops-with-mlflow-and-azure.html) to set-up MLOps using Azure Databricks & Azure ML. As mentioned in the blog, we deploy MLflow model into an Azure ML environment using the built in MLflow deployment capabilities, which is used for inference. A couple of questions -\n1. How and where does the data prep come into picture before inference and how I can integrate that piece.\n2. How to create a re-training workflow for the model?\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1607958581913,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/197618\/mlops-using-azure-databricks-amp-azure-ml-question.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-15T08:22:17.753Z",
                "Answer_score":0,
                "Answer_body":"@KiranPurushotham-8848 Thanks, using Databricks to build models and track using MLFlow. Then wants to deploy the model using MLFlow->AML service integration and wants to monitor the model. To work around the limitation of MLflow deployment, you can switch to AML deployment but use the model created and registered by MLFlow at AML.\nFirst, add mflow to conda dependencies to be able to use it in your scoring script, then in init method, load the model using mlflow API, for example:\nmodel = mlflow.pytorch.load_model(model_dir)\nYou need to check artifact structure of the mode registered in AML to construct model_dir correctly because it was created using MLFlow API.\n\nYou may implement ML Ops with a hybrid setup:\nCloud Part:\n\u2022 Azure DevOps can orchestrate Azure ML Service for MLOps practices.\n\u2022 Azure ML Service can be used to training and orchestrating model development, an MLOps manual in link.\n\nOn Prems:\n\u2022 We can train models using data & CPU power on local, on prems.\n\u2022 We can run Azure DevOps pipelines on prems with the Azure DevOps Server running on an On Prems Hardware.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: mlops using azure databricks & - question on data prep for model inference and retraining.; Content: i am using this blog (https:\/\/databricks.com\/blog\/2020\/10\/13\/using-mlops-with--and-azure.html) to set-up mlops using azure databricks & . as mentioned in the blog, we deploy model into an environment using the built in deployment capabilities, which is used for inference. a couple of questions - 1. how and where does the data prep come into picture before inference and how i can integrate that piece. 2. how to create a re-training workflow for the model? thanks.",
        "Question_original_content_gpt_summary":"The user is looking for guidance on how to integrate data preparation into their MLOps workflow using Azure Databricks and MLFlow, as well as how to create a retraining workflow for their model.",
        "Question_preprocessed_content":"Title: mlops using azure databricks & question on data prep for model inference and retraining.; Content: i am using this blog to mlops using azure databricks & . as mentioned in the blog, we deploy model into an environment using the built in deployment capabilities, which is used for inference. a couple of questions . how and where does the data prep come into picture before inference and how i can integrate that piece. . how to create a workflow for the model? thanks.",
        "Answer_original_content":"@kiranpurushotham-8848 thanks, using databricks to build models and track using mlflow. then wants to deploy the model using mlflow->aml service integration and wants to monitor the model. to work around the limitation of mlflow deployment, you can switch to aml deployment but use the model created and registered by mlflow at aml. first, add mflow to conda dependencies to be able to use it in your scoring script, then in init method, load the model using mlflow api, for example: model = mlflow.pytorch.load_model(model_dir) you need to check artifact structure of the mode registered in aml to construct model_dir correctly because it was created using mlflow api. you may implement ml ops with a hybrid setup: cloud part: azure devops can orchestrate service for mlops practices. service can be used to training and orchestrating model development, an mlops manual in link. on prems: we can train models using data & cpu power on local, on prems. we can run azure devops pipelines on prems with the azure devops server running on an on prems hardware.",
        "Answer_original_content_gpt_summary":"Possible solutions to integrating data preparation into MLOps workflow using Azure Databricks and MLFlow and creating a retraining workflow for the model include building models and tracking them using Databricks and MLFlow, deploying the model using MLFlow->AML service integration, monitoring the model, and implementing ML Ops with a hybrid setup. The hybrid setup involves using Azure DevOps to orchestrate services for MLOps practices in the cloud and training models using data and CPU power on-premises. Azure DevOps pipelines can be run on-premises with the Azure DevOps server running on on-premises hardware.",
        "Answer_preprocessed_content":"thanks, using databricks to build models and track using . then wants to deploy the model using service integration and wants to monitor the model. to work around the limitation of deployment, you can switch to aml deployment but use the model created and registered by at aml. first, add mflow to conda dependencies to be able to use it in your scoring script, then in init method, load the model using api, for example model you need to check artifact structure of the mode registered in aml to construct correctly because it was created using api. you may implement ml ops with a hybrid setup cloud part azure devops can orchestrate service for mlops practices. service can be used to training and orchestrating model development, an mlops manual in link. on prems we can train models using data & cpu power on local, on prems. we can run azure devops pipelines on prems with the azure devops server running on an on prems hardware."
    },
    {
        "Question_id":59648275.0,
        "Question_title":"What to define as entrypoint when initializing a pytorch estimator with a custom docker image for training on AWS Sagemaker?",
        "Question_body":"<p>So I created a docker image for training. In the dockerfile I have an entrypoint defined such that when <code>docker run<\/code> is executed, it will start running my python code.\nTo use this on aws sagemaker in my understanding I need to create a pytorch estimator in a jupyter notebook in sagemaker. I tried something like this:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.pytorch import PyTorch\n\nsagemaker_session = sagemaker.Session()\n\nrole = sagemaker.get_execution_role()\n\nestimator = PyTorch(entry_point='train.py',\n                    role=role,\n                    framework_version='1.3.1',\n                    image_name='xxx.ecr.eu-west-1.amazonaws.com\/xxx:latest',\n                    train_instance_count=1,\n                    train_instance_type='ml.p3.xlarge',\n                    hyperparameters={})\n\nestimator.fit({})\n\n<\/code><\/pre>\n\n<p>In the documentation I found that as image name I can specify the link the my docker image on aws ecr. When I try to execute this it keeps complaining<\/p>\n\n<pre><code>[Errno 2] No such file or directory: 'train.py'\n<\/code><\/pre>\n\n<p>It complains immidiatly, so surely I am doing something completely wrong. I would expect that first my docker image should run, and than it could find out that the entry point does not exist.<\/p>\n\n<p>But besides this, why do I need to specify an entry point, as in, should it not be clear that the entry to my training is simply <code>docker run<\/code>?<\/p>\n\n<p>For maybe better understanding. The entrypoint python file in my docker image looks like this:<\/p>\n\n<pre><code>if __name__=='__main__':\n    parser = argparse.ArgumentParser()\n\n    # Hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=5)\n    parser.add_argument('--batch_size', type=int, default=16)\n    parser.add_argument('--learning_rate', type=float, default=0.0001)\n\n    # Data and output directories\n    parser.add_argument('--output_data_dir', type=str, default=os.environ['OUTPUT_DATA_DIR'])\n    parser.add_argument('--train_data_path', type=str, default=os.environ['CHANNEL_TRAIN'])\n    parser.add_argument('--valid_data_path', type=str, default=os.environ['CHANNEL_VALID'])\n\n    # Start training\n    ...\n<\/code><\/pre>\n\n<p>Later I would like to specify the hyperparameters and data channels. But for now I simply do not understand what to put as entry point. In the documentation it says that the entrypoint is required and it should be a local\/global path to the entrypoint...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1578494679740,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":768.0,
        "Owner_creation_time":1447151270223,
        "Owner_last_access_time":1663856936432,
        "Owner_reputation":141.0,
        "Owner_up_votes":31.0,
        "Owner_down_votes":0.0,
        "Owner_views":9.0,
        "Answer_body":"<p>If you really would like to use a complete separate by yourself build docker image, you should create an Amazon Sagemaker algorithm (which is one of the options in the Sagemaker menu). Here you have to specify a link to your docker image on amazon ECR as well as the input parameters and data channels etc. When choosing this options, you should <strong>not<\/strong> use the PyTorch estimater but the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/algorithm.html\" rel=\"nofollow noreferrer\">Algoritm estimater<\/a>. This way you indeed don't have to specify an entrypoint because it simple runs the docker when training and the default entrypoint can be defined in your docker file.<\/p>\n\n<p>The Pytorch estimator can be used when having you own model code, but you would like to run this code in an off-the-shelf Sagemaker PyTorch docker image. This is why you have to for example specify the PyTorch framework version. In this case the entrypoint file by default should be placed next to where your jupyter notebook is stored (just upload the file by clicking on the upload button). The PyTorch estimator inherits all options from the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"nofollow noreferrer\">framework estimator<\/a> where options can be found where to place the entrypoint and model, for example <em>source_dir<\/em>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579270087087,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1578495990750,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59648275",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what to define as entrypoint when initializing a pytorch estimator with a custom docker image for training on ?; Content: so i created a docker image for training. in the dockerfile i have an entrypoint defined such that when docker run is executed, it will start running my python code. to use this on in my understanding i need to create a pytorch estimator in a jupyter notebook in . i tried something like this: import from .pytorch import pytorch _session = .session() role = .get_execution_role() estimator = pytorch(entry_point='train.py', role=role, framework_version='1.3.1', image_name='xxx.ecr.eu-west-1.amazonaws.com\/xxx:latest', train_instance_count=1, train_instance_type='ml.p3.xlarge', hyperparameters={}) estimator.fit({}) in the documentation i found that as image name i can specify the link the my docker image on aws ecr. when i try to execute this it keeps complaining [errno 2] no such file or directory: 'train.py' it complains immidiatly, so surely i am doing something completely wrong. i would expect that first my docker image should run, and than it could find out that the entry point does not exist. but besides this, why do i need to specify an entry point, as in, should it not be clear that the entry to my training is simply docker run? for maybe better understanding. the entrypoint python file in my docker image looks like this: if __name__=='__main__': parser = argparse.argumentparser() # hyperparameters sent by the client are passed as command-line arguments to the script. parser.add_argument('--epochs', type=int, default=5) parser.add_argument('--batch_size', type=int, default=16) parser.add_argument('--learning_rate', type=float, default=0.0001) # data and output directories parser.add_argument('--output_data_dir', type=str, default=os.environ['output_data_dir']) parser.add_argument('--train_data_path', type=str, default=os.environ['channel_train']) parser.add_argument('--valid_data_path', type=str, default=os.environ['channel_valid']) # start training ... later i would like to specify the hyperparameters and data channels. but for now i simply do not understand what to put as entry point. in the documentation it says that the entrypoint is required and it should be a local\/global path to the entrypoint...",
        "Question_original_content_gpt_summary":"The user is encountering challenges with defining an entrypoint when initializing a PyTorch Estimator with a custom Docker image for training on.",
        "Question_preprocessed_content":"Title: what to define as entrypoint when initializing a pytorch estimator with a custom docker image for training on ?; Content: so i created a docker image for training. in the dockerfile i have an entrypoint defined such that when is executed, it will start running my python code. to use this on in my understanding i need to create a pytorch estimator in a jupyter notebook in . i tried something like this in the documentation i found that as image name i can specify the link the my docker image on aws ecr. when i try to execute this it keeps complaining it complains immidiatly, so surely i am doing something completely wrong. i would expect that first my docker image should run, and than it could find out that the entry point does not exist. but besides this, why do i need to specify an entry point, as in, should it not be clear that the entry to my training is simply ? for maybe better understanding. the entrypoint python file in my docker image looks like this later i would like to specify the hyperparameters and data channels. but for now i simply do not understand what to put as entry point. in the documentation it says that the entrypoint is required and it should be a path to the",
        "Answer_original_content":"if you really would like to use a complete separate by yourself build docker image, you should create an algorithm (which is one of the options in the menu). here you have to specify a link to your docker image on amazon ecr as well as the input parameters and data channels etc. when choosing this options, you should not use the pytorch estimater but the algoritm estimater. this way you indeed don't have to specify an entrypoint because it simple runs the docker when training and the default entrypoint can be defined in your docker file. the pytorch estimator can be used when having you own model code, but you would like to run this code in an off-the-shelf pytorch docker image. this is why you have to for example specify the pytorch framework version. in this case the entrypoint file by default should be placed next to where your jupyter notebook is stored (just upload the file by clicking on the upload button). the pytorch estimator inherits all options from the framework estimator where options can be found where to place the entrypoint and model, for example source_dir.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. If the user wants to use a completely separate custom-built Docker image, they should create an algorithm and specify a link to their Docker image on Amazon ECR, input parameters, and data channels. They should not use the PyTorch estimator but the algorithm estimator. This way, they don't have to specify an entrypoint because it simply runs the Docker when training, and the default entrypoint can be defined in the Docker file.\n\n2. If the user has their own model code but wants to run it in an off-the-shelf PyTorch Docker image, they can use the PyTorch estimator. They need to specify the PyTorch framework version, and the entrypoint file should be placed next to where their Jupyter notebook is stored. The PyTorch estimator inherits all options from the framework estimator, where options can be found for where to place the entrypoint and model, for example, source_dir.",
        "Answer_preprocessed_content":"if you really would like to use a complete separate by yourself build docker image, you should create an algorithm . here you have to specify a link to your docker image on amazon ecr as well as the input parameters and data channels etc. when choosing this options, you should not use the pytorch estimater but the algoritm estimater. this way you indeed don't have to specify an entrypoint because it simple runs the docker when training and the default entrypoint can be defined in your docker file. the pytorch estimator can be used when having you own model code, but you would like to run this code in an pytorch docker image. this is why you have to for example specify the pytorch framework version. in this case the entrypoint file by default should be placed next to where your jupyter notebook is stored . the pytorch estimator inherits all options from the framework estimator where options can be found where to place the entrypoint and model, for example"
    },
    {
        "Question_id":44932098.0,
        "Question_title":"Azure ML- Execute Python Script -Datatime.date not working",
        "Question_body":"<p>I am getting the following error when I try to convert a datetime variable to date.<\/p>\n\n<p><strong>My Code<\/strong><\/p>\n\n<pre><code>import datetime as dt \n\ndf['TXN_DATE_2'] = df['TXN_DATE'].dt.date\n<\/code><\/pre>\n\n<p><strong>Error<\/strong><\/p>\n\n<blockquote>\n  <p>raise NotImplementedError('Python Bridge conversion table not\n  implemented for type [{0}]'.format(value.getType()))\n  NotImplementedError: Python Bridge conversion table not implemented\n  for type [] Process returned with non-zero exit\n  code 1<\/p>\n<\/blockquote>\n\n<p>Can anyone please tell me what is going on.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1499274119590,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":449.0,
        "Owner_creation_time":1499273894443,
        "Owner_last_access_time":1645126505767,
        "Owner_reputation":35.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":12.0,
        "Answer_body":"<p>Please try to use the code below to convert as you want.<\/p>\n\n<pre><code>import pandas as pd\nimport datetime as dt\ndf['TXN_DATE_2'] = pd.to_datetime(df['TXN_DATE']).dt.date\n<\/code><\/pre>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1499329512083,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1499281298240,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44932098",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - execute python script -datatime.date not working; Content: i am getting the following error when i try to convert a datetime variable to date. my code import datetime as dt df['txn_date_2'] = df['txn_date'].dt.date error raise notimplementederror('python bridge conversion table not implemented for type [{0}]'.format(value.gettype())) notimplementederror: python bridge conversion table not implemented for type [] process returned with non-zero exit code 1 can anyone please tell me what is going on.",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to convert a datetime variable to date using the datetime module in Python.",
        "Question_preprocessed_content":"Title: execute python script not working; Content: i am getting the following error when i try to convert a datetime variable to date. my code error raise notimplementederror notimplementederror python bridge conversion table not implemented for type process returned with exit code can anyone please tell me what is going on.",
        "Answer_original_content":"please try to use the code below to convert as you want. import pandas as pd import datetime as dt df['txn_date_2'] = pd.to_datetime(df['txn_date']).dt.date hope it helps.",
        "Answer_original_content_gpt_summary":"Solution: The answer suggests using the pandas library to convert the datetime variable to date. The code provided imports pandas and datetime modules and uses the pd.to_datetime() function to convert the datetime variable to date. The .dt.date method is then used to extract the date from the datetime object.",
        "Answer_preprocessed_content":"please try to use the code below to convert as you want. hope it helps."
    },
    {
        "Question_id":56835306.0,
        "Question_title":"Download file using boto3 within Docker container deployed on Sagemaker Endpoint",
        "Question_body":"<p>I have built my own Docker container that provides inference code to be deployed as endpoint on Amazon Sagemaker. However, this container needs to have access to some files from s3. The used IAM role has access to all s3 buckets that I am trying to reach.<\/p>\n\n<p>Code to download files using a boto3 client:<\/p>\n\n<pre><code>import boto3\n\nmodel_bucket = 'my-bucket'\n\ndef download_file_from_s3(s3_path, local_path):\n    client = boto3.client('s3')\n    client.download_file(model_bucket, s3_path, local_path)\n<\/code><\/pre>\n\n<p>The IAM role's policies:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::my-bucket\/*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Starting the docker container locally allows me to download files from s3 just like expected. <\/p>\n\n<p>Deploying as an endpoint on Sagemaker, however, the request times out:<\/p>\n\n<pre><code>botocore.vendored.requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='my-bucket.s3.eu-central-1.amazonaws.com', port=443): Max retries exceeded with url: \/path\/to\/my-file (Caused by ConnectTimeoutError(&lt;botocore.awsrequest.AWSHTTPSConnection object at 0x7f66244e69b0&gt;, 'Connection to my-bucket.s3.eu-central-1.amazonaws.com timed out. (connect timeout=60)'))\n<\/code><\/pre>\n\n<p>Any help is appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1561982365937,
        "Question_favorite_count":1.0,
        "Question_score":4.0,
        "Question_view_count":1171.0,
        "Owner_creation_time":1456411465888,
        "Owner_last_access_time":1663933136732,
        "Owner_reputation":105.0,
        "Owner_up_votes":11.0,
        "Owner_down_votes":0.0,
        "Owner_views":25.0,
        "Answer_body":"<p>For anyone coming across this question, when creating a model, the 'Enable Network Isolation' property defaults to True.\nFrom AWS docs:<\/p>\n<blockquote>\n<p>If you enable network isolation, the containers are not able to make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment.<\/p>\n<\/blockquote>\n<p>So this property needs to be set to False in order to connect to any other AWS service.<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/5qERm.jpg\" alt=\"AWS Sagemaker UI Network Isolation set to False\" \/><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1562142120396,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1641299334400,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56835306",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: download file using boto3 within docker container deployed on endpoint; Content: i have built my own docker container that provides inference code to be deployed as endpoint on . however, this container needs to have access to some files from s3. the used iam role has access to all s3 buckets that i am trying to reach. code to download files using a boto3 client: import boto3 model_bucket = 'my-bucket' def download_file_from_s3(s3_path, local_path): client = boto3.client('s3') client.download_file(model_bucket, s3_path, local_path) the iam role's policies: { \"version\": \"2012-10-17\", \"statement\": [ { \"action\": [ \"s3:getobject\", \"s3:putobject\", \"s3:deleteobject\" ], \"effect\": \"allow\", \"resource\": [ \"arn:aws:s3:::my-bucket\/*\" ] } ] } starting the docker container locally allows me to download files from s3 just like expected. deploying as an endpoint on , however, the request times out: botocore.vendored.requests.exceptions.connecttimeout: httpsconnectionpool(host='my-bucket.s3.eu-central-1.amazonaws.com', port=443): max retries exceeded with url: \/path\/to\/my-file (caused by connecttimeouterror(<botocore.awsrequest.awshttpsconnection object at 0x7f66244e69b0>, 'connection to my-bucket.s3.eu-central-1.amazonaws.com timed out. (connect timeout=60)')) any help is appreciated!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with downloading files from an S3 bucket using a boto3 client within a Docker container deployed on an endpoint.",
        "Question_preprocessed_content":"Title: download file using boto within docker container deployed on endpoint; Content: i have built my own docker container that provides inference code to be deployed as endpoint on . however, this container needs to have access to some files from s . the used iam role has access to all s buckets that i am trying to reach. code to download files using a boto client the iam role's policies starting the docker container locally allows me to download files from s just like expected. deploying as an endpoint on , however, the request times out any help is appreciated!",
        "Answer_original_content":"for anyone coming across this question, when creating a model, the 'enable network isolation' property defaults to true. from aws docs: if you enable network isolation, the containers are not able to make any outbound network calls, even to other aws services such as amazon s3. additionally, no aws credentials are made available to the container runtime environment. so this property needs to be set to false in order to connect to any other aws service.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of downloading files from an S3 bucket using a boto3 client within a Docker container deployed on an endpoint is to set the 'enable network isolation' property to false when creating a model. This will allow the container to make outbound network calls, including to other AWS services such as Amazon S3, and make AWS credentials available to the container runtime environment.",
        "Answer_preprocessed_content":"for anyone coming across this question, when creating a model, the 'enable network isolation' property defaults to true. from aws docs if you enable network isolation, the containers are not able to make any outbound network calls, even to other aws services such as amazon s . additionally, no aws credentials are made available to the container runtime environment. so this property needs to be set to false in order to connect to any other aws service."
    },
    {
        "Question_id":70933814.0,
        "Question_title":"SageMaker custom model output path for tensorflow when creating from s3 artifacts",
        "Question_body":"<p>I'm running the following code to create an endpoint with a preexisting model:<\/p>\n<pre><code>from sagemaker.tensorflow import serving\nsagemaker_session = sagemaker.Session()\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',\n                                     entry_point=&quot;inference.py&quot;,\n                                     source_dir=&quot;inf_source_dir&quot;,\n                                     role=get_execution_role(),\n                                     framework_version='1.14',\n                                     sagemaker_session=sagemaker_session)\n<\/code><\/pre>\n<p>However this create a copy of the model into the default sagemaker bucket. How can I pass a custom path? I've tried model_dir, and output_path but neither are accepted as parameters<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1643669778813,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":216.0,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_reputation":1387.0,
        "Owner_up_votes":51.0,
        "Owner_down_votes":1.0,
        "Owner_views":153.0,
        "Answer_body":"<p>The SageMaker Python SDK repackages your model to include your <code>entry_point<\/code> and <code>source_dir<\/code> files and uploads this &quot;new&quot; tar ball to the SageMaker default bucket.<\/p>\n<p>You can change this behavior by setting the <code>default_bucket<\/code> in your <code>sagemaker_session<\/code> as follows:<\/p>\n<pre><code>sagemaker_session = sagemaker.Session(default_bucket=&quot;&lt;mybucket&gt;&quot;)\n\nclf_sm_model = serving.Model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz',         \n                    .\n                    .\n                    sagemaker_session=sagemaker_session)\n                    .\n                    )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645727041932,
        "Answer_score":3.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70933814",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: custom model output path for tensorflow when creating from s3 artifacts; Content: i'm running the following code to create an endpoint with a preexisting model: from .tensorflow import serving _session = .session() clf_sm_model = serving.model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz', entry_point=\"inference.py\", source_dir=\"inf_source_dir\", role=get_execution_role(), framework_version='1.14', _session=_session) however this create a copy of the model into the default bucket. how can i pass a custom path? i've tried model_dir, and output_path but neither are accepted as parameters",
        "Question_original_content_gpt_summary":"The user is encountering challenges with customizing the output path for a TensorFlow model when creating an endpoint from S3 artifacts.",
        "Question_preprocessed_content":"Title: custom model output path for tensorflow when creating from s artifacts; Content: i'm running the following code to create an endpoint with a preexisting model however this create a copy of the model into the default bucket. how can i pass a custom path? i've tried and but neither are accepted as parameters",
        "Answer_original_content":"the python sdk repackages your model to include your entry_point and source_dir files and uploads this \"new\" tar ball to the default bucket. you can change this behavior by setting the default_bucket in your _session as follows: _session = .session(default_bucket=\"<mybucket>\") clf_sm_model = serving.model(model_data='s3:\/\/mybucket\/mytrainedmodel\/model.tar.gz', . . _session=_session) . )",
        "Answer_original_content_gpt_summary":"Possible solution: To customize the output path for a TensorFlow model when creating an endpoint from S3 artifacts, the user can set the default_bucket in their _session to their desired bucket. This will change the behavior of the python sdk to upload the \"new\" tar ball to the specified bucket instead of the default bucket.",
        "Answer_preprocessed_content":"the python sdk repackages your model to include your and files and uploads this new tar ball to the default bucket. you can change this behavior by setting the in your as follows"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker GT Streaming Labelling Job Internal Server Error (Job Failed)",
        "Question_body":"I launched a Sagemaker Ground Truth Labeling Job with a vendor for 3D Point Cloud Object Tracking using the API. Yesterday, my job failed with the reason for failure\n\nInternalServerError: We encountered an internal error. Submit a new job.\n\n\nThe last three Cloudwatch logs before I saw the failure were\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"BATCH_ANNOTATION_STATUS_EVALUATED\", \"event-log-message\": \"Batch of annotation tasks completed with status FAILED. \" }\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"EXPORTED_LABELED_MANIFEST\", \"event-log-message\": \"Labeled manifest written to S3 output location.\" }\n\n{ \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"IDLE_TIMER_COUNT_INTERRUPTED\", \"event-log-message\": \"System detected incoming objects. Timer to monitor idleness was reset.\" }\n\nMy vendor confirmed that they had annotations saved before this happened, but my output manifest is empty. In addition, I set the expiration time for my job to be 30 days, and this failed at 20 days. What could have caused this error and is there any known method for retrieving the worker annotations that were saved but not submitted?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1661525322145,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":61.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZD0isCgLSYqxXhdah_0jkQ\/sage-maker-gt-streaming-labelling-job-internal-server-error-job-failed",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-01T20:36:14.799Z",
                "Answer_score":0,
                "Answer_body":"I am afraid to inform you that the \"Internal Server Error\" can occur due to various reasons, In order to deep-dive further and investigate in to the service level logs we would request you to open a support ticket with the AWS premium support with the following information.\n\n-- Account Id\n\n-- Region\n\n-- Failed Job ARNs and cloudwatch logs\n\nNote :Due to security reason, this post is not suitable for sharing customer's resource and account details.\n\nThank you.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: gt streaming labelling job internal server error (job failed); Content: i launched a ground truth labeling job with a vendor for 3d point cloud object tracking using the api. yesterday, my job failed with the reason for failure internalservererror: we encountered an internal error. submit a new job. the last three cloudwatch logs before i saw the failure were { \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"batch_annotation_status_evaluated\", \"event-log-message\": \"batch of annotation tasks completed with status failed. \" } { \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"exported_labeled_manifest\", \"event-log-message\": \"labeled manifest written to s3 output location.\" } { \"labeling-job-name\": \"scand-trial2-seq10v2stream\", \"event-name\": \"idle_timer_count_interrupted\", \"event-log-message\": \"system detected incoming objects. timer to monitor idleness was reset.\" } my vendor confirmed that they had annotations saved before this happened, but my output manifest is empty. in addition, i set the expiration time for my job to be 30 days, and this failed at 20 days. what could have caused this error and is there any known method for retrieving the worker annotations that were saved but not submitted?",
        "Question_original_content_gpt_summary":"The user encountered an internal server error while launching a Ground Truth Labeling job with a vendor for 3D point cloud object tracking, resulting in an empty output manifest despite the vendor confirming that annotations had been saved before the error occurred.",
        "Question_preprocessed_content":"Title: gt streaming labelling job internal server error ; Content: i launched a ground truth labeling job with a vendor for d point cloud object tracking using the api. yesterday, my job failed with the reason for failure internalservererror we encountered an internal error. submit a new job. the last three cloudwatch logs before i saw the failure were batch of annotation tasks completed with status failed. labeled manifest written to s output system detected incoming objects. timer to monitor idleness was my vendor confirmed that they had annotations saved before this happened, but my output manifest is empty. in addition, i set the expiration time for my job to be days, and this failed at days. what could have caused this error and is there any known method for retrieving the worker annotations that were saved but not submitted?",
        "Answer_original_content":"i am afraid to inform you that the \"internal server error\" can occur due to various reasons, in order to deep-dive further and investigate in to the service level logs we would request you to open a support ticket with the aws premium support with the following information. -- account id -- region -- failed job arns and cloudwatch logs note :due to security reason, this post is not suitable for sharing customer's resource and account details. thank you.",
        "Answer_original_content_gpt_summary":"Possible solutions to the internal server error encountered while launching a Ground Truth Labeling job with a vendor for 3D point cloud object tracking include opening a support ticket with AWS premium support and providing the account ID, region, failed job ARNs, and CloudWatch logs for further investigation.",
        "Answer_preprocessed_content":"i am afraid to inform you that the internal server error can occur due to various reasons, in order to further and investigate in to the service level logs we would request you to open a support ticket with the aws premium support with the following information. account id region failed job arns and cloudwatch logs note due to security reason, this post is not suitable for sharing customer's resource and account details. thank you."
    },
    {
        "Question_id":60554471.0,
        "Question_title":"ML for object search",
        "Question_body":"<p>I'm trying to find a way to build ML using AWS, preferably using their services such as SageMaker and not just EC2, for object detection in images using an image as input.<\/p>\n\n<p>AWS Rekognition offers Image Comparison and Object detection APIs, but they are not exactly what I'm looking for, the comparison works only with faces and not objects and object detection is too basic.<\/p>\n\n<p>AlibabCloud has that functionality as a service (<a href=\"https:\/\/www.alibabacloud.com\/product\/imagesearch\" rel=\"nofollow noreferrer\">https:\/\/www.alibabacloud.com\/product\/imagesearch<\/a>) but I would like to use something similar on AWS, rather than Alibaba.<\/p>\n\n<p>How would I go about and build something like this?<\/p>\n\n<p>Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1583445060257,
        "Question_favorite_count":1.0,
        "Question_score":2.0,
        "Question_view_count":86.0,
        "Owner_creation_time":1365591820927,
        "Owner_last_access_time":1616423319590,
        "Owner_reputation":549.0,
        "Owner_up_votes":59.0,
        "Owner_down_votes":2.0,
        "Owner_views":89.0,
        "Answer_body":"<p><em>edited 03\/08\/2020 to add pointers for visual search<\/em><\/p>\n\n<p>Since you seem interested both in the tasks of <strong>object detection<\/strong> (input an image, and return bounding boxes with object classes) and <strong>visual search<\/strong> (input an image and return relevant images) let me give you pointers for both :)<\/p>\n\n<p>For <strong>object detection<\/strong> you have 3 options:<\/p>\n\n<ul>\n<li><strong>Using the managed service <a href=\"https:\/\/aws.amazon.com\/rekognition\/custom-labels-features\/?nc1=h_ls\" rel=\"nofollow noreferrer\">Amazon Rekognition Custom Labels<\/a><\/strong>. The key benefits of this service is that (1) it doesn't require writing ML code, as the service runs autoML internally to find the best model, (2) it is very flexible in terms of interaction (SDKs, console), data loading and annotation and (3) it can work even with small datasets (typically a few hundred images or less).<\/li>\n<li><strong>Using SageMaker Object Detection model<\/strong> (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection.html\" rel=\"nofollow noreferrer\">documentation<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_birds\/object_detection_birds.ipynb\" rel=\"nofollow noreferrer\">demo<\/a>). In this option, the model is also already written (SSD architecture with Resnet or VGG backbone) and you just need to choose  or tune <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/object-detection-api-config.html\" rel=\"nofollow noreferrer\">hyperparameters<\/a><\/li>\n<li><strong>Using your own model on Amazon SageMaker<\/strong>. This could be your own code in docker, or code from an ML framework in a SageMaker ML Framework container. There are such containers for <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html\" rel=\"nofollow noreferrer\">Pytorch<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html\" rel=\"nofollow noreferrer\">Tensorflow<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_mxnet.html\" rel=\"nofollow noreferrer\">MXNet<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_chainer.html\" rel=\"nofollow noreferrer\">Chainer<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">Sklearn<\/a>. In terms of model code, I recommend considering <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\"><strong><code>gluoncv<\/code><\/strong><\/a>, a compact python computer vision toolkit (based on mxnet backend) that comes with <a href=\"https:\/\/gluon-cv.mxnet.io\/model_zoo\/detection.html\" rel=\"nofollow noreferrer\">many state-of-the-art models<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/build\/examples_detection\/index.html\" rel=\"nofollow noreferrer\">tutorials<\/a> for object detection<\/li>\n<\/ul>\n\n<p>The task of <strong>visual search<\/strong> requires more customization, since you need to provide the info of (1) what you define as search relevancy (eg is it visual similarity? or object complementarity? etc) and (2) the collection among which to search. If all you need is visual similarity, a popular option is to transform images into vectors with a pre-trained neural network and run kNN search between the query image and the collection of transformed images. There are 2 tutos showing how to build such systems on AWS here:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/visual-search-on-aws-part-1-engine-implementation-with-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Blog Post Visual Search on AWS<\/a> (MXNet resnet embeddings +\nSageMaker kNN)<\/li>\n<li><a href=\"https:\/\/thomasdelteil.github.io\/VisualSearch_MXNet\/\" rel=\"nofollow noreferrer\">Visual Search on MMS demo<\/a> (MXNet resnet\nembeddings + HNSW kNN on AWS Fargate)<\/li>\n<\/ul>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1583500304256,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":1586898520163,
        "Answer_last_edit_time":1583703139203,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60554471",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: ml for object search; Content: i'm trying to find a way to build ml using aws, preferably using their services such as and not just ec2, for object detection in images using an image as input. aws rekognition offers image comparison and object detection apis, but they are not exactly what i'm looking for, the comparison works only with faces and not objects and object detection is too basic. alibabcloud has that functionality as a service (https:\/\/www.alibabacloud.com\/product\/imagesearch) but i would like to use something similar on aws, rather than alibaba. how would i go about and build something like this? thank you.",
        "Question_original_content_gpt_summary":"The user is trying to find a way to build a machine learning model using AWS services for object detection in images using an image as input, but is having difficulty finding the right services to do so.",
        "Question_preprocessed_content":"Title: ml for object search; Content: i'm trying to find a way to build ml using aws, preferably using their services such as and not just ec , for object detection in images using an image as input. aws rekognition offers image comparison and object detection apis, but they are not exactly what i'm looking for, the comparison works only with faces and not objects and object detection is too basic. alibabcloud has that functionality as a service but i would like to use something similar on aws, rather than alibaba. how would i go about and build something like this? thank you.",
        "Answer_original_content":"edited 03\/08\/2020 to add pointers for visual search since you seem interested both in the tasks of object detection (input an image, and return bounding boxes with object classes) and visual search (input an image and return relevant images) let me give you pointers for both :) for object detection you have 3 options: using the managed service amazon rekognition custom labels. the key benefits of this service is that (1) it doesn't require writing ml code, as the service runs automl internally to find the best model, (2) it is very flexible in terms of interaction (sdks, console), data loading and annotation and (3) it can work even with small datasets (typically a few hundred images or less). using object detection model (documentation, demo). in this option, the model is also already written (ssd architecture with resnet or vgg backbone) and you just need to choose or tune hyperparameters using your own model on . this could be your own code in docker, or code from an ml framework in a ml framework container. there are such containers for pytorch, tensorflow, mxnet, chainer and sklearn. in terms of model code, i recommend considering gluoncv, a compact python computer vision toolkit (based on mxnet backend) that comes with many state-of-the-art models and tutorials for object detection the task of visual search requires more customization, since you need to provide the info of (1) what you define as search relevancy (eg is it visual similarity? or object complementarity? etc) and (2) the collection among which to search. if all you need is visual similarity, a popular option is to transform images into vectors with a pre-trained neural network and run knn search between the query image and the collection of transformed images. there are 2 tutos showing how to build such systems on aws here: blog post visual search on aws (mxnet resnet embeddings + knn) visual search on mms demo (mxnet resnet embeddings + hnsw knn on aws fargate)",
        "Answer_original_content_gpt_summary":"Possible solutions for building a machine learning model for object detection in images using AWS services are: using Amazon Rekognition Custom Labels, using an object detection model with pre-written code and tuning hyperparameters, and using GluonCV, a compact Python computer vision toolkit. For visual search, transforming images into vectors with a pre-trained neural network and running KNN search between the query image and the collection of transformed images is a popular option. AWS provides tutorials on how to build such systems using MXNet ResNet embeddings and KNN on AWS Fargate.",
        "Answer_preprocessed_content":"edited to add pointers for visual search since you seem interested both in the tasks of object detection and visual search let me give you pointers for both for object detection you have options using the managed service amazon rekognition custom labels. the key benefits of this service is that it doesn't require writing ml code, as the service runs automl internally to find the best model, it is very flexible in terms of interaction , data loading and annotation and it can work even with small datasets . using object detection model . in this option, the model is also already written and you just need to choose or tune hyperparameters using your own model on . this could be your own code in docker, or code from an ml framework in a ml framework container. there are such containers for pytorch, tensorflow, mxnet, chainer and sklearn. in terms of model code, i recommend considering , a compact python computer vision toolkit that comes with many models and tutorials for object detection the task of visual search requires more customization, since you need to provide the info of what you define as search relevancy and the collection among which to search. if all you need is visual similarity, a popular option is to transform images into vectors with a neural network and run knn search between the query image and the collection of transformed images. there are tutos showing how to build such systems on aws here blog post visual search on aws visual search on mms demo"
    },
    {
        "Question_id":62171724.0,
        "Question_title":"How can I access the Workspace object from a training script in AzureML?",
        "Question_body":"<p>I want to access the Workspace object in my <code>train.py<\/code> script, when running in an Estimator.  <\/p>\n\n<p>I currently can access the Run object, using the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n<\/code><\/pre>\n\n<p>But I cannot seem to get my hands on the Workspace object in my training script.  I would use this mostly to get access to the Datastores and Datasets (as I would hope to keep all data set references inside the training script, instead of passing them as input datasets)<\/p>\n\n<p>Any idea if\/how this is possible ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1591183699477,
        "Question_favorite_count":1.0,
        "Question_score":4.0,
        "Question_view_count":1148.0,
        "Owner_creation_time":1360655430743,
        "Owner_last_access_time":1663784892907,
        "Owner_reputation":2947.0,
        "Owner_up_votes":297.0,
        "Owner_down_votes":16.0,
        "Owner_views":355.0,
        "Answer_body":"<p>Sure, try this:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\nws = run.experiment.workspace\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1591197955080,
        "Answer_score":12.0,
        "Owner_location":"Belgium",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62171724",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i access the workspace object from a training script in ?; Content: i want to access the workspace object in my train.py script, when running in an estimator. i currently can access the run object, using the following code: run = run.get_context() but i cannot seem to get my hands on the workspace object in my training script. i would use this mostly to get access to the datastores and datasets (as i would hope to keep all data set references inside the training script, instead of passing them as input datasets) any idea if\/how this is possible ?",
        "Question_original_content_gpt_summary":"The user is trying to access the workspace object from a training script in an estimator, but is unable to do so.",
        "Question_preprocessed_content":"Title: how can i access the workspace object from a training script in ?; Content: i want to access the workspace object in my script, when running in an estimator. i currently can access the run object, using the following code but i cannot seem to get my hands on the workspace object in my training script. i would use this mostly to get access to the datastores and datasets any idea this is possible ?",
        "Answer_original_content":"sure, try this: from .core.run import run run = run.get_context() ws = run.experiment.workspace",
        "Answer_original_content_gpt_summary":"The solution to accessing the workspace object from a training script in an estimator is to import the \"run\" module and use the \"get_context()\" method to get the workspace object.",
        "Answer_preprocessed_content":"sure, try this"
    },
    {
        "Question_id":null,
        "Question_title":"certification test for AI-900: Microsoft Azure AI Fundamentals not available",
        "Question_body":"Hallo, i would like make an appointment for Exam AI-900: Microsoft Azure AI Fundamentals.\nHowever this exam is currently not available at Pearson vue or Certiport. When can i expect this again? Is there an alternative ?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1662205880130,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/992629\/exam-ai-900-microsoft-azure-ai-fundamentals-not-av.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-03T13:15:33.98Z",
                "Answer_score":0,
                "Answer_body":"Hi Jurian,\n\nThis is available in PearsonVue check this. ai-900\n\nAny specific region you are trying from?\n\n\n\n\n\n==\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: certification test for ai-900: microsoft azure ai fundamentals not available; Content: hallo, i would like make an appointment for exam ai-900: microsoft azure ai fundamentals. however this exam is currently not available at pearson vue or certiport. when can i expect this again? is there an alternative ?",
        "Question_original_content_gpt_summary":"The user is looking to take the AI-900: Microsoft Azure AI Fundamentals certification test, but is unable to find it available at Pearson Vue or Certiport.",
        "Question_preprocessed_content":"Title: certification test for microsoft azure ai fundamentals not available; Content: hallo, i would like make an appointment for exam microsoft azure ai fundamentals. however this exam is currently not available at pearson vue or certiport. when can i expect this again? is there an alternative ?",
        "Answer_original_content":"hi jurian, this is available in pearsonvue check this. ai-900 any specific region you are trying from? == please \"accept the answer\" if the information helped you. this will help us and others in the community as well.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Check Pearson Vue again for the AI-900: Microsoft Azure AI Fundamentals certification test.\n- Specify the region where the user is trying to take the test. \n\nSummary: The answer suggests checking Pearson Vue again for the certification test and asking for the specific region where the user is trying to take the test.",
        "Answer_preprocessed_content":"hi jurian, this is available in pearsonvue check this. any specific region you are trying from? please accept the answer if the information helped you. this will help us and others in the community as well."
    },
    {
        "Question_id":53595157.0,
        "Question_title":"AWS Sagemaker Deploy fails",
        "Question_body":"<p>I am following the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-deploy-model.html\" rel=\"noreferrer\">sage maker documentation<\/a> to train and deploy an ML model. I am using the high-level Python library provided by Amazon SageMaker to achieve this. <\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>The deployment fails with error<\/p>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.c4.8xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. <\/p>\n\n<p>Where am I going wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1543844726537,
        "Question_favorite_count":1.0,
        "Question_score":7.0,
        "Question_view_count":5932.0,
        "Owner_creation_time":1373018880576,
        "Owner_last_access_time":1613887520940,
        "Owner_reputation":305.0,
        "Owner_up_votes":16.0,
        "Owner_down_votes":0.0,
        "Owner_views":12.0,
        "Answer_body":"<p>I resolved the issue by changing the instance type:<\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1543906305888,
        "Answer_score":7.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53595157",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deploy fails; Content: i am following the sage maker documentation to train and deploy an ml model. i am using the high-level python library provided by to achieve this. kmeans_predictor = kmeans.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge') the deployment fails with error resourcelimitexceeded: an error occurred (resourcelimitexceeded) when calling the createendpoint operation: the account-level service limit 'ml.c4.8xlarge for endpoint usage' is 0 instances, with current utilization of 0 instances and a request delta of 1 instances. where am i going wrong?",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to deploy an ML model using the Sage Maker documentation and the high-level Python library provided by AWS.",
        "Question_preprocessed_content":"Title: deploy fails; Content: i am following the sage maker documentation to train and deploy an ml model. i am using the python library provided by to achieve this. the deployment fails with error resourcelimitexceeded an error occurred when calling the createendpoint operation the service limit for endpoint usage' is instances, with current utilization of instances and a request delta of instances. where am i going wrong?",
        "Answer_original_content":"i resolved the issue by changing the instance type: kmeans_predictor = kmeans.deploy(initial_instance_count=1, instance_type='ml.t2.medium')",
        "Answer_original_content_gpt_summary":"The solution to the error encountered when deploying an ML model using Sage Maker documentation and AWS high-level Python library is to change the instance type to 'ml.t2.medium'.",
        "Answer_preprocessed_content":"i resolved the issue by changing the instance type"
    },
    {
        "Question_id":68823606.0,
        "Question_title":"Difference between tracking_uri and the backend store uri in MLFLOW",
        "Question_body":"<p>I am using Mlflow for my project hosting it in an EC2 instance. I was wondering in MlFlow what is the difference between the backend_store_uri we set when we launch the server and the trarcking_uri ?<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1629231900840,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":682.0,
        "Owner_creation_time":1585131052300,
        "Owner_last_access_time":1663921245110,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":"<p><code>tracking_uri<\/code> is the URL of the MLflow server (remote, or built-in in Databricks) that will be used to log metadata &amp; model (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/quickstart.html#launch-a-tracking-server-on-a-remote-machine\" rel=\"nofollow noreferrer\">doc<\/a>).  In your case, this will be the URL pointing to your EC2 instance that should be configured in programs that will log parameters into your server.<\/p>\n<p><code>backend_store_uri<\/code> - is used by MLflow server to configure where to store this data - on filesystem, in SQL-compatible database, etc. (see <a href=\"https:\/\/mlflow.org\/docs\/latest\/cli.html#cmdoption-mlflow-server-backend-store-uri\" rel=\"nofollow noreferrer\">doc<\/a>). If you use SQL database, then you also need to provide the <code>--default-artifact-root<\/code> option to point where to store generated artifacts (images, model files, etc.)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1629269923972,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68823606",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: difference between tracking_uri and the backend store uri in ; Content: i am using for my project hosting it in an ec2 instance. i was wondering in what is the difference between the backend_store_uri we set when we launch the server and the trarcking_uri ? thanks,",
        "Question_original_content_gpt_summary":"The user is wondering what the difference is between the backend_store_uri and the tracking_uri when launching a server using .",
        "Question_preprocessed_content":"Title: difference between and the backend store uri in ; Content: i am using for my project hosting it in an ec instance. i was wondering in what is the difference between the we set when we launch the server and the ? thanks,",
        "Answer_original_content":"tracking_uri is the url of the server (remote, or built-in in databricks) that will be used to log metadata & model (see doc). in your case, this will be the url pointing to your ec2 instance that should be configured in programs that will log parameters into your server. backend_store_uri - is used by server to configure where to store this data - on filesystem, in sql-compatible database, etc. (see doc). if you use sql database, then you also need to provide the --default-artifact-root option to point where to store generated artifacts (images, model files, etc.)",
        "Answer_original_content_gpt_summary":"The answer explains that the tracking_uri is the URL of the server used to log metadata and models, while the backend_store_uri is used to configure where to store this data, such as on a filesystem or in a SQL-compatible database. If a SQL database is used, the --default-artifact-root option must also be provided to specify where to store generated artifacts.",
        "Answer_preprocessed_content":"is the url of the server that will be used to log metadata & model . in your case, this will be the url pointing to your ec instance that should be configured in programs that will log parameters into your server. is used by server to configure where to store this data on filesystem, in database, etc. . if you use sql database, then you also need to provide the option to point where to store generated artifacts"
    },
    {
        "Question_id":64286191.0,
        "Question_title":"How to add keyboard shortcuts to AWS Ground Truth labeler UI?",
        "Question_body":"<p>I'm using AWS Sagemaker Ground Truth for a Custom Labeling Task that involves editing bounding boxes and their labels.  Ground Truth's UI has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in UI elements like &quot;No adjustments needed&quot; or the &quot;Submit&quot; button.<\/p>\n<p>Is there a way to add such shortcuts?  I've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts.  It doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1602271699933,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":412.0,
        "Owner_creation_time":1289772110723,
        "Owner_last_access_time":1663029415383,
        "Owner_reputation":309.0,
        "Owner_up_votes":14.0,
        "Owner_down_votes":0.0,
        "Owner_views":40.0,
        "Answer_body":"<p>Could try something like:<\/p>\n<pre><code>document.addEventListener('keydown', function(event) {\n  if (event.shiftKey &amp;&amp; event.keyCode === 13) {\n    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();\n  }\n});\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1605831050088,
        "Answer_score":3.0,
        "Owner_location":"Mt Kisco, NY",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64286191",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to add keyboard shortcuts to aws ground truth labeler ui?; Content: i'm using ground truth for a custom labeling task that involves editing bounding boxes and their labels. ground truth's ui has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in ui elements like \"no adjustments needed\" or the \"submit\" button. is there a way to add such shortcuts? i've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts. it doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.",
        "Question_original_content_gpt_summary":"The user is looking for a way to add keyboard shortcuts to the AWS Ground Truth Labeler UI for built-in UI elements such as \"No Adjustments Needed\" and the \"Submit\" button.",
        "Question_preprocessed_content":"Title: how to add keyboard shortcuts to aws ground truth labeler ui?; Content: i'm using ground truth for a custom labeling task that involves editing bounding boxes and their labels. ground truth's ui has keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other ui elements like no adjustments needed or the submit button. is there a way to add such shortcuts? i've looked at the for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts. it doesn't even look like or support specifying a shortcut as an attribute.",
        "Answer_original_content":"could try something like: document.addeventlistener('keydown', function(event) { if (event.shiftkey && event.keycode === 13) { document.getelementsbytagname('crowd-bounding-box')[0].shadowroot.getelementbyid('nothing-to-adjust').queryselector('label').click(); } });",
        "Answer_original_content_gpt_summary":"The answer suggests adding a keyboard shortcut to the AWS Ground Truth Labeler UI by using a JavaScript code that listens for a specific key combination and then triggers a click event on the desired UI element. Specifically, the code provided targets the \"No Adjustments Needed\" label using the tag name and ID of the relevant HTML elements.",
        "Answer_preprocessed_content":"could try something like"
    },
    {
        "Question_id":null,
        "Question_title":"Archive runs",
        "Question_body":"<p>Thanks for your good product.<\/p>\n<p>It would be good to add an archive feature for runs.<\/p>\n<p>In a project, we may try many ideas. But most of them result in no outcomes. It would be good to archive those runs to keep the workspace clean.<\/p>\n<p>It is not a good option to delete them, because we may check them in future for some cases, such as ablation study.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1675273585873,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":115.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/archive-runs\/3793",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-01T17:54:51.728Z",
                "Answer_body":"<p>Currently, i\u2019m using an \u2018archive\u2019 tag with filtering to do this. But it is not decent.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-01T22:46:17.181Z",
                "Answer_body":"<p>Hi Geo, thank you for the feature request! I can put in a ticket for this, but first can you tell me why putting an \u2018archive\u2019 tag isn\u2019t a good workaround for you?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-02T11:07:42.585Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lesliewandb\">@lesliewandb<\/a> , it\u2019s ok but not decent and efficient.<\/p>\n<p>I think we can still use tags to do this archive feature, but it would be good to add a multi-selection feature for runs and tag them all at once. It is very useful for users who deal with hundreds of experiments, such as hyperparameter searching.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-07T01:08:41.056Z",
                "Answer_body":"<p>I see, this makes sense. Thank you for the clarification here! I\u2019ll create the ticket and let you know when there\u2019s any update on it<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-17T21:51:42.465Z",
                "Answer_body":"<p>I believe currently wandb does support multiple selection. But not in the workspace view. In the table view I can select and tag multiple runs at once.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: archive runs; Content: thanks for your good product. it would be good to add an archive feature for runs. in a project, we may try many ideas. but most of them result in no outcomes. it would be good to archive those runs to keep the workspace clean. it is not a good option to delete them, because we may check them in future for some cases, such as ablation study.",
        "Question_original_content_gpt_summary":"The user is requesting an archive feature for runs in order to keep their workspace clean and to be able to access them in the future for ablation studies.",
        "Question_preprocessed_content":"Title: archive runs; Content: thanks for your good product. it would be good to add an archive feature for runs. in a project, we may try many ideas. but most of them result in no outcomes. it would be good to archive those runs to keep the workspace clean. it is not a good option to delete them, because we may check them in future for some cases, such as ablation study.",
        "Answer_original_content":"i believe currently does support multiple selection. but not in the workspace view. in the table view i can select and tag multiple runs at once.",
        "Answer_original_content_gpt_summary":"The answer suggests that the platform supports selecting and tagging multiple runs at once in the table view, but not in the workspace view. This could potentially be a solution for the user's request for an archive feature to keep their workspace clean and access runs in the future for ablation studies.",
        "Answer_preprocessed_content":"i believe currently does support multiple selection. but not in the workspace view. in the table view i can select and tag multiple runs at once."
    },
    {
        "Question_id":57212696.0,
        "Question_title":"Gluonnlp installation not found on Sagemaker jupyter notebook",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/I8c93.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I8c93.png\" alt=\"enter image description here\"><\/a>I am attempting to install Gluonnlp to a sagemaker jupyter notebook. Im using the command <code>!sudo pip3 install gluonnlp<\/code> to install.  Which is successful.  However on import I get <code>ModuleNotFoundError: No module named 'gluonnlp'<\/code><\/p>\n\n<p>I got the same issue when attempting to install mxnet with pip in the same notebook.  It was resolved when I conda installed mxnet instead.  However conda install has not been working for gluonnlp as it cannot find the package.  I can't seem to find a way to conda install gluonnlp.  Any suggestions would be highly appreciated.<\/p>\n\n<p>Here are some of the commands I have tried<\/p>\n\n<p><code>!sudo pip3 install gluonnlp<\/code><\/p>\n\n<p><code>!conda install gluonnlp<\/code> --> Anaconda cant find the package in any channels<\/p>\n\n<pre><code>!conda install pip --y\n!sudo pip3 install gluonnlp\n\n!sudo pip3 install gluonnlp\n\n!conda install -c conda-forge gluonnlp --y\n<\/code><\/pre>\n\n<p>All these commands on my import \nimport warnings<\/p>\n\n<pre><code>warnings.filterwarnings('ignore')\n\nimport io\nimport random\nimport numpy as np\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom bert import data, model\n<\/code><\/pre>\n\n<p>result in the error<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'gluonnlp'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6.0,
        "Question_creation_time":1564111030610,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":2015.0,
        "Owner_creation_time":1531840489147,
        "Owner_last_access_time":1636067734716,
        "Owner_reputation":425.0,
        "Owner_up_votes":12.0,
        "Owner_down_votes":1.0,
        "Owner_views":92.0,
        "Answer_body":"<p>this is as simple as creating a Jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing:<\/p>\n\n<pre><code>!pip install gluonnlp\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1564315813180,
        "Answer_score":0.0,
        "Owner_location":"Berkeley, CA, USA",
        "Question_last_edit_time":1564413127092,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57212696",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: gluonnlp installation not found on jupyter notebook; Content: i am attempting to install gluonnlp to a jupyter notebook. im using the command !sudo pip3 install gluonnlp to install. which is successful. however on import i get modulenotfounderror: no module named 'gluonnlp' i got the same issue when attempting to install mxnet with pip in the same notebook. it was resolved when i conda installed mxnet instead. however conda install has not been working for gluonnlp as it cannot find the package. i can't seem to find a way to conda install gluonnlp. any suggestions would be highly appreciated. here are some of the commands i have tried !sudo pip3 install gluonnlp !conda install gluonnlp --> anaconda cant find the package in any channels !conda install pip --y !sudo pip3 install gluonnlp !sudo pip3 install gluonnlp !conda install -c conda-forge gluonnlp --y all these commands on my import import warnings warnings.filterwarnings('ignore') import io import random import numpy as np import mxnet as mx import gluonnlp as nlp from bert import data, model result in the error modulenotfounderror: no module named 'gluonnlp'",
        "Question_original_content_gpt_summary":"The user is encountering challenges with installing and importing the gluonnlp package on a Jupyter notebook, despite trying various commands.",
        "Question_preprocessed_content":"Title: gluonnlp installation not found on jupyter notebook; Content: i am attempting to install gluonnlp to a jupyter notebook. im using the command to install. which is successful. however on import i get i got the same issue when attempting to install mxnet with pip in the same notebook. it was resolved when i conda installed mxnet instead. however conda install has not been working for gluonnlp as it cannot find the package. i can't seem to find a way to conda install gluonnlp. any suggestions would be highly appreciated. here are some of the commands i have tried anaconda cant find the package in any channels all these commands on my import import warnings result in the error",
        "Answer_original_content":"this is as simple as creating a jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing: !pip install gluonnlp",
        "Answer_original_content_gpt_summary":"The solution to the challenge of installing and importing the gluonnlp package on a Jupyter notebook is to create a notebook using the 'conda_mxnet_p36' kernel and add a cell containing the command \"!pip install gluonnlp\".",
        "Answer_preprocessed_content":"this is as simple as creating a jupyter notebook using the kernel, and adding a cell containing"
    },
    {
        "Question_id":null,
        "Question_title":"Error while creating pipeline between first and second page - first step runs get error when second steps start",
        "Question_body":"first step of pipeline\n\ndata_prep_step = PythonScriptStep(\nscript_name='data_prep.py',\nsource_directory='.\/src',\narguments=[\"--data_path\", dataset.as_mount(), \"--out_folder\", output_data],\ncompute_target='cpu-cluster',\nrunconfig=aml_run_config,\nallow_reuse=True\n)\n\nsecond step of pipeline\n\ntrain_step = PythonScriptStep(\nscript_name='train.py',\nsource_directory='.\/src',\narguments=[\"--output_folder\", output_data.as_input()],\ncompute_target='cpu-cluster',\nrunconfig=aml_run_config,\nallow_reuse=True\n)\n\nrun\n\ntrain_pipeline = Pipeline(workspace = ws, steps = [data_prep_step, train_step])\nexperiment = Experiment(workspace = ws, name = 'training-pipeline' )\npipeline_run = experiment.submit(train_pipeline)\n\ncode first step completes, I get error when second step starts\n\n\ncode for first step below\n\ndef split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\nfiles = []\nfor filename in os.listdir(SOURCE):\nfile = os.path.join(SOURCE, filename)\nif os.path.getsize(file) > 0:\nfiles.append(filename)\nelse:\nprint(filename + \" is zero length, so ignoring.\")\n\n training_length = int(len(files) * SPLIT_SIZE)\n testing_length = int(len(files) - training_length)\n shuffled_set = random.sample(files, len(files))\n training_set = shuffled_set[0:training_length]\n testing_set = shuffled_set[training_length:]\n for filename in training_set:\n     this_file = os.path.join(SOURCE, filename)\n     destination = os.path.join(TRAINING, filename)\n     copy(this_file, TRAINING)\n for filename in testing_set:\n     this_file = os.path.join(SOURCE, filename)\n     destination = os.path.join(TESTING, filename)\n     copy(this_file, TESTING)\n\n\n\nrun = Run.get_context()\nif name == \"main\":\n\n parser = argparse.ArgumentParser()\n parser.add_argument('--data_path',\n                     type=str,\n                     help='Path to uploaded data')\n parser.add_argument('--out_folder', \n                    type=str\n                        \n                    )\n #parser.add_argument('--data_path_test', \n #                    type=str,\n #                   help='Path to test dataflow')\n #args = parser.parse_args()\n args = parser.parse_args()\n output_folder = args.out_folder\n inputs = args.data_path\n    \n try:\n     os.makedirs(os.path.join(output_folder, '\/train\/defect'), exist_ok=True) #'args.data_path_folder\/train\/defect\/')\n     os.makedirs(os.path.join(output_folder, '\/train\/no-defect'), exist_ok=True) #'args.data_path_folder\/train\/no-defect\/')\n     os.makedirs(os.path.join(output_folder, '\/test\/defect'), exist_ok=True) #'args.data_path_folder\/test\/defect\/')\n     os.makedirs(os.path.join(output_folder, '\/test\/no-defect'), exist_ok=True) #'args.data_path_folder\/test\/no-defect\/')\n     #os.mkdir('\/tmp\/cats-v-dogs\/training\/dogs')\n     #os.mkdir('\/tmp\/cats-v-dogs\/testing\/cats')\n     #os.mkdir('\/tmp\/cats-v-dogs\/testing\/dogs')\n except OSError:\n     pass\n    \n train_datagen = ImageDataGenerator(\n rescale = 1.\/255)\n val_datagen = ImageDataGenerator(\n rescale = 1.\/255)\n test_datagen = ImageDataGenerator(\n rescale = 1.\/255)\n    \n class_mode = 'binary'\n batch_size = 5\n    \n    \n NO_DEFECT_SOURCE_DIR =  os.path.join(inputs, \"Good\")\n TRAINING_NO_DEFECT_DIR = os.path.join(output_folder, '\/train\/no-defect') #'output_folder\/train\/no-defect\/'   #os.path.join(args.data_path_train, \"no-defect\/\")\n TESTING_NO_DEFECT_DIR =  os.path.join(output_folder, '\/test\/no-defect') #'output_folder\/test\/no-defect\/'    #os.path.join(args.data_path_test, \"no-defect\/\")\n DEFECT_SOURCE_DIR = os.path.join(inputs, \"Defective\")\n TRAINING_DEFECT_DIR = os.path.join(output_folder, '\/train\/defect') #'output_folder\/train\/defect\/' #os.path.join(args.data_path_train, \"defect\/\")\n TESTING_DEFECT_DIR = os.path.join(output_folder, '\/test\/defect') #'output_folder\/test\/defect\/' #os.path.join(args.data_path_test, \"defect\/\")\n    \n split_size = .8\n split_data(NO_DEFECT_SOURCE_DIR, TRAINING_NO_DEFECT_DIR, TESTING_NO_DEFECT_DIR, split_size)\n split_data(DEFECT_SOURCE_DIR, TRAINING_DEFECT_DIR, TESTING_DEFECT_DIR, split_size)\n\n\n\nerror received below\n\n\n\n{'code': data-capability.DatasetMountSession:input_915071c1.ExecutionError, 'message':\nError Code: ScriptExecution.StreamAccess.NotFound\n, 'target': , 'category': UserError, 'error_details': [{'key': NonCompliantReason, 'value': Error Code: ScriptExecution.StreamAccess.NotFound Failed Step: 92a8bfed-63f0-497a-bbcf-b0bfa1be2d9a Error Message: ScriptExecutionException was caused by StreamAccessException. StreamAccessException was caused by NotFoundException. Found no resources for the input provided: 'https:\/\/mich7068071609.blob.core.windows.net\/azureml-blobstore-e23f8d3d-4bfa-4d73-8330-db867b66a523\/dataset\/3c085033-67b7-4c25-8e29-1e58a993e90a\/prepped\/' | session_id=067cfe37-82a4-46e1-900c-8184e503ebfb}, {'key': StackTrace, 'value': File \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/data_capability\/capability_session.py\", line 47, in start (data_path, sub_data_path) = session.start() File \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/data_capability\/data_sessions.py\", line 171, in start if self._is_single_file: File \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/data_capability\/data_sessions.py\", line 119, in _is_single_file path = dataflow._to_pyrecords()[0][temp_column]\n\nFile \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/azureml\/dataprep\/api\/dataflow.py\", line 756, in _to_pyrecords\nintermediate_files = _write_preppy_with_fallback('Dataflow.to_pyrecords', self, span_context=to_dprep_span_context(span.get_context()))\n\nFile \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/azureml\/dataprep\/api\/_dataframereader.py\", line 190, in _write_preppy_with_fallback\n_execute_with_fallback(activity, dataflow_to_execute, force_clex=force_clex, span_context=span_context)\n\nFile \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/azureml\/dataprep\/api\/_dataframereader.py\", line 238, in _execute_with_fallback\nclex_execute()\n\nFile \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/azureml\/dataprep\/api\/_dataframereader.py\", line 219, in clex_execute\nspan_context=span_context\n\nFile \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/azureml\/dataprep\/api\/_aml_helper.py\", line 38, in wrapper\nreturn send_message_func(op_code, message, cancellation_token)\n\nFile \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py\", line 154, in execute_anonymous_activity\nresponse = self._message_channel.send_message('Engine.ExecuteActivity', message_args, cancellation_token)\n\nFile \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py\", line 291, in send_message\nraise_engine_error(response['error'])\n\nFile \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/azureml\/dataprep\/api\/errorhandlers.py\", line 10, in raise_engine_error\nraise ExecutionError(error_response)\n}, ], 'inner_e",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1647462322017,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/775279\/error-while-creating-pipeline-between-first-and-se.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-04T09:52:01.617Z",
                "Answer_score":0,
                "Answer_body":"Same error and I can confirm --data_path\", dataset.as_download() worked.\n\nToo difficult to get solution for pipeline debugging, any troubleshooting guidance here?\n\nShould I just post error I got to debug? No efficiency to do so.\n\nThank you anyway.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: error while creating pipeline between first and second page - first step runs get error when second steps start; Content: first step of pipeline data_prep_step = pythonscriptstep( script_name='data_prep.py', source_directory='.\/src', arguments=[\"--data_path\", dataset.as_mount(), \"--out_folder\", output_data], compute_target='cpu-cluster', runconfig=aml_run_config, allow_reuse=true ) second step of pipeline train_step = pythonscriptstep( script_name='train.py', source_directory='.\/src', arguments=[\"--output_folder\", output_data.as_input()], compute_target='cpu-cluster', runconfig=aml_run_config, allow_reuse=true ) run train_pipeline = pipeline(workspace = ws, steps = [data_prep_step, train_step]) experiment = experiment(workspace = ws, name = 'training-pipeline' ) pipeline_run = experiment.submit(train_pipeline) code first step completes, i get error when second step starts code for first step below def split_data(source, training, testing, split_size): files = [] for filename in os.listdir(source): file = os.path.join(source, filename) if os.path.getsize(file) > 0: files.append(filename) else: print(filename + \" is zero length, so ignoring.\") training_length = int(len(files) * split_size) testing_length = int(len(files) - training_length) shuffled_set = random.sample(files, len(files)) training_set = shuffled_set[0:training_length] testing_set = shuffled_set[training_length:] for filename in training_set: this_file = os.path.join(source, filename) destination = os.path.join(training, filename) copy(this_file, training) for filename in testing_set: this_file = os.path.join(source, filename) destination = os.path.join(testing, filename) copy(this_file, testing) run = run.get_context() if name == \"main\": parser = argparse.argumentparser() parser.add_argument('--data_path', type=str, help='path to uploaded data') parser.add_argument('--out_folder', type=str ) #parser.add_argument('--data_path_test', # type=str, # help='path to test dataflow') #args = parser.parse_args() args = parser.parse_args() output_folder = args.out_folder inputs = args.data_path try: os.makedirs(os.path.join(output_folder, '\/train\/defect'), exist_ok=true) #'args.data_path_folder\/train\/defect\/') os.makedirs(os.path.join(output_folder, '\/train\/no-defect'), exist_ok=true) #'args.data_path_folder\/train\/no-defect\/') os.makedirs(os.path.join(output_folder, '\/test\/defect'), exist_ok=true) #'args.data_path_folder\/test\/defect\/') os.makedirs(os.path.join(output_folder, '\/test\/no-defect'), exist_ok=true) #'args.data_path_folder\/test\/no-defect\/') #os.mkdir('\/tmp\/cats-v-dogs\/training\/dogs') #os.mkdir('\/tmp\/cats-v-dogs\/testing\/cats') #os.mkdir('\/tmp\/cats-v-dogs\/testing\/dogs') except oserror: pass train_datagen = imagedatagenerator( rescale = 1.\/255) val_datagen = imagedatagenerator( rescale = 1.\/255) test_datagen = imagedatagenerator( rescale = 1.\/255) class_mode = 'binary' batch_size = 5 no_defect_source_dir = os.path.join(inputs, \"good\") training_no_defect_dir = os.path.join(output_folder, '\/train\/no-defect') #'output_folder\/train\/no-defect\/' #os.path.join(args.data_path_train, \"no-defect\/\") testing_no_defect_dir = os.path.join(output_folder, '\/test\/no-defect') #'output_folder\/test\/no-defect\/' #os.path.join(args.data_path_test, \"no-defect\/\") defect_source_dir = os.path.join(inputs, \"defective\") training_defect_dir = os.path.join(output_folder, '\/train\/defect') #'output_folder\/train\/defect\/' #os.path.join(args.data_path_train, \"defect\/\") testing_defect_dir = os.path.join(output_folder, '\/test\/defect') #'output_folder\/test\/defect\/' #os.path.join(args.data_path_test, \"defect\/\") split_size = .8 split_data(no_defect_source_dir, training_no_defect_dir, testing_no_defect_dir, split_size) split_data(defect_source_dir, training_defect_dir, testing_defect_dir, split_size) error received below {'code': data-capability.datasetmountsession:input_915071c1.executionerror, 'message': error code: scriptexecution.streamaccess.notfound , 'target': , 'category': usererror, 'error_details': [{'key': noncompliantreason, 'value': error code: scriptexecution.streamaccess.notfound failed step: 92a8bfed-63f0-497a-bbcf-b0bfa1be2d9a error message: scriptexecutionexception was caused by streamaccessexception. streamaccessexception was caused by notfoundexception. found no resources for the input provided: 'https:\/\/mich7068071609.blob.core.windows.net\/-blobstore-e23f8d3d-4bfa-4d73-8330-db867b66a523\/dataset\/3c085033-67b7-4c25-8e29-1e58a993e90a\/prepped\/' | session_id=067cfe37-82a4-46e1-900c-8184e503ebfb}, {'key': stacktrace, 'value': file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/data_capability\/capability_session.py\", line 47, in start (data_path, sub_data_path) = session.start() file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/data_capability\/data_sessions.py\", line 171, in start if self._is_single_file: file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/data_capability\/data_sessions.py\", line 119, in _is_single_file path = dataflow._to_pyrecords()[0][temp_column] file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/\/dataprep\/api\/dataflow.py\", line 756, in _to_pyrecords intermediate_files = _write_preppy_with_fallback('dataflow.to_pyrecords', self, span_context=to_dprep_span_context(span.get_context())) file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/\/dataprep\/api\/_dataframereader.py\", line 190, in _write_preppy_with_fallback _execute_with_fallback(activity, dataflow_to_execute, force_clex=force_clex, span_context=span_context) file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/\/dataprep\/api\/_dataframereader.py\", line 238, in _execute_with_fallback clex_execute() file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/\/dataprep\/api\/_dataframereader.py\", line 219, in clex_execute span_context=span_context file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/\/dataprep\/api\/_aml_helper.py\", line 38, in wrapper return send_message_func(op_code, message, cancellation_token) file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/\/dataprep\/api\/engineapi\/api.py\", line 154, in execute_anonymous_activity response = self._message_channel.send_message('engine.executeactivity', message_args, cancellation_token) file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/\/dataprep\/api\/engineapi\/engine.py\", line 291, in send_message raise_engine_error(response['error']) file \"\/opt\/miniconda\/envs\/data-capability\/lib\/python3.7\/site-packages\/\/dataprep\/api\/errorhandlers.py\", line 10, in raise_engine_error raise executionerror(error_response) }, ], 'inner_e",
        "Question_original_content_gpt_summary":"The user encountered challenges while creating a pipeline between the first and second page, resulting in an error when the second step started.",
        "Question_preprocessed_content":"Title: error while creating pipeline between first and second page first step runs get error when second steps start; Content: first step of pipeline pythonscriptstep , second step of pipeline pythonscriptstep , run pipeline experiment experiment code first step completes, i get error when second step starts code for first step below def training, testing, files for filename in file filename if else print int int len for filename in filename destination filename training for filename in filename destination filename testing run if name main parser type str, help 'path to uploaded data' type str type str, help 'path to test dataflow' args args inputs try except oserror pass imagedatagenerator imagedatagenerator imagedatagenerator 'binary' good defective . error received below 'code' 'message' error code , 'target' , 'category' usererror, file line , in self, file line , in file line , in file line , in file line , in wrapper return message, file line , in response file line , in file line , in raise , ,",
        "Answer_original_content":"same error and i can confirm --data_path\", dataset.as_download() worked. too difficult to get solution for pipeline debugging, any troubleshooting guidance here? should i just post error i got to debug? no efficiency to do so. thank you anyway.",
        "Answer_original_content_gpt_summary":"Possible solutions mentioned in the answer are:\n- Using \"--data_path\" and \"dataset.as_download()\" to resolve the error.\n- Posting the error message for troubleshooting guidance.",
        "Answer_preprocessed_content":"same error and i can confirm worked. too difficult to get solution for pipeline debugging, any troubleshooting guidance here? should i just post error i got to debug? no efficiency to do so. thank you anyway."
    },
    {
        "Question_id":30133814.0,
        "Question_title":"How to build an image classification dataset in Azure?",
        "Question_body":"<p>I've a set of images that have a single classification of OPEN (they show something that is open).  I couldn't find a way to directly add a status of open to the image reader dataset so I have FULL OUTER JOIN-ed a single ENTER DATA to an IMAGE READER as per the following.  This seems like a hack, does anyone know the \"right\" way to do this?\n<img src=\"https:\/\/i.stack.imgur.com\/Kt1Rv.png\" alt=\"enter image description here\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1431124435077,
        "Question_favorite_count":1.0,
        "Question_score":16.0,
        "Question_view_count":1403.0,
        "Owner_creation_time":1322863592968,
        "Owner_last_access_time":1664017479012,
        "Owner_reputation":373.0,
        "Owner_up_votes":119.0,
        "Owner_down_votes":4.0,
        "Owner_views":34.0,
        "Answer_body":"<p>Another way is to have R or python code that replicates the status for each image and then use add-columns. I think R\/Python code to just replicate the status for each image may be easier and faster than outer join.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1431886381620,
        "Answer_score":5.0,
        "Owner_location":null,
        "Question_last_edit_time":1446192398648,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30133814",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to build an image classification dataset in azure?; Content: i've a set of images that have a single classification of open (they show something that is open). i couldn't find a way to directly add a status of open to the image reader dataset so i have full outer join-ed a single enter data to an image reader as per the following. this seems like a hack, does anyone know the \"right\" way to do this?",
        "Question_original_content_gpt_summary":"The user is attempting to build an image classification dataset in Azure and is looking for the \"right\" way to do so, as their current solution appears to be a hack.",
        "Question_preprocessed_content":"Title: how to build an image classification dataset in azure?; Content: i've a set of images that have a single classification of open . i couldn't find a way to directly add a status of open to the image reader dataset so i have full outer a single enter data to an image reader as per the following. this seems like a hack, does anyone know the right way to do this?",
        "Answer_original_content":"another way is to have r or python code that replicates the status for each image and then use add-columns. i think r\/python code to just replicate the status for each image may be easier and faster than outer join.",
        "Answer_original_content_gpt_summary":"Possible solution: The user can use R or Python code to replicate the status for each image and then use add-columns instead of using an outer join. This approach may be easier and faster.",
        "Answer_preprocessed_content":"another way is to have r or python code that replicates the status for each image and then use i think code to just replicate the status for each image may be easier and faster than outer join."
    },
    {
        "Question_id":62734059.0,
        "Question_title":"How to update pandas version in SageMaker notebook terminal?",
        "Question_body":"<p>I'm using SageMaker JupyterLab, but I found pandas is out of date, what's the process of updating it?<\/p>\n<p>I tried this:\nIn terminal:<\/p>\n<pre><code>cd SageMaker\nconda update pandas\n<\/code><\/pre>\n<p>The package has been updated to 1.0.5\nbut when I use this command in SageMaker instance:<\/p>\n<pre><code>import pandas\nprint(pandas,__version__)\n\nreturn:\n0.24.2\n<\/code><\/pre>\n<p>It didn't work at all, can someone help me? Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1593894159730,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1158.0,
        "Owner_creation_time":1540920956270,
        "Owner_last_access_time":1663875036883,
        "Owner_reputation":2385.0,
        "Owner_up_votes":1007.0,
        "Owner_down_votes":16.0,
        "Owner_views":585.0,
        "Answer_body":"<p>If you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1593895412640,
        "Answer_score":2.0,
        "Owner_location":"United Kingdom",
        "Question_last_edit_time":1593895091990,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62734059",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to update pandas version in notebook terminal?; Content: i'm using jupyterlab, but i found pandas is out of date, what's the process of updating it? i tried this: in terminal: cd conda update pandas the package has been updated to 1.0.5 but when i use this command in instance: import pandas print(pandas,__version__) return: 0.24.2 it didn't work at all, can someone help me? thanks.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in updating their pandas version in their JupyterLab notebook terminal.",
        "Question_preprocessed_content":"Title: how to update pandas version in notebook terminal?; Content: i'm using jupyterlab, but i found pandas is out of date, what's the process of updating it? i tried this in terminal the package has been updated to but when i use this command in instance it didn't work at all, can someone help me? thanks.",
        "Answer_original_content":"if you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using lifecycle configuration.",
        "Answer_original_content_gpt_summary":"The solution to updating pandas version in JupyterLab notebook terminal is to use lifecycle configuration at launch to perform any upgrades or modifications to the kernel of the notebook.",
        "Answer_preprocessed_content":"if you want to perform any kind of upgrades or modification to the kernel of the notebook you can do this at launch by using lifecycle configuration."
    },
    {
        "Question_id":null,
        "Question_title":"How to import CSV file as a dataset for Azure machine learning",
        "Question_body":"I need to import CSV files as a dataset for my Azure machine learning experiment but I will get an error in execution, kindly provide me with the correct steps.\nThe aim of the experiment is to generate a demand forecast in MS D365 F&O based on the historical data provided in the CSV files.",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1615376822993,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/307727\/how-to-import-csv-file-as-a-dataset-for-azure-mach.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-11T07:27:22.347Z",
                "Answer_score":1,
                "Answer_body":"Hi @AbdelrahmanMorsy-9613\nThank you for posting in Q & A.\n\nAzure ML Studio Classic import data\nImport your training data into Azure Machine Learning Studio (classic) from various data sources\n\nAzure ML Designer import data\nImport data into Azure Machine Learning designer\n\n\n\n\nPlease don\u2019t forget to Accept the answer and up-vote wherever the information provided helps you, this can be beneficial to other community members.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-03-11T17:58:47.43Z",
                "Answer_score":0,
                "Answer_body":"The Azure AI gallery is a great resource to view sample experiments. Here's a forecasting model for Dynamics 365 example. Regarding the error, please follow-up on this thread. Thanks!",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to import csv file as a dataset for ; Content: i need to import csv files as a dataset for my experiment but i will get an error in execution, kindly provide me with the correct steps. the aim of the experiment is to generate a demand forecast in ms d365 f&o based on the historical data provided in the csv files.",
        "Question_original_content_gpt_summary":"The user needs to import CSV files as a dataset for an experiment to generate a demand forecast in MS D365 F&O based on the historical data provided in the CSV files, but is encountering an error in execution.",
        "Question_preprocessed_content":"Title: how to import csv file as a dataset for ; Content: i need to import csv files as a dataset for my experiment but i will get an error in execution, kindly provide me with the correct steps. the aim of the experiment is to generate a demand forecast in ms d f&o based on the historical data provided in the csv files.",
        "Answer_original_content":"hi @abdelrahmanmorsy-9613 thank you for posting in q & a. studio classic import data import your training data into studio (classic) from various data sources designer import data import data into designer please dont forget to accept the answer and up-vote wherever the information provided helps you, this can be beneficial to other community members.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. Import the CSV files as training data into Studio (classic) from various data sources.\n2. Import the data into Designer.\n\nThe answer does not provide any specific solution to the error encountered during execution.",
        "Answer_preprocessed_content":"hi thank you for posting in q & a. studio classic import data import your training data into studio from various data sources designer import data import data into designer please dont forget to accept the answer and wherever the information provided helps you, this can be beneficial to other community members."
    },
    {
        "Question_id":null,
        "Question_title":"Run Time is inaccurate because of including upload time",
        "Question_body":"<p>Some runs will spend minutes because of my terrible network:<\/p>\n<pre><code class=\"lang-shell\">wandb: Waiting for W&amp;B process to finish... (success).\n<\/code><\/pre>\n<p>I find the Run Time column in UI will also contain the uploading time (by comparing with other runs\u2019 Run Time).<\/p>\n<p>My script is organized as follow:<\/p>\n<pre><code class=\"lang-python\">def main(config):\n    ...\n    wandb.init(wandb_config)\n    ...\n\nif __name__ == '__main__':\n    config = blabla\n    for p in [p1, p2, p3]:  # for loop to tune hyperparameters\n        config.param = p\n        main(config)\n<\/code><\/pre>\n<p>To fix this issue, should I use <code>wandb.finish<\/code> in the end of the <code>main()<\/code> function? As the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/finish\">doc<\/a> of <code>wandb.finish<\/code> lists:<\/p>\n<blockquote>\n<p>Marks a run as finished, and finishes uploading all data.<\/p>\n<\/blockquote>\n<p>I worry about whether this func will kill my slow data uploading worker.<\/p>\n<p>Or any other solutions?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":null,
        "Question_creation_time":1670249098960,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":83.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/run-time-is-inaccurate-because-of-including-upload-time\/3499",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-06T10:04:08.165Z",
                "Answer_body":"<p>I find the Run Time is also inaccurate when sync the offline-run instance. After sync, the Run-Time is about 1 min (seems to be the time spent on the sync command).<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-08T14:34:09.006Z",
                "Answer_body":"<p>Hi Yago!<\/p>\n<p>Looking into this.<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-14T22:12:44.048Z",
                "Answer_body":"<p>Hi Yao!<\/p>\n<p>Sorry for the late response. I have consulted with a few others on my team and this seems to be a bug, considering the fact that you still get the wrong time using the offline mode.<\/p>\n<p>Would you send us some info on how to reproduce this so we can fix it asap?<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-16T10:05:55.002Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"artsiom\" data-post=\"4\" data-topic=\"3499\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sea2.discourse-cdn.com\/business7\/user_avatar\/community.wandb.ai\/artsiom\/40\/1001_2.png\" class=\"avatar\"> artsiom:<\/div>\n<blockquote>\n<p>reproduce<\/p>\n<\/blockquote>\n<\/aside>\n<p>I think reproduce this issue is easy.<\/p>\n<ol>\n<li>Run wandb in offline mode.<\/li>\n<li>After the run is finish, use <code>wandb sync run_path<\/code> command to upload the run.<\/li>\n<li>The RunTime colum is several seconds (time the <code>sync<\/code> command spent) in the UI of uploaded run.<\/li>\n<\/ol>\n<p>My test code is like:<\/p>\n<pre><code class=\"lang-python\">os.environ['WANDB_MODE'] = 'offline'\nrun = wandb.init(...)\n...\nrun.finish()\n<\/code><\/pre>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-21T20:48:23.323Z",
                "Answer_body":"<p>Hi Yao!<\/p>\n<p>Thank you for the directions. I was able to reproduce it and will send it over to the engineers!<br>\nThe only real workaround for this is recording the run-times yourself and then logging them to wandb.<\/p>\n<p>Artsiom<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-27T21:36:19.734Z",
                "Answer_body":"<p>Hi Yao,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-03T15:30:48.984Z",
                "Answer_body":"<p>Hi Yao, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-14T10:06:10.411Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: run time is inaccurate because of including upload time; Content: some runs will spend minutes because of my terrible network: : waiting for w&b process to finish... (success). i find the run time column in ui will also contain the uploading time (by comparing with other runs run time). my script is organized as follow: def main(config): ... .init(_config) ... if __name__ == '__main__': config = blabla for p in [p1, p2, p3]: # for loop to tune hyperparameters config.param = p main(config) to fix this issue, should i use .finish in the end of the main() function? as the doc of .finish lists: marks a run as finished, and finishes uploading all data. i worry about whether this func will kill my slow data uploading worker. or any other solutions?",
        "Question_original_content_gpt_summary":"The user is encountering an issue with run time being inaccurately reported due to the time spent uploading data, and is seeking a solution to fix this issue.",
        "Question_preprocessed_content":"Title: run time is inaccurate because of including upload time; Content: some runs will spend minutes because of my terrible network i find the run time column in ui will also contain the uploading time . my script is organized as follow to fix this issue, should i use in the end of the function? as the doc of lists marks a run as finished, and finishes uploading all data. i worry about whether this func will kill my slow data uploading worker. or any other solutions?",
        "Answer_original_content":"i find the run time is also inaccurate when sync the offline-run instance. after sync, the run-time is about 1 min (seems to be the time spent on the sync command). hi yago! looking into this. cheers! artsiom hi yao! sorry for the late response. i have consulted with a few others on my team and this seems to be a bug, considering the fact that you still get the wrong time using the offline mode. would you send us some info on how to reproduce this so we can fix it asap? cheers! artsiom artsiom: reproduce i think reproduce this issue is easy. run in offline mode. after the run is finish, use sync run_path command to upload the run. the runtime colum is several seconds (time the sync command spent) in the ui of uploaded run. my test code is like: os.environ['_mode'] = 'offline' run = .init(...) ... run.finish() hi yao! thank you for the directions. i was able to reproduce it and will send it over to the engineers! the only real workaround for this is recording the run-times yourself and then logging them to . artsiom hi yao, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi yao, since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The user is experiencing inaccuracies in reported run time due to time spent uploading data. The answer suggests that this is a known bug and the only workaround is to record the run times manually and log them. The user is asked to provide information on how to reproduce the issue so that it can be fixed. The conversation ends with a request for the user to confirm if the issue has been resolved or if further assistance is needed.",
        "Answer_preprocessed_content":"i find the run time is also inaccurate when sync the instance. after sync, the is about min . hi yago! looking into this. cheers! artsiom hi yao! sorry for the late response. i have consulted with a few others on my team and this seems to be a bug, considering the fact that you still get the wrong time using the offline mode. would you send us some info on how to reproduce this so we can fix it asap? cheers! artsiom artsiom reproduce i think reproduce this issue is easy. run in offline mode. after the run is finish, use command to upload the run. the runtime colum is several seconds in the ui of uploaded run. my test code is like hi yao! thank you for the directions. i was able to reproduce it and will send it over to the engineers! the only real workaround for this is recording the yourself and then logging them to . artsiom hi yao, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi yao, since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Formato di serializzazione eventi",
        "Question_body":"Buongiorno, nel output di un processo di analisi di flusso non posso cambiare formato da JSON a CVS come spiegato dal tutorial Microsoft \"Previsioni meteo usando i dati del sensore dall'hub IoT in Machine Learning Studio (versione classica)\".\nQualcuno ha qualche idea di come risolvere?\nGrazie.",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1649832685723,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/810747\/formato-di-serializzazione-eventi.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-13T12:09:55.54Z",
                "Answer_score":0,
                "Answer_body":"Hello, in the output of a stream analysis process I cannot change format from JSON to CVS as explained by the Microsoft tutorial \"Weather forecast using sensor data from IoT hub in Machine Learning Studio (classic version)\".\nAnyone have any idea how to fix?\nThanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: formato di serializzazione eventi; Content: buongiorno, nel output di un processo di analisi di flusso non posso cambiare formato da json a cvs come spiegato dal tutorial microsoft \"previsioni meteo usando i dati del sensore dall'hub iot in machine learning studio (versione classica)\". qualcuno ha qualche idea di come risolvere? grazie.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in changing the output format of a flow analysis process from JSON to CSV as explained in a Microsoft tutorial.",
        "Question_preprocessed_content":"Title: formato di serializzazione eventi; Content: buongiorno, nel output di un processo di analisi di flusso non posso cambiare formato da json a cvs come spiegato dal tutorial microsoft previsioni meteo usando i dati del sensore dall'hub iot in machine learning studio . qualcuno ha qualche idea di come risolvere? grazie.",
        "Answer_original_content":"hello, in the output of a stream analysis process i cannot change format from json to cvs as explained by the microsoft tutorial \"weather forecast using sensor data from iot hub in machine learning studio (classic version)\". anyone have any idea how to fix? thanks.",
        "Answer_original_content_gpt_summary":"There is no specific solution provided in the answer. The user is seeking help in changing the output format of a flow analysis process from JSON to CSV as explained in a Microsoft tutorial.",
        "Answer_preprocessed_content":"hello, in the output of a stream analysis process i cannot change format from json to cvs as explained by the microsoft tutorial weather forecast using sensor data from iot hub in machine learning studio . anyone have any idea how to fix? thanks."
    },
    {
        "Question_id":53872444.0,
        "Question_title":"Cannot read \".parquet\" files in Azure Jupyter Notebook (Python 2 and 3)",
        "Question_body":"<p>I am currently trying to open parquet files using Azure Jupyter Notebooks. I have tried both Python kernels (2 and 3).\nAfter the installation of <em>pyarrow<\/em> I can import the module only if the Python kernel is 2 (not working with Python 3)<\/p>\n\n<p>Here is what I've done so far (for clarity, I am not mentioning all my various attempts, such as using <em>conda<\/em> instead of <em>pip<\/em>, as it also failed):<\/p>\n\n<pre><code>!pip install --upgrade pip\n!pip install -I Cython==0.28.5\n!pip install pyarrow\n\nimport pandas  \nimport pyarrow\nimport pyarrow.parquet\n\n#so far, so good\n\nfilePath_parquet = \"foo.parquet\"\ntable_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n<\/code><\/pre>\n\n<p>This works well if I'm doing that off-line (using Spyder, Python v.3.7.0). But it fails using an Azure Notebook.<\/p>\n\n<pre><code> AttributeErrorTraceback (most recent call last)\n&lt;ipython-input-54-2739da3f2d20&gt; in &lt;module&gt;()\n      6 \n      7 #table_parquet_raw = pd.read_parquet(filePath_parquet, engine='pyarrow')\n----&gt; 8 table_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n\nAttributeError: 'module' object has no attribute 'read_parquet'\n<\/code><\/pre>\n\n<p>Any idea please?<\/p>\n\n<p>Thank you in advance !<\/p>\n\n<p>EDIT:<\/p>\n\n<p>Thank you very much for your reply Peter Pan !\nI have typed these  statements, here is what I got:<\/p>\n\n<p>1.<\/p>\n\n<pre><code>    print(pandas.__dict__)\n<\/code><\/pre>\n\n<p>=> read_parquet does not appear<\/p>\n\n<p>2.<\/p>\n\n<pre><code>    print(pandas.__file__)\n<\/code><\/pre>\n\n<p>=> I get:<\/p>\n\n<pre><code>    \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/pandas\/__init__.py\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li><p>import sys; print(sys.path) => I get:<\/p>\n\n<pre><code>['', '\/home\/nbuser\/anaconda3_23\/lib\/python34.zip',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/plat-linux',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/lib-dynload',\n'\/home\/nbuser\/.local\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/Sphinx-1.3.1-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/IPython\/extensions',\n'\/home\/nbuser\/.ipython']\n<\/code><\/pre><\/li>\n<\/ol>\n\n<p>Do you have any idea please ?<\/p>\n\n<p>EDIT 2:<\/p>\n\n<p>Dear @PeterPan, I have typed both <code>!conda update conda<\/code> and  <code>!conda update pandas<\/code> : when checking the Pandas version (<code>pandas.__version__<\/code>), it is still <code>0.19.2<\/code>.<\/p>\n\n<p>I have also tried with <code>!conda update pandas -y -f<\/code>, it returns:\n`Fetching package metadata ...........\nSolving package specifications: .<\/p>\n\n<p>Package plan for installation in environment \/home\/nbuser\/anaconda3_23:<\/p>\n\n<p>The following NEW packages will be INSTALLED:<\/p>\n\n<pre><code>pandas: 0.19.2-np111py34_1`\n<\/code><\/pre>\n\n<p>When typing:\n<code>!pip install --upgrade pandas<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Requirement already up-to-date: pandas in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\nRequirement already up-to-date: pytz&gt;=2011k in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: numpy&gt;=1.9.0 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: python-dateutil&gt;=2 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: six&gt;=1.5 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from python-dateutil&gt;=2-&gt;pandas)<\/code><\/p>\n\n<p>Finally, when typing:<\/p>\n\n<p><code>!pip install --upgrade pandas==0.24.0<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Collecting pandas==0.24.0\n  Could not find a version that satisfies the requirement pandas==0.24.0 (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0rc1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0rc2, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0rc1, 0.19.0, 0.19.1, 0.19.2, 0.20.0rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0rc1, 0.21.0, 0.21.1, 0.22.0)\nNo matching distribution found for pandas==0.24.0<\/code><\/p>\n\n<p>Therefore, my guess is that the problem comes from the way the packages are managed in Azure. Updating a package (here Pandas), should lead to an update to the latest version available, shouldn't it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1545322944093,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":1879.0,
        "Owner_creation_time":1545322329030,
        "Owner_last_access_time":1556103635952,
        "Owner_reputation":25.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":5.0,
        "Answer_body":"<p>I tried to reproduce your issue on my Azure Jupyter Notebook, but failed. There was no any issue for me without doing your two steps <code>!pip install --upgrade pip<\/code> &amp; <code>!pip install -I Cython==0.28.5<\/code> which I think not matter.<\/p>\n\n<p>Please run some codes below to check your import package <code>pandas<\/code> whether be correct.<\/p>\n\n<ol>\n<li>Run <code>print(pandas.__dict__)<\/code> to check whether has the description of <code>read_parquet<\/code> function in the output.<\/li>\n<li>Run <code>print(pandas.__file__)<\/code> to check whether you imported a different <code>pandas<\/code> package.<\/li>\n<li>Run <code>import sys; print(sys.path)<\/code> to check the order of paths whether there is a same named file or directory under these paths.<\/li>\n<\/ol>\n\n<p>If there is a same file or directory named <code>pandas<\/code>, you just need to rename it and restart your <code>ipynb<\/code> to re-run. It's a common issue which you can refer to these SO threads <a href=\"https:\/\/stackoverflow.com\/questions\/35341363\/attributeerror-module-object-has-no-attribute-reader\">AttributeError: &#39;module&#39; object has no attribute &#39;reader&#39;<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/36250353\/importing-installed-package-from-script-raises-attributeerror-module-has-no-at\">Importing installed package from script raises &quot;AttributeError: module has no attribute&quot; or &quot;ImportError: cannot import name&quot;<\/a>.<\/p>\n\n<p>In Other cases, please update your post for more details to let me know.<\/p>\n\n<hr>\n\n<p>The latest <code>pandas<\/code> version should be <code>0.23.4<\/code>, not <code>0.24.0<\/code>.<\/p>\n\n<p>I tried to find out the earliest version of <code>pandas<\/code> which support the <code>read_parquet<\/code> feature via search the function name <code>read_parquet<\/code> in the documents of different version from <code>0.19.2<\/code> to <code>0.23.3<\/code>. Then, I found <code>pandas<\/code> supports <code>read_parquet<\/code> feature after the version <code>0.21.1<\/code>, as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The new features shown in the <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/whatsnew.html\" rel=\"nofollow noreferrer\"><code>What's New<\/code><\/a> of version <code>0.21.1<\/code>\n<a href=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>According to your <code>EDIT 2<\/code> description, it seems that you are using Python 3.4 in Azure Jupyter Notebook. Not all <code>pandas<\/code> versions support Python 3.4 version.<\/p>\n\n<p>The versions <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.21.1<\/code><\/a> &amp; <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.22\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.22.0<\/code><\/a> offically support Python 2.7,3.5, and 3.6, as below.\n<a href=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the <a href=\"https:\/\/pypi.org\/project\/pandas\/\" rel=\"nofollow noreferrer\">PyPI page for <code>pandas<\/code><\/a> also requires the Python version as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6613J.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6613J.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So you can try to install the <code>pandas<\/code> versions <code>0.21.1<\/code> &amp; <code>0.22.0<\/code> in the current notebook of Python 3.4. if failed, please create a new notebook in Python <code>2.7<\/code> or <code>&gt;=3.5<\/code> to install <code>pandas<\/code> version <code>&gt;= 0.21.1<\/code> to use the function <code>read_parquet<\/code>.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1545730275287,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1547188516027,
        "Answer_last_edit_time":1547190062312,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53872444",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: cannot read \".parquet\" files in azure jupyter notebook (python 2 and 3); Content: i am currently trying to open parquet files using azure jupyter notebooks. i have tried both python kernels (2 and 3). after the installation of pyarrow i can import the module only if the python kernel is 2 (not working with python 3) here is what i've done so far (for clarity, i am not mentioning all my various attempts, such as using conda instead of pip, as it also failed): !pip install --upgrade pip !pip install -i cython==0.28.5 !pip install pyarrow import pandas import pyarrow import pyarrow.parquet #so far, so good filepath_parquet = \"foo.parquet\" table_parquet_raw = pandas.read_parquet(filepath_parquet, engine='pyarrow') this works well if i'm doing that off-line (using spyder, python v.3.7.0). but it fails using an azure notebook. attributeerrortraceback (most recent call last) <ipython-input-54-2739da3f2d20> in <module>() 6 7 #table_parquet_raw = pd.read_parquet(filepath_parquet, engine='pyarrow') ----> 8 table_parquet_raw = pandas.read_parquet(filepath_parquet, engine='pyarrow') attributeerror: 'module' object has no attribute 'read_parquet' any idea please? thank you in advance ! edit: thank you very much for your reply peter pan ! i have typed these statements, here is what i got: 1. print(pandas.__dict__) => read_parquet does not appear 2. print(pandas.__file__) => i get: \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/pandas\/__init__.py import sys; print(sys.path) => i get: ['', '\/home\/nbuser\/anaconda3_23\/lib\/python34.zip', '\/home\/nbuser\/anaconda3_23\/lib\/python3.4', '\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/plat-linux', '\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/lib-dynload', '\/home\/nbuser\/.local\/lib\/python3.4\/site-packages', '\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages', '\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/sphinx-1.3.1-py3.4.egg', '\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg', '\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/ipython\/extensions', '\/home\/nbuser\/.ipython'] do you have any idea please ? edit 2: dear @peterpan, i have typed both !conda update conda and !conda update pandas : when checking the pandas version (pandas.__version__), it is still 0.19.2. i have also tried with !conda update pandas -y -f, it returns: `fetching package metadata ........... solving package specifications: . package plan for installation in environment \/home\/nbuser\/anaconda3_23: the following new packages will be installed: pandas: 0.19.2-np111py34_1` when typing: !pip install --upgrade pandas i get: requirement already up-to-date: pandas in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages requirement already up-to-date: pytz>=2011k in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas) requirement already up-to-date: numpy>=1.9.0 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas) requirement already up-to-date: python-dateutil>=2 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas) requirement already up-to-date: six>=1.5 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from python-dateutil>=2->pandas) finally, when typing: !pip install --upgrade pandas==0.24.0 i get: collecting pandas==0.24.0 could not find a version that satisfies the requirement pandas==0.24.0 (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0rc1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0rc2, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0rc1, 0.19.0, 0.19.1, 0.19.2, 0.20.0rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0rc1, 0.21.0, 0.21.1, 0.22.0) no matching distribution found for pandas==0.24.0 therefore, my guess is that the problem comes from the way the packages are managed in azure. updating a package (here pandas), should lead to an update to the latest version available, shouldn't it?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with reading \".parquet\" files in Azure Jupyter Notebook using both Python 2 and 3, despite attempting various methods such as using 'conda' instead of 'pip'.",
        "Question_preprocessed_content":"Title: cannot read files in azure jupyter notebook ; Content: i am currently trying to open parquet files using azure jupyter notebooks. i have tried both python kernels . after the installation of pyarrow i can import the module only if the python kernel is here is what i've done so far this works well if i'm doing that . but it fails using an azure notebook. any idea please? thank you in advance ! edit thank you very much for your reply peter pan ! i have typed these statements, here is what i got . does not appear . i get import sys; i get do you have any idea please ? edit dear i have typed both and when checking the pandas version , it is still . i have also tried with , it returns `fetching package metadata solving package specifications . package plan for installation in environment the following new packages will be installed when typing i get finally, when typing i get therefore, my guess is that the problem comes from the way the packages are managed in azure. updating a package , should lead to an update to the latest version available, shouldn't it?",
        "Answer_original_content":"i tried to reproduce your issue on my azure jupyter notebook, but failed. there was no any issue for me without doing your two steps !pip install --upgrade pip & !pip install -i cython==0.28.5 which i think not matter. please run some codes below to check your import package pandas whether be correct. run print(pandas.__dict__) to check whether has the description of read_parquet function in the output. run print(pandas.__file__) to check whether you imported a different pandas package. run import sys; print(sys.path) to check the order of paths whether there is a same named file or directory under these paths. if there is a same file or directory named pandas, you just need to rename it and restart your ipynb to re-run. it's a common issue which you can refer to these so threads attributeerror: 'module' object has no attribute 'reader' and importing installed package from script raises \"attributeerror: module has no attribute\" or \"importerror: cannot import name\". in other cases, please update your post for more details to let me know. the latest pandas version should be 0.23.4, not 0.24.0. i tried to find out the earliest version of pandas which support the read_parquet feature via search the function name read_parquet in the documents of different version from 0.19.2 to 0.23.3. then, i found pandas supports read_parquet feature after the version 0.21.1, as below. the new features shown in the what's new of version 0.21.1 according to your edit 2 description, it seems that you are using python 3.4 in azure jupyter notebook. not all pandas versions support python 3.4 version. the versions 0.21.1 & 0.22.0 offically support python 2.7,3.5, and 3.6, as below. and the pypi page for pandas also requires the python version as below. so you can try to install the pandas versions 0.21.1 & 0.22.0 in the current notebook of python 3.4. if failed, please create a new notebook in python 2.7 or >=3.5 to install pandas version >= 0.21.1 to use the function read_parquet.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of reading \".parquet\" files in Azure Jupyter Notebook using both Python 2 and 3 include checking the import package pandas, renaming any same named file or directory, updating pandas to the latest version, and installing pandas versions 0.21.1 & 0.22.0 in the current notebook of Python 3.4 or creating a new notebook in Python 2.7 or >=3.5 to install pandas version >= 0.21.1 to use the function read_parquet.",
        "Answer_preprocessed_content":"i tried to reproduce your issue on my azure jupyter notebook, but failed. there was no any issue for me without doing your two steps & which i think not matter. please run some codes below to check your import package whether be correct. run to check whether has the description of function in the output. run to check whether you imported a different package. run to check the order of paths whether there is a same named file or directory under these paths. if there is a same file or directory named , you just need to rename it and restart your to it's a common issue which you can refer to these so threads attributeerror 'module' object has no attribute 'reader' and importing installed package from script raises attributeerror module has no attribute or importerror cannot import name . in other cases, please update your post for more details to let me know. the latest version should be , not . i tried to find out the earliest version of which support the feature via search the function name in the documents of different version from to . then, i found supports feature after the version , as below. the new features shown in the of version according to your description, it seems that you are using python in azure jupyter notebook. not all versions support python version. the versions & offically support python and as below. and the pypi page for also requires the python version as below. so you can try to install the versions & in the current notebook of python if failed, please create a new notebook in python or to install version to use the function ."
    },
    {
        "Question_id":36110109.0,
        "Question_title":"Has anyone any experience on implementing the R Package XGBoost within the Azure ML Studio environment?",
        "Question_body":"<p>I was hoping that someone would have tried to or had success in implementing it and would have knowledge of any pitfalls in using it.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1458446579210,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":436.0,
        "Owner_creation_time":1428820274636,
        "Owner_last_access_time":1663952987080,
        "Owner_reputation":65.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":21.0,
        "Answer_body":"<p>You need to zip &amp; load the package windows binaries in dataset &amp; import it to the R environment.<\/p>\n<p>You can follow the instructions over <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Using-XGBoost-to-build-Graduation-Admit-Model-1\" rel=\"nofollow noreferrer\">here<\/a>. I couldn't import it for the latest version, so I simply downloaded the xgboost version from this experiment &amp; loaded it to my saved datasets<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/archive\/blogs\/benjguin\/how-to-upload-an-r-package-to-azure-machine-learning\" rel=\"nofollow noreferrer\">This<\/a> is for any generic packages which are not preloaded in the environment<\/p>\n<p>The following is a <a href=\"https:\/\/web.archive.org\/web\/20161020215633\/https:\/\/azure.microsoft.com\/en-in\/documentation\/articles\/machine-learning-r-csharp-web-service-examples\/\" rel=\"nofollow noreferrer\">list of experiments<\/a> to publish R models as a web service<\/p>\n<p>Hope this helps!<\/p>\n<p>Edit: You can also simply change the R version to Microsoft Open R (current version 3.2.2) and you can import xgboost as any common library<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1474621313252,
        "Answer_score":1.0,
        "Owner_location":"Lexington, KY, United States",
        "Question_last_edit_time":1459352400923,
        "Answer_last_edit_time":1643619684692,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36110109",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: has anyone any experience on implementing the r package xgboost within the studio environment?; Content: i was hoping that someone would have tried to or had success in implementing it and would have knowledge of any pitfalls in using it.",
        "Question_original_content_gpt_summary":"The user is hoping to find someone with experience in implementing the R package xgboost within the Studio environment, and to gain knowledge of any potential pitfalls.",
        "Question_preprocessed_content":"Title: has anyone any experience on implementing the r package xgboost within the studio environment?; Content: i was hoping that someone would have tried to or had success in implementing it and would have knowledge of any pitfalls in using it.",
        "Answer_original_content":"you need to zip & load the package windows binaries in dataset & import it to the r environment. you can follow the instructions over here. i couldn't import it for the latest version, so i simply downloaded the xgboost version from this experiment & loaded it to my saved datasets this is for any generic packages which are not preloaded in the environment the following is a list of experiments to publish r models as a web service hope this helps! edit: you can also simply change the r version to microsoft open r (current version 3.2.2) and you can import xgboost as any common library",
        "Answer_original_content_gpt_summary":"Possible solutions to implementing the R package xgboost within the Studio environment include zipping and loading the package windows binaries in dataset and importing it to the R environment, downloading the xgboost version from an experiment and loading it to saved datasets, changing the R version to Microsoft Open R, and following a list of experiments to publish R models as a web service.",
        "Answer_preprocessed_content":"you need to zip & load the package windows binaries in dataset & import it to the r environment. you can follow the instructions over here. i couldn't import it for the latest version, so i simply downloaded the xgboost version from this experiment & loaded it to my saved datasets this is for any generic packages which are not preloaded in the environment the following is a list of experiments to publish r models as a web service hope this helps! edit you can also simply change the r version to microsoft open r and you can import xgboost as any common library"
    },
    {
        "Question_id":55151159.0,
        "Question_title":"How do I \"denormalize\" data in Azure Machine Learning Studio",
        "Question_body":"<p>I'm using a neural network to predict how expensive a box will be to ship given the various sizes of the objects that go into them. <\/p>\n\n<p>I'm normalizing all my data to make everything fit between 0 and 1, but how do I reverse the normalization so I can view my data set and see how close it is?<\/p>\n\n<p>Here's what I have so far for reference:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/emjkl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/emjkl.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1552511011983,
        "Question_favorite_count":1.0,
        "Question_score":1.0,
        "Question_view_count":724.0,
        "Owner_creation_time":1337873523780,
        "Owner_last_access_time":1662860472680,
        "Owner_reputation":2577.0,
        "Owner_up_votes":59.0,
        "Owner_down_votes":13.0,
        "Owner_views":363.0,
        "Answer_body":"<p>I wouldn't try to reverse the normalization, but instead join the normalized version of the dataset with the original version after you apply <code>Score Model<\/code>.<\/p>\n\n<p>The <code>Join Data<\/code> module should work if you have a key column, otherwise just use <code>Add Columns<\/code>. <\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1552552367800,
        "Answer_score":1.0,
        "Owner_location":"Houston, TX, USA",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55151159",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i \"denormalize\" data in studio; Content: i'm using a neural network to predict how expensive a box will be to ship given the various sizes of the objects that go into them. i'm normalizing all my data to make everything fit between 0 and 1, but how do i reverse the normalization so i can view my data set and see how close it is? here's what i have so far for reference:",
        "Question_original_content_gpt_summary":"The user is trying to use a neural network to predict the cost of shipping a box based on the sizes of the objects inside, and is having difficulty reversing the normalization process to view the data set and assess its accuracy.",
        "Question_preprocessed_content":"Title: how do i denormalize data in studio; Content: i'm using a neural network to predict how expensive a box will be to ship given the various sizes of the objects that go into them. i'm normalizing all my data to make everything fit between and , but how do i reverse the normalization so i can view my data set and see how close it is? here's what i have so far for reference",
        "Answer_original_content":"i wouldn't try to reverse the normalization, but instead join the normalized version of the dataset with the original version after you apply score model. the join data module should work if you have a key column, otherwise just use add columns.",
        "Answer_original_content_gpt_summary":"The solution to the user's difficulty in reversing the normalization process to view the data set and assess its accuracy is to join the normalized version of the dataset with the original version after applying the score model. The join data module can be used if there is a key column, otherwise, the add columns module can be used.",
        "Answer_preprocessed_content":"i wouldn't try to reverse the normalization, but instead join the normalized version of the dataset with the original version after you apply . the module should work if you have a key column, otherwise just use ."
    },
    {
        "Question_id":73058582.0,
        "Question_title":"Access denied for aws public sagemaker xgboost registry",
        "Question_body":"<p>I am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title<\/a>, however whenever I run the sagemaker pipeline I get the error:<\/p>\n<pre><code>ClientError: Failed to invoke sagemaker:CreateModelPackage. \nError Details: Access denied for registry ID: 246618743249, repository name: sagemaker-xgboost. \nPlease check if your ECR image exists and has proper pull permissions for SageMaker.\n<\/code><\/pre>\n<p>Here is the attached role boundary I am using to run the pipeline:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: [\n                &quot;codebuild:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;codepipeline:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;events:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:DescribeLogGroups&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:iam::xxxxxxxxxxxx:role\/ml-*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:*&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:ecr:us-west-2:246618743249:repository\/246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>and below is the attached policies for the role:<\/p>\n<pre><code>{\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: &quot;ecr:*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Sid&quot;: &quot;&quot;\n        }\n    ],\n    &quot;Version&quot;: &quot;2012-10-17&quot;\n}\n<\/code><\/pre>\n<p>plus the AWSCodePipelineFullAccess, AWSCodeBuildAdminAccess, and AmazonSageMakerFullAccess managed policies.<\/p>\n<p>Why can't I access the image\/why am I getting this error? As you can see I gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1658354562373,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":61.0,
        "Owner_creation_time":1653511725307,
        "Owner_last_access_time":1663251784528,
        "Owner_reputation":35.0,
        "Owner_up_votes":25.0,
        "Owner_down_votes":0.0,
        "Owner_views":26.0,
        "Answer_body":"<p>I had to change the boundary to be this: <code> arn:aws:ecr:us-west-2:246618743249:repository\/sagemaker-xgboost<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658508649572,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1658354892732,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73058582",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: access denied for aws public xgboost registry; Content: i am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here: https:\/\/docs.aws.amazon.com\/\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title, however whenever i run the pipeline i get the error: clienterror: failed to invoke :createmodelpackage. error details: access denied for registry id: 246618743249, repository name: -xgboost. please check if your ecr image exists and has proper pull permissions for . here is the attached role boundary i am using to run the pipeline: { \"version\": \"2012-10-17\", \"statement\": [ { \"action\": [ \"codebuild:*\" ], \"resource\": \"*\", \"effect\": \"allow\" }, { \"action\": [ \"codepipeline:*\" ], \"resource\": \"*\", \"effect\": \"allow\" }, { \"action\": [ \"events:*\" ], \"resource\": \"*\", \"effect\": \"allow\" }, { \"action\": [ \"logs:createloggroup\", \"logs:createlogstream\", \"logs:describelogstreams\", \"logs:describeloggroups\", \"logs:putlogevents\" ], \"resource\": \"*\", \"effect\": \"allow\" }, { \"action\": [ \"iam:passrole\" ], \"resource\": [ \"arn:aws:iam::xxxxxxxxxxxx:role\/ml-*\" ], \"effect\": \"allow\" }, { \"action\": [ \"ecr:*\" ], \"resource\": [ \"arn:aws:ecr:us-west-2:246618743249:repository\/246618743249.dkr.ecr.us-west-2.amazonaws.com\/-xgboost:1.0-1\" ], \"effect\": \"allow\" }, { \"action\": [ \"ecr:getauthorizationtoken\" ], \"resource\": [ \"*\" ], \"effect\": \"allow\" } ] } and below is the attached policies for the role: { \"statement\": [ { \"action\": \"ecr:*\", \"effect\": \"allow\", \"resource\": \"*\", \"sid\": \"\" } ], \"version\": \"2012-10-17\" } plus the awscodepipelinefullaccess, awscodebuildadminaccess, and amazonfullaccess managed policies. why can't i access the image\/why am i getting this error? as you can see i gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.",
        "Question_original_content_gpt_summary":"The user is encountering an \"access denied\" error when attempting to pull a prebuilt XGBoost image from the public AWS XGBoost registry, despite having given their role full permissions for the ECR registry.",
        "Question_preprocessed_content":"Title: access denied for aws public xgboost registry; Content: i am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here however whenever i run the pipeline i get the error here is the attached role boundary i am using to run the pipeline and below is the attached policies for the role plus the awscodepipelinefullaccess, awscodebuildadminaccess, and amazonfullaccess managed policies. why can't i access the am i getting this error? as you can see i gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.",
        "Answer_original_content":"i had to change the boundary to be this: arn:aws:ecr:us-west-2:246618743249:repository\/-xgboost",
        "Answer_original_content_gpt_summary":"The solution to the \"access denied\" error when pulling a prebuilt XGBoost image from the public AWS XGBoost registry is to change the boundary to \"arn:aws:ecr:us-west-2:246618743249:repository\/-xgboost\".",
        "Answer_preprocessed_content":"i had to change the boundary to be this"
    },
    {
        "Question_id":56134165.0,
        "Question_title":"Render hyperlink in pandas df in jupyterlab",
        "Question_body":"<p>I am trying to render a url inside a pandas dataframe output.  I followed along with some of the other examples out there, here is my implementation:<\/p>\n\n<pre><code>def create_url(product_name):\n    search = 'http:\/\/www.example.com\/search'\n    url = 'http:\/\/www.example.com\/search\/'+product_name\n    return url\n\ndef make_clickable(url):\n    return '&lt;a target=\"_blank\" href=\"{}\"&gt;{}&lt;\/a&gt;'.format(url, url)\n\n...\n\ndf['url'] = df['product_name'].apply(format_url)\ndf.style.format({'url': make_clickable})\n<\/code><\/pre>\n\n<p>This produces a correctly formatted raw text hyperlink, however its not clickable within the output.<\/p>\n\n<p>I should add that I'm doing this in an AWS sagemaker jupyterlab notebook which potentially disables hyperlinking in the output.  Not sure how I would check that though.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1557848494950,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":241.0,
        "Owner_creation_time":1376684625670,
        "Owner_last_access_time":1663967616407,
        "Owner_reputation":1105.0,
        "Owner_up_votes":658.0,
        "Owner_down_votes":2.0,
        "Owner_views":222.0,
        "Answer_body":"<p>If this doesn't work, I'm guessing it's an AWS thing<\/p>\n\n<ol>\n<li><code>IPython.display.HTML<\/code><\/li>\n<li><code>pandas.DataFrame.to_html<\/code> with <code>escape=False<\/code><\/li>\n<li><code>pandas.set_option('display.max_colwidth', 2000)<\/code> Must be a large number to accommodate length of link tag.  I'll say that I think this is broken.  It shouldn't be necessary to set <code>'display.max_colwidth'<\/code> in order to make sure <code>to_html<\/code> outputs properly.  But it is :-\/<\/li>\n<\/ol>\n\n<hr>\n\n<pre><code>from IPython import display\n\npd.set_option('display.max_colwidth', 2000)\n\ndisplay.HTML(df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=False))\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcVj6.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1557849422180,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1557931098116,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56134165",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: render hyperlink in pandas df in jupyterlab; Content: i am trying to render a url inside a pandas dataframe output. i followed along with some of the other examples out there, here is my implementation: def create_url(product_name): search = 'http:\/\/www.example.com\/search' url = 'http:\/\/www.example.com\/search\/'+product_name return url def make_clickable(url): return '<a target=\"_blank\" href=\"{}\">{}<\/a>'.format(url, url) ... df['url'] = df['product_name'].apply(format_url) df.style.format({'url': make_clickable}) this produces a correctly formatted raw text hyperlink, however its not clickable within the output. i should add that i'm doing this in an jupyterlab notebook which potentially disables hyperlinking in the output. not sure how i would check that though.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in rendering a URL inside a pandas dataframe output in a JupyterLab notebook, which may be disabling hyperlinking in the output.",
        "Question_preprocessed_content":"Title: render hyperlink in pandas df in jupyterlab; Content: i am trying to render a url inside a pandas dataframe output. i followed along with some of the other examples out there, here is my implementation this produces a correctly formatted raw text hyperlink, however its not clickable within the output. i should add that i'm doing this in an jupyterlab notebook which potentially disables hyperlinking in the output. not sure how i would check that though.",
        "Answer_original_content":"if this doesn't work, i'm guessing it's an aws thing ipython.display.html pandas.dataframe.to_html with escape=false pandas.set_option('display.max_colwidth', 2000) must be a large number to accommodate length of link tag. i'll say that i think this is broken. it shouldn't be necessary to set 'display.max_colwidth' in order to make sure to_html outputs properly. but it is :-\/ from ipython import display pd.set_option('display.max_colwidth', 2000) display.html(df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=false))",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include using the `ipython.display.html` and `pandas.dataframe.to_html` functions with `escape=false` and setting `pandas.set_option('display.max_colwidth', 2000)` to a large number to accommodate the length of the link tag. Another solution involves importing `ipython.display` and using `display.html` with `df.assign(url=[*map(make_clickable, df.url)]).to_html(escape=false)`. However, the answer also suggests that the issue may be a problem with AWS and that it shouldn't be necessary to set `display.max_colwidth` to ensure proper output.",
        "Answer_preprocessed_content":"if this doesn't work, i'm guessing it's an aws thing with must be a large number to accommodate length of link tag. i'll say that i think this is broken. it shouldn't be necessary to set in order to make sure outputs properly. but it is"
    },
    {
        "Question_id":null,
        "Question_title":"Training a TensorFlow model in Azure ML",
        "Question_body":"I am following the link below for training a TensorFlow model in Azure ML:\n\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/ml-frameworks\/tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb\n\nHowever, as my training dataset is in a container named \"sample-datasets\" in ADLS Gen2, I changed the following code (in the above link) to refer to the paths in my data lake. So I replaced code A (in the link above) with code B (my code)\n\nCode A:\n\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-labels-idx1-ubyte.gz',\nfilename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-labels-idx1-ubyte.gz',\nfilename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))\n\n\n\n\nCode B:\n\nfrom azureml.core.dataset import Dataset\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))\n\nBut I receive the following error:\n\nHTTPError: HTTP Error 401: Server failed to authenticate the request. Please refer to the information in the www-authenticate header.\n\nCan you please let me know how I can train the model using my data which are stored in the data lake? More precisely, how my Python code can copy the training dataset from my data lake into data_folder?\n\nPS: Please note that I have already granted the Blob Storage data Contributor role on my data lake storage account to my Azure ML workspace as a managed identity.",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1649367124903,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/804968\/training-a-tensorflow-model-in-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-08T12:21:38.973Z",
                "Answer_score":0,
                "Answer_body":"anonymous user I have not worked on ADLS scenarios with Azure ML but I have added the ADLS tag to this thread for others to chip in and add their views.\n\nBased on the documentation for ADLS REST API it supports Azure Active Directory (Azure AD), Shared Key, and shared access signature (SAS) authorization with the APIs that are available to download the files from its storage. So, I think a direct download might not work in this case without authentication.\n\nI think the easiest way to get your files locally from ADLS is to use the python SDK to authenticate using account key or AD as listed here.\n\nIf you have many files that needs to be downloaded and referenced in your ML experiments then you may also consider to use the import data module of designer for designer experiments or register them as dataset from dataset tab of ml.azure.com which can also be referenced using the Azure ML SDK.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-04-19T15:42:44.427Z",
                "Answer_score":0,
                "Answer_body":"I solved the problem by assigning an user-assigned managed identity to the target compute to access my ASDLS Gen2",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: training a tensorflow model in ; Content: i am following the link below for training a tensorflow model in : https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/master\/how-to-use-\/ml-frameworks\/tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb however, as my training dataset is in a container named \"sample-datasets\" in adls gen2, i changed the following code (in the above link) to refer to the paths in my data lake. so i replaced code a (in the link above) with code b (my code) code a: urllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz')) urllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz')) urllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz')) urllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz')) code b: from .core.dataset import dataset urllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz')) urllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz')) urllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz')) urllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz')) but i receive the following error: httperror: http error 401: server failed to authenticate the request. please refer to the information in the www-authenticate header. can you please let me know how i can train the model using my data which are stored in the data lake? more precisely, how my python code can copy the training dataset from my data lake into data_folder? ps: please note that i have already granted the blob storage data contributor role on my data lake storage account to my workspace as a managed identity.",
        "Question_original_content_gpt_summary":"The user is encountering challenges training a TensorFlow model using data stored in a data lake, and is receiving an authentication error when attempting to copy the training dataset into the data folder.",
        "Question_preprocessed_content":"Title: training a tensorflow model in ; Content: i am following the link below for training a tensorflow model in however, as my training dataset is in a container named in adls gen , i changed the following code to refer to the paths in my data lake. so i replaced code a with code b code a code b from import dataset but i receive the following error httperror http error server failed to authenticate the request. please refer to the information in the header. can you please let me know how i can train the model using my data which are stored in the data lake? more precisely, how my python code can copy the training dataset from my data lake into ps please note that i have already granted the blob storage data contributor role on my data lake storage account to my workspace as a managed identity.",
        "Answer_original_content":"anonymous user i have not worked on adls scenarios with but i have added the adls tag to this thread for others to chip in and add their views. based on the documentation for adls rest api it supports azure active directory (azure ad), shared key, and shared access signature (sas) authorization with the apis that are available to download the files from its storage. so, i think a direct download might not work in this case without authentication. i think the easiest way to get your files locally from adls is to use the python sdk to authenticate using account key or ad as listed here. if you have many files that needs to be downloaded and referenced in your ml experiments then you may also consider to use the import data module of designer for designer experiments or register them as dataset from dataset tab of ml.azure.com which can also be referenced using the sdk. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of training a TensorFlow model using data stored in a data lake and receiving an authentication error when attempting to copy the training dataset into the data folder are: using the Python SDK to authenticate using account key or Azure Active Directory (AD), using the import data module of Designer for Designer experiments, or registering the files as a dataset from the dataset tab of ml.azure.com which can also be referenced using the SDK.",
        "Answer_preprocessed_content":"anonymous user i have not worked on adls scenarios with but i have added the adls tag to this thread for others to chip in and add their views. based on the documentation for adls rest api it supports azure active directory , shared key, and shared access signature authorization with the apis that are available to download the files from its storage. so, i think a direct download might not work in this case without authentication. i think the easiest way to get your files locally from adls is to use the python sdk to authenticate using account key or ad as listed here. if you have many files that needs to be downloaded and referenced in your ml experiments then you may also consider to use the import data module of designer for designer experiments or register them as dataset from dataset tab of which can also be referenced using the sdk. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":null,
        "Question_title":"Running multiple batches of an experiment with different hyperparameter flag values",
        "Question_body":"<p>I am trying to utilize the grid search capability of guild to run an experiment multiple times with different hyperparameter flag values in a python file for SVM on the iris dataset.<\/p>\n<p>My hyperparameter flags are defined as a dictionary:<br>\nhyperparam_dict = {\u2018kernel\u2019: \u2018linear\u2019, \u2018test_split\u2019: 0.1, \u2018random_seed\u2019: 2, \u2018degree\u2019: 4, \u2018gamma\u2019: 50} and I want to be able to run the experiment with various values for each hyperparameter to test the accuracy value of every combination of flag values\u2026 e.g \u2018test_split\u2019 = [0.1, 0.2, 0.3], \u2018degree\u2019= [1, 2, 3, 4], etc.<\/p>\n<p>When I follow the example from the get-started.ipynb:<br>\n_ = guild.run(train, x=[-0.5,-0.4,-0.3,-0.2,-0.1])<br>\nwith my code:<br>\nguild.run(main, hyperparam_dict[\u2018test_split\u2019] =  [0.1, 0.2, 0.3],) I am getting an error.<\/p>\n<p>Is there any way what I am trying to achieve, be done??<\/p>\n<p>Looking for any help to try and resolve this issue!<\/p>\n<p>Thank you.<\/p>\n<p>Original guild.yml file:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/a15c24e93e41c11d4bd4960f978ec4c8b2a9631e.png\" alt=\"image\" data-base62-sha1=\"n1si1CVnP83aIsR6893PR5wXRVQ\" width=\"536\" height=\"425\"><\/p>\n<p>Notebook commands where I am trying to achieve running multiple runs of an experiment with three different  'test_split\" values to be tested.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/c66eda4f888723d0624bac1b6a69ef5d75f7a89c.png\" data-download-href=\"\/uploads\/short-url\/sjq2ufe4bGzrQn5kM5Fk1zyT0q8.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/c66eda4f888723d0624bac1b6a69ef5d75f7a89c.png\" alt=\"image\" data-base62-sha1=\"sjq2ufe4bGzrQn5kM5Fk1zyT0q8\" width=\"690\" height=\"197\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/c66eda4f888723d0624bac1b6a69ef5d75f7a89c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">727\u00d7208 9.91 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1624051984492,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":364.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/running-multiple-batches-of-an-experiment-with-different-hyperparameter-flag-values\/724",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-25T04:25:58.675Z",
                "Answer_body":"<p>You should update the dict value before you call <code>guild.run<\/code>, i.e.<\/p>\n<pre><code class=\"lang-python\">hyperparm_dict['test_split'] = [.5, .4, .3]\nguild.run(main, **hyperparam_dict)\n<\/code><\/pre>",
                "Answer_score":52.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-02T18:41:28.069Z",
                "Answer_body":"<p>You can also use this syntax:<\/p>\n<pre><code class=\"lang-python\">guild.run(main, test_split=[0.5, 0.4, 0.3], **hyperparam_dict)\n<\/code><\/pre>",
                "Answer_score":2.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-19T16:37:54.364Z",
                "Answer_body":"<p>Thank you both for the reply, I was able to fix the issue and it is working properly now. The solution that worked ended up being updating the dictionary within the guild.yml file before the guild.run command is executed.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: running multiple batches of an experiment with different hyperparameter flag values; Content: i am trying to utilize the grid search capability of guild to run an experiment multiple times with different hyperparameter flag values in a python file for svm on the iris dataset. my hyperparameter flags are defined as a dictionary: hyperparam_dict = {\u2018kernel\u2019: \u2018linear\u2019, \u2018test_split\u2019: 0.1, \u2018random_seed\u2019: 2, \u2018degree\u2019: 4, \u2018gamma\u2019: 50} and i want to be able to run the experiment with various values for each hyperparameter to test the accuracy value of every combination of flag values\u2026 e.g \u2018test_split\u2019 = [0.1, 0.2, 0.3], \u2018degree\u2019= [1, 2, 3, 4], etc. when i follow the example from the get-started.ipynb: _ = guild.run(train, x=[-0.5,-0.4,-0.3,-0.2,-0.1]) with my code: guild.run(main, hyperparam_dict[\u2018test_split\u2019] = [0.1, 0.2, 0.3],) i am getting an error. is there any way what i am trying to achieve, be done?? looking for any help to try and resolve this issue! thank you. original guild.yml file: notebook commands where i am trying to achieve running multiple runs of an experiment with three different 'test_split\" values to be tested. image727\u00d7208 9.91 kb",
        "Question_original_content_gpt_summary":"The user is encountering challenges in running multiple batches of an experiment with different hyperparameter flag values in order to test the accuracy of each combination.",
        "Question_preprocessed_content":"Title: running multiple batches of an experiment with different hyperparameter flag values; Content: i am trying to utilize the grid search capability of guild to run an experiment multiple times with different hyperparameter flag values in a python file for svm on the iris dataset. my hyperparameter flags are defined as a dictionary and i want to be able to run the experiment with various values for each hyperparameter to test the accuracy value of every combination of flag values , degree , etc. when i follow the example from the _ with my code , i am getting an error. is there any way what i am trying to achieve, be done?? looking for any help to try and resolve this issue! thank you. original file notebook commands where i am trying to achieve running multiple runs of an experiment with three different values to be tested. image kb",
        "Answer_original_content":"you should update the dict value before you call guild.run, i.e. hyperparm_dict['test_split'] = [.5, .4, .3] guild.run(main, **hyperparam_dict) you can also use this syntax: guild.run(main, test_split=[0.5, 0.4, 0.3], **hyperparam_dict) thank you both for the reply, i was able to fix the issue and it is working properly now. the solution that worked ended up being updating the dictionary within the guild.yml file before the guild.run command is executed.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of running multiple batches of an experiment with different hyperparameter flag values is to update the dictionary within the guild.yml file before executing the guild.run command. The answer also suggests updating the dict value before calling guild.run or using a specific syntax for guild.run.",
        "Answer_preprocessed_content":"you should update the dict value before you call , you can also use this syntax thank you both for the reply, i was able to fix the issue and it is working properly now. the solution that worked ended up being updating the dictionary within the file before the command is executed."
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker PIPE Mode vs FSx ?",
        "Question_body":"Hi, SageMaker supports training data streaming via PIPE mode, and also reading from FSx distributed file system. Those options seem to provide same value: low latency, high throughput.\n\nWhat are the reasons for using one or the other?\nDo we have any benchmark of PIPE vs FSx for SageMaker, in terms of costs and speed?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1579692074000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":124.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyS6bjxG4R4qtnrXzA3uSeg\/sage-maker-pipe-mode-vs-f-sx",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-01-22T22:29:08.000Z",
                "Answer_score":0,
                "Answer_body":"I can think of the following scenarios\n\nPipemode cons\n\n** UPDATED**\n\nData Shuffling - In pipe mode you are working with streaming data and hence you cannot perform data shuffle operations unless you are prepared to shuffle within batches (as in wait to read a batch of records and shuffle within the batch in Pipe mode). Of if your data is distributed across multiples files, then you could use Sagemaker data shuffle to perform file level shuffle\n\nData readers - There are default data readers for pipemode that come with Tensorflow for formats like csv, tfrecord etc. But if you have custom data formats or using a different deep leaning framework, yYou would have to use custom data readers to deal with the raw bytes and understand the logical end of record. You could also use ml-io to see if any of the built-in pipe mode readers work for your usecase\n\nPIPE mode streams the data for each epoch from S3 and hence will be slower than FSX when you run a few epochs\n\nFSX:\n\nFSX works by lazy loading the s3 file and hence it has a start up delay but gets faster during repeated training.\n\nThere is no dependency on the framework and your existing code will work as is..\n\nThe only con of using FSX is the additional storage costs, but I would almost prefer FSX to pipe mode in most cases.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pipe mode vs fsx ?; Content: hi, supports training data streaming via pipe mode, and also reading from fsx distributed file system. those options seem to provide same value: low latency, high throughput. what are the reasons for using one or the other? do we have any benchmark of pipe vs fsx for , in terms of costs and speed?",
        "Question_original_content_gpt_summary":"The user is trying to determine the differences between using pipe mode and FSx for training data streaming, and is looking for benchmarks to compare the costs and speed of the two options.",
        "Question_preprocessed_content":"Title: pipe mode vs fsx ?; Content: hi, supports training data streaming via pipe mode, and also reading from fsx distributed file system. those options seem to provide same value low latency, high throughput. what are the reasons for using one or the other? do we have any benchmark of pipe vs fsx for , in terms of costs and speed?",
        "Answer_original_content":"i can think of the following scenarios pipemode cons ** updated** data shuffling - in pipe mode you are working with streaming data and hence you cannot perform data shuffle operations unless you are prepared to shuffle within batches (as in wait to read a batch of records and shuffle within the batch in pipe mode). of if your data is distributed across multiples files, then you could use data shuffle to perform file level shuffle data readers - there are default data readers for pipemode that come with tensorflow for formats like csv, tfrecord etc. but if you have custom data formats or using a different deep leaning framework, yyou would have to use custom data readers to deal with the raw bytes and understand the logical end of record. you could also use ml-io to see if any of the built-in pipe mode readers work for your usecase pipe mode streams the data for each epoch from s3 and hence will be slower than fsx when you run a few epochs fsx: fsx works by lazy loading the s3 file and hence it has a start up delay but gets faster during repeated training. there is no dependency on the framework and your existing code will work as is.. the only con of using fsx is the additional storage costs, but i would almost prefer fsx to pipe mode in most cases.",
        "Answer_original_content_gpt_summary":"Possible solutions for comparing the costs and speed of using pipe mode and FSx for training data streaming are provided in the answer. The answer highlights that pipe mode has cons such as updated data shuffling and data readers, while FSx has a start-up delay but gets faster during repeated training. Additionally, the only con of using FSx is the additional storage costs. Overall, the answer suggests that FSx is preferred over pipe mode in most cases.",
        "Answer_preprocessed_content":"i can think of the following scenarios pipemode cons updated data shuffling in pipe mode you are working with streaming data and hence you cannot perform data shuffle operations unless you are prepared to shuffle within batches . of if your data is distributed across multiples files, then you could use data shuffle to perform file level shuffle data readers there are default data readers for pipemode that come with tensorflow for formats like csv, tfrecord etc. but if you have custom data formats or using a different deep leaning framework, yyou would have to use custom data readers to deal with the raw bytes and understand the logical end of record. you could also use to see if any of the pipe mode readers work for your usecase pipe mode streams the data for each epoch from s and hence will be slower than fsx when you run a few epochs fsx fsx works by lazy loading the s file and hence it has a start up delay but gets faster during repeated training. there is no dependency on the framework and your existing code will work as the only con of using fsx is the additional storage costs, but i would almost prefer fsx to pipe mode in most cases."
    },
    {
        "Question_id":63571552.0,
        "Question_title":"joblib.dump() fails when saving model to temporary data store in AMLS",
        "Question_body":"<p>I am training a model using AMLS. I have a training pipeline in which step 1 trains a model then saves the output in temporary datastore model_folder using<\/p>\n<pre><code>os.makedirs(output_folder, exist_ok=True)\noutput_path = output_folder + &quot;\/model.pkl&quot;\njoblib.dump(value=model, filename=output_path)\n<\/code><\/pre>\n<p>Step 2 loads the model and registers it. The model folder is defined in the pipeline as<\/p>\n<pre><code>model_folder = PipelineData(&quot;model_folder&quot;, datastore=ws.get_default_datastore())\n<\/code><\/pre>\n<p>However, step 1 fails when it tries to save the model with the following ServiceError:<\/p>\n<p>Failed to upload outputs due to Exception: Microsoft.RelInfra.Common.Exceptions.OperationFailedException: Cannot upload output xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx. ---&gt; Microsoft.WindowsAzure.Storage.StorageException: This request is not authorized to perform this operation using this permission.<\/p>\n<p>How can I solve this? Earlier in my code I had no problem interacting with the default datastore using<\/p>\n<pre><code>default_ds = ws.get_default_datastore()\ndefault_ds.upload_files(...)\n<\/code><\/pre>\n<p>My <code>70_driver_log.txt<\/code> is as follows:<\/p>\n<pre><code>[2020-08-25T04:03:27.315114] Entering context manager injector.\n[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['train_word2vec.py', '--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10'])\nStarting the daemon thread to refresh tokens in background for process with pid = 113\nEntering Run History Context Manager.\nCurrent directory:  \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\nPreparing to call script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\nAfter variable expansion, calling script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\n\nScript type = None\n[nltk_data] Downloading package stopwords to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/stopwords.zip.\n[nltk_data] Downloading package wordnet to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/wordnet.zip.\nOUTPUT FOLDER: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder\nLoading SQL data...\nLoading abbreviation data...\n\/azureml-envs\/azureml_xxxxx\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py:1783: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n  self.obj[item_labels[indexer[info_axis]]] = value\nPre-processing data...\nSuccesfully pre-processed the the text data\nTraining Word2Vec model...\nSaving the model...\nStarting the daemon thread to refresh tokens in background for process with pid = 113\n\n\nThe experiment completed successfully. Finalizing run...\n[2020-08-25T04:03:52.293994] TimeoutHandler __init__\n[2020-08-25T04:03:52.294149] TimeoutHandler __enter__\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n2 items cleaning up...\nCleanup took 0.44109439849853516 seconds\n[2020-08-25T04:03:52.818991] TimeoutHandler __exit__\n2020\/08\/25 04:04:00 logger.go:293: Process Exiting with Code:  0\n<\/code><\/pre>\n<p>My arg parse arguments include<\/p>\n<pre><code>parser.add_argument('--output_folder', type=str, dest='output_folder', default=&quot;output_folder&quot;, help='output folder')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":6.0,
        "Question_creation_time":1598325386190,
        "Question_favorite_count":1.0,
        "Question_score":2.0,
        "Question_view_count":571.0,
        "Owner_creation_time":1589738451347,
        "Owner_last_access_time":1656358607687,
        "Owner_reputation":179.0,
        "Owner_up_votes":2.0,
        "Owner_down_votes":0.0,
        "Owner_views":53.0,
        "Answer_body":"<p>Fixed this problem by adding my AMLS workspace to a 'storage blob data contributor' role in the AMLS default storage account. It seemly like usually this role is added by default, but it didn't happen in my case.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1598745717623,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1598328703460,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63571552",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: joblib.dump() fails when saving model to temporary data store in amls; Content: i am training a model using amls. i have a training pipeline in which step 1 trains a model then saves the output in temporary datastore model_folder using os.makedirs(output_folder, exist_ok=true) output_path = output_folder + \"\/model.pkl\" joblib.dump(value=model, filename=output_path) step 2 loads the model and registers it. the model folder is defined in the pipeline as model_folder = pipelinedata(\"model_folder\", datastore=ws.get_default_datastore()) however, step 1 fails when it tries to save the model with the following serviceerror: failed to upload outputs due to exception: microsoft.relinfra.common.exceptions.operationfailedexception: cannot upload output xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx. ---> microsoft.windowsazure.storage.storageexception: this request is not authorized to perform this operation using this permission. how can i solve this? earlier in my code i had no problem interacting with the default datastore using default_ds = ws.get_default_datastore() default_ds.upload_files(...) my 70_driver_log.txt is as follows: [2020-08-25t04:03:27.315114] entering context manager injector. [context_manager_injector.py] command line options: namespace(inject=['projectpythonpath:context_managers.projectpythonpath', 'runhistory:context_managers.runhistory', 'trackusererror:context_managers.trackusererror'], invocation=['train_word2vec.py', '--output_folder', '\/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/aiworkspace\/\/xxxxx\/mounts\/workspaceblobstore\/\/xxxxx\/model_folder', '--model_type', 'wo', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']) starting the daemon thread to refresh tokens in background for process with pid = 113 entering run history context manager. current directory: \/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/aiworkspace\/\/xxxxx\/mounts\/workspaceblobstore\/\/xxxxx preparing to call script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/aiworkspace\/\/xxxxx\/mounts\/workspaceblobstore\/\/xxxxx\/model_folder', '--model_type', 'wo', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10'] after variable expansion, calling script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/aiworkspace\/\/xxxxx\/mounts\/workspaceblobstore\/\/xxxxx\/model_folder', '--model_type', 'wo', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10'] script type = none [nltk_data] downloading package stopwords to \/root\/nltk_data... [nltk_data] unzipping corpora\/stopwords.zip. [nltk_data] downloading package wordnet to \/root\/nltk_data... [nltk_data] unzipping corpora\/wordnet.zip. output folder: \/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/aiworkspace\/\/xxxxx\/mounts\/workspaceblobstore\/\/xxxxx\/model_folder loading sql data... loading abbreviation data... \/-envs\/_xxxxx\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py:1783: settingwithcopywarning: a value is trying to be set on a copy of a slice from a dataframe. try using .loc[row_indexer,col_indexer] = value instead see the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy self.obj[item_labels[indexer[info_axis]]] = value pre-processing data... succesfully pre-processed the the text data training word2vec model... saving the model... starting the daemon thread to refresh tokens in background for process with pid = 113 the experiment completed successfully. finalizing run... [2020-08-25t04:03:52.293994] timeouthandler __init__ [2020-08-25t04:03:52.294149] timeouthandler __enter__ cleaning up all outstanding run operations, waiting 300.0 seconds 2 items cleaning up... cleanup took 0.44109439849853516 seconds [2020-08-25t04:03:52.818991] timeouthandler __exit__ 2020\/08\/25 04:04:00 logger.go:293: process exiting with code: 0 my arg parse arguments include parser.add_argument('--output_folder', type=str, dest='output_folder', default=\"output_folder\", help='output folder')",
        "Question_original_content_gpt_summary":"The user is encountering an issue with joblib.dump() failing when saving a model to a temporary data store in amls, resulting in a Microsoft.WindowsAzure.Storage.StorageException.",
        "Question_preprocessed_content":"Title: fails when saving model to temporary data store in amls; Content: i am training a model using amls. i have a training pipeline in which step trains a model then saves the output in temporary datastore using step loads the model and registers it. the model folder is defined in the pipeline as however, step fails when it tries to save the model with the following serviceerror failed to upload outputs due to exception cannot upload output this request is not authorized to perform this operation using this permission. how can i solve this? earlier in my code i had no problem interacting with the default datastore using my is as follows my arg parse arguments include",
        "Answer_original_content":"fixed this problem by adding my amls workspace to a 'storage blob data contributor' role in the amls default storage account. it seemly like usually this role is added by default, but it didn't happen in my case.",
        "Answer_original_content_gpt_summary":"The solution to the issue with joblib.dump() failing when saving a model to a temporary data store in amls is to add the amls workspace to a 'storage blob data contributor' role in the amls default storage account. This role is usually added by default, but in some cases, it may need to be added manually.",
        "Answer_preprocessed_content":"fixed this problem by adding my amls workspace to a 'storage blob data contributor' role in the amls default storage account. it seemly like usually this role is added by default, but it didn't happen in my case."
    },
    {
        "Question_id":58816515.0,
        "Question_title":"Databricks UDF calling an external web service cannot be serialised (PicklingError)",
        "Question_body":"<p>I am using Databricks and have a column in a dataframe that I need to update for every record with an external web service call. In this case it is using the Azure Machine Learning Service SDK and does a service call. This code works fine when not run as a UDF in spark (ie. just python) however it throws a serialization error when I try to call it as a UDF. The same happens if I use a lambda and a map with an rdd.<\/p>\n\n<p>The model uses fastText and can be invoked fine from Postman or python via a normal http call or using the WebService SDK from AMLS - it's just when it is a UDF that it fails with this message:<\/p>\n\n<p>TypeError: can't pickle _thread._local objects<\/p>\n\n<p>The only workaround I can think of is to loop through each record in the dataframe sequentially and update the record with a call, however this is not very efficient. I don't know if this is a spark error or because the service is loading a fasttext model. When I use the UDF and mock a return value it works though.<\/p>\n\n<p>Error at bottom...<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core import Workspace\n\ndef predictModelValue2(summary, modelName, modelLabel):  \n    raw_data = '[{\"label\": \"' + modelLabel + '\", \"model\": \"' + modelName + '\", \"as_full_account\": \"' + summary + '\"}]'\n    prediction = service.run(raw_data)\n    return prediction\n\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.functions import udf\n\npredictModelValueUDF = udf(predictModelValue2)\n\nDVIRCRAMFItemsDFScored1 = DVIRCRAMFItemsDF.withColumn(\"Result\", predictModelValueUDF(\"Summary\", \"ModelName\", \"ModelLabel\"))\n<\/code><\/pre>\n\n<blockquote>\n  <p>TypeError: can't pickle _thread._local objects<\/p>\n  \n  <p>During handling of the above exception, another exception occurred:<\/p>\n  \n  <p>PicklingError                             Traceback (most recent call\n  last)  in \n  ----> 2 x = df.withColumn(\"Result\", predictModelValueUDF(\"Summary\",\n  \"ModelName\", \"ModelLabel\"))<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in wrapper(*args)\n      194         @functools.wraps(self.func, assigned=assignments)\n      195         def wrapper(*args):\n  --> 196             return self(*args)\n      197 \n      198         wrapper.<strong>name<\/strong> = self._name<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in <strong>call<\/strong>(self, *cols)\n      172 \n      173     def <strong>call<\/strong>(self, *cols):\n  --> 174         judf = self._judf\n      175         sc = SparkContext._active_spark_context\n      176         return Column(judf.apply(_to_seq(sc, cols, _to_java_column)))<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _judf(self)\n      156         # and should have a minimal performance impact.\n      157         if self._judf_placeholder is None:\n  --> 158             self._judf_placeholder = self._create_judf()\n      159         return self._judf_placeholder\n      160 <\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _create_judf(self)\n      165         sc = spark.sparkContext\n      166 \n  --> 167         wrapped_func = _wrap_function(sc, self.func, self.returnType)\n      168         jdt = spark._jsparkSession.parseDataType(self.returnType.json())\n      169         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/sql\/udf.py in _wrap_function(sc,\n  func, returnType)\n       33 def _wrap_function(sc, func, returnType):\n       34     command = (func, returnType)\n  ---> 35     pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n       36     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n       37                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/rdd.py in _prepare_for_python_RDD(sc,\n  command)    2461     # the serialized command will be compressed by\n  broadcast    2462     ser = CloudPickleSerializer()\n  -> 2463     pickled_command = ser.dumps(command)    2464     if len(pickled_command) >\n  sc._jvm.PythonUtils.getBroadcastThreshold(sc._jsc):  # Default 1M<br>\n  2465         # The broadcast will have same life cycle as created\n  PythonRDD<\/p>\n  \n  <p>\/databricks\/spark\/python\/pyspark\/serializers.py in dumps(self, obj)\n      709                 msg = \"Could not serialize object: %s: %s\" % (e.<strong>class<\/strong>.<strong>name<\/strong>, emsg)\n      710             cloudpickle.print_exec(sys.stderr)\n  --> 711             raise pickle.PicklingError(msg)\n      712 \n      713 <\/p>\n  \n  <p>PicklingError: Could not serialize object: TypeError: can't pickle\n  _thread._local objects<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1573553894003,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":931.0,
        "Owner_creation_time":1256089885500,
        "Owner_last_access_time":1663046676847,
        "Owner_reputation":4947.0,
        "Owner_up_votes":277.0,
        "Owner_down_votes":8.0,
        "Owner_views":531.0,
        "Answer_body":"<p>I am not expert in DataBricks or Spark, but pickling functions from the local notebook context is always problematic when you are touching complex objects like the <code>service<\/code> object. In this particular case, I would recommend removing the dependency on the azureML <code>service<\/code> object and just use <code>requests<\/code> to call the service. <\/p>\n\n<p>Pull the key from the service:<\/p>\n\n<pre><code># retrieve the API keys. two keys were generated.\nkey1, key2 = service.get_keys()\nscoring_uri = service.scoring_uri\n<\/code><\/pre>\n\n<p>You should be able to use these strings in the UDF directly without pickling issues -- <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/9233ce089afb81d466076e36e7e61c3ce4cfafec\/how-to-use-azureml\/ml-frameworks\/chainer\/deployment\/train-hyperparameter-tune-deploy-with-chainer\/train-hyperparameter-tune-deploy-with-chainer.ipynb\" rel=\"nofollow noreferrer\">here is an example<\/a> of  how you would call the service with just requests. Below applied to your UDF:<\/p>\n\n<pre><code>import requests, json\ndef predictModelValue2(summary, modelName, modelLabel):  \n  input_data = json.dumps({\"summary\": summary, \"modelName\":, ....})\n\n  headers = {'Content-Type':'application\/json', 'Authorization': 'Bearer ' + key1}\n\n  # call the service for scoring\n  resp = requests.post(scoring_uri, input_data, headers=headers)\n\n  return resp.text[1]\n\n<\/code><\/pre>\n\n<p>On a side node, though: your UDF will be called for each row in your data frame and each time it will make a network call -- that will be very slow. I would recommend looking for ways to batch the execution. As you can see from your constructed json <code>service.run<\/code> will accept an array of items, so you should call it in batches of 100s or so.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1573841167916,
        "Answer_score":1.0,
        "Owner_location":"Sydney, Australia",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1573844785320,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58816515",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: databricks udf calling an external web service cannot be serialised (picklingerror); Content: i am using databricks and have a column in a dataframe that i need to update for every record with an external web service call. in this case it is using the service sdk and does a service call. this code works fine when not run as a udf in spark (ie. just python) however it throws a serialization error when i try to call it as a udf. the same happens if i use a lambda and a map with an rdd. the model uses fasttext and can be invoked fine from postman or python via a normal http call or using the webservice sdk from amls - it's just when it is a udf that it fails with this message: typeerror: can't pickle _thread._local objects the only workaround i can think of is to loop through each record in the dataframe sequentially and update the record with a call, however this is not very efficient. i don't know if this is a spark error or because the service is loading a fasttext model. when i use the udf and mock a return value it works though. error at bottom... from .core.webservice import webservice, aciwebservice from .core import workspace def predictmodelvalue2(summary, modelname, modellabel): raw_data = '[{\"label\": \"' + modellabel + '\", \"model\": \"' + modelname + '\", \"as_full_account\": \"' + summary + '\"}]' prediction = service.run(raw_data) return prediction from pyspark.sql.types import floattype from pyspark.sql.functions import udf predictmodelvalueudf = udf(predictmodelvalue2) dvircramfitemsdfscored1 = dvircramfitemsdf.withcolumn(\"result\", predictmodelvalueudf(\"summary\", \"modelname\", \"modellabel\")) typeerror: can't pickle _thread._local objects during handling of the above exception, another exception occurred: picklingerror traceback (most recent call last) in ----> 2 x = df.withcolumn(\"result\", predictmodelvalueudf(\"summary\", \"modelname\", \"modellabel\")) \/databricks\/spark\/python\/pyspark\/sql\/udf.py in wrapper(*args) 194 @functools.wraps(self.func, assigned=assignments) 195 def wrapper(*args): --> 196 return self(*args) 197 198 wrapper.name = self._name \/databricks\/spark\/python\/pyspark\/sql\/udf.py in call(self, *cols) 172 173 def call(self, *cols): --> 174 judf = self._judf 175 sc = sparkcontext._active_spark_context 176 return column(judf.apply(_to_seq(sc, cols, _to_java_column))) \/databricks\/spark\/python\/pyspark\/sql\/udf.py in _judf(self) 156 # and should have a minimal performance impact. 157 if self._judf_placeholder is none: --> 158 self._judf_placeholder = self._create_judf() 159 return self._judf_placeholder 160 \/databricks\/spark\/python\/pyspark\/sql\/udf.py in _create_judf(self) 165 sc = spark.sparkcontext 166 --> 167 wrapped_func = _wrap_function(sc, self.func, self.returntype) 168 jdt = spark._jsparksession.parsedatatype(self.returntype.json()) 169 judf = sc._jvm.org.apache.spark.sql.execution.python.userdefinedpythonfunction( \/databricks\/spark\/python\/pyspark\/sql\/udf.py in _wrap_function(sc, func, returntype) 33 def _wrap_function(sc, func, returntype): 34 command = (func, returntype) ---> 35 pickled_command, broadcast_vars, env, includes = _prepare_for_python_rdd(sc, command) 36 return sc._jvm.pythonfunction(bytearray(pickled_command), env, includes, sc.pythonexec, 37 sc.pythonver, broadcast_vars, sc._javaaccumulator) \/databricks\/spark\/python\/pyspark\/rdd.py in _prepare_for_python_rdd(sc, command) 2461 # the serialized command will be compressed by broadcast 2462 ser = cloudpickleserializer() -> 2463 pickled_command = ser.dumps(command) 2464 if len(pickled_command) > sc._jvm.pythonutils.getbroadcastthreshold(sc._jsc): # default 1m 2465 # the broadcast will have same life cycle as created pythonrdd \/databricks\/spark\/python\/pyspark\/serializers.py in dumps(self, obj) 709 msg = \"could not serialize object: %s: %s\" % (e.class.name, emsg) 710 cloudpickle.print_exec(sys.stderr) --> 711 raise pickle.picklingerror(msg) 712 713 picklingerror: could not serialize object: typeerror: can't pickle _thread._local objects",
        "Question_original_content_gpt_summary":"The user is encountering a serialization error when attempting to call an external web service as a UDF in Databricks, despite the code working fine when not run as a UDF.",
        "Question_preprocessed_content":"Title: databricks udf calling an external web service cannot be serialised ; Content: i am using databricks and have a column in a dataframe that i need to update for every record with an external web service call. in this case it is using the service sdk and does a service call. this code works fine when not run as a udf in spark however it throws a serialization error when i try to call it as a udf. the same happens if i use a lambda and a map with an rdd. the model uses fasttext and can be invoked fine from postman or python via a normal http call or using the webservice sdk from amls it's just when it is a udf that it fails with this message typeerror can't pickle objects the only workaround i can think of is to loop through each record in the dataframe sequentially and update the record with a call, however this is not very efficient. i don't know if this is a spark error or because the service is loading a fasttext model. when i use the udf and mock a return value it works though. error at typeerror can't pickle objects during handling of the above exception, another exception occurred picklingerror traceback in x predictmodelvalueudf in wrapper assigned assignments def wrapper return self in call def call judf sc return cols, in and should have a minimal performance impact. if is none return in sc jdt judf in func, returntype def func, returntype command env, includes command return env, includes, in command the serialized command will be compressed by broadcast ser cloudpickleserializer if default m the broadcast will have same life cycle as created pythonrdd in dumps msg could not serialize object %s %s % raise picklingerror could not serialize object typeerror can't pickle objects",
        "Answer_original_content":"i am not expert in databricks or spark, but pickling functions from the local notebook context is always problematic when you are touching complex objects like the service object. in this particular case, i would recommend removing the dependency on the service object and just use requests to call the service. pull the key from the service: # retrieve the api keys. two keys were generated. key1, key2 = service.get_keys() scoring_uri = service.scoring_uri you should be able to use these strings in the udf directly without pickling issues -- here is an example of how you would call the service with just requests. below applied to your udf: import requests, json def predictmodelvalue2(summary, modelname, modellabel): input_data = json.dumps({\"summary\": summary, \"modelname\":, ....}) headers = {'content-type':'application\/json', 'authorization': 'bearer ' + key1} # call the service for scoring resp = requests.post(scoring_uri, input_data, headers=headers) return resp.text[1] on a side node, though: your udf will be called for each row in your data frame and each time it will make a network call -- that will be very slow. i would recommend looking for ways to batch the execution. as you can see from your constructed json service.run will accept an array of items, so you should call it in batches of 100s or so.",
        "Answer_original_content_gpt_summary":"The solution to the serialization error when calling an external web service as a UDF in Databricks is to remove the dependency on the service object and use requests to call the service instead. The user should retrieve the API keys and scoring URI from the service and use them directly in the UDF without pickling issues. The example code provided shows how to call the service with just requests. However, the UDF will be slow as it will make a network call for each row in the data frame, so the user should look for ways to batch the execution.",
        "Answer_preprocessed_content":"i am not expert in databricks or spark, but pickling functions from the local notebook context is always problematic when you are touching complex objects like the object. in this particular case, i would recommend removing the dependency on the object and just use to call the service. pull the key from the service you should be able to use these strings in the udf directly without pickling issues here is an example of how you would call the service with just requests. below applied to your udf on a side node, though your udf will be called for each row in your data frame and each time it will make a network call that will be very slow. i would recommend looking for ways to batch the execution. as you can see from your constructed json will accept an array of items, so you should call it in batches of s or so."
    },
    {
        "Question_id":73412851.0,
        "Question_title":"What is the meaning of 'config = wandb.config'?",
        "Question_body":"<p>I try to do the settings for a sweep for my Logistic regression model. I read the tutorials of wandb and cannot understand how to make the configurations and especially the meaning of <code>config=wandb.config<\/code> in the tutorials. I would really appreciate it if someone gave me a good explanation of the steps. Here is what I've done:<\/p>\n<pre><code>sweep_config = {\n    'method': 'grid'\n}\n\nmetric = {\n    'name': 'f1-score',\n    'goal': 'maximize'\n}\n\nsweep_config['metric'] = metric\n\nparameters = {\n    'penalty': {\n        'values': ['l2']\n    },\n    'C': {\n        'values': [0.01, 0.1, 1.0, 10.0, 100.0]\n    }\n}\n\nsweep_config['parameters'] = parameters\n<\/code><\/pre>\n<p>Then I create the yaml file:<\/p>\n<pre><code>stream = open('config.yaml', 'w')\nyaml.dump(sweep_config, stream) \n<\/code><\/pre>\n<p>Then it's time for training:<\/p>\n<pre><code>with wandb.init(project=WANDB_PROJECT_NAME):\n    config = wandb.config\n    \n    features = pd.read_csv('data\/x_features.csv')\n    vectorizer = TfidfVectorizer(ngram_range=(1,2))\n\n    X_features = features = vectorizer.fit_transform(features['lemmatized_reason'])\n\n    y_labels = pd.read_csv('data\/y_labels.csv')\n\n    split_data = train_test_split(X_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels)\n    features_train, labels_train = split_data[0], split_data[2]\n    features_test, labels_test = split_data[1], split_data[3]\n    \n    config = wandb.config\n    log_reg = LogisticRegression(\n        penalty=config.penalty,\n        C = config.C\n    )\n    \n    log_reg.fit(features_train, labels_train)\n    \n    labels_pred = log_reg.predict(features_test)\n    labels_proba = log_reg.predict_proba(features_test)\n    labels=list(map(str,y_labels['label'].unique()))\n    \n    # Visualize single plot\n    cm = wandb.sklearn.plot_confusion_matrix(labels_test, labels_pred, labels)\n    \n    score_f1 = f1_score(labels_test, labels_pred, average='weighted')\n    \n    sm = wandb.sklearn.plot_summary_metrics(\n    log_reg, features_train, labels_train, features_test, labels_test)\n    \n    roc = wandb.sklearn.plot_roc(labels_test, labels_proba)\n    \n    wandb.log({\n        &quot;f1-weighted-log-regression-tfidf-skf&quot;: score_f1, \n        &quot;roc-log-regression-tfidf-skf&quot;: roc, \n        &quot;conf-mat-logistic-regression-tfidf-skf&quot;: cm,\n        &quot;summary-metrics-logistic-regression-tfidf-skf&quot;: sm\n        })\n<\/code><\/pre>\n<p>And finally sweep_id and agent outside of <code>with<\/code> statement:<\/p>\n<pre><code>sweep_id = wandb.sweep(sweep_config, project=&quot;multiple-classifiers&quot;)\nwandb.agent(sweep_id)\n<\/code><\/pre>\n<p>There is something major I am missing here with this config thing, that I just cannot understand.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1660891849347,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":61.0,
        "Owner_creation_time":1557474244067,
        "Owner_last_access_time":1663941664767,
        "Owner_reputation":626.0,
        "Owner_up_votes":120.0,
        "Owner_down_votes":3.0,
        "Owner_views":140.0,
        "Answer_body":"<p>I work at Weights &amp; Biases. With wandb Sweeps, the idea is that wandb needs to be able to change the hyperparameters in the sweep.<\/p>\n<p>The below section where the hyperparameters are passed to <code>LogisticRegression<\/code> could also be re-written<\/p>\n<pre><code>config = wandb.config\nlog_reg = LogisticRegression(\n    penalty=config.penalty,\n    C = config.C\n)\n<\/code><\/pre>\n<p>like this:<\/p>\n<pre><code>log_reg = LogisticRegression(\n    penalty=wandb.config.penalty,\n    C = wandb.config.C\n)\n<\/code><\/pre>\n<p>However, I think you're missing defining a train function or train script, which needs to also be passed to wandb. With out it, your example above won't work.<\/p>\n<p>Below is a minimal example that should help. Hopefully the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\" rel=\"nofollow noreferrer\">sweeps documentation<\/a> can also help.<\/p>\n<pre><code>import numpy as np \nimport random\nimport wandb\n\n#  Step 1: Define sweep config\nsweep_configuration = {\n    'method': 'random',\n    'name': 'sweep',\n    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n    'parameters': \n    {\n        'batch_size': {'values': [16, 32, 64]},\n        'epochs': {'values': [5, 10, 15]},\n        'lr': {'max': 0.1, 'min': 0.0001}\n     }\n}\n\n#  Step 2: Initialize sweep by passing in config\nsweep_id = wandb.sweep(sweep_configuration)\n\ndef train_one_epoch(epoch, lr, bs): \n  acc = 0.25 + ((epoch\/30) +  (random.random()\/10))\n  loss = 0.2 + (1 - ((epoch-1)\/10 +  random.random()\/5))\n  return acc, loss\n\ndef evaluate_one_epoch(epoch): \n  acc = 0.1 + ((epoch\/20) +  (random.random()\/10))\n  loss = 0.25 + (1 - ((epoch-1)\/10 +  random.random()\/6))\n  return acc, loss\n\ndef train():\n    run = wandb.init()\n\n    #  Step 3: Use hyperparameter values from `wandb.config`\n    lr  =  wandb.config.lr\n    bs = wandb.config.batch_size\n    epochs = wandb.config.epochs\n\n    for epoch in np.arange(1, epochs):\n      train_acc, train_loss = train_one_epoch(epoch, lr, bs)\n      val_acc, val_loss = evaluate_one_epoch(epoch)\n\n      wandb.log({\n        'epoch': epoch, \n        'train_acc': train_acc,\n        'train_loss': train_loss, \n        'val_acc': val_acc, \n        'val_loss': val_loss\n      })\n\n#  Step 4: Launch sweep by making a call to `wandb.agent`\nwandb.agent(sweep_id, function=train, count=4)\n<\/code><\/pre>\n<p>Finally, can you share the link where you found the code above? Maybe we need to update some examples :)<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1660907847087,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73412851",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what is the meaning of 'config = .config'?; Content: i try to do the settings for a sweep for my logistic regression model. i read the tutorials of and cannot understand how to make the configurations and especially the meaning of config=.config in the tutorials. i would really appreciate it if someone gave me a good explanation of the steps. here is what i've done: sweep_config = { 'method': 'grid' } metric = { 'name': 'f1-score', 'goal': 'maximize' } sweep_config['metric'] = metric parameters = { 'penalty': { 'values': ['l2'] }, 'c': { 'values': [0.01, 0.1, 1.0, 10.0, 100.0] } } sweep_config['parameters'] = parameters then i create the yaml file: stream = open('config.yaml', 'w') yaml.dump(sweep_config, stream) then it's time for training: with .init(project=_project_name): config = .config features = pd.read_csv('data\/x_features.csv') vectorizer = tfidfvectorizer(ngram_range=(1,2)) x_features = features = vectorizer.fit_transform(features['lemmatized_reason']) y_labels = pd.read_csv('data\/y_labels.csv') split_data = train_test_split(x_features, y_labels, train_size = 0.85, test_size = 0.15, stratify=y_labels) features_train, labels_train = split_data[0], split_data[2] features_test, labels_test = split_data[1], split_data[3] config = .config log_reg = logisticregression( penalty=config.penalty, c = config.c ) log_reg.fit(features_train, labels_train) labels_pred = log_reg.predict(features_test) labels_proba = log_reg.predict_proba(features_test) labels=list(map(str,y_labels['label'].unique())) # visualize single plot cm = .sklearn.plot_confusion_matrix(labels_test, labels_pred, labels) score_f1 = f1_score(labels_test, labels_pred, average='weighted') sm = .sklearn.plot_summary_metrics( log_reg, features_train, labels_train, features_test, labels_test) roc = .sklearn.plot_roc(labels_test, labels_proba) .log({ \"f1-weighted-log-regression-tfidf-skf\": score_f1, \"roc-log-regression-tfidf-skf\": roc, \"conf-mat-logistic-regression-tfidf-skf\": cm, \"summary-metrics-logistic-regression-tfidf-skf\": sm }) and finally sweep_id and agent outside of with statement: sweep_id = .sweep(sweep_config, project=\"multiple-classifiers\") .agent(sweep_id) there is something major i am missing here with this config thing, that i just cannot understand.",
        "Question_original_content_gpt_summary":"The user is struggling to understand the meaning of 'config = .config' in the tutorials for setting up a sweep for a logistic regression model.",
        "Question_preprocessed_content":"Title: what is the meaning of 'config ; Content: i try to do the settings for a sweep for my logistic regression model. i read the tutorials of and cannot understand how to make the configurations and especially the meaning of in the tutorials. i would really appreciate it if someone gave me a good explanation of the steps. here is what i've done then i create the yaml file then it's time for training and finally and agent outside of statement there is something major i am missing here with this config thing, that i just cannot understand.",
        "Answer_original_content":"i work at . with sweeps, the idea is that needs to be able to change the hyperparameters in the sweep. the below section where the hyperparameters are passed to logisticregression could also be re-written config = .config log_reg = logisticregression( penalty=config.penalty, c = config.c ) like this: log_reg = logisticregression( penalty=.config.penalty, c = .config.c ) however, i think you're missing defining a train function or train script, which needs to also be passed to . with out it, your example above won't work. below is a minimal example that should help. hopefully the sweeps documentation can also help. import numpy as np import random import # step 1: define sweep config sweep_configuration = { 'method': 'random', 'name': 'sweep', 'metric': {'goal': 'maximize', 'name': 'val_acc'}, 'parameters': { 'batch_size': {'values': [16, 32, 64]}, 'epochs': {'values': [5, 10, 15]}, 'lr': {'max': 0.1, 'min': 0.0001} } } # step 2: initialize sweep by passing in config sweep_id = .sweep(sweep_configuration) def train_one_epoch(epoch, lr, bs): acc = 0.25 + ((epoch\/30) + (random.random()\/10)) loss = 0.2 + (1 - ((epoch-1)\/10 + random.random()\/5)) return acc, loss def evaluate_one_epoch(epoch): acc = 0.1 + ((epoch\/20) + (random.random()\/10)) loss = 0.25 + (1 - ((epoch-1)\/10 + random.random()\/6)) return acc, loss def train(): run = .init() # step 3: use hyperparameter values from `.config` lr = .config.lr bs = .config.batch_size epochs = .config.epochs for epoch in np.arange(1, epochs): train_acc, train_loss = train_one_epoch(epoch, lr, bs) val_acc, val_loss = evaluate_one_epoch(epoch) .log({ 'epoch': epoch, 'train_acc': train_acc, 'train_loss': train_loss, 'val_acc': val_acc, 'val_loss': val_loss }) # step 4: launch sweep by making a call to `.agent` .agent(sweep_id, function=train, count=4) finally, can you share the link where you found the code above? maybe we need to update some examples :)",
        "Answer_original_content_gpt_summary":"Possible solutions extracted from the answer are:\n\n- The 'config = .config' line is used to change the hyperparameters in the sweep.\n- The hyperparameters can also be passed directly to the logistic regression model.\n- A train function or train script needs to be defined and passed to the sweep for the example to work.\n- A minimal example is provided to help with setting up the sweep.\n- The hyperparameter values from `.config` can be used in the train function.\n- The sweep can be launched by making a call to `.agent`.\n- The link where the code was found may need to be updated.",
        "Answer_preprocessed_content":"i work at . with sweeps, the idea is that needs to be able to change the hyperparameters in the sweep. the below section where the hyperparameters are passed to could also be like this however, i think you're missing defining a train function or train script, which needs to also be passed to . with out it, your example above won't work. below is a minimal example that should help. hopefully the sweeps documentation can also help. finally, can you share the link where you found the code above? maybe we need to update some examples"
    },
    {
        "Question_id":57471129.0,
        "Question_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Question_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1565670597500,
        "Question_favorite_count":null,
        "Question_score":5.0,
        "Question_view_count":1402.0,
        "Owner_creation_time":1408145271463,
        "Owner_last_access_time":1620887445852,
        "Owner_reputation":65.0,
        "Owner_up_votes":4.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1565705412680,
        "Answer_score":1.0,
        "Owner_location":"India",
        "Question_last_edit_time":1565670701316,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57471129",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unable to use gpu to train a nn model in service using p100-nc6s-v2 compute. fails wth cuda error; Content: i\u2019ve recently started working with azure for ml and am trying to use machine learning service workspace. i\u2019ve set up a workspace with the compute set to nc6s-v2 machines since i need train a nn using images on gpu. the issue is that the training still happens on the cpu \u2013 the logs say it\u2019s not able to find cuda. here\u2019s the warning log when running my script. any clues how to solve this issue? i\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. here's my code for the estimator, script_params = { '--input_data_folder': ds.path('dataset').as_mount(), '--zip_file_name': 'train.zip', '--run_mode': 'train' } est = estimator(source_directory='.\/scripts', script_params=script_params, compute_target=compute_target, entry_script='main.py', conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu'] ) run = exp.submit(config=est) run.wait_for_completion(show_output=true) the compute target was made as per the sample code on github: compute_name = \"p100-nc6s-v2\" compute_min_nodes = 0 compute_max_nodes = 4 vm_size = \"standard_nc6s_v2\" if compute_name in ws.compute_targets: compute_target = ws.compute_targets[compute_name] if compute_target and type(compute_target) is amlcompute: print('found compute target. just use it. ' + compute_name) else: print('creating a new compute target...') provisioning_config = amlcompute.provisioning_configuration(vm_size=vm_size, min_nodes=compute_min_nodes, max_nodes=compute_max_nodes) # create the cluster compute_target = computetarget.create( ws, compute_name, provisioning_config) # can poll for a minimum number of nodes and for a specific timeout. # if no min node count is provided it will use the scale settings for the cluster compute_target.wait_for_completion( show_output=true, min_node_count=none, timeout_in_minutes=20) # for a more detailed view of current amlcompute status, use get_status() print(compute_target.get_status().serialize()) this is the warning with which it fails to use the gpu: 2019-08-12 14:50:16.961247: i tensorflow\/compiler\/xla\/service\/service.cc:168] xla service 0x55a7ce570830 executing computations on platform host. devices: 2019-08-12 14:50:16.961278: i tensorflow\/compiler\/xla\/service\/service.cc:175] streamexecutor device (0): <undefined>, <undefined> 2019-08-12 14:50:16.971025: i tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: no such file or directory; ld_library_path: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/-envs\/_5fdf05c5671519f307e0f43128b8610e\/lib: 2019-08-12 14:50:16.971054: e tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuinit: unknown error (303) 2019-08-12 14:50:16.971081: i tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving cuda diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000 2019-08-12 14:50:16.971089: i tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000 2019-08-12 14:50:16.971164: i tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: not found: was unable to find libcuda.so dso loaded into this program 2019-08-12 14:50:16.971202: i tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4 device mapping: \/job:localhost\/replica:0\/task:0\/device:xla_cpu:0 -> device: xla_cpu device 2019-08-12 14:50:16.973301: i tensorflow\/core\/common_runtime\/direct_session.cc:296] device mapping: \/job:localhost\/replica:0\/task:0\/device:xla_cpu:0 -> device: xla_cpu device it's currently using the cpu as per the logs. any clues how to resolve the issue here?",
        "Question_original_content_gpt_summary":"The user is unable to use the GPU to train a neural network model in Azure Machine Learning Service using the P100-NC6S-V2 compute, resulting in a CUDA error.",
        "Question_preprocessed_content":"Title: unable to use gpu to train a nn model in service using compute. fails wth cuda error; Content: ive recently started working with azure for ml and am trying to use machine learning service workspace. ive set up a workspace with the compute set to machines since i need train a nn using images on gpu. the issue is that the training still happens on the cpu the logs say its not able to find cuda. heres the warning log when running my script. any clues how to solve this issue? ive also mentioned explicitly package in the conda packages option of the estimator. here's my code for the estimator, the compute target was made as per the sample code on github this is the warning with which it fails to use the gpu it's currently using the cpu as per the logs. any clues how to resolve the issue here?",
        "Answer_original_content":"instead of base estimator, you can use the tensorflow estimator with keras and other libraries layered on top. that way you don't have to worry about setting up and configuring the gpu libraries, as the tensorflow estimator uses a docker image with gpu libraries pre-configured. see here for documentation: api reference you can use conda_packages argument to specify additional libraries. also set argument use_gpu = true. example notebook",
        "Answer_original_content_gpt_summary":"The solution to the user's problem of being unable to use the GPU to train a neural network model in Azure Machine Learning Service using the P100-NC6S-V2 compute is to use the tensorflow estimator with keras and other libraries layered on top. This way, the user does not have to worry about setting up and configuring the GPU libraries, as the tensorflow estimator uses a docker image with GPU libraries pre-configured. The user can specify additional libraries using the conda_packages argument and set the use_gpu argument to true. An example notebook is available for reference.",
        "Answer_preprocessed_content":"instead of base estimator, you can use the tensorflow estimator with keras and other libraries layered on top. that way you don't have to worry about setting up and configuring the gpu libraries, as the tensorflow estimator uses a docker image with gpu libraries see here for documentation api reference you can use argument to specify additional libraries. also set argument . example notebook"
    },
    {
        "Question_id":58659160.0,
        "Question_title":"How to specify pip extra-index-url when creating an azureml environment?",
        "Question_body":"<p>When AzureML creates a python environment and runs <code>pip install<\/code>, I'd like it to use additional non-public indices. Is there a way to do that?<\/p>\n\n<p>I'm running my python script on an AzureML compute. The environment is created from pip requirements as per <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-use-environments#conda-and-pip-specification-files\" rel=\"nofollow noreferrer\">docs<\/a>. The script now references a package in a private index. To run the script on a local or build machine I just specify <code>PIP_EXTRA_INDEX_URL<\/code> environment variable with credentials to the index before running <code>pip install -c ...<\/code>. How to enable same functionality on AzureML environment prep process?<\/p>\n\n<p>AzureML docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-use-environments#private-wheel-files\" rel=\"nofollow noreferrer\">suggest<\/a> that I directly supply wheel files instead of package names. That means I have to manually do all the work that pip is built for: identify private packages among other requirements, choose right versions and platform, download them.<\/p>\n\n<p>Ideally, I would have to just write something like this:<\/p>\n\n<pre><code>myenv = Environment.from_pip_requirements(\n    name = \"myenv\",\n    file_path = \"path-to-pip-requirements-file\",\n    extra-index-url = [\"url1\", \"url2\"])\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1572610849127,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":5362.0,
        "Owner_creation_time":1518706063680,
        "Owner_last_access_time":1661360016967,
        "Owner_reputation":95.0,
        "Owner_up_votes":2.0,
        "Owner_down_votes":0.0,
        "Owner_views":15.0,
        "Answer_body":"<p>It appears, there is a <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py#set-pip-option-pip-option-\" rel=\"nofollow noreferrer\"><code>set_pip_option<\/code> method<\/a> in the SDK which sorts out the problem with one single extra-index-url, e.g.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.environment import CondaDependencies\ndep = CondaDependencies.create(pip_packages=[\"pyyaml\", \"param\"])\ndep.set_pip_option(\"--extra-index-url https:\/\/user:password@extra.index\/url\")\n<\/code><\/pre>\n\n<p>Unfortunately, second call to this function replaces the first value with the new one. For the <code>--extra-index-url<\/code> option this logic should be changed in order to support search in more than 2 indices (one public, one private).<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1572973318072,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58659160",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to specify pip extra-index-url when creating an environment?; Content: when creates a python environment and runs pip install, i'd like it to use additional non-public indices. is there a way to do that? i'm running my python script on an compute. the environment is created from pip requirements as per docs. the script now references a package in a private index. to run the script on a local or build machine i just specify pip_extra_index_url environment variable with credentials to the index before running pip install -c .... how to enable same functionality on environment prep process? docs suggest that i directly supply wheel files instead of package names. that means i have to manually do all the work that pip is built for: identify private packages among other requirements, choose right versions and platform, download them. ideally, i would have to just write something like this: myenv = environment.from_pip_requirements( name = \"myenv\", file_path = \"path-to-pip-requirements-file\", extra-index-url = [\"url1\", \"url2\"])",
        "Question_original_content_gpt_summary":"The user is encountering challenges with specifying a pip extra-index-url when creating a Python environment in order to access packages from a private index.",
        "Question_preprocessed_content":"Title: how to specify pip when creating an environment?; Content: when creates a python environment and runs , i'd like it to use additional indices. is there a way to do that? i'm running my python script on an compute. the environment is created from pip requirements as per docs. the script now references a package in a private index. to run the script on a local or build machine i just specify environment variable with credentials to the index before running . how to enable same functionality on environment prep process? docs suggest that i directly supply wheel files instead of package names. that means i have to manually do all the work that pip is built for identify private packages among other requirements, choose right versions and platform, download them. ideally, i would have to just write something like this",
        "Answer_original_content":"it appears, there is a set_pip_option method in the sdk which sorts out the problem with one single extra-index-url, e.g. from .core.environment import condadependencies dep = condadependencies.create(pip_packages=[\"pyyaml\", \"param\"]) dep.set_pip_option(\"--extra-index-url https:\/\/user:password@extra.index\/url\") unfortunately, second call to this function replaces the first value with the new one. for the --extra-index-url option this logic should be changed in order to support search in more than 2 indices (one public, one private).",
        "Answer_original_content_gpt_summary":"The answer suggests using the set_pip_option method in the SDK to specify a single extra-index-url for accessing packages from a private index. However, this method currently only supports one extra-index-url and a second call to the function replaces the first value with the new one. The logic for the --extra-index-url option needs to be changed to support searching in more than two indices (one public and one private).",
        "Answer_preprocessed_content":"it appears, there is a method in the sdk which sorts out the problem with one single unfortunately, second call to this function replaces the first value with the new one. for the option this logic should be changed in order to support search in more than indices ."
    },
    {
        "Question_id":50334563.0,
        "Question_title":"Deployment of an Azure ML Experiment as a Web Service through Azure Machine Learning Studio",
        "Question_body":"<p>I used Machine learning tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/create-experiment\" rel=\"nofollow noreferrer\">Create your first data science experiment in Azure Machine Learning Studio<\/a> to create an <code>Experiment<\/code> and then converted it to a <code>predictive experiment<\/code>. Now I'm trying to deploy it as a Web Service by following this article that was referenced in the above article: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/publish-a-machine-learning-web-service#deploy-it-as-a-web-service\" rel=\"nofollow noreferrer\">Deploy it as a web service<\/a>. But when I click on <code>Run<\/code> and then on <code>Deploy Web Service<\/code>, I don't see the <code>Price Plan<\/code> dropdown and <code>Plan Name<\/code> input box etc as mentioned in the section <code>Machine Learning Web Service portal Deploy Experiment Page<\/code> of the second article above. After I clicked on Deploy Web Service link in ML studio, I got the page shown below.<strong>Question<\/strong>: What I may be doing wrong?<\/p>\n\n<p>Note: You can click on the picture to get a larger view.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/G3TKo.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1526313605237,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":330.0,
        "Owner_creation_time":1330144099340,
        "Owner_last_access_time":1664039192276,
        "Owner_reputation":19815.0,
        "Owner_up_votes":2703.0,
        "Owner_down_votes":22.0,
        "Owner_views":2272.0,
        "Answer_body":"<p>I think it depends on what workspace you're in. If you're in the free one then you get the screen that you already get, but if you create a workspace in the Azure portal and use that one, then you will get a screen like below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/drRpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/drRpa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>To create a new workspace, in the Azure Portal, create a new \"Machine Learning Studio Workspace\" and when you go to Azure ML Studio select the new workspace from the top right.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1526322971967,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50334563",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deployment of an experiment as a web service through studio; Content: i used machine learning tutorial: create your first data science experiment in studio to create an experiment and then converted it to a predictive experiment. now i'm trying to deploy it as a web service by following this article that was referenced in the above article: deploy it as a web service. but when i click on run and then on deploy web service, i don't see the price plan dropdown and plan name input box etc as mentioned in the section machine learning web service portal deploy experiment page of the second article above. after i clicked on deploy web service link in ml studio, i got the page shown below.question: what i may be doing wrong? note: you can click on the picture to get a larger view.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with deploying an experiment as a web service through studio, as they are not seeing the price plan dropdown and plan name input box when they click on the 'deploy web service' link.",
        "Question_preprocessed_content":"Title: deployment of an experiment as a web service through studio; Content: i used machine learning tutorial create your first data science experiment in studio to create an and then converted it to a . now i'm trying to deploy it as a web service by following this article that was referenced in the above article deploy it as a web service. but when i click on and then on , i don't see the dropdown and input box etc as mentioned in the section of the second article above. after i clicked on deploy web service link in ml studio, i got the page shown what i may be doing wrong? note you can click on the picture to get a larger view.",
        "Answer_original_content":"i think it depends on what workspace you're in. if you're in the free one then you get the screen that you already get, but if you create a workspace in the azure portal and use that one, then you will get a screen like below. to create a new workspace, in the azure portal, create a new \"machine learning studio workspace\" and when you go to studio select the new workspace from the top right.",
        "Answer_original_content_gpt_summary":"The possible solution to the challenge of not seeing the price plan dropdown and plan name input box when deploying an experiment as a web service through studio is to create a new \"machine learning studio workspace\" in the Azure portal and select the new workspace from the top right when in studio. This solution is dependent on the workspace being used, as the free workspace may not have the necessary features.",
        "Answer_preprocessed_content":"i think it depends on what workspace you're in. if you're in the free one then you get the screen that you already get, but if you create a workspace in the azure portal and use that one, then you will get a screen like below. to create a new workspace, in the azure portal, create a new machine learning studio workspace and when you go to studio select the new workspace from the top right."
    },
    {
        "Question_id":null,
        "Question_title":"Accelerate launch and WandB agent , run the main function 4 seperate times for 4 GPUS",
        "Question_body":"<p>Hello,<\/p>\n<p>So I have changed my YAML file environment variable from \u201cpython3\u201d to \u201caccelerate launch\u201d.  I am trying to use this in conjunction with wandb agent &lt;username\/proj_name\/sweep_id&gt; on a SLURM compute cluster.<\/p>\n<p>So the error is that it runs the main function 4 times, which then instantiates the arguments 4 times and we error out because it is trying to create a page that was created on the first step. And then inevitably fails.<\/p>\n<p>I should mention that the script does work with just python3 and so it is a matter of using \u201caccelerate launch\u201d to take advantage of my multiple GPUs.<\/p>\n<pre><code class=\"lang-bash\">#!\/bin\/bash\n#SBATCH --job-name=tav_mae\n# Give job a name\n#SBATCH --time 02-20:00 # time (DD-HH:MM)\n#SBATCH --nodes=1\n#SBATCH --gpus-per-node=v100l:4 # request GPU\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=6 # maximum CPU cores per GPU request: 6 on Cedar, 16 on Graham.\n#SBATCH --mem=150G # memory per node\n#SBATCH --account=ctb-whkchun # Runs it on the dedicated nodes we have\n#SBATCH --output=\/scratch\/prsood\/tav_mae\/logs\/%N-%j.out # %N for node name, %j for jobID # Remember to mae logs-dir\nmodule load StdEnv\/2020\nmodule load cuda\nmodule load cudnn\/8.0.3\nwandb agent ddi\/TAVFormer2\/ncdfi75j --count 20\n<\/code><\/pre>\n<p>YAML configuration file:<\/p>\n<pre data-code-wrap=\"YAML\"><code class=\"lang-plaintext\">program: ..\/tav_nn.py\ncommand:\n  - ${env}\n  - accelerate\n  - launch\n  - ${program}\n  - \"--dataset\"\n\nmethod: bayes\n\nmetric:\n  goal: minimize\n  name: train\/train_loss\nparameters:\n  epoch: \n    values: [5 , 7 , 9]\n  learning_rate:\n    distribution: uniform\n    min: 0.000001\n    max: 0.0001\n  batch_size:\n    values: [2 , 4 , 8 , 1]\n  weight_decay:\n    values: [0.0001 , 0.00001 , 0.000001 , 0.0000001, 0.00000001]  \n  seed:\n    values: [32, 64, 96]\n  dropout:\n    values: [0.0,0.1,0.2]\n  early_div:\n    values: [True,False]\n  patience:\n    values: [10]\n  clip:\n    values: [1]\n  T_max:\n   values: [5,10]\n  hidden_layers:\n    values: [\"300\"]\n<\/code><\/pre>\n<p>Error output<\/p>\n<pre data-code-wrap=\"console\"><code class=\"lang-plaintext\">wandb: Starting wandb agent \ud83d\udd75\ufe0f\n2023-02-03 00:32:20,126 - wandb.wandb_agent - INFO - Running runs: []\n2023-02-03 00:32:21,726 - wandb.wandb_agent - INFO - Agent received command: run\n2023-02-03 00:32:21,728 - wandb.wandb_agent - INFO - Agent starting run with config:\n        T_max: 10\n        batch_size: 2\n        clip: 1\n        dropout: 0.1\n        early_div: True\n        epoch: 7\n        hidden_layers: 300\n        label_task: emotion\n        learning_rate: 3.736221739657802e-05\n        model: MAE_encoder\n        patience: 10\n        seed: 96\n        weight_decay: 0.0001\n2023-02-03 00:32:21,736 - wandb.wandb_agent - INFO - About to run command: \/usr\/bin\/env accelerate launch ..\/tav_nn.py --dataset ..\/..\/data\/IEMOCAP_df\n2023-02-03 00:32:26,772 - wandb.wandb_agent - INFO - Running runs: ['6fnujhey']\nwandb: Currently logged in as: prsood (ddi). Use `wandb login --relogin` to force relogin\nwandb: Currently logged in as: prsood (ddi). Use `wandb login --relogin` to force relogin\nwandb: Currently logged in as: prsood (ddi). Use `wandb login --relogin` to force relogin\nwandb: Currently logged in as: prsood (ddi). Use `wandb login --relogin` to force relogin\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\nwandb: WARNING Ignored wandb.init() arg entity when running a sweep.\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\nwandb: WARNING Ignored wandb.init() arg entity when running a sweep.\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\nwandb: WARNING Ignored wandb.init() arg entity when running a sweep.\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\nwandb: WARNING Ignored wandb.init() arg entity when running a sweep.\nThread WriterThread: wandb.init()...\nTraceback (most recent call last):\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 50, in run\n    self._run()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 101, in _run\n    self._process(record)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 351, in _process\n    self._wm.write(record)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/writer.py\", line 28, in write\n    self.open()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/writer.py\", line 24, in open\n    self._ds.open_for_write(self._settings.sync_file)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/datastore.py\", line 77, in open_for_write\n    self._fp = open(fname, open_flags)\nFileExistsError: [Errno 17] File exists: '\/project\/6051551\/prsood\/multi-modal-emotion\/TripleModels\/run_slurm\/wandb\/run-20230203_003340-6fnujhey\/run-6fnujhey.wandb'\nThread WriterThread:\nTraceback (most recent call last):\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 50, in run\n    self._run()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 101, in _run\n    self._process(record)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 351, in _process\n    self._wm.write(record)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/writer.py\", line 28, in write\n    self.open()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/writer.py\", line 24, in open\n    self._ds.open_for_write(self._settings.sync_file)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/datastore.py\", line 77, in open_for_write\n    self._fp = open(fname, open_flags)\nFileExistsError: [Errno 17] File exists: '\/project\/6051551\/prsood\/multi-modal-emotion\/TripleModels\/run_slurm\/wandb\/run-20230203_003340-6fnujhey\/run-6fnujhey.wandb'\nThread WriterThread:\nTraceback (most recent call last):\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 50, in run\n    self._run()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal_util.py\", line 101, in _run\n    self._process(record)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/internal.py\", line 351, in _process\n    self._wm.write(record)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/writer.py\", line 28, in write\n    self.open()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/writer.py\", line 24, in open\n    self._ds.open_for_write(self._settings.sync_file)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/internal\/datastore.py\", line 77, in open_for_write\n    self._fp = open(fname, open_flags)\nFileExistsError: [Errno 17] File exists: '\/project\/6051551\/prsood\/multi-modal-emotion\/TripleModels\/run_slurm\/wandb\/run-20230203_003340-6fnujhey\/run-6fnujhey.wandb'\nwandb: ERROR Internal wandb error: file data was not synced\nProblem at: ..\/tav_nn.py 106 main...\nTraceback (most recent call last):\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1078, in init\n    run = wi.init()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 739, in init\n    _ = backend.interface.communicate_run_start(run_obj)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 235, in communicate_run_start\n    result = self._communicate_run_start(run_start)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 484, in _communicate_run_start\n    result = self._communicate(rec)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 255, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 58, in _communicate_async\n    future = self._router.send_and_receive(rec, local=local)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/router.py\", line 94, in send_and_receive\n    self._send_message(rec)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/router_sock.py\", line 36, in _send_message\n    self._sock_client.send_record_communicate(record)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 216, in send_record_communicate\n    self.send_server_request(server_req)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nBrokenPipeError: [Errno 32] Broken pipe\nwandb: ERROR Abnormal program exit..\nTraceback (most recent call last):\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1078, in init\n    run = wi.init()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 739, in init\n    _ = backend.interface.communicate_run_start(run_obj)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 235, in communicate_run_start\n    result = self._communicate_run_start(run_start)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 484, in _communicate_run_start\n    result = self._communicate(rec)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 255, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/interface_sock.py\", line 58, in _communicate_async\n    future = self._router.send_and_receive(rec, local=local)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/router.py\", line 94, in send_and_receive\n    self._send_message(rec)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/interface\/router_sock.py\", line 36, in _send_message\n    self._sock_client.send_record_communicate(record)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 216, in send_record_communicate\n    self.send_server_request(server_req)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nBrokenPipeError: [Errno 32] Broken pipe\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"..\/tav_nn.py\", line 183, in &lt;module&gt;\n    main()\n  File \"..\/tav_nn.py\", line 106, in main\n    run = wandb.init(project=project_name, entity=\"ddi\" , config = args)\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1116, in init\n    raise Exception(\"problem\") from error_seen\nException: problem\nProblem at:Problem at:Problem at: ..\/tav_nn.py 106 main\n ..\/tav_nn.py  106..\/tav_nn.py  main106\n main\nTraceback (most recent call last):\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1078, in init\n    run = wi.init()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 698, in init\n    timeout=self.settings.init_timeout, on_progress=self._on_progress_init\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/mailbox.py\", line 259, in wait\n    raise MailboxError(\"transport failed\")\nwandb.errors.MailboxError: transport failed\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1078, in init\n    run = wi.init()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1078, in init\n    run = wi.init()\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 698, in init\n    timeout=self.settings.init_timeout, on_progress=self._on_progress_init\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/mailbox.py\", line 259, in wait\n    raise MailboxError(\"transport failed\")\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/wandb_init.py\", line 698, in init\n    timeout=self.settings.init_timeout, on_progress=self._on_progress_init\nwandb.errors.MailboxError: transport failed\n  File \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/wandb\/sdk\/lib\/mailbox.py\", line 259, in wait\n    raise MailboxError(\"transport failed\")\nwandb.errors.MailboxError: transport failed\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 213168 closing signal SIGTERM\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 213170 closing signal SIGTERM\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 213171 closing signal SIGTERM\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 213169) of binary: \/project\/6051551\/prsood\/sarcasm_venv\/bin\/python3\n2023-02-03 00:33:54,480 - wandb.wandb_agent - INFO - Cleaning up finished run: 6fnujhey\n<\/code><\/pre>\n<p>accelerate config file:<\/p>\n<pre><code class=\"lang-bash\">- `Accelerate` version: 0.16.0\n- Platform: Linux-3.10.0-1160.80.1.el7.x86_64-x86_64-with-centos-7.9.2009-Core\n- Python version: 3.7.7\n- Numpy version: 1.21.4\n- PyTorch version (GPU?): 1.10.0 (False)\n- `Accelerate` default config:\n        - compute_environment: LOCAL_MACHINE\n        - distributed_type: FSDP\n        - mixed_precision: no\n        - use_cpu: False\n        - dynamo_backend: NO\n        - num_processes: 4\n        - machine_rank: 0\n        - num_machines: 1\n        - rdzv_backend: static\n        - same_network: True\n        - main_training_function: main\n        - deepspeed_config: {}\n        - fsdp_config: {'fsdp_auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'fsdp_backward_prefetch_policy': 'BACKWARD_PRE', 'fsdp_offload_params': True, 'fsdp_sharding_strategy': 2, 'fsdp_state_dict_type': 'SHARDED_STATE_DICT', 'fsdp_transformer_layer_cls_to_wrap': 'TransformerBlock'}\n        - megatron_lm_config: {}\n        - downcast_bf16: no\n<\/code><\/pre>\n<p>My initial python script that it can\u2019t get past<\/p>\n<pre><code class=\"lang-python\">def main():\n    \n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n    project_name = \"MLP_test_text\"\n    args = arg_parse(project_name)\n    run = wandb.init(project=project_name, entity=\"ddi\" , config = args)\n    config = wandb.config\n    np.random.seed(config.seed)\n    torch.random.manual_seed(config.seed)\n\n    \n\n    param_dict = {\n        'epoch':config.epoch ,\n        'patience':config.patience ,\n        'lr': config.learning_rate ,\n        'clip': config.clip ,\n        'batch_size':8,#config.batch_size ,\n        'weight_decay':config.weight_decay ,\n        'model': config.model,\n        'T_max':config.T_max ,\n        'seed':config.seed,\n        'label_task':config.label_task,\n    }\n\n    df = pd.read_pickle(f\"{args.dataset}.pkl\")\n    if param_dict['label_task'] == \"sentiment\":\n        number_index = \"sentiment\"\n        label_index = \"sentiment_label\"\n    else:\n        number_index = \"emotion\"\n        label_index = \"emotion_label\"\n\n\n    df_train = df[df['split'] == \"train\"] \n    df_test = df[df['split'] == \"test\"] \n    df_val = df[df['split'] == \"val\"] \n\n\n        \n    df = df[~df['timings'].isna()] # Still seeing what the best configuration is for these\n\n    \"\"\"\n    Due to data imbalance we are going to reweigh our CrossEntropyLoss\n    To do this we calculate 1 - (num_class\/len(df)) the rest of the functions are just to order them properly and then convert to a tensor\n    \"\"\"\n    \n    \n    weights = torch.Tensor(list(dict(sorted((dict(1 - (df[number_index].value_counts()\/len(df))).items()))).values()))\n    label2id = df.drop_duplicates(label_index).set_index(label_index).to_dict()[number_index]\n    id2label = {v: k for k, v in label2id.items()}\n\n    model_param = {\n        'output_dim':len(weights) ,\n        'dropout' : config.dropout,\n        'early_div' : config.early_div\n    }\n    \n    param_dict['weights'] = weights\n    param_dict['label2id'] = label2id\n    param_dict['id2label'] = id2label\n\n    print(f\" in main \\n param_dict = {param_dict} \\n model_param = {model_param} \\n df {args.dataset} , with df_size = {len(df)} \\n \")\n    \n    world_size = torch.cuda.device_count()\n    print(f\"world_size = {world_size}\" , flush=True)\n   \n    runModel(\"cuda\", world_size ,df_train , df_val , df_test ,param_dict , model_param , run )\n    \nif __name__ == '__main__':\n    main()\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1675414676045,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":55.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/accelerate-launch-and-wandb-agent-run-the-main-function-4-seperate-times-for-4-gpus\/3809",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-08T11:01:31.653Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/prsood\">@prsood<\/a>, thanks for reporting this and for the detailed explanation! As discussed <a href=\"https:\/\/discuss.huggingface.co\/t\/weights-biases-sweep-with-multi-gpu-accelerate-launch\/26417\/2\" rel=\"noopener nofollow ugc\">here<\/a>, it seems you need to add <code>if main_process:<\/code> to your code  in order to make the distinction between the main process and the rest of them. Could you please try setting this and see if it works? Thanks!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T13:13:59.164Z",
                "Answer_body":"<p>Hi prsood,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: accelerate launch and agent , run the main function 4 seperate times for 4 gpus; Content: hello, so i have changed my yaml file environment variable from python3 to accelerate launch. i am trying to use this in conjunction with agent <username\/proj_name\/sweep_id> on a slurm compute cluster. so the error is that it runs the main function 4 times, which then instantiates the arguments 4 times and we error out because it is trying to create a page that was created on the first step. and then inevitably fails. i should mention that the script does work with just python3 and so it is a matter of using accelerate launch to take advantage of my multiple gpus. #!\/bin\/bash #sbatch --job-name=tav_mae # give job a name #sbatch --time 02-20:00 # time (dd-hh:mm) #sbatch --nodes=1 #sbatch --gpus-per-node=v100l:4 # request gpu #sbatch --ntasks-per-node=4 #sbatch --cpus-per-task=6 # maximum cpu cores per gpu request: 6 on cedar, 16 on graham. #sbatch --mem=150g # memory per node #sbatch --account=ctb-whkchun # runs it on the dedicated nodes we have #sbatch --output=\/scratch\/prsood\/tav_mae\/logs\/%n-%j.out # %n for node name, %j for jobid # remember to mae logs-dir module load stdenv\/2020 module load cuda module load cudnn\/8.0.3 agent ddi\/tavformer2\/ncdfi75j --count 20 yaml configuration file: program: ..\/tav_nn.py command: - ${env} - accelerate - launch - ${program} - \"--dataset\" method: bayes metric: goal: minimize name: train\/train_loss parameters: epoch: values: [5 , 7 , 9] learning_rate: distribution: uniform min: 0.000001 max: 0.0001 batch_size: values: [2 , 4 , 8 , 1] weight_decay: values: [0.0001 , 0.00001 , 0.000001 , 0.0000001, 0.00000001] seed: values: [32, 64, 96] dropout: values: [0.0,0.1,0.2] early_div: values: [true,false] patience: values: [10] clip: values: [1] t_max: values: [5,10] hidden_layers: values: [\"300\"] error output : starting agent 2023-02-03 00:32:20,126 - ._agent - info - running runs: [] 2023-02-03 00:32:21,726 - ._agent - info - agent received command: run 2023-02-03 00:32:21,728 - ._agent - info - agent starting run with config: t_max: 10 batch_size: 2 clip: 1 dropout: 0.1 early_div: true epoch: 7 hidden_layers: 300 label_task: emotion learning_rate: 3.736221739657802e-05 model: mae_encoder patience: 10 seed: 96 weight_decay: 0.0001 2023-02-03 00:32:21,736 - ._agent - info - about to run command: \/usr\/bin\/env accelerate launch ..\/tav_nn.py --dataset ..\/..\/data\/iemocap_df 2023-02-03 00:32:26,772 - ._agent - info - running runs: ['6fnujhey'] : currently logged in as: prsood (ddi). use ` login --relogin` to force relogin : currently logged in as: prsood (ddi). use ` login --relogin` to force relogin : currently logged in as: prsood (ddi). use ` login --relogin` to force relogin : currently logged in as: prsood (ddi). use ` login --relogin` to force relogin : warning ignored .init() arg project when running a sweep. : warning ignored .init() arg entity when running a sweep. : warning ignored .init() arg project when running a sweep. : warning ignored .init() arg entity when running a sweep. : warning ignored .init() arg project when running a sweep. : warning ignored .init() arg entity when running a sweep. : warning ignored .init() arg project when running a sweep. : warning ignored .init() arg entity when running a sweep. thread writerthread: .init()... traceback (most recent call last): file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/internal_util.py\", line 50, in run self._run() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/internal_util.py\", line 101, in _run self._process(record) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/internal.py\", line 351, in _process self._wm.write(record) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/writer.py\", line 28, in write self.open() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/writer.py\", line 24, in open self._ds.open_for_write(self._settings.sync_file) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/datastore.py\", line 77, in open_for_write self._fp = open(fname, open_flags) fileexistserror: [errno 17] file exists: '\/project\/6051551\/prsood\/multi-modal-emotion\/triplemodels\/run_slurm\/\/run-20230203_003340-6fnujhey\/run-6fnujhey.' thread writerthread: traceback (most recent call last): file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/internal_util.py\", line 50, in run self._run() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/internal_util.py\", line 101, in _run self._process(record) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/internal.py\", line 351, in _process self._wm.write(record) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/writer.py\", line 28, in write self.open() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/writer.py\", line 24, in open self._ds.open_for_write(self._settings.sync_file) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/datastore.py\", line 77, in open_for_write self._fp = open(fname, open_flags) fileexistserror: [errno 17] file exists: '\/project\/6051551\/prsood\/multi-modal-emotion\/triplemodels\/run_slurm\/\/run-20230203_003340-6fnujhey\/run-6fnujhey.' thread writerthread: traceback (most recent call last): file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/internal_util.py\", line 50, in run self._run() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/internal_util.py\", line 101, in _run self._process(record) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/internal.py\", line 351, in _process self._wm.write(record) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/writer.py\", line 28, in write self.open() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/writer.py\", line 24, in open self._ds.open_for_write(self._settings.sync_file) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/internal\/datastore.py\", line 77, in open_for_write self._fp = open(fname, open_flags) fileexistserror: [errno 17] file exists: '\/project\/6051551\/prsood\/multi-modal-emotion\/triplemodels\/run_slurm\/\/run-20230203_003340-6fnujhey\/run-6fnujhey.' : error internal error: file data was not synced problem at: ..\/tav_nn.py 106 main... traceback (most recent call last): file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 1078, in init run = wi.init() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 739, in init _ = backend.interface.communicate_run_start(run_obj) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/interface.py\", line 235, in communicate_run_start result = self._communicate_run_start(run_start) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/interface_shared.py\", line 484, in _communicate_run_start result = self._communicate(rec) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/interface_shared.py\", line 255, in _communicate return self._communicate_async(rec, local=local).get(timeout=timeout) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/interface_sock.py\", line 58, in _communicate_async future = self._router.send_and_receive(rec, local=local) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/router.py\", line 94, in send_and_receive self._send_message(rec) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/router_sock.py\", line 36, in _send_message self._sock_client.send_record_communicate(record) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/sock_client.py\", line 216, in send_record_communicate self.send_server_request(server_req) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/sock_client.py\", line 155, in send_server_request self._send_message(msg) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/sock_client.py\", line 152, in _send_message self._sendall_with_error_handle(header + data) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/sock_client.py\", line 130, in _sendall_with_error_handle sent = self._sock.send(data) brokenpipeerror: [errno 32] broken pipe : error abnormal program exit.. traceback (most recent call last): file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 1078, in init run = wi.init() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 739, in init _ = backend.interface.communicate_run_start(run_obj) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/interface.py\", line 235, in communicate_run_start result = self._communicate_run_start(run_start) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/interface_shared.py\", line 484, in _communicate_run_start result = self._communicate(rec) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/interface_shared.py\", line 255, in _communicate return self._communicate_async(rec, local=local).get(timeout=timeout) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/interface_sock.py\", line 58, in _communicate_async future = self._router.send_and_receive(rec, local=local) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/router.py\", line 94, in send_and_receive self._send_message(rec) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/interface\/router_sock.py\", line 36, in _send_message self._sock_client.send_record_communicate(record) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/sock_client.py\", line 216, in send_record_communicate self.send_server_request(server_req) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/sock_client.py\", line 155, in send_server_request self._send_message(msg) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/sock_client.py\", line 152, in _send_message self._sendall_with_error_handle(header + data) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/sock_client.py\", line 130, in _sendall_with_error_handle sent = self._sock.send(data) brokenpipeerror: [errno 32] broken pipe the above exception was the direct cause of the following exception: traceback (most recent call last): file \"..\/tav_nn.py\", line 183, in <module> main() file \"..\/tav_nn.py\", line 106, in main run = .init(project=project_name, entity=\"ddi\" , config = args) file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 1116, in init raise exception(\"problem\") from error_seen exception: problem problem at:problem at:problem at: ..\/tav_nn.py 106 main ..\/tav_nn.py 106..\/tav_nn.py main106 main traceback (most recent call last): file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 1078, in init run = wi.init() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 698, in init timeout=self.settings.init_timeout, on_progress=self._on_progress_init file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/mailbox.py\", line 259, in wait raise mailboxerror(\"transport failed\") .errors.mailboxerror: transport failed traceback (most recent call last): traceback (most recent call last): file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 1078, in init run = wi.init() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 1078, in init run = wi.init() file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 698, in init timeout=self.settings.init_timeout, on_progress=self._on_progress_init file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/mailbox.py\", line 259, in wait raise mailboxerror(\"transport failed\") file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/_init.py\", line 698, in init timeout=self.settings.init_timeout, on_progress=self._on_progress_init .errors.mailboxerror: transport failed file \"\/project\/6051551\/prsood\/sarcasm_venv\/lib\/python3.7\/site-packages\/\/sdk\/lib\/mailbox.py\", line 259, in wait raise mailboxerror(\"transport failed\") .errors.mailboxerror: transport failed warning:torch.distributed.elastic.multiprocessing.api:sending process 213168 closing signal sigterm warning:torch.distributed.elastic.multiprocessing.api:sending process 213170 closing signal sigterm warning:torch.distributed.elastic.multiprocessing.api:sending process 213171 closing signal sigterm error:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 213169) of binary: \/project\/6051551\/prsood\/sarcasm_venv\/bin\/python3 2023-02-03 00:33:54,480 - ._agent - info - cleaning up finished run: 6fnujhey accelerate config file: - `accelerate` version: 0.16.0 - platform: linux-3.10.0-1160.80.1.el7.x86_64-x86_64-with-centos-7.9.2009-core - python version: 3.7.7 - numpy version: 1.21.4 - pytorch version (gpu?): 1.10.0 (false) - `accelerate` default config: - compute_environment: local_machine - distributed_type: fsdp - mixed_precision: no - use_cpu: false - dynamo_backend: no - num_processes: 4 - machine_rank: 0 - num_machines: 1 - rdzv_backend: static - same_network: true - main_training_function: main - deepspeed_config: {} - fsdp_config: {'fsdp_auto_wrap_policy': 'transformer_based_wrap', 'fsdp_backward_prefetch_policy': 'backward_pre', 'fsdp_offload_params': true, 'fsdp_sharding_strategy': 2, 'fsdp_state_dict_type': 'sharded_state_dict', 'fsdp_transformer_layer_cls_to_wrap': 'transformerblock'} - megatron_lm_config: {} - downcast_bf16: no my initial python script that it cant get past def main(): os.environ[\"tokenizers_parallelism\"] = \"true\" project_name = \"mlp_test_text\" args = arg_parse(project_name) run = .init(project=project_name, entity=\"ddi\" , config = args) config = .config np.random.seed(config.seed) torch.random.manual_seed(config.seed) param_dict = { 'epoch':config.epoch , 'patience':config.patience , 'lr': config.learning_rate , 'clip': config.clip , 'batch_size':8,#config.batch_size , 'weight_decay':config.weight_decay , 'model': config.model, 't_max':config.t_max , 'seed':config.seed, 'label_task':config.label_task, } df = pd.read_pickle(f\"{args.dataset}.pkl\") if param_dict['label_task'] == \"sentiment\": number_index = \"sentiment\" label_index = \"sentiment_label\" else: number_index = \"emotion\" label_index = \"emotion_label\" df_train = df[df['split'] == \"train\"] df_test = df[df['split'] == \"test\"] df_val = df[df['split'] == \"val\"] df = df[~df['timings'].isna()] # still seeing what the best configuration is for these \"\"\" due to data imbalance we are going to reweigh our crossentropyloss to do this we calculate 1 - (num_class\/len(df)) the rest of the functions are just to order them properly and then convert to a tensor \"\"\" weights = torch.tensor(list(dict(sorted((dict(1 - (df[number_index].value_counts()\/len(df))).items()))).values())) label2id = df.drop_duplicates(label_index).set_index(label_index).to_dict()[number_index] id2label = {v: k for k, v in label2id.items()} model_param = { 'output_dim':len(weights) , 'dropout' : config.dropout, 'early_div' : config.early_div } param_dict['weights'] = weights param_dict['label2id'] = label2id param_dict['id2label'] = id2label print(f\" in main \\n param_dict = {param_dict} \\n model_param = {model_param} \\n df {args.dataset} , with df_size = {len(df)} \\n \") world_size = torch.cuda.device_count() print(f\"world_size = {world_size}\" , flush=true) runmodel(\"cuda\", world_size ,df_train , df_val , df_test ,param_dict , model_param , run ) if __name__ == '__main__': main()",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"Title: accelerate launch and agent , run the main function seperate times for gpus; Content: hello, so i have changed my yaml file environment variable from python to accelerate launch. i am trying to use this in conjunction with agent on a slurm compute cluster. so the error is that it runs the main function times, which then instantiates the arguments times and we error out because it is trying to create a page that was created on the first step. and then inevitably fails. i should mention that the script does work with just python and so it is a matter of using accelerate launch to take advantage of my multiple gpus. yaml configuration file error output accelerate config file my initial python script that it cant get past",
        "Answer_original_content":"hi @prsood, thanks for reporting this and for the detailed explanation! as discussed here, it seems you need to add if main_process: to your code in order to make the distinction between the main process and the rest of them. could you please try setting this and see if it works? thanks! hi prsood, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis",
        "Answer_original_content_gpt_summary":"Possible solution: The answer suggests adding \"if main_process\" to the code to differentiate between the main process and the rest of them. The user is requested to try setting this and see if it works. The follow-up message indicates that the support team is waiting for the user's response and is ready to provide further assistance if needed.",
        "Answer_preprocessed_content":"hi thanks for reporting this and for the detailed explanation! as discussed here, it seems you need to add to your code in order to make the distinction between the main process and the rest of them. could you please try setting this and see if it works? thanks! hi prsood, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis"
    },
    {
        "Question_id":40234432.0,
        "Question_title":"Create a model that predicts an event based on other time series events and properties of an object",
        "Question_body":"<p>I have the following data:<\/p>\n\n<ul>\n<li>Identifier of a person<\/li>\n<li>Days in location (starts at 1 and runs until event)<\/li>\n<li>Age of person in months at that time (so this increases as the days in location increase too).<\/li>\n<li>Smoker (boolean), doesn't change over time in our case<\/li>\n<li>Sex, doesn't change over time<\/li>\n<li>Fall (boolean) this is an event that may never happen, or can happen multiple times during the complete period for a certain person<\/li>\n<li>Number of wounds: (this can go from 0 to 8), a wound mostly doesn't heal immediately so it mostly stays open for a certain period of time<\/li>\n<li>Event we want to predict (boolean), only the last row of a person will have value true for this<\/li>\n<\/ul>\n\n<p>I have this data for 1500 people (in total 1500000 records so on average about 1000 records per person). For some people the event I want to predict takes place after a couple of days, for some after 10 years.  For everybody in the dataset the event will take place, so the last record for a certain identifier will always have the event we want to predict as 1.<\/p>\n\n<p>I'm new to this and all the documentation I have found so far doesn't demonstrate time series for multiple persons or objects. When I for example split the data in the machine learning studio, I want to keep records of the same person over time together.<\/p>\n\n<p>Would it be possible to feed the system after the model is trained with new records and for each day that passes it would give the estimate of the event taking place in the next 5 days?<\/p>\n\n<p>Edit: sample data of 2 persons: <a href=\"http:\/\/pastebin.com\/KU4bjKwJ\" rel=\"nofollow\">http:\/\/pastebin.com\/KU4bjKwJ<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1477382328947,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":596.0,
        "Owner_creation_time":1345413556180,
        "Owner_last_access_time":1664014926740,
        "Owner_reputation":1946.0,
        "Owner_up_votes":256.0,
        "Owner_down_votes":7.0,
        "Owner_views":211.0,
        "Answer_body":"<p>sounds like very similar to this sample:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/df7c518dcba7407fb855377339d6589f\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/df7c518dcba7407fb855377339d6589f<\/a><\/p>\n\n<p>Unfortunately there is going to be a bit of R code involved. Yes you should be able to retrain the model with new data.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1477404012396,
        "Answer_score":0.0,
        "Owner_location":"Belgium",
        "Question_last_edit_time":1477400304008,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40234432",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: create a model that predicts an event based on other time series events and properties of an object; Content: i have the following data: identifier of a person days in location (starts at 1 and runs until event) age of person in months at that time (so this increases as the days in location increase too). smoker (boolean), doesn't change over time in our case sex, doesn't change over time fall (boolean) this is an event that may never happen, or can happen multiple times during the complete period for a certain person number of wounds: (this can go from 0 to 8), a wound mostly doesn't heal immediately so it mostly stays open for a certain period of time event we want to predict (boolean), only the last row of a person will have value true for this i have this data for 1500 people (in total 1500000 records so on average about 1000 records per person). for some people the event i want to predict takes place after a couple of days, for some after 10 years. for everybody in the dataset the event will take place, so the last record for a certain identifier will always have the event we want to predict as 1. i'm new to this and all the documentation i have found so far doesn't demonstrate time series for multiple persons or objects. when i for example split the data in the machine learning studio, i want to keep records of the same person over time together. would it be possible to feed the system after the model is trained with new records and for each day that passes it would give the estimate of the event taking place in the next 5 days? edit: sample data of 2 persons: http:\/\/pastebin.com\/ku4bjkwj",
        "Question_original_content_gpt_summary":"The user is attempting to create a model that predicts an event based on other time series events and properties of an object, and is encountering challenges in splitting the data in the machine learning studio while keeping records of the same person over time together.",
        "Question_preprocessed_content":"Title: create a model that predicts an event based on other time series events and properties of an object; Content: i have the following data identifier of a person days in location age of person in months at that time . smoker , doesn't change over time in our case sex, doesn't change over time fall this is an event that may never happen, or can happen multiple times during the complete period for a certain person number of wounds , a wound mostly doesn't heal immediately so it mostly stays open for a certain period of time event we want to predict , only the last row of a person will have value true for this i have this data for people . for some people the event i want to predict takes place after a couple of days, for some after years. for everybody in the dataset the event will take place, so the last record for a certain identifier will always have the event we want to predict as . i'm new to this and all the documentation i have found so far doesn't demonstrate time series for multiple persons or objects. when i for example split the data in the machine learning studio, i want to keep records of the same person over time together. would it be possible to feed the system after the model is trained with new records and for each day that passes it would give the estimate of the event taking place in the next days? edit sample data of persons",
        "Answer_original_content":"sounds like very similar to this sample: https:\/\/gallery.cortanaintelligence.com\/experiment\/df7c518dcba7407fb855377339d6589f unfortunately there is going to be a bit of r code involved. yes you should be able to retrain the model with new data.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Check out the provided sample experiment for guidance.\n- Use R code to address the challenge of splitting the data while keeping records of the same person over time together.\n- Retrain the model with new data.",
        "Answer_preprocessed_content":"sounds like very similar to this sample unfortunately there is going to be a bit of r code involved. yes you should be able to retrain the model with new data."
    },
    {
        "Question_id":null,
        "Question_title":"The azure cli command \"az ml attach folder\" is directly adding .azureml directory to .amlignore , so where to put config.json when using Azure devops pipeline to submit script to aml workspace?",
        "Question_body":"Hello MS team,\n\nI am using Azure devOps pipeline to submit a control script to the Azure-ML workspace. This control script in turn kicks off the Azure-ML pipeline containing pythonscriptsteps and hyperdrive step.\n\nMy directory structure:\n\n.\n\u251c\u2500\u2500\u2500.vscode\n\u251c\u2500\u2500\u2500Automation\n\u251c\u2500\u2500\u2500Build\n\u2514\u2500\u2500\u2500Source\n\u251c\u2500\u2500\u2500.azureml\n\u251c\u2500\u2500\u2500.vscode\n\u251c\u2500\u2500\u2500amlcode\n\u2502 \u251c\u2500\u2500\u2500projectcode\n\u2502 \u2514\u2500\u2500\u2500pycache\n\u251c\u2500\u2500\u2500config\n\u251c\u2500\u2500\u2500Data\n\u251c\u2500\u2500\u2500setup\n\u251c\u2500\u2500\u2500tests\n\u2502 \u251c\u2500\u2500\u2500.pytest_cache\n\u2502 \u2502 \u2514\u2500\u2500\u2500v\n\u2502 \u2502 \u2514\u2500\u2500\u2500cache\n\u2502 \u2514\u2500\u2500\u2500pycache\n\u2514\u2500\u2500\u2500pycache\n\n\n\n\nSo here one of the azure cli task in Azure DevOps pipeline uses:\n\naz ml folder attach -w $(azureml.workspaceName) -g $(azureml.resourceGroup)\n\nThis command attaches my whole directory to the AML workspace and automatically creates \".amlignore\" and \".azureml\" is automatically added to that.\n\nSo it is throwing an authentication error as the config.json() is not found because it is generally put in the path \/.azureml.\n\nWhere to put the config.json() then? What is the best practice?",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1643414978747,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/714713\/the-azure-cli-command-34az-ml-attach-folder34-is-d.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-31T07:16:57.497Z",
                "Answer_score":1,
                "Answer_body":"@ShivapriyaKatta-8600 The command az ml folder attach will create the directories and add the config file to .azureml to ensure the workspace resources are easily accessible. You can lookup the note section of the command for reference.\n\nThis command creates a .azureml subdirectory that contains example runconfig and conda environment files. It also contains a config.json file that is used to communicate with your Azure Machine Learning workspace.\n\n\n\n\nThe authentication error in your case could be because az login command might have been missed which allows the cli to authenticate interactively or service principal or MI and then run rest of the commands. You can try to run this and check if the attach works successfully.\n\nAlso, with the devops pipeline I am not sure if az devops login is required to be run but if the above command fails even after az login authentication you can try az devops login.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-01T05:55:55.68Z",
                "Answer_score":0,
                "Answer_body":"Thank you...Got the issue resolved! The command \"az ml folder attach\" won't override the \".amlignore\" if the user creates it beforehand. So I just created \".amlignore\" how I want it to be and removed \".azureml\" from it.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-01T06:49:04.323Z",
                "Answer_score":0,
                "Answer_body":"Thanks a lot!... I understood what \"az ml folder attach\" is doing (have been reading Azure docs), what I wanted is when I do the following:\n\nSubmit a script using Service principal from Azure git repo (where my code lies), what is the point of using \"az ml folder attach\"?\n\nbecause anyway when I am submitting script from my local PC or from Azure DevOps (without \"az ml folder attach\"), the script runs.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: the azure cli command \"az ml attach folder\" is directly adding . directory to .amlignore , so where to put config.json when using azure devops pipeline to submit script to aml workspace?; Content: hello ms team, i am using azure devops pipeline to submit a control script to the azure-ml workspace. this control script in turn kicks off the azure-ml pipeline containing pythonscriptsteps and hyperdrive step. my directory structure: . \u251c\u2500\u2500\u2500.vscode \u251c\u2500\u2500\u2500automation \u251c\u2500\u2500\u2500build \u2514\u2500\u2500\u2500source \u251c\u2500\u2500\u2500. \u251c\u2500\u2500\u2500.vscode \u251c\u2500\u2500\u2500amlcode \u2502 \u251c\u2500\u2500\u2500projectcode \u2502 \u2514\u2500\u2500\u2500pycache \u251c\u2500\u2500\u2500config \u251c\u2500\u2500\u2500data \u251c\u2500\u2500\u2500setup \u251c\u2500\u2500\u2500tests \u2502 \u251c\u2500\u2500\u2500.pytest_cache \u2502 \u2502 \u2514\u2500\u2500\u2500v \u2502 \u2502 \u2514\u2500\u2500\u2500cache \u2502 \u2514\u2500\u2500\u2500pycache \u2514\u2500\u2500\u2500pycache so here one of the azure cli task in azure devops pipeline uses: az ml folder attach -w $(.workspacename) -g $(.resourcegroup) this command attaches my whole directory to the aml workspace and automatically creates \".amlignore\" and \".\" is automatically added to that. so it is throwing an authentication error as the config.json() is not found because it is generally put in the path \/.. where to put the config.json() then? what is the best practice?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with the Azure CLI command \"az ml attach folder\" directly adding the \". directory\" to the \".amlignore\" file, making it difficult to know where to put the config.json file when using an Azure DevOps pipeline to submit a script to an AML workspace.",
        "Question_preprocessed_content":"Title: the azure cli command az ml attach folder is directly adding . directory to .amlignore , so where to put when using azure devops pipeline to submit script to aml workspace?; Content: hello ms team, i am using azure devops pipeline to submit a control script to the workspace. this control script in turn kicks off the pipeline containing pythonscriptsteps and hyperdrive step. my directory structure . .vscode automation build source . .vscode amlcode projectcode pycache config data setup tests v cache pycache pycache so here one of the azure cli task in azure devops pipeline uses az ml folder attach w g this command attaches my whole directory to the aml workspace and automatically creates and is automatically added to that. so it is throwing an authentication error as the is not found because it is generally put in the path where to put the then? what is the best practice?",
        "Answer_original_content":"@shivapriyakatta-8600 the command az ml folder attach will create the directories and add the config file to . to ensure the workspace resources are easily accessible. you can lookup the note section of the command for reference. this command creates a . subdirectory that contains example runconfig and conda environment files. it also contains a config.json file that is used to communicate with your workspace. the authentication error in your case could be because az login command might have been missed which allows the cli to authenticate interactively or service principal or mi and then run rest of the commands. you can try to run this and check if the attach works successfully. also, with the devops pipeline i am not sure if az devops login is required to be run but if the above command fails even after az login authentication you can try az devops login. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"The solution to the challenge with the Azure CLI command \"az ml attach folder\" is to use the command \"az ml folder attach\" instead, which creates the necessary directories and adds the config file to ensure the workspace resources are easily accessible. The command creates a subdirectory that contains example runconfig and conda environment files, as well as a config.json file that is used to communicate with the workspace. The authentication error could be resolved by running the \"az login\" command before running the rest of the commands. Additionally, if the above command fails even after authentication, the user can try running \"az devops login\".",
        "Answer_preprocessed_content":"the command az ml folder attach will create the directories and add the config file to . to ensure the workspace resources are easily accessible. you can lookup the note section of the command for reference. this command creates a . subdirectory that contains example runconfig and conda environment files. it also contains a file that is used to communicate with your workspace. the authentication error in your case could be because az login command might have been missed which allows the cli to authenticate interactively or service principal or mi and then run rest of the commands. you can try to run this and check if the attach works successfully. also, with the devops pipeline i am not sure if az devops login is required to be run but if the above command fails even after az login authentication you can try az devops login. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":null,
        "Question_title":"Sweep when each experiment consists on 2 trains?",
        "Question_body":"<p>Hi! I need to create a sweep where, for each run I actually need the script to run 2 separate training processes, as I need to pre-train some modules (one call to train.py) and then I need to train the actual model using the pre-trained parts (another call to train.py)<br>\nIs there a way of doing this?<br>\nThanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1676454015003,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":30.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-when-each-experiment-consists-on-2-trains\/3885",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-15T12:28:01.911Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/arcmle\">@arcmle<\/a>, thanks for your question! If I\u2019m understanding you properly, this should be doable by specifying these instructions inside the function you\u2019re passing to the agent like:<\/p>\n<pre><code class=\"lang-auto\">def main():\n  pretrained_model = train()\n  final_model = train(pretrained_model)\nwandb.agent(id, function=main, count=1)\n<\/code><\/pre>\n<p>Please let me know if this would work for you and feel free to ask any questions!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-21T09:45:52.047Z",
                "Answer_body":"<p>Hi arcmle,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sweep when each experiment consists on 2 trains?; Content: hi! i need to create a sweep where, for each run i actually need the script to run 2 separate training processes, as i need to pre-train some modules (one call to train.py) and then i need to train the actual model using the pre-trained parts (another call to train.py) is there a way of doing this? thanks!",
        "Question_original_content_gpt_summary":"The user is looking for a way to run two separate training processes in a sweep for each experiment.",
        "Question_preprocessed_content":"Title: sweep when each experiment consists on trains?; Content: hi! i need to create a sweep where, for each run i actually need the script to run separate training processes, as i need to some modules and then i need to train the actual model using the parts is there a way of doing this? thanks!",
        "Answer_original_content":"hi @arcmle, thanks for your question! if im understanding you properly, this should be doable by specifying these instructions inside the function youre passing to the agent like: def main(): pretrained_model = train() final_model = train(pretrained_model) .agent(id, function=main, count=1) please let me know if this would work for you and feel free to ask any questions! hi arcmle, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis",
        "Answer_original_content_gpt_summary":"The answer suggests that the user can specify instructions inside the function they are passing to the agent to run two separate training processes in a sweep for each experiment. The example given is to define a function with two training processes, one using a pretrained model and the other using the final model from the first process. The answer also includes a follow-up message asking if the user needs further assistance.",
        "Answer_preprocessed_content":"hi thanks for your question! if im understanding you properly, this should be doable by specifying these instructions inside the function youre passing to the agent like please let me know if this would work for you and feel free to ask any questions! hi arcmle, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis"
    },
    {
        "Question_id":null,
        "Question_title":"Attached AKS not available in Azure ML Create Model Deployment UI",
        "Question_body":"I have AKS cluster attached with Azure ML for inference cluster,\nBut when i try to create End point using UI, kubernetes cluster is not available in the drop down menu\nwhy the AKS cluster that attached in Azure ML is not available in the drop down menu?",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1636497254520,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/621129\/attached-aks-not-available-in-azure-ml-create-mode.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-10T04:27:51.993Z",
                "Answer_score":0,
                "Answer_body":"Hi @Edhotp-9431\n\nA machine learning model registered in your workspace.\n\nAzure Machine Learning can deploy trained machine learning models to Azure Kubernetes Service.\nYou must first either create an Azure Kubernetes Service (AKS) cluster from your Azure ML workspace, or attach an existing AKS cluster.\n\nCheck with the limitations\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-kubernetes?tabs=python#limitations\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python\n\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, so that it can help others in the community looking for help on similar topics.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-10T18:27:38.86Z",
                "Answer_score":0,
                "Answer_body":"Hi @Edhotp-9431\n\nThe + Create option on Endpoints UI page, is based off our v2 APIs : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-cli\n\nYou Kubernetes will show up, if you have attached them using the following docs : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-arc-kubernetes?tabs=studio\nThe cluster will show up if you are attaching it the following way:\n\nIf you don't see your cluster after following the above documentation, please let us know!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: attached aks not available in create model deployment ui; Content: i have aks cluster attached with for inference cluster, but when i try to create end point using ui, kubernetes cluster is not available in the drop down menu why the aks cluster that attached in is not available in the drop down menu?",
        "Question_original_content_gpt_summary":"The user is encountering an issue where the AKS cluster they have attached to their inference cluster is not available in the drop down menu when attempting to create an endpoint using the UI.",
        "Question_preprocessed_content":"Title: attached aks not available in create model deployment ui; Content: i have aks cluster attached with for inference cluster, but when i try to create end point using ui, kubernetes cluster is not available in the drop down menu why the aks cluster that attached in is not available in the drop down menu?",
        "Answer_original_content":"hi @edhotp-9431 a machine learning model registered in your workspace. can deploy trained machine learning models to azure kubernetes service. you must first either create an azure kubernetes service (aks) cluster from your workspace, or attach an existing aks cluster. check with the limitations https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-kubernetes?tabs=python#limitations https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python if the answer is helpful, please click accept answer and up-vote, so that it can help others in the community looking for help on similar topics. hi @edhotp-9431 the + create option on endpoints ui page, is based off our v2 apis : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-cli you kubernetes will show up, if you have attached them using the following docs : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-attach-arc-kubernetes?tabs=studio the cluster will show up if you are attaching it the following way: if you don't see your cluster after following the above documentation, please let us know!",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. Create an Azure Kubernetes Service (AKS) cluster from the workspace or attach an existing AKS cluster.\n2. Check the limitations of attaching an AKS cluster to the workspace.\n3. Follow the documentation to attach the AKS cluster to the workspace.\n4. If the cluster is not showing up after following the documentation, inform the support team.\n\nIn summary, the answer provides possible solutions to the issue of the AKS cluster not being available in the drop-down menu when creating an endpoint using the UI.",
        "Answer_preprocessed_content":"hi a machine learning model registered in your workspace. can deploy trained machine learning models to azure kubernetes service. you must first either create an azure kubernetes service cluster from your workspace, or attach an existing aks cluster. check with the limitations if the answer is helpful, please click accept answer and so that it can help others in the community looking for help on similar topics. hi the + create option on endpoints ui page, is based off our v apis you kubernetes will show up, if you have attached them using the following docs the cluster will show up if you are attaching it the following way if you don't see your cluster after following the above documentation, please let us know!"
    },
    {
        "Question_id":67807756.0,
        "Question_title":"ModuleNotFoundError: No module named 'ruamel' when excuting from azureml.core",
        "Question_body":"<p>I am trying to execute the Azure ml sdk from the local system using the Jupyter notebook. When I run the below code i am getting an error.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\n\nModuleNotFoundError: No module named 'ruamel' \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1622646368700,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":332.0,
        "Owner_creation_time":1599816833352,
        "Owner_last_access_time":1636176255430,
        "Owner_reputation":329.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":1.0,
        "Owner_views":58.0,
        "Answer_body":"<p>You have to add pip 20.1.1<\/p>\n<p>Conda ruamel needs higher version of pip<\/p>\n<pre><code>conda install pip=20.1.1\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626743937907,
        "Answer_score":0.0,
        "Owner_location":"New Delhi, Delhi, India",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67807756",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: modulenotfounderror: no module named 'ruamel' when excuting from .core; Content: i am trying to execute the sdk from the local system using the jupyter notebook. when i run the below code i am getting an error. from .core import workspace, datastore, dataset modulenotfounderror: no module named 'ruamel'",
        "Question_original_content_gpt_summary":"The user is encountering a ModuleNotFoundError when attempting to execute the SDK from their local system using the Jupyter Notebook.",
        "Question_preprocessed_content":"Title: modulenotfounderror no module named 'ruamel' when excuting from ; Content: i am trying to execute the sdk from the local system using the jupyter notebook. when i run the below code i am getting an error.",
        "Answer_original_content":"you have to add pip 20.1.1 conda ruamel needs higher version of pip conda install pip=20.1.1",
        "Answer_original_content_gpt_summary":"To resolve the ModuleNotFoundError when executing the SDK from a local system using Jupyter Notebook, the user can try adding pip 20.1.1 conda ruamel and installing a higher version of pip using conda install pip=20.1.1.",
        "Answer_preprocessed_content":"you have to add pip conda ruamel needs higher version of pip"
    },
    {
        "Question_id":null,
        "Question_title":"Rate Limit Exceeded wandb increase rate limit",
        "Question_body":"<p>Hi all, I get the following error in my logs:<\/p>\n<pre><code class=\"lang-auto\">^[[34m^[[1mwandb^[[0m: Network error (HTTPError), entering retry loop.^M                                                                                                                               \n306 ^[[34m^[[1mwandb^[[0m: 429 encountered (Filestream rate limit exceeded, retrying in 36.30758655296977 seconds), retrying request^M    \n<\/code><\/pre>\n<p>It appears i am sending too many requests. would it be possible to increase my rate limit?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1670650066619,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":139.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/rate-limit-exceeded-wandb-increase-rate-limit\/3522",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-08T05:28:14.442Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: rate limit exceeded increase rate limit; Content: hi all, i get the following error in my logs: ^[[34m^[[1m^[[0m: network error (httperror), entering retry loop.^m 306 ^[[34m^[[1m^[[0m: 429 encountered (filestream rate limit exceeded, retrying in 36.30758655296977 seconds), retrying request^m it appears i am sending too many requests. would it be possible to increase my rate limit?",
        "Question_original_content_gpt_summary":"The user is encountering a rate limit exceeded error and is requesting an increase in their rate limit.",
        "Question_preprocessed_content":"Title: rate limit exceeded increase rate limit; Content: hi all, i get the following error in my logs it appears i am sending too many requests. would it be possible to increase my rate limit?",
        "Answer_original_content":"this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer as it is not related to the question. The answer only states that the topic is closed and no new replies are allowed.",
        "Answer_preprocessed_content":"this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":71155959.0,
        "Question_title":"How to merge data (CSV) files from multiple branches (Git and DVC)?",
        "Question_body":"<p><strong>Background<\/strong>: In my projects I'm using GIT and <a href=\"https:\/\/dvc.org\/\" rel=\"nofollow noreferrer\">DVC<\/a> to keep track of versions:<\/p>\n<ul>\n<li>GIT - only for source codes<\/li>\n<li>DVC - for dataset, model objects and outputs<\/li>\n<\/ul>\n<p>I'm testing different approaches in separate branches, i.e:<\/p>\n<ul>\n<li>random_forest<\/li>\n<li>neural_network_1<\/li>\n<li>...<\/li>\n<\/ul>\n<p>Typically as an output I'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). As a consequence in different branches I've different pred_test.csv files. The structure of the file is very simple, it contains two columns:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction<\/li>\n<\/ul>\n<p><strong>Question<\/strong>: What is the best way to merge those prediction files into single big file?<\/p>\n<p>I would like to obtain a file with structure:<\/p>\n<ul>\n<li>ID<\/li>\n<li>Prediction_random_forest<\/li>\n<li>Prediction_neural_network_1<\/li>\n<li>Prediction_...<\/li>\n<\/ul>\n<p>My main issue is how to access files with predictions which are in different branches?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5.0,
        "Question_creation_time":1645092064283,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":274.0,
        "Owner_creation_time":1265742671200,
        "Owner_last_access_time":1658834085252,
        "Owner_reputation":2735.0,
        "Owner_up_votes":190.0,
        "Owner_down_votes":7.0,
        "Owner_views":552.0,
        "Answer_body":"<p>I would try to use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\" rel=\"nofollow noreferrer\"><code>dvc get<\/code><\/a> in this case:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>dvc get -o random_forest_pred.csv --rev random_forest . pred_test.csv\n<\/code><\/pre>\n<p>It should bring the <code>pred_test.csv<\/code> from the <code>random_forest<\/code> branch.<\/p>\n<blockquote>\n<p>Mind the <code>.<\/code> before the <code>pred_test.csv<\/code> please, it's needed and it means that &quot;use the current repo&quot;, since <code>dvc get<\/code> could also be used on other repos (e.g. GitHub URL)<\/p>\n<\/blockquote>\n<p>Then I think you could use some CLI or write a script to join the files:<\/p>\n<p><a href=\"https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file\">https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1645113091963,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":1645132430276,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71155959",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to merge data (csv) files from multiple branches (git and )?; Content: background: in my projects i'm using git and to keep track of versions: git - only for source codes - for dataset, model objects and outputs i'm testing different approaches in separate branches, i.e: random_forest neural_network_1 ... typically as an output i'm keeping predictions in csv file with standarised name (i.e.: pred_test.csv). as a consequence in different branches i've different pred_test.csv files. the structure of the file is very simple, it contains two columns: id prediction question: what is the best way to merge those prediction files into single big file? i would like to obtain a file with structure: id prediction_random_forest prediction_neural_network_1 prediction_... my main issue is how to access files with predictions which are in different branches?",
        "Question_original_content_gpt_summary":"The user is looking for the best way to merge multiple CSV files from different branches in Git and into a single file with the structure of two columns: ID and prediction.",
        "Question_preprocessed_content":"Title: how to merge data files from multiple branches ?; Content: background in my projects i'm using git and to keep track of versions git only for source codes for dataset, model objects and outputs i'm testing different approaches in separate branches, typically as an output i'm keeping predictions in csv file with standarised name . as a consequence in different branches i've different files. the structure of the file is very simple, it contains two columns id prediction question what is the best way to merge those prediction files into single big file? i would like to obtain a file with structure id my main issue is how to access files with predictions which are in different branches?",
        "Answer_original_content":"i would try to use get in this case: get -o random_forest_pred.csv --rev random_forest . pred_test.csv it should bring the pred_test.csv from the random_forest branch. mind the . before the pred_test.csv please, it's needed and it means that \"use the current repo\", since get could also be used on other repos (e.g. github url) then i think you could use some cli or write a script to join the files: https:\/\/unix.stackexchange.com\/questions\/293775\/merging-contents-of-multiple-csv-files-into-single-csv-file",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include using the \"get\" command to retrieve the CSV file from a specific branch in Git, and then using a command-line interface or script to merge the contents of multiple CSV files into a single file.",
        "Answer_preprocessed_content":"i would try to use in this case it should bring the from the branch. mind the before the please, it's needed and it means that use the current repo , since could also be used on other repos then i think you could use some cli or write a script to join the files"
    },
    {
        "Question_id":53717284.0,
        "Question_title":"invalid subscript type 'list' Azure Machine Learning",
        "Question_body":"<p><strong>PROBLEM<\/strong> <\/p>\n\n<p>I deployed my experiments in Azure Machine Learning as a Web Service. The experiments ran without error. <\/p>\n\n<p>But when testing using <code>REQUEST\/RESPONSE<\/code>, I'm getting the error below:<\/p>\n\n<blockquote>\n  <p>Execute R Script Piped (RPackage) : The following error occurred during evaluation of R script: R_tryEval: return error: Error in split(df, list(df$PRO_NAME, df$Illness_Code))[Ind] : invalid subscript type 'list'<\/p>\n<\/blockquote>\n\n<p>This is the code:<\/p>\n\n<pre><code># Loop through the dataframe and apply model\nInd &lt;- sapply(split(df, list(df$PRO_NAME,df$Illness_Code)), \n              function(x)nrow(x)&gt;1)\n\nout &lt;- lapply(\n    split(df, list(df$PRO_NAME, df$Illness_Code))[Ind],\n    function(c){\n        m &lt;- lm(formula = COUNT ~ YEAR, data = c)\n        coef(m)\n    })\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1544501617417,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Owner_creation_time":1467451434136,
        "Owner_last_access_time":1637568204556,
        "Owner_reputation":111.0,
        "Owner_up_votes":18.0,
        "Owner_down_votes":0.0,
        "Owner_views":36.0,
        "Answer_body":"<p><strong>FIXED<\/strong><\/p>\n\n<p><strong>Problem:<\/strong><\/p>\n\n<p>Some R codes don't work if input data is limited (e.g 1-2 rows only)<\/p>\n\n<p><strong>Solution:<\/strong><\/p>\n\n<p>Load data by <code>Batch<\/code> instead of <code>REQUEST\/RESPONSE<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1544578308848,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53717284",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: invalid subscript type 'list' ; Content: problem i deployed my experiments in as a web service. the experiments ran without error. but when testing using request\/response, i'm getting the error below: execute r script piped (rpackage) : the following error occurred during evaluation of r script: r_tryeval: return error: error in split(df, list(df$pro_name, df$illness_code))[ind] : invalid subscript type 'list' this is the code: # loop through the dataframe and apply model ind <- sapply(split(df, list(df$pro_name,df$illness_code)), function(x)nrow(x)>1) out <- lapply( split(df, list(df$pro_name, df$illness_code))[ind], function(c){ m <- lm(formula = count ~ year, data = c) coef(m) })",
        "Question_original_content_gpt_summary":"The user encountered a challenge with their web service deployment where they received an error message of \"invalid subscript type 'list'\" when testing with request\/response.",
        "Question_preprocessed_content":"Title: invalid subscript type 'list' ; Content: problem i deployed my experiments in as a web service. the experiments ran without error. but when testing using , i'm getting the error below execute r script piped the following error occurred during evaluation of r script return error error in split ind invalid subscript type 'list' this is the code",
        "Answer_original_content":"fixed problem: some r codes don't work if input data is limited (e.g 1-2 rows only) solution: load data by batch instead of request\/response",
        "Answer_original_content_gpt_summary":"The solution to the challenge of receiving an error message of \"invalid subscript type 'list'\" when testing a web service deployment with limited input data is to load the data by batch instead of using request\/response.",
        "Answer_preprocessed_content":"fixed problem some r codes don't work if input data is limited solution load data by instead of"
    },
    {
        "Question_id":59599721.0,
        "Question_title":"Is AWS Sage Maker Auto Pilot suitable for NLP?",
        "Question_body":"<p>Is AWS Sage Maker Auto Pilot suitable for NLP?<\/p>\n\n<p>We currently have a tensorflow model that does classification on input of a sequence of URLS (\nWe transform the URLs to Word vec and Char vec to feed it to the model).<\/p>\n\n<p>Looking at Sage Maker Auto Pilot documentation it says that it works on input in tabular form.\nI was wondering if we could use it to for our use case.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1578226347093,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":261.0,
        "Owner_creation_time":1458550179920,
        "Owner_last_access_time":1658058794840,
        "Owner_reputation":383.0,
        "Owner_up_votes":10.0,
        "Owner_down_votes":0.0,
        "Owner_views":19.0,
        "Answer_body":"<p>No. SageMaker AutoPilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. Technically, I guess you could pass embeddings in CSV format, and pray that XGBoost figures them out, but I seriously doubt that this would deliver meaningful results :)<\/p>\n\n<p>Amazon Comprehend does support fully managed custom classification models <a href=\"https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html<\/a>. It may be worth taking a look at it.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1578756147747,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59599721",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is aws sage maker auto pilot suitable for nlp?; Content: is aws sage maker auto pilot suitable for nlp? we currently have a tensorflow model that does classification on input of a sequence of urls ( we transform the urls to word vec and char vec to feed it to the model). looking at sage maker auto pilot documentation it says that it works on input in tabular form. i was wondering if we could use it to for our use case.",
        "Question_original_content_gpt_summary":"The user is wondering if AWS Sage Maker Auto Pilot is suitable for Natural Language Processing (NLP) given that their current model requires input in the form of a sequence of URLs.",
        "Question_preprocessed_content":"Title: is aws sage maker auto pilot suitable for nlp?; Content: is aws sage maker auto pilot suitable for nlp? we currently have a tensorflow model that does classification on input of a sequence of urls . looking at sage maker auto pilot documentation it says that it works on input in tabular form. i was wondering if we could use it to for our use case.",
        "Answer_original_content":"no. autopilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. technically, i guess you could pass embeddings in csv format, and pray that xgboost figures them out, but i seriously doubt that this would deliver meaningful results :) amazon comprehend does support fully managed custom classification models https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html. it may be worth taking a look at it.",
        "Answer_original_content_gpt_summary":"The answer states that AWS Sage Maker Auto Pilot is not suitable for Natural Language Processing (NLP) as it currently only supports classification and regression problems on tabular data. However, the answer suggests using Amazon Comprehend, which supports fully managed custom classification models and may be worth exploring for NLP needs.",
        "Answer_preprocessed_content":"no. autopilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. technically, i guess you could pass embeddings in csv format, and pray that xgboost figures them out, but i seriously doubt that this would deliver meaningful results amazon comprehend does support fully managed custom classification models it may be worth taking a look at it."
    },
    {
        "Question_id":41855344.0,
        "Question_title":"Azure ML: How to save and process CSV files with semicolon as delimiter?",
        "Question_body":"<p>Azure ML support says to me that delimiter must be comma, this would cause too much hassle with data having semicolon as separator and with a lot of commas in the cell values. <\/p>\n\n<p>So how to process semicolon separated CSV files in Azure ML? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1485357971913,
        "Question_favorite_count":0.0,
        "Question_score":5.0,
        "Question_view_count":2908.0,
        "Owner_creation_time":1251372839052,
        "Owner_last_access_time":1653648989307,
        "Owner_reputation":48616.0,
        "Owner_up_votes":1240.0,
        "Owner_down_votes":41.0,
        "Owner_views":3348.0,
        "Answer_body":"<p>Azure ML only accepts the comma <code>,<\/code> separated CSV. Do a little work around.\nOpen your data file using a text editor. (Notepad will do the trick). Find and replace all semicolons with 'tab' (Make it a TSV) and the commas in data values may not occur a problem then. Make sure to define that the input is a TSV; not a CSV. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1485838771543,
        "Answer_score":7.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41855344",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: : how to save and process csv files with semicolon as delimiter?; Content: support says to me that delimiter must be comma, this would cause too much hassle with data having semicolon as separator and with a lot of commas in the cell values. so how to process semicolon separated csv files in ?",
        "Question_original_content_gpt_summary":"The user is facing a challenge of how to save and process CSV files with semicolon as the delimiter instead of the default comma.",
        "Question_preprocessed_content":"Title: how to save and process csv files with semicolon as delimiter?; Content: support says to me that delimiter must be comma, this would cause too much hassle with data having semicolon as separator and with a lot of commas in the cell values. so how to process semicolon separated csv files in ?",
        "Answer_original_content":"only accepts the comma , separated csv. do a little work around. open your data file using a text editor. (notepad will do the trick). find and replace all semicolons with 'tab' (make it a tsv) and the commas in data values may not occur a problem then. make sure to define that the input is a tsv; not a csv.",
        "Answer_original_content_gpt_summary":"The solution to saving and processing CSV files with semicolon as the delimiter is to open the file in a text editor, replace all semicolons with tabs to create a TSV file, and define the input as a TSV instead of a CSV.",
        "Answer_preprocessed_content":"only accepts the comma separated csv. do a little work around. open your data file using a text editor. . find and replace all semicolons with 'tab' and the commas in data values may not occur a problem then. make sure to define that the input is a tsv; not a csv."
    },
    {
        "Question_id":null,
        "Question_title":"Accessing SageMaker Notebooks without accessing the console",
        "Question_body":"Is it possible to access SageMaker Notebooks without accessing the console?\n\nDo we have a best practice for that? In the create-presigned-notebook-instance-url command, what is the --session-expiration-duration-in-seconds: is it the validity duration of the URL or the max session duration once the URL has been clicked?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1543947347000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":240.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp9lMw9-ESm-27BWY_RgCSg\/accessing-sage-maker-notebooks-without-accessing-the-console",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2018-12-04T19:28:56.000Z",
                "Answer_score":0,
                "Answer_body":"I have experimented with CreatePresignedNotebookInstanceUrl a number of times. It returns an \"AuthorizedUrl\" string in the form: https:\/\/<notebook_instance_name>.notebook.<region>.sagemaker.aws?authToken=<a_very_long_string>\n\nI used the URL in another browser with no AWS console's session cookies (not logged in to the console) and it worked (could access my notebooks).\n\nThe parameter SessionExpirationDurationInSeconds is... well, exactly what it says, The number of seconds the presigned url is valid for. The API accepts a range of [1800, 43200] in seconds, which is equivalent to : 30 minutes to 12 hours .\n\nI hope this helps",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: accessing notebooks without accessing the console; Content: is it possible to access notebooks without accessing the console? do we have a best practice for that? in the create-presigned-notebook-instance-url command, what is the --session-expiration-duration-in-seconds: is it the validity duration of the url or the max session duration once the url has been clicked?",
        "Question_original_content_gpt_summary":"The user is looking for a best practice to access notebooks without accessing the console, and is inquiring about the validity duration of the URL created by the create-presigned-notebook-instance-url command.",
        "Question_preprocessed_content":"Title: accessing notebooks without accessing the console; Content: is it possible to access notebooks without accessing the console? do we have a best practice for that? in the command, what is the is it the validity duration of the url or the max session duration once the url has been clicked?",
        "Answer_original_content":"i have experimented with createpresignednotebookinstanceurl a number of times. it returns an \"authorizedurl\" string in the form: https:\/\/.notebook...aws?authtoken= i used the url in another browser with no aws console's session cookies (not logged in to the console) and it worked (could access my notebooks). the parameter sessionexpirationdurationinseconds is... well, exactly what it says, the number of seconds the presigned url is valid for. the api accepts a range of [1800, 43200] in seconds, which is equivalent to : 30 minutes to 12 hours . i hope this helps",
        "Answer_original_content_gpt_summary":"The answer suggests that the user can use the \"create-presigned-notebook-instance-url\" command to generate an authorized URL to access notebooks without accessing the console. The URL has a validity duration that can be set using the \"sessionexpirationdurationinseconds\" parameter, which can be set between 30 minutes to 12 hours.",
        "Answer_preprocessed_content":"i have experimented with createpresignednotebookinstanceurl a number of times. it returns an authorizedurl string in the form i used the url in another browser with no aws console's session cookies and it worked . the parameter sessionexpirationdurationinseconds well, exactly what it says, the number of seconds the presigned url is valid for. the api accepts a range of in seconds, which is equivalent to minutes to hours . i hope this helps"
    },
    {
        "Question_id":73113256.0,
        "Question_title":"Hyperparameter data types and scales not being validated",
        "Question_body":"<p>On past week, I was implementing some code to <a href=\"https:\/\/github.com\/explosion\/spaCy\/discussions\/11126#discussioncomment-3191163\" rel=\"nofollow noreferrer\">tune hyperparameters on a spaCy model, using Vertex AI<\/a>. From that experience, I have several questions, but since they might no be directly related to each other, I decided to open one case per each question.<\/p>\n<p>In this case, I would like to understand what is exactly going on, when I set the following hyperparameters, in some HP tuning job:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4C78.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w4C78.png\" alt=\"hyperparameters\" \/><\/a><\/p>\n<p>Notice <strong>both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't<\/strong> (UPDATE: at least with my current understanding of the docs). I have the sensation that <em>&quot;Vertex AI does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make ANY sense&quot;<\/em>. Allow me to insert a couple of comments on each example:<\/p>\n<ul>\n<li><code>dropout<\/code>: This variable should be <em>&quot;scaled linearly between 0 and 1&quot;<\/em> ... However what I can see in the HP tuning jobs, are values <em>&quot;scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5&quot;<\/em>. Now this reasoning is a bit naive, as I am not 100% sure if <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\" rel=\"nofollow noreferrer\">this algorithm<\/a> had to do in the values selection, or <em>&quot;Google Console understood I only had the interval [0.1,0.3] to choose values from&quot;<\/em>. (UPDATE) Plus, how can a variable be &quot;discrete and linear&quot; at the same time?<\/li>\n<li><code>batch_size<\/code>: I think I know what's going on with this one, I just want to confirm: 3 categorical values (&quot;500&quot;, &quot;1000&quot; &amp; &quot;2000&quot;) are being selected &quot;as they are&quot;, since they have a SHP of &quot;UNESPECIFIED&quot;.<\/li>\n<\/ul>\n<p>(*) Notice both the HP names, as well as their values, were just &quot;examples on the spot&quot;, they don't intend to be &quot;good starting points&quot;. HP tuning initial values selection is NOT the point of this query.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7.0,
        "Question_creation_time":1658770653850,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":133.0,
        "Owner_creation_time":1629385138956,
        "Owner_last_access_time":1663953209400,
        "Owner_reputation":395.0,
        "Owner_up_votes":18.0,
        "Owner_down_votes":4.0,
        "Owner_views":38.0,
        "Answer_body":"<p>If the type is Categorical, then the scale type is irrelevant and ignored. If the type is DoubleValueSpec, IntegerValueSpec, or DiscreteValueSpec, then the scale type will govern which values are picked more often.<\/p>\n<p>Regarding how a variable can be both Discrete and have a scale: Discrete variables are still numeric in nature. For example, if the discrete values are <code>[1, 10, 100]<\/code>, the ScaleType will determine whether the optimization algorithm considers &quot;distance&quot; between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).<\/p>",
        "Answer_comment_count":6.0,
        "Answer_creation_time":1660669810152,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1660741967556,
        "Answer_last_edit_time":1660850228732,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73113256",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: hyperparameter data types and scales not being validated; Content: on past week, i was implementing some code to tune hyperparameters on a spacy model, using . from that experience, i have several questions, but since they might no be directly related to each other, i decided to open one case per each question. in this case, i would like to understand what is exactly going on, when i set the following hyperparameters, in some hp tuning job: notice both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't (update: at least with my current understanding of the docs). i have the sensation that \" does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make any sense\". allow me to insert a couple of comments on each example: dropout: this variable should be \"scaled linearly between 0 and 1\" ... however what i can see in the hp tuning jobs, are values \"scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5\". now this reasoning is a bit naive, as i am not 100% sure if this algorithm had to do in the values selection, or \"google console understood i only had the interval [0.1,0.3] to choose values from\". (update) plus, how can a variable be \"discrete and linear\" at the same time? batch_size: i think i know what's going on with this one, i just want to confirm: 3 categorical values (\"500\", \"1000\" & \"2000\") are being selected \"as they are\", since they have a shp of \"unespecified\". (*) notice both the hp names, as well as their values, were just \"examples on the spot\", they don't intend to be \"good starting points\". hp tuning initial values selection is not the point of this query. thank you.",
        "Question_original_content_gpt_summary":"The user encountered challenges with hyperparameter data types and scales not being validated when tuning a spacy model, leading to confusion about the values being selected and how they are being interpreted.",
        "Question_preprocessed_content":"Title: hyperparameter data types and scales not being validated; Content: on past week, i was implementing some code to tune hyperparameters on a spacy model, using . from that experience, i have several questions, but since they might no be directly related to each other, i decided to open one case per each question. in this case, i would like to understand what is exactly going on, when i set the following hyperparameters, in some hp tuning job notice both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't . i have the sensation that does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make any sense . allow me to insert a couple of comments on each example this variable should be scaled linearly between and however what i can see in the hp tuning jobs, are values scaled linearly between and and nothing in the interval to now this reasoning is a bit naive, as i am not % sure if this algorithm had to do in the values selection, or google console understood i only had the interval to choose values from . plus, how can a variable be discrete and linear at the same time? i think i know what's going on with this one, i just want to confirm categorical values are being selected as they are , since they have a shp of unespecified . notice both the hp names, as well as their values, were just examples on the spot , they don't intend to be good starting points . hp tuning initial values selection is not the point of this query. thank you.",
        "Answer_original_content":"if the type is categorical, then the scale type is irrelevant and ignored. if the type is doublevaluespec, integervaluespec, or discretevaluespec, then the scale type will govern which values are picked more often. regarding how a variable can be both discrete and have a scale: discrete variables are still numeric in nature. for example, if the discrete values are [1, 10, 100], the scaletype will determine whether the optimization algorithm considers \"distance\" between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).",
        "Answer_original_content_gpt_summary":"The answer suggests that if the type of hyperparameter is categorical, then the scale type is irrelevant. However, if the type is doublevaluespec, integervaluespec, or discretevaluespec, then the scale type will determine which values are picked more often. Additionally, the answer clarifies that discrete variables are still numeric in nature and the scaletype will determine how the optimization algorithm considers the distance between the values.",
        "Answer_preprocessed_content":"if the type is categorical, then the scale type is irrelevant and ignored. if the type is doublevaluespec, integervaluespec, or discretevaluespec, then the scale type will govern which values are picked more often. regarding how a variable can be both discrete and have a scale discrete variables are still numeric in nature. for example, if the discrete values are , the scaletype will determine whether the optimization algorithm considers distance between and versus and the same or smaller ."
    },
    {
        "Question_id":null,
        "Question_title":"Bug in WandB's Keras callback when specifying args",
        "Question_body":"<p>Hi, I am using WandB\u2019s Keras callback in <code>autokeras<\/code> (which uses <code>keras-tuner<\/code> behind the scenes along with other modules). However, if I specify any args things immediately grind down to a halt.<\/p>\n<p>For reproduction, using the official <a href=\"https:\/\/autokeras.com\/tutorial\/image_regression\/\" rel=\"noopener nofollow ugc\">example<\/a> would be adequate.<\/p>\n<p>The problem for me is here,<\/p>\n<pre><code class=\"lang-auto\">WandbCB = WandbCallback(\n    monitor=\"val_loss\", verbose=0, mode=\"min\",\n    log_weights=(True), save_model=(True),\n    validation_data=validation,\n    predictions=5, generator=validation, input_type='images', output_type='images',\n    log_evaluation=(True), validation_steps=None, class_colors=None,\n)\n\nmodel.fit(x=training, validation_data=validation, batch_size=BATCH_SIZE, shuffle=True, callbacks=[WandCB, EStop])\n<\/code><\/pre>\n<p>However, if I specify WandB callback <em>without<\/em> any args,<\/p>\n<pre><code class=\"lang-auto\">model.fit(x=training, validation_data=validation, batch_size=BATCH_SIZE, shuffle=True, callbacks=[WandbCallback(), EStop])\n<\/code><\/pre>\n<p>it works very well.<\/p>\n<p>This is the error I am getting in the former case,<\/p>\n<pre><code class=\"lang-auto\">2021-12-31 19:18:38.582746: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n2021-12-31 19:18:38.583261: I tensorflow\/core\/platform\/profile_utils\/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz\nTraceback (most recent call last):\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/site-packages\/keras_tuner\/engine\/tuner.py\", line 287, in _deepcopy_callbacks\n    callbacks = copy.deepcopy(callbacks)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 216, in _deepcopy_list\n    append(deepcopy(a, memo))\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy\n    y = _reconstruct(x, memo, *rv)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct\n    state = deepcopy(state, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy\n    y = copier(x, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 169, in deepcopy\n    rv = reductor(4)\nTypeError: can't pickle _thread.RLock objects\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"nas.py\", line 125, in &lt;module&gt;\n    log_evaluation=True, validation_steps=None, class_colors=None), EStop])\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/site-packages\/autokeras\/auto_model.py\", line 291, in fit\n    **kwargs\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/site-packages\/autokeras\/engine\/tuner.py\", line 175, in search\n    new_callbacks = self._deepcopy_callbacks(callbacks)\n  File \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/site-packages\/keras_tuner\/engine\/tuner.py\", line 293, in _deepcopy_callbacks\n    \"It is not possible to do `copy.deepcopy(%s)`\" % (callbacks,)\nValueError: All callbacks used during a search should be deep-copyable (since they are reused across trials). It is not possible to do `copy.deepcopy([&lt;wandb.integration.keras.keras.WandbCallback object at 0x7f399310b690&gt;, &lt;tensorflow.python.keras.callbacks.EarlyStopping object at 0x7f39918b1710&gt;])`\n\nwandb: Waiting for W&amp;B process to finish, PID 6373... (failed 1). Press ctrl-c to abort syncing.\n<\/code><\/pre>\n<p>Does anyone have any idea?<\/p>\n<p><strong>EDIT-<\/strong> This is the <code>EarlyStopping<\/code> snippet<\/p>\n<pre><code class=\"lang-auto\">EStop = tf.keras.callbacks.EarlyStopping(\n    monitor='MAPEMetric', min_delta=2, patience=3, verbose=0, mode=\"min\", baseline=100, #baseline is 100\n    restore_best_weights=True)\n<\/code><\/pre>\n<p>The difference is perhaps the metrics being monitored in both - MAPEMetric is a simple custom TF metric that computes the \u201cMean Absolute Percentage Error\u201d;<\/p>\n<pre><code class=\"lang-auto\">def MAPEMetric(target, output):\n        return tf.math.reduce_mean(tf.math.abs((output - target) \/ output)) * 100   #100 for %\n<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1640978828765,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":210.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/bug-in-wandbs-keras-callback-when-specifying-args\/1651",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-04T20:37:33.261Z",
                "Answer_body":"<p>Hello Neel,<\/p>\n<p>Unfortunately I was not able to reproduce your issue. Could you possibly send a minimal reproduction in a colab?<\/p>\n<p>Thanks,<\/p>\n<p>Nate<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-11T21:07:37.271Z",
                "Answer_body":"<p>Hi Neel,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Thanks,<\/p>\n<p>Nate<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-19T18:03:06.471Z",
                "Answer_body":"<p>Hi Neel, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-01T19:27:27.833Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: bug in 's keras callback when specifying args; Content: hi, i am using \u2019s keras callback in autokeras (which uses keras-tuner behind the scenes along with other modules). however, if i specify any args things immediately grind down to a halt. for reproduction, using the official example would be adequate. the problem for me is here, cb = callback( monitor=\"val_loss\", verbose=0, mode=\"min\", log_weights=(true), save_model=(true), validation_data=validation, predictions=5, generator=validation, input_type='images', output_type='images', log_evaluation=(true), validation_steps=none, class_colors=none, ) model.fit(x=training, validation_data=validation, batch_size=batch_size, shuffle=true, callbacks=[wandcb, estop]) however, if i specify callback without any args, model.fit(x=training, validation_data=validation, batch_size=batch_size, shuffle=true, callbacks=[callback(), estop]) it works very well. this is the error i am getting in the former case, 2021-12-31 19:18:38.582746: i tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:176] none of the mlir optimization passes are enabled (registered 2) 2021-12-31 19:18:38.583261: i tensorflow\/core\/platform\/profile_utils\/cpu_utils.cc:114] cpu frequency: 2199995000 hz traceback (most recent call last): file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/site-packages\/keras_tuner\/engine\/tuner.py\", line 287, in _deepcopy_callbacks callbacks = copy.deepcopy(callbacks) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy y = copier(x, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 216, in _deepcopy_list append(deepcopy(a, memo)) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy y = _reconstruct(x, memo, *rv) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct state = deepcopy(state, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy y = copier(x, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy y = _reconstruct(x, memo, *rv) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct state = deepcopy(state, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy y = copier(x, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy y = _reconstruct(x, memo, *rv) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct state = deepcopy(state, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy y = copier(x, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy y = _reconstruct(x, memo, *rv) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct state = deepcopy(state, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy y = copier(x, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 180, in deepcopy y = _reconstruct(x, memo, *rv) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 281, in _reconstruct state = deepcopy(state, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 150, in deepcopy y = copier(x, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 241, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/copy.py\", line 169, in deepcopy rv = reductor(4) typeerror: can't pickle _thread.rlock objects during handling of the above exception, another exception occurred: traceback (most recent call last): file \"nas.py\", line 125, in <module> log_evaluation=true, validation_steps=none, class_colors=none), estop]) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/site-packages\/autokeras\/auto_model.py\", line 291, in fit **kwargs file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/site-packages\/autokeras\/engine\/tuner.py\", line 175, in search new_callbacks = self._deepcopy_callbacks(callbacks) file \"\/usr\/local\/envs\/ak_env\/lib\/python3.7\/site-packages\/keras_tuner\/engine\/tuner.py\", line 293, in _deepcopy_callbacks \"it is not possible to do `copy.deepcopy(%s)`\" % (callbacks,) valueerror: all callbacks used during a search should be deep-copyable (since they are reused across trials). it is not possible to do `copy.deepcopy([<.integration.keras.keras.callback object at 0x7f399310b690>, <tensorflow.python.keras.callbacks.earlystopping object at 0x7f39918b1710>])` : waiting for w&b process to finish, pid 6373... (failed 1). press ctrl-c to abort syncing. does anyone have any idea? edit- this is the earlystopping snippet estop = tf.keras.callbacks.earlystopping( monitor='mapemetric', min_delta=2, patience=3, verbose=0, mode=\"min\", baseline=100, #baseline is 100 restore_best_weights=true) the difference is perhaps the metrics being monitored in both - mapemetric is a simple custom tf metric that computes the \u201cmean absolute percentage error\u201d; def mapemetric(target, output): return tf.math.reduce_mean(tf.math.abs((output - target) \/ output)) * 100 #100 for %",
        "Question_original_content_gpt_summary":"The user is encountering a bug when specifying args in a keras callback when using autokeras, which is causing the model to grind to a halt and throw an error.",
        "Question_preprocessed_content":"Title: bug in 's keras callback when specifying args; Content: hi, i am using s keras callback in . however, if i specify any args things immediately grind down to a halt. for reproduction, using the official example would be adequate. the problem for me is here, however, if i specify callback without any args, it works very well. this is the error i am getting in the former case, does anyone have any idea? edit this is the snippet the difference is perhaps the metrics being monitored in both mapemetric is a simple custom tf metric that computes the mean absolute percentage error;",
        "Answer_original_content":"hello neel, unfortunately i was not able to reproduce your issue. could you possibly send a minimal reproduction in a colab? thanks, nate hi neel, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. thanks, nate hi neel, since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no possible solutions provided in the answer. The answer is a series of follow-up messages from a support team member regarding a bug report.",
        "Answer_preprocessed_content":"hello neel, unfortunately i was not able to reproduce your issue. could you possibly send a minimal reproduction in a colab? thanks, nate hi neel, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. thanks, nate hi neel, since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"ClientError: object_detection_augmented_manifest_training template",
        "Question_body":"Hello,\n\nMy aim is to create a model for garden birds. I have 293 photos of birds that I have put through 2 custom labelling jobs in ground truth for training and validation. The issue I encountered was being able to have multiple labels on the bounding box which I managed to do via creating a custom labelling job with the following labels:\n\n<crowd-bounding-box\r\n    name=\"annotatedResult\"\r\n    labels=\"['Blackbird', 'Blue tit', 'Coal tit', 'Dunnock', 'Great tit', 'Long-tailed tit', 'Nuthatch', 'Pigeon', 'Robin']\" .....\n\n\nI now have 2 output manifest files with many lines of this:\n\n{\"source-ref\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\",\"BirdLabel\":{\"workerId\":\"privateXXXXX\",\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXX\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1619,\"top\":840,\"label\":\"Blackbird\",\"left\":1287,\"height\":753}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"BirdLabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10T15:41:52+0000\"}}\n\n\nAfter this job was successful, I made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template.\n\nI have filled in the necessary sections, I then run it and received this error when I have the Content Type to 'application\/x-image' with Record wrapper type:RecordIO : 'ClientError: train channel is not specified.'\n\nI then changed the channel to train_annotation instead of train and I receive this error message: \"ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)\\n\\nCaused by: u'train' is a required property\n\nAdditional information can be provided if neccessary.\nAny help would be much apreciated! Thank you.\n\nEdited by: LuciA on Jan 16, 2019 1:12 PM\n\nEdited by: LuciA on Jan 16, 2019 1:18 PM\n\nEdited by: LuciA on Jan 16, 2019 1:19 PM",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1547563664000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":52.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjk2-AZ6VQl68Zdm1Owq-_A\/client-error-object-detection-augmented-manifest-training-template",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-02-07T18:20:27.000Z",
                "Answer_score":0,
                "Answer_body":"Hi LuciA - I'm an engineer at AWS. Thanks for continuing to try the service in the face of some difficulties. Can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb?\n\nIt looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". Can you please check the syntax and let us know if your results change?",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2019-01-29T22:57:02.000Z",
                "Answer_score":0,
                "Answer_body":"Hi LuciA,\n\nThanks for trying out the SageMaker object detection algorithm. When using the Pipe mode with an AugmentedManifestFile, you need to specify the RecordWrapperType as RecordIO and ContentType as application\/x-recordio. Can you please retry with these suggestions and revert back if you continue to see issues.\n\nThanks!\nRegards,\nAmazon SageMaker Team",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-02-10T12:15:37.000Z",
                "Answer_score":0,
                "Answer_body":"Hi vrkhareataws\n\nThank you for getting back to me. I have tried the changes suggested and now recieve a new error message, 'InternalServerError: We encountered an internal error. Please try again.' I have added a snippet of code below for more insight:\n\ntraining_params = \\\r\n{\r\n    \"AlgorithmSpecification\": {\r\n        \"TrainingImage\": training_image, \r\n        \"TrainingInputMode\": \"Pipe\"\r\n    },\r\n    \"RoleArn\": role,\r\n    \"OutputDataConfig\": {\r\n        \"S3OutputPath\": s3_output_path\r\n    },\r\n    \"ResourceConfig\": {\r\n        \"InstanceCount\": 1,   \r\n        \"InstanceType\": \"ml.p3.2xlarge\",\r\n        \"VolumeSizeInGB\": 50\r\n    },\r\n    \"TrainingJobName\": job_name,\r\n    \"HyperParameters\": { \r\n         \"base_network\": \"resnet-50\",\r\n         \"use_pretrained_model\": \"1\",\r\n         \"num_classes\": \"1\",\r\n         \"mini_batch_size\": \"1\",\r\n         \"epochs\": \"5\",\r\n         \"learning_rate\": \"0.001\",\r\n         \"lr_scheduler_step\": \"3,6\",\r\n         \"lr_scheduler_factor\": \"0.1\",\r\n         \"optimizer\": \"rmsprop\",\r\n         \"momentum\": \"0.9\",\r\n         \"weight_decay\": \"0.0005\",\r\n         \"overlap_threshold\": \"0.5\",\r\n         \"nms_threshold\": \"0.45\",\r\n         \"image_shape\": \"300\",\r\n         \"label_width\": \"350\",\r\n         \"num_training_samples\": str(num_training_samples)\r\n    },\r\n    \"StoppingCondition\": {\r\n        \"MaxRuntimeInSeconds\": 86400\r\n    },\r\n    \"InputDataConfig\": [\r\n        {\r\n            \"ChannelName\": \"train\",\r\n            \"DataSource\": {\r\n                \"S3DataSource\": {\r\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\r\n                    \"S3Uri\": s3_train_data_path,\r\n                    \"S3DataDistributionType\": \"FullyReplicated\",\r\n                    \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] \r\n                }\r\n            },\r\n            \"ContentType\": \"application\/x-recordio\",\r\n            \"RecordWrapperType\": \"RecordIO\",\r\n            \"CompressionType\": \"None\"\r\n        },\r\n        {\r\n            \"ChannelName\": \"validation\",\r\n            \"DataSource\": {\r\n                \"S3DataSource\": {\r\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\r\n                    \"S3Uri\": s3_validation_data_path,\r\n                    \"S3DataDistributionType\": \"FullyReplicated\",\r\n                    \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] \r\n                }\r\n            },\r\n            \"ContentType\": \"application\/x-recordio\",\r\n            \"RecordWrapperType\": \"RecordIO\",\r\n            \"CompressionType\": \"None\"\r\n        }\r\n    ]\r\n}\n\n\nI had run 2 seperate labelling jobs to get my training and validation set which have different names. \"Bird-Label\" for my validation and \"Bird-Label-Train\" for training. As you can see in the \"AttributeNames\" fields.\n\nI would also like to point out that my decision for using the image content type was due to this sentence, \"However you can also train in pipe mode using the image files (image\/png, image\/jpeg, and application\/x-image), without creating RecordIO files, by using the augmented manifest format.\" from this guide (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html).\n\nPlease let me know if you understand what is causing this error. Thank You!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-30T10:05:35.000Z",
                "Answer_score":0,
                "Answer_body":"Hi JonathanB-AWS\n\nThank you for sending me really useful links.\n\nHaving sorted through the information, I understood that my JSON was slightly different. Having gone back to basics, I created another labelling job, but used the 'Bouding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!\n\nAs my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!\n\ni.e.\n\n{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\r\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nThe original mapping was 0:'Bird' for all images through the labelling job.\n\nThank you for your help!",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: clienterror: object_detection_augmented_manifest_training template; Content: hello, my aim is to create a model for garden birds. i have 293 photos of birds that i have put through 2 custom labelling jobs in ground truth for training and validation. the issue i encountered was being able to have multiple labels on the bounding box which i managed to do via creating a custom labelling job with the following labels: <crowd-bounding-box name=\"annotatedresult\" labels=\"['blackbird', 'blue tit', 'coal tit', 'dunnock', 'great tit', 'long-tailed tit', 'nuthatch', 'pigeon', 'robin']\" ..... i now have 2 output manifest files with many lines of this: {\"source-ref\":\"s3:\/\/xxxxx\/blackbird_1.jpg\",\"birdlabel\":{\"workerid\":\"privatexxxxx\",\"imagesource\":{\"s3uri\":\"s3:\/\/xxxxx\/blackbird_1.jpg\"},\"boxesinfo\":{\"annotatedresult\":{\"boundingboxes\":[{\"width\":1619,\"top\":840,\"label\":\"blackbird\",\"left\":1287,\"height\":753}],\"inputimageproperties\":{\"width\":3872,\"height\":2592}}}},\"birdlabel-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"birdlabel\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-10t15:41:52+0000\"}} after this job was successful, i made an ml.p3.2xlarge instance, using the object_detection_augmented_manifest_training template. i have filled in the necessary sections, i then run it and received this error when i have the content type to 'application\/x-image' with record wrapper type:recordio : 'clienterror: train channel is not specified.' i then changed the channel to train_annotation instead of train and i receive this error message: \"clienterror: unable to initialize the algorithm. failed to validate input data configuration. (caused by validationerror)\\n\\ncaused by: u'train' is a required property additional information can be provided if neccessary. any help would be much apreciated! thank you. edited by: lucia on jan 16, 2019 1:12 pm edited by: lucia on jan 16, 2019 1:18 pm edited by: lucia on jan 16, 2019 1:19 pm",
        "Question_original_content_gpt_summary":"The user encountered challenges while attempting to create a model for garden birds using an ML.p3.2xlarge instance and the object_detection_augmented_manifest_training template, receiving errors related to the train channel and input data configuration.",
        "Question_preprocessed_content":"Title: clienterror template; Content: hello, my aim is to create a model for garden birds. i have photos of birds that i have put through custom labelling jobs in ground truth for training and validation. the issue i encountered was being able to have multiple labels on the bounding box which i managed to do via creating a custom labelling job with the following labels name annotatedresult labels 'blackbird', 'blue tit', 'coal tit', 'dunnock', 'great tit', tit', 'nuthatch', 'pigeon', 'robin' i now have output manifest files with many lines of this after this job was successful, i made an instance, using the template. i have filled in the necessary sections, i then run it and received this error when i have the content type to with record wrapper type recordio 'clienterror train channel is not i then changed the channel to instead of train and i receive this error message clienterror unable to initialize the algorithm. failed to validate input data configuration. by u'train' is a required property additional information can be provided if neccessary. any help would be much apreciated! thank you. edited by lucia on jan , pm edited by lucia on jan , pm edited by lucia on jan , pm",
        "Answer_original_content":"hi lucia - i'm an engineer at aws. thanks for continuing to try the service in the face of some difficulties. can you please cross-reference your augmented manifest against the samples shown in https:\/\/aws.amazon.com\/blogs\/aws\/amazon--ground-truth-build-highly-accurate-datasets-and-reduce-labeling-costs-by-up-to-70\/, https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/ground_truth_labeling_jobs\/object_detection_augmented_manifest_training\/object_detection_augmented_manifest_training.ipynb, and https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/ground_truth_labeling_jobs\/ground_truth_object_detection_tutorial\/object_detection_tutorial.ipynb? it looks like your format is a little different, e.g., the algorithm expects to see keys called \"annotations\" and \"image_size\". can you please check the syntax and let us know if your results change?",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's challenge of encountering errors related to the train channel and input data configuration while creating a model for garden birds using an ML.p3.2xlarge instance and the object_detection_augmented_manifest_training template are to cross-reference the augmented manifest against the samples provided in the links shared in the answer and check the syntax to ensure that the format matches the expected keys called \"annotations\" and \"image_size\".",
        "Answer_preprocessed_content":"hi lucia i'm an engineer at aws. thanks for continuing to try the service in the face of some difficulties. can you please your augmented manifest against the samples shown in and it looks like your format is a little different, the algorithm expects to see keys called annotations and can you please check the syntax and let us know if your results change?"
    },
    {
        "Question_id":49548422.0,
        "Question_title":"Training data in S3 in AWS Sagemaker",
        "Question_body":"<p>I've uploaded my own Jupyter notebook to Sagemaker, and am trying to create an iterator for my training \/ validation data which is in S3, as follow:<\/p>\n\n<pre><code>train = mx.io.ImageRecordIter(\n        path_imgrec         = \u2018s3:\/\/bucket-name\/train.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>\n\n<p>I receive the following exception: <\/p>\n\n<pre><code>MXNetError: [04:33:32] src\/io\/s3_filesys.cc:899: Need to set enviroment variable AWS_SECRET_ACCESS_KEY to use S3\n<\/code><\/pre>\n\n<p>I've checked that the IAM role attached with this notebook instance has S3 access. Any clues on what might be needed to fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1522300028073,
        "Question_favorite_count":3.0,
        "Question_score":3.0,
        "Question_view_count":1353.0,
        "Owner_creation_time":1449031669083,
        "Owner_last_access_time":1663956894043,
        "Owner_reputation":777.0,
        "Owner_up_votes":119.0,
        "Owner_down_votes":2.0,
        "Owner_views":103.0,
        "Answer_body":"<p>If your IAM roles are setup correctly, then you need to download the file to the Sagemaker instance first and then work on it. Here's how:<\/p>\n\n<pre><code># Import roles\nimport sagemaker\nrole = sagemaker.get_execution_role()\n\n# Download file locally\ns3 = boto3.resource('s3')\ns3.Bucket(bucket).download_file('your_training_s3_file.rec', 'training.rec')\n\n#Access locally\ntrain = mx.io.ImageRecordIter(path_imgrec=\u2018training.rec\u2019 \u2026\u2026 )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1522305475727,
        "Answer_score":4.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49548422",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: training data in s3 in ; Content: i've uploaded my own jupyter notebook to , and am trying to create an iterator for my training \/ validation data which is in s3, as follow: train = mx.io.imagerecorditer( path_imgrec = \u2018s3:\/\/bucket-name\/train.rec\u2019 \u2026\u2026 ) i receive the following exception: mxneterror: [04:33:32] src\/io\/s3_filesys.cc:899: need to set enviroment variable aws_secret_access_key to use s3 i've checked that the iam role attached with this notebook instance has s3 access. any clues on what might be needed to fix this?",
        "Question_original_content_gpt_summary":"The user is encountering an issue with creating an iterator for their training\/validation data stored in s3, receiving an exception that they need to set an environment variable for aws_secret_access_key.",
        "Question_preprocessed_content":"Title: training data in s in ; Content: i've uploaded my own jupyter notebook to , and am trying to create an iterator for my training \/ validation data which is in s , as follow i receive the following exception i've checked that the iam role attached with this notebook instance has s access. any clues on what might be needed to fix this?",
        "Answer_original_content":"if your iam roles are setup correctly, then you need to download the file to the instance first and then work on it. here's how: # import roles import role = .get_execution_role() # download file locally s3 = boto3.resource('s3') s3.bucket(bucket).download_file('your_training_s3_file.rec', 'training.rec') #access locally train = mx.io.imagerecorditer(path_imgrec=training.rec )",
        "Answer_original_content_gpt_summary":"The solution to the issue of creating an iterator for training\/validation data stored in s3 and receiving an exception that an environment variable for aws_secret_access_key needs to be set is to download the file to the instance first and then work on it. The answer provides a code snippet that imports roles, downloads the file locally, and accesses it locally using mx.io.imagerecorditer.",
        "Answer_preprocessed_content":"if your iam roles are setup correctly, then you need to download the file to the instance first and then work on it. here's how"
    },
    {
        "Question_id":73501103.0,
        "Question_title":"Getting Bad request while searching run in mlflow",
        "Question_body":"<p>Training a ml model with mlflow in azure environment.<\/p>\n<pre><code>import mlflow\nfrom mlflow import MlflowClient\nfrom azureml.core import Experiment, Workspace\n\nexperiment_name = 'housing-lin-mlflow'\n\nexperiment = Experiment(ws, experiment_name)\n\nruns = mlflow.search_runs(experiment_ids=[ experiment.id ])\n\n<\/code><\/pre>\n<p>While fetching runs from search_runs getting this error :<\/p>\n<pre><code>RestException: BAD_REQUEST: For input string: &quot;5b649b3c-3b8f-497a-bb4f&quot;\n<\/code><\/pre>\n<p>MLflow version : 1.28.0\nIn Azure studio jobs have been created and successfully run.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1661517215980,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Owner_creation_time":1582101477803,
        "Owner_last_access_time":1663953873503,
        "Owner_reputation":171.0,
        "Owner_up_votes":17.0,
        "Owner_down_votes":0.0,
        "Owner_views":53.0,
        "Answer_body":"<p>The bad request in MLFlow after successful running the job is because of not giving proper API permissions for the application.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rP6Ja.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Search for <strong>MLFLOW<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TGU2C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Scroll down<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/s50AL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s50AL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Click on View API Permissions<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/f7Txf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Under API permissions, assign the permissions according to the application running region and requirements. Checkout the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models-mlflow\" rel=\"nofollow noreferrer\">document<\/a> for further information.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661603882123,
        "Answer_score":1.0,
        "Owner_location":"Delhi, India",
        "Question_last_edit_time":1661625379892,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73501103",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: getting bad request while searching run in ; Content: training a ml model with in azure environment. import from import client from .core import experiment, workspace experiment_name = 'housing-lin-' experiment = experiment(ws, experiment_name) runs = .search_runs(experiment_ids=[ experiment.id ]) while fetching runs from search_runs getting this error : restexception: bad_request: for input string: \"5b649b3c-3b8f-497a-bb4f\" version : 1.28.0 in azure studio jobs have been created and successfully run.",
        "Question_original_content_gpt_summary":"The user is encountering a \"bad request\" error while attempting to search for runs in MLflow while training a ML model in an Azure environment.",
        "Question_preprocessed_content":"Title: getting bad request while searching run in ; Content: training a ml model with in azure environment. while fetching runs from getting this error version in azure studio jobs have been created and successfully run.",
        "Answer_original_content":"the bad request in mlflow after successful running the job is because of not giving proper api permissions for the application. search for mlflow scroll down click on view api permissions under api permissions, assign the permissions according to the application running region and requirements. checkout the document for further information.",
        "Answer_original_content_gpt_summary":"The solution to the \"bad request\" error in MLflow during ML model training in an Azure environment is to assign proper API permissions to the application. This can be done by scrolling down to \"View API Permissions\" under API Permissions in MLflow and assigning the necessary permissions based on the application's region and requirements. Further information can be found in the documentation.",
        "Answer_preprocessed_content":"the bad request in after successful running the job is because of not giving proper api permissions for the application. search for scroll down click on view api permissions under api permissions, assign the permissions according to the application running region and requirements. checkout the document for further information."
    },
    {
        "Question_id":52055933.0,
        "Question_title":"Azure Machine Learning Studio append rows to dataset",
        "Question_body":"<p>My \"experiment\" is like this,<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AgnGE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AgnGE.png\" alt=\"Experiment\"><\/a><\/p>\n\n<p>I have 10 rows (excluding header) in \"Dataset.csv\" and 3 rows (excluding header) in the CSV being imported by <em>Import Data<\/em>. The schema of both CSVs is same. I want <em>Add Rows<\/em> to <strong>append<\/strong> the 3 rows to Dataset.csv.<\/p>\n\n<p>The real \"Dataset.csv\" has more than 25,000 rows and is expected to grow. Hence, using <em>Export Data<\/em> to generate a merged dataset (as a new CSV) is not a feasible solution. Any way to implement <strong>append<\/strong> for this scenario?<\/p>\n\n<p>Thanks<\/p>\n\n<p>Update 1:\nDataset.csv is present in ML Studios <em>Dataset<\/em>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LBimY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LBimY.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8.0,
        "Question_creation_time":1535452910717,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":590.0,
        "Owner_creation_time":1409841727700,
        "Owner_last_access_time":1663859408296,
        "Owner_reputation":805.0,
        "Owner_up_votes":599.0,
        "Owner_down_votes":0.0,
        "Owner_views":137.0,
        "Answer_body":"<p>So it turns out the <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow noreferrer\">Python SDK<\/a> has an <code>update_from_dataframe<\/code> method on it that can be used to update a dataset that has been uploaded to Azure ML Studio. If you're unable to use a new CSV and need to update an existing data set, then this should do the trick.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1535538465923,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":1535461331150,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52055933",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: studio append rows to dataset; Content: my \"experiment\" is like this, i have 10 rows (excluding header) in \"dataset.csv\" and 3 rows (excluding header) in the csv being imported by import data. the schema of both csvs is same. i want add rows to append the 3 rows to dataset.csv. the real \"dataset.csv\" has more than 25,000 rows and is expected to grow. hence, using export data to generate a merged dataset (as a new csv) is not a feasible solution. any way to implement append for this scenario? thanks update 1: dataset.csv is present in ml studios dataset.",
        "Question_original_content_gpt_summary":"The user is looking for a way to append 3 rows from an imported CSV to an existing dataset.csv with more than 25,000 rows in ML Studio.",
        "Question_preprocessed_content":"Title: studio append rows to dataset; Content: my experiment is like this, i have rows in and rows in the csv being imported by import data. the schema of both csvs is same. i want add rows to append the rows to the real has more than , rows and is expected to grow. hence, using export data to generate a merged dataset is not a feasible solution. any way to implement append for this scenario? thanks update is present in ml studios dataset.",
        "Answer_original_content":"so it turns out the python sdk has an update_from_dataframe method on it that can be used to update a dataset that has been uploaded to studio. if you're unable to use a new csv and need to update an existing data set, then this should do the trick.",
        "Answer_original_content_gpt_summary":"Possible solution: Use the Python SDK's update_from_dataframe method to update an existing dataset in ML Studio with the 3 rows from the imported CSV.",
        "Answer_preprocessed_content":"so it turns out the python sdk has an method on it that can be used to update a dataset that has been uploaded to studio. if you're unable to use a new csv and need to update an existing data set, then this should do the trick."
    },
    {
        "Question_id":null,
        "Question_title":"Move a .dvc stage without re-run",
        "Question_body":"<p>I just ran a <code>dvc run ...<\/code> but forgot to specify the <code>-f<\/code> flag so it created a <code>.dvc<\/code> file in an unintended location. Is there a smart way of moving it without re-running the stage? This method should take care of location and relative paths <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1589374711255,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":445.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/move-a-dvc-stage-without-re-run\/384",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-13T15:12:12.312Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drorata\">@drorata<\/a>,<\/p>\n<p>Unfortunatelly it seems you need to just move the DVC-file and manually change the paths in it to make them relative to the new location. Or you can change them into absolute paths.<\/p>\n<p>If you think you may be moving stage files in the future, maybe consider specifying the dependencies and outputs as absolute paths when you use <code>dvc run<\/code>.<\/p>\n<p>It would be great to have this as a feature request though, would you like to open an issue in out Git repo?<\/p>\n<p>Thanks<\/p>",
                "Answer_score":436.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-13T17:36:12.540Z",
                "Answer_body":"<p>Done! <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3795\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/issues\/3795<\/a><\/p>",
                "Answer_score":15.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-13T18:50:45.040Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/drorata\">@drorata<\/a> have you had chance to try the pre-release 1.0a? It has changed a lot the way pipelines are organized - it\u2019s a single file now that is intended to be human readable\/editable. Would be really great if you give it a try and let us know if solves the problem with renaming\/moving things.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-13T19:03:45.378Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> as I mentioned in the ticket, I didn\u2019t have the chance to put in action the changes introduced  in 1.0a. Most of my time with DVC is currently invested in a single project where I\u2019m worried about backward incompatibilities.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: move a . stage without re-run; Content: i just ran a run ... but forgot to specify the -f flag so it created a . file in an unintended location. is there a smart way of moving it without re-running the stage? this method should take care of location and relative paths",
        "Question_original_content_gpt_summary":"The user is looking for a way to move a . stage file without having to re-run the stage, while also taking into account the location and relative paths.",
        "Question_preprocessed_content":"Title: move a . stage without ; Content: i just ran a but forgot to specify the flag so it created a file in an unintended location. is there a smart way of moving it without the stage? this method should take care of location and relative paths",
        "Answer_original_content":"hi @drorata, unfortunatelly it seems you need to just move the -file and manually change the paths in it to make them relative to the new location. or you can change them into absolute paths. if you think you may be moving stage files in the future, maybe consider specifying the dependencies and outputs as absolute paths when you use run. it would be great to have this as a feature request though, would you like to open an issue in out git repo? thanks done! https:\/\/github.com\/iterative\/\/issues\/3795 @drorata have you had chance to try the pre-release 1.0a? it has changed a lot the way pipelines are organized - its a single file now that is intended to be human readable\/editable. would be really great if you give it a try and let us know if solves the problem with renaming\/moving things. @shcheklein as i mentioned in the ticket, i didnt have the chance to put in action the changes introduced in 1.0a. most of my time with is currently invested in a single project where im worried about backward incompatibilities.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are: manually changing the paths in the file to make them relative to the new location or changing them into absolute paths. Another solution is to specify the dependencies and outputs as absolute paths when using run. The answer also suggests trying the pre-release 1.0a, which has changed the way pipelines are organized and is intended to be human-readable\/editable.",
        "Answer_preprocessed_content":"hi unfortunatelly it seems you need to just move the and manually change the paths in it to make them relative to the new location. or you can change them into absolute paths. if you think you may be moving stage files in the future, maybe consider specifying the dependencies and outputs as absolute paths when you use . it would be great to have this as a feature request though, would you like to open an issue in out git repo? thanks done! have you had chance to try the it has changed a lot the way pipelines are organized its a single file now that is intended to be human would be really great if you give it a try and let us know if solves the problem with things. as i mentioned in the ticket, i didnt have the chance to put in action the changes introduced in most of my time with is currently invested in a single project where im worried about backward incompatibilities."
    },
    {
        "Question_id":50352412.0,
        "Question_title":"How to prevent a NoCredentialsError when calling the fit method in SageMaker?",
        "Question_body":"<p>I am a newbie when it comes to Python SageMaker (my background is C#). Currently, I have a problem because the last method call (I mean the fit method) results in a \"NoCredentialsError\". I do not understand that. The AWS credentials have been set and I do use them to communicate with AWS, for example to communicate with S3. How can I prevent this error? <\/p>\n\n<pre><code>import io\nimport os\nimport gzip\nimport pickle\nimport urllib.request\nimport boto3\nimport sagemaker\nimport sagemaker.amazon.common as smac\n\nDOWNLOADED_FILENAME = 'C:\/Users\/Daan\/PycharmProjects\/downloads\/mnist.pkl.gz'\nif not os.path.exists(DOWNLOADED_FILENAME):\n    urllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", DOWNLOADED_FILENAME)\n\nwith gzip.open(DOWNLOADED_FILENAME, 'rb') as f:\n    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\nvectors = train_set[0].T\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, vectors)\nbuf.seek(0)\nkey = 'recordio-pb-data'\nbucket_name = 'SOMEKINDOFBUCKETNAME'\nprefix = 'sagemaker\/pca'\npath = os.path.join(prefix, 'train', key)\nprint(path)\n\nsession = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\nregion='eu-west-1'\nsagemakerSession= sagemaker.Session(sagemaker_client=client,boto_session=session)\ns3_resource=session.resource('s3')\nbucket = s3_resource.Bucket(bucket_name)\ncurrent_bucket = bucket.Object(path)\n\ntrain_data = 's3:\/\/{}\/{}\/train\/{}'.format(bucket_name, prefix, key)\nprint('uploading training data location: {}'.format(train_data))\ncurrent_bucket.upload_fileobj(buf)\n\noutput_location = 's3:\/\/{}\/{}\/output'.format('SOMEBUCKETNAME', prefix)\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nregion='eu-west-1'\n\ncontainers = {'us-west-2': 'SOMELOCATION',\n              'us-east-1': 'SOMELOCATION',\n              'us-east-2': 'SOMELOCATION',\n              'eu-west-1': 'SOMELOCATION'}\ncontainer = containers[region]\n\nrole='AmazonSageMaker-ExecutionRole-SOMEVALUE'\npca = sagemaker.estimator.Estimator(container,\n                                    role,\n                                    train_instance_count=1,\n                                    train_instance_type='ml.c4.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sagemakerSession)\n\n\npca.set_hyperparameters(feature_dim=50000,\n                        num_components=10,\n                        subtract_mean=True,\n                        algorithm_mode='randomized',\n                        mini_batch_size=200)\n\npca.fit(inputs=train_data)\n\nprint('END')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1526393414957,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":447.0,
        "Owner_creation_time":1358429250663,
        "Owner_last_access_time":1663950242467,
        "Owner_reputation":2120.0,
        "Owner_up_votes":100.0,
        "Owner_down_votes":3.0,
        "Owner_views":279.0,
        "Answer_body":"<p>I am not sure if you have masked the actual access id and key or this is what you are running.<\/p>\n<pre><code>session = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\n<\/code><\/pre>\n<p>I am hoping you are providing the actual aws_access_key_id and aws_secret_access_key in the above lines of code.<\/p>\n<p>Another way of specifying the same and not hardcoding in the code is to create a credentials file in your profile directory i.e.<\/p>\n<p>in Mac    ~\/.aws\/<\/p>\n<p>and in Windows <code>&quot;%UserProfile%\\.aws&quot;<\/code><\/p>\n<p>the file is a plain text file with a name &quot;credentials&quot; (without the quotes).\nfile contains<\/p>\n<pre><code>[default]\naws_access_key_id=XXXXXXXXXXXXXX\naws_secret_access_key=YYYYYYYYYYYYYYYYYYYYYYYYYYY\n<\/code><\/pre>\n<p>AWS CLI would pick it up from the above location and use it. You can also use non-default profiles and pass on the profile with<\/p>\n<pre><code>os.environ[&quot;AWS_PROFILE&quot;] = &quot;profile-name&quot;\n<\/code><\/pre>\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1526405500043,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1636970707960,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50352412",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to prevent a nocredentialserror when calling the fit method in ?; Content: i am a newbie when it comes to python (my background is c#). currently, i have a problem because the last method call (i mean the fit method) results in a \"nocredentialserror\". i do not understand that. the aws credentials have been set and i do use them to communicate with aws, for example to communicate with s3. how can i prevent this error? import io import os import gzip import pickle import urllib.request import boto3 import import .amazon.common as smac downloaded_filename = 'c:\/users\/daan\/pycharmprojects\/downloads\/mnist.pkl.gz' if not os.path.exists(downloaded_filename): urllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", downloaded_filename) with gzip.open(downloaded_filename, 'rb') as f: train_set, valid_set, test_set = pickle.load(f, encoding='latin1') vectors = train_set[0].t buf = io.bytesio() smac.write_numpy_to_dense_tensor(buf, vectors) buf.seek(0) key = 'recordio-pb-data' bucket_name = 'somekindofbucketname' prefix = '\/pca' path = os.path.join(prefix, 'train', key) print(path) session = boto3.session.session(aws_access_key_id='secret',aws_secret_access_key='secret',region_name='eu-west-1') client = boto3.client('',region_name='eu-west-1',aws_access_key_id='secret',aws_secret_access_key='secret') region='eu-west-1' session= .session(_client=client,boto_session=session) s3_resource=session.resource('s3') bucket = s3_resource.bucket(bucket_name) current_bucket = bucket.object(path) train_data = 's3:\/\/{}\/{}\/train\/{}'.format(bucket_name, prefix, key) print('uploading training data location: {}'.format(train_data)) current_bucket.upload_fileobj(buf) output_location = 's3:\/\/{}\/{}\/output'.format('somebucketname', prefix) print('training artifacts will be uploaded to: {}'.format(output_location)) region='eu-west-1' containers = {'us-west-2': 'somelocation', 'us-east-1': 'somelocation', 'us-east-2': 'somelocation', 'eu-west-1': 'somelocation'} container = containers[region] role='amazon-executionrole-somevalue' pca = .estimator.estimator(container, role, train_instance_count=1, train_instance_type='ml.c4.xlarge', output_path=output_location, _session=session) pca.set_hyperparameters(feature_dim=50000, num_components=10, subtract_mean=true, algorithm_mode='randomized', mini_batch_size=200) pca.fit(inputs=train_data) print('end')",
        "Question_original_content_gpt_summary":"The user is encountering a \"NoCredentialsError\" when calling the fit method in Python, despite having set the AWS credentials and successfully using them to communicate with S3.",
        "Question_preprocessed_content":"Title: how to prevent a nocredentialserror when calling the fit method in ?; Content: i am a newbie when it comes to python . currently, i have a problem because the last method call results in a nocredentialserror . i do not understand that. the aws credentials have been set and i do use them to communicate with aws, for example to communicate with s . how can i prevent this error?",
        "Answer_original_content":"i am not sure if you have masked the actual access id and key or this is what you are running. session = boto3.session.session(aws_access_key_id='secret',aws_secret_access_key='secret',region_name='eu-west-1') client = boto3.client('',region_name='eu-west-1',aws_access_key_id='secret',aws_secret_access_key='secret') i am hoping you are providing the actual aws_access_key_id and aws_secret_access_key in the above lines of code. another way of specifying the same and not hardcoding in the code is to create a credentials file in your profile directory i.e. in mac ~\/.aws\/ and in windows \"%userprofile%\\.aws\" the file is a plain text file with a name \"credentials\" (without the quotes). file contains [default] aws_access_key_id=xxxxxxxxxxxxxx aws_secret_access_key=yyyyyyyyyyyyyyyyyyyyyyyyyyy aws cli would pick it up from the above location and use it. you can also use non-default profiles and pass on the profile with os.environ[\"aws_profile\"] = \"profile-name\" hope this helps.",
        "Answer_original_content_gpt_summary":"The possible solutions to the \"NoCredentialsError\" when calling the fit method in Python are to ensure that the actual AWS access key and secret access key are provided in the code or to create a credentials file in the profile directory. The credentials file should contain the access key and secret access key, and AWS CLI would pick it up from the specified location. Additionally, non-default profiles can be used and passed on with the os.environ command.",
        "Answer_preprocessed_content":"i am not sure if you have masked the actual access id and key or this is what you are running. i am hoping you are providing the actual and in the above lines of code. another way of specifying the same and not hardcoding in the code is to create a credentials file in your profile directory in mac and in windows the file is a plain text file with a name credentials . file contains aws cli would pick it up from the above location and use it. you can also use profiles and pass on the profile with hope this helps."
    },
    {
        "Question_id":null,
        "Question_title":"Why is Designer so slow to execute?",
        "Question_body":"I'm running the simple tutorials, preparing for DP 100. I was wondering why the execution of Designer Pipelines is so slow, even when running very simple operations and minuscule DataFrames such as Automobile price data (Raw). I also may start working for a company that has been using Azure for Machine Learning and the interviewer commented something along the lines of being it very very slow.\n\nIf a very simple model training pipeline on a 200 records DataFrame took almost 20 minutes, I keep wondering how long it would take to compute a real world data pipeline.\n\nAny insights? Thank you.",
        "Question_answer_count":5,
        "Question_comment_count":1.0,
        "Question_creation_time":1601819361423,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/116085\/why-is-designer-so-slow-to-execute.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-05T10:10:32.187Z",
                "Answer_score":0,
                "Answer_body":"@ivangvi can you please check if your compute has spinned up or is a cold compute in this case? If you are running on a cold compute, it may take several minutes to spin up. Also Azure Machine Learning is running on the backend of Azure Machine Learning pipeline, so if your input data hasn't changed, next time pipeline is automatically using the cached result of that module so it should be fast compared with first time running time.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-13T21:28:42.713Z",
                "Answer_score":0,
                "Answer_body":"Designer is pretty interesting, but I agree: IT IS TOO SLOW. Unusably slow, unfortunately.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-12T19:30:38.833Z",
                "Answer_score":0,
                "Answer_body":"I can't believe it has been over 7 months since the first post in this thread and this fundamental issue still persists! As @ivangvi indicating, simply following Microsoft's own ML tutorial (here: https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/create-regression-model-azure-machine-learning-designer\/explore-data), takes 20 min just to apply a few simple transformation to a 205 row x 36 column dataset. Just removing one column took 5 min! The whole thing would take milliseconds in a local Jupiter notebook. Why would anyone ever use this? I'm baffled.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-17T12:16:01.32Z",
                "Answer_score":0,
                "Answer_body":"Same for me, I am experimenting with some Udacity courses that were created by MS, and the pipelines are so slow to the point of unbearabe. For on example the course says that the pipeline should take around 10 mins and the actual time was 50 mins. There was another case were my lab free time of 1hr elapsed and the pipeline didnt complete the execution....",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T17:40:02.17Z",
                "Answer_score":0,
                "Answer_body":"I used to teach Azure Classic and it was great. Now, I need to switch to Azure Designer but it is too slow.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why is designer so slow to execute?; Content: i'm running the simple tutorials, preparing for dp 100. i was wondering why the execution of designer pipelines is so slow, even when running very simple operations and minuscule dataframes such as automobile price data (raw). i also may start working for a company that has been using azure for machine learning and the interviewer commented something along the lines of being it very very slow. if a very simple model training pipeline on a 200 records dataframe took almost 20 minutes, i keep wondering how long it would take to compute a real world data pipeline. any insights? thank you.",
        "Question_original_content_gpt_summary":"The user is encountering slow execution times when running simple operations and small dataframes in Azure Designer, and is concerned about how long it would take to compute a real-world data pipeline.",
        "Question_preprocessed_content":"Title: why is designer so slow to execute?; Content: i'm running the simple tutorials, preparing for dp . i was wondering why the execution of designer pipelines is so slow, even when running very simple operations and minuscule dataframes such as automobile price data . i also may start working for a company that has been using azure for machine learning and the interviewer commented something along the lines of being it very very slow. if a very simple model training pipeline on a records dataframe took almost minutes, i keep wondering how long it would take to compute a real world data pipeline. any insights? thank you.",
        "Answer_original_content":"@ivangvi can you please check if your compute has spinned up or is a cold compute in this case? if you are running on a cold compute, it may take several minutes to spin up. also is running on the backend of pipeline, so if your input data hasn't changed, next time pipeline is automatically using the cached result of that module so it should be fast compared with first time running time. designer is pretty interesting, but i agree: it is too slow. unusably slow, unfortunately. i can't believe it has been over 7 months since the first post in this thread and this fundamental issue still persists! as @ivangvi indicating, simply following microsoft's own ml tutorial (here: https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/create-regression-model-azure-machine-learning-designer\/explore-data), takes 20 min just to apply a few simple transformation to a 205 row x 36 column dataset. just removing one column took 5 min! the whole thing would take milliseconds in a local jupiter notebook. why would anyone ever use this? i'm baffled. same for me, i am experimenting with some udacity courses that were created by ms, and the pipelines are so slow to the point of unbearabe. for on example the course says that the pipeline should take around 10 mins and the actual time was 50 mins. there was another case were my lab free time of 1hr elapsed and the pipeline didnt complete the execution.... i used to teach azure classic and it was great. now, i need to switch to azure designer but it is too slow.",
        "Answer_original_content_gpt_summary":"Possible solutions to the slow execution times in Azure Designer include checking if the compute has spinned up or is a cold compute, using the cached result of the module if the input data hasn't changed, and being patient with the slow performance of the Designer. However, the answer also suggests that the Designer is too slow and unusable, and that using a local Jupiter notebook would be faster.",
        "Answer_preprocessed_content":"can you please check if your compute has spinned up or is a cold compute in this case? if you are running on a cold compute, it may take several minutes to spin up. also is running on the backend of pipeline, so if your input data hasn't changed, next time pipeline is automatically using the cached result of that module so it should be fast compared with first time running time. designer is pretty interesting, but i agree it is too slow. unusably slow, unfortunately. i can't believe it has been over months since the first post in this thread and this fundamental issue still persists! as indicating, simply following microsoft's own ml tutorial , takes min just to apply a few simple transformation to a row x column dataset. just removing one column took min! the whole thing would take milliseconds in a local jupiter notebook. why would anyone ever use this? i'm baffled. same for me, i am experimenting with some udacity courses that were created by ms, and the pipelines are so slow to the point of unbearabe. for on example the course says that the pipeline should take around mins and the actual time was mins. there was another case were my lab free time of hr elapsed and the pipeline didnt complete the i used to teach azure classic and it was great. now, i need to switch to azure designer but it is too slow."
    },
    {
        "Question_id":null,
        "Question_title":"Download Report as LaTeX not working",
        "Question_body":"<p>Hey,<\/p>\n<p>I\u2019m trying to download a <a href=\"https:\/\/wandb.ai\/dezzardhd\/eval_finals\/reports\/P1850-dim16--VmlldzoxNzc0NzA3?accessToken=7f0xyvcyq6j27xipyvuwhgbm9237uf2uft18zxt0kyhlutbg5b1ynodog330pqpa\">report<\/a> as LaTeX.<br>\nThe images and tables are not being downloaded. The folders contained in the zip archive are empty.<\/p>\n<p>That\u2019s a bug I guess.<\/p>\n<p>DezzardHD<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1649017163494,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":104.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/download-report-as-latex-not-working\/2182",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-04T11:37:32.088Z",
                "Answer_body":"<p>Hey Moritz,<\/p>\n<p>Thanks for reporting this. I\u2019ll file a ticket so that our team can look into this.<\/p>\n<p>Best,<br>\nArman<\/p>",
                "Answer_score":15.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-02T20:19:49.405Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: download report as latex not working; Content: hey, i\u2019m trying to download a report as latex. the images and tables are not being downloaded. the folders contained in the zip archive are empty. that\u2019s a bug i guess. dezzardhd",
        "Question_original_content_gpt_summary":"The user is encountering a bug where images and tables are not being downloaded when attempting to download a report as LaTeX.",
        "Question_preprocessed_content":"Title: download report as latex not working; Content: hey, im trying to download a report as latex. the images and tables are not being downloaded. the folders contained in the zip archive are empty. thats a bug i guess. dezzardhd",
        "Answer_original_content":"hey moritz, thanks for reporting this. ill file a ticket so that our team can look into this. best, arman this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer. The user reported a bug where images and tables are not being downloaded when attempting to download a report as LaTeX. The responder acknowledged the issue and said they will file a ticket for their team to investigate.",
        "Answer_preprocessed_content":"hey moritz, thanks for reporting this. ill file a ticket so that our team can look into this. best, arman this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Tensorboard taking long to startup",
        "Question_body":"<p>I\u2019m getting this warning:<\/p>\n<p>WARNING: Guild took 9.56 seconds to prepare runs. To reduce startup time, try running with \u2018\u2013skip-images\u2019 or \u2018\u2013skip-hparams\u2019 options or reduce the number of runs with filters. Try \u2018guild tensorboard --help\u2019 for filter options.<\/p>\n<p>Adding skip-images or skip-hparams does not help.<\/p>\n<p>I suspect this happens because I have a resource (symbolic link) to a directory with a lot of files. Is it possible to configure guild tensorboard to ignore this directory?<\/p>\n<p>I found the -O option in: <a href=\"https:\/\/my.guild.ai\/t\/command-tensorboard\/127\" class=\"inline-onebox\">Command: tensorboard<\/a><br>\nbut adding -O logdir=tb did not work (had to abort because it never finished).<\/p>\n<p>Any ideas?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_creation_time":1593771427991,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":950.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-taking-long-to-startup\/212",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-03T12:23:47.051Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/samedii\">@samedii<\/a> welcome to the new site!<\/p>\n<p>There\u2019s no easy workaround that I can think of. To me this behavior is arguably a bug. Guild is following symlinks to find run files that might be used for various TensorBoard plugins (e.g. projections, etc.) and I don\u2019t understand why.<\/p>\n<p>I\u2019ll spend some time investigating and look at changing this this. There may be a good reason for the current behavior.<\/p>\n<p>Either way I\u2019ll address this in master. 0.7 is frozen - rc11 is the last release candidate barring some world ending bug that\u2019s discovered.<\/p>\n<p>Are you able to compile and run from source?<\/p>",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-03T13:09:05.696Z",
                "Answer_body":"<p>Hello <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><br>\nI see, thanks for checking! I don\u2019t think I had any trouble running from source when I did so previously.<\/p>\n<p>I confirmed that the issue was the symbolic link. When I removed it, guild started tb very quickly.<\/p>\n<p>I think this issue would be solved for my specific case if I could specify where to look for tb-logs in the guild.yml. I could maybe use -O logdir=dir\/to\/guild\/runs\/id\/tb already but that\u2019s quite a hassle<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-03T13:22:29.023Z",
                "Answer_body":"<p><code>-O<\/code> is meant to pass options along to TensorBoard without any knowledge of what\u2019s being passed. I think <code>logdir<\/code> needs to be explicitly ignored (with warning message) as Guild takes over the function of setting up and specifying the logdir that TensorBoard sees.<\/p>\n<p>I see what you\u2019re getting at. I agree this should be in the Guild file. I hate to complicate things, but given the flexibility of the tool, where the script can put files anywhere it wants, I think the spec would have to follow the line of <code>sourcecode<\/code> and use Guild\u2019s file select spec.<\/p>\n<p>I think maybe this interface?<\/p>\n<pre><code class=\"lang-yaml\">op:\n  tensorboard:\n    logdir: relpath-to-tb-files\n<\/code><\/pre>\n<p>In this case <code>logdir<\/code> is a fully supported file select spec like that used for <code>sourcecode<\/code>.<\/p>",
                "Answer_score":16.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-03T14:43:51.040Z",
                "Answer_body":"<p>Yes that would work well in my case and I think most people can easily adapt their code to it if they have these kinds of issues<\/p>",
                "Answer_score":51.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-03T14:50:55.909Z",
                "Answer_body":"<p>I just applied this commit:<\/p>\n<aside class=\"onebox githubcommit\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/commit\/341abcd46eb13158275cf5254888d32b4b8203cf\" target=\"_blank\">github.com\/guildai\/guildai<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Commit\">\n    <svg width=\"60\" height=\"60\" class=\"github-icon\" viewBox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M10.86 7c-.45-1.72-2-3-3.86-3-1.86 0-3.41 1.28-3.86 3H0v2h3.14c.45 1.72 2 3 3.86 3 1.86 0 3.41-1.28 3.86-3H14V7h-3.14zM7 10.2c-1.22 0-2.2-.98-2.2-2.2 0-1.22.98-2.2 2.2-2.2 1.22 0 2.2.98 2.2 2.2 0 1.22-.98 2.2-2.2 2.2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/commit\/341abcd46eb13158275cf5254888d32b4b8203cf\" target=\"_blank\">Don't traverse symlinks when linking to run files for TensorBoard<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        committed <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2020-07-03\" data-time=\"14:44:15\" data-timezone=\"UTC\">02:44PM - 03 Jul 20 UTC<\/span>\n      <\/div>\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/gar1t\" target=\"_blank\">\n          <img alt=\"gar1t\" src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/84ac3354a76fe15593cedb56fe486a0ed93d5440.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          gar1t\n        <\/a>\n        \n      <\/div>\n\n      <div class=\"lines\" title=\"changed 1 files with 2 additions and 2 deletions\">\n        <a href=\"https:\/\/github.com\/guildai\/guildai\/commit\/341abcd46eb13158275cf5254888d32b4b8203cf\" target=\"_blank\">\n          <span class=\"added\">+2<\/span>\n          <span class=\"removed\">-2<\/span>\n        <\/a>\n      <\/div>\n    <\/div>\n\n  <\/div>\n<\/div>\n\n\n  <div class=\"github-row\">\n    <pre class=\"github-content\" style=\"white-space: normal;\">It's not clear why symlinks were being traversed in the first\nplace. TensorBoard should avoid letting linked files in anyway. This\nmay have...<\/pre>\n  <\/div>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>If you grab the latest from master that issue should be resolved for you. Just make sure you\u2019re using Guild from source.You can run <code>guild check<\/code> and look for <code>guild_install_location<\/code> to confirm it\u2019s running from the repo and not from an installed package. (You may need to run <code>hash-r<\/code> to clear your shell\u2019s cached location of the <code>guild<\/code> exe.)<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-04T21:04:54.103Z",
                "Answer_body":"<p>3 posts were split to a new topic: <a href=\"\/t\/issues-with-guild-file-output-scalars-and-sourcecode\/218\">Issues with Guild file - output-scalars and sourcecode<\/a><\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tensorboard taking long to startup; Content: i\u2019m getting this warning: warning: guild took 9.56 seconds to prepare runs. to reduce startup time, try running with \u2018\u2013skip-images\u2019 or \u2018\u2013skip-hparams\u2019 options or reduce the number of runs with filters. try \u2018guild tensorboard --help\u2019 for filter options. adding skip-images or skip-hparams does not help. i suspect this happens because i have a resource (symbolic link) to a directory with a lot of files. is it possible to configure guild tensorboard to ignore this directory? i found the -o option in: command: tensorboard but adding -o logdir=tb did not work (had to abort because it never finished). any ideas?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with Tensorboard taking a long time to startup, and is unable to reduce the startup time with the '--skip-images' or '--skip-hparams' options or reducing the number of runs with filters, and is looking for a way to configure Guild Tensorboard to ignore a directory with a lot of files.",
        "Question_preprocessed_content":"Title: tensorboard taking long to startup; Content: im getting this warning warning guild took seconds to prepare runs. to reduce startup time, try running with or options or reduce the number of runs with filters. try guild tensorboard for filter options. adding or does not help. i suspect this happens because i have a resource to a directory with a lot of files. is it possible to configure guild tensorboard to ignore this directory? i found the o option in command tensorboard but adding o logdir tb did not work . any ideas?",
        "Answer_original_content":"hi @samedii welcome to the new site! theres no easy workaround that i can think of. to me this behavior is arguably a bug. guild is following symlinks to find run files that might be used for various tensorboard plugins (e.g. projections, etc.) and i dont understand why. ill spend some time investigating and look at changing this this. there may be a good reason for the current behavior. either way ill address this in master. 0.7 is frozen - rc11 is the last release candidate barring some world ending bug thats discovered. are you able to compile and run from source? hello i see, thanks for checking! i dont think i had any trouble running from source when i did so previously. i confirmed that the issue was the symbolic link. when i removed it, guild started tb very quickly. i think this issue would be solved for my specific case if i could specify where to look for tb-logs in the guild.yml. i could maybe use -o logdir=dir\/to\/guild\/runs\/id\/tb already but thats quite a hassle -o is meant to pass options along to tensorboard without any knowledge of whats being passed. i think logdir needs to be explicitly ignored (with warning message) as guild takes over the function of setting up and specifying the logdir that tensorboard sees. i see what youre getting at. i agree this should be in the guild file. i hate to complicate things, but given the flexibility of the tool, where the script can put files anywhere it wants, i think the spec would have to follow the line of sourcecode and use guilds file select spec. i think maybe this interface? op: tensorboard: logdir: relpath-to-tb-files in this case logdir is a fully supported file select spec like that used for sourcecode. yes that would work well in my case and i think most people can easily adapt their code to it if they have these kinds of issues i just applied this commit: github.com\/guildai\/guildai don't traverse symlinks when linking to run files for tensorboard committed 02:44pm - 03 jul 20 utc gar1t +2 -2 it's not clear why symlinks were being traversed in the first place. tensorboard should avoid letting linked files in anyway. this may have... if you grab the latest from master that issue should be resolved for you. just make sure youre using guild from source.you can run guild check and look for guild_install_location to confirm its running from the repo and not from an installed package. (you may need to run hash-r to clear your shells cached location of the guild exe.) 3 posts were split to a new topic: issues with guild file - output-scalars and sourcecode",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of Tensorboard taking a long time to startup include removing symbolic links, specifying where to look for tb-logs in the guild.yml, and using the op: tensorboard: logdir: relpath-to-tb-files interface. The issue of symbolic links being traversed has been resolved in the latest version of Guild, which can be obtained by running guild check and confirming that it is running from the repo and not from an installed package.",
        "Answer_preprocessed_content":"hi welcome to the new site! theres no easy workaround that i can think of. to me this behavior is arguably a bug. guild is following symlinks to find run files that might be used for various tensorboard plugins and i dont understand why. ill spend some time investigating and look at changing this this. there may be a good reason for the current behavior. either way ill address this in master. is frozen rc is the last release candidate barring some world ending bug thats discovered. are you able to compile and run from source? hello i see, thanks for checking! i dont think i had any trouble running from source when i did so previously. i confirmed that the issue was the symbolic link. when i removed it, guild started tb very quickly. i think this issue would be solved for my specific case if i could specify where to look for in the i could maybe use o already but thats quite a hassle is meant to pass options along to tensorboard without any knowledge of whats being passed. i think needs to be explicitly ignored as guild takes over the function of setting up and specifying the logdir that tensorboard sees. i see what youre getting at. i agree this should be in the guild file. i hate to complicate things, but given the flexibility of the tool, where the script can put files anywhere it wants, i think the spec would have to follow the line of and use guilds file select spec. i think maybe this interface? in this case is a fully supported file select spec like that used for . yes that would work well in my case and i think most people can easily adapt their code to it if they have these kinds of issues i just applied this commit don't traverse symlinks when linking to run files for tensorboard committed pm jul utc gar t + it's not clear why symlinks were being traversed in the first place. tensorboard should avoid letting linked files in anyway. this may if you grab the latest from master that issue should be resolved for you. just make sure youre using guild from can run and look for to confirm its running from the repo and not from an installed package. posts were split to a new topic issues with guild file and sourcecode"
    },
    {
        "Question_id":71858668.0,
        "Question_title":"How to use tensorflow hub in Azure ML",
        "Question_body":"<p>I am trying to use  TensorFlow Hub in Azure ML Studio<\/p>\n<p>I am using the kernel Python 3.8 PT and TF<\/p>\n<p>And I installed  a few modules:<\/p>\n<pre><code>!pip install bert-for-tf2\n!pip install sentencepiece\n!pip install &quot;tensorflow&gt;=2.0.0&quot;\n!pip install --upgrade tensorflow-hub\n<\/code><\/pre>\n<p>With pip list, I can see they are installed:<\/p>\n<pre><code>tensorflow                              2.8.0\ntensorflow-estimator                    2.3.0\ntensorflow-gpu                          2.3.0\ntensorflow-hub                          0.12.0\ntensorflow-io-gcs-filesystem            0.24.0\n<\/code><\/pre>\n<p>However when I try to use it as per the documentation (<a href=\"https:\/\/www.tensorflow.org\/hub\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/hub<\/a>)<\/p>\n<p>Then I get the classic:<\/p>\n<pre><code>ModuleNotFoundError: No module named 'tensorflow_hub'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1649857871967,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":112.0,
        "Owner_creation_time":1302030303092,
        "Owner_last_access_time":1663332147472,
        "Owner_reputation":30340.0,
        "Owner_up_votes":1667.0,
        "Owner_down_votes":79.0,
        "Owner_views":2937.0,
        "Answer_body":"<p>To resolve this <code>ModuleNotFoundError: No module named 'tensorflow_hub'<\/code>  error, try following ways:<\/p>\n<ul>\n<li>Try installing\/upgrading the latest version of <code>tensorflow<\/code> and <code>tensorflow-hub<\/code> and then import:<\/li>\n<\/ul>\n<pre><code>!pip install --upgrade tensorflow\n\n!pip install --upgrade tensorflow_hub\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n<\/code><\/pre>\n<ul>\n<li>Install the current environment as a new kernel:<\/li>\n<\/ul>\n<pre><code>python3 -m ipykernel install --user --name=testenvironment\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/stackoverflow.com\/questions\/63884339\/modulenotfounderror-no-module-named-tensorflow-hub\">ModuleNotFoundError: No module named 'tensorflow_hub', No module named 'tensorflow_hub'<\/a> and <a href=\"https:\/\/github.com\/tensorflow\/hub\/issues\/767\" rel=\"nofollow noreferrer\">How to use Tensorflow Hub Model?<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650950165912,
        "Answer_score":1.0,
        "Owner_location":"Brussels, B\u00e9lgica",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71858668",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to use tensorflow hub in ; Content: i am trying to use tensorflow hub in studio i am using the kernel python 3.8 pt and tf and i installed a few modules: !pip install bert-for-tf2 !pip install sentencepiece !pip install \"tensorflow>=2.0.0\" !pip install --upgrade tensorflow-hub with pip list, i can see they are installed: tensorflow 2.8.0 tensorflow-estimator 2.3.0 tensorflow-gpu 2.3.0 tensorflow-hub 0.12.0 tensorflow-io-gcs-filesystem 0.24.0 however when i try to use it as per the documentation (https:\/\/www.tensorflow.org\/hub) then i get the classic: modulenotfounderror: no module named 'tensorflow_hub'",
        "Question_original_content_gpt_summary":"The user is encountering challenges with using TensorFlow Hub in Studio, as they are unable to import the module despite having installed it.",
        "Question_preprocessed_content":"Title: how to use tensorflow hub in ; Content: i am trying to use tensorflow hub in studio i am using the kernel python pt and tf and i installed a few modules with pip list, i can see they are installed however when i try to use it as per the documentation then i get the classic",
        "Answer_original_content":"to resolve this modulenotfounderror: no module named 'tensorflow_hub' error, try following ways: try installing\/upgrading the latest version of tensorflow and tensorflow-hub and then import: !pip install --upgrade tensorflow !pip install --upgrade tensorflow_hub import tensorflow as tf import tensorflow_hub as hub install the current environment as a new kernel: python3 -m ipykernel install --user --name=testenvironment you can refer to modulenotfounderror: no module named 'tensorflow_hub', no module named 'tensorflow_hub' and how to use tensorflow hub model?",
        "Answer_original_content_gpt_summary":"Possible solutions to resolve the \"modulenotfounderror: no module named 'tensorflow_hub'\" error include installing\/upgrading the latest version of TensorFlow and TensorFlow Hub, importing the modules using the correct syntax, and installing the current environment as a new kernel. The answer also suggests referring to resources on how to use TensorFlow Hub models.",
        "Answer_preprocessed_content":"to resolve this error, try following ways try the latest version of and and then import install the current environment as a new kernel you can refer to modulenotfounderror no module named no module named and how to use tensorflow hub model?"
    },
    {
        "Question_id":68802388.0,
        "Question_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models when it does?",
        "Question_body":"<p>Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.<\/p>\n<pre><code>create_endpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name1,\n            &quot;VariantName&quot;: model_name1,\n        },\n         {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name2,\n            &quot;VariantName&quot;: model_name2,\n        }\n    ]\n)\n<\/code><\/pre>\n<p>I confirm in the GUI that it in fact has multiple models. I invoke it like this:<\/p>\n<pre><code>response = client.invoke_endpoint(\n    EndpointName=endpoint_name, \n    TargetModel=model_name1,\n    ContentType=&quot;text\/x-libsvm&quot;, \n    Body=payload\n)\n<\/code><\/pre>\n<p>and get this error:<\/p>\n<blockquote>\n<p>ValidationError: An error occurred (ValidationError) when calling the\nInvokeEndpoint operation: Endpoint\nmy-endpoint1 is not a multi-model endpoint\nand does not support target model header.<\/p>\n<\/blockquote>\n<p>The same problem was discussed <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">here<\/a> with no resolution.<\/p>\n<p>How can I invoke a multimodel endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1629114933853,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":247.0,
        "Owner_creation_time":1227171471292,
        "Owner_last_access_time":1664047108080,
        "Owner_reputation":17500.0,
        "Owner_up_votes":463.0,
        "Owner_down_votes":87.0,
        "Owner_views":1561.0,
        "Answer_body":"<p>The answer (see <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">GitHub<\/a> discussion) is that this error message is simply false.<\/p>\n<p>To avoid this error, the model's local filename (usually for the form <code>model_filename.tar.gz<\/code>) must be used, not the model name.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html\" rel=\"nofollow noreferrer\">documentation<\/a> does say this, though it lacks essential detail.<\/p>\n<p>I found <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">this to be the best example<\/a>.  See the last part  of that Notebook, in which <code>invoke_endpoint<\/code> is used (rather than a predictor as used earlier in the Notebook).<\/p>\n<p>As to the location of that model file: This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">Notebook<\/a> says:<\/p>\n<blockquote>\n<p>When creating the Model entity for multi-model endpoints, the container's ModelDataUrl is the S3 prefix where the model\nartifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1629283623048,
        "Answer_score":0.0,
        "Owner_location":"Israel",
        "Question_last_edit_time":1629119102992,
        "Answer_last_edit_time":1629959666336,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68802388",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why do i get an error that endpoint does not have multiple models when it does?; Content: invoking a multimodel endpoint, i get an error that it is not multimodel. i create it like this. create_endpoint_config_response = client.create_endpoint_config( endpointconfigname=endpoint_config_name, productionvariants=[ { \"instancetype\": \"ml.m5.large\", \"initialvariantweight\": 0.5, \"initialinstancecount\": 1, \"modelname\": model_name1, \"variantname\": model_name1, }, { \"instancetype\": \"ml.m5.large\", \"initialvariantweight\": 0.5, \"initialinstancecount\": 1, \"modelname\": model_name2, \"variantname\": model_name2, } ] ) i confirm in the gui that it in fact has multiple models. i invoke it like this: response = client.invoke_endpoint( endpointname=endpoint_name, targetmodel=model_name1, contenttype=\"text\/x-libsvm\", body=payload ) and get this error: validationerror: an error occurred (validationerror) when calling the invokeendpoint operation: endpoint my-endpoint1 is not a multi-model endpoint and does not support target model header. the same problem was discussed here with no resolution. how can i invoke a multimodel endpoint?",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to invoke a multi-model endpoint, despite confirming that the endpoint has multiple models in the GUI.",
        "Question_preprocessed_content":"Title: why do i get an error that endpoint does not have multiple models when it does?; Content: invoking a multimodel endpoint, i get an error that it is not multimodel. i create it like this. i confirm in the gui that it in fact has multiple models. i invoke it like this and get this error validationerror an error occurred when calling the invokeendpoint operation endpoint is not a endpoint and does not support target model header. the same problem was discussed here with no resolution. how can i invoke a multimodel endpoint?",
        "Answer_original_content":"the answer (see github discussion) is that this error message is simply false. to avoid this error, the model's local filename (usually for the form model_filename.tar.gz) must be used, not the model name. the documentation does say this, though it lacks essential detail. i found this to be the best example. see the last part of that notebook, in which invoke_endpoint is used (rather than a predictor as used earlier in the notebook). as to the location of that model file: this notebook says: when creating the model entity for multi-model endpoints, the container's modeldataurl is the s3 prefix where the model artifacts that are invokable by the endpoint are located. the rest of the s3 path will be specified when invoking the model.",
        "Answer_original_content_gpt_summary":"Possible solutions to the error encountered when invoking a multi-model endpoint are to use the model's local filename instead of the model name and to specify the rest of the S3 path when invoking the model. The documentation lacks essential detail, but an example notebook is provided for reference. When creating the model entity for multi-model endpoints, the container's modeldataurl should be set to the S3 prefix where the model artifacts are located.",
        "Answer_preprocessed_content":"the answer is that this error message is simply false. to avoid this error, the model's local filename must be used, not the model name. the documentation does say this, though it lacks essential detail. i found this to be the best example. see the last part of that notebook, in which is used . as to the location of that model file this notebook says when creating the model entity for endpoints, the container's modeldataurl is the s prefix where the model artifacts that are invokable by the endpoint are located. the rest of the s path will be specified when invoking the model."
    },
    {
        "Question_id":null,
        "Question_title":"Greengrass for data processing and ML model training",
        "Question_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1556295399000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-04-26T16:21:21.000Z",
                "Answer_score":0,
                "Answer_body":"As a native service offering, Greengrass has support for deploying models to the edge and running inference code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: greengrass for data processing and ml model training; Content: is it possible to train and deploy ml models in greengrass? or is greengrass limited to inference while training is done using in cloud?",
        "Question_original_content_gpt_summary":"The user is wondering if it is possible to use AWS Greengrass for both data processing and ML model training, or if it is limited to inference while training is done in the cloud.",
        "Question_preprocessed_content":"Title: greengrass for data processing and ml model training; Content: is it possible to train and deploy ml models in greengrass? or is greengrass limited to inference while training is done using in cloud?",
        "Answer_original_content":"as a native service offering, greengrass has support for deploying models to the edge and running inference code against those models. nothing prevents you from deploying your own code to the edge that would train a model, but i suspect you wouldn't be able to store it as a greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to gg.",
        "Answer_original_content_gpt_summary":"The answer suggests that AWS Greengrass has native support for deploying ML models to the edge and running inference code against those models. However, it is possible to deploy custom code to the edge for training models, but it may not be possible to store them as a local resource for later inferences without a round trip to the cloud and redeploying to Greengrass.",
        "Answer_preprocessed_content":"as a native service offering, greengrass has support for deploying models to the edge and running inference code against those models. nothing prevents you from deploying your own code to the edge that would train a model, but i suspect you wouldn't be able to store it as a greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to gg."
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML 'Designer': how to view logistic regression model coefficients \/ intercept",
        "Question_body":"Using Azure ML Designer it is easy to create a model using the Two-Class Logistic Regression & Train Model components. However it does not seem to be possible to view the regression coefficients \/ intercept (ie. the weights applied to the feature values within the model). How can we go about viewing the model coefficients? Are they stored in one of the Train Model output files (eg. data.ilearner) in a way that can be viewed \/ exported to a human readable format?\n\nNote: this question relates to the Azure Machine Learning Studio (not the older 'classic' version where I believe it was possible to 'right-click' and visualise the model coefficients).",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1607433503623,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/190478\/azure-ml-39designer39-how-to-view-logistic-regress.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-09T05:26:10.807Z",
                "Answer_score":0,
                "Answer_body":"Thanks for reaching out. These are the metrics reported when evaluating binary classification models. You can view the results by clicking evaluate model > visualize > evaluation results. Hope this helps.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-05T20:37:41.743Z",
                "Answer_score":0,
                "Answer_body":"Any update on this? Is it still not possible to get the coefficients for the trained linear regression model?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: 'designer': how to view logistic regression model coefficients \/ intercept; Content: using designer it is easy to create a model using the two-class logistic regression & train model components. however it does not seem to be possible to view the regression coefficients \/ intercept (ie. the weights applied to the feature values within the model). how can we go about viewing the model coefficients? are they stored in one of the train model output files (eg. data.ilearner) in a way that can be viewed \/ exported to a human readable format? note: this question relates to the studio (not the older 'classic' version where i believe it was possible to 'right-click' and visualise the model coefficients).",
        "Question_original_content_gpt_summary":"The user is trying to view the logistic regression model coefficients and intercepts in Designer, but is unable to find a way to do so.",
        "Question_preprocessed_content":"Title: 'designer' how to view logistic regression model coefficients \/ intercept; Content: using designer it is easy to create a model using the logistic regression & train model components. however it does not seem to be possible to view the regression coefficients \/ intercept . how can we go about viewing the model coefficients? are they stored in one of the train model output files in a way that can be viewed \/ exported to a human readable format? note this question relates to the studio .",
        "Answer_original_content":"thanks for reaching out. these are the metrics reported when evaluating binary classification models. you can view the results by clicking evaluate model > visualize > evaluation results. hope this helps. any update on this? is it still not possible to get the coefficients for the trained linear regression model?",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Click on \"Evaluate Model\" > \"Visualize\" > \"Evaluation Results\" to view the logistic regression model coefficients and intercepts in Designer. \n\nSummary: \nTo view the logistic regression model coefficients and intercepts in Designer, the user can click on \"Evaluate Model\" > \"Visualize\" > \"Evaluation Results\".",
        "Answer_preprocessed_content":"thanks for reaching out. these are the metrics reported when evaluating binary classification models. you can view the results by clicking evaluate model visualize evaluation results. hope this helps. any update on this? is it still not possible to get the coefficients for the trained linear regression model?"
    },
    {
        "Question_id":null,
        "Question_title":"Will Google provide MTQP in Cloud Translation API?",
        "Question_body":"Hi, I discovered with interest that your Google Translation Hub advanced tier offers document post-editing features, and, as part of that, includes an MTQP quality prediction score on a segment by segment basis. \n\nThis would be a very interesting feature to include in Cloud Translation API, particularly for TMS and CAT tools like Trados\/MemoQ\/Memsource that could then provide that information to the translator, similar to what a fuzzy match is for traditional Translation Memory technology.\n\nIt could also be very useful to decide whether a raw machine translation process (without review) is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality profile.So my question is whether Google is looking at making this available in the API, or whether Google is treating that as proprietary information that you guys do not want to make available outside of your Google Translation Hub?  I really hope the answer is the former, not the latter...\n\nThank you.\n\nMichel Farhi\nPrincipal Localization Engineer\nNI (formerly National Instruments)",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1666700820000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":41.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Will-Google-provide-MTQP-in-Cloud-Translation-API\/td-p\/482115\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-25T12:27:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\u00a0\n\nI discovered with interest that your Google Translation Hub advanced tier offers document post-editing features, and, as part of that, includes an MTQP quality prediction score on a segment by segment basis.\u00a0\n\nThis would be a very interesting feature to include in Cloud Translation API, particularly for TMS and CAT tools like Trados\/MemoQ\/Memsource that could then provide that information to the translator, similar to what a fuzzy match is for traditional Translation Memory technology.\n\nIt could also be very useful to decide whether a raw machine translation process (without review) is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality profile.\n\nSo my question is whether Google is looking at making this available in the API, or whether Google is treating that as proprietary information that you guys do not want to make available outside of your Google Translation Hub?\u00a0 I really hope the answer is the former, not the latter...\n\nThank you.\n\nMichel Farhi\nPrincipal Localization Engineer\nNI (formerly National Instruments)"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: will google provide mtqp in cloud translation api?; Content: hi, i discovered with interest that your google translation hub advanced tier offers document post-editing features, and, as part of that, includes an mtqp quality prediction score on a segment by segment basis. this would be a very interesting feature to include in cloud translation api, particularly for tms and cat tools like trados\/memoq\/memsource that could then provide that information to the translator, similar to what a fuzzy match is for traditional translation memory technology. it could also be very useful to decide whether a raw machine translation process (without review) is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality profile.so my question is whether google is looking at making this available in the api, or whether google is treating that as proprietary information that you guys do not want to make available outside of your google translation hub? i really hope the answer is the former, not the latter... thank you. michel farhi principal localization engineer ni (formerly national instruments)",
        "Question_original_content_gpt_summary":"The user is inquiring about the possibility of Google providing MTQP in Cloud Translation API to enable TMS and CAT tools to provide quality prediction scores to translators.",
        "Question_preprocessed_content":"Title: will google provide mtqp in cloud translation api?; Content: hi, i discovered with interest that your google translation hub advanced tier offers document features, and, as part of that, includes an mtqp quality prediction score on a segment by segment basis. this would be a very interesting feature to include in cloud translation api, particularly for tms and cat tools like that could then provide that information to the translator, similar to what a fuzzy match is for traditional translation memory technology. it could also be very useful to decide whether a raw machine translation process is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality my question is whether google is looking at making this available in the api, or whether google is treating that as proprietary information that you guys do not want to make available outside of your google translation hub? i really hope the answer is the former, not the thank you. michel farhi principal localization engineer ni",
        "Answer_original_content":"hi, i discovered with interest that your google translation hub advanced tier offers document post-editing features, and, as part of that, includes an mtqp quality prediction score on a segment by segment basis. this would be a very interesting feature to include in cloud translation api, particularly for tms and cat tools like trados\/memoq\/memsource that could then provide that information to the translator, similar to what a fuzzy match is for traditional translation memory technology. it could also be very useful to decide whether a raw machine translation process (without review) is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality profile. so my question is whether google is looking at making this available in the api, or whether google is treating that as proprietary information that you guys do not want to make available outside of your google translation hub? i really hope the answer is the former, not the latter... thank you. michel farhi principal localization engineer ni (formerly national instruments)",
        "Answer_original_content_gpt_summary":"The user is asking if Google can provide MTQP in Cloud Translation API to enable TMS and CAT tools to provide quality prediction scores to translators. The answer suggests that Google's Translation Hub Advanced Tier offers document post-editing features, including an MTQP quality prediction score on a segment by segment basis. This feature could be useful for TMS and CAT tools to provide information to the translator, similar to what a fuzzy match is for traditional translation memory technology. The answer does not provide a clear solution but suggests that the user's request is interesting and hopes that Google will make it available in the API.",
        "Answer_preprocessed_content":"hi, i discovered with interest that your google translation hub advanced tier offers document features, and, as part of that, includes an mtqp quality prediction score on a segment by segment basis. this would be a very interesting feature to include in cloud translation api, particularly for tms and cat tools like that could then provide that information to the translator, similar to what a fuzzy match is for traditional translation memory technology. it could also be very useful to decide whether a raw machine translation process is suitable for a document, or to identify the few segments that absolutely must go through human editing depending on your acceptable quality profile. so my question is whether google is looking at making this available in the api, or whether google is treating that as proprietary information that you guys do not want to make available outside of your google translation hub? i really hope the answer is the former, not the thank you. michel farhi principal localization engineer ni"
    },
    {
        "Question_id":null,
        "Question_title":"S3 remote permissions and integrity best practices",
        "Question_body":"<p>Does anyone have recommendations about best practices for S3 remote set-up?<\/p>\n<p>I\u2019m specifically interested if there are thoughts around permissions and integrity of data in S3. To enable S3, remotes users would be granted read &amp; write permissions. As long as users use the dvc tooling it appears the data should relatively save. However, granting permissions would allow direct access outside of dvc tooling. This seems to open the possibility of a scenario where the data history could be corrupted.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1553787278231,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1674.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/s3-remote-permissions-and-integrity-best-practices\/165",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-03-28T21:04:30.222Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/meby\">@meby<\/a>!<\/p>\n<p>Great question! All data history is stored in your git repo. Dvc remote only stores the data itself, which is stored under checksum names, so unless you are directly uploading a file with incorrect checksum(which shouldn\u2019t happen with dvc, since it checks that cache files are not corrupted before uploading them) you should be fine. To help mitigate the risk, you might want to create separate creds for each team member specifically to use them with dvc s3 remote. Also, using different buckets on s3 for different dvc projects is also a good idea when combined with per-project-per-team-member creds. And, of course, backups might be a good idea as well <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> If you are using dvc pipelines too, then you should be able to reproduce all other data from your source data in single <code>dvc repro<\/code> command, so that adds an additional level of assurance even if you do lose your data <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":103.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-03-29T16:38:22.822Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>,<\/p>\n<p>Thanks for the quick reply. I like the suggestion per project and per team creds.<\/p>\n<p>I also like the ability to reproduce from source data if intermediates are lost. One of my main concerns of course is not losing the source data and also ensuring that we don\u2019t lose the final generated artifacts that would be deployed to production. One thing we\u2019re evaluating is whether we can configure DVC &amp; S3 so that we can store those in S3 or if we should rely on external archiving where a majority of users would be granted only read access.<\/p>\n<p>With S3, I believe we could turn on versioning at the bucket level and not grant <code>s3:DeleteObject<\/code> permissions. This would ensure users could create and update objects but never delete and we\u2019d have a version history for all updates if we needed to roll back.<\/p>\n<p>The DVC <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/remote-add#options\" rel=\"nofollow noopener\">docs for S3<\/a> list <code>s3:DeleteObject<\/code> as a required permission. Is this necessary for \u201cevery day\u201d workflows?<\/p>\n<p>Specifically, I\u2019m trying to understand how this relates to my mental model of git history. Once a specific commit hash is in the repo it is there forever unless you\u2019re rewriting the history which is not an \u201cevery day\u201d workflow.<\/p>\n<p>Thanks,<br>\nMatt<\/p>",
                "Answer_score":103.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-03-31T11:15:19.766Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/meby\">@meby<\/a>  !<\/p>\n<p>s3:DeleteObject is not required, unless you are using <code>dvc gc<\/code> (garbage collector), so your proposed way of not granting s3:DeleteObject would definitely work for day-to-day workflow <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":22.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: s3 remote permissions and integrity best practices; Content: does anyone have recommendations about best practices for s3 remote set-up? i\u2019m specifically interested if there are thoughts around permissions and integrity of data in s3. to enable s3, remotes users would be granted read & write permissions. as long as users use the tooling it appears the data should relatively save. however, granting permissions would allow direct access outside of tooling. this seems to open the possibility of a scenario where the data history could be corrupted.",
        "Question_original_content_gpt_summary":"The user is looking for best practices for setting up S3 remotes, with a focus on permissions and data integrity, to ensure that data remains secure and uncorrupted.",
        "Question_preprocessed_content":"Title: s remote permissions and integrity best practices; Content: does anyone have recommendations about best practices for s remote im specifically interested if there are thoughts around permissions and integrity of data in s . to enable s , remotes users would be granted read & write permissions. as long as users use the tooling it appears the data should relatively save. however, granting permissions would allow direct access outside of tooling. this seems to open the possibility of a scenario where the data history could be corrupted.",
        "Answer_original_content":"hi @meby! great question! all data history is stored in your git repo. remote only stores the data itself, which is stored under checksum names, so unless you are directly uploading a file with incorrect checksum(which shouldnt happen with , since it checks that cache files are not corrupted before uploading them) you should be fine. to help mitigate the risk, you might want to create separate creds for each team member specifically to use them with s3 remote. also, using different buckets on s3 for different projects is also a good idea when combined with per-project-per-team-member creds. and, of course, backups might be a good idea as well if you are using pipelines too, then you should be able to reproduce all other data from your source data in single repro command, so that adds an additional level of assurance even if you do lose your data thanks, ruslan hi @kupruser, thanks for the quick reply. i like the suggestion per project and per team creds. i also like the ability to reproduce from source data if intermediates are lost. one of my main concerns of course is not losing the source data and also ensuring that we dont lose the final generated artifacts that would be deployed to production. one thing were evaluating is whether we can configure & s3 so that we can store those in s3 or if we should rely on external archiving where a majority of users would be granted only read access. with s3, i believe we could turn on versioning at the bucket level and not grant s3:deleteobject permissions. this would ensure users could create and update objects but never delete and wed have a version history for all updates if we needed to roll back. the docs for s3 list s3:deleteobject as a required permission. is this necessary for every day workflows? specifically, im trying to understand how this relates to my mental model of git history. once a specific commit hash is in the repo it is there forever unless youre rewriting the history which is not an every day workflow. thanks, matt hi @meby ! s3:deleteobject is not required, unless you are using gc (garbage collector), so your proposed way of not granting s3:deleteobject would definitely work for day-to-day workflow thanks, ruslan",
        "Answer_original_content_gpt_summary":"Possible solutions for setting up S3 remotes with a focus on permissions and data integrity include creating separate credentials for each team member, using different buckets on S3 for different projects, enabling versioning at the bucket level, and not granting s3:deleteobject permissions unless using garbage collector. Backups and reproducing from source data can also add an additional level of assurance.",
        "Answer_preprocessed_content":"hi great question! all data history is stored in your git repo. remote only stores the data itself, which is stored under checksum names, so unless you are directly uploading a file with incorrect checksum you should be fine. to help mitigate the risk, you might want to create separate creds for each team member specifically to use them with s remote. also, using different buckets on s for different projects is also a good idea when combined with creds. and, of course, backups might be a good idea as well if you are using pipelines too, then you should be able to reproduce all other data from your source data in single command, so that adds an additional level of assurance even if you do lose your data thanks, ruslan hi thanks for the quick reply. i like the suggestion per project and per team creds. i also like the ability to reproduce from source data if intermediates are lost. one of my main concerns of course is not losing the source data and also ensuring that we dont lose the final generated artifacts that would be deployed to production. one thing were evaluating is whether we can configure & s so that we can store those in s or if we should rely on external archiving where a majority of users would be granted only read access. with s , i believe we could turn on versioning at the bucket level and not grant permissions. this would ensure users could create and update objects but never delete and wed have a version history for all updates if we needed to roll back. the docs for s list as a required permission. is this necessary for every day workflows? specifically, im trying to understand how this relates to my mental model of git history. once a specific commit hash is in the repo it is there forever unless youre rewriting the history which is not an every day workflow. thanks, matt hi ! s deleteobject is not required, unless you are using , so your proposed way of not granting s deleteobject would definitely work for workflow thanks, ruslan"
    },
    {
        "Question_id":null,
        "Question_title":"AutoML: problem with univariate time series forecasting",
        "Question_body":"I'm having troubles generating univariate time series forecasts with Azure Automated Machine Learning (I know...).\n\nWhat I'm doing\n\nSo I have about 5 years worth of monthly observations in a dataframe that looks like this:\n\n\tdate\ttarget_value\t\n\t2015-02-01\t123\t\n\t2015-03-01\t456\t\n\t2015-04-01\t789\t\n\t...\t...\t\n\nI want to forecast target_value based on past values of target_value, i.e. univariate forecasting like ARIMA for instance.\nSo I am setting up the AutoML forecast like this:\n\n\n\n # that's the dataframe as shown above\n train_data = Dataset.Tabular.from_delimited_files(path=datastore.path(my_remote_filename))\n    \n # ...other code...\n    \n forecasting_parameters = ForecastingParameters(\n     time_column_name='date',\n     forecast_horizon=2,\n     target_lags='auto',\n     freq='MS'\n )\n    \n automl_config = AutoMLConfig(task='forecasting',\n                              debug_log='automl_forecasting_function.log',\n                              primary_metric='normalized_root_mean_squared_error',\n                              enable_dnn=True,\n                              experiment_timeout_hours=8.0,\n                              enable_early_stopping=True,\n                              training_data=train_data,\n                              compute_target='my-cluster',\n                              n_cross_validations=3,\n                              verbosity=logging.INFO,\n                              max_concurrent_iterations=4,\n                              max_cores_per_iteration=-1,\n                              label_column_name='target_value',\n                              forecasting_parameters=forecasting_parameters)\n\n\n\nWhat the problem is\n\nBut AutoML does not seem to generate the forecast for target_value based on past values of target_value. It seems to use the date column as the independent variable!\nThe feature importance chart also shows date as the input feature:\n\nAs a side note: running multivariate forecasts works fine.\nWhen I use a dataset like this, feature_1 and feature_2 are used (i.e. as the X) to forecast target_value (i.e. the y)\n\n\tdate\tfeature_1\tfeature_2\ttarget_value\t\n\t2015-02-01\t10\t7\t123\t\n\t2015-03-01\t30\t2\t456\t\n\t2015-04-01\t20\t5\t789\t\n\t...\t...\t...\t...\t\n\nMy questions therefore\nHow do I need to set up a univariate AutoML forecast to forecast target_value based on past observations target_value?\nI assumed generating lagged values for target_value etc. is exactly what AutoML is supposed to do.\n\nThanks!",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1617739313070,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/346598\/automl-problem-with-univariate-time-series-forecas.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-08T04:12:36.817Z",
                "Answer_score":0,
                "Answer_body":"@movingabout-2877 Thanks, AutoML does use the date column as an independent variable. We engineer several features from it, this is a standard practice for learning seasonal patterns. In the given scenario the date column will be featurized to represent 'day', 'month', 'day of week' etc. This is done to train regression-based model on this data, which will use the generated columns for prediction.\n\nPlease remove the target_lags='auto' to allow selection of Arima. We have to block certain models (e.g. Arima) when the target lags are set. This is a product gap that we're in the process of fixing.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: automl: problem with univariate time series forecasting; Content: i'm having troubles generating univariate time series forecasts with azure automated machine learning (i know...). what i'm doing so i have about 5 years worth of monthly observations in a dataframe that looks like this: date target_value 2015-02-01 123 2015-03-01 456 2015-04-01 789 ... ... i want to forecast target_value based on past values of target_value, i.e. univariate forecasting like arima for instance. so i am setting up the automl forecast like this: # that's the dataframe as shown above train_data = dataset.tabular.from_delimited_files(path=datastore.path(my_remote_filename)) # ...other code... forecasting_parameters = forecastingparameters( time_column_name='date', forecast_horizon=2, target_lags='auto', freq='ms' ) automl_config = automlconfig(task='forecasting', debug_log='automl_forecasting_function.log', primary_metric='normalized_root_mean_squared_error', enable_dnn=true, experiment_timeout_hours=8.0, enable_early_stopping=true, training_data=train_data, compute_target='my-cluster', n_cross_validations=3, verbosity=logging.info, max_concurrent_iterations=4, max_cores_per_iteration=-1, label_column_name='target_value', forecasting_parameters=forecasting_parameters) what the problem is but automl does not seem to generate the forecast for target_value based on past values of target_value. it seems to use the date column as the independent variable! the feature importance chart also shows date as the input feature: as a side note: running multivariate forecasts works fine. when i use a dataset like this, feature_1 and feature_2 are used (i.e. as the x) to forecast target_value (i.e. the y) date feature_1 feature_2 target_value 2015-02-01 10 7 123 2015-03-01 30 2 456 2015-04-01 20 5 789 ... ... ... ... my questions therefore how do i need to set up a univariate automl forecast to forecast target_value based on past observations target_value? i assumed generating lagged values for target_value etc. is exactly what automl is supposed to do. thanks!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with generating univariate time series forecasts with Azure Automated Machine Learning, as it appears to be using the date column as the independent variable instead of past values of the target_value.",
        "Question_preprocessed_content":"Title: automl problem with univariate time series forecasting; Content: i'm having troubles generating univariate time series forecasts with azure automated machine learning . what i'm doing so i have about years worth of monthly observations in a dataframe that looks like this date i want to forecast based on past values of univariate forecasting like arima for instance. so i am setting up the automl forecast like this that's the dataframe as shown above forecastingparameters automlconfig what the problem is but automl does not seem to generate the forecast for based on past values of it seems to use the date column as the independent variable! the feature importance chart also shows date as the input feature as a side note running multivariate forecasts works fine. when i use a dataset like this, and are used to forecast date my questions therefore how do i need to set up a univariate automl forecast to forecast based on past observations i assumed generating lagged values for etc. is exactly what automl is supposed to do. thanks!",
        "Answer_original_content":"@movingabout-2877 thanks, automl does use the date column as an independent variable. we engineer several features from it, this is a standard practice for learning seasonal patterns. in the given scenario the date column will be featurized to represent 'day', 'month', 'day of week' etc. this is done to train regression-based model on this data, which will use the generated columns for prediction. please remove the target_lags='auto' to allow selection of arima. we have to block certain models (e.g. arima) when the target lags are set. this is a product gap that we're in the process of fixing.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n- Azure Automated Machine Learning uses the date column as an independent variable and engineers several features from it to learn seasonal patterns.\n- The date column will be featurized to represent 'day', 'month', 'day of week', etc. to train a regression-based model on this data, which will use the generated columns for prediction.\n- To allow selection of ARIMA, remove the target_lags='auto' as certain models (e.g. ARIMA) are blocked when the target lags are set.\n- There is a product gap that Azure Automated Machine Learning is in the process of fixing.",
        "Answer_preprocessed_content":"thanks, automl does use the date column as an independent variable. we engineer several features from it, this is a standard practice for learning seasonal patterns. in the given scenario the date column will be featurized to represent 'day', 'month', 'day of week' etc. this is done to train model on this data, which will use the generated columns for prediction. please remove the to allow selection of arima. we have to block certain models when the target lags are set. this is a product gap that we're in the process of fixing."
    },
    {
        "Question_id":null,
        "Question_title":"DVC + Github Actions + GCP Storage",
        "Question_body":"<p>I\u2019m trying triggering a pipeline to run DVC and download the data from GCP Storage but the log of GitHub Actions returns the following error:<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object., 401\n<\/code><\/pre>\n<p>I think this happens due to giving the right permissions to the Service Account but the one that I\u2019m using has the <em><strong>Storage Object Viewer<\/strong><\/em>, which gives the permission I need.<\/p>\n<p>Here is part of my pipeline file<\/p>\n<pre><code class=\"lang-auto\">- name: Setup Cloud SDK\n  uses: google-github-actions\/setup-gcloud@v0.2.0\n  with:\n    project_id: ${{ secrets.GCP_PROJECT }}\n    service_account_key: ${{ secrets.GCP_KEY }}\n    export_default_credentials: true\n\n- name: CML Run\n  shell: bash\n  env:\n    repo_token: ${{ secrets.GITHUB_TOKEN }}\n    GOOGLE_APPLICATION_CREDENTIALS: ${{ secrets.GCP_KEY }}\n  run: |\n    # run-cache and reproduce pipeline\n    dvc remote add -d -f myremote gs:\/\/myproject\/\n    dvc pull mypath\/data.csv.zip.dvc\n    dvc repro -m\n    \n    # Report metrics\n    echo \"## Metrics\" &gt;&gt; report.md\n    git fetch --prune\n    dvc metrics diff main --show-md &gt;&gt; report.md\n    \n    # Publish confusion matrix diff\n    echo -e \"## Plots\\n### Confusion Matrix\" &gt;&gt; report.md\n    cml-publish $PWD\/mypath\/reports\/confusion-matrix.png --md &gt;&gt; report.md\n    cml-send-comment report.md\n<\/code><\/pre>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_creation_time":1628712122880,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":675.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-github-actions-gcp-storage\/840",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-12T08:34:26.314Z",
                "Answer_body":"<p>Can you please share the verbose traceback about the run of the command that is raising that error? There is also a possibility that the google storage backend we use doesn\u2019t recognize the login method and fallbacking to anonymous authentication, so setting the <code>credentialpath<\/code> to the tokenfile might work (via <code>dvc remote modify --local<\/code>).<\/p>",
                "Answer_score":21.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-12T12:34:35.519Z",
                "Answer_body":"<p>Sure! Here is the traceback<\/p>\n<pre><code class=\"lang-auto\">Run # run-cache and reproduce pipeline\nSetting 'mlops-talks' as a default remote.\n_request non-retriable exception: Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object., 401\nTraceback (most recent call last):\n  File \"\/home\/runner\/.local\/lib\/python3.8\/site-packages\/gcsfs\/retry.py\", line 110, in retry_request\n    return await func(*args, **kwargs)\n  File \"\/home\/runner\/.local\/lib\/python3.8\/site-packages\/gcsfs\/core.py\", line 332, in _request\n    validate_response(status, contents, path)\n  File \"\/home\/runner\/.local\/lib\/python3.8\/site-packages\/gcsfs\/retry.py\", line 97, in validate_response\n    raise HttpError(error)\ngcsfs.retry.HttpError: Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object., 401\nERROR: unexpected error - Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object., 401\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\nError: Process completed with exit code 255.\n<\/code><\/pre>",
                "Answer_score":16.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-12T13:31:55.458Z",
                "Answer_body":"<p>It seems like it is possible that <code>gcsfs<\/code> is not recognizing your credentials and falling back to the <code>anonymous<\/code> login. Can you try to do the same with setting the <code>credentialpath<\/code>  to your service account file and test it again?<\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-12T14:01:25.728Z",
                "Answer_body":"<p>Sure! I did this but here is another error:<\/p>\n<pre><code class=\"lang-auto\">ERROR: unrecognized arguments: *** *** *** *** *** *** *** *** *** *** ***\nusage: dvc remote modify [-h] [--global | --system | --project | --local]\n                         [-q | -v] [-u]\n                         name option [value]\n<\/code><\/pre>\n<p>Here it is with the <code>credentialpath<\/code><\/p>\n<pre><code class=\"lang-auto\">dvc remote modify --local myremote credentialpath $GOOGLE_APPLICATION_CREDENTIALS\n<\/code><\/pre>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-12T17:46:00.368Z",
                "Answer_body":"<p><code>credentialpath<\/code> takes a path where the credentials file is stored, from what I understand the <code>$GOOGLE_APPLKICATION_CREDENTIALS<\/code> is the contents of that file, so you should probably write it off to a temporary file (e.g <code>\/tmp\/creds.json<\/code>) and then pass that file as the argument<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-12T17:48:31.918Z",
                "Answer_body":"<p>Well, I\u2019m doing this using GitHub Actions, that\u2019s why I\u2019m using that <code>$GOOGLE_APPLICATION_CREDENTIALS<\/code> which is a secret with the contents of that json file. Can I use that configure the credentials using github secrets?<\/p>",
                "Answer_score":26.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: + github actions + gcp storage; Content: i\u2019m trying triggering a pipeline to run and download the data from gcp storage but the log of github actions returns the following error: error: unexpected error - anonymous caller does not have storage.objects.get access to the google cloud storage object., 401 i think this happens due to giving the right permissions to the service account but the one that i\u2019m using has the storage object viewer, which gives the permission i need. here is part of my pipeline file - name: setup cloud sdk uses: google-github-actions\/setup-gcloud@v0.2.0 with: project_id: ${{ secrets.gcp_project }} service_account_key: ${{ secrets.gcp_key }} export_default_credentials: true - name: cml run shell: bash env: repo_token: ${{ secrets.github_token }} google_application_credentials: ${{ secrets.gcp_key }} run: | # run-cache and reproduce pipeline remote add -d -f myremote gs:\/\/myproject\/ pull mypath\/data.csv.zip. repro -m # report metrics echo \"## metrics\" >> report.md git fetch --prune metrics diff main --show-md >> report.md # publish confusion matrix diff echo -e \"## plots\\n### confusion matrix\" >> report.md cml-publish $pwd\/mypath\/reports\/confusion-matrix.png --md >> report.md cml-send-comment report.md",
        "Question_original_content_gpt_summary":"The user is encountering challenges with triggering a pipeline to run and download data from GCP Storage, due to not having the correct permissions for the service account.",
        "Question_preprocessed_content":"Title: + github actions + gcp storage; Content: im trying triggering a pipeline to run and download the data from gcp storage but the log of github actions returns the following error i think this happens due to giving the right permissions to the service account but the one that im using has the storage object viewer, which gives the permission i need. here is part of my pipeline file",
        "Answer_original_content":"can you please share the verbose traceback about the run of the command that is raising that error? there is also a possibility that the google storage backend we use doesnt recognize the login method and fallbacking to anonymous authentication, so setting the credentialpath to the tokenfile might work (via remote modify --local). sure! here is the traceback run # run-cache and reproduce pipeline setting 'mlops-talks' as a default remote. _request non-retriable exception: anonymous caller does not have storage.objects.get access to the google cloud storage object., 401 traceback (most recent call last): file \"\/home\/runner\/.local\/lib\/python3.8\/site-packages\/gcsfs\/retry.py\", line 110, in retry_request return await func(*args, **kwargs) file \"\/home\/runner\/.local\/lib\/python3.8\/site-packages\/gcsfs\/core.py\", line 332, in _request validate_response(status, contents, path) file \"\/home\/runner\/.local\/lib\/python3.8\/site-packages\/gcsfs\/retry.py\", line 97, in validate_response raise httperror(error) gcsfs.retry.httperror: anonymous caller does not have storage.objects.get access to the google cloud storage object., 401 error: unexpected error - anonymous caller does not have storage.objects.get access to the google cloud storage object., 401 having any troubles? hit us up at https:\/\/.org\/support, we are always happy to help! error: process completed with exit code 255. it seems like it is possible that gcsfs is not recognizing your credentials and falling back to the anonymous login. can you try to do the same with setting the credentialpath to your service account file and test it again? sure! i did this but here is another error: error: unrecognized arguments: *** *** *** *** *** *** *** *** *** *** *** usage: remote modify [-h] [--global | --system | --project | --local] [-q | -v] [-u] name option [value] here it is with the credentialpath remote modify --local myremote credentialpath $google_application_credentials credentialpath takes a path where the credentials file is stored, from what i understand the $google_applkication_credentials is the contents of that file, so you should probably write it off to a temporary file (e.g \/tmp\/creds.json) and then pass that file as the argument well, im doing this using github actions, thats why im using that $google_application_credentials which is a secret with the contents of that json file. can i use that configure the credentials using github secrets?",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. Share the verbose traceback about the run of the command that is raising that error.\n2. Set the credential path to the token file via remote modify --local.\n3. Try to do the same with setting the credential path to your service account file and test it again.\n4. Configure the credentials using Github secrets.",
        "Answer_preprocessed_content":"can you please share the verbose traceback about the run of the command that is raising that error? there is also a possibility that the google storage backend we use doesnt recognize the login method and fallbacking to anonymous authentication, so setting the to the tokenfile might work . sure! here is the traceback it seems like it is possible that is not recognizing your credentials and falling back to the login. can you try to do the same with setting the to your service account file and test it again? sure! i did this but here is another error here it is with the takes a path where the credentials file is stored, from what i understand the is the contents of that file, so you should probably write it off to a temporary file and then pass that file as the argument well, im doing this using github actions, thats why im using that which is a secret with the contents of that json file. can i use that configure the credentials using github secrets?"
    },
    {
        "Question_id":null,
        "Question_title":"WandB icevision not showing prediction",
        "Question_body":"<p>Hello everyone,<\/p>\n<p>I am using WanbB in IceVision through the fastai integration, you could have a look here for <a href=\"https:\/\/airctic.com\/0.12.0\/wandb_efficientdet\/\" rel=\"noopener nofollow ugc\">IceVision WandB<\/a> but i get the following message:<\/p>\n<pre><code class=\"lang-auto\">Could not gather input dimensions\nWandbCallback was not able to prepare a DataLoader for logging prediction samples -&gt; 'Dataset' object has no attribute 'items'\n<\/code><\/pre>\n<p>Any help appreciated.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1658141609792,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":166.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-icevision-not-showing-prediction\/2767",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-20T17:21:42.676Z",
                "Answer_body":"<p>Hi Fabio, can you give me more information on your case? For example, how are you implementing IceVision WandB and what tools are you using when you do so?<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-25T21:48:48.036Z",
                "Answer_body":"<p>Hi Fabio, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-16T10:54:22.351Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: icevision not showing prediction; Content: hello everyone, i am using wanbb in icevision through the fastai integration, you could have a look here for icevision but i get the following message: could not gather input dimensions callback was not able to prepare a dataloader for logging prediction samples -> 'dataset' object has no attribute 'items' any help appreciated.",
        "Question_original_content_gpt_summary":"The user encountered an issue with icevision not showing predictions due to a 'dataset' object not having an attribute 'items'.",
        "Question_preprocessed_content":"Title: icevision not showing prediction; Content: hello everyone, i am using wanbb in icevision through the fastai integration, you could have a look here for icevision but i get the following message any help appreciated.",
        "Answer_original_content":"hi fabio, can you give me more information on your case? for example, how are you implementing icevision and what tools are you using when you do so? warmly, leslie hi fabio, since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer. The responder is asking for more information about the issue and how the user is implementing icevision. The responder also mentions that the request will be closed if there is no response from the user.",
        "Answer_preprocessed_content":"hi fabio, can you give me more information on your case? for example, how are you implementing icevision and what tools are you using when you do so? warmly, leslie hi fabio, since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Convert File Dataset into a Dataframe to use in a pipeline",
        "Question_body":"Hello,\n\nI would like to convert a file dataset into a dataframe using a python script to use the data in a pipeline. I need to use the file dataset as i want to train my model using the files and not the table.\n\nThank you!",
        "Question_answer_count":4,
        "Question_comment_count":0.0,
        "Question_creation_time":1630504637667,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/536168\/convert-file-dataset-into-a-dataframe-to-use-in-a.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-02T10:17:48.457Z",
                "Answer_score":0,
                "Answer_body":"@MEZIANEYani-9720 I think you could try this to use the filedataset as pandas dataframe, download and use it for your experiment's training.\n\n from azureml.core import Dataset\n from azureml.opendatasets import MNIST\n import pandas as pd\n import os\n data_folder = os.path.join(os.getcwd(), 'data')\n os.makedirs(data_folder, exist_ok=True)\n    \n #Download the dataset\n mnist_file_dataset = MNIST.get_file_dataset()\n mnist_file_dataset.download(data_folder, overwrite=True)\n #Use the files in dataframe\n df = pd.DataFrame(mnist_file_dataset.to_path())\n print(df)\n    \n #Register the dataset for training\n mnist_file_dataset = mnist_file_dataset.register(workspace=ws,\n                                                  name='mnist_opendataset',\n                                                  description='training and test dataset',\n                                                  create_new_version=True)",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-06T14:45:29.03Z",
                "Answer_score":0,
                "Answer_body":"My aim is to run a pipeline (pre-process data and tune model hyperparameters) that I already have with design using as input data not each row of a table as it does with a tabular dataset but rather for each CVS file that represents an object (its information with a lot of rows) as input since the random selection per frame is amplifying the performance of the model. I have the data as tabular and files in a dataset. I have managed to get the path of each cvs file; but cannot read them as part of a new dataframe. I have the data in a datastore and dataset, so I don\u2019t know if to accomplish this I should store the data elsewhere (I have not been working long with Azure so I am not acquainted with all the storage possibilities and the interactions between these and the ML studio.)\n\nI manage to do this in python with the following code:\nlistOfFile = os.listdir(path)\nfor file in listOfFile:\nnew_well=pd.read_csv(os.path.join(path,file))\n\n\n\n\nAnd in Azure this is as far as I have gotten without result:\n\n   ds = Dataset.get_by_name(ws, name='well files')\n  ds.download(data_folder, overwrite=True)\n  df = pd.DataFrame(ds.to_path())\n  df= dirr+df\n files = pd.DataFrame(df)\n well = map(pd.read_csv, files)\n\n\n\n\n\nbut I cannot use this output of well into the design pipeline due to being class map.\n\nThank you very much for your help. It is greatly appreciated as I really have no clue whatsoever on how to proceed or solve this.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-09T14:19:01.737Z",
                "Answer_score":0,
                "Answer_body":"@romungi-MSFT\n\nIs there a way to do this with multiples .cvs documents?\nI have a folder full of cvs files I need to read, is there a way to give the path of the folder and for the program to read all of the cvs files within that folder?\nThere are a lot so not really feasible to do them one by one.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-10T08:24:33.5Z",
                "Answer_score":0,
                "Answer_body":"Ok I managed with this very simple line:\n\n                      tabular_dataset_3 = Dataset.Tabular.from_delimited_files(path=(datastore,'weather\/**\/*.csv'))\n\n\n\nHowever, I\u2019m afraid this will not help me accomplish my objective as all the files are now in the same tabular dataset and now, I need the training of a model to be done considering the files and not all the rows, meaning that there will be random selection per frame and not per document as I desired. I need to pre-process the data and split the training and test dataset based on the csv documents, not on a table containing all the data points.\n\nThank you for your help!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: convert file dataset into a dataframe to use in a pipeline; Content: hello, i would like to convert a file dataset into a dataframe using a python script to use the data in a pipeline. i need to use the file dataset as i want to train my model using the files and not the table. thank you!",
        "Question_original_content_gpt_summary":"The user needs to convert a file dataset into a dataframe using a Python script in order to use the data in a pipeline for training a model.",
        "Question_preprocessed_content":"Title: convert file dataset into a dataframe to use in a pipeline; Content: hello, i would like to convert a file dataset into a dataframe using a python script to use the data in a pipeline. i need to use the file dataset as i want to train my model using the files and not the table. thank you!",
        "Answer_original_content":"@mezianeyani-9720 i think you could try this to use the filedataset as pandas dataframe, download and use it for your experiment's training. from .core import dataset from .opendatasets import mnist import pandas as pd import os data_folder = os.path.join(os.getcwd(), 'data') os.makedirs(data_folder, exist_ok=true) #download the dataset mnist_file_dataset = mnist.get_file_dataset() mnist_file_dataset.download(data_folder, overwrite=true) #use the files in dataframe df = pd.dataframe(mnist_file_dataset.to_path()) print(df) #register the dataset for training mnist_file_dataset = mnist_file_dataset.register(workspace=ws, name='mnist_opendataset', description='training and test dataset', create_new_version=true) my aim is to run a pipeline (pre-process data and tune model hyperparameters) that i already have with design using as input data not each row of a table as it does with a tabular dataset but rather for each cvs file that represents an object (its information with a lot of rows) as input since the random selection per frame is amplifying the performance of the model. i have the data as tabular and files in a dataset. i have managed to get the path of each cvs file; but cannot read them as part of a new dataframe. i have the data in a datastore and dataset, so i dont know if to accomplish this i should store the data elsewhere (i have not been working long with azure so i am not acquainted with all the storage possibilities and the interactions between these and the ml studio.) i manage to do this in python with the following code: listoffile = os.listdir(path) for file in listoffile: new_well=pd.read_csv(os.path.join(path,file)) and in azure this is as far as i have gotten without result: ds = dataset.get_by_name(ws, name='well files') ds.download(data_folder, overwrite=true) df = pd.dataframe(ds.to_path()) df= dirr+df files = pd.dataframe(df) well = map(pd.read_csv, files) but i cannot use this output of well into the design pipeline due to being class map. thank you very much for your help. it is greatly appreciated as i really have no clue whatsoever on how to proceed or solve this. @romungi-msft is there a way to do this with multiples .cvs documents? i have a folder full of cvs files i need to read, is there a way to give the path of the folder and for the program to read all of the cvs files within that folder? there are a lot so not really feasible to do them one by one. ok i managed with this very simple line: tabular_dataset_3 = dataset.tabular.from_delimited_files(path=(datastore,'weather\/**\/*.csv')) however, im afraid this will not help me accomplish my objective as all the files are now in the same tabular dataset and now, i need the training of a model to be done considering the files and not all the rows, meaning that there will be random selection per frame and not per document as i desired. i need to pre-process the data and split the training and test dataset based on the csv documents, not on a table containing all the data points. thank you for your help!",
        "Answer_original_content_gpt_summary":"Possible solutions to convert a file dataset into a dataframe using a Python script are:\n\n- Download the dataset using the `mnist` library and convert it to a dataframe using `pandas`.\n- Use the `os` library to get the path of each CSV file, read them using `pandas`, and store them in a list.\n- Use the `dataset.tabular.from_delimited_files` method to read all the CSV files in a folder and create a tabular dataset.\n- Pre-process the data and split the training and test dataset based on the CSV documents, not on a table containing all the data points.",
        "Answer_preprocessed_content":"i think you could try this to use the filedataset as pandas dataframe, download and use it for your experiment's training. from import dataset from import mnist import pandas as pd import os 'data' download the dataset overwrite true use the files in dataframe df print register the dataset for training description 'training and test dataset', my aim is to run a pipeline that i already have with design using as input data not each row of a table as it does with a tabular dataset but rather for each cvs file that represents an object as input since the random selection per frame is amplifying the performance of the model. i have the data as tabular and files in a dataset. i have managed to get the path of each cvs file; but cannot read them as part of a new dataframe. i have the data in a datastore and dataset, so i dont know if to accomplish this i should store the data elsewhere i manage to do this in python with the following code listoffile for file in listoffile and in azure this is as far as i have gotten without result ds name 'well files' overwrite true df df dirr+df files well files but i cannot use this output of well into the design pipeline due to being class map. thank you very much for your help. it is greatly appreciated as i really have no clue whatsoever on how to proceed or solve this. is there a way to do this with multiples .cvs documents? i have a folder full of cvs files i need to read, is there a way to give the path of the folder and for the program to read all of the cvs files within that folder? there are a lot so not really feasible to do them one by one. ok i managed with this very simple line however, im afraid this will not help me accomplish my objective as all the files are now in the same tabular dataset and now, i need the training of a model to be done considering the files and not all the rows, meaning that there will be random selection per frame and not per document as i desired. i need to the data and split the training and test dataset based on the csv documents, not on a table containing all the data points. thank you for your help!"
    },
    {
        "Question_id":65148768.0,
        "Question_title":"how to upload images from csv file that have column with labels and urls of images in Azure machine learning by code",
        "Question_body":"<p>I have a dataset(CSV) file that looks like this one column with an identifier and another with the URL of the image, and I need to download images to my storage in azure machine learning, maybe someone could help what is the core should be?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1607106159790,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":127.0,
        "Owner_creation_time":1547213373932,
        "Owner_last_access_time":1616057404400,
        "Owner_reputation":47.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":59.0,
        "Answer_body":"<p>Please refer to the sample code provided in this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data\" rel=\"nofollow noreferrer\">tutorial<\/a>. Specifically, where it explains how to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-bring-data#upload\" rel=\"nofollow noreferrer\">upload data to Azure<\/a>.<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\ndatastore.upload(src_dir='.\/data',\n                 target_path='datasets\/cifar10',\n                 overwrite=True)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1607110542867,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1607459547320,
        "Answer_last_edit_time":1607110967612,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65148768",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to upload images from csv file that have column with labels and urls of images in by code; Content: i have a dataset(csv) file that looks like this one column with an identifier and another with the url of the image, and i need to download images to my storage in , maybe someone could help what is the core should be?",
        "Question_original_content_gpt_summary":"The user needs help with downloading images from a CSV file with an identifier and URL column to their storage in code.",
        "Question_preprocessed_content":"Title: how to upload images from csv file that have column with labels and urls of images in by code; Content: i have a dataset file that looks like this one column with an identifier and another with the url of the image, and i need to download images to my storage in , maybe someone could help what is the core should be?",
        "Answer_original_content":"please refer to the sample code provided in this tutorial. specifically, where it explains how to upload data to azure. from .core import workspace ws = workspace.from_config() datastore = ws.get_default_datastore() datastore.upload(src_dir='.\/data', target_path='datasets\/cifar10', overwrite=true)",
        "Answer_original_content_gpt_summary":"The solution to downloading images from a CSV file with an identifier and URL column to storage in code is to refer to the sample code provided in a tutorial. Specifically, the tutorial explains how to upload data to Azure using the provided code snippet. The code imports the workspace from the .core module, gets the default datastore, and uploads the data from the source directory to the target path. The \"overwrite\" parameter is set to true to replace any existing data.",
        "Answer_preprocessed_content":"please refer to the sample code provided in this tutorial. specifically, where it explains how to upload data to azure."
    },
    {
        "Question_id":73733464.0,
        "Question_title":"What should be used for deployed_model in DeployModelRequest in Vertex AI pipeline?",
        "Question_body":"<p>i am trying to deploy a model in Vertex AI pipeline component using <code>DeployModelRequest<\/code>. I try to get the model using <code>GetModelRequest<\/code><\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)       \n    \n    deploy_request = aiplatform_v1.types.DeployModelRequest(endpoint=end_point, \n                                                                deployed_model=model_info)\n    client.deploy_model(request=deploy_request)\n<\/code><\/pre>\n<p>but this gives:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected \ngoogle.cloud.aiplatform.v1.DeployedModel got Model\n<\/code><\/pre>\n<p>I have also tried <code>deployed_model=model_info.deployed_models[0]<\/code> but this gave:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected\n google.cloud.aiplatform.v1.DeployedModel got DeployedModelRef.\n<\/code><\/pre>\n<p>So what do I use for <code>deployed_model<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6.0,
        "Question_creation_time":1663254259473,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":39.0,
        "Owner_creation_time":1351154914716,
        "Owner_last_access_time":1663927832783,
        "Owner_reputation":2564.0,
        "Owner_up_votes":304.0,
        "Owner_down_votes":8.0,
        "Owner_views":451.0,
        "Answer_body":"<p>As simple as:<\/p>\n<pre><code>machine_spec = MachineSpec(machine_type=&quot;n1-standard-2&quot;)\ndedicated_resources = DedicatedResources(machine_spec=machine_spec, \n                                         min_replica_count=1, \n                                         max_replica_count=1)\ndepmodel= DeployedModel(model=model_name, dedicated_resources=dedicated_resources) \n\ndeploy_request = aiplatform_v1.types.DeployModelRequest(\n                   endpoint=end_point, deployed_model=depmodel, \n                   traffic_split={'0':100})\nclient.deploy_model(request=deploy_request)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1663344904267,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73733464",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what should be used for deployed_model in deploymodelrequest in pipeline?; Content: i am trying to deploy a model in pipeline component using deploymodelrequest. i try to get the model using getmodelrequest model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}' model_request = aiplatform_v1.types.getmodelrequest(name=model_name) model_info = client_model.get_model(request=model_request) deploy_request = aiplatform_v1.types.deploymodelrequest(endpoint=end_point, deployed_model=model_info) client.deploy_model(request=deploy_request) but this gives: typeerror: parameter to mergefrom() must be instance of same class: expected google.cloud.aiplatform.v1.deployedmodel got model i have also tried deployed_model=model_info.deployed_models[0] but this gave: typeerror: parameter to mergefrom() must be instance of same class: expected google.cloud.aiplatform.v1.deployedmodel got deployedmodelref. so what do i use for deployed_model?",
        "Question_original_content_gpt_summary":"The user is encountering challenges when attempting to deploy a model in a pipeline component using deploymodelrequest, as they are receiving TypeErrors when trying to use the model info from getmodelrequest.",
        "Question_preprocessed_content":"Title: what should be used for in deploymodelrequest in pipeline?; Content: i am trying to deploy a model in pipeline component using . i try to get the model using but this gives i have also tried but this gave so what do i use for ?",
        "Answer_original_content":"as simple as: machine_spec = machinespec(machine_type=\"n1-standard-2\") dedicated_resources = dedicatedresources(machine_spec=machine_spec, min_replica_count=1, max_replica_count=1) depmodel= deployedmodel(model=model_name, dedicated_resources=dedicated_resources) deploy_request = aiplatform_v1.types.deploymodelrequest( endpoint=end_point, deployed_model=depmodel, traffic_split={'0':100}) client.deploy_model(request=deploy_request)",
        "Answer_original_content_gpt_summary":"The answer does not provide any solutions to the specific challenge mentioned in the question. Instead, it provides code snippets for deploying a model in a pipeline component using deploymodelrequest.",
        "Answer_preprocessed_content":"as simple as"
    },
    {
        "Question_id":null,
        "Question_title":"Use LightGBM algorithms in Azure Machine Learning Designer",
        "Question_body":"Wonder anyone can help on this. My customer wonder if Azure Machine Learning Designer (GUI) can support using Python open library e.g. LightGBM algorithms to develop machine learning model. It seems that it is supported but could not find a supporting document.\nThe customer is currently using Python programming with LightGBM algorithms etc to develop machine learning model with other ML tool. Thanks!",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1639119129643,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/658828\/use-lightgbm-algorithms-in-azure-machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-10T14:14:43.29Z",
                "Answer_score":0,
                "Answer_body":"@LeeChongYew-9493 Thanks for the question. Custom module is designed for reusable tasks in machine learning workflow, like data process, training, scoring etc.\nstudio designer can manage model deployment as described here: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: use lightgbm algorithms in designer; Content: wonder anyone can help on this. my customer wonder if designer (gui) can support using python open library e.g. lightgbm algorithms to develop machine learning model. it seems that it is supported but could not find a supporting document. the customer is currently using python programming with lightgbm algorithms etc to develop machine learning model with other ml tool. thanks!",
        "Question_original_content_gpt_summary":"The user is looking for a way to use the LightGBM algorithms in Designer to develop a machine learning model.",
        "Question_preprocessed_content":"Title: use lightgbm algorithms in designer; Content: wonder anyone can help on this. my customer wonder if designer can support using python open library lightgbm algorithms to develop machine learning model. it seems that it is supported but could not find a supporting document. the customer is currently using python programming with lightgbm algorithms etc to develop machine learning model with other ml tool. thanks!",
        "Answer_original_content":"@leechongyew-9493 thanks for the question. custom module is designed for reusable tasks in machine learning workflow, like data process, training, scoring etc. studio designer can manage model deployment as described here: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Use custom modules in Designer for reusable tasks in machine learning workflow, such as data processing, training, and scoring.\n- Manage model deployment in Designer as described in the provided link.",
        "Answer_preprocessed_content":"thanks for the question. custom module is designed for reusable tasks in machine learning workflow, like data process, training, scoring etc. studio designer can manage model deployment as described here"
    },
    {
        "Question_id":null,
        "Question_title":"Parallel batch trial pipeline",
        "Question_body":"<p>In a batched trial pipeline see <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/196\" rel=\"nofollow noopener\">196<\/a>, how can we run as fast as possible parallelizing everything we can ?<\/p>\n<p>Example Pipeline:<\/p>\n<pre><code>operation_1: # has no dependencies\n\noperation_2: \n    requires:\n        - operation: operation_1\n    flags-dest: globals\n    flags-import:\n        - some_param\n\noperation_3: \n    requires:\n        - operation: operation_2\n    flags-dest: globals\n    flags-import:\n        - some_other_param\n\npipeline:\n  steps:\n    - run: operation_1\n    - run: operation_2\n      flags:\n        some_param: [a, b]\n    - run: operation_3\n      flags:\n        some_other_param: [1, 2, 3]\n<\/code><\/pre>\n<pre><code># create 6 queues as we have 6 batch trials that can be done in parallel (a1, a2, a3, b1, b2, b3, c1, c2, c3)\n# run this command 6 times\nguild run queue --background \n\nguild run pipeline --stage\n<\/code><\/pre>\n<p>For some reason this leads to out of sequence events happening, like operation 2 being run before operation 1 resulting in an error. Is this the correct way to do this?<\/p>\n<p>Further, is there a good way of timing this to sanity check parallel works faster i.e time batched trials run?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1591810114957,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":360.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/parallel-batch-trial-pipeline\/142",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-10T23:26:07.272Z",
                "Answer_body":"<p>Don\u2019t use staged runs for this. Just run your pipelines in the background. At some point Guild will be optimized for parallel runs and you won\u2019t have to think about this (as you say - we want to run everything as fast as possible). But at the moment, you need to use parallel OS processes to manage parallel runs.<\/p>",
                "Answer_score":3.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: parallel batch trial pipeline; Content: in a batched trial pipeline see 196, how can we run as fast as possible parallelizing everything we can ? example pipeline: operation_1: # has no dependencies operation_2: requires: - operation: operation_1 flags-dest: globals flags-import: - some_param operation_3: requires: - operation: operation_2 flags-dest: globals flags-import: - some_other_param pipeline: steps: - run: operation_1 - run: operation_2 flags: some_param: [a, b] - run: operation_3 flags: some_other_param: [1, 2, 3] # create 6 queues as we have 6 batch trials that can be done in parallel (a1, a2, a3, b1, b2, b3, c1, c2, c3) # run this command 6 times guild run queue --background guild run pipeline --stage for some reason this leads to out of sequence events happening, like operation 2 being run before operation 1 resulting in an error. is this the correct way to do this? further, is there a good way of timing this to sanity check parallel works faster i.e time batched trials run?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with running a parallel batch trial pipeline, where operations are being run out of sequence and timing is not being accurately tracked.",
        "Question_preprocessed_content":"Title: parallel batch trial pipeline; Content: in a batched trial pipeline see , how can we run as fast as possible parallelizing everything we can ? example pipeline for some reason this leads to out of sequence events happening, like operation being run before operation resulting in an error. is this the correct way to do this? further, is there a good way of timing this to sanity check parallel works faster time batched trials run?",
        "Answer_original_content":"dont use staged runs for this. just run your pipelines in the background. at some point guild will be optimized for parallel runs and you wont have to think about this (as you say - we want to run everything as fast as possible). but at the moment, you need to use parallel os processes to manage parallel runs.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of running a parallel batch trial pipeline include avoiding staged runs and running pipelines in the background. It is recommended to use parallel OS processes to manage parallel runs until Guild is optimized for parallel runs.",
        "Answer_preprocessed_content":"dont use staged runs for this. just run your pipelines in the background. at some point guild will be optimized for parallel runs and you wont have to think about this . but at the moment, you need to use parallel os processes to manage parallel runs."
    },
    {
        "Question_id":null,
        "Question_title":"AI & MLOps Garage (Demos + Hands-on w\/ Prizes) - March 9th",
        "Question_body":" Join Google Cloud Industry experts for a half-day dedicated to the possibilities of MLOps & AI. Dig deeper into how you can mature your current Machine Learning & AI practices. Not a Data Scientist or a ML Engineer? No worries! Let us help you get started with modern ML technologies like AutoML and ML w\/ SQL. End the day\u2019s learning by building an MLOps pipeline to automate data engineering, model training & model deployment.   Registration Link : https:\/\/inthecloud.withgoogle.com\/machine-learning-ai-garage-series\/register.html   Through conversations and hands-on workshops, you\u2019ll explore:   The newest AI and ML innovations, use cases, and best practices How to build high-quality ML models with minimal effort How to use automation to your advantage    Running AI and ML solutions both in the cloud and on-premises  LinkedIn Event  Registration Page",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1645956660000,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":93.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-amp-MLOps-Garage-Demos-Hands-on-w-Prizes-March-9th\/td-p\/397728\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-27T10:11:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Join Google Cloud Industry experts for a half-day dedicated to the possibilities of MLOps & AI. Dig deeper into how you can mature your current Machine Learning & AI practices. Not a Data Scientist or a ML Engineer? No worries! Let us help you get started with modern ML technologies like AutoML and ML w\/ SQL. End the day\u2019s learning by building an MLOps pipeline to automate data engineering, model training & model deployment.\n\n\u00a0\n\n\u00a0\n\n\u00a0Registration Link : https:\/\/inthecloud.withgoogle.com\/machine-learning-ai-garage-series\/register.html\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\nThrough conversations and hands-on workshops, you\u2019ll explore:\n\n\u00a0\n\n\u00a0\n\n\u00a0The newest AI and ML innovations, use cases, and best practices\n\n\u00a0How to build high-quality ML models with minimal effort\n\n\u00a0How to use automation to your advantage\n\n\u00a0\n\n\u00a0\n\n\u00a0\n\n\u00a0Running AI and ML solutions both in the cloud and on-premises\n\n\u00a0\n\n\u00a0LinkedIn Event\u00a0\n\n\u00a0Registration Page"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: ai & mlops garage (demos + hands-on w\/ prizes) - march 9th; Content: join google cloud industry experts for a half-day dedicated to the possibilities of mlops & ai. dig deeper into how you can mature your current machine learning & ai practices. not a data scientist or a ml engineer? no worries! let us help you get started with modern ml technologies like automl and ml w\/ sql. end the day\u2019s learning by building an mlops pipeline to automate data engineering, model training & model deployment. registration link : https:\/\/inthecloud.withgoogle.com\/machine-learning-ai-garage-series\/register.html through conversations and hands-on workshops, you\u2019ll explore: the newest ai and ml innovations, use cases, and best practices how to build high-quality ml models with minimal effort how to use automation to your advantage running ai and ml solutions both in the cloud and on-premises linkedin event registration page",
        "Question_original_content_gpt_summary":"The user will explore the newest AI and ML innovations, use cases, and best practices, learn how to build high-quality ML models with minimal effort, and use automation to their advantage to run AI and ML solutions both in the cloud and on-premises.",
        "Question_preprocessed_content":"Title: ai & mlops garage march th; Content: join google cloud industry experts for a dedicated to the possibilities of mlops & ai. dig deeper into how you can mature your current machine learning & ai practices. not a data scientist or a ml engineer? no worries! let us help you get started with modern ml technologies like automl and ml w\/ sql. end the days learning by building an mlops pipeline to automate data engineering, model training & model deployment. registration link through conversations and workshops, youll explore the newest ai and ml innovations, use cases, and best practices how to build ml models with minimal effort how to use automation to your advantage running ai and ml solutions both in the cloud and linkedin event registration page",
        "Answer_original_content":"join google cloud industry experts for a half-day dedicated to the possibilities of mlops & ai. dig deeper into how you can mature your current machine learning & ai practices. not a data scientist or a ml engineer? no worries! let us help you get started with modern ml technologies like automl and ml w\/ sql. end the days learning by building an mlops pipeline to automate data engineering, model training & model deployment. registration link : https:\/\/inthecloud.withgoogle.com\/machine-learning-ai-garage-series\/register.html through conversations and hands-on workshops, youll explore: the newest ai and ml innovations, use cases, and best practices how to build high-quality ml models with minimal effort how to use automation to your advantage running ai and ml solutions both in the cloud and on-premises linkedin event registration page",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include joining a half-day event hosted by Google Cloud industry experts to learn about MLOps and AI, exploring the newest AI and ML innovations, use cases, and best practices, learning how to build high-quality ML models with minimal effort, using automation to run AI and ML solutions both in the cloud and on-premises, and building an MLOps pipeline to automate data engineering, model training, and model deployment. The event is open to everyone, including those who are not data scientists or ML engineers, and registration can be done through the provided link.",
        "Answer_preprocessed_content":"join google cloud industry experts for a dedicated to the possibilities of mlops & ai. dig deeper into how you can mature your current machine learning & ai practices. not a data scientist or a ml engineer? no worries! let us help you get started with modern ml technologies like automl and ml w\/ sql. end the days learning by building an mlops pipeline to automate data engineering, model training & model deployment. registration link through conversations and workshops, youll explore the newest ai and ml innovations, use cases, and best practices how to build ml models with minimal effort how to use automation to your advantage running ai and ml solutions both in the cloud and linkedin event registration page"
    },
    {
        "Question_id":null,
        "Question_title":"Form Parsing in Document AI",
        "Question_body":"Hi All,We are currently using Document AI for form parsing scanned documents and we are now required to capture the checkboxes data from the form.",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1651717920000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":144.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Form-Parsing-in-Document-AI\/td-p\/420076\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-06T16:14:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I think that you would find this laboratory that google offers helpful since it explain step by step how form parsing works within google Document AI."
            },
            {
                "Answer_creation_time":"2022-05-10T00:00:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Thank you for your response. I have gone through the lab course but still couldn't find answers on the checkbox count limitation of the Form-parsing using Document AI.\nIs that limitation due to Pricing? Can you please help me with this?"
            },
            {
                "Answer_creation_time":"2022-05-13T15:48:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I found this guide that you might found useful, i couldn't find any information about limitations, so i would say that it shouldn't limit you, or maybe the documentation wasn't processed succsesfully."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: form parsing in document ai; Content: hi all,we are currently using document ai for form parsing scanned documents and we are now required to capture the checkboxes data from the form.",
        "Question_original_content_gpt_summary":"The user is facing a challenge of capturing checkbox data from a form using Document AI.",
        "Question_preprocessed_content":"Title: form parsing in document ai; Content: hi all,we are currently using document ai for form parsing scanned documents and we are now required to capture the checkboxes data from the form.",
        "Answer_original_content":"i think that you would find this laboratory that google offers helpful since it explain step by step how form parsing works within google document ai. thank you for your response. i have gone through the lab course but still couldn't find answers on the checkbox count limitation of the form-parsing using document ai. is that limitation due to pricing? can you please help me with this? i found this guide that you might found useful, i couldn't find any information about limitations, so i would say that it shouldn't limit you, or maybe the documentation wasn't processed succsesfully.",
        "Answer_original_content_gpt_summary":"Possible solutions mentioned in the answer are: \n- Check out the laboratory that Google offers which explains step by step how form parsing works within Google Document AI.\n- Refer to the guide provided in the answer which might be useful.\n- There is no information about limitations, so it shouldn't limit the user.",
        "Answer_preprocessed_content":"i think that you would find this laboratory that google offers helpful since it explain step by step how form parsing works within google document ai. thank you for your response. i have gone through the lab course but still couldn't find answers on the checkbox count limitation of the using document ai. is that limitation due to pricing? can you please help me with this? i found this guide that you might found useful, i couldn't find any information about limitations, so i would say that it shouldn't limit you, or maybe the documentation wasn't processed succsesfully."
    },
    {
        "Question_id":70379395.0,
        "Question_title":"Vertex AI - Viewing Pipeline Output",
        "Question_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1639659363547,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":346.0,
        "Owner_creation_time":1407245826703,
        "Owner_last_access_time":1663849819648,
        "Owner_reputation":745.0,
        "Owner_up_votes":210.0,
        "Owner_down_votes":6.0,
        "Owner_views":168.0,
        "Answer_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1639667359983,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":1639726542407,
        "Answer_last_edit_time":1639750456896,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - viewing pipeline output; Content: i have followed this tutorial to create my first scheduled pipeline to run every minute. the only thing it does is prints \"hello, <any-greet-string>\" and also returns this same string. i can see that it is running because the last run time updates and the last run result is \"success\" every time. my question is very simple: where can i see this string printed and the output of my pipeline?",
        "Question_original_content_gpt_summary":"The user is encountering difficulty viewing the output of their scheduled pipeline, which prints a string and returns the same string.",
        "Question_preprocessed_content":"Title: viewing pipeline output; Content: i have followed this tutorial to create my first scheduled pipeline to run every minute. the only thing it does is prints and also returns this same string. i can see that it is running because the last run time updates and the last run result is success every time. my question is very simple where can i see this string printed and the output of my pipeline?",
        "Answer_original_content":"the output of the print() statements in the pipeline can be found in \"cloud logging\" with the appropriate filters. to check logs for each component in the pipeline, click on the respective component in the console and click \"view logs\" in the right pane. a new pane with the logs will open in the pipeline page which will allow us to see the output of the component. refer to the below screenshot. i ran a sample pipeline from this codelab, intro to vertex pipelines and below is the output for one of the print() statements in the pipeline. update: every component in a pipeline run is deployed as an individual custom job. corresponding to the sample pipeline consisting 3 components, there are 3 entries in the \"custom jobs\" section as shown below. therefore, to view the logs on the run level, we would need to query the log entries with the respective job_ids of the pipeline components and the job_id of the cloud scheduler job. the query would look like this. resource.labels.job_id=(\"job_id_1\" or \"job_id_2\" [or \"job_id_n\"...]) severity>=default if there are no simultaneous pipeline runs, a simpler query like below can be used. resource.type=(\"cloud_scheduler_job\" or \"ml_job\") severity>=default",
        "Answer_original_content_gpt_summary":"Possible solutions to view the output of a scheduled pipeline that prints and returns a string are: \n1. Check the cloud logging with appropriate filters to find the output of the print() statements in the pipeline. \n2. Click on the respective component in the console and click \"view logs\" in the right pane to check logs for each component in the pipeline. \n3. Query the log entries with the respective job_ids of the pipeline components and the job_id of the cloud scheduler job to view the logs on the run level. \n4. Use a simpler query like resource.type=(\"cloud_scheduler_job\" or \"ml_job\") severity>=default if there are no simultaneous pipeline runs.",
        "Answer_preprocessed_content":"the output of the statements in the pipeline can be found in cloud logging with the appropriate filters. to check logs for each component in the pipeline, click on the respective component in the console and click view logs in the right pane. a new pane with the logs will open in the pipeline page which will allow us to see the output of the component. refer to the below screenshot. i ran a sample pipeline from this codelab, intro to vertex pipelines and below is the output for one of the statements in the pipeline. update every component in a pipeline run is deployed as an individual custom job. corresponding to the sample pipeline consisting components, there are entries in the custom jobs section as shown below. therefore, to view the logs on the run level, we would need to query the log entries with the respective s of the pipeline components and the of the cloud scheduler job. the query would look like this. if there are no simultaneous pipeline runs, a simpler query like below can be used."
    },
    {
        "Question_id":57017876.0,
        "Question_title":"Where to deploy machine learning model for API predictions?",
        "Question_body":"<p>I created a machine learning model with <a href=\"https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html\" rel=\"nofollow noreferrer\">Prophet<\/a>:<\/p>\n\n<p><a href=\"https:\/\/www.kaggle.com\/marcmetz\/ticket-sales-prediction-facebook-prophet\" rel=\"nofollow noreferrer\">https:\/\/www.kaggle.com\/marcmetz\/ticket-sales-prediction-facebook-prophet<\/a><\/p>\n\n<p>I have a web application running with Django. From that application, I want to be able to lookup predictions from the model I created. I assume the best way to do is to deploy my model on Google Cloud Platform or AWS (?) and access forecasts through API calls from my web application to one of these services.<\/p>\n\n<p>My question now: Is that way I described it the right way to do so? I still struggle to decide if either AWS or Google Cloud is the better solution for my case, especially with Prophet. I could only find examples with <code>scikit-learn<\/code>. Any of you who has experience with that and can point me in the right direction?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1563010127430,
        "Question_favorite_count":0.0,
        "Question_score":2.0,
        "Question_view_count":187.0,
        "Owner_creation_time":1537976685540,
        "Owner_last_access_time":1634030094903,
        "Owner_reputation":2879.0,
        "Owner_up_votes":70.0,
        "Owner_down_votes":3.0,
        "Owner_views":5051.0,
        "Answer_body":"<p>It really depends on the type of model that you are using. In many cases, the model inference is getting a data point (similar to the data points you trained it with) and the model will generate a prediction to that requested data point. In such cases, you need to host the model somewhere in the cloud or on the edge. <\/p>\n\n<p>However, Prophet is often generating the predictions for the future as part of the training of the model. In this case, you only need to serve the predictions that were already calculated, and you can serve them as a CSV file from S3, or as lookup values from a DynamoDB or other lookup data stores. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1563019168512,
        "Answer_score":4.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57017876",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: where to deploy machine learning model for api predictions?; Content: i created a machine learning model with prophet: https:\/\/www.kaggle.com\/marcmetz\/ticket-sales-prediction-facebook-prophet i have a web application running with django. from that application, i want to be able to lookup predictions from the model i created. i assume the best way to do is to deploy my model on google cloud platform or aws (?) and access forecasts through api calls from my web application to one of these services. my question now: is that way i described it the right way to do so? i still struggle to decide if either aws or google cloud is the better solution for my case, especially with prophet. i could only find examples with scikit-learn. any of you who has experience with that and can point me in the right direction?",
        "Question_original_content_gpt_summary":"The user is struggling to decide between AWS and Google Cloud Platform to deploy their machine learning model created with Prophet for API predictions from their web application.",
        "Question_preprocessed_content":"Title: where to deploy machine learning model for api predictions?; Content: i created a machine learning model with prophet i have a web application running with django. from that application, i want to be able to lookup predictions from the model i created. i assume the best way to do is to deploy my model on google cloud platform or aws and access forecasts through api calls from my web application to one of these services. my question now is that way i described it the right way to do so? i still struggle to decide if either aws or google cloud is the better solution for my case, especially with prophet. i could only find examples with . any of you who has experience with that and can point me in the right direction?",
        "Answer_original_content":"it really depends on the type of model that you are using. in many cases, the model inference is getting a data point (similar to the data points you trained it with) and the model will generate a prediction to that requested data point. in such cases, you need to host the model somewhere in the cloud or on the edge. however, prophet is often generating the predictions for the future as part of the training of the model. in this case, you only need to serve the predictions that were already calculated, and you can serve them as a csv file from s3, or as lookup values from a dynamodb or other lookup data stores.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n- If the machine learning model is generating predictions for the future as part of the training, the user only needs to serve the predictions that were already calculated. This can be done by serving them as a CSV file from S3 or as lookup values from a DynamoDB or other lookup data stores.\n- If the machine learning model is getting a data point and generating a prediction for that requested data point, the user needs to host the model somewhere in the cloud or on the edge. The choice between AWS and Google Cloud Platform depends on the type of model being used.",
        "Answer_preprocessed_content":"it really depends on the type of model that you are using. in many cases, the model inference is getting a data point and the model will generate a prediction to that requested data point. in such cases, you need to host the model somewhere in the cloud or on the edge. however, prophet is often generating the predictions for the future as part of the training of the model. in this case, you only need to serve the predictions that were already calculated, and you can serve them as a csv file from s , or as lookup values from a dynamodb or other lookup data stores."
    },
    {
        "Question_id":null,
        "Question_title":"Save audio file from speech to text stream",
        "Question_body":"I am using @Google-cloud\/speech for streaming audio from the browser to my nodejs backend.\nI would like to save the recorded audio.\nI see no option to do so. Any suggestions? Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1646292900000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Save-audio-file-from-speech-to-text-stream\/td-p\/398993\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-08T13:17:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hey,\u00a0\n\nYou shall probably use other packages for recording such as recordrtc as mentioned at [1].\u00a0\u00a0\n\n[1]\u00a0https:\/\/www.leeboonstra.dev\/chatbots\/building-your-own-voice-ai-3\/"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: save audio file from speech to text stream; Content: i am using @google-cloud\/speech for streaming audio from the browser to my nodejs backend. i would like to save the recorded audio. i see no option to do so. any suggestions? thanks.",
        "Question_original_content_gpt_summary":"The user is looking for a way to save an audio file from a speech to text stream using @google-cloud\/speech for streaming audio from the browser to their Node.js backend.",
        "Question_preprocessed_content":"Title: save audio file from speech to text stream; Content: i am using for streaming audio from the browser to my nodejs backend. i would like to save the recorded audio. i see no option to do so. any suggestions? thanks.",
        "Answer_original_content":"hey, you shall probably use other packages for recording such as recordrtc as mentioned at [1]. [1]https:\/\/www.leeboonstra.dev\/chatbots\/building-your-own-voice-ai-3\/",
        "Answer_original_content_gpt_summary":"Possible solution: The user can use other packages such as recordrtc for recording audio files from a speech to text stream in their Node.js backend.",
        "Answer_preprocessed_content":"hey, you shall probably use other packages for recording such as recordrtc as mentioned at ."
    },
    {
        "Question_id":null,
        "Question_title":"Recover a missing AML Run",
        "Question_body":"Hi,\n\nIs it possible to recover an Azure ML Run that seems to have been deleted? I can still see the files in Blob Storage, but it's not showing up in the AML portal.\n\nThanks,\nMelissa",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1653661664120,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/867216\/recover-a-missing-aml-run.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-30T07:17:01.007Z",
                "Answer_score":0,
                "Answer_body":"@mbristow I would recommend to raise a support case from Azure portal through Help + Support blade. The service team of Azure ML would assess your setup to check if a particular run can be restored. The longer it is since the run got deleted the chances of restoring it would be lower.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: recover a missing aml run; Content: hi, is it possible to recover an run that seems to have been deleted? i can still see the files in blob storage, but it's not showing up in the aml portal. thanks, melissa",
        "Question_original_content_gpt_summary":"The user Melissa is trying to recover a missing AML run that is still visible in Blob Storage but not in the AML Portal.",
        "Question_preprocessed_content":"Title: recover a missing aml run; Content: hi, is it possible to recover an run that seems to have been deleted? i can still see the files in blob storage, but it's not showing up in the aml portal. thanks, melissa",
        "Answer_original_content":"@mbristow i would recommend to raise a support case from azure portal through help + support blade. the service team of would assess your setup to check if a particular run can be restored. the longer it is since the run got deleted the chances of restoring it would be lower. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"Possible solution: The user can raise a support case from the Azure portal through the help + support blade. The service team will assess the setup to check if the missing AML run can be restored. However, the longer it has been since the run got deleted, the lower the chances of restoring it.",
        "Answer_preprocessed_content":"i would recommend to raise a support case from azure portal through help + support blade. the service team of would assess your setup to check if a particular run can be restored. the longer it is since the run got deleted the chances of restoring it would be lower. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":null,
        "Question_title":"Custom Dockerfile on Azure Environment with python poetry",
        "Question_body":"I am new to docker and environments. This could be basics but i have been trying to install packages in my pyproject.toml file in Dockerfile without success.\n\nI have tried using poetry to export requirements.txt file and using it with the\nEnvironment.from_pip_requirements('requirements.txt') function and a Dockerfile.\n\nBut could there be any elegant solution to use toml file directly for creating a custom environment ?",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1638821054557,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/653688\/custom-dockerfile-on-azure-environment-with-python.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-08T16:31:16.963Z",
                "Answer_score":1,
                "Answer_body":"Thanks for the response, @Ram-msft\nUsing the Dockerfile :\n\n FROM python:3.8-slim-buster\n ENV PYTHONUNBUFFERED=1 \\\n     PYTHONDONTWRITEBYTECODE=1 \\\n     PIP_NO_CACHE_DIR=1 \\\n     PIP_DISABLE_PIP_VERSION_CHECK=1 \\\n     POETRY_VERSION=1.1.7 \\\n     PYLINT_VERSION=2.9.4\n    \n RUN pip install pylint==$PYLINT_VERSION \\\n     && pip install \"poetry==$POETRY_VERSION\" \n    \n COPY pyproject.toml .\/\n RUN poetry config virtualenvs.create false",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-12-07T15:37:01.407Z",
                "Answer_score":0,
                "Answer_body":"@AntaraDas-4298 Thanks for the question. Can you please share the sample\/document that you are trying. I would recommend to use yml file that is relatively easy to get from pip requirements file\n\nfrom azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nenv = Environment(\u201cmyenv\u201d)\nenv.python.conda_dependencies = CondaDependencies(\u201cmy_yaml_file\u201d)\n\n\n\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-custom-image",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: custom dockerfile on azure environment with python poetry; Content: i am new to docker and environments. this could be basics but i have been trying to install packages in my pyproject.toml file in dockerfile without success. i have tried using poetry to export requirements.txt file and using it with the environment.from_pip_requirements('requirements.txt') function and a dockerfile. but could there be any elegant solution to use toml file directly for creating a custom environment ?",
        "Question_original_content_gpt_summary":"The user is trying to create a custom dockerfile on an Azure environment with Python Poetry, but is having difficulty installing packages in their pyproject.toml file.",
        "Question_preprocessed_content":"Title: custom dockerfile on azure environment with python poetry; Content: i am new to docker and environments. this could be basics but i have been trying to install packages in my file in dockerfile without success. i have tried using poetry to export file and using it with the function and a dockerfile. but could there be any elegant solution to use toml file directly for creating a custom environment ?",
        "Answer_original_content":"thanks for the response, @ram-msft using the dockerfile : from python:3.8-slim-buster env pythonunbuffered=1 \\ pythondontwritebytecode=1 \\ pip_no_cache_dir=1 \\ pip_disable_pip_version_check=1 \\ poetry_version=1.1.7 \\ pylint_version=2.9.4 run pip install pylint==$pylint_version \\ && pip install \"poetry==$poetry_version\" copy pyproject.toml .\/ run poetry config virtualenvs.create false",
        "Answer_original_content_gpt_summary":"The answer suggests using a specific dockerfile with environment variables and installing the required packages using pip. It also includes copying the pyproject.toml file and disabling virtual environment creation.",
        "Answer_preprocessed_content":"thanks for the response, using the dockerfile from env pythonunbuffered \\ pythondontwritebytecode \\ \\ \\ \\ run pip install \\ && pip install copy .\/ run poetry config false"
    },
    {
        "Question_id":54715601.0,
        "Question_title":"How do I create a Sagemaker training job with my own Tensorflow code without having to build a container?",
        "Question_body":"<p>I'm trying to define a Sagemaker Training Job with an existing Python class. To my understanding, I could create my own container but would rather not deal with container management.<\/p>\n\n<p>When choosing \"Algorithm Source\" there is the option of \"Your own algorithm source\" but nothing is listed under resources. Where does this come from?<\/p>\n\n<p>I know I could do this through a notebook, but I really want this defined in a job that can be invoked through an endpoint.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1550257341217,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":781.0,
        "Owner_creation_time":1366768533200,
        "Owner_last_access_time":1574479428332,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":"<p>As Bruno has said you will have to use a container somewhere, but you can use an existing container to run your own custom tensorflow code.<\/p>\n\n<p>There is a good example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">in the sagemaker github<\/a> for how to do this.<\/p>\n\n<p>The way this works is you modify your code to have an entry point which takes argparse command line arguments, and then you point a 'Sagemaker Tensorflow estimator' to the entry point. Then when you call fit on the sagemaker estimator it will download the tensorflow container and run your custom code in there.<\/p>\n\n<p>So you start off with your own custom code that looks something like this<\/p>\n\n<pre><code># my_custom_code.py\nimport tensorflow as tf\nimport numpy as np\n\ndef build_net():\n    # single fully connected\n    image_place = tf.placeholder(tf.float32, [None, 28*28])\n    label_place = tf.placeholder(tf.int32, [None,])\n    net = tf.layers.dense(image_place, units=1024, activation=tf.nn.relu)\n    net = tf.layers.dense(net, units=10, activation=None)\n    return image_place, label_place, net\n\n\ndef process_data():\n    # load\n    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n\n    # center\n    x_train = x_train \/ 255.0\n    m = x_train.mean()\n    x_train = x_train - m\n\n    # convert to right types\n    x_train = x_train.astype(np.float32)\n    y_train = y_train.astype(np.int32)\n\n    # reshape so flat\n    x_train = np.reshape(x_train, [-1, 28*28])\n    return x_train, y_train\n\n\ndef train_model(init_learn, epochs):\n    image_p, label_p, logit = build_net()\n    x_train, y_train = process_data()\n\n    loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n        logits=logit,\n        labels=label_p)\n    optimiser = tf.train.AdamOptimizer(init_learn)\n    train_step = optimiser.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for _ in range(epochs):\n            sess.run(train_step, feed_dict={image_p: x_train, label_p: y_train})\n\n\nif __name__ == '__main__':\n    train_model(0.001, 10)\n<\/code><\/pre>\n\n<p>To make it work with sagemaker we need to create a command line entry point, which will allow sagemaker to run it in the container it will download for us eventually.<\/p>\n\n<pre><code># entry.py\n\nimport argparse\nfrom my_custom_code import train_model\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        '--model_dir',\n        type=str)\n    parser.add_argument(\n        '--init_learn',\n        type=float)\n    parser.add_argument(\n        '--epochs',\n        type=int)\n    args = parser.parse_args()\n    train_model(args.init_learn, args.epochs)\n<\/code><\/pre>\n\n<p>Apart from specifying the arguments my function needs to take, we also need to provide a <code>model_dir<\/code> argument. This is always required, and is an S3 location which is where an model artifacts will be saved when the training job completes. Note that you don't need to specify what this value is (though you can) as Sagemaker will provide a default location in S3 for you.<\/p>\n\n<p>So we have modified our code, now we need to actually run it on Sagemaker. Go to the AWS console and fire up a small instance from Sagemaker. Download your custom code to the instance, and then create a jupyter notebook as follows:<\/p>\n\n<pre><code># sagemaker_run.ipyb\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nhyperparameters = {\n    'epochs': 10,\n    'init_learn': 0.001}\n\nrole = sagemaker.get_execution_role()\nsource_dir = '\/path\/to\/folder\/with\/my\/code\/on\/instance'\nestimator = TensorFlow(\n    entry_point='entry.py',\n    source_dir=source_dir,\n    train_instance_type='ml.t2.medium',\n    train_instance_count=1,\n    hyperparameters=hyperparameters,\n    role=role,\n    py_version='py3',\n    framework_version='1.12.0',\n    script_mode=True)\n\nestimator.fit()\n<\/code><\/pre>\n\n<p>Running the above will:<\/p>\n\n<ul>\n<li>Spin up an ml.t2.medium instance<\/li>\n<li>Download the tensorflow 1.12.0 container to the instance<\/li>\n<li>Download any data we specify in fit to the newly created instance in fit (in this case nothing)<\/li>\n<li>Run our code on the instance<\/li>\n<li>upload the model artifacts to model_dir<\/li>\n<\/ul>\n\n<p>And that is pretty much it. There is of course a lot not mentioned here but you can:<\/p>\n\n<ul>\n<li>Download training\/testing data from s3<\/li>\n<li>Save checkpoint files, and tensorboard files during training and upload them to s3<\/li>\n<\/ul>\n\n<p>The best resource I found was the example I shared but here are all the things I was looking at to get this working:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_quickstart\/tensorflow_script_mode_quickstart.ipynb\" rel=\"nofollow noreferrer\">example code again<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">documentation<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/aws\/sagemaker-containers#list-of-provided-environment-variables-by-sagemaker-containers\" rel=\"nofollow noreferrer\">explanation of environment variables<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1550361262523,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54715601",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i create a training job with my own tensorflow code without having to build a container?; Content: i'm trying to define a training job with an existing python class. to my understanding, i could create my own container but would rather not deal with container management. when choosing \"algorithm source\" there is the option of \"your own algorithm source\" but nothing is listed under resources. where does this come from? i know i could do this through a notebook, but i really want this defined in a job that can be invoked through an endpoint.",
        "Question_original_content_gpt_summary":"The user is trying to create a training job with their own TensorFlow code without having to build a container, but is having difficulty understanding how to access the \"your own algorithm source\" option under resources.",
        "Question_preprocessed_content":"Title: how do i create a training job with my own tensorflow code without having to build a container?; Content: i'm trying to define a training job with an existing python class. to my understanding, i could create my own container but would rather not deal with container management. when choosing algorithm source there is the option of your own algorithm source but nothing is listed under resources. where does this come from? i know i could do this through a notebook, but i really want this defined in a job that can be invoked through an endpoint.",
        "Answer_original_content":"as bruno has said you will have to use a container somewhere, but you can use an existing container to run your own custom tensorflow code. there is a good example in the github for how to do this. the way this works is you modify your code to have an entry point which takes argparse command line arguments, and then you point a ' tensorflow estimator' to the entry point. then when you call fit on the estimator it will download the tensorflow container and run your custom code in there. so you start off with your own custom code that looks something like this # my_custom_code.py import tensorflow as tf import numpy as np def build_net(): # single fully connected image_place = tf.placeholder(tf.float32, [none, 28*28]) label_place = tf.placeholder(tf.int32, [none,]) net = tf.layers.dense(image_place, units=1024, activation=tf.nn.relu) net = tf.layers.dense(net, units=10, activation=none) return image_place, label_place, net def process_data(): # load (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data() # center x_train = x_train \/ 255.0 m = x_train.mean() x_train = x_train - m # convert to right types x_train = x_train.astype(np.float32) y_train = y_train.astype(np.int32) # reshape so flat x_train = np.reshape(x_train, [-1, 28*28]) return x_train, y_train def train_model(init_learn, epochs): image_p, label_p, logit = build_net() x_train, y_train = process_data() loss = tf.nn.softmax_cross_entropy_with_logits_v2( logits=logit, labels=label_p) optimiser = tf.train.adamoptimizer(init_learn) train_step = optimiser.minimize(loss) with tf.session() as sess: sess.run(tf.global_variables_initializer()) for _ in range(epochs): sess.run(train_step, feed_dict={image_p: x_train, label_p: y_train}) if __name__ == '__main__': train_model(0.001, 10) to make it work with we need to create a command line entry point, which will allow to run it in the container it will download for us eventually. # entry.py import argparse from my_custom_code import train_model if __name__ == '__main__': parser = argparse.argumentparser( formatter_class=argparse.argumentdefaultshelpformatter) parser.add_argument( '--model_dir', type=str) parser.add_argument( '--init_learn', type=float) parser.add_argument( '--epochs', type=int) args = parser.parse_args() train_model(args.init_learn, args.epochs) apart from specifying the arguments my function needs to take, we also need to provide a model_dir argument. this is always required, and is an s3 location which is where an model artifacts will be saved when the training job completes. note that you don't need to specify what this value is (though you can) as will provide a default location in s3 for you. so we have modified our code, now we need to actually run it on . go to the aws console and fire up a small instance from . download your custom code to the instance, and then create a jupyter notebook as follows: # _run.ipyb import from .tensorflow import tensorflow hyperparameters = { 'epochs': 10, 'init_learn': 0.001} role = .get_execution_role() source_dir = '\/path\/to\/folder\/with\/my\/code\/on\/instance' estimator = tensorflow( entry_point='entry.py', source_dir=source_dir, train_instance_type='ml.t2.medium', train_instance_count=1, hyperparameters=hyperparameters, role=role, py_version='py3', framework_version='1.12.0', script_mode=true) estimator.fit() running the above will: spin up an ml.t2.medium instance download the tensorflow 1.12.0 container to the instance download any data we specify in fit to the newly created instance in fit (in this case nothing) run our code on the instance upload the model artifacts to model_dir and that is pretty much it. there is of course a lot not mentioned here but you can: download training\/testing data from s3 save checkpoint files, and tensorboard files during training and upload them to s3 the best resource i found was the example i shared but here are all the things i was looking at to get this working: example code again documentation explanation of environment variables",
        "Answer_original_content_gpt_summary":"Possible solutions to create a training job with custom TensorFlow code without building a container are to modify the code to have an entry point that takes argparse command line arguments, point a 'tensorflow estimator' to the entry point, and call fit on the estimator. This will download the TensorFlow container and run the custom code in there. Additionally, a command line entry point needs to be created to allow running the code in the container. The model_dir argument is always required and is an S3 location where model artifacts will be saved when the training job completes. The code needs to be downloaded to an instance and run using a Jupyter notebook. The example code and documentation can be helpful resources.",
        "Answer_preprocessed_content":"as bruno has said you will have to use a container somewhere, but you can use an existing container to run your own custom tensorflow code. there is a good example in the github for how to do this. the way this works is you modify your code to have an entry point which takes argparse command line arguments, and then you point a ' tensorflow estimator' to the entry point. then when you call fit on the estimator it will download the tensorflow container and run your custom code in there. so you start off with your own custom code that looks something like this to make it work with we need to create a command line entry point, which will allow to run it in the container it will download for us eventually. apart from specifying the arguments my function needs to take, we also need to provide a argument. this is always required, and is an s location which is where an model artifacts will be saved when the training job completes. note that you don't need to specify what this value is as will provide a default location in s for you. so we have modified our code, now we need to actually run it on . go to the aws console and fire up a small instance from . download your custom code to the instance, and then create a jupyter notebook as follows running the above will spin up an instance download the tensorflow container to the instance download any data we specify in fit to the newly created instance in fit run our code on the instance upload the model artifacts to and that is pretty much it. there is of course a lot not mentioned here but you can download data from s save checkpoint files, and tensorboard files during training and upload them to s the best resource i found was the example i shared but here are all the things i was looking at to get this working example code again documentation explanation of environment variables"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to remove old versions of data files?",
        "Question_body":"<p>My dvc repository got big, and I do not need the old versions of data files.<\/p>\n<p>Is there any way to remove them from the repository, to save the disk space?<\/p>\n<p>In fact, if there are no other options, I would remove my dvc repository and start it again.<\/p>\n<p>However, I do not want to recreate my dvc files (pipelines), keeping the existing ones.<\/p>\n<p>Is this possible?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1574756258176,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":1008.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/is-it-possible-to-remove-old-versions-of-data-files\/256",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-11-26T09:09:30.645Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/byoussin\">@byoussin<\/a> !<\/p>\n<p>Sure, you can use <code>dvc gc<\/code> command for that. In short running <code>dvc gc<\/code>(no args) will remove everything from cache, except for those cache files that are used in your current workspace. Check out <a href=\"https:\/\/dvc.org\/doc\/command-reference\/gc\">https:\/\/dvc.org\/doc\/command-reference\/gc<\/a> for more details. And let us know if you have any further questions <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_score":37.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is it possible to remove old versions of data files?; Content: my repository got big, and i do not need the old versions of data files. is there any way to remove them from the repository, to save the disk space? in fact, if there are no other options, i would remove my repository and start it again. however, i do not want to recreate my files (pipelines), keeping the existing ones. is this possible? thanks",
        "Question_original_content_gpt_summary":"The user is looking for a way to remove old versions of data files from their repository without having to recreate their existing files and pipelines.",
        "Question_preprocessed_content":"Title: is it possible to remove old versions of data files?; Content: my repository got big, and i do not need the old versions of data files. is there any way to remove them from the repository, to save the disk space? in fact, if there are no other options, i would remove my repository and start it again. however, i do not want to recreate my files , keeping the existing ones. is this possible? thanks",
        "Answer_original_content":"hi @byoussin ! sure, you can use gc command for that. in short running gc(no args) will remove everything from cache, except for those cache files that are used in your current workspace. check out https:\/\/.org\/doc\/command-reference\/gc for more details. and let us know if you have any further questions",
        "Answer_original_content_gpt_summary":"Possible solution: The user can use the \"gc\" command to remove old versions of data files from their repository. Running \"gc\" without any arguments will remove everything from cache, except for those cache files that are used in the current workspace. More details can be found at https:\/\/.org\/doc\/command-reference\/gc.",
        "Answer_preprocessed_content":"hi ! sure, you can use command for that. in short running will remove everything from cache, except for those cache files that are used in your current workspace. check out for more details. and let us know if you have any further questions"
    },
    {
        "Question_id":70610418.0,
        "Question_title":"Why doesn't my Kedro starter prompt for input?",
        "Question_body":"<p>I would like to create my own Kedro starter. I have tried to replicate the relevant portions of the pandas iris starter. I have a <code>cookiecutter.json<\/code> file with what I believe are appropriate mappings, and I have changed the repo and package directory names as well as any references to Kedro version such that they work with cookie cutter.<\/p>\n<p>I am able to generate a new project from my starter with <code>kedro new --starter=path\/to\/my\/starter<\/code>. <strong>However, the newly created project uses the default values for the project, package, and repo names, without prompting me for any input in the terminal<\/strong>.<\/p>\n<p>Have I misconfigured something? How can I create a starter that will prompt users to override the defaults when creating new projects?<\/p>\n<p>Here are the contents of <code>cookiecutter.json<\/code> in the top directory of my starter project:<\/p>\n<pre><code>{\n    &quot;project_name&quot;: &quot;default&quot;,\n    &quot;repo_name&quot;: &quot;{{ cookiecutter.project_name }}&quot;,\n    &quot;python_package&quot;: &quot;{{ cookiecutter.repo_name }}&quot;,\n    &quot;kedro_version&quot;: &quot;{{ cookiecutter.kedro_version }}&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1641486876077,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":79.0,
        "Owner_creation_time":1416091573812,
        "Owner_last_access_time":1659708520190,
        "Owner_reputation":197.0,
        "Owner_up_votes":36.0,
        "Owner_down_votes":0.0,
        "Owner_views":23.0,
        "Answer_body":"<p>I think you may be missing <code>prompts.yml<\/code>\n<a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/main\/kedro\/templates\/project\/prompts.yml<\/a><\/p>\n<p>Full instructions can be found here:\n<a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/05_create_kedro_starters.html<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1641488156140,
        "Answer_score":1.0,
        "Owner_location":"Texas, USA",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70610418",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why doesn't my starter prompt for input?; Content: i would like to create my own starter. i have tried to replicate the relevant portions of the pandas iris starter. i have a cookiecutter.json file with what i believe are appropriate mappings, and i have changed the repo and package directory names as well as any references to version such that they work with cookie cutter. i am able to generate a new project from my starter with new --starter=path\/to\/my\/starter. however, the newly created project uses the default values for the project, package, and repo names, without prompting me for any input in the terminal. have i misconfigured something? how can i create a starter that will prompt users to override the defaults when creating new projects? here are the contents of cookiecutter.json in the top directory of my starter project: { \"project_name\": \"default\", \"repo_name\": \"{{ cookiecutter.project_name }}\", \"python_package\": \"{{ cookiecutter.repo_name }}\", \"_version\": \"{{ cookiecutter._version }}\" }",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with their starter prompt not prompting for input when creating a new project.",
        "Question_preprocessed_content":"Title: why doesn't my starter prompt for input?; Content: i would like to create my own starter. i have tried to replicate the relevant portions of the pandas iris starter. i have a file with what i believe are appropriate mappings, and i have changed the repo and package directory names as well as any references to version such that they work with cookie cutter. i am able to generate a new project from my starter with . however, the newly created project uses the default values for the project, package, and repo names, without prompting me for any input in the terminal. have i misconfigured something? how can i create a starter that will prompt users to override the defaults when creating new projects? here are the contents of in the top directory of my starter project",
        "Answer_original_content":"i think you may be missing prompts.yml https:\/\/github.com\/quantumblacklabs\/\/blob\/main\/\/templates\/project\/prompts.yml full instructions can be found here: https:\/\/.readthedocs.io\/en\/stable\/07_extend_\/05_create__starters.html",
        "Answer_original_content_gpt_summary":"The possible solution to the challenge of the starter prompt not prompting for input when creating a new project is to check if the prompts.yml file is missing. The user can find the full instructions on how to resolve the issue at the provided link.",
        "Answer_preprocessed_content":"i think you may be missing full instructions can be found here"
    },
    {
        "Question_id":null,
        "Question_title":"Facing issues in gitlab-runner for ci-cml and face issue to use AWS-s3 bucket for dvc in gitlab",
        "Question_body":"<p>I am working on Mlops. so train.py is my script in which i am using dvc for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and I have to create ci-cml pipeline, but not able to use s3 bucket to use dvc data and facing error when try run .gitlab-runner so find error in gitlab-runner.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1662585071904,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":86.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/facing-issues-in-gitlab-runner-for-ci-cml-and-face-issue-to-use-aws-s3-bucket-for-dvc-in-gitlab\/1332",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-07T21:13:25.421Z",
                "Answer_body":"<p>I am working on Mlops. so train.py is my script in which i am using dvc for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and I have to create ci-cml pipeline, but not able to use s3 bucket to use dvc data and facing error when try run .gitlab-runner so find error in gitlab-runner.<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-13T07:43:25.229Z",
                "Answer_body":"<p>What is the error message that you get?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: facing issues in gitlab-runner for ci-cml and face issue to use aws-s3 bucket for in gitlab; Content: i am working on mlops. so train.py is my script in which i am using for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and i have to create ci-cml pipeline, but not able to use s3 bucket to use data and facing error when try run .gitlab-runner so find error in gitlab-runner.",
        "Question_original_content_gpt_summary":"The user is facing issues with using Gitlab-Runner for CI-CML and using an AWS-S3 bucket for data versioning and training a model.",
        "Question_preprocessed_content":"Title: facing issues in for and face issue to use bucket for in gitlab; Content: i am working on mlops. so is my script in which i am using for data versioning and train the model by using s bucket. this all thing i push on gitlab and i have to create pipeline, but not able to use s bucket to use data and facing error when try run so find error in",
        "Answer_original_content":"i am working on mlops. so train.py is my script in which i am using for data versioning and train the model by using s3 bucket. this all thing i push on gitlab and i have to create ci-cml pipeline, but not able to use s3 bucket to use data and facing error when try run .gitlab-runner so find error in gitlab-runner. what is the error message that you get?",
        "Answer_original_content_gpt_summary":"Possible solutions mentioned in the answer are:\n- Using an AWS-S3 bucket for data versioning and training a model.\n- Pushing the code on Gitlab and creating a CI-CML pipeline.\n- Finding the error in Gitlab-Runner to resolve the issue.",
        "Answer_preprocessed_content":"i am working on mlops. so is my script in which i am using for data versioning and train the model by using s bucket. this all thing i push on gitlab and i have to create pipeline, but not able to use s bucket to use data and facing error when try run so find error in what is the error message that you get?"
    },
    {
        "Question_id":58377444.0,
        "Question_title":"swagger.json example json for forecast model doesn't seem to return predictions",
        "Question_body":"<p>When trying to make predictions for forecasting models using Azure ML Service, the swagger.json includes the following schema for input:<\/p>\n\n<pre><code>\"example\": {\"data\": [{\"date\": \"2019-08-30T00:00:00.000Z\", \"y_query\": 1.0}]}\n<\/code><\/pre>\n\n<p>However, when I feed this as an input to generate predictions, I receive the following error:<\/p>\n\n<pre><code>data= {\"data\": [{\"date\": \"2019-08-30T00:00:00.000Z\", \"y_query\": 1 }]}\n# Convert to JSON string\ninput_data = json.dumps(data)\n\n# Set the content type\nheaders = {'Content-Type': 'application\/json'}\n# If authentication is enabled, set the authorization header\n#headers['Authorization'] = f'Bearer {key}'\n\n# Make the request and display the response\nresp = requests.post(scoring_uri, input_data, headers=headers)\nprint(resp.text)\n\n<\/code><\/pre>\n\n<pre><code>\"{\\\"error\\\": \\\"DataException:\\\\n\\\\tMessage: y values are present for each date. Nothing to forecast.\\\\n\\\\tInnerException None\\\\n\\\\tErrorResponse \\\\n{\\\\n    \\\\\\\"error\\\\\\\": {\\\\n        \\\\\\\"code\\\\\\\": \\\\\\\"UserError\\\\\\\",\\\\n        \\\\\\\"inner_error\\\\\\\": {\\\\n            \\\\\\\"code\\\\\\\": \\\\\\\"InvalidData\\\\\\\"\\\\n        },\\\\n        \\\\\\\"message\\\\\\\": \\\\\\\"y values are present for each date. Nothing to forecast.\\\\\\\"\\\\n    }\\\\n}\\\"}\"\n<\/code><\/pre>\n\n<p>I have tried not passing a y value, which causes an 'expected two axis got one' and passing 0 as the y_query. Any guidance on how to make predictions using this approach would be greatly appreciated. <\/p>\n\n<p>The documentation for web services is here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":6.0,
        "Question_creation_time":1571058071523,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":530.0,
        "Owner_creation_time":1464949169832,
        "Owner_last_access_time":1657738559383,
        "Owner_reputation":25.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":14.0,
        "Answer_body":"<p>Try using nan as the value for y_query. and make sure the date is the next time unit after the one that was used in the training set.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1571417536483,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58377444",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: swagger.json example json for forecast model doesn't seem to return predictions; Content: when trying to make predictions for forecasting models using service, the swagger.json includes the following schema for input: \"example\": {\"data\": [{\"date\": \"2019-08-30t00:00:00.000z\", \"y_query\": 1.0}]} however, when i feed this as an input to generate predictions, i receive the following error: data= {\"data\": [{\"date\": \"2019-08-30t00:00:00.000z\", \"y_query\": 1 }]} # convert to json string input_data = json.dumps(data) # set the content type headers = {'content-type': 'application\/json'} # if authentication is enabled, set the authorization header #headers['authorization'] = f'bearer {key}' # make the request and display the response resp = requests.post(scoring_uri, input_data, headers=headers) print(resp.text) \"{\\\"error\\\": \\\"dataexception:\\\\n\\\\tmessage: y values are present for each date. nothing to forecast.\\\\n\\\\tinnerexception none\\\\n\\\\terrorresponse \\\\n{\\\\n \\\\\\\"error\\\\\\\": {\\\\n \\\\\\\"code\\\\\\\": \\\\\\\"usererror\\\\\\\",\\\\n \\\\\\\"inner_error\\\\\\\": {\\\\n \\\\\\\"code\\\\\\\": \\\\\\\"invaliddata\\\\\\\"\\\\n },\\\\n \\\\\\\"message\\\\\\\": \\\\\\\"y values are present for each date. nothing to forecast.\\\\\\\"\\\\n }\\\\n}\\\"}\" i have tried not passing a y value, which causes an 'expected two axis got one' and passing 0 as the y_query. any guidance on how to make predictions using this approach would be greatly appreciated. the documentation for web services is here: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-consume-web-service",
        "Question_original_content_gpt_summary":"The user is encountering challenges when trying to make predictions for forecasting models using a service, as the swagger.json example json does not seem to return predictions.",
        "Question_preprocessed_content":"Title: example json for forecast model doesn't seem to return predictions; Content: when trying to make predictions for forecasting models using service, the includes the following schema for input however, when i feed this as an input to generate predictions, i receive the following error i have tried not passing a y value, which causes an 'expected two axis got one' and passing as the any guidance on how to make predictions using this approach would be greatly appreciated. the documentation for web services is here",
        "Answer_original_content":"try using nan as the value for y_query. and make sure the date is the next time unit after the one that was used in the training set.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of not getting predictions from a forecasting model service using swagger.json example json are: using \"nan\" as the value for y_query and ensuring that the date used is the next time unit after the one used in the training set.",
        "Answer_preprocessed_content":"try using nan as the value for and make sure the date is the next time unit after the one that was used in the training set."
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Studio - create domain error",
        "Question_body":"A customer is trying to setup Sagemaker studio. He is following our published instructions to set up using IAM: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-iam.html\n\nBut is getting an error: User: arn:aws:iam:xxxx:user\/user1 is not authorized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker: us-east-2:xxxx:domain\/yyyy\n\nHe has admin priviledges on the account and AmazonSageMakerFullAccess. We noticed that the AmazonSageMakerFullAccess policy actually has a limitation. You can perform all sagemaker actions, but not on a resource with arn \u201carn:aws:sagemaker:::domain\/*\u201d. We confirmed there are no other domains in that region with the CLI as you are only allowed one \u2013 so that isn\u2019t blocking. And aws sagemaker list-user-profiles returns no user profiles.\n\nHas anyone seen that error before or know the workaround? Should he create a custom policy to enable creating domains or would there be any implications of that? Are there specific permissions he should have so as to onboard using IAM?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1586796156000,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":792.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyWQfPusnSHG6Ujfzx27o1w\/sagemaker-studio-create-domain-error",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-04-13T19:51:10.000Z",
                "Answer_score":1,
                "Answer_body":"A user with admin privileges would have access to \"iam:CreateServiceLinkedRole\" and \"sagemaker:CreateDomain\" actions, unless SCPs or permissions boundaries are involved. However, for the purpose of onboarding Amazon SageMaker Studio with limited permissions, I would grant the user least privilege by reviewing Control Access to the Amazon SageMaker API by Using Identity-based Policies and Actions, Resources, and Condition Keys for Amazon SageMaker documentation:\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"sagemaker:CreateDomain\",\n    \"Resource\": \"arn:aws:sagemaker:<REGION>:<ACCOUNT-ID>:domain\/*\"\n}\n\n\nNOTE: An AWS account is limited to one Domain, per region, see CreateDomain.\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"iam:CreateServiceLinkedRole\",\n    \"Resource\": \"*\",\n    \"Condition\": {\n        \"StringEquals\": {\n            \"iam:AWSServiceName\": \"sagemaker.amazonaws.com\"\n        }\n    }\n}\n\n\nCheers!",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: studio - create domain error; Content: a customer is trying to setup studio. he is following our published instructions to set up using iam: https:\/\/docs.aws.amazon.com\/\/latest\/dg\/onboard-iam.html but is getting an error: user: arn:aws:iam:xxxx:user\/user1 is not authorized to perform: :createdomain on resource: arn:aws:: us-east-2:xxxx:domain\/yyyy he has admin priviledges on the account and amazonfullaccess. we noticed that the amazonfullaccess policy actually has a limitation. you can perform all actions, but not on a resource with arn \u201carn:aws::::domain\/*\u201d. we confirmed there are no other domains in that region with the cli as you are only allowed one \u2013 so that isn\u2019t blocking. and list-user-profiles returns no user profiles. has anyone seen that error before or know the workaround? should he create a custom policy to enable creating domains or would there be any implications of that? are there specific permissions he should have so as to onboard using iam?",
        "Question_original_content_gpt_summary":"The user is encountering an error while trying to set up Studio using IAM, and is looking for a workaround or specific permissions to enable creating domains.",
        "Question_preprocessed_content":"Title: studio create domain error; Content: a customer is trying to setup studio. he is following our published instructions to set up using iam but is getting an error user is not authorized to perform createdomain on resource arn he has admin priviledges on the account and amazonfullaccess. we noticed that the amazonfullaccess policy actually has a limitation. you can perform all actions, but not on a resource with arn we confirmed there are no other domains in that region with the cli as you are only allowed one so that isnt blocking. and returns no user profiles. has anyone seen that error before or know the workaround? should he create a custom policy to enable creating domains or would there be any implications of that? are there specific permissions he should have so as to onboard using iam?",
        "Answer_original_content":"a user with admin privileges would have access to \"iam:createservicelinkedrole\" and \":createdomain\" actions, unless scps or permissions boundaries are involved. however, for the purpose of onboarding studio with limited permissions, i would grant the user least privilege by reviewing control access to the api by using identity-based policies and actions, resources, and condition keys for documentation: { \"effect\": \"allow\", \"action\": \":createdomain\", \"resource\": \"arn:aws::::domain\/*\" } note: an aws account is limited to one domain, per region, see createdomain. { \"effect\": \"allow\", \"action\": \"iam:createservicelinkedrole\", \"resource\": \"*\", \"condition\": { \"stringequals\": { \"iam:awsservicename\": \".amazonaws.com\" } } } cheers!",
        "Answer_original_content_gpt_summary":"Possible solutions to the error encountered while setting up Studio using IAM include granting the user least privilege by reviewing control access to the API using identity-based policies and actions, resources, and condition keys. A user with admin privileges would have access to \"iam:createservicelinkedrole\" and \":createdomain\" actions, unless scps or permissions boundaries are involved. Additionally, the user can use the following documentation to create an allow effect for the \"iam:createservicelinkedrole\" and \":createdomain\" actions: { \"effect\": \"allow\", \"action\": \":createdomain\", \"resource\": \"arn:aws::::domain\/*\" } and { \"effect\": \"allow\", \"action\": \"iam:createservicelinkedrole\", \"resource\": \"*\", \"condition\": { \"stringequals\": { \"iam:awsservicename\": \".amazonaws.com\" } } }.",
        "Answer_preprocessed_content":"a user with admin privileges would have access to iam createservicelinle and createdomain actions, unless scps or permissions boundaries are involved. however, for the purpose of onboarding studio with limited permissions, i would grant the user least privilege by reviewing control access to the api by using policies and actions, resources, and condition keys for documentation effect allow , action createdomain , resource note an aws account is limited to one domain, per region, see createdomain. effect allow , action iam createservicelinle , resource , condition cheers!"
    },
    {
        "Question_id":63920599.0,
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1600261190477,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":405.0,
        "Owner_creation_time":1600260166047,
        "Owner_last_access_time":1615561616230,
        "Owner_reputation":15.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1600604920243,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1600855880503,
        "Answer_last_edit_time":1600855957376,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: powerbi and integration (through ); Content: i'm currently trying to integrate an ml model currently deployed as a webservice on with powerbi. i see that it can be integrated but the model requires the addition of a schema file when it is being deployed as a webservice. without this, the model can't be viewed in powerbi. the problem that i have come up against is that i use to log ml model performances and subsequently to deploy a selected model onto as a webservice using 's integration - ..deploy(). this unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in powerbi as it lacks the required schema file. my options seem to be: find a workaround, possibly using the working rest api of the model in a power query. rewrite the deployment code and handle the webservice deployment steps in azure instead of . i thought i would ask to see if i am maybe missing something as i can't find a workaround using my current code to define a schema file in when deploying with ..deploy().",
        "Question_original_content_gpt_summary":"The user is encountering challenges in integrating an ML model deployed as a webservice on Azure with PowerBI, as the model requires the addition of a schema file when it is being deployed as a webservice, and the current code does not have the option to define a schema file when deploying with mlflow..deploy().",
        "Question_preprocessed_content":"Title: powerbi and integration ; Content: i'm currently trying to integrate an ml model currently deployed as a webservice on with powerbi. i see that it can be integrated but the model requires the addition of a schema file when it is being deployed as a webservice. without this, the model can't be viewed in powerbi. the problem that i have come up against is that i use to log ml model performances and subsequently to deploy a selected model onto as a webservice using 's integration this unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in powerbi as it lacks the required schema file. my options seem to be find a workaround, possibly using the working rest api of the model in a power query. rewrite the deployment code and handle the webservice deployment steps in azure instead of . i thought i would ask to see if i am maybe missing something as i can't find a workaround using my current code to define a schema file in when deploying with",
        "Answer_original_content":"point number 2 is the way we solved this issue. instead of using to deploy to a scoring service on azure, we wrote a custom code which load model when container is initialised. scoring code is something like this: import os import json from .pyfunc import load_model from inference_schema.schema_decorators import input_schema, output_schema from inference_schema.parameter_types.numpy_parameter_type import numpyparametertype def init(): global model model = load_model(os.path.join(os.environ.get(\"azureml_model_dir\"), \"awesome_model\")) @input_schema('data', numpyparametertype(input_sample)) @output_schema(numpyparametertype(output_sample)) def run(data): return model.predict(data)",
        "Answer_original_content_gpt_summary":"The answer suggests a solution to the challenge of integrating an ML model deployed as a webservice on Azure with PowerBI. The solution involves writing custom code to load the model when the container is initialized, and using a scoring code to predict the output. The code includes importing necessary libraries, defining input and output schemas, and using the predict() function to return the predicted output.",
        "Answer_preprocessed_content":"point number is the way we solved this issue. instead of using to deploy to a scoring service on azure, we wrote a custom code which load model when container is initialised. scoring code is something like this"
    },
    {
        "Question_id":58019308.0,
        "Question_title":"ScriptRunConfig with datastore reference on AML",
        "Question_body":"<p>When trying to run a ScriptRunConfig, using :<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>src = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', ds.as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>\n\n<p>It doesn't work and breaks with this when I submit the job : <\/p>\n\n<pre><code>... lots of things... and then\nTypeError: Object of type 'DataReference' is not JSON serializable\n<\/code><\/pre>\n\n<p>However if I run it with the Estimator, it works. One of the differences is the fact that with a <code>ScriptRunConfig<\/code> we're using a list for parameters and the other is a dictionary.<\/p>\n\n<p>Thanks for any pointers!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1568929720367,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":1541.0,
        "Owner_creation_time":1538275960603,
        "Owner_last_access_time":1658458641830,
        "Owner_reputation":381.0,
        "Owner_up_votes":75.0,
        "Owner_down_votes":2.0,
        "Owner_views":50.0,
        "Answer_body":"<p>Being able to use <code>DataReference<\/code> in <code>ScriptRunConfig<\/code> is a bit more involved than doing just <code>ds.as_mount()<\/code>. You will need to convert it into a string in <code>arguments<\/code> and then update the <code>RunConfiguration<\/code>'s <code>data_references<\/code> section with the <code>DataReferenceConfiguration<\/code> created from <code>ds<\/code>. Please <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\" rel=\"nofollow noreferrer\">see here<\/a> for an example notebook on how to do that.<\/p>\n<p>If you are just reading from the input location and not doing any writes to it, please check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\"><code>Dataset<\/code><\/a>. It allows you to do exactly what you are doing without doing anything extra. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">Here is an example notebook<\/a> that shows this in action.<\/p>\n<p>Below is a short version of the notebook<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\n\n# more imports and code\n\nds = Datastore(workspace, 'mydatastore')\ndataset = Dataset.File.from_files(path=(ds, 'path\/to\/input-data\/within-datastore'))\n\nsrc = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', dataset.as_named_input('input').as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1568945686667,
        "Answer_score":4.0,
        "Owner_location":"Montreal, QC, Canada",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1595974462436,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58019308",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: scriptrunconfig with datastore reference on aml; Content: when trying to run a scriptrunconfig, using : src = scriptrunconfig(source_directory=project_folder, script='train.py', arguments=['--input-data-dir', ds.as_mount(), '--reg', '0.99'], run_config=run_config) run = experiment.submit(config=src) it doesn't work and breaks with this when i submit the job : ... lots of things... and then typeerror: object of type 'datareference' is not json serializable however if i run it with the estimator, it works. one of the differences is the fact that with a scriptrunconfig we're using a list for parameters and the other is a dictionary. thanks for any pointers!",
        "Question_original_content_gpt_summary":"The user is encountering a TypeError when trying to run a scriptrunconfig with a datastore reference on Azure Machine Learning, but the same code works when using an estimator.",
        "Question_preprocessed_content":"Title: scriptrunconfig with datastore reference on aml; Content: when trying to run a scriptrunconfig, using it doesn't work and breaks with this when i submit the job however if i run it with the estimator, it works. one of the differences is the fact that with a we're using a list for parameters and the other is a dictionary. thanks for any pointers!",
        "Answer_original_content":"being able to use datareference in scriptrunconfig is a bit more involved than doing just ds.as_mount(). you will need to convert it into a string in arguments and then update the runconfiguration's data_references section with the datareferenceconfiguration created from ds. please see here for an example notebook on how to do that. if you are just reading from the input location and not doing any writes to it, please check out dataset. it allows you to do exactly what you are doing without doing anything extra. here is an example notebook that shows this in action. below is a short version of the notebook from .core import dataset # more imports and code ds = datastore(workspace, 'mydatastore') dataset = dataset.file.from_files(path=(ds, 'path\/to\/input-data\/within-datastore')) src = scriptrunconfig(source_directory=project_folder, script='train.py', arguments=['--input-data-dir', dataset.as_named_input('input').as_mount(), '--reg', '0.99'], run_config=run_config) run = experiment.submit(config=src)",
        "Answer_original_content_gpt_summary":"Possible solutions to the TypeError encountered when using a datastore reference in scriptrunconfig on Azure Machine Learning are: converting the reference into a string in arguments and updating the runconfiguration's data_references section with the datareferenceconfiguration created from the datastore, or using the dataset module if only reading from the input location and not doing any writes to it. Examples of how to implement these solutions can be found in the provided notebooks.",
        "Answer_preprocessed_content":"being able to use in is a bit more involved than doing just . you will need to convert it into a string in and then update the 's section with the created from . please see here for an example notebook on how to do that. if you are just reading from the input location and not doing any writes to it, please check out . it allows you to do exactly what you are doing without doing anything extra. here is an example notebook that shows this in action. below is a short version of the notebook"
    },
    {
        "Question_id":62541587.0,
        "Question_title":"How to access python variables in Sagemaker Jupyter Notebook shell command",
        "Question_body":"<p>In one of the cells of Sagemaker notebook, I've set a variable<\/p>\n<pre><code>region=&quot;us-west-2&quot;\n<\/code><\/pre>\n<p>In subsequent cell, I run following 2 shell commands<\/p>\n<pre><code>!echo $region\n<\/code><\/pre>\n<p>Output<\/p>\n<pre><code>us-west-2\n<\/code><\/pre>\n<p>However, unable to run aws shell command using this variable<\/p>\n<pre><code>!aws ecr get-login-password --region $region\n<\/code><\/pre>\n<p><code>$ variable-name<\/code> doesn't help inside jupyter cell <code>! shell command<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1592937308720,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1285.0,
        "Owner_creation_time":1437915365627,
        "Owner_last_access_time":1663934285580,
        "Owner_reputation":2665.0,
        "Owner_up_votes":1461.0,
        "Owner_down_votes":14.0,
        "Owner_views":722.0,
        "Answer_body":"<p>As answered here: <a href=\"https:\/\/stackoverflow.com\/a\/19674648\/5157515\">https:\/\/stackoverflow.com\/a\/19674648\/5157515<\/a><\/p>\n<p>There's no direct way to access python variables with <code>!<\/code> command.<\/p>\n<p>But with magic command <code>%%bash<\/code> it is possible<\/p>\n<pre><code>%%bash  -s &quot;$region&quot;\naws ecr get-login-password --region $1\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1593150774772,
        "Answer_score":1.0,
        "Owner_location":"London, UK",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62541587",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to access python variables in jupyter notebook shell command; Content: in one of the cells of notebook, i've set a variable region=\"us-west-2\" in subsequent cell, i run following 2 shell commands !echo $region output us-west-2 however, unable to run aws shell command using this variable !aws ecr get-login-password --region $region $ variable-name doesn't help inside jupyter cell ! shell command",
        "Question_original_content_gpt_summary":"The user is encountering challenges accessing Python variables in a Jupyter Notebook shell command.",
        "Question_preprocessed_content":"Title: how to access python variables in jupyter notebook shell command; Content: in one of the cells of notebook, i've set a variable in subsequent cell, i run following shell commands output however, unable to run aws shell command using this variable doesn't help inside jupyter cell",
        "Answer_original_content":"as answered here: https:\/\/stackoverflow.com\/a\/19674648\/5157515 there's no direct way to access python variables with ! command. but with magic command %%bash it is possible %%bash -s \"$region\" aws ecr get-login-password --region $1",
        "Answer_original_content_gpt_summary":"The answer suggests that there is no direct way to access Python variables in a Jupyter Notebook shell command. However, it provides a possible solution by using the magic command %%bash with the appropriate arguments to access the required variables. The solution involves using the command %%bash -s \"$region\" aws ecr get-login-password --region $1.",
        "Answer_preprocessed_content":"as answered here there's no direct way to access python variables with command. but with magic command it is possible"
    },
    {
        "Question_id":null,
        "Question_title":"How to access Azure Machine Learning Studio without a Public IP",
        "Question_body":"Dear Support Team,\n\nHope this email finds you well. I am writing to you because I am trying to access Azure ML Studio without a public IP but I am having some troubles.\n\nI saw the tutorial \"Create a secure workspace\" tutorial and I wanted to confirm that making the jump box is a \"must\" for accessing the azure ML studio without public IP, or I can do it from my browser after making all the secure space?\n\nIs there any method I can do it? I just can not upload any file\/ or see previous files\n\nAlso I have this error:\n[2]: \/answers\/storage\/attachments\/176304-screen-shot-2022-02-21-at-171846.png\n\nHowever I have the contributor permission\n![176315-screen-shot-2022-02-21-at-171938.png][1]![176304-screen-shot-2022-02-21-at-171846.png][2]\n[1]: \/answers\/storage\/attachments\/176315-screen-shot-2022-02-21-at-171938.png\n\nCan you help me with this?\n\nThank you for your time and hope you are having a great day,\n\nCatherine",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1645431850553,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/743872\/how-to-access-azure-machine-learning-studio-withou.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-23T06:31:08.52Z",
                "Answer_score":0,
                "Answer_body":"In the studio, select Datastores.\n\nTo update an existing datastore, select the datastore and select Update credentials.\n\nTo create a new datastore, select + New datastore.\n\nIn the datastore settings, select Yes for Use workspace managed identity for data preview and profiling in Azure Machine Learning studio.\n\nIn the Networking settings for the Azure Storage Account, add the Microsoft.MachineLearningService\/workspaces Resource type, and set the Instance name to the workspace.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to access studio without a public ip; Content: dear support team, hope this email finds you well. i am writing to you because i am trying to access studio without a public ip but i am having some troubles. i saw the tutorial \"create a secure workspace\" tutorial and i wanted to confirm that making the jump box is a \"must\" for accessing the studio without public ip, or i can do it from my browser after making all the secure space? is there any method i can do it? i just can not upload any file\/ or see previous files also i have this error: [2]: \/answers\/storage\/attachments\/176304-screen-shot-2022-02-21-at-171846.png however i have the contributor permission ![176315-screen-shot-2022-02-21-at-171938.png][1]![176304-screen-shot-2022-02-21-at-171846.png][2] [1]: \/answers\/storage\/attachments\/176315-screen-shot-2022-02-21-at-171938.png can you help me with this? thank you for your time and hope you are having a great day, catherine",
        "Question_original_content_gpt_summary":"The user is encountering challenges in accessing studio without a public IP, and is seeking assistance with an error they are receiving.",
        "Question_preprocessed_content":"Title: how to access studio without a public ip; Content: dear support team, hope this email finds you well. i am writing to you because i am trying to access studio without a public ip but i am having some troubles. i saw the tutorial create a secure workspace tutorial and i wanted to confirm that making the jump box is a must for accessing the studio without public ip, or i can do it from my browser after making all the secure space? is there any method i can do it? i just can not upload any file\/ or see previous files also i have this error however i have the contributor permission can you help me with this? thank you for your time and hope you are having a great day, catherine",
        "Answer_original_content":"in the studio, select datastores. to update an existing datastore, select the datastore and select update credentials. to create a new datastore, select + new datastore. in the datastore settings, select yes for use workspace managed identity for data preview and profiling in studio. in the networking settings for the azure storage account, add the microsoft.machinelearningservice\/workspaces resource type, and set the instance name to the workspace.",
        "Answer_original_content_gpt_summary":"Possible solutions to accessing studio without a public IP and resolving an error include updating or creating a datastore, selecting \"yes\" for using workspace managed identity for data preview and profiling in studio, and adding the microsoft.machinelearningservice\/workspaces resource type in the networking settings for the Azure storage account with the instance name set to the workspace.",
        "Answer_preprocessed_content":"in the studio, select datastores. to update an existing datastore, select the datastore and select update credentials. to create a new datastore, select + new datastore. in the datastore settings, select yes for use workspace managed identity for data preview and profiling in studio. in the networking settings for the azure storage account, add the resource type, and set the instance name to the workspace."
    },
    {
        "Question_id":64169189.0,
        "Question_title":"How can I deploy a re-trained Sagemaker model to an endpoint?",
        "Question_body":"<p>With an <code>sagemaker.estimator.Estimator<\/code>, I want to re-<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.deploy\" rel=\"nofollow noreferrer\">deploy<\/a> a model after retraining (calling <code>fit<\/code> with new data).<\/p>\n<p>When I call this<\/p>\n<pre><code>estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')\n<\/code><\/pre>\n<p>I get an error<\/p>\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) \nwhen calling the CreateEndpoint operation: \nCannot create already existing endpoint &quot;arn:aws:sagemaker:eu-east- \n1:1776401913911:endpoint\/zyx&quot;.\n<\/code><\/pre>\n<p>Apparently I want to use functionality like <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateEndpoint.html\" rel=\"nofollow noreferrer\">UpdateEndpoint<\/a>. How do I access that functionality from this API?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1601630651783,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":1958.0,
        "Owner_creation_time":1227171471292,
        "Owner_last_access_time":1664047108080,
        "Owner_reputation":17500.0,
        "Owner_up_votes":463.0,
        "Owner_down_votes":87.0,
        "Owner_views":1561.0,
        "Answer_body":"<p>Yes, under the hood the <code>model.deploy<\/code> creates a model, an endpoint configuration and an endpoint. When you call again the method from an already-deployed, trained estimator it will create an error because a similarly-configured endpoint is already deployed. What I encourage you to try:<\/p>\n<ul>\n<li><p>use the <code>update_endpoint=True<\/code> parameter. From the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"noreferrer\">SageMaker SDK doc<\/a>:\n<em>&quot;Additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing\nSageMaker endpoint. This can be done by specifying the existing\nendpoint name for the <code>endpoint_name<\/code> parameter along with the\n<code>update_endpoint<\/code> parameter as True within your <code>deploy()<\/code> call.&quot;<\/em><\/p>\n<\/li>\n<li><p>Alternatively, if you want to create a separate model you can specify a new <code>model_name<\/code> in your <code>deploy<\/code><\/p>\n<\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1601891469683,
        "Answer_score":6.0,
        "Owner_location":"Israel",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1602140796110,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64169189",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i deploy a re-trained model to an endpoint?; Content: with an .estimator.estimator, i want to re-deploy a model after retraining (calling fit with new data). when i call this estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge') i get an error botocore.exceptions.clienterror: an error occurred (validationexception) when calling the createendpoint operation: cannot create already existing endpoint \"arn:aws::eu-east- 1:1776401913911:endpoint\/zyx\". apparently i want to use functionality like updateendpoint. how do i access that functionality from this api?",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to deploy a re-trained model to an endpoint using an estimator.estimator, and is looking for a way to access the updateendpoint functionality from the API.",
        "Question_preprocessed_content":"Title: how can i deploy a model to an endpoint?; Content: with an , i want to a model after retraining . when i call this i get an error apparently i want to use functionality like updateendpoint. how do i access that functionality from this api?",
        "Answer_original_content":"yes, under the hood the model.deploy creates a model, an endpoint configuration and an endpoint. when you call again the method from an already-deployed, trained estimator it will create an error because a similarly-configured endpoint is already deployed. what i encourage you to try: use the update_endpoint=true parameter. from the sdk doc: \"additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing endpoint. this can be done by specifying the existing endpoint name for the endpoint_name parameter along with the update_endpoint parameter as true within your deploy() call.\" alternatively, if you want to create a separate model you can specify a new model_name in your deploy",
        "Answer_original_content_gpt_summary":"Possible solutions to the error encountered when deploying a re-trained model to an endpoint using an estimator.estimator are: using the update_endpoint=true parameter to deploy a different endpoint configuration that links to the model to an already existing endpoint, or specifying a new model_name in the deploy() call to create a separate model.",
        "Answer_preprocessed_content":"yes, under the hood the creates a model, an endpoint configuration and an endpoint. when you call again the method from an trained estimator it will create an error because a endpoint is already deployed. what i encourage you to try use the parameter. from the sdk doc additionally, it is possible to deploy a different endpoint configuration, which links to your model, to an already existing endpoint. this can be done by specifying the existing endpoint name for the parameter along with the parameter as true within your alternatively, if you want to create a separate model you can specify a new in your"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker endpoint creation fails for Multi Model",
        "Question_body":"When using scikit to create multi model, it throws an exception, but when in single model it works.\n\nComplains about model_fn implementation or ping issues, any tips on how to fix this?\n\ne.g container={\\n\", \" 'Image' : image_uri,\", \" 'Mode': 'MultiModel',\", \" 'ModelDataUrl': 's3:\/\/somepatch\/with\/all\/models\/,\", \" 'Environment': {'SAGEMAKER_SUBMIT_DIRECTORY': mme_artifacts_path,\", \" 'SAGEMAKER_PROGRAM': 'inference.py'} \"\n\nFile \"\/miniconda3\/bin\/serve\", line 8, in <module> sys.exit(serving_entrypoint()) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving.py\", line 144, in serving_entrypoint start_model_server() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_sklearn_container\/serving_mms.py\", line 124, in start_model_server modules.import_module(serving_env.module_dir, serving_env.module_name) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 263, in import_module six.reraise(_errors.ImportModuleError, _errors.ImportModuleError(e), sys.exc_info()[2]) File \"\/miniconda3\/lib\/python3.7\/site-packages\/six.py\", line 702, in reraise raise value.with_traceback(tb) File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_modules.py\", line 258, in import_module module = importlib.import_module(name) File \"\/miniconda3\/lib\/python3.7\/importlib\/init.py\", line 118, in import_module if name.startswith('.'):",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1649256875061,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":119.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2bUNsPi3Rgautb0ZycrziA\/sage-maker-endpoint-creation-fails-for-multi-model",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-08T09:24:37.439Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nWe would need a bit more information to help you, including a bit more detail on how you create the model (code wise) as well as more details on the errors you receive.\n\nI would also suggest first trying out one of the available examples for multi model endpoints, like this one: Amazon SageMaker Multi-Model Endpoints using Scikit Learn and from there modify to your own needs.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: endpoint creation fails for multi model; Content: when using scikit to create multi model, it throws an exception, but when in single model it works. complains about model_fn implementation or ping issues, any tips on how to fix this? e.g container={\\n\", \" 'image' : image_uri,\", \" 'mode': 'multimodel',\", \" 'modeldataurl': 's3:\/\/somepatch\/with\/all\/models\/,\", \" 'environment': {'_submit_directory': mme_artifacts_path,\", \" '_program': 'inference.py'} \" file \"\/miniconda3\/bin\/serve\", line 8, in sys.exit(serving_entrypoint()) file \"\/miniconda3\/lib\/python3.7\/site-packages\/_sklearn_container\/serving.py\", line 144, in serving_entrypoint start_model_server() file \"\/miniconda3\/lib\/python3.7\/site-packages\/_sklearn_container\/serving_mms.py\", line 124, in start_model_server modules.import_module(serving_env.module_dir, serving_env.module_name) file \"\/miniconda3\/lib\/python3.7\/site-packages\/_containers\/_modules.py\", line 263, in import_module six.reraise(_errors.importmoduleerror, _errors.importmoduleerror(e), sys.exc_info()[2]) file \"\/miniconda3\/lib\/python3.7\/site-packages\/six.py\", line 702, in reraise raise value.with_traceback(tb) file \"\/miniconda3\/lib\/python3.7\/site-packages\/_containers\/_modules.py\", line 258, in import_module module = importlib.import_module(name) file \"\/miniconda3\/lib\/python3.7\/importlib\/init.py\", line 118, in import_module if name.startswith('.'):",
        "Question_original_content_gpt_summary":"The user is encountering challenges with creating an endpoint for a multi model using scikit, as an exception is thrown when attempting to do so, but it works when using a single model.",
        "Question_preprocessed_content":"Title: endpoint creation fails for multi model; Content: when using scikit to create multi model, it throws an exception, but when in single model it works. complains about implementation or ping issues, any tips on how to fix this? 'image' 'mode' 'multimodel', , 'modeldataurl' 'environment' file line , in file line , in file line , in file line , in file line , in reraise raise file line , in module file line , in if",
        "Answer_original_content":"hi, we would need a bit more information to help you, including a bit more detail on how you create the model (code wise) as well as more details on the errors you receive. i would also suggest first trying out one of the available examples for multi model endpoints, like this one: multi-model endpoints using scikit learn and from there modify to your own needs.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Provide more information on how the model is created and the errors received.\n- Try using one of the available examples for multi-model endpoints using scikit learn and modify it to fit the user's needs.",
        "Answer_preprocessed_content":"hi, we would need a bit more information to help you, including a bit more detail on how you create the model as well as more details on the errors you receive. i would also suggest first trying out one of the available examples for multi model endpoints, like this one endpoints using scikit learn and from there modify to your own needs."
    },
    {
        "Question_id":62012264.0,
        "Question_title":"AWS SageMaker SparkML Schema Eroor: member.environment' failed to satisfy constraint",
        "Question_body":"<p>I am deploying a model onto AWS via Sagemaker:<\/p>\n\n<p>I set up my JSON schema as follow:<\/p>\n\n<pre><code>import json\nschema = {\n    \"input\": [\n        {\n            \"name\": \"V1\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V2\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V3\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V4\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V5\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V6\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V7\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V8\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V9\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V10\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V11\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V12\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V13\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V14\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V15\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V16\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V17\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V18\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V19\",\n            \"type\": \"double\"\n        }, \n                {\n            \"name\": \"V20\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V21\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V22\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V23\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V24\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V25\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V26\",\n            \"type\": \"double\"\n        }, \n        {\n            \"name\": \"V27\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"V28\",\n            \"type\": \"double\"\n        },\n        {\n            \"name\": \"Amount\",\n            \"type\": \"double\"\n        },         \n    ],\n    \"output\": \n        {\n            \"name\": \"features\",\n            \"type\": \"double\",\n            \"struct\": \"vector\"\n        }\n}\nschema_json = json.dumps(schema)\nprint(schema_json)\n<\/code><\/pre>\n\n<p>And deployed as:<\/p>\n\n<pre><code>from sagemaker.model import Model\nfrom sagemaker.pipeline import PipelineModel\nfrom sagemaker.sparkml.model import SparkMLModel\n\nsparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\nsparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\nxgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n\nmodel_name = 'inference-pipeline-' + timestamp_prefix\nsm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])\n\n    endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I got the error as below:<\/p>\n\n<p>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 1 validation error detected: Value '{SAGEMAKER_SPARKML_SCHEMA={\"input\": [{\"type\": \"double\", \"name\": \"V1\"}, {\"type\": \"double\", \"name\": \"V2\"}, {\"type\": \"double\", \"name\": \"V3\"}, {\"type\": \"double\", \"name\": \"V4\"}, {\"type\": \"double\", \"name\": \"V5\"}, {\"type\": \"double\", \"name\": \"V6\"}, {\"type\": \"double\", \"name\": \"V7\"}, {\"type\": \"double\", \"name\": \"V8\"}, {\"type\": \"double\", \"name\": \"V9\"}, {\"type\": \"double\", \"name\": \"V10\"}, {\"type\": \"double\", \"name\": \"V11\"}, {\"type\": \"double\", \"name\": \"V12\"}, {\"type\": \"double\", \"name\": \"V13\"}, {\"type\": \"double\", \"name\": \"V14\"}, {\"type\": \"double\", \"name\": \"V15\"}, {\"type\": \"double\", \"name\": \"V16\"}, {\"type\": \"double\", \"name\": \"V17\"}, {\"type\": \"double\", \"name\": \"V18\"}, {\"type\": \"double\", \"name\": \"V19\"}, {\"type\": \"double\", \"name\": \"V20\"}, {\"type\": \"double\", \"name\": \"V21\"}, {\"type\": \"double\", \"name\": \"V22\"}, {\"type\": \"double\", \"name\": \"V23\"}, {\"type\": \"double\", \"name\": \"V24\"}, {\"type\": \"double\", \"name\": \"V25\"}, {\"type\": \"double\", \"name\": \"V26\"}, {\"type\": \"double\", \"name\": \"V27\"}, {\"type\": \"double\", \"name\": \"V28\"}, {\"type\": \"double\", \"name\": \"Amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.<strong>member.environment' failed to satisfy constraint: Map value must satisfy constraint: [Member must have length less than or equal to 1024<\/strong>,** Member must have length greater than or equal to 0, Member must satisfy regular expression pattern: [\\S\\s]*]<\/p>\n\n<p>I try to reduce my features to 20 and it able to deploy. Just wondering how can I Pass the schema with 29 attributes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1590448983843,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":226.0,
        "Owner_creation_time":1359061977540,
        "Owner_last_access_time":1608951269536,
        "Owner_reputation":427.0,
        "Owner_up_votes":11.0,
        "Owner_down_votes":0.0,
        "Owner_views":62.0,
        "Answer_body":"<p>I do not think the environment length of 1024 limit will be increased in a short time. To work around this, you could try to rebuild the spark ml container with the <code>SAGEMAKER_SPARKML_SCHEMA<\/code> env var:<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-sparkml-serving-container\/blob\/master\/README.md#running-the-image-locally<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1600448991412,
        "Answer_score":0.0,
        "Owner_location":"New York, NY, USA",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62012264",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sparkml schema eroor: member.environment' failed to satisfy constraint; Content: i am deploying a model onto aws via : i set up my json schema as follow: import json schema = { \"input\": [ { \"name\": \"v1\", \"type\": \"double\" }, { \"name\": \"v2\", \"type\": \"double\" }, { \"name\": \"v3\", \"type\": \"double\" }, { \"name\": \"v4\", \"type\": \"double\" }, { \"name\": \"v5\", \"type\": \"double\" }, { \"name\": \"v6\", \"type\": \"double\" }, { \"name\": \"v7\", \"type\": \"double\" }, { \"name\": \"v8\", \"type\": \"double\" }, { \"name\": \"v9\", \"type\": \"double\" }, { \"name\": \"v10\", \"type\": \"double\" }, { \"name\": \"v11\", \"type\": \"double\" }, { \"name\": \"v12\", \"type\": \"double\" }, { \"name\": \"v13\", \"type\": \"double\" }, { \"name\": \"v14\", \"type\": \"double\" }, { \"name\": \"v15\", \"type\": \"double\" }, { \"name\": \"v16\", \"type\": \"double\" }, { \"name\": \"v17\", \"type\": \"double\" }, { \"name\": \"v18\", \"type\": \"double\" }, { \"name\": \"v19\", \"type\": \"double\" }, { \"name\": \"v20\", \"type\": \"double\" }, { \"name\": \"v21\", \"type\": \"double\" }, { \"name\": \"v22\", \"type\": \"double\" }, { \"name\": \"v23\", \"type\": \"double\" }, { \"name\": \"v24\", \"type\": \"double\" }, { \"name\": \"v25\", \"type\": \"double\" }, { \"name\": \"v26\", \"type\": \"double\" }, { \"name\": \"v27\", \"type\": \"double\" }, { \"name\": \"v28\", \"type\": \"double\" }, { \"name\": \"amount\", \"type\": \"double\" }, ], \"output\": { \"name\": \"features\", \"type\": \"double\", \"struct\": \"vector\" } } schema_json = json.dumps(schema) print(schema_json) and deployed as: from .model import model from .pipeline import pipelinemodel from .sparkml.model import sparkmlmodel sparkml_data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz') # passing the schema defined above by using an environment variable that -sparkml-serving understands sparkml_model = sparkmlmodel(model_data=sparkml_data, env={'_sparkml_schema' : schema_json}) xgb_model = model(model_data=xgb_model.model_data, image=training_image) model_name = 'inference-pipeline-' + timestamp_prefix sm_model = pipelinemodel(name=model_name, role=role, models=[sparkml_model, xgb_model]) endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name) i got the error as below: clienterror: an error occurred (validationexception) when calling the createmodel operation: 1 validation error detected: value '{_sparkml_schema={\"input\": [{\"type\": \"double\", \"name\": \"v1\"}, {\"type\": \"double\", \"name\": \"v2\"}, {\"type\": \"double\", \"name\": \"v3\"}, {\"type\": \"double\", \"name\": \"v4\"}, {\"type\": \"double\", \"name\": \"v5\"}, {\"type\": \"double\", \"name\": \"v6\"}, {\"type\": \"double\", \"name\": \"v7\"}, {\"type\": \"double\", \"name\": \"v8\"}, {\"type\": \"double\", \"name\": \"v9\"}, {\"type\": \"double\", \"name\": \"v10\"}, {\"type\": \"double\", \"name\": \"v11\"}, {\"type\": \"double\", \"name\": \"v12\"}, {\"type\": \"double\", \"name\": \"v13\"}, {\"type\": \"double\", \"name\": \"v14\"}, {\"type\": \"double\", \"name\": \"v15\"}, {\"type\": \"double\", \"name\": \"v16\"}, {\"type\": \"double\", \"name\": \"v17\"}, {\"type\": \"double\", \"name\": \"v18\"}, {\"type\": \"double\", \"name\": \"v19\"}, {\"type\": \"double\", \"name\": \"v20\"}, {\"type\": \"double\", \"name\": \"v21\"}, {\"type\": \"double\", \"name\": \"v22\"}, {\"type\": \"double\", \"name\": \"v23\"}, {\"type\": \"double\", \"name\": \"v24\"}, {\"type\": \"double\", \"name\": \"v25\"}, {\"type\": \"double\", \"name\": \"v26\"}, {\"type\": \"double\", \"name\": \"v27\"}, {\"type\": \"double\", \"name\": \"v28\"}, {\"type\": \"double\", \"name\": \"amount\"}], \"output\": {\"type\": \"double\", \"name\": \"features\", \"struct\": \"vector\"}}}' at 'containers.1**.member.environment' failed to satisfy constraint: map value must satisfy constraint: [member must have length less than or equal to 1024,** member must have length greater than or equal to 0, member must satisfy regular expression pattern: [\\s\\s]*] i try to reduce my features to 20 and it able to deploy. just wondering how can i pass the schema with 29 attributes?",
        "Question_original_content_gpt_summary":"The user encountered a challenge when attempting to deploy a model onto AWS via SparkML, as the schema with 29 attributes failed to satisfy a constraint.",
        "Question_preprocessed_content":"Title: sparkml schema eroor failed to satisfy constraint; Content: i am deploying a model onto aws via i set up my json schema as follow and deployed as i got the error as below clienterror an error occurred when calling the createmodel operation validation error detected value , output ' at failed to satisfy constraint map value must satisfy constraint i try to reduce my features to and it able to deploy. just wondering how can i pass the schema with attributes?",
        "Answer_original_content":"i do not think the environment length of 1024 limit will be increased in a short time. to work around this, you could try to rebuild the spark ml container with the _sparkml_schema env var: https:\/\/github.com\/aws\/-sparkml-serving-container\/blob\/master\/readme.md#running-the-image-locally",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of deploying a model onto AWS via SparkML with a schema of 29 attributes that failed to satisfy a constraint are to rebuild the SparkML container with the _sparkml_schema environment variable or to wait for an increase in the environment length limit, which may not happen in the short term.",
        "Answer_preprocessed_content":"i do not think the environment length of limit will be increased in a short time. to work around this, you could try to rebuild the spark ml container with the env var"
    },
    {
        "Question_id":null,
        "Question_title":"can a sagemaker endpoint be made public?",
        "Question_body":"is there a way to make a sagemaker endpoint be accessible publicly ?",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1664239929738,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":40.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU96EIw32SSxmx3plPtUUcYA\/can-a-sagemaker-endpoint-be-made-public",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-27T09:46:53.193Z",
                "Answer_score":3,
                "Answer_body":"I believe the way to make a sagemaker inference endpoint public is to use api-gw infront of it. Check out this solution https:\/\/docs.aws.amazon.com\/solutions\/latest\/constructs\/aws-apigateway-sagemakerendpoint.html",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-27T02:32:12.501Z",
                "Answer_score":2,
                "Answer_body":"Hello. In order to make an Amazon SageMaker Real-Time Endpoint public, you can create and manage APIs through an API Gateway. This is an official blog that is showing you a possible solution:\n\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can a endpoint be made public?; Content: is there a way to make a endpoint be accessible publicly ?",
        "Question_original_content_gpt_summary":"The user is wondering if there is a way to make a endpoint publicly accessible.",
        "Question_preprocessed_content":"Title: can a endpoint be made public?; Content: is there a way to make a endpoint be accessible publicly ?",
        "Answer_original_content":"i believe the way to make a inference endpoint public is to use api-gw infront of it. check out this solution https:\/\/docs.aws.amazon.com\/solutions\/latest\/constructs\/aws-apigateway-endpoint.html hello. in order to make an real-time endpoint public, you can create and manage apis through an api gateway. this is an official blog that is showing you a possible solution: https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-\/",
        "Answer_original_content_gpt_summary":"Possible solutions to make an endpoint publicly accessible are to use API Gateway in front of it or to create and manage APIs through an API Gateway. The user can check out the provided links for more information on how to implement these solutions.",
        "Answer_preprocessed_content":"i believe the way to make a inference endpoint public is to use infront of it. check out this solution hello. in order to make an endpoint public, you can create and manage apis through an api gateway. this is an official blog that is showing you a possible solution"
    },
    {
        "Question_id":56422325.0,
        "Question_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size is too high",
        "Question_body":"<p>I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50. <\/p>\n\n<p>I keep on getting this error in Sagemaker.<\/p>\n\n<blockquote>\n  <p>Customer Error: No training data processed. Either the training\n  channel is empty or the mini-batch size is too high. Verify that\n  training data contains non-empty files and the mini-batch size is less\n  than the number of records per training host.<\/p>\n<\/blockquote>\n\n<p>I am using this InputDataConfig<\/p>\n\n<pre><code>InputDataConfig=[\n            {\n                'ChannelName': 'train',\n                'DataSource': {\n                    'S3DataSource': {\n                        'S3DataType': 'S3Prefix',\n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',\n                        'S3DataDistributionType': 'FullyReplicated'\n                    }\n                },\n                'ContentType': 'text\/csv',\n                'CompressionType': 'Gzip'\n            }\n        ],\n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got <\/p>\n\n<blockquote>\n  <p>Customer Error: Unable to initialize the algorithm. Failed to validate\n  input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: {u'training': {u'TrainingInputMode': u'Pipe',\n  u'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\n  u'S3DistributionType': u'FullyReplicated'}} is not valid under any of\n  the given schemas<\/p>\n<\/blockquote>\n\n<p>I went back to train as that seems to be what is needed. But what am I doing wrong with that? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1559544607823,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":599.0,
        "Owner_creation_time":1364896706347,
        "Owner_last_access_time":1664082840048,
        "Owner_reputation":6584.0,
        "Owner_up_votes":477.0,
        "Owner_down_votes":15.0,
        "Owner_views":962.0,
        "Answer_body":"<p>Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559566976623,
        "Answer_score":3.0,
        "Owner_location":"Noida, India",
        "Question_last_edit_time":1559545639143,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56422325",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - either the training channel is empty or the mini-batch size is too high; Content: i am trying to train a linear learner model in . my training set is 422 rows split into 4 files on aws s3. the mini-batch size that i set is 50. i keep on getting this error in . customer error: no training data processed. either the training channel is empty or the mini-batch size is too high. verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host. i am using this inputdataconfig inputdataconfig=[ { 'channelname': 'train', 'datasource': { 's3datasource': { 's3datatype': 's3prefix', 's3uri': 's3:\/\/my_s3_bucket\/rest_of_prefix\/exported\/', 's3datadistributiontype': 'fullyreplicated' } }, 'contenttype': 'text\/csv', 'compressiontype': 'gzip' } ], i am not sure what i am doing wrong here. i tried increasing the number of records to 5547495 split across 6 files. the same error. that makes me think that somehow the config itself has something missing. due to which it seems to think training channel is just not present. i tried changing 'train' to 'training' as that is what the erorr message is saying. but then i got customer error: unable to initialize the algorithm. failed to validate input data configuration. (caused by validationerror) caused by: {u'training': {u'traininginputmode': u'pipe', u'contenttype': u'text\/csv', u'recordwrappertype': u'none', u's3distributiontype': u'fullyreplicated'}} is not valid under any of the given schemas i went back to train as that seems to be what is needed. but what am i doing wrong with that?",
        "Question_original_content_gpt_summary":"The user is encountering challenges while trying to train a linear learner model on AWS S3, as they are receiving errors related to the training channel being empty or the mini-batch size being too high.",
        "Question_preprocessed_content":"Title: either the training channel is empty or the size is too high; Content: i am trying to train a linear learner model in . my training set is rows split into files on aws s . the size that i set is . i keep on getting this error in . customer error no training data processed. either the training channel is empty or the size is too high. verify that training data contains files and the size is less than the number of records per training host. i am using this inputdataconfig i am not sure what i am doing wrong here. i tried increasing the number of records to split across files. the same error. that makes me think that somehow the config itself has something missing. due to which it seems to think training channel is just not present. i tried changing 'train' to 'training' as that is what the erorr message is saying. but then i got customer error unable to initialize the algorithm. failed to validate input data configuration. caused by is not valid under any of the given schemas i went back to train as that seems to be what is needed. but what am i doing wrong with that?",
        "Answer_original_content":"found the problem. the compressiontype was mentioned as 'gzip' but i had changed the actual file to be not compressed when doing the exports. as soon as i changed it to be 'none' the training went smoothly.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of receiving errors related to an empty training channel or a high mini-batch size while training a linear learner model on AWS S3 is to ensure that the compression type mentioned is the same as the actual file being used for exports. Changing the compression type to 'none' can help resolve the issue and allow for smooth training.",
        "Answer_preprocessed_content":"found the problem. the compressiontype was mentioned as 'gzip' but i had changed the actual file to be not compressed when doing the exports. as soon as i changed it to be 'none' the training went smoothly."
    },
    {
        "Question_id":null,
        "Question_title":"Async Inference not able to process later requests",
        "Question_body":"Hi there, hope all of you are fine.\n\nI am trying to deploy a train-on-inference type model. I am done with BYOC, and it is working completely fine with real-time inference endpoints. Also, I am able to make it work with Async inference, and concurrent requests on the same instance are also being handled. But, the later requests, never get processed, without any logical error. Also once the endpoint gets scaled down to 0 instance, it fails to scales up.\n\nThese are some of error and warning messages which I get intermittently:\n\n\n\ndata-log:\n2022-03-23T11:23:17.723:[sagemaker logs] [5ea751c9-9271-4533-bc09-c117791e1372] Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n\n\nwarnings:\n\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/core\/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n\n\nKindly help me with this. Thanks.",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1648126119561,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":172.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqAl1qUyYRK-cbY3DGH-X9g\/async-inference-not-able-to-process-later-requests",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-29T06:30:15.076Z",
                "Answer_score":1,
                "Answer_body":"Hello, I'm running into the exact same issue. I used the same guide and the async endpoint doesn't scale up or down.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-14T14:12:50.944Z",
                "Answer_score":0,
                "Answer_body":"Hi, hope you are fine. Thanks for getting back to me. This is what I am using:\n\n\n# Configure Autoscaling on asynchronous endpoint down to zero instances\nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=4,\n)\n\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\",  # The namespace of the AWS service that provides the resource.\n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 2.0,  # The target value for the metric. - here the metric is - SageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 300,  # The cooldown period helps you prevent your Auto Scaling group from launching or terminating\n        # additional instances before the effects of previous activities are visible.\n        # You can configure the length of time based on your instance startup time or other application needs.\n        # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start.\n        \"ScaleOutCooldown\": 300  # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n        # 'DisableScaleIn': True|False - ndicates whether scale in by the target tracking policy is disabled.\n        # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n    },\n)",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: async inference not able to process later requests; Content: hi there, hope all of you are fine. i am trying to deploy a train-on-inference type model. i am done with byoc, and it is working completely fine with real-time inference endpoints. also, i am able to make it work with async inference, and concurrent requests on the same instance are also being handled. but, the later requests, never get processed, without any logical error. also once the endpoint gets scaled down to 0 instance, it fails to scales up. these are some of error and warning messages which i get intermittently: data-log: 2022-03-23t11:23:17.723:[ logs] [5ea751c9-9271-4533-bc09-c117791e1372] received server error (500) from primary with message \" warnings: \/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/core\/getlimits.py:499: userwarning: the value of the smallest subnormal for type is zero. setattr(self, word, getattr(machar, word).flat[0]) kindly help me with this. thanks.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with async inference not being able to process later requests, as well as scaling up issues when the endpoint is scaled down to 0 instance, with intermittent errors and warnings.",
        "Question_preprocessed_content":"Title: async inference not able to process later requests; Content: hi there, hope all of you are fine. i am trying to deploy a type model. i am done with byoc, and it is working completely fine with inference endpoints. also, i am able to make it work with async inference, and concurrent requests on the same instance are also being handled. but, the later requests, never get processed, without any logical error. also once the endpoint gets scaled down to instance, it fails to scales up. these are some of error and warning messages which i get intermittently logs received server error from primary with message warnings userwarning the value of the smallest subnormal for type is zero. setattr .flat kindly help me with this. thanks.",
        "Answer_original_content":"hello, i'm running into the exact same issue. i used the same guide and the async endpoint doesn't scale up or down. hi, hope you are fine. thanks for getting back to me. this is what i am using: # configure autoscaling on asynchronous endpoint down to zero instances response = client.register_scalable_target( servicenamespace=\"\", resourceid=resource_id, scalabledimension=\":variant:desiredinstancecount\", mincapacity=0, maxcapacity=4, ) response = client.put_scaling_policy( policyname=\"invocations-scalingpolicy\", servicenamespace=\"\", # the namespace of the aws service that provides the resource. resourceid=resource_id, # endpoint name scalabledimension=\":variant:desiredinstancecount\", # supports only instance count policytype=\"targettrackingscaling\", # 'stepscaling'|'targettrackingscaling' targettrackingscalingpolicyconfiguration={ \"targetvalue\": 2.0, # the target value for the metric. - here the metric is - variantinvocationsperinstance \"customizedmetricspecification\": { \"metricname\": \"approximatebacklogsizeperinstance\", \"namespace\": \"aws\/\", \"dimensions\": [{\"name\": \"endpointname\", \"value\": endpoint_name}], \"statistic\": \"average\", }, \"scaleincooldown\": 300, # the cooldown period helps you prevent your auto scaling group from launching or terminating # additional instances before the effects of previous activities are visible. # you can configure the length of time based on your instance startup time or other application needs. # scaleincooldown - the amount of time, in seconds, after a scale in activity completes before another scale in activity can start. \"scaleoutcooldown\": 300 # scaleoutcooldown - the amount of time, in seconds, after a scale out activity completes before another scale out activity can start. # 'disablescalein': true|false - ndicates whether scale in by the target tracking policy is disabled. # if the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. }, )",
        "Answer_original_content_gpt_summary":"The answer provides a possible solution to the challenges faced by the user with async inference not being able to process later requests and scaling up issues when the endpoint is scaled down to 0 instance. The solution involves configuring autoscaling on asynchronous endpoint down to zero instances using AWS services. The solution includes setting up a scaling policy, defining the target value for the metric, and configuring the cooldown period to prevent launching or terminating additional instances before the effects of previous activities are visible.",
        "Answer_preprocessed_content":"hello, i'm running into the exact same issue. i used the same guide and the async endpoint doesn't scale up or down. hi, hope you are fine. thanks for getting back to me. this is what i am using configure autoscaling on asynchronous endpoint down to zero instances response servicenamespace , scalabledimension variant desiredinstancecount , mincapacity , maxcapacity , response servicenamespace , the namespace of the aws service that provides the resource. endpoint name scalabledimension variant desiredinstancecount , supports only instance count policytype targettrackingscaling , 'stepscaling' 'targettrackingscaling' targettrackingscalingpolicyconfiguration targetvalue the target value for the metric. here the metric is variantinvocationsperinstance customizedmetricspecification , statistic average , , scaleincooldown , the cooldown period helps you prevent your auto scaling group from launching or terminating additional instances before the effects of previous activities are visible. you can configure the length of time based on your instance startup time or other application needs. scaleincooldown the amount of time, in seconds, after a scale in activity completes before another scale in activity can start. scaleoutcooldown scaleoutcooldown the amount of time, in seconds, after a scale out activity completes before another scale out activity can start. 'disablescalein' true false ndicates whether scale in by the target tracking policy is disabled. if the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource. ,"
    },
    {
        "Question_id":71169178.0,
        "Question_title":"azureml.contrib.dataset vs azureml.data",
        "Question_body":"<p>Looks like AzureML Python SDK has two Dataset packages exposed over API:<\/p>\n<ol>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.contrib.dataset<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-dataset\/azureml.contrib.dataset.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.data<\/a><\/li>\n<\/ol>\n<p>The documentation doesn't clearly mention the difference or when should we use which one? But, it creates confusion for sure. For example, There are two Tabular Dataset classes exposed over API. And they have different APIs for different functions:<\/p>\n<ol>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.data.TabularDataset<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-contrib-dataset\/azureml.contrib.dataset.tabulardataset?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.contrib.dataset.TabularDataset<\/a><\/li>\n<\/ol>\n<p>Any suggestion about when should I use which package will be helpful.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1645165311677,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":24.0,
        "Owner_creation_time":1280505139752,
        "Owner_last_access_time":1663935737867,
        "Owner_reputation":4265.0,
        "Owner_up_votes":315.0,
        "Owner_down_votes":11.0,
        "Owner_views":403.0,
        "Answer_body":"<p>As per the <a href=\"https:\/\/pypi.org\/project\/azureml-contrib-dataset\/\" rel=\"nofollow noreferrer\">PyPi<\/a>, <code>azureml.contrib.dataset<\/code> has been deprecated and <code>azureml.data<\/code> should be used instead:<\/p>\n<blockquote>\n<p>The azureml-contrib-dataset package has been deprecated and might not\nreceive future updates and removed from the distribution altogether.\nPlease use azureml-core instead.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645168074896,
        "Answer_score":0.0,
        "Owner_location":"Bangalore, India",
        "Question_last_edit_time":1645167982083,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71169178",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: .contrib.dataset vs .data; Content: looks like python sdk has two dataset packages exposed over api: .contrib.dataset .data the documentation doesn't clearly mention the difference or when should we use which one? but, it creates confusion for sure. for example, there are two tabular dataset classes exposed over api. and they have different apis for different functions: .data.tabulardataset .contrib.dataset.tabulardataset any suggestion about when should i use which package will be helpful.",
        "Question_original_content_gpt_summary":"The user is facing confusion regarding the two dataset packages exposed over the Python SDK API, .contrib.dataset and .data, and is seeking guidance on when to use each one.",
        "Question_preprocessed_content":"Title: vs ; Content: looks like python sdk has two dataset packages exposed over api the documentation doesn't clearly mention the difference or when should we use which one? but, it creates confusion for sure. for example, there are two tabular dataset classes exposed over api. and they have different apis for different functions any suggestion about when should i use which package will be helpful.",
        "Answer_original_content":"as per the pypi, .contrib.dataset has been deprecated and .data should be used instead: the -contrib-dataset package has been deprecated and might not receive future updates and removed from the distribution altogether. please use -core instead.",
        "Answer_original_content_gpt_summary":"The solution to the user's confusion regarding the two dataset packages exposed over the Python SDK API is to use .data instead of .contrib.dataset as the latter has been deprecated and might not receive future updates. The user should use -core instead.",
        "Answer_preprocessed_content":"as per the pypi, has been deprecated and should be used instead the package has been deprecated and might not receive future updates and removed from the distribution altogether. please use instead."
    },
    {
        "Question_id":73471486.0,
        "Question_title":"How to prevent storing data in Jupyter project tree when writing data from Sagemaker to S3",
        "Question_body":"<p>I am new to AWS Sagemaker and I wrote data to my S3 bucket.\nBut these datasets also appear in the working tree of my jupyter instance.<\/p>\n<p>How can I move data directly to S3 without saving it &quot;locally&quot;?<\/p>\n<p>My code:<\/p>\n<pre><code>import os\nimport pandas as pd\n\nimport sagemaker, boto3\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\n\n# please provide your own bucket and folder path of your bucket here\nbucket = &quot;test-bucket2342343&quot;\nsm_sess = sagemaker.Session(default_bucket=bucket)\nfile_path = &quot;Use Cases\/Sagemaker Demo\/xgboost&quot;\n\n# data \ndf_train = pd.DataFrame({'X':[0,100,200,400,450,  550,600,800,1600],\n                         'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\ndf_test = pd.DataFrame({'X':[10,90,240,459,120,  650,700,1800,1300],\n                        'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\n# move to S3 \ndf_train[['y','X']].to_csv('train.csv', header=False, index=False)\n\ndf_val = df_test.copy()\ndf_val[['y','X']].to_csv('val.csv', header=False, index=False)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;train.csv&quot;)).upload_file(&quot;train.csv&quot;)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;val.csv&quot;)).upload_file(&quot;val.csv&quot;)\n\n<\/code><\/pre>\n<p>It successfully appears in my S3 bucket.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But it also appears here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1661336683943,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":17.0,
        "Owner_creation_time":1565941261083,
        "Owner_last_access_time":1664037909263,
        "Owner_reputation":188.0,
        "Owner_up_votes":134.0,
        "Owner_down_votes":4.0,
        "Owner_views":43.0,
        "Answer_body":"<p>with Pandas you can save to S3 directly (<a href=\"https:\/\/stackoverflow.com\/a\/56275519\/121956\">relevant answer<\/a>). For example:<\/p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c'])\ndf.to_csv('s3:\/\/test-bucket2342343\/\/tmp.csv', index=False)\n<\/code><\/pre>\n<p>Or, use what you currently do and delete the local files:<\/p>\n<pre><code>import os\nos.remove('train.csv')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1661447960212,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1661337201248,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73471486",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to prevent storing data in jupyter project tree when writing data from to s3; Content: i am new to and i wrote data to my s3 bucket. but these datasets also appear in the working tree of my jupyter instance. how can i move data directly to s3 without saving it \"locally\"? my code: import os import pandas as pd import , boto3 from import get_execution_role from .inputs import traininginput from .serializers import csvserializer # please provide your own bucket and folder path of your bucket here bucket = \"test-bucket2342343\" sm_sess = .session(default_bucket=bucket) file_path = \"use cases\/ demo\/xgboost\" # data df_train = pd.dataframe({'x':[0,100,200,400,450, 550,600,800,1600], 'y':[0,0, 0, 0, 0, 1, 1, 1, 1]}) df_test = pd.dataframe({'x':[10,90,240,459,120, 650,700,1800,1300], 'y':[0,0, 0, 0, 0, 1, 1, 1, 1]}) # move to s3 df_train[['y','x']].to_csv('train.csv', header=false, index=false) df_val = df_test.copy() df_val[['y','x']].to_csv('val.csv', header=false, index=false) boto3.session().resource(\"s3\").bucket(bucket) \\ .object(os.path.join(file_path, \"train.csv\")).upload_file(\"train.csv\") boto3.session().resource(\"s3\").bucket(bucket) \\ .object(os.path.join(file_path, \"val.csv\")).upload_file(\"val.csv\") it successfully appears in my s3 bucket. but it also appears here:",
        "Question_original_content_gpt_summary":"The user is encountering a challenge of preventing data from being stored in the working tree of their Jupyter instance when writing data to an S3 bucket.",
        "Question_preprocessed_content":"Title: how to prevent storing data in jupyter project tree when writing data from to s ; Content: i am new to and i wrote data to my s bucket. but these datasets also appear in the working tree of my jupyter instance. how can i move data directly to s without saving it locally ? my code it successfully appears in my s bucket. but it also appears here",
        "Answer_original_content":"with pandas you can save to s3 directly (relevant answer). for example: import pandas as pd df = pd.dataframe( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c']) df.to_csv('s3:\/\/test-bucket2342343\/\/tmp.csv', index=false) or, use what you currently do and delete the local files: import os os.remove('train.csv')",
        "Answer_original_content_gpt_summary":"The answer suggests two possible solutions to prevent data from being stored in the working tree of a Jupyter instance when writing data to an S3 bucket. The first solution is to use pandas to save data directly to S3. The second solution is to delete the local files after writing them to S3 using the current method.",
        "Answer_preprocessed_content":"with pandas you can save to s directly . for example or, use what you currently do and delete the local files"
    },
    {
        "Question_id":56341012.0,
        "Question_title":"Docker image not found during local deployment (\"no such image\")",
        "Question_body":"<p>I want to test my service and to do so I deploy it locally and until now everything worked fine. However, for some unrelated reason I was forced to delete all my docker images and since then I'm unable to deploy the service locally. Upon deployment I receive the following error:<\/p>\n\n<blockquote>\n  <p>404 Client Error: Not Found for url:\n  http+docker:\/\/localnpipe\/v1.39\/images\/471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\/json<\/p>\n<\/blockquote>\n\n<p>And also:<\/p>\n\n<blockquote>\n  <p>ImageNotFound: 404 Client Error: Not Found (\"no such image: \n  471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814: No\n  such image:\n  sha256:471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\")<\/p>\n<\/blockquote>\n\n<p>What I did to deploy the model:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.webservice import LocalWebservice\nfrom azureml.core.model import InferenceConfig\n\nws = Workspace.from_config(\"config.json\")\n\ndeployment_config = LocalWebservice.deploy_configuration(port=8890)\n\ninference_config = InferenceConfig(runtime= \"python\", \n                               entry_script=\"score.py\",\n                               conda_file=\"env.yml\")\n\nmodel_box = Model(ws, \"box\")\nmodel_view = Model(ws, \"view_crop\")\nmodel_damage = Model(ws, \"damage_crop\")\n\nservice = Model.deploy(ws, \"test-service\", [model_box, model_view, model_damage], inference_config, deployment_config)\n\nservice.wait_for_deployment(True)\n<\/code><\/pre>\n\n<p>I understand why there is no image present, but I would expect that it is downloaded in that case.<\/p>\n\n<p>Is there a way to force the build process to re-download the docker base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1559042269360,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":3685.0,
        "Owner_creation_time":1408356046196,
        "Owner_last_access_time":1663936006376,
        "Owner_reputation":594.0,
        "Owner_up_votes":403.0,
        "Owner_down_votes":5.0,
        "Owner_views":55.0,
        "Answer_body":"<p>I just found the problem and corresponding solution:<\/p>\n\n<p>I deleted all images but there where still some containers based on deleted images present. Deleting the corresponding container had the desired effect that the docker image is reloaded from the server.<\/p>\n\n<p>You can delete all containers with <code>docker kill $(docker ps -q)<\/code>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1559043123528,
        "Answer_score":2.0,
        "Owner_location":"Bonn, Deutschland",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56341012",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: docker image not found during local deployment (\"no such image\"); Content: i want to test my service and to do so i deploy it locally and until now everything worked fine. however, for some unrelated reason i was forced to delete all my docker images and since then i'm unable to deploy the service locally. upon deployment i receive the following error: 404 client error: not found for url: http+docker:\/\/localnpipe\/v1.39\/images\/471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\/json and also: imagenotfound: 404 client error: not found (\"no such image: 471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814: no such image: sha256:471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\") what i did to deploy the model: from .core.model import model from .core import workspace from .core.webservice import localwebservice from .core.model import inferenceconfig ws = workspace.from_config(\"config.json\") deployment_config = localwebservice.deploy_configuration(port=8890) inference_config = inferenceconfig(runtime= \"python\", entry_script=\"score.py\", conda_file=\"env.yml\") model_box = model(ws, \"box\") model_view = model(ws, \"view_crop\") model_damage = model(ws, \"damage_crop\") service = model.deploy(ws, \"test-service\", [model_box, model_view, model_damage], inference_config, deployment_config) service.wait_for_deployment(true) i understand why there is no image present, but i would expect that it is downloaded in that case. is there a way to force the build process to re-download the docker base image? thanks in advance.",
        "Question_original_content_gpt_summary":"The user is encountering an issue with their local deployment of a service, where they are receiving an error message indicating that a docker image is not found (\"no such image\"), and are looking for a way to force the build process to re-download the docker base image.",
        "Question_preprocessed_content":"Title: docker image not found during local deployment ; Content: i want to test my service and to do so i deploy it locally and until now everything worked fine. however, for some unrelated reason i was forced to delete all my docker images and since then i'm unable to deploy the service locally. upon deployment i receive the following error client error not found for url and also imagenotfound client error not found what i did to deploy the model i understand why there is no image present, but i would expect that it is downloaded in that case. is there a way to force the build process to the docker base image? thanks in advance.",
        "Answer_original_content":"i just found the problem and corresponding solution: i deleted all images but there where still some containers based on deleted images present. deleting the corresponding container had the desired effect that the docker image is reloaded from the server. you can delete all containers with docker kill $(docker ps -q).",
        "Answer_original_content_gpt_summary":"The solution to the issue of receiving a \"no such image\" error message during local deployment of a service is to delete all containers based on deleted images and then reload the docker image from the server. This can be done by running the command \"docker kill $(docker ps -q)\" to delete all containers.",
        "Answer_preprocessed_content":"i just found the problem and corresponding solution i deleted all images but there where still some containers based on deleted images present. deleting the corresponding container had the desired effect that the docker image is reloaded from the server. you can delete all containers with ."
    },
    {
        "Question_id":70670669.0,
        "Question_title":"How do parallel trials in GCP Vertex AI work?",
        "Question_body":"<p>When you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. After that, you also select the type and count of the workers. What I don't understand is when I make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the CPU. However, if one task occupies all of the CPU's resources, how can 2 of them run in parallel? Does GCP provision more than 1 machine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1641920680143,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":168.0,
        "Owner_creation_time":1579801831103,
        "Owner_last_access_time":1663207732260,
        "Owner_reputation":71.0,
        "Owner_up_votes":5.0,
        "Owner_down_votes":0.0,
        "Owner_views":30.0,
        "Answer_body":"<p><strong>Parallel trials<\/strong> allows you to run the trials concurrently depending on your input on the maximum number of trials.<\/p>\n<p>You are correct with your statement &quot;<em>one worker, each task is said to occupy 100% of the CPU<\/em>&quot; and for GCP to run other tasks in parallel,<\/p>\n<blockquote>\n<p>the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). The work pool spec that you set for your job is used for each individual training cluster.<\/p>\n<\/blockquote>\n<p>Please see <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning#parallel-trials\" rel=\"nofollow noreferrer\">Parallel Trials Documentation<\/a> for more details.<\/p>\n<p>And for more details about Hyperparameter Tuning, you may refer to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning\" rel=\"nofollow noreferrer\">Hyperparameter Tuning Documentation<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641974198663,
        "Answer_score":1.0,
        "Owner_location":"Tempe, AZ, USA",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70670669",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do parallel trials in gcp work?; Content: when you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. after that, you also select the type and count of the workers. what i don't understand is when i make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the cpu. however, if one task occupies all of the cpu's resources, how can 2 of them run in parallel? does gcp provision more than 1 machine?",
        "Question_original_content_gpt_summary":"The user is questioning how GCP can run multiple trials in parallel with only one worker, as each task is said to occupy 100% of the CPU's resources.",
        "Question_preprocessed_content":"Title: how do parallel trials in gcp work?; Content: when you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. after that, you also select the type and count of the workers. what i don't understand is when i make two or more trials run in parallel, yet only one worker, each task is said to occupy % of the cpu. however, if one task occupies all of the cpu's resources, how can of them run in parallel? does gcp provision more than machine?",
        "Answer_original_content":"parallel trials allows you to run the trials concurrently depending on your input on the maximum number of trials. you are correct with your statement \"one worker, each task is said to occupy 100% of the cpu\" and for gcp to run other tasks in parallel, the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). the work pool spec that you set for your job is used for each individual training cluster. please see parallel trials documentation for more details. and for more details about hyperparameter tuning, you may refer to hyperparameter tuning documentation.",
        "Answer_original_content_gpt_summary":"The answer suggests that GCP can run multiple trials in parallel by provisioning multiple training processing clusters or multiple individual machines in the case of a single-process trainer. The user can set the work pool spec for their job, and the parallel trials allow running the trials concurrently depending on the maximum number of trials. The answer also provides links to the parallel trials and hyperparameter tuning documentation for more details.",
        "Answer_preprocessed_content":"parallel trials allows you to run the trials concurrently depending on your input on the maximum number of trials. you are correct with your statement one worker, each task is said to occupy % of the cpu and for gcp to run other tasks in parallel, the hyperparameter tuning service provisions multiple training processing clusters . the work pool spec that you set for your job is used for each individual training cluster. please see parallel trials documentation for more details. and for more details about hyperparameter tuning, you may refer to hyperparameter tuning documentation."
    },
    {
        "Question_id":null,
        "Question_title":"Vocal emojis in Speech-to-Text",
        "Question_body":"Hello! I am majoring in Theoretical Linguistics this year and I would like to write my dissertation on Google Cloud API and the vocal emojis supported, delving into the neural network to find out how they are translated. I have seen that my native language is missing and could build a dataset of spoken forms. Following the tutorial for using the Speech-to-Text API with Phyton I found out that very little information on this project are public.Should I contact some specific person\/service via my institutional account to receive material for a study case?Thank you!",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1667461260000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":25.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vocal-emojis-in-Speech-to-Text\/td-p\/485418\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-03T07:41:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello! I am majoring in Theoretical Linguistics this year and I would like to write my dissertation on Google Cloud API and the vocal emojis supported, delving into the neural network to find out how they are translated. I have seen that my native language is missing and could build a dataset of spoken forms. Following the tutorial for using the Speech-to-Text API with Phyton I found out that very little information on this project are public.\n\nShould I contact some specific person\/service via my institutional account to receive material for a study case?\n\nThank you!"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: vocal emojis in speech-to-text; Content: hello! i am majoring in theoretical linguistics this year and i would like to write my dissertation on google cloud api and the vocal emojis supported, delving into the neural network to find out how they are translated. i have seen that my native language is missing and could build a dataset of spoken forms. following the tutorial for using the speech-to-text api with phyton i found out that very little information on this project are public.should i contact some specific person\/service via my institutional account to receive material for a study case?thank you!",
        "Question_original_content_gpt_summary":"The user is looking to write their dissertation on Google Cloud API and vocal emojis, but is having difficulty finding public information on the project and is unsure of who to contact for more information.",
        "Question_preprocessed_content":"Title: vocal emojis in ; Content: hello! i am majoring in theoretical linguistics this year and i would like to write my dissertation on google cloud api and the vocal emojis supported, delving into the neural network to find out how they are translated. i have seen that my native language is missing and could build a dataset of spoken forms. following the tutorial for using the api with phyton i found out that very little information on this project are i contact some specific via my institutional account to receive material for a study case?thank you!",
        "Answer_original_content":"hello! i am majoring in theoretical linguistics this year and i would like to write my dissertation on google cloud api and the vocal emojis supported, delving into the neural network to find out how they are translated. i have seen that my native language is missing and could build a dataset of spoken forms. following the tutorial for using the speech-to-text api with phyton i found out that very little information on this project are public. should i contact some specific person\/service via my institutional account to receive material for a study case? thank you!",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include building a dataset of spoken forms for the missing native language, delving into the neural network to find out how vocal emojis are translated, and contacting a specific person or service via the institutional account to receive material for a study case.",
        "Answer_preprocessed_content":"hello! i am majoring in theoretical linguistics this year and i would like to write my dissertation on google cloud api and the vocal emojis supported, delving into the neural network to find out how they are translated. i have seen that my native language is missing and could build a dataset of spoken forms. following the tutorial for using the api with phyton i found out that very little information on this project are public. should i contact some specific via my institutional account to receive material for a study case? thank you!"
    },
    {
        "Question_id":40714064.0,
        "Question_title":"How \"Azure ML export data to SQL database by insert in a row of database.\"",
        "Question_body":"<p>I can only export data from AzureML by write instead to database that created previously. I need to know How to insert and fetch the data continuously the database because I need to use old data as well as the new data that get as the AzureML output to plot graph.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1479709864747,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":193.0,
        "Owner_creation_time":1455005478600,
        "Owner_last_access_time":1649248026316,
        "Owner_reputation":3.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":"<ol>\n<li>Create a web service from the AzureML experiment. <\/li>\n<li>Access the web service using a program you written from C# or any language.<\/li>\n<li>You can get the output of the web service as a JSON.<\/li>\n<li>Use typical SQL ADD\/UPDATE queries to update the table<\/li>\n<li>When giving an input for the web service, fetch the data from the DB and pass as the JSON for it. <\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1480308703072,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1480314865163,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40714064",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how \" export data to sql database by insert in a row of database.\"; Content: i can only export data from by write instead to database that created previously. i need to know how to insert and fetch the data continuously the database because i need to use old data as well as the new data that get as the output to plot graph.",
        "Question_original_content_gpt_summary":"The user is trying to export data to an SQL database by inserting it into a row of the database, but is having difficulty understanding how to insert and fetch the data continuously from the database in order to use both old and new data to plot a graph.",
        "Question_preprocessed_content":"Title: how export data to sql database by insert in a row of ; Content: i can only export data from by write instead to database that created previously. i need to know how to insert and fetch the data continuously the database because i need to use old data as well as the new data that get as the output to plot graph.",
        "Answer_original_content":"create a web service from the experiment. access the web service using a program you written from c# or any language. you can get the output of the web service as a json. use typical sql add\/update queries to update the table when giving an input for the web service, fetch the data from the db and pass as the json for it.",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's difficulty in inserting and fetching data continuously from an SQL database include creating a web service from the experiment, accessing the web service using a program written in C# or any language, getting the output of the web service as a JSON, using typical SQL add\/update queries to update the table when giving an input for the web service, and fetching the data from the database and passing it as the JSON for it.",
        "Answer_preprocessed_content":"create a web service from the experiment. access the web service using a program you written from c or any language. you can get the output of the web service as a json. use typical sql queries to update the table when giving an input for the web service, fetch the data from the db and pass as the json for it."
    },
    {
        "Question_id":null,
        "Question_title":"How to train TensorFlow pretrained Object Detection model in AzureML?",
        "Question_body":"I have a problem to do custom TensorFlow2 Object Detection in AzureML (see https:\/\/www.tensorflow.org\/hub\/tutorials\/tf2_object_detection). I failed to install the API with error message as follow:\nobject_detection\/protos\/calibration.proto:41:3: Expected \"required\", \"optional\", or \"repeated\".\nobject_detection\/protos\/calibration.proto:41:6: Expected field name.\nobject_detection\/protos\/calibration.proto:53:3: Expected \"required\", \"optional\", or \"repeated\".\nobject_detection\/protos\/calibration.proto:53:6: Expected field name.\nERROR: Could not install packages due to an EnvironmentError: [('\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/gpu-notebook\/code\/Users\/mengoon.lee\/models\/research\/a3c_blogpost\/a3c_cartpole.py',\nPlease help me. Thanks in advance.",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1612228828727,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/254691\/how-to-train-tensorflow-pretrained-object-detectio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-24T13:59:00.253Z",
                "Answer_score":0,
                "Answer_body":"@MengOonLee-5731 Thanks, Please follow the doc for protoc installation. We have tried from the AML Jupyter notebook compute instance (ex: Virtual machine size STANDARD_DS3_V2 CPU) we are able to follow the steps without the error that you are facing.\n\nThis is an underlying issue with shutil.copytree when doing dev install from mounted share to local conda env. https:\/\/bugs.python.org\/issue24564\n\nMitigation is to use the disk rather than fileshare for dev installs.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to train tensorflow pretrained object detection model in ?; Content: i have a problem to do custom tensorflow2 object detection in (see https:\/\/www.tensorflow.org\/hub\/tutorials\/tf2_object_detection). i failed to install the api with error message as follow: object_detection\/protos\/calibration.proto:41:3: expected \"required\", \"optional\", or \"repeated\". object_detection\/protos\/calibration.proto:41:6: expected field name. object_detection\/protos\/calibration.proto:53:3: expected \"required\", \"optional\", or \"repeated\". object_detection\/protos\/calibration.proto:53:6: expected field name. error: could not install packages due to an environmenterror: [('\/mnt\/batch\/tasks\/shared\/ls_root\/mounts\/clusters\/gpu-notebook\/code\/users\/mengoon.lee\/models\/research\/a3c_blogpost\/a3c_cartpole.py', please help me. thanks in advance.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with installing the TensorFlow2 object detection API, receiving error messages related to the calibration.proto file.",
        "Question_preprocessed_content":"Title: how to train tensorflow pretrained object detection model in ?; Content: i have a problem to do custom tensorflow object detection in . i failed to install the api with error message as follow expected required , optional , or repeated . expected field name. expected required , optional , or repeated . expected field name. error could not install packages due to an environmenterror please help me. thanks in advance.",
        "Answer_original_content":"@mengoonlee-5731 thanks, please follow the doc for protoc installation. we have tried from the aml jupyter notebook compute instance (ex: virtual machine size standard_ds3_v2 cpu) we are able to follow the steps without the error that you are facing. this is an underlying issue with shutil.copytree when doing dev install from mounted share to local conda env. https:\/\/bugs.python.org\/issue24564 mitigation is to use the disk rather than fileshare for dev installs.",
        "Answer_original_content_gpt_summary":"The answer suggests following the documentation for protoc installation and using a virtual machine to avoid the error related to shutil.copytree when doing dev install from mounted share to local conda env. The mitigation is to use the disk rather than fileshare for dev installs.",
        "Answer_preprocessed_content":"thanks, please follow the doc for protoc installation. we have tried from the aml jupyter notebook compute instance we are able to follow the steps without the error that you are facing. this is an underlying issue with when doing dev install from mounted share to local conda env. mitigation is to use the disk rather than fileshare for dev installs."
    },
    {
        "Question_id":72712449.0,
        "Question_title":"SageMaker Pipeline - Processing step for ImageClassification model",
        "Question_body":"<p>I'm trying to solve ImageClassification task. I have prepared a code to train, evaluate and deploy tensorflow model in SageMaker Notebook. I'm new with SageMaker and SageMaker Pipeline too. Currently, I'm trying to split my code and create SageMaker pipeline to solve Image Classification task.\nIn reference to AWS documentation there is <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-processing\" rel=\"nofollow noreferrer\">Processing steps<\/a>. I have a code which read data from S3 and use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\" rel=\"nofollow noreferrer\">ImageGenerator<\/a> to generate augmented images on the fly while tensorflow model is still in the training stage.<\/p>\n<p>I don't find anything of how I can use <code>ImageGenerator<\/code> inside of Processing step in SageMaker Pipeline.<\/p>\n<p>My Code of <code>ImageGenerator<\/code>:<\/p>\n<pre><code>def load_data(mode):\n    if mode == 'TRAIN':\n        datagen = ImageDataGenerator(\n            rescale=1. \/ 255,\n            rotation_range = 0.5,\n            shear_range=0.2,\n            zoom_range=0.2,\n            width_shift_range = 0.2,\n            height_shift_range = 0.2,\n            fill_mode = 'nearest',\n            horizontal_flip=True)\n    else:\n        datagen = ImageDataGenerator(rescale=1. \/ 255)\n    return datagen\n\n\ndef get_flow_from_directory(datagen,\n                            data_dir,\n                            batch_size,\n                            shuffle=True):\n    assert os.path.exists(data_dir), (&quot;Unable to find images resources for input&quot;)\n    generator = datagen.flow_from_directory(data_dir,\n                                            class_mode = &quot;categorical&quot;,\n                                            target_size=(HEIGHT, WIDTH),\n                                            batch_size=batch_size,\n                                            shuffle=shuffle\n                                            )\n    print('Labels are: ', generator.class_indices)\n    return generator\n<\/code><\/pre>\n<p>Question is - does it possible to use <code>ImageGenerator<\/code> inside of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-and-manage-steps.html#step-type-processing\" rel=\"nofollow noreferrer\">Processing step<\/a> of SageMaker Pipeline?\nI'd appreciate for any ideas, Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1655888089677,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Owner_creation_time":1470228490790,
        "Owner_last_access_time":1663345526560,
        "Owner_reputation":504.0,
        "Owner_up_votes":758.0,
        "Owner_down_votes":3.0,
        "Owner_views":82.0,
        "Answer_body":"<p>So, <code>ImageGenerator<\/code> and <code>flow_from_directory<\/code> I continue use inside of Training step. Processing step I skip at all, just use Training, Evaluating and Register model.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656099367803,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72712449",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pipeline - processing step for imageclassification model; Content: i'm trying to solve imageclassification task. i have prepared a code to train, evaluate and deploy tensorflow model in notebook. i'm new with and pipeline too. currently, i'm trying to split my code and create pipeline to solve image classification task. in reference to aws documentation there is processing steps. i have a code which read data from s3 and use imagegenerator to generate augmented images on the fly while tensorflow model is still in the training stage. i don't find anything of how i can use imagegenerator inside of processing step in pipeline. my code of imagegenerator: def load_data(mode): if mode == 'train': datagen = imagedatagenerator( rescale=1. \/ 255, rotation_range = 0.5, shear_range=0.2, zoom_range=0.2, width_shift_range = 0.2, height_shift_range = 0.2, fill_mode = 'nearest', horizontal_flip=true) else: datagen = imagedatagenerator(rescale=1. \/ 255) return datagen def get_flow_from_directory(datagen, data_dir, batch_size, shuffle=true): assert os.path.exists(data_dir), (\"unable to find images resources for input\") generator = datagen.flow_from_directory(data_dir, class_mode = \"categorical\", target_size=(height, width), batch_size=batch_size, shuffle=shuffle ) print('labels are: ', generator.class_indices) return generator question is - does it possible to use imagegenerator inside of processing step of pipeline? i'd appreciate for any ideas, thanks.",
        "Question_original_content_gpt_summary":"The user is trying to solve an image classification task using a TensorFlow model and AWS Pipeline, and is struggling to figure out how to use the ImageDataGenerator within the Processing Step of the Pipeline.",
        "Question_preprocessed_content":"Title: pipeline processing step for imageclassification model; Content: i'm trying to solve imageclassification task. i have prepared a code to train, evaluate and deploy tensorflow model in notebook. i'm new with and pipeline too. currently, i'm trying to split my code and create pipeline to solve image classification task. in reference to aws documentation there is processing steps. i have a code which read data from s and use imagegenerator to generate augmented images on the fly while tensorflow model is still in the training stage. i don't find anything of how i can use inside of processing step in pipeline. my code of question is does it possible to use inside of processing step of pipeline? i'd appreciate for any ideas, thanks.",
        "Answer_original_content":"so, imagegenerator and flow_from_directory i continue use inside of training step. processing step i skip at all, just use training, evaluating and register model.",
        "Answer_original_content_gpt_summary":"The answer suggests skipping the Processing Step of the AWS Pipeline and only using the training, evaluating, and registering model steps. The ImageDataGenerator and flow_from_directory should be used within the training step.",
        "Answer_preprocessed_content":"so, and i continue use inside of training step. processing step i skip at all, just use training, evaluating and register model."
    },
    {
        "Question_id":null,
        "Question_title":"Google translator is free or has any kind of pricing?",
        "Question_body":"I'm using this code for translating my website in my angular project. I'm not using translate API provided by google cloud. So, I just need to confirm that the source I'm using is paid for publicly available (free)?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1662421920000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":64.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-translator-is-free-or-has-any-kind-of-pricing\/td-p\/463225\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-15T11:47:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"TRANSLATION is a basic free Google service for users to translate their website content on the web browser side. There are some disadvantages using this such as the Search Engine Optimization and there are limits while using it."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: google translator is free or has any kind of pricing?; Content: i'm using this code for translating my website in my angular project. i'm not using translate api provided by google cloud. so, i just need to confirm that the source i'm using is paid for publicly available (free)?",
        "Question_original_content_gpt_summary":"The user is questioning whether the Google Translator code they are using for their Angular project is free or has any kind of pricing.",
        "Question_preprocessed_content":"Title: google translator is free or has any kind of pricing?; Content: i'm using this code for translating my website in my angular project. i'm not using translate api provided by google cloud. so, i just need to confirm that the source i'm using is paid for publicly available ?",
        "Answer_original_content":"translation is a basic free google service for users to translate their website content on the web browser side. there are some disadvantages using this such as the search engine optimization and there are limits while using it.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n- The Google Translator code used for the Angular project is a basic free service.\n- Users can translate their website content on the web browser side.\n- There are some disadvantages to using this service, such as search engine optimization issues and usage limits. \n\nIn summary, the Google Translator code used for the Angular project is a free service, but there are some limitations and drawbacks to consider.",
        "Answer_preprocessed_content":"translation is a basic free google service for users to translate their website content on the web browser side. there are some disadvantages using this such as the search engine optimization and there are limits while using it."
    },
    {
        "Question_id":null,
        "Question_title":"Partial download data",
        "Question_body":"<p>I pushed N images and meta info to remote repository, then remove all data locally.<\/p>\n<p>After some time my colleague want download this dataset, but not fully - he filter meta info and he knows paths to necessary images.<\/p>\n<p>Is there a way download part of data from remote repository?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1582013387314,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":598.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/partial-download-data\/322",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-02-20T22:57:03.138Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/miraina\">@miraina<\/a>!<\/p>\n<blockquote>\n<p>Is there a way download part of data<\/p>\n<\/blockquote>\n<p>Yes! Per <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/3002\">https:\/\/github.com\/iterative\/dvc\/pull\/3002<\/a> and <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/3309\">https:\/\/github.com\/iterative\/dvc\/pull\/3309<\/a> you should now be able to specify paths to pull\/import. Our docs are currently still outdated about this though, it seems\u2026 But feel free to take a look at those PRs and try it, and please find us in dvc.or\/chat if you have specific usage questions \u2013 it may be a little faster to get answers over there.<\/p>",
                "Answer_score":67.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: partial download data; Content: i pushed n images and meta info to remote repository, then remove all data locally. after some time my colleague want download this dataset, but not fully - he filter meta info and he knows paths to necessary images. is there a way download part of data from remote repository?",
        "Question_original_content_gpt_summary":"The user is looking for a way to partially download data from a remote repository that has been previously pushed with images and meta info, but then removed locally.",
        "Question_preprocessed_content":"Title: partial download data; Content: i pushed n images and meta info to remote repository, then remove all data locally. after some time my colleague want download this dataset, but not fully he filter meta info and he knows paths to necessary images. is there a way download part of data from remote repository?",
        "Answer_original_content":"hi @miraina! is there a way download part of data yes! per https:\/\/github.com\/iterative\/\/pull\/3002 and https:\/\/github.com\/iterative\/\/pull\/3309 you should now be able to specify paths to pull\/import. our docs are currently still outdated about this though, it seems but feel free to take a look at those prs and try it, and please find us in .or\/chat if you have specific usage questions it may be a little faster to get answers over there.",
        "Answer_original_content_gpt_summary":"Possible solutions to partially download data from a remote repository that has been previously pushed with images and meta info, but then removed locally are to specify paths to pull\/import. The user can refer to the links provided in the answer and try it out. They can also seek help in .or\/chat if they have specific usage questions.",
        "Answer_preprocessed_content":"hi is there a way download part of data yes! per and you should now be able to specify paths to our docs are currently still outdated about this though, it seems but feel free to take a look at those prs and try it, and please find us in if you have specific usage questions it may be a little faster to get answers over there."
    },
    {
        "Question_id":null,
        "Question_title":"How to log configs except for configs that need to be tuned in w&b when I use ray tune for tuning",
        "Question_body":"<p>Hallo! I want to log some configs that are not for tuning, for example, the name for datasets, and the name for different losses. When I call WandbLoggerCallback in tune.Tuner,  it turns out that only the hyperparameters that it tuned are logged, other configs are not there, even though I have called wandb.config[\u201clossfn\u201d] = \"cross_entropy \" at the beginning.<\/p>\n<p>for example, as you see that there is nothing in the columns dataset and lossfn.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1.jpeg\" data-download-href=\"\/uploads\/short-url\/2KUKjJmSyAImPaso4ffQ0ZnBcsh.jpeg?dl=1\" title=\"Screenshot 2022-09-06 at 21.14.38\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_690x336.jpeg\" alt=\"Screenshot 2022-09-06 at 21.14.38\" data-base62-sha1=\"2KUKjJmSyAImPaso4ffQ0ZnBcsh\" width=\"690\" height=\"336\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_690x336.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_1035x504.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_1380x672.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1351feeae05de6ac2eac8a7aa62014b8d737b5b1_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-09-06 at 21.14.38<\/span><span class=\"informations\">1920\u00d7936 85.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":9,
        "Question_comment_count":null,
        "Question_creation_time":1662491691603,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":145.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-log-configs-except-for-configs-that-need-to-be-tuned-in-w-b-when-i-use-ray-tune-for-tuning\/3076",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-12T16:30:43.219Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a>, you may just need to call <code>wandb.run.update()<\/code>  after you update the config to set the config for the run. If that doesn\u2019t work could you should me the order in which you are setting the config and initializing the WandbLoggerCallback object?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-13T16:58:24.399Z",
                "Answer_body":"<p>Hi Nate, thanks for your reply. wanb.run.update() doesn\u2019t work. I only initialize the WandbLoggerCallback object in the tune.Tuner. The setting config is before the WandbLoggerCallback. I can show you the whole function.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2.png\" data-download-href=\"\/uploads\/short-url\/97ZvpaBquUOOytSM5apu62oXsH0.png?dl=1\" title=\"Screenshot 2022-09-13 at 18.57.19\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_548x500.png\" alt=\"Screenshot 2022-09-13 at 18.57.19\" data-base62-sha1=\"97ZvpaBquUOOytSM5apu62oXsH0\" width=\"548\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_548x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_822x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_1096x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ffad77b104a7d39f0bc8f86e6fb4c8a66dbf2c2_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-09-13 at 18.57.19<\/span><span class=\"informations\">1914\u00d71744 318 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-13T16:59:19.850Z",
                "Answer_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52.png\" data-download-href=\"\/uploads\/short-url\/fheOub5Y58zSSzKSoC92fLLzHVw.png?dl=1\" title=\"Screenshot 2022-09-13 at 18.59.05\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_575x500.png\" alt=\"Screenshot 2022-09-13 at 18.59.05\" data-base62-sha1=\"fheOub5Y58zSSzKSoC92fLLzHVw\" width=\"575\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_575x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_862x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_1150x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/6b137998aa3161e7e777cd8252e85a02b0f38c52_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-09-13 at 18.59.05<\/span><span class=\"informations\">1760\u00d71528 272 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-03T15:29:52.377Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a>, it looks like we don\u2019t have a way to do this currently with our Ray integration. Could you possibly add the parameters to your search space but only as single values? This wouldn\u2019t affect your search space since they are single values but would log the config values to W&amp;B.<\/p>\n<p>If you\u2019d like I can put in a feature request for a more official way of doing this?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-07T14:09:39.979Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a> I wanted to follow up and see if this worked for you and if you\u2019d like us to make a feature request around this?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-11T19:10:55.641Z",
                "Answer_body":"<p>hi\uff01Sorry for the late response! It still doesn\u2019t work. It would be great if you make a feature request of course! Thanks!<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-27T13:53:48.069Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a>, sorry for the delay! I was able to get this to work by using <code>wandb.config.update(&lt;dictionary of config I'd like to add&gt;)<\/code>. I know we tried <code>run.update<\/code> but could you possibly try using <code>wandb.config.update()<\/code> instead of simply assigning the values to the config object?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-01T16:19:06.140Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ziwencheng\">@ziwencheng<\/a>, I just wanted to bump this and see if you got a chance to try the above method of updating the config?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-31T16:19:57.576Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to log configs except for configs that need to be tuned in w&b when i use ray tune for tuning; Content: hallo! i want to log some configs that are not for tuning, for example, the name for datasets, and the name for different losses. when i call loggercallback in tune.tuner, it turns out that only the hyperparameters that it tuned are logged, other configs are not there, even though i have called .config[\u201clossfn\u201d] = \"cross_entropy \" at the beginning. for example, as you see that there is nothing in the columns dataset and lossfn. screenshot 2022-09-06 at 21.14.381920\u00d7936 85.2 kb",
        "Question_original_content_gpt_summary":"The user is facing a challenge of logging configs that are not meant to be tuned in Weights & Biases when using Ray Tune for tuning.",
        "Question_preprocessed_content":"Title: how to log configs except for configs that need to be tuned in w&b when i use ray tune for tuning; Content: hallo! i want to log some configs that are not for tuning, for example, the name for datasets, and the name for different losses. when i call loggercallback in it turns out that only the hyperparameters that it tuned are logged, other configs are not there, even though i have called at the beginning. for example, as you see that there is nothing in the columns dataset and lossfn. screenshot at kb",
        "Answer_original_content":"hi @ziwencheng, you may just need to call .run.update() after you update the config to set the config for the run. if that doesnt work could you should me the order in which you are setting the config and initializing the loggercallback object? thank you, nate hi nate, thanks for your reply. wanb.run.update() doesnt work. i only initialize the loggercallback object in the tune.tuner. the setting config is before the loggercallback. i can show you the whole function. screenshot 2022-09-13 at 18.57.1919141744 318 kb screenshot 2022-09-13 at 18.59.0517601528 272 kb @ziwencheng, it looks like we dont have a way to do this currently with our ray integration. could you possibly add the parameters to your search space but only as single values? this wouldnt affect your search space since they are single values but would log the config values to w&b. if youd like i can put in a feature request for a more official way of doing this? thank you, nate @ziwencheng i wanted to follow up and see if this worked for you and if youd like us to make a feature request around this? thank you, nate hisorry for the late response! it still doesnt work. it would be great if you make a feature request of course! thanks! hi @ziwencheng, sorry for the delay! i was able to get this to work by using .config.update(<dictionary of config i'd like to add>). i know we tried run.update but could you possibly try using .config.update() instead of simply assigning the values to the config object? thank you, nate @ziwencheng, i just wanted to bump this and see if you got a chance to try the above method of updating the config? thank you, nate this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The user is facing a challenge of logging configs that are not meant to be tuned in Weights & Biases when using Ray Tune for tuning. One possible solution is to add the parameters to the search space but only as single values, which would log the config values to W&B. Another solution is to use .config.update() instead of simply assigning the values to the config object.",
        "Answer_preprocessed_content":"hi you may just need to call after you update the config to set the config for the run. if that doesnt work could you should me the order in which you are setting the config and initializing the loggercallback object? thank you, nate hi nate, thanks for your reply. doesnt work. i only initialize the loggercallback object in the the setting config is before the loggercallback. i can show you the whole function. screenshot at kb screenshot at kb it looks like we dont have a way to do this currently with our ray integration. could you possibly add the parameters to your search space but only as single values? this wouldnt affect your search space since they are single values but would log the config values to w&b. if youd like i can put in a feature request for a more official way of doing this? thank you, nate i wanted to follow up and see if this worked for you and if youd like us to make a feature request around this? thank you, nate hisorry for the late response! it still doesnt work. it would be great if you make a feature request of course! thanks! hi sorry for the delay! i was able to get this to work by using . i know we tried but could you possibly try using instead of simply assigning the values to the config object? thank you, nate i just wanted to bump this and see if you got a chance to try the above method of updating the config? thank you, nate this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Page scrolls when moving panels in the web UI",
        "Question_body":"<p>Hello !<\/p>\n<p>When dragging panels within sections or from one section to another, the webpage scrolls back to the top. This makes it impossible to move panels around.<\/p>\n<p>Can this bug be fixed?<br>\nThank you.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1673361904230,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":96.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/page-scrolls-when-moving-panels-in-the-web-ui\/3672",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-11T13:38:00.099Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/bdevillers\">@bdevillers<\/a>, thanks for writing in! I just tried and it seems that I can move panels across sections without any issue, would you mind sharing a link to the Workspace where this is happening and also a video showing this? Thanks!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-11T14:48:02.894Z",
                "Answer_body":"<p>Hi and thank you for your reply!<br>\nThis is on a private project. But here is a GIF of this issue.<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/b6626daffe110a891586f561f1a0fab9e314ab7a.gif\" alt=\"scroll_when_moving_bug\" data-base62-sha1=\"q1rMsCsIXnpVJpCS5KsONT7JBmG\" width=\"640\" height=\"356\" class=\"animated\"><\/p>\n<p>However, I think this is a Firefox-related bug, as it seems that this does not happen in Chrome.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-20T18:13:31.498Z",
                "Answer_body":"<p>Thanks for sharing this <a class=\"mention\" href=\"\/u\/bdevillers\">@bdevillers<\/a>! I just checked and I\u2019m getting the same behaviour in Firefox so I\u2019ll report it as a bug!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: page scrolls when moving panels in the web ui; Content: hello ! when dragging panels within sections or from one section to another, the webpage scrolls back to the top. this makes it impossible to move panels around. can this bug be fixed? thank you.",
        "Question_original_content_gpt_summary":"The user is experiencing difficulty moving panels within sections or from one section to another due to the webpage scrolling back to the top.",
        "Question_preprocessed_content":"Title: page scrolls when moving panels in the web ui; Content: hello ! when dragging panels within sections or from one section to another, the webpage scrolls back to the top. this makes it impossible to move panels around. can this bug be fixed? thank you.",
        "Answer_original_content":"hi @bdevillers, thanks for writing in! i just tried and it seems that i can move panels across sections without any issue, would you mind sharing a link to the workspace where this is happening and also a video showing this? thanks! hi and thank you for your reply! this is on a private project. but here is a gif of this issue. however, i think this is a firefox-related bug, as it seems that this does not happen in chrome. thanks for sharing this @bdevillers! i just checked and im getting the same behaviour in firefox so ill report it as a bug!",
        "Answer_original_content_gpt_summary":"There is a bug in Firefox that causes the webpage to scroll back to the top when trying to move panels within sections or from one section to another. The possible solution is to use a different browser such as Chrome until the bug is fixed.",
        "Answer_preprocessed_content":"hi thanks for writing in! i just tried and it seems that i can move panels across sections without any issue, would you mind sharing a link to the workspace where this is happening and also a video showing this? thanks! hi and thank you for your reply! this is on a private project. but here is a gif of this issue. however, i think this is a bug, as it seems that this does not happen in chrome. thanks for sharing this i just checked and im getting the same behaviour in firefox so ill report it as a bug!"
    },
    {
        "Question_id":null,
        "Question_title":"Anyways to have multiple webservice output?",
        "Question_body":"Is training two web service at the same time doable in studio ? Since I want to train with multiple models but it seems only one would work",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1653904138797,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/869627\/anyways-to-have-multiple-webservice-output.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-30T22:54:40.947Z",
                "Answer_score":0,
                "Answer_body":"Hello @carter-5275\n\nThanks for reaching out to us. If you are asking about studio classic, for your question, quick answer is Yes.\n\nWhat you need to do is, using the \"save as train model\" function to save your models so that you could use it directly in the future. Both studio classic and studio can do it.\n\nThen you can pull all saved models you want to use to your structure so that you can have multi-webservice directly in the studio.\n\nIf you are asking studio, the answer is yes as well. What you need to do is only drag another webservice output directly to you structure.\n\nI hope this helps.\n\nRegards,\nYutong\n\n\n\n\n-Please kindly accept the answer if you feel helpful, thanks.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: anyways to have multiple webservice output?; Content: is training two web service at the same time doable in studio ? since i want to train with multiple models but it seems only one would work",
        "Question_original_content_gpt_summary":"The user is looking for a way to train multiple web services at the same time in Studio.",
        "Question_preprocessed_content":"Title: anyways to have multiple webservice output?; Content: is training two web service at the same time doable in studio ? since i want to train with multiple models but it seems only one would work",
        "Answer_original_content":"hello @carter-5275 thanks for reaching out to us. if you are asking about studio classic, for your question, quick answer is yes. what you need to do is, using the \"save as train model\" function to save your models so that you could use it directly in the future. both studio classic and studio can do it. then you can pull all saved models you want to use to your structure so that you can have multi-webservice directly in the studio. if you are asking studio, the answer is yes as well. what you need to do is only drag another webservice output directly to you structure. i hope this helps. regards, yutong -please kindly accept the answer if you feel helpful, thanks.",
        "Answer_original_content_gpt_summary":"Possible solutions to train multiple web services at the same time in Studio are:\n\n1. In Studio Classic, use the \"save as train model\" function to save models and pull all saved models to the structure to have multi-web service directly in the studio.\n2. In Studio, drag another web service output directly to the structure.",
        "Answer_preprocessed_content":"hello thanks for reaching out to us. if you are asking about studio classic, for your question, quick answer is yes. what you need to do is, using the save as train model function to save your models so that you could use it directly in the future. both studio classic and studio can do it. then you can pull all saved models you want to use to your structure so that you can have directly in the studio. if you are asking studio, the answer is yes as well. what you need to do is only drag another webservice output directly to you structure. i hope this helps. regards, yutong please kindly accept the answer if you feel helpful, thanks."
    },
    {
        "Question_id":null,
        "Question_title":"How to view runs on all remotes?",
        "Question_body":"<p>Hi, I have concurrent runs on multiple remotes (say 4 runs on 4 different remote). Is there a way to view the status of all runs from a single interface? I would imagine that <code>guild runs<\/code> should show them, but for some reason the remote runs appear as \u201cterminated\u201d (while in fact running) and switch to \u201ccompleted\u201d when they are finished.<br>\nI am working with a cluster that has shared drives if that could be taken advantage of.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1614500077203,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":377.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/how-to-view-runs-on-all-remotes\/554",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-01T14:40:33.045Z",
                "Answer_body":"<p>Guild has an issue when showing \u201crunning\u201d status on shared drives where the underlying processes are owned by another system. Unfortunately there\u2019s no good workaround for that, other than using Guild\u2019s remote copy\/sync capability. However that\u2019s quite inefficient as it requires whole copies of the runs just to get an accurate status.<\/p>\n<p>I\u2019ll bump the priority on this item as it comes up a fair amount, esp e.g. when working with Slurm and other HPC envs that use shared drives.<\/p>\n<p>I opened a new <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/272\">issue on GitHub<\/a> to explicitly track the resolution of this problem. If you watch that issue you\u2019ll get updates when it\u2019s resolved.<\/p>\n<p>Sorry about that!<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-01T14:51:35.000Z",
                "Answer_body":"<p>Thank you.<\/p>",
                "Answer_score":2.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to view runs on all remotes?; Content: hi, i have concurrent runs on multiple remotes (say 4 runs on 4 different remote). is there a way to view the status of all runs from a single interface? i would imagine that guild runs should show them, but for some reason the remote runs appear as \u201cterminated\u201d (while in fact running) and switch to \u201ccompleted\u201d when they are finished. i am working with a cluster that has shared drives if that could be taken advantage of. thanks!",
        "Question_original_content_gpt_summary":"The user is trying to find a way to view the status of concurrent runs on multiple remotes from a single interface.",
        "Question_preprocessed_content":"Title: how to view runs on all remotes?; Content: hi, i have concurrent runs on multiple remotes . is there a way to view the status of all runs from a single interface? i would imagine that should show them, but for some reason the remote runs appear as terminated and switch to completed when they are finished. i am working with a cluster that has shared drives if that could be taken advantage of. thanks!",
        "Answer_original_content":"guild has an issue when showing running status on shared drives where the underlying processes are owned by another system. unfortunately theres no good workaround for that, other than using guilds remote copy\/sync capability. however thats quite inefficient as it requires whole copies of the runs just to get an accurate status. ill bump the priority on this item as it comes up a fair amount, esp e.g. when working with slurm and other hpc envs that use shared drives. i opened a new issue on github to explicitly track the resolution of this problem. if you watch that issue youll get updates when its resolved. sorry about that! thank you.",
        "Answer_original_content_gpt_summary":"There is currently no good workaround for viewing the status of concurrent runs on multiple remotes from a single interface when using guild on shared drives. However, using guild's remote copy\/sync capability is an option, although it is inefficient as it requires whole copies of the runs just to get an accurate status. The issue has been raised on Github and the priority has been bumped up to resolve the problem. Watching the issue on Github will provide updates when it is resolved.",
        "Answer_preprocessed_content":"guild has an issue when showing running status on shared drives where the underlying processes are owned by another system. unfortunately theres no good workaround for that, other than using guilds remote capability. however thats quite inefficient as it requires whole copies of the runs just to get an accurate status. ill bump the priority on this item as it comes up a fair amount, esp when working with slurm and other hpc envs that use shared drives. i opened a new issue on github to explicitly track the resolution of this problem. if you watch that issue youll get updates when its resolved. sorry about that! thank you."
    },
    {
        "Question_id":null,
        "Question_title":"Yolo5 model deployment into SageMaker endpoint",
        "Question_body":"I have a trained yolo5 model which I deployed into Real-time end-point in SageMaker. I tried almost all gpus computes available but I could not get better than 2.6 sec for inference time. This is a light model and my target is to have < 1sec inference time. Could you please help me with any hints? I transformed the model from pytorch to TF format. Thank you!",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1662693076387,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhNfeNlTPQSOwfUg_rCnUpw\/yolo-5-model-deployment-into-sage-maker-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-09T11:25:25.875Z",
                "Answer_score":0,
                "Answer_body":"Have you read the following two blog posts?\n\nMaximize TensorFlow performance on Amazon SageMaker endpoints for real-time inference\nReduce computer vision inference latency using gRPC with TensorFlow serving on Amazon SageMaker?\n\nThe first explores a few parameters that you can use to maximize the performance of a TensorFlow-based SageMaker real-time endpoint. These parameters are in essence overprovisioning serving processes and adjusting their parallel processing capabilities. As we saw in the tables, this overprovisioning and adjustment leads to better utilization of resources and higher throughput, sometimes an increase as much as 1,000%.\n\nThe second demonstrates how to reduce model serving latency for TensorFlow computer vision models on SageMaker via in-server gRPC communication, leading to gains of 75% in latency improvement in the example shown.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: yolo5 model deployment into endpoint; Content: i have a trained yolo5 model which i deployed into real-time end-point in . i tried almost all gpus computes available but i could not get better than 2.6 sec for inference time. this is a light model and my target is to have < 1sec inference time. could you please help me with any hints? i transformed the model from pytorch to tf format. thank you!",
        "Question_original_content_gpt_summary":"The user is having difficulty deploying a YOLO5 model into a real-time endpoint, and is unable to achieve an inference time of less than 1 second despite trying various GPUs.",
        "Question_preprocessed_content":"Title: yolo model deployment into endpoint; Content: i have a trained yolo model which i deployed into in . i tried almost all gpus computes available but i could not get better than sec for inference time. this is a light model and my target is to have sec inference time. could you please help me with any hints? i transformed the model from pytorch to tf format. thank you!",
        "Answer_original_content":"have you read the following two blog posts? maximize tensorflow performance on endpoints for real-time inference reduce computer vision inference latency using grpc with tensorflow serving on ? the first explores a few parameters that you can use to maximize the performance of a tensorflow-based real-time endpoint. these parameters are in essence overprovisioning serving processes and adjusting their parallel processing capabilities. as we saw in the tables, this overprovisioning and adjustment leads to better utilization of resources and higher throughput, sometimes an increase as much as 1,000%. the second demonstrates how to reduce model serving latency for tensorflow computer vision models on via in-server grpc communication, leading to gains of 75% in latency improvement in the example shown.",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's difficulty in deploying a YOLO5 model into a real-time endpoint and achieving an inference time of less than 1 second are to read two blog posts that explore parameters to maximize the performance of a tensorflow-based real-time endpoint and reduce model serving latency for tensorflow computer vision models using in-server grpc communication. These solutions can lead to better utilization of resources, higher throughput, and gains of up to 1,000% and 75% in latency improvement, respectively.",
        "Answer_preprocessed_content":"have you read the following two blog posts? maximize tensorflow performance on endpoints for inference reduce computer vision inference latency using grpc with tensorflow serving on ? the first explores a few parameters that you can use to maximize the performance of a endpoint. these parameters are in essence overprovisioning serving processes and adjusting their parallel processing capabilities. as we saw in the tables, this overprovisioning and adjustment leads to better utilization of resources and higher throughput, sometimes an increase as much as , %. the second demonstrates how to reduce model serving latency for tensorflow computer vision models on via grpc communication, leading to gains of % in latency improvement in the example shown."
    },
    {
        "Question_id":null,
        "Question_title":"Scan_history() is empty",
        "Question_body":"<p>Hi, when I run <code>run.history()<\/code>, I get a sampled version of the history as expected (although the number of samples fluctuates). But when I run <code>run.scan_history()<\/code>, I get an empty object (i.e. 0 rows).<\/p>\n<p>Any idea why this is happening or how it could be fixed?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1675419840337,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":80.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/scan-history-is-empty\/3811",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-03T16:11:05.149Z",
                "Answer_body":"<p>Hi Christian,<br>\nHappy Friday!<\/p>\n<p>Could you send me some code I can try reproducing this with?<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-07T11:08:30.162Z",
                "Answer_body":"<p>Hi Artsiom, thanks for your reply!<\/p>\n<p>Sure, here is an API call to my project:<\/p>\n<pre><code class=\"lang-auto\">import wandb\napi = wandb.Api()\nruns = api.runs('chs20\/scratch-public')\nfor run in runs:\n    history = run.history()\n    scan_history = run.scan_history()\n    print(f'run: {run.name}')\n    print(f'history columns: {len(history.columns)}')\n    print(f'scan_history rows: {len(scan_history.rows)}')\n<\/code><\/pre>\n<p>This outputs for me:<\/p>\n<pre><code class=\"lang-auto\">run: 2023-02-07_10:48:43\nhistory columns: 13\nscan_history rows: 0\n<\/code><\/pre>\n<p>I would have expected the number of <code>history<\/code> columns  to match the number of <code>scan_history<\/code> rows.  In particular I would not expect <code>scan_history<\/code> to have zero rows.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T09:04:57.998Z",
                "Answer_body":"<p>Any ideas what is causing this?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T21:36:11.943Z",
                "Answer_body":"<p>Sorry for the delay in response! So far I haven\u2019t been able to reproduce this on my side so trying a few things <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-20T08:56:15.814Z",
                "Answer_body":"<p>Does the code snippet yield a different output for you?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: scan_history() is empty; Content: hi, when i run run.history(), i get a sampled version of the history as expected (although the number of samples fluctuates). but when i run run.scan_history(), i get an empty object (i.e. 0 rows). any idea why this is happening or how it could be fixed?",
        "Question_original_content_gpt_summary":"The user is encountering an issue where the scan_history() function is returning an empty object.",
        "Question_preprocessed_content":"Title: is empty; Content: hi, when i run , i get a sampled version of the history as expected . but when i run , i get an empty object . any idea why this is happening or how it could be fixed?",
        "Answer_original_content":"hi christian, happy friday! could you send me some code i can try reproducing this with? cheers! artsiom hi artsiom, thanks for your reply! sure, here is an api call to my project: import api = .api() runs = api.runs('chs20\/scratch-public') for run in runs: history = run.history() scan_history = run.scan_history() print(f'run: {run.name}') print(f'history columns: {len(history.columns)}') print(f'scan_history rows: {len(scan_history.rows)}') this outputs for me: run: 2023-02-07_10:48:43 history columns: 13 scan_history rows: 0 i would have expected the number of history columns to match the number of scan_history rows. in particular i would not expect scan_history to have zero rows. any ideas what is causing this? sorry for the delay in response! so far i havent been able to reproduce this on my side so trying a few things does the code snippet yield a different output for you?",
        "Answer_original_content_gpt_summary":"There are no explicit solutions provided in the answer. The answer is a conversation between two individuals discussing an issue with the scan_history() function. The person encountering the issue provides a code snippet that outputs an empty scan_history object, which they believe should not be empty. The other person asks for more information to try and reproduce the issue.",
        "Answer_preprocessed_content":"hi christian, happy friday! could you send me some code i can try reproducing this with? cheers! artsiom hi artsiom, thanks for your reply! sure, here is an api call to my project this outputs for me i would have expected the number of columns to match the number of rows. in particular i would not expect to have zero rows. any ideas what is causing this? sorry for the delay in response! so far i havent been able to reproduce this on my side so trying a few things does the code snippet yield a different output for you?"
    },
    {
        "Question_id":null,
        "Question_title":"Weird login error with wandb?",
        "Question_body":"<p>Error<\/p>\n<pre><code class=\"lang-auto\">---- Running your python main ----\nwandb=&lt;module 'wandb' from '\/home\/miranda9\/miniconda3\/envs\/meta_learning_a100\/lib\/python3.9\/site-packages\/wandb\/__init__.py'&gt;\nwandb: W&amp;B API key is configured. Use `wandb login --relogin` to force relogin\nwandb: Network error (ReadTimeout), entering retry loop.\nwandb: Network error (ReadTimeout), entering retry loop.\nProblem at: \/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/wandb_logging\/common.py 25 setup_wand\nwandb: ERROR Error communicating with wandb process\nwandb: ERROR try: wandb.init(settings=wandb.Settings(start_method='fork'))\nwandb: ERROR or:  wandb.init(settings=wandb.Settings(start_method='thread'))\nwandb: ERROR For more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\nTraceback (most recent call last):\n  File \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_diversity_with_task2vec.py\", line 323, in &lt;module&gt;\n    main()\n  File \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_diversity_with_task2vec.py\", line 254, in main\n    args: Namespace = load_args()\n  File \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_diversity_with_task2vec.py\", line 247, in load_args\n    setup_wand(args)\n  File \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/wandb_logging\/common.py\", line 25, in setup_wand\n    wandb.init(project=args.wandb_project,\n  File \"\/home\/miranda9\/miniconda3\/envs\/meta_learning_a100\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 999, in init\n    run = wi.init()\n  File \"\/home\/miranda9\/miniconda3\/envs\/meta_learning_a100\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_init.py\", line 653, in init\n    raise UsageError(error_message)\nwandb.errors.UsageError: Error communicating with wandb process\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\nFor more info see: https:\/\/docs.wandb.ai\/library\/init#init-start-error\n<\/code><\/pre>\n<p>Why is this happening and what is the solution?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1651943863759,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":93.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/weird-login-error-with-wandb\/2379",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-10T22:19:23.551Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/brando\">@brando<\/a>,<\/p>\n<p>I\u2019m sorry to hear you are facing this. This error usually occurs due to multiprocessing and issues with the OS. I\u2019m curious if you have tried the 2 suggestions in the error message:<\/p>\n<pre><code class=\"lang-auto\">wandb.init(settings=wandb.Settings(start_method='fork'))\nwandb.init(settings=wandb.Settings(start_method='thread'))\n<\/code><\/pre>\n<p>and if either of them worked for you. Additionally, could you share what operating system you are running your code on?<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-16T20:00:53.705Z",
                "Answer_body":"<p>Hi Brando,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-09T22:19:42.435Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: weird login error with ?; Content: error ---- running your python main ---- =<module '' from '\/home\/miranda9\/miniconda3\/envs\/meta_learning_a100\/lib\/python3.9\/site-packages\/\/__init__.py'> : w&b api key is configured. use ` login --relogin` to force relogin : network error (readtimeout), entering retry loop. : network error (readtimeout), entering retry loop. problem at: \/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/_logging\/common.py 25 setup_wand : error error communicating with process : error try: .init(settings=.settings(start_method='fork')) : error or: .init(settings=.settings(start_method='thread')) : error for more info see: https:\/\/docs..ai\/library\/init#init-start-error traceback (most recent call last): file \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_diversity_with_task2vec.py\", line 323, in <module> main() file \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_diversity_with_task2vec.py\", line 254, in main args: namespace = load_args() file \"\/home\/miranda9\/diversity-for-predictive-success-of-meta-learning\/div_src\/diversity_src\/experiment_mains\/main_diversity_with_task2vec.py\", line 247, in load_args setup_wand(args) file \"\/home\/miranda9\/ultimate-utils\/ultimate-utils-proj-src\/uutils\/logging_uu\/_logging\/common.py\", line 25, in setup_wand .init(project=args._project, file \"\/home\/miranda9\/miniconda3\/envs\/meta_learning_a100\/lib\/python3.9\/site-packages\/\/sdk\/_init.py\", line 999, in init run = wi.init() file \"\/home\/miranda9\/miniconda3\/envs\/meta_learning_a100\/lib\/python3.9\/site-packages\/\/sdk\/_init.py\", line 653, in init raise usageerror(error_message) .errors.usageerror: error communicating with process try: .init(settings=.settings(start_method='fork')) or: .init(settings=.settings(start_method='thread')) for more info see: https:\/\/docs..ai\/library\/init#init-start-error why is this happening and what is the solution?",
        "Question_original_content_gpt_summary":"The user encountered a weird login error with a network error (readtimeout) and a traceback error communicating with process, and is seeking a solution.",
        "Question_preprocessed_content":"Title: weird login error with ?; Content: error why is this happening and what is the solution?",
        "Answer_original_content":"hey @brando, im sorry to hear you are facing this. this error usually occurs due to multiprocessing and issues with the os. im curious if you have tried the 2 suggestions in the error message: .init(settings=.settings(start_method='fork')) .init(settings=.settings(start_method='thread')) and if either of them worked for you. additionally, could you share what operating system you are running your code on? thanks, ramit hi brando, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Possible solutions to the login error with network and traceback errors include trying the suggested solutions in the error message, which involve initializing settings with start_method as 'fork' or 'thread', and checking the operating system being used.",
        "Answer_preprocessed_content":"hey im sorry to hear you are facing this. this error usually occurs due to multiprocessing and issues with the os. im curious if you have tried the suggestions in the error message and if either of them worked for you. additionally, could you share what operating system you are running your code on? thanks, ramit hi brando, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Wandb sweep project always getting created in a specific team",
        "Question_body":"<p>Following is my simple Python code along with the sweep.yaml file I am using:<\/p>\n<pre><code class=\"lang-auto\">import wandb\nwandb.init(\n    entity=\"kanishkanarch\", \n    project=\"wandb-sweep-testing\", \n    name=\"my_first_sweep\"\n)\nconfig = wandb.config\nseed = config.seed\nsum = 0 \nsum += seed\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">program: testing.py\nmethod: random\nmetric:\n  name: sum \n  goal: maximize\nparameters:\n  seed:\n    min: 1\n    max: 10\n<\/code><\/pre>\n<p>I created a public project on my personal wandb page by the name <code>wandb-sweep-testing<\/code> but whenever I run <code>wandb sweep sweep.yaml<\/code> file it shows that the project is getting created in another team that I am part of.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/ffd3fc04357ae6289f8d318afb8f06375cc7e20b.png\" data-download-href=\"\/uploads\/short-url\/Av9VD7f5wslMTylimEN27Ms3Qsb.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ffd3fc04357ae6289f8d318afb8f06375cc7e20b_2_690x74.png\" alt=\"image\" data-base62-sha1=\"Av9VD7f5wslMTylimEN27Ms3Qsb\" width=\"690\" height=\"74\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ffd3fc04357ae6289f8d318afb8f06375cc7e20b_2_690x74.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ffd3fc04357ae6289f8d318afb8f06375cc7e20b_2_1035x111.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/ffd3fc04357ae6289f8d318afb8f06375cc7e20b.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ffd3fc04357ae6289f8d318afb8f06375cc7e20b_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1115\u00d7120 62.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>How can I make a project on my personal profile not on my team\u2019s profile?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1660134392672,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":98.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-sweep-project-always-getting-created-in-a-specific-team\/2887",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-12T19:34:48.815Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kanishkanarch\">@kanishkanarch<\/a>,<\/p>\n<p>You should be able to set all runs to log to your personal account through the settings page here : <a href=\"http:\/\/wandb.ai\/settings\" class=\"inline-onebox\">Weights &amp; Biases<\/a> under Project Defaults &gt; Default Location to create new projects.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":21.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-11T19:34:58.348Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sweep project always getting created in a specific team; Content: following is my simple python code along with the sweep.yaml file i am using: import .init( entity=\"kanishkanarch\", project=\"-sweep-testing\", name=\"my_first_sweep\" ) config = .config seed = config.seed sum = 0 sum += seed program: testing.py method: random metric: name: sum goal: maximize parameters: seed: min: 1 max: 10 i created a public project on my personal page by the name -sweep-testing but whenever i run sweep sweep.yaml file it shows that the project is getting created in another team that i am part of. image1115\u00d7120 62.7 kb how can i make a project on my personal profile not on my team\u2019s profile?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where their sweep project is always getting created in a specific team, despite their attempts to create it on their personal profile.",
        "Question_preprocessed_content":"Title: sweep project always getting created in a specific team; Content: following is my simple python code along with the file i am using i created a public project on my personal page by the name but whenever i run file it shows that the project is getting created in another team that i am part of. image kb how can i make a project on my personal profile not on my teams profile?",
        "Answer_original_content":"hi @kanishkanarch, you should be able to set all runs to log to your personal account through the settings page here : under project defaults > default location to create new projects. thanks, ramit this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Solution: The user can set all runs to log to their personal account through the settings page under project defaults > default location to create new projects.",
        "Answer_preprocessed_content":"hi you should be able to set all runs to log to your personal account through the settings page here under project defaults default location to create new projects. thanks, ramit this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Get the list of input data in real time azure ml webservice",
        "Question_body":"Hi, I am new to azure ml and I have a question about Real-time inference .\nafter deploying the service on ACI how can I get the list of input data that I should enter to make a prediction using python code from the parameters of the webservice ( endpoint , api key , webservice name , workspace,..)\n\nthank you",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1647338919760,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/772716\/get-the-list-of-input-data-in-real-time-azure-ml-w.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-27T11:38:48.373Z",
                "Answer_score":0,
                "Answer_body":"Hi, I'm sorry for the delay of my answer,\n\nI am following this guidance :\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=python\n\nand as I said I want to Identify the input data. Let's say I had a friend who deployed a model as real-time inference and I am in the other hand want to make my predictions and I just have the REST Endpoint and API key, how can I identify what should be the input data from these information.\nthe guide I am following didn't specify how to identify the input data structure. I know it should be in Json format like the following Json :\n{\n\"data\":\n[\n<model-specific-data-structure>\n]\n}\nWhat I want to know is how to get the <model-specific-data-structure> with only the deployment output like REST API, swagger_uri.\n\nI hope you understand my problem.\n\nThanks again.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: get the list of input data in real time webservice; Content: hi, i am new to and i have a question about real-time inference . after deploying the service on aci how can i get the list of input data that i should enter to make a prediction using python code from the parameters of the webservice ( endpoint , api key , webservice name , workspace,..) thank you",
        "Question_original_content_gpt_summary":"The user is looking for a way to get the list of input data in order to make a prediction using Python code from the parameters of a real-time inference webservice.",
        "Question_preprocessed_content":"Title: get the list of input data in real time webservice; Content: hi, i am new to and i have a question about inference . after deploying the service on aci how can i get the list of input data that i should enter to make a prediction using python code from the parameters of the webservice thank you",
        "Answer_original_content":"hi, i'm sorry for the delay of my answer, i am following this guidance : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=python and as i said i want to identify the input data. let's say i had a friend who deployed a model as real-time inference and i am in the other hand want to make my predictions and i just have the rest endpoint and api key, how can i identify what should be the input data from these information. the guide i am following didn't specify how to identify the input data structure. i know it should be in json format like the following json : { \"data\": [ ] } what i want to know is how to get the with only the deployment output like rest api, swagger_uri. i hope you understand my problem. thanks again.",
        "Answer_original_content_gpt_summary":"The user is trying to identify the input data structure for a real-time inference webservice using Python code. The answer suggests following the guidance provided in the Microsoft Azure documentation and mentions that the input data should be in JSON format. However, the answer does not provide any specific solutions to identify the input data structure from the deployment output like REST API or Swagger URI.",
        "Answer_preprocessed_content":"hi, i'm sorry for the delay of my answer, i am following this guidance and as i said i want to identify the input data. let's say i had a friend who deployed a model as inference and i am in the other hand want to make my predictions and i just have the rest endpoint and api key, how can i identify what should be the input data from these information. the guide i am following didn't specify how to identify the input data structure. i know it should be in json format like the following json data what i want to know is how to get the with only the deployment output like rest api, i hope you understand my problem. thanks again."
    },
    {
        "Question_id":68463080.0,
        "Question_title":"how create azure machine learning scoring image using local package",
        "Question_body":"<p>I have pkl package saved in my azure devops repository<\/p>\n<p>using below code it searches for package in workspace.\nHow to provide package saved in repository<\/p>\n<pre><code> ws = Workspace.get(\n         name=workspace_name,\n         subscription_id=subscription_id,\n        resource_group=resource_group,\n        auth=cli_auth)\n\nimage_config = ContainerImage.image_configuration(\n    execution_script=&quot;score.py&quot;,\n    runtime=&quot;python-slim&quot;,\n    conda_file=&quot;conda.yml&quot;,\n    description=&quot;Image with ridge regression model&quot;,\n    tags={&quot;area&quot;: &quot;ml&quot;, &quot;type&quot;: &quot;dev&quot;},\n)\n\nimage = Image.create(\n    name=image_name,  models=[model], image_config=image_config, workspace=ws\n)\n\nimage.wait_for_creation(show_output=True)\n\nif image.creation_state != &quot;Succeeded&quot;:\n    raise Exception(&quot;Image creation status: {image.creation_state}&quot;)\n\nprint(\n    &quot;{}(v.{} [{}]) stored at {} with build log {}&quot;.format(\n        image.name,\n        image.version,\n        image.creation_state,\n        image.image_location,\n        image.image_build_log_uri,\n    )\n)\n\n# Writing the image details to \/aml_config\/image.json\nimage_json = {}\nimage_json[&quot;image_name&quot;] = image.name\nimage_json[&quot;image_version&quot;] = image.version\nimage_json[&quot;image_location&quot;] = image.image_location\nwith open(&quot;aml_config\/image.json&quot;, &quot;w&quot;) as outfile:\n    json.dump(image_json, outfile)\n<\/code><\/pre>\n<p>I tried to provide path to models but its fails saying package not found<\/p>\n<p>models = $(System.DefaultWorkingDirectory)\/package_model.pkl<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1626831896507,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":140.0,
        "Owner_creation_time":1567209656790,
        "Owner_last_access_time":1663349187992,
        "Owner_reputation":417.0,
        "Owner_up_votes":53.0,
        "Owner_down_votes":0.0,
        "Owner_views":233.0,
        "Answer_body":"<p>Register model:\nRegister a file or folder as a model by calling <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-\" rel=\"nofollow noreferrer\">Model.register()<\/a>.<\/p>\n<p>In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace.<\/p>\n<pre><code>model = Model.register(workspace=ws,\n                       model_name='',                # Name of the registered model in your workspace.\n                       model_path='',  # Local file to upload and register as a model.\n                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n                       sample_input_dataset=input_dataset,\n                       sample_output_dataset=output_dataset,\n                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n                       description='Ridge regression model to predict diabetes progression.',\n                       tags={'area': 'diabetes', 'type': 'regression'})\n\nprint('Name:', model.name)\nprint('Version:', model.version)\n<\/code><\/pre>\n<p>Deploy machine learning models to Azure: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a><\/p>\n<p>To Troubleshooting remote model deployment Please follow the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?tabs=azcli#function-fails-get_model_path\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1627276170736,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1627278873550,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68463080",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how create scoring image using local package; Content: i have pkl package saved in my azure devops repository using below code it searches for package in workspace. how to provide package saved in repository ws = workspace.get( name=workspace_name, subscription_id=subscription_id, resource_group=resource_group, auth=cli_auth) image_config = containerimage.image_configuration( execution_script=\"score.py\", runtime=\"python-slim\", conda_file=\"conda.yml\", description=\"image with ridge regression model\", tags={\"area\": \"ml\", \"type\": \"dev\"}, ) image = image.create( name=image_name, models=[model], image_config=image_config, workspace=ws ) image.wait_for_creation(show_output=true) if image.creation_state != \"succeeded\": raise exception(\"image creation status: {image.creation_state}\") print( \"{}(v.{} [{}]) stored at {} with build log {}\".format( image.name, image.version, image.creation_state, image.image_location, image.image_build_log_uri, ) ) # writing the image details to \/aml_config\/image.json image_json = {} image_json[\"image_name\"] = image.name image_json[\"image_version\"] = image.version image_json[\"image_location\"] = image.image_location with open(\"aml_config\/image.json\", \"w\") as outfile: json.dump(image_json, outfile) i tried to provide path to models but its fails saying package not found models = $(system.defaultworkingdirectory)\/package_model.pkl",
        "Question_original_content_gpt_summary":"The user is facing challenges in creating a scoring image using a local package saved in their Azure DevOps repository.",
        "Question_preprocessed_content":"Title: how create scoring image using local package; Content: i have pkl package saved in my azure devops repository using below code it searches for package in workspace. how to provide package saved in repository i tried to provide path to models but its fails saying package not found models",
        "Answer_original_content":"register model: register a file or folder as a model by calling model.register(). in addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. using tags, for instance, you can categorize your models and apply filters when listing models in your workspace. model = model.register(workspace=ws, model_name='', # name of the registered model in your workspace. model_path='', # local file to upload and register as a model. model_framework=model.framework.scikitlearn, # framework used to create the model. model_framework_version=sklearn.__version__, # version of scikit-learn used to create the model. sample_input_dataset=input_dataset, sample_output_dataset=output_dataset, resource_configuration=resourceconfiguration(cpu=1, memory_in_gb=0.5), description='ridge regression model to predict diabetes progression.', tags={'area': 'diabetes', 'type': 'regression'}) print('name:', model.name) print('version:', model.version) deploy machine learning models to azure: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python to troubleshooting remote model deployment please follow the document.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of creating a scoring image using a local package saved in Azure DevOps repository is to register the model by calling model.register(). This will store model metadata such as model description, tags, and framework information that will be useful when managing and deploying models in the workspace. Additionally, the user can deploy machine learning models to Azure by following the documentation provided. For troubleshooting remote model deployment, the user can refer to the document as well.",
        "Answer_preprocessed_content":"register model register a file or folder as a model by calling in addition to the content of the model file itself, your registered model will also store model metadata model description, tags, and framework information that will be useful when managing and deploying models in your workspace. using tags, for instance, you can categorize your models and apply filters when listing models in your workspace. deploy machine learning models to azure to troubleshooting remote model deployment please follow the document."
    },
    {
        "Question_id":65054187.0,
        "Question_title":"Can you use sagemaker Python libraries on my localhost?",
        "Question_body":"<p>I am interested if one can import sagemaker packages on your own local Python environment or whether they are restricted to AWS Sagemaker?<\/p>\n<pre><code>from sagemaker_automl import AutoMLInteractiveRunner, AutoMLLocalCandidate\n<\/code><\/pre>\n<p>For instance can I somehow download the sagemaker_automl?<\/p>\n<p>I know the there are no sagemaker packages available in the conda repository. Perhaps there is some other way to get them.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1606595637157,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":162.0,
        "Owner_creation_time":1310821356880,
        "Owner_last_access_time":1664050779590,
        "Owner_reputation":14913.0,
        "Owner_up_votes":307.0,
        "Owner_down_votes":1.0,
        "Owner_views":1093.0,
        "Answer_body":"<p>The Sagemaker Python SDK is <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\" rel=\"nofollow noreferrer\">open source and on GitHub<\/a>, as well as published on <a href=\"https:\/\/pypi.org\/project\/sagemaker\/\" rel=\"nofollow noreferrer\">Pypi<\/a>.<\/p>\n<p>You can install it by running:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install sagemaker\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1606601111440,
        "Answer_score":3.0,
        "Owner_location":"Slovenia",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65054187",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can you use python libraries on my localhost?; Content: i am interested if one can import packages on your own local python environment or whether they are restricted to ? from _automl import automlinteractiverunner, automllocalcandidate for instance can i somehow download the _automl? i know the there are no packages available in the conda repository. perhaps there is some other way to get them.",
        "Question_original_content_gpt_summary":"The user is questioning if they can import packages on their own local Python environment and if they can download the _automl package, as it is not available in the Conda repository.",
        "Question_preprocessed_content":"Title: can you use python libraries on my localhost?; Content: i am interested if one can import packages on your own local python environment or whether they are restricted to ? for instance can i somehow download the i know the there are no packages available in the conda repository. perhaps there is some other way to get them.",
        "Answer_original_content":"the python sdk is open source and on github, as well as published on pypi. you can install it by running: pip install",
        "Answer_original_content_gpt_summary":"Possible solutions: \n\n- The user can import packages on their own local Python environment. \n- The user can download the _automl package from the Python SDK, which is open source and available on GitHub and PyPI. \n- To install the _automl package, the user can run the command \"pip install\" in their local Python environment.",
        "Answer_preprocessed_content":"the python sdk is open source and on github, as well as published on pypi. you can install it by running"
    },
    {
        "Question_id":null,
        "Question_title":"Use S3 as a git repo",
        "Question_body":"I have a sagemaker notebook that has no connections to internet or codecommit but has access to 1 s3 bucket. I would like to use that 1 s3 bucket as a place to house git repos, ideally I would like to be able to pull\/push to repos in that bucket from other sagemaker notebooks or ec2 instances that have connections to that bucket. Has anyone tried this before?",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1656607653458,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":236.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUeFHB_qvQw67d9knOyw1Ig\/use-s-3-as-a-git-repo",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-30T16:53:27.277Z",
                "Answer_score":1,
                "Answer_body":"Hi! I believe from AWS we don't have an official solution for this strategy. The are some solutions out there like https:\/\/github.com\/bgahagan\/git-remote-s3 you could use if you can upload the latest release to the S3 bucket and install it on your notebook. Hope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-01T12:12:07.011Z",
                "Answer_score":0,
                "Answer_body":"Check out these assets to see if they can help to accomplish this: https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/ https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: use s3 as a git repo; Content: i have a notebook that has no connections to internet or codecommit but has access to 1 s3 bucket. i would like to use that 1 s3 bucket as a place to house git repos, ideally i would like to be able to pull\/push to repos in that bucket from other notebooks or ec2 instances that have connections to that bucket. has anyone tried this before?",
        "Question_original_content_gpt_summary":"The user is looking for a way to use an S3 bucket as a place to house Git repos, and to be able to pull\/push to repos in that bucket from other notebooks or EC2 instances that have connections to that bucket.",
        "Question_preprocessed_content":"Title: use s as a git repo; Content: i have a notebook that has no connections to internet or codecommit but has access to s bucket. i would like to use that s bucket as a place to house git repos, ideally i would like to be able to to repos in that bucket from other notebooks or ec instances that have connections to that bucket. has anyone tried this before?",
        "Answer_original_content":"hi! i believe from aws we don't have an official solution for this strategy. the are some solutions out there like https:\/\/github.com\/bgahagan\/git-remote-s3 you could use if you can upload the latest release to the s3 bucket and install it on your notebook. hope this helps! check out these assets to see if they can help to accomplish this: https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon--notebook-instances\/ https:\/\/docs.aws.amazon.com\/\/latest\/dg\/nbi-git-repo.html",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include using a third-party solution like git-remote-s3, uploading the latest release to the S3 bucket, and installing it on the notebook. The answer also suggests checking out some AWS resources like the common workflows on Amazon Notebook Instances and the documentation on using Git repos.",
        "Answer_preprocessed_content":"hi! i believe from aws we don't have an official solution for this strategy. the are some solutions out there like you could use if you can upload the latest release to the s bucket and install it on your notebook. hope this helps! check out these assets to see if they can help to accomplish this"
    },
    {
        "Question_id":null,
        "Question_title":"How do I query whether a run object is disabled (`RunDisabled`)?",
        "Question_body":"<p>Given a <code>run<\/code> object (such as the one retuned by <code>wandb.init()<\/code>, what\u2019s the right way to query its status?<br>\nE.g. `run.status == \u2018RunDisabled\u2019?<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":null,
        "Question_creation_time":1650222897307,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":156.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-i-query-whether-a-run-object-is-disabled-rundisabled\/2256",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-18T07:54:47.591Z",
                "Answer_body":"<p>Hey there,<\/p>\n<p>The run object returned by wandb.init() doesn\u2019t have a state attribute but the run object that is returned by our Public API does.<\/p>\n<p>Here is an example you can use:<\/p>\n<p>api = wandb.Api()<br>\nrun = api.run(\u2018entity\/project\/run_id\u2019)<br>\nrun.state<\/p>\n<p>Best,<br>\nArman<\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-19T16:31:43.776Z",
                "Answer_body":"<pre><code class=\"lang-auto\">wandb_run = wandb.init(\n  entity=\"my_entity\",\n  project=\"my_project\",\n  job_type=\"my_cool_job\",\n  mode=\"disabled\"\n)\n\napi = wandb.Api()\nrun = api.run(f\"my_entity\/my_project\/{wandb_run.id}\")\n<\/code><\/pre>\n<p>Returns an error: <code>wandb.errors.CommError: Could not find run &lt;Run ...  (not found)&gt;<\/code><\/p>\n<p>Please advise - am I doing something wrong?<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-21T11:14:18.444Z",
                "Answer_body":"<p>Yes, you are initializing the experiment in disabled mode which will just mock out all the wandb method calls. So the run doesn\u2019t really exist.<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-22T15:25:34.249Z",
                "Answer_body":"<p>Hi Arman,<\/p>\n<p>Sorry, I\u2019m not feeling I\u2019m getting a clear answer here. Let me rephrase my question: Given a <code>run<\/code> object as returned by <code>wand.init()<\/code> (or a <code>run id<\/code>), how do I find out if it\u2019s a valid run (i.e. not <code>disabled<\/code>)?<br>\nCan you provide a short and concise code to implement this check?<\/p>\n<p>Thanks,<br>\nRan<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-03T03:42:42.164Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/ranshadmi-nexite\">@ranshadmi-nexite<\/a>, apologies about the delay on this. I found a workaround for this. You can use this code snippet to check if the run is disabled:<\/p>\n<pre><code class=\"lang-auto\">run = wandb.init()\nisinstance(run.mode, wandb.sdk.lib.disabled.RunDisabled)\n<\/code><\/pre>\n<p>If the run is not disable, run.mode type will be string, if it is disabled, the isinstance call will return True.<\/p>\n<p>Please let me know if this works for you.<\/p>\n<p>Best,<br>\nArman<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-03T11:27:16.886Z",
                "Answer_body":"<p>What you suggest would work. I also found another workaround which doesn\u2019t require the <code>run<\/code> variable to be supplied (and is slightly prettier I think):<\/p>\n<pre><code class=\"lang-auto\">import wandb\nprint(\"active\" if isinstance(wandb.run, wandb.sdk.wandb_run.Run) else \"inactive\")\n<\/code><\/pre>\n<p>What do you think?<\/p>\n<p>Anyway I think you guys in W&amp;B need to provide a simpler and more concise API call to get the current run status. Just my thought.<\/p>\n<p>Thanks,<br>\nRan<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-05T15:27:05.981Z",
                "Answer_body":"<p>Yeah that should work as well. Thanks for the feedback <a class=\"mention\" href=\"\/u\/ranshadmi-nexite\">@ranshadmi-nexite<\/a>. I\u2019ll share it with the team. In the meantime i just want to make sure that you don\u2019t have any other questions.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-05T16:47:51.792Z",
                "Answer_body":"<p>No further questions regarding this topic, thank you.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-04T16:47:54.525Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i query whether a run object is disabled (`rundisabled`)?; Content: given a run object (such as the one retuned by .init(), what\u2019s the right way to query its status? e.g. `run.status == \u2018rundisabled\u2019?",
        "Question_original_content_gpt_summary":"The user is trying to query the status of a run object to determine if it is disabled (`rundisabled`).",
        "Question_preprocessed_content":"Title: how do i query whether a run object is disabled ?; Content: given a object such as the one retuned by , whats the right way to query its status? rundisabled?",
        "Answer_original_content":"hey there, the run object returned by .init() doesnt have a state attribute but the run object that is returned by our public api does. here is an example you can use: api = .api() run = api.run(entity\/project\/run_id) run.state best, arman _run = .init( entity=\"my_entity\", project=\"my_project\", job_type=\"my_cool_job\", mode=\"disabled\" ) api = .api() run = api.run(f\"my_entity\/my_project\/{_run.id}\") returns an error: .errors.commerror: could not find run <run ... (not found)> please advise - am i doing something wrong? yes, you are initializing the experiment in disabled mode which will just mock out all the method calls. so the run doesnt really exist. hi arman, sorry, im not feeling im getting a clear answer here. let me rephrase my question: given a run object as returned by wand.init() (or a run id), how do i find out if its a valid run (i.e. not disabled)? can you provide a short and concise code to implement this check? thanks, ran hey @ranshadmi-nexite, apologies about the delay on this. i found a workaround for this. you can use this code snippet to check if the run is disabled: run = .init() isinstance(run.mode, .sdk.lib.disabled.rundisabled) if the run is not disable, run.mode type will be string, if it is disabled, the isinstance call will return true. please let me know if this works for you. best, arman what you suggest would work. i also found another workaround which doesnt require the run variable to be supplied (and is slightly prettier i think): import print(\"active\" if isinstance(.run, .sdk._run.run) else \"inactive\") what do you think? anyway i think you guys in w&b need to provide a simpler and more concise api call to get the current run status. just my thought. thanks, ran yeah that should work as well. thanks for the feedback @ranshadmi-nexite. ill share it with the team. in the meantime i just want to make sure that you dont have any other questions. no further questions regarding this topic, thank you. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Possible solutions to determine if a run object is disabled include using the `run.state` attribute if the run object is returned by the public API, or checking if the `run.mode` attribute is an instance of `rundisabled` if the run object is returned by `.init()`. Another workaround involves checking if `.run` is an instance of `run`. However, the user suggests that W&B should provide a simpler and more concise API call to get the current run status.",
        "Answer_preprocessed_content":"hey there, the run object returned by doesnt have a state attribute but the run object that is returned by our public api does. here is an example you can use api run best, arman returns an error please advise am i doing something wrong? yes, you are initializing the experiment in disabled mode which will just mock out all the method calls. so the run doesnt really exist. hi arman, sorry, im not feeling im getting a clear answer here. let me rephrase my question given a object as returned by , how do i find out if its a valid run ? can you provide a short and concise code to implement this check? thanks, ran hey apologies about the delay on this. i found a workaround for this. you can use this code snippet to check if the run is disabled if the run is not disable, type will be string, if it is disabled, the isinstance call will return true. please let me know if this works for you. best, arman what you suggest would work. i also found another workaround which doesnt require the variable to be supplied what do you think? anyway i think you guys in w&b need to provide a simpler and more concise api call to get the current run status. just my thought. thanks, ran yeah that should work as well. thanks for the feedback ill share it with the team. in the meantime i just want to make sure that you dont have any other questions. no further questions regarding this topic, thank you. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Hyperparameter search for RL(stable baseline3)",
        "Question_body":"<p>It seems the hyperparameter search guide in documentation is catered for supervised setting. Is there any guide on adapting it to reinforcement learning setting, specifically stable baseline3?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1667204781913,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":267.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hyperparameter-search-for-rl-stable-baseline3\/3349",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-02T13:58:39.731Z",
                "Answer_body":"<p>Hi Zeyu Xhang,<\/p>\n<p>Thanks for your question! Here you have an article about sweeps with RL. Also, I was wondering if you could explain me exactly in which step are you having issues to create the sweep and so I can help you better. Thanks!<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-07T16:09:59.737Z",
                "Answer_body":"<p>Hi Zeyu,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-30T08:27:02.346Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: hyperparameter search for rl(stable baseline3); Content: it seems the hyperparameter search guide in documentation is catered for supervised setting. is there any guide on adapting it to reinforcement learning setting, specifically stable baseline3?",
        "Question_original_content_gpt_summary":"The user is looking for guidance on how to adapt the hyperparameter search guide in the documentation to a reinforcement learning setting, specifically stable baseline3.",
        "Question_preprocessed_content":"Title: hyperparameter search for rl ; Content: it seems the hyperparameter search guide in documentation is catered for supervised setting. is there any guide on adapting it to reinforcement learning setting, specifically stable baseline ?",
        "Answer_original_content":"hi zeyu xhang, thanks for your question! here you have an article about sweeps with rl. also, i was wondering if you could explain me exactly in which step are you having issues to create the sweep and so i can help you better. thanks! best, luis hi zeyu, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no specific solutions provided in the answer. The responder provides an article about sweeps with reinforcement learning and asks for more information about the user's specific issue to better assist them. The responder also follows up with the user to see if their issue has been resolved.",
        "Answer_preprocessed_content":"hi zeyu xhang, thanks for your question! here you have an article about sweeps with rl. also, i was wondering if you could explain me exactly in which step are you having issues to create the sweep and so i can help you better. thanks! best, luis hi zeyu, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":70538098.0,
        "Question_title":"Databricks MLFlow AutoML XGBoost can't predict_proba()",
        "Question_body":"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).<\/p>\n<p>The outputted model is of this variety:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n      artifact_path: model\n      flavor: mlflow.sklearn\n      run_id: 123456789\n<\/code><\/pre>\n<p>Any idea why when I use <code>model.predict_proba(X)<\/code>, I get this response?<\/p>\n<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'<\/code><\/p>\n<p>I know it is possible to get the probabilities because ROC\/AUC is a metric used for tuning the model. Any help would be amazing!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1640912523883,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":451.0,
        "Owner_creation_time":1562828557216,
        "Owner_last_access_time":1663796637888,
        "Owner_reputation":77.0,
        "Owner_up_votes":35.0,
        "Owner_down_votes":0.0,
        "Owner_views":17.0,
        "Answer_body":"<p>I had the same issue with catboost model.\nThe way I solved it was by saving the artifacts in a local dir<\/p>\n<pre><code>import os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = &quot;\/dbfs\/FileStore\/user\/models&quot;\nlocal_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```\n\n```model_path = '\/dbfs\/FileStore\/user\/models\/model\/model.cb'\nmodel = CatBoostClassifier()\nmodel = model.load_model(model_path)\nmodel.predict_proba(test_set)```\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646744187860,
        "Answer_score":2.0,
        "Owner_location":"San Francisco, CA, USA",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70538098",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: databricks automl xgboost can't predict_proba(); Content: i used automl in databricks notebooks for a binary classification problem and the winning model flavor was xgboost (big surprise). the outputted model is of this variety: .pyfunc.loaded_model: artifact_path: model flavor: .sklearn run_id: 123456789 any idea why when i use model.predict_proba(x), i get this response? attributeerror: 'pyfuncmodel' object has no attribute 'predict_proba' i know it is possible to get the probabilities because roc\/auc is a metric used for tuning the model. any help would be amazing!",
        "Question_original_content_gpt_summary":"The user is encountering an issue with their databricks notebook, where the xgboost model they are using does not have the attribute 'predict_proba', despite the fact that it is necessary for tuning the model.",
        "Question_preprocessed_content":"Title: databricks automl xgboost can't ; Content: i used automl in databricks notebooks for a binary classification problem and the winning model flavor was xgboost . the outputted model is of this variety any idea why when i use , i get this response? i know it is possible to get the probabilities because is a metric used for tuning the model. any help would be amazing!",
        "Answer_original_content":"i had the same issue with catboost model. the way i solved it was by saving the artifacts in a local dir import os from .tracking import client client = client() local_dir = \"\/dbfs\/filestore\/user\/models\" local_path = client.download_artifacts('run_id', \"model\", local_dir)``` ```model_path = '\/dbfs\/filestore\/user\/models\/model\/model.cb' model = catboostclassifier() model = model.load_model(model_path) model.predict_proba(test_set)```",
        "Answer_original_content_gpt_summary":"The possible solution to the issue with the xgboost model not having the attribute 'predict_proba' is to save the artifacts in a local directory and load the model from that directory using the appropriate library. This solution was used for a similar issue with the catboost model.",
        "Answer_preprocessed_content":"i had the same issue with catboost model. the way i solved it was by saving the artifacts in a local dir"
    },
    {
        "Question_id":null,
        "Question_title":"How to show run name in repport Table",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m using Tables to log text (input, output, and expected) and I\u2019m showing it in a report.<\/p>\n<p>That\u2019s working perfectly, I changed \u201cMerged Table\u201d to \u201cList of tables\u201d to be able to change Run and see one table per run.<\/p>\n<p>However, it shows \u201c1 of 8\u201d so okay I need to open runs, check the first one, and get its name.<\/p>\n<p>Is there any way to show the run name instead of the little colors dot in the table ?<\/p>\n<p>Thanks in advance,<br>\nHave a great day <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=11\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1642444355909,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":303.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-show-run-name-in-repport-table\/1775",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-18T21:18:29.053Z",
                "Answer_body":"<p>Hi Pierre! You can add the run name by first creating a new column on your table. This is done by clicking on the three dots on the right of a column name and then clicking on either \u2018Insert 1 left\u2019 or \u2018insert 1 right\u2019. Then, under \u2018Cell expression\u2019, type in row.run.name and the run names will appear in that column.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-24T13:15:38.081Z",
                "Answer_body":"<p>Hi Pierre, I\u2019m just checking in to see if you still need help with this?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-27T15:12:12.744Z",
                "Answer_body":"<p>Hi Pierre, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-18T18:33:20.302Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to show run name in repport table; Content: hi, i\u2019m using tables to log text (input, output, and expected) and i\u2019m showing it in a report. that\u2019s working perfectly, i changed \u201cmerged table\u201d to \u201clist of tables\u201d to be able to change run and see one table per run. however, it shows \u201c1 of 8\u201d so okay i need to open runs, check the first one, and get its name. is there any way to show the run name instead of the little colors dot in the table ? thanks in advance, have a great day",
        "Question_original_content_gpt_summary":"The user is facing a challenge of displaying the run name in a report table instead of a small colored dot.",
        "Question_preprocessed_content":"Title: how to show run name in repport table; Content: hi, im using tables to log text and im showing it in a report. thats working perfectly, i changed merged table to list of tables to be able to change run and see one table per run. however, it shows of so okay i need to open runs, check the first one, and get its name. is there any way to show the run name instead of the little colors dot in the table ? thanks in advance, have a great day",
        "Answer_original_content":"hi pierre! you can add the run name by first creating a new column on your table. this is done by clicking on the three dots on the right of a column name and then clicking on either insert 1 left or insert 1 right. then, under cell expression, type in row.run.name and the run names will appear in that column. hi pierre, im just checking in to see if you still need help with this? hi pierre, since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"To display the run name in a report table instead of a small colored dot, the user can create a new column on the table by clicking on the three dots on the right of a column name and then clicking on either insert 1 left or insert 1 right. Then, under cell expression, type in row.run.name and the run names will appear in that column.",
        "Answer_preprocessed_content":"hi pierre! you can add the run name by first creating a new column on your table. this is done by clicking on the three dots on the right of a column name and then clicking on either insert left or insert right. then, under cell expression, type in and the run names will appear in that column. hi pierre, im just checking in to see if you still need help with this? hi pierre, since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Min loss using Weave panel",
        "Question_body":"<p>I logged loss in my run. I would like to compare runs by min loss. I found <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/736\" rel=\"noopener nofollow ugc\">this<\/a> feature request, which suggests using a Weave panel with:<\/p>\n<pre><code class=\"lang-auto\">runs.map((row) =&gt; row.history[\"loss\"].min)\n<\/code><\/pre>\n<p>This works when I add the panel on the run page (i.e. <code>\/&lt;team&gt;\/&lt;project&gt;\/runs\/&lt;run id&gt;<\/code>). However, when I add the panel on the project page (i.e. <code>\/&lt;team&gt;\/&lt;project&gt;<\/code>), all runs show an empty value.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1676309777932,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/min-loss-using-weave-panel\/3874",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-15T20:49:35.671Z",
                "Answer_body":"<p>Hello Szymon!<\/p>\n<p>If you want to compare runs by min loss in the project page, I would recommend making a bar chart panel by going to <code>Add panel<\/code> \u2192 <code>Bar chart<\/code> \u2192 set the <code>Metric<\/code> to <code>training\/loss.min<\/code>.<\/p>\n<p>Does this answer your question?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-21T21:07:11.701Z",
                "Answer_body":"<p>Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_score":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: min loss using weave panel; Content: i logged loss in my run. i would like to compare runs by min loss. i found this feature request, which suggests using a weave panel with: runs.map((row) => row.history[\"loss\"].min) this works when i add the panel on the run page (i.e. \/<team>\/<project>\/runs\/<run id>). however, when i add the panel on the project page (i.e. \/<team>\/<project>), all runs show an empty value.",
        "Question_original_content_gpt_summary":"The user is attempting to compare runs by minimum loss using a weave panel, but is encountering an issue where all runs show an empty value when the panel is added to the project page.",
        "Question_preprocessed_content":"Title: min loss using weave panel; Content: i logged loss in my run. i would like to compare runs by min loss. i found this feature request, which suggests using a weave panel with this works when i add the panel on the run page . however, when i add the panel on the project page , all runs show an empty value.",
        "Answer_original_content":"hello szymon! if you want to compare runs by min loss in the project page, i would recommend making a bar chart panel by going to add panel bar chart set the metric to training\/loss.min. does this answer your question? hi there, i wanted to follow up on this request. please let us know if we can be of further assistance or if your issue has been resolved.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Create a bar chart panel by going to \"add panel\" and selecting \"bar chart\". \n- Set the metric to \"training\/loss.min\". \n\nSummary: To compare runs by minimum loss in the project page, the user can create a bar chart panel and set the metric to \"training\/loss.min\".",
        "Answer_preprocessed_content":"hello szymon! if you want to compare runs by min loss in the project page, i would recommend making a bar chart panel by going to set the to . does this answer your question? hi there, i wanted to follow up on this request. please let us know if we can be of further assistance or if your issue has been resolved."
    },
    {
        "Question_id":37519858.0,
        "Question_title":"What type should the returned scores from an R scoring script?",
        "Question_body":"<p>I am attempting to develop an Azure ML experiment that uses R to perform predictions of a continuous response variable. The initial experiment is relatively simple, incorporating only a few experiment items, including \"Create R Model\", \"Train Model\" and \"Score Model\", along with some data input.<\/p>\n\n<p>I have written a training script and a scoring script, both of which appear to execute without errors when I run the experiment within ML Studio. However, when I examine the scored dataset, the score values are all missing values. So I am concerned that my scoring script could be returning scores incorrectly. Can anyone advise what type I should be returning? Is it meant to be a single column data.frame, or something else?<\/p>\n\n<p>It is also possible that my scores are not being properly calculated within the scoring script, although I have run the training and scoring scripts within R Studio, which shows the expected results. It would also be helpful if someone could suggest how to perform debugging of my scoring script in some way, so that I could determine whereabouts the code is failing to behave as expected.<\/p>\n\n<p>Thanks, Paul<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1464593030317,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":238.0,
        "Owner_creation_time":1464571689107,
        "Owner_last_access_time":1470182061488,
        "Owner_reputation":3.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":"<p>Try using this sample and compare with yours - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1464675335140,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37519858",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what type should the returned scores from an r scoring script?; Content: i am attempting to develop an experiment that uses r to perform predictions of a continuous response variable. the initial experiment is relatively simple, incorporating only a few experiment items, including \"create r model\", \"train model\" and \"score model\", along with some data input. i have written a training script and a scoring script, both of which appear to execute without errors when i run the experiment within ml studio. however, when i examine the scored dataset, the score values are all missing values. so i am concerned that my scoring script could be returning scores incorrectly. can anyone advise what type i should be returning? is it meant to be a single column data.frame, or something else? it is also possible that my scores are not being properly calculated within the scoring script, although i have run the training and scoring scripts within r studio, which shows the expected results. it would also be helpful if someone could suggest how to perform debugging of my scoring script in some way, so that i could determine whereabouts the code is failing to behave as expected. thanks, paul",
        "Question_original_content_gpt_summary":"The user is attempting to develop an experiment that uses R to perform predictions of a continuous response variable, but is encountering issues with the scoring script returning incorrect scores and is looking for advice on how to debug the script.",
        "Question_preprocessed_content":"Title: what type should the returned scores from an r scoring script?; Content: i am attempting to develop an experiment that uses r to perform predictions of a continuous response variable. the initial experiment is relatively simple, incorporating only a few experiment items, including create r model , train model and score model , along with some data input. i have written a training script and a scoring script, both of which appear to execute without errors when i run the experiment within ml studio. however, when i examine the scored dataset, the score values are all missing values. so i am concerned that my scoring script could be returning scores incorrectly. can anyone advise what type i should be returning? is it meant to be a single column or something else? it is also possible that my scores are not being properly calculated within the scoring script, although i have run the training and scoring scripts within r studio, which shows the expected results. it would also be helpful if someone could suggest how to perform debugging of my scoring script in some way, so that i could determine whereabouts the code is failing to behave as expected. thanks, paul",
        "Answer_original_content":"try using this sample and compare with yours - https:\/\/gallery.cortanaintelligence.com\/experiment\/compare-sample-5-in-r-vs-azure-ml-1",
        "Answer_original_content_gpt_summary":"The solution suggested for debugging the scoring script for an experiment that uses R to perform predictions of a continuous response variable is to compare the user's script with a sample script available at https:\/\/gallery.cortanaintelligence.com\/experiment\/compare-sample-5-in-r-vs-azure-ml-1.",
        "Answer_preprocessed_content":"try using this sample and compare with yours"
    },
    {
        "Question_id":null,
        "Question_title":"InternalServerError when launching Jupyterlabs in Azure Machine Learning workspace",
        "Question_body":"My workspace is in EastUS2.\nI have created compute instances from multiple sku's and I always receive the following error when creating instances:\n\n{\n\"error\": {\n\"code\": \"ServiceError\",\n\"severity\": null,\n\"message\": \"InternalServerError\",\n\"messageFormat\": null,\n\"messageParameters\": null,\n\"referenceCode\": null,\n\"detailsUri\": null,\n\"target\": null,\n\"details\": [],\n\"innerError\": null,\n\"debugInfo\": null,\n\"additionalInfo\": null\n},\n\"correlation\": {\n\"operation\": \"f0bc2b1a27a3534eb83eac4f3f71fedf\",\n\"request\": \"589656f8c89b684a\"\n},\n\"environment\": \"eastus2\",\n\"location\": \"eastus2\",\n\"time\": \"2021-09-29T01:01:09.3745269+00:00\",\n\"componentName\": \"notebook-instance-proxy\"\n}\n\nWhat can I do to resolve this issue? I have tried restarting, recreating, and testing other sku's.",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1632877552993,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/569900\/internalservererror-when-launching-jupyterlabs-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-04T21:34:28.567Z",
                "Answer_score":0,
                "Answer_body":"@DwayneThomson-9529\n\nHello, a hotfix has been deployed on Friday, please have another try to see if this works on your end.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: internalservererror when launching jupyterlabs in workspace; Content: my workspace is in eastus2. i have created compute instances from multiple sku's and i always receive the following error when creating instances: { \"error\": { \"code\": \"serviceerror\", \"severity\": null, \"message\": \"internalservererror\", \"messageformat\": null, \"messageparameters\": null, \"referencecode\": null, \"detailsuri\": null, \"target\": null, \"details\": [], \"innererror\": null, \"debuginfo\": null, \"additionalinfo\": null }, \"correlation\": { \"operation\": \"f0bc2b1a27a3534eb83eac4f3f71fedf\", \"request\": \"589656f8c89b684a\" }, \"environment\": \"eastus2\", \"location\": \"eastus2\", \"time\": \"2021-09-29t01:01:09.3745269+00:00\", \"componentname\": \"notebook-instance-proxy\" } what can i do to resolve this issue? i have tried restarting, recreating, and testing other sku's.",
        "Question_original_content_gpt_summary":"The user encountered an \"internalservererror\" when launching jupyterlabs in their workspace in eastus2, despite trying multiple solutions such as restarting, recreating, and testing other sku's.",
        "Question_preprocessed_content":"Title: internalservererror when launching jupyterlabs in workspace; Content: my workspace is in eastus . i have created compute instances from multiple sku's and i always receive the following error when creating instances error , correlation , environment eastus , location eastus , time componentname what can i do to resolve this issue? i have tried restarting, recreating, and testing other sku's.",
        "Answer_original_content":"@dwaynethomson-9529 hello, a hotfix has been deployed on friday, please have another try to see if this works on your end. regards, yutong",
        "Answer_original_content_gpt_summary":"Possible solution: A hotfix has been deployed on Friday, so the user can try launching jupyterlabs again to see if the issue has been resolved.",
        "Answer_preprocessed_content":"hello, a hotfix has been deployed on friday, please have another try to see if this works on your end. regards, yutong"
    },
    {
        "Question_id":69920078.0,
        "Question_title":"What does \"save_graph\" keyword in WandbCallback mean?",
        "Question_body":"<p>I'm using Weights and Biases to track my deep learning models. To monitor everything I use the <code>WandbCallback<\/code> in <code>.fit<\/code>.\nIn the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/integrations\/keras\/wandbcallback\" rel=\"nofollow noreferrer\">WandbCallback documentation<\/a> there is the keyword <code>save_graph<\/code> which defaults to True. The description is very brief and I wondered what that saved graph is and what it's for? Is saving a graph a costly operation? For what is it needed? (like does it complement something else, like saving the best model?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1636577514790,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":60.0,
        "Owner_creation_time":1600299419448,
        "Owner_last_access_time":1662833034528,
        "Owner_reputation":62.0,
        "Owner_up_votes":6.0,
        "Owner_down_votes":0.0,
        "Owner_views":7.0,
        "Answer_body":"<p>That is used to create log a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/data-types\/graph\" rel=\"nofollow noreferrer\">wandb.Graph<\/a> of the model. This class is typically used for saving and diplaying neural net models. It represents the graph as an array of nodes and edges. The nodes can have labels that can be visualized by wandb. Here's an example of the graph that it produces: <a href=\"https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model<\/a><\/p>\n<p>Here's the code that does that within the callback. <a href=\"https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636642289972,
        "Answer_score":1.0,
        "Owner_location":"Germany",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69920078",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what does \"save_graph\" keyword in callback mean?; Content: i'm using to track my deep learning models. to monitor everything i use the callback in .fit. in the callback documentation there is the keyword save_graph which defaults to true. the description is very brief and i wondered what that saved graph is and what it's for? is saving a graph a costly operation? for what is it needed? (like does it complement something else, like saving the best model?)",
        "Question_original_content_gpt_summary":"The user is encountering challenges understanding the purpose of the \"save_graph\" keyword in the Keras callback documentation, and is questioning the cost and necessity of saving a graph.",
        "Question_preprocessed_content":"Title: what does keyword in callback mean?; Content: i'm using to track my deep learning models. to monitor everything i use the in . in the callback documentation there is the keyword which defaults to true. the description is very brief and i wondered what that saved graph is and what it's for? is saving a graph a costly operation? for what is it needed?",
        "Answer_original_content":"that is used to create log a .graph of the model. this class is typically used for saving and diplaying neural net models. it represents the graph as an array of nodes and edges. the nodes can have labels that can be visualized by . here's an example of the graph that it produces: https:\/\/.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model here's the code that does that within the callback. https:\/\/github.com\/\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/\/integration\/keras\/keras.py#l552",
        "Answer_original_content_gpt_summary":"The \"save_graph\" keyword in Keras callback documentation is used to create a graph of the model, which is typically used for saving and displaying neural net models. The graph is represented as an array of nodes and edges, and the nodes can have labels that can be visualized. The answer provides an example of the graph that is produced and the code that does that within the callback.",
        "Answer_preprocessed_content":"that is used to create log a of the model. this class is typically used for saving and diplaying neural net models. it represents the graph as an array of nodes and edges. the nodes can have labels that can be visualized by . here's an example of the graph that it produces here's the code that does that within the callback."
    },
    {
        "Question_id":null,
        "Question_title":"Pulling ML Models from MLFLow in a VM in Azure to the VM that Pipeline Runs",
        "Question_body":"We have a pipeline in a repo in Azure Devops for evaluating ML model performances after each commit. Currently models are manually put into repo. We also have a MLFlow server in a VM in Azure, so instead of putting models manually into the repo we want to pull models from MLFlow which also resides in a Azure VM into the VM that pipeline runs. Is there a way to accomplish this by using service connections or something?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1629093489833,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/514806\/pulling-ml-models-from-mlflow-in-a-vm-in-azure-to.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-17T01:46:49.143Z",
                "Answer_score":1,
                "Answer_body":"@KarahanTolgaAVLTR-6434 Thanks for the question. Can you please add more details about the Model packaging and deploy steps that you performed. We have forwarded to the product team to check on this.\n\nYou can find the Azure ML + MLflow examples here: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/track-and-monitor-experiments\/using-mlflow",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pulling ml models from in a vm in azure to the vm that pipeline runs; Content: we have a pipeline in a repo in azure devops for evaluating ml model performances after each commit. currently models are manually put into repo. we also have a server in a vm in azure, so instead of putting models manually into the repo we want to pull models from which also resides in a azure vm into the vm that pipeline runs. is there a way to accomplish this by using service connections or something?",
        "Question_original_content_gpt_summary":"The user is looking for a way to pull ML models from an Azure VM hosting MLFlow into the VM that their pipeline runs on, using service connections or something similar.",
        "Question_preprocessed_content":"Title: pulling ml models from in a vm in azure to the vm that pipeline runs; Content: we have a pipeline in a repo in azure devops for evaluating ml model performances after each commit. currently models are manually put into repo. we also have a server in a vm in azure, so instead of putting models manually into the repo we want to pull models from which also resides in a azure vm into the vm that pipeline runs. is there a way to accomplish this by using service connections or something?",
        "Answer_original_content":"@karahantolgaavltr-6434 thanks for the question. can you please add more details about the model packaging and deploy steps that you performed. we have forwarded to the product team to check on this. you can find the + mlflow examples here: https:\/\/github.com\/azure\/machinelearningnotebooks\/tree\/master\/how-to-use-\/track-and-monitor-experiments\/using-mlflow",
        "Answer_original_content_gpt_summary":"There are no specific solutions provided in the answer. The responder is asking for more details about the model packaging and deploy steps that were performed and has forwarded the question to the product team. They also provide a link to MLFlow examples on GitHub.",
        "Answer_preprocessed_content":"thanks for the question. can you please add more details about the model packaging and deploy steps that you performed. we have forwarded to the product team to check on this. you can find the + examples here"
    },
    {
        "Question_id":65145994.0,
        "Question_title":"Saving an Matlabplot as an MLFlow artifact",
        "Question_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1607094596640,
        "Question_favorite_count":1.0,
        "Question_score":8.0,
        "Question_view_count":5219.0,
        "Owner_creation_time":1316705139196,
        "Owner_last_access_time":1663085821243,
        "Owner_reputation":6711.0,
        "Owner_up_votes":353.0,
        "Owner_down_votes":3.0,
        "Owner_views":819.0,
        "Answer_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1607094854147,
        "Answer_score":7.0,
        "Owner_location":"Boston, MA",
        "Question_last_edit_time":1607191847983,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: saving an matlabplot as an artifact; Content: i am using databricks and spark 7.4ml, the following code successfully logs the params and metrics, and i can see the roccurve.png in the gui (just the item in the tree below the model). but the actually plot is blank. why? with .start_run(run_name=\"logistic-regression\") as run: pipemodel = pipe.fit(traindf) .spark.log_model(pipemodel, \"model\") predtest = pipemodel.transform(testdf) predtrain = pipemodel.transform(traindf) evaluator=binaryclassificationevaluator(labelcol=\"arrivedlate\") trainroc = evaluator.evaluate(predtrain) testroc = evaluator.evaluate(predtest) print(f\"train roc: {trainroc}\") print(f\"test roc: {testroc}\") .log_param(\"dataset name\", \"flights \" + datasetname) .log_metric(key=\"train roc\", value=trainroc) .log_metric(key=\"test roc\", value=testroc) lrmodel = pipemodel.stages[3] trainingsummary = lrmodel.summary roc = trainingsummary.roc.topandas() plt.plot(roc['fpr'],roc['tpr']) plt.ylabel('false positive rate') plt.xlabel('true positive rate') plt.title('roc curve') plt.show() plt.savefig(\"roccurve.png\") .log_artifact(\"roccurve.png\") plt.close() display(predtest.select(stringcols + [\"arrivedlate\", \"prediction\"])) what the notebook shows: what the shows:",
        "Question_original_content_gpt_summary":"The user is encountering challenges with saving a matlabplot as an artifact in Databricks and Spark 7.4ml.",
        "Question_preprocessed_content":"Title: saving an matlabplot as an artifact; Content: i am using databricks and spark the following code successfully logs the params and metrics, and i can see the in the gui . but the actually plot is blank. why? what the notebook shows what the shows",
        "Answer_original_content":"put plt.show() after plt.savefig() - plt.show() will remove your plot because it is shown already.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of saving a matlabplot as an artifact in Databricks and Spark 7.4ml is to put plt.show() after plt.savefig(). This will remove the plot because it is already shown.",
        "Answer_preprocessed_content":"put after will remove your plot because it is shown already."
    },
    {
        "Question_id":61550297.0,
        "Question_title":"Await returns too soon",
        "Question_body":"<p>I am retrieving a list of image filenames from DynamoDB and using those image filenames to replace the default <code>src=<\/code> image in a portion of a website.<\/p>\n\n<p>I'm a JS novice, so I'm certainly missing something, but my function is returning the list of filenames too late.<\/p>\n\n<p>My inline script is:<\/p>\n\n<pre><code>&lt;script&gt;\n        customElements.whenDefined( 'crowd-bounding-box' ).then( () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>\n\n<p>My JS function is:<\/p>\n\n<pre><code>async function imageslist() {\n    const username = \"sampleuser\";\n    const params = {\n        TableName: \"mytable\",\n        FilterExpression: \"attribute_not_exists(\" + username + \")\",\n        ReturnConsumedCapacity: \"NONE\"\n    };\n    try {\n        var data = await ddb.scan(params).promise()\n        var imglist = [];\n        for(let i = 0; i &lt; data.Items.length; i++) {\n            imglist.push(data.Items[i].img.S);\n        };\n        imglist.sort();\n        var firstimg = imglist[0];\n        console.log(firstimg);\n        return imglist\n    } catch(error) {\n        console.error(error);\n    }\n}\n<\/code><\/pre>\n\n<p>The console reports <code>Result of newImg is: [object Promise]<\/code> and shortly after that, it reports the expected filename.  After the page has been rendered, I can input <code>newImg<\/code> in the console and I receive the expected result.<\/p>\n\n<p>Am I using the <strong>await<\/strong> syntax improperly?<\/p>\n\n<p>Side note: This site uses the Crowd HTML Elements (for Ground Truth and Mechanical Turk), so I'm forced to have the <code>src=<\/code> attribute present in my <code>crowd-bounding-box<\/code> tag and it must be a non-zero value.  I'm loading a default image and replacing it with another image.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1588364900607,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":63.0,
        "Owner_creation_time":1483444144907,
        "Owner_last_access_time":1643982821672,
        "Owner_reputation":312.0,
        "Owner_up_votes":28.0,
        "Owner_down_votes":1.0,
        "Owner_views":65.0,
        "Answer_body":"<p>Anytime you use the <code>await<\/code> keyword, you must use the <code>async<\/code> keyword before the function definition (which you have done). The thing is, any time an async function is called, it will always return a <code>Promise<\/code> object since it expects that some asynchronous task will take place within the function.<\/p>\n\n<p>Therefore,you'll need to <code>await<\/code> the result of the imageslist function and make the surrounding function <code>async<\/code>.<\/p>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;script&gt;\n    customElements.whenDefined( 'crowd-bounding-box' ).then( async () =&gt; {  \n        var imgBox = document.getElementById('annotator');\n        newImg = await imageslist();\n        console.log(\"Result of newImg is: \" + newImg);\n        imgBox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newImg;\n    } )\n&lt;\/script&gt;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1588365818456,
        "Answer_score":1.0,
        "Owner_location":"Hoth",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61550297",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: await returns too soon; Content: i am retrieving a list of image filenames from dynamodb and using those image filenames to replace the default src= image in a portion of a website. i'm a js novice, so i'm certainly missing something, but my function is returning the list of filenames too late. my inline script is: <script> customelements.whendefined( 'crowd-bounding-box' ).then( () => { var imgbox = document.getelementbyid('annotator'); newimg = imageslist(); console.log(\"result of newimg is: \" + newimg); imgbox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newimg; } ) <\/script> my js function is: async function imageslist() { const username = \"sampleuser\"; const params = { tablename: \"mytable\", filterexpression: \"attribute_not_exists(\" + username + \")\", returnconsumedcapacity: \"none\" }; try { var data = await ddb.scan(params).promise() var imglist = []; for(let i = 0; i < data.items.length; i++) { imglist.push(data.items[i].img.s); }; imglist.sort(); var firstimg = imglist[0]; console.log(firstimg); return imglist } catch(error) { console.error(error); } } the console reports result of newimg is: [object promise] and shortly after that, it reports the expected filename. after the page has been rendered, i can input newimg in the console and i receive the expected result. am i using the await syntax improperly? side note: this site uses the crowd html elements (for ground truth and mechanical turk), so i'm forced to have the src= attribute present in my crowd-bounding-box tag and it must be a non-zero value. i'm loading a default image and replacing it with another image.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with the 'await' syntax, where the function is returning the list of filenames too late and the console reports '[object promise]' instead of the expected filename.",
        "Question_preprocessed_content":"Title: await returns too soon; Content: i am retrieving a list of image filenames from dynamodb and using those image filenames to replace the default image in a portion of a website. i'm a js novice, so i'm certainly missing something, but my function is returning the list of filenames too late. my inline script is my js function is the console reports and shortly after that, it reports the expected filename. after the page has been rendered, i can input in the console and i receive the expected result. am i using the await syntax improperly? side note this site uses the crowd html elements , so i'm forced to have the attribute present in my tag and it must be a value. i'm loading a default image and replacing it with another image.",
        "Answer_original_content":"anytime you use the await keyword, you must use the async keyword before the function definition (which you have done). the thing is, any time an async function is called, it will always return a promise object since it expects that some asynchronous task will take place within the function. therefore,you'll need to await the result of the imageslist function and make the surrounding function async. <script> customelements.whendefined( 'crowd-bounding-box' ).then( async () => { var imgbox = document.getelementbyid('annotator'); newimg = await imageslist(); console.log(\"result of newimg is: \" + newimg); imgbox.src = \"https:\/\/my-images-bucket.s3.amazonaws.com\/\" + newimg; } ) <\/script>",
        "Answer_original_content_gpt_summary":"The solution to the challenge with the 'await' syntax is to use the async keyword before the function definition and await the result of the imageslist function. The surrounding function should also be made async.",
        "Answer_preprocessed_content":"anytime you use the keyword, you must use the keyword before the function definition . the thing is, any time an async function is called, it will always return a object since it expects that some asynchronous task will take place within the function. therefore,you'll need to the result of the imageslist function and make the surrounding function ."
    },
    {
        "Question_id":null,
        "Question_title":"Define_metric parameter 'hidden' not working?",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m new to W&amp;B and was setting up my metrics like this:<\/p>\n<pre><code class=\"lang-auto\"># define x-axes\nwandb.define_metric(\"batch#\", hidden=True)   # \"hidden\" not working\nwandb.define_metric(\"epoch#\", hidden=True)\n# define metrics and match to x-axis\nwandb.define_metric(\"loss\", step_metric=\"#batch#\")\nwandb.define_metric(\"f1\", step_metric=\"epoch#\")\n<\/code><\/pre>\n<p>I want graphs of loss\/batch# and f1\/epoch# on my dashboard, but not graphs of batch#\/step or epoch#\/step. Therefore, I set the function parameter \u2018hidden\u2019 to True for these. It does not work, I still get automatically generated panels of them. Any advice?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1674661085424,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":60.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/define-metric-parameter-hidden-not-working\/3748",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-30T18:41:09.863Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tim123\">@tim123<\/a> thanks so much for spotting and reporting this issue. We were able to reproduce it on our end, and I have reported it to our engineering team. I also linked this discussion with the internal ticket, so that we can reach out to you here if there are any further updates. In the meantime, these plots can be only manually deleted from the UI.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: define_metric parameter 'hidden' not working?; Content: hi, im new to w&b and was setting up my metrics like this: # define x-axes .define_metric(\"batch#\", hidden=true) # \"hidden\" not working .define_metric(\"epoch#\", hidden=true) # define metrics and match to x-axis .define_metric(\"loss\", step_metric=\"#batch#\") .define_metric(\"f1\", step_metric=\"epoch#\") i want graphs of loss\/batch# and f1\/epoch# on my dashboard, but not graphs of batch#\/step or epoch#\/step. therefore, i set the function parameter hidden to true for these. it does not work, i still get automatically generated panels of them. any advice?",
        "Question_original_content_gpt_summary":"The user is attempting to set up metrics with the 'hidden' parameter set to true, but is still getting automatically generated panels of them.",
        "Question_preprocessed_content":"Title: parameter 'hidden' not working?; Content: hi, im new to w&b and was setting up my metrics like this i want graphs of and on my dashboard, but not graphs of or therefore, i set the function parameter hidden to true for these. it does not work, i still get automatically generated panels of them. any advice?",
        "Answer_original_content":"hi @tim123 thanks so much for spotting and reporting this issue. we were able to reproduce it on our end, and i have reported it to our engineering team. i also linked this discussion with the internal ticket, so that we can reach out to you here if there are any further updates. in the meantime, these plots can be only manually deleted from the ui.",
        "Answer_original_content_gpt_summary":"There is an issue with the 'hidden' parameter not working properly when setting up metrics. The engineering team has been notified and is working on a solution. In the meantime, the only solution is to manually delete the automatically generated panels from the UI.",
        "Answer_preprocessed_content":"hi thanks so much for spotting and reporting this issue. we were able to reproduce it on our end, and i have reported it to our engineering team. i also linked this discussion with the internal ticket, so that we can reach out to you here if there are any further updates. in the meantime, these plots can be only manually deleted from the ui."
    },
    {
        "Question_id":null,
        "Question_title":"ClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.",
        "Question_body":"Hi Dears,\n\nI am building ML model using DeepAR Algorithm.\n\nI faced this error while i reached to this point : Error :\n\nClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.\n\nCode: from sagemaker.tuner import ( IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner, ) from sagemaker import image_uris\n\ncontainer = image_uris.retrieve(region= 'af-south-1', framework=\"forecasting-deepar\")\n\ndeepar = sagemaker.estimator.Estimator( container, role, instance_count=1, instance_type=\"ml.m5.2xlarge\", use_spot_instances=True, # use spot instances max_run=1800, # max training time in seconds max_wait=1800, # seconds to wait for spot instance output_path=\"s3:\/\/{}\/{}\".format(bucket, output_path), sagemaker_session=sess, ) freq = \"D\" context_length = 300\n\ndeepar.set_hyperparameters( time_freq=freq, context_length=str(context_length), prediction_length=str(prediction_length) )\n\nCan you please help in solving the error? I have to do that in af-south-1 region.\n\nThanks Basem\n\nhyperparameter_ranges = { \"mini_batch_size\": IntegerParameter(100, 400), \"epochs\": IntegerParameter(200, 400), \"num_cells\": IntegerParameter(30, 100), \"likelihood\": CategoricalParameter([\"negative-binomial\", \"student-T\"]), \"learning_rate\": ContinuousParameter(0.0001, 0.1), }\n\nobjective_metric_name = \"test:RMSE\"\n\ntuner = HyperparameterTuner( deepar, objective_metric_name, hyperparameter_ranges, max_jobs=10, strategy=\"Bayesian\", objective_type=\"Minimize\", max_parallel_jobs=10, early_stopping_type=\"Auto\", )\n\ns3_input_train = sagemaker.inputs.TrainingInput( s3_data=\"s3:\/\/{}\/{}\/train\/\".format(bucket, prefix), content_type=\"json\" ) s3_input_test = sagemaker.inputs.TrainingInput( s3_data=\"s3:\/\/{}\/{}\/test\/\".format(bucket, prefix), content_type=\"json\" )\n\ntuner.fit({\"train\": s3_input_train, \"test\": s3_input_test}, include_cls_metadata=False) tuner.wait()",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1652655830031,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":135.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkAwy2tG8QreIWmLTGIUAqg\/client-error-an-error-occurred-unknown-operation-exception-when-calling-the-create-hyper-parameter-tuning-job-operation-the-requested-operation-is-not-supported-in-the-called-region",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-20T16:09:16.983Z",
                "Answer_score":0,
                "Answer_body":"The error message indicates that CreateHyperParameterTuningJob operation is not supported in the region you're currently using. If possible, try the notebook in a region that supports HPO jobs.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: clienterror: an error occurred (unknownoperationexception) when calling the createhyperparametertuningjob operation: the requested operation is not supported in the called region.; Content: hi dears, i am building ml model using deepar algorithm. i faced this error while i reached to this point : error : clienterror: an error occurred (unknownoperationexception) when calling the createhyperparametertuningjob operation: the requested operation is not supported in the called region. code: from .tuner import ( integerparameter, categoricalparameter, continuousparameter, hyperparametertuner, ) from import image_uris container = image_uris.retrieve(region= 'af-south-1', framework=\"forecasting-deepar\") deepar = .estimator.estimator( container, role, instance_count=1, instance_type=\"ml.m5.2xlarge\", use_spot_instances=true, # use spot instances max_run=1800, # max training time in seconds max_wait=1800, # seconds to wait for spot instance output_path=\"s3:\/\/{}\/{}\".format(bucket, output_path), _session=sess, ) freq = \"d\" context_length = 300 deepar.set_hyperparameters( time_freq=freq, context_length=str(context_length), prediction_length=str(prediction_length) ) can you please help in solving the error? i have to do that in af-south-1 region. thanks basem hyperparameter_ranges = { \"mini_batch_size\": integerparameter(100, 400), \"epochs\": integerparameter(200, 400), \"num_cells\": integerparameter(30, 100), \"likelihood\": categoricalparameter([\"negative-binomial\", \"student-t\"]), \"learning_rate\": continuousparameter(0.0001, 0.1), } objective_metric_name = \"test:rmse\" tuner = hyperparametertuner( deepar, objective_metric_name, hyperparameter_ranges, max_jobs=10, strategy=\"bayesian\", objective_type=\"minimize\", max_parallel_jobs=10, early_stopping_type=\"auto\", ) s3_input_train = .inputs.traininginput( s3_data=\"s3:\/\/{}\/{}\/train\/\".format(bucket, prefix), content_type=\"json\" ) s3_input_test = .inputs.traininginput( s3_data=\"s3:\/\/{}\/{}\/test\/\".format(bucket, prefix), content_type=\"json\" ) tuner.fit({\"train\": s3_input_train, \"test\": s3_input_test}, include_cls_metadata=false) tuner.wait()",
        "Question_original_content_gpt_summary":"The user encountered a clienterror when calling the createhyperparametertuningjob operation in the af-south-1 region while building an ML model using the DeepAR algorithm.",
        "Question_preprocessed_content":"Title: clienterror an error occurred when calling the createhyperparametertuningjob operation the requested operation is not supported in the called region.; Content: hi dears, i am building ml model using deepar algorithm. i faced this error while i reached to this point error clienterror an error occurred when calling the createhyperparametertuningjob operation the requested operation is not supported in the called region. code from import from import container deepar container, role, use spot instances max training time in seconds seconds to wait for spot instance freq d can you please help in solving the error? i have to do that in region. thanks basem test rmse tuner hyperparametertuner prefix , prefix , test",
        "Answer_original_content":"the error message indicates that createhyperparametertuningjob operation is not supported in the region you're currently using. if possible, try the notebook in a region that supports hpo jobs.",
        "Answer_original_content_gpt_summary":"Summary: The solution to the client error encountered while building an ML model using the DeepAR algorithm is to try the notebook in a region that supports hyperparameter tuning jobs as the error message indicates that the createhyperparametertuningjob operation is not supported in the current region.",
        "Answer_preprocessed_content":"the error message indicates that createhyperparametertuningjob operation is not supported in the region you're currently using. if possible, try the notebook in a region that supports hpo jobs."
    },
    {
        "Question_id":56851463.0,
        "Question_title":"How do I specify mlflow MLproject with zero parameters?",
        "Question_body":"<p>I tried to create MLproject with zero parameters as:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    parameters:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>when I get an error:<\/p>\n\n<pre><code>  Traceback (most recent call last):\n File \"\/home\/ubuntu\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/cli.py\", line 137, in run\n    run_id=run_id,\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/__init__.py\", line 85, in _run\n    project = _project_spec.load_project(work_dir)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 40, in load_project\n    entry_points[name] = EntryPoint(name, parameters, command)\n  File \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/mlflow\/projects\/_project_spec.py\", line 87, in __init__\n    self.parameters = {k: Parameter(k, v) for (k, v) in parameters.items()}\nAttributeError: 'NoneType' object has no attribute 'items'\n<\/code><\/pre>\n\n<p>Am I missing something or mlflow does not allow project with  zero parameters?<\/p>\n\n<p>I have also posted this at my public repo of: <a href=\"https:\/\/github.com\/sameermahajan\/mlflow-try\" rel=\"nofollow noreferrer\">https:\/\/github.com\/sameermahajan\/mlflow-try<\/a> if someone would like to try out:<\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/sameermahajan\/mlflow-try.git\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1562066976100,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":353.0,
        "Owner_creation_time":1384343462316,
        "Owner_last_access_time":1663916912328,
        "Owner_reputation":478.0,
        "Owner_up_votes":65.0,
        "Owner_down_votes":4.0,
        "Owner_views":118.0,
        "Answer_body":"<p>For this, you completely drop the 'parameters' section as below:<\/p>\n\n<pre><code>name: test\n\nconda_env: conda.yaml\n\nentry_points:\n  main:\n    command: \"python test.py\"\n<\/code><\/pre>\n\n<p>(I thought I had tried it earlier but I was trying too many different ways to may be miss out on this one)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1562240543612,
        "Answer_score":0.0,
        "Owner_location":"Pune, Maharashtra, India",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56851463",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i specify mlproject with zero parameters?; Content: i tried to create mlproject with zero parameters as: name: test conda_env: conda.yaml entry_points: main: parameters: command: \"python test.py\" when i get an error: traceback (most recent call last): file \"\/home\/ubuntu\/.local\/bin\/\", line 11, in <module> sys.exit(cli()) file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 764, in __call__ return self.main(*args, **kwargs) file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 717, in main rv = self.invoke(ctx) file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 1137, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 956, in invoke return ctx.invoke(self.callback, **ctx.params) file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/click\/core.py\", line 555, in invoke return callback(*args, **kwargs) file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/\/cli.py\", line 137, in run run_id=run_id, file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/\/projects\/__init__.py\", line 230, in run use_conda=use_conda, storage_dir=storage_dir, synchronous=synchronous, run_id=run_id) file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/\/projects\/__init__.py\", line 85, in _run project = _project_spec.load_project(work_dir) file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/\/projects\/_project_spec.py\", line 40, in load_project entry_points[name] = entrypoint(name, parameters, command) file \"\/home\/ubuntu\/.local\/lib\/python2.7\/site-packages\/\/projects\/_project_spec.py\", line 87, in __init__ self.parameters = {k: parameter(k, v) for (k, v) in parameters.items()} attributeerror: 'nonetype' object has no attribute 'items' am i missing something or does not allow project with zero parameters? i have also posted this at my public repo of: https:\/\/github.com\/sameermahajan\/-try if someone would like to try out: run https:\/\/github.com\/sameermahajan\/-try.git",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to create an MLProject with zero parameters.",
        "Question_preprocessed_content":"Title: how do i specify mlproject with zero parameters?; Content: i tried to create mlproject with zero parameters as when i get an error am i missing something or does not allow project with zero parameters? i have also posted this at my public repo of if someone would like to try out",
        "Answer_original_content":"for this, you completely drop the 'parameters' section as below: name: test conda_env: conda.yaml entry_points: main: command: \"python test.py\" (i thought i had tried it earlier but i was trying too many different ways to may be miss out on this one)",
        "Answer_original_content_gpt_summary":"Solution: To create an MLProject with zero parameters, drop the 'parameters' section completely and only include the 'name', 'conda_env', and 'entry_points' sections. The 'entry_points' section should include the command to run the desired Python script.",
        "Answer_preprocessed_content":"for this, you completely drop the 'parameters' section as below i thought i had tried it earlier but i was trying too many different ways to may be miss out on this one"
    },
    {
        "Question_id":null,
        "Question_title":"Artificial Intelligence - list of APIs",
        "Question_body":"Good day.Is it possible to list the Google APIs in the Artificial Intelligence, especially in the Voice recognitions, comparison, analytics etc? Thank you.",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1627065000000,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":485.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Artificial-Intelligence-list-of-APIs\/td-p\/164618\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-25T12:52:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Let's try this link for a start ...\nhttps:\/\/cloud.google.com\/products\/ai\n\nThis appears to list all the various GCP products related to AI and, if one were to delve into each product, one would find the corresponding APIs."
            },
            {
                "Answer_creation_time":"2021-08-02T03:27:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"And take a look at the \"Google Cloud in\u00a0 Words\" cheat sheet\n\nhttps:\/\/github.com\/gregsramblings\/google-cloud-4-words\/blob\/master\/Brochure.pdf"
            },
            {
                "Answer_creation_time":"2021-08-22T20:43:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"@pascal_reddig\u00a0wrote: mcdvoice\n\n\nAnd take a look at the \"Google Cloud in\u00a0 Words\" cheat sheet\n\nhttps:\/\/github.com\/gregsramblings\/google-cloud-4-words\/blob\/master\/Brochure.pdf\n\nCarry on, don\u2019t stop posting like this"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: artificial intelligence - list of apis; Content: good day.is it possible to list the google apis in the artificial intelligence, especially in the voice recognitions, comparison, analytics etc? thank you.",
        "Question_original_content_gpt_summary":"The user is seeking a list of Google APIs related to Artificial Intelligence, particularly in the areas of voice recognition, comparison, and analytics.",
        "Question_preprocessed_content":"Title: artificial intelligence list of apis; Content: good it possible to list the google apis in the artificial intelligence, especially in the voice recognitions, comparison, analytics etc? thank you.",
        "Answer_original_content":"let's try this link for a start ... https:\/\/cloud.google.com\/products\/ai this appears to list all the various gcp products related to ai and, if one were to delve into each product, one would find the corresponding apis. and take a look at the \"google cloud in words\" cheat sheet https:\/\/github.com\/gregsramblings\/google-cloud-4-words\/blob\/master\/brochure.pdf @pascal_reddigwrote: mcdvoice and take a look at the \"google cloud in words\" cheat sheet https:\/\/github.com\/gregsramblings\/google-cloud-4-words\/blob\/master\/brochure.pdf carry on, dont stop posting like this",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's question include visiting the link provided (https:\/\/cloud.google.com\/products\/ai) which lists various Google Cloud Platform (GCP) products related to AI, and exploring each product to find the corresponding APIs. Another solution is to refer to the \"Google Cloud in Words\" cheat sheet (https:\/\/github.com\/gregsramblings\/google-cloud-4-words\/blob\/master\/brochure.pdf) for a quick overview of GCP products and their corresponding APIs.",
        "Answer_preprocessed_content":"let's try this link for a start this appears to list all the various gcp products related to ai and, if one were to delve into each product, one would find the corresponding apis. and take a look at the google cloud in words cheat sheet mcdvoice and take a look at the google cloud in words cheat sheet carry on, dont stop posting like this"
    },
    {
        "Question_id":57724414.0,
        "Question_title":"How to make the inputs and model have the same shape (RLlib Ray Sagemaker reinforcement learning)",
        "Question_body":"<p>I have a mismatch in shapes between inputs and the model of my reinforcement learning project.<\/p>\n\n<p>I have been closely following the AWS examples, specifically the cartpole example. However I have built my own custom environment. What I am struggling to understand is how to change my environment so that it is able to work with the prebuilt Ray RLEstimator.<\/p>\n\n<p>Here is the code for the environment:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\n\nimport numpy as np\n\n# from float_space import FloatSpace\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        self.observation_space = Box(np.array(0.0),np.array(1000))\n#         self.observation_space = FloatSpace(price)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n        next_state = self.price\n#         next_state = self.observation_space.sample()\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n\n#         self.observation_space.value = np.random.rand()\n#         return self.observation_space.sample()\n        self.price = np.random.rand()\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        self.price += PRICE_ADJUSTMENT[action]\n\n    def _get_reward(self,price):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    env = gym.make('ArrivalSim-v0')\n\n    env.reset()\n    for _ in range(20):\n        test = env.action_space.sample()\n        print(test)\n        print(env.observation_space)\n        env.step(test)  # take a random action\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n\n<\/code><\/pre>\n\n<p>Here is the training script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nfrom ray.tune.registry import register_env\nfrom gym.envs.registration import register\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\n\n\nclass MyLauncher(SageMakerRayLauncher):\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n\n            }\n          }\n        }\n\nif __name__ == \"__main__\":\n    MyLauncher().train_main()\n<\/code><\/pre>\n\n<p>Here is the code I run in Jupyter:<\/p>\n\n<pre><code>metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\nenvironment = env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\n\nestimator = RLEstimator(entry_point=\"train.py\",\n                        source_dir='.',\n                        toolkit=RLToolkit.RAY,\n                        toolkit_version='0.6.5',\n                        framework=RLFramework.TENSORFLOW,\n                        dependencies=[\"sagemaker_rl\"],\n#                         image_name='price-response-ray-cpu',\n                        role=role,\n#                         train_instance_type=\"ml.c5.2xlarge\",\n                        train_instance_type='local',\n                        train_instance_count=1,\n#                         output_path=s3_output_path,\n#                         base_job_name=job_name_prefix,\n                        metric_definitions=metric_definitions\n#                         hyperparameters={\n                          # Attention scientists!  You can override any Ray algorithm parameter here:\n                          #\"rl.training.config.horizon\": 5000,\n                          #\"rl.training.config.num_sgd_iter\": 10,\n                        #}\n                    )\n\nestimator.fit(wait=True)\njob_name = estimator.latest_training_job.job_name\nprint(\"Training job: %s\" % job_name)\n<\/code><\/pre>\n\n<p>The error message I have been receiving has been the following:<\/p>\n\n<pre><code>algo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.1\/4.1 GB\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 2\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.4\/4.1 GB\nalgo-1-dxwxx_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-dxwxx_1  | Number of trials: 1 ({'RUNNING': 1})\nalgo-1-dxwxx_1  | RUNNING trials:\nalgo-1-dxwxx_1  |  - PPO_ArrivalSim-v0_0:   RUNNING\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,030  WARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,063   ERROR trial_runner.py:460 -- Error processing event.\nalgo-1-dxwxx_1  | Traceback (most recent call last):\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trial_runner.py\", line 409, in _process_trial\nalgo-1-dxwxx_1  |     result = self.trial_executor.fetch_result(trial)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/ray_trial_executor.py\", line 314, in fetch_result\nalgo-1-dxwxx_1  |     result = ray.get(trial_future[0])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/worker.py\", line 2316, in get\nalgo-1-dxwxx_1  |     raise value\nalgo-1-dxwxx_1  | ray.exceptions.RayTaskError: ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 83, in __init__\nalgo-1-dxwxx_1  |     restored, num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 135, in _build_layers_v2\nalgo-1-dxwxx_1  |     raise NotImplementedError\nalgo-1-dxwxx_1  | NotImplementedError\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | During handling of the above exception, another exception occurred:\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 276, in __init__\nalgo-1-dxwxx_1  |     Trainable.__init__(self, config, logger_creator)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trainable.py\", line 88, in __init__\nalgo-1-dxwxx_1  |     self._setup(copy.deepcopy(self.config))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 373, in _setup\nalgo-1-dxwxx_1  |     self._init()\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo.py\", line 77, in _init\nalgo-1-dxwxx_1  |     self.env_creator, self._policy_graph)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 506, in make_local_evaluator\nalgo-1-dxwxx_1  |     extra_config or {}))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 714, in _make_evaluator\nalgo-1-dxwxx_1  |     async_remote_worker_envs=config[\"async_remote_worker_envs\"])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 288, in __init__\nalgo-1-dxwxx_1  |     self._build_policy_map(policy_dict, policy_config)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 661, in _build_policy_map\nalgo-1-dxwxx_1  |     policy_map[name] = cls(obs_space, act_space, merged_conf)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo_policy_graph.py\", line 176, in __init__\nalgo-1-dxwxx_1  |     seq_lens=existing_seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 215, in get_model\nalgo-1-dxwxx_1  |     seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 255, in _get_model\nalgo-1-dxwxx_1  |     num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 86, in __init__\nalgo-1-dxwxx_1  |     input_dict[\"obs\"], num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/fcnet.py\", line 37, in _build_layers\nalgo-1-dxwxx_1  |     scope=label)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/framework\/python\/ops\/arg_scope.py\", line 182, in func_with_args\nalgo-1-dxwxx_1  |     return func(*args, **current_args)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/layers\/python\/layers\/layers.py\", line 1854, in fully_connected\nalgo-1-dxwxx_1  |     outputs = layer.apply(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 817, in apply\nalgo-1-dxwxx_1  |     return self.__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/layers\/base.py\", line 374, in __call__\nalgo-1-dxwxx_1  |     outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 730, in __call__\nalgo-1-dxwxx_1  |     self._assert_input_compatibility(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 1493, in _assert_input_compatibility\nalgo-1-dxwxx_1  |     str(x.shape.as_list()))\nalgo-1-dxwxx_1  | ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,064   INFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_ArrivalSim-v0_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,076   INFO trial_runner.py:497 -- Attempting to recover trial state from last checkpoint.\nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,041  INFO policy_evaluator.py:278 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n<\/code><\/pre>\n\n<p>I am not sure how to change the input the environment gives to the model or the models setup itself. It seems the documentations are quite obscure. I have a hunch that problem lies with the observation and action spaces<\/p>\n\n<p>Here is the reference to the original aws project example:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1567158570927,
        "Question_favorite_count":1.0,
        "Question_score":2.0,
        "Question_view_count":1492.0,
        "Owner_creation_time":1565097950652,
        "Owner_last_access_time":1663855024663,
        "Owner_reputation":95.0,
        "Owner_up_votes":8.0,
        "Owner_down_votes":0.0,
        "Owner_views":55.0,
        "Answer_body":"<p><strong>Possible reason:<\/strong><\/p>\n\n<p>The error message:<\/p>\n\n<p><code>ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]<\/code><\/p>\n\n<p>Your original environment obs space is <code>self.observation_space = Box(np.array(0.0),np.array(1000))<\/code>.<\/p>\n\n<p>Displaying the shape of your environment obs space gives:<\/p>\n\n<p><code>print(Box(np.array(0.0), np.array(1000), dtype=np.float32).shape)<\/code> = <code>()<\/code><\/p>\n\n<p>This could be indicated by <code>Full shape received: [None]<\/code> in the error message.<\/p>\n\n<p>If you pass the shape <code>(1,1)<\/code> into <code>np.zeros<\/code>, you get the expected  <code>min_ndim=2<\/code>:<\/p>\n\n<p><code>x = np.zeros((1, 1))\nprint(x)\n[[0.]]\nprint(x.ndim)\n2<\/code><\/p>\n\n<p><strong>Suggested solution:<\/strong><\/p>\n\n<p>I assume that you want your environment obs space to range from 0.0 to 1000.0 as indicated by the <code>self.price = np.random.rand()<\/code> in your <code>reset<\/code> function.<\/p>\n\n<p>Try using the following for your environment obs space:<\/p>\n\n<p><code>self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)<\/code><\/p>\n\n<p>I hope that by setting the <code>Box<\/code> with an explicit <code>shape<\/code> helps.<\/p>\n\n<p><strike>\n<strong>EDIT (20190903):<\/strong><\/p>\n\n<p>I have modified your training script. This modification includes new imports, custom model class, model registration &amp; addition of registered custom model to config. For readability, only sections added are shown below. The entire modified training script is available in this <a href=\"https:\/\/gist.github.com\/ChuaCheowHuan\/ddf70654bd928d70e5415c947d4d43f3\" rel=\"nofollow noreferrer\">gist<\/a>. Please run with the proposed obs space as describe above.<\/p>\n\n<p>New additional imports:<\/p>\n\n<pre><code># new imports\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\nfrom ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n\nfrom ray.rllib.utils import try_import_tf\nfrom ray.tune import grid_search\n\ntf = try_import_tf()\n# end new imports\n<\/code><\/pre>\n\n<p>Custom model class:<\/p>\n\n<pre><code># Custom model class (fcnet)\nclass CustomModel(TFModelV2):\n    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n\n    def __init__(self, obs_space, action_space, num_outputs, model_config,\n                 name):\n        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n                                          model_config, name)\n        self.model = FullyConnectedNetwork(obs_space, action_space,\n                                           num_outputs, model_config, name)\n        self.register_variables(self.model.variables())\n\n    def forward(self, input_dict, state, seq_lens):\n        return self.model.forward(input_dict, state, seq_lens)\n\n    def value_function(self):\n        return self.model.value_function()\n<\/code><\/pre>\n\n<p>Registered &amp; add custom model:<\/p>\n\n<pre><code>    def get_experiment_config(self):\n\n\n        # Register custom model\n        ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n\n\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n\n\n              \"model\": {\"custom_model\": \"my_model\"}, # Add registered custom model\n\n\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n            }\n          }\n        }\n<\/code><\/pre>\n\n<p><\/strike><\/p>\n\n<p><strong>EDIT 2 (20190910):<\/strong><\/p>\n\n<p>To show that it works, truncated output from Sagemaker (Jupyter notebook instance):<\/p>\n\n<pre><code>.\n.\n.\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 10.103020961393266 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 9.663759180463124 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.189261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.18926178] 5.67785342790426 False {}\nalgo-1-y2ayw_1  | price b = 0.189261780930142\nalgo-1-y2ayw_1  | price a = -0.06073821906985799\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [-0.06073822] 0 True {}\nalgo-1-y2ayw_1  | Result for PPO_ArrivalSim-v0_0:\nalgo-1-y2ayw_1  |   date: 2019-09-10_11-51-13\nalgo-1-y2ayw_1  |   done: true\nalgo-1-y2ayw_1  |   episode_len_mean: 126.72727272727273\nalgo-1-y2ayw_1  |   episode_reward_max: 15772.677709596366\nalgo-1-y2ayw_1  |   episode_reward_mean: 2964.4609668691965\nalgo-1-y2ayw_1  |   episode_reward_min: 0.0\nalgo-1-y2ayw_1  |   episodes: 5\nalgo-1-y2ayw_1  |   experiment_id: 5d3b9f2988854a0db164a2e5e9a7550f\nalgo-1-y2ayw_1  |   hostname: 2dae585dcc65\nalgo-1-y2ayw_1  |   info:\nalgo-1-y2ayw_1  |     cur_lr: 4.999999873689376e-05\nalgo-1-y2ayw_1  |     entropy: 1.0670874118804932\nalgo-1-y2ayw_1  |     grad_time_ms: 1195.066\nalgo-1-y2ayw_1  |     kl: 3.391784191131592\nalgo-1-y2ayw_1  |     load_time_ms: 44.725\nalgo-1-y2ayw_1  |     num_steps_sampled: 463\nalgo-1-y2ayw_1  |     num_steps_trained: 463\nalgo-1-y2ayw_1  |     policy_loss: -0.05383850634098053\nalgo-1-y2ayw_1  |     sample_time_ms: 621.282\nalgo-1-y2ayw_1  |     total_loss: 2194493.5\nalgo-1-y2ayw_1  |     update_time_ms: 145.352\nalgo-1-y2ayw_1  |     vf_explained_var: -5.519390106201172e-05\nalgo-1-y2ayw_1  |     vf_loss: 2194492.5\nalgo-1-y2ayw_1  |   iterations_since_restore: 2\nalgo-1-y2ayw_1  |   node_ip: 172.18.0.2\nalgo-1-y2ayw_1  |   pid: 77\nalgo-1-y2ayw_1  |   policy_reward_mean: {}\nalgo-1-y2ayw_1  |   time_since_restore: 4.55129861831665\nalgo-1-y2ayw_1  |   time_this_iter_s: 1.3484764099121094\nalgo-1-y2ayw_1  |   time_total_s: 4.55129861831665\nalgo-1-y2ayw_1  |   timestamp: 1568116273\nalgo-1-y2ayw_1  |   timesteps_since_restore: 463\nalgo-1-y2ayw_1  |   timesteps_this_iter: 234\nalgo-1-y2ayw_1  |   timesteps_total: 463\nalgo-1-y2ayw_1  |   training_iteration: 2\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000781a7b5b94a203683f8f789e593abbb1.\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000d3507bc6b41ee1c9fc36292eeae69557.\nalgo-1-y2ayw_1  | == Status ==\nalgo-1-y2ayw_1  | Using FIFO scheduling algorithm.\nalgo-1-y2ayw_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-y2ayw_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-y2ayw_1  | TERMINATED trials:\nalgo-1-y2ayw_1  |  - PPO_ArrivalSim-v0_0:   TERMINATED [pid=77], 4 s, 2 iter, 463 ts, 2.96e+03 rew\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | Saved model configuration.\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.extra_data as \/opt\/ml\/model\/checkpoint.extra_data\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.tune_metadata as \/opt\/ml\/model\/checkpoint.tune_metadata\nalgo-1-y2ayw_1  | Created LogSyncer for \/root\/ray_results\/PPO_ArrivalSim-v0_2019-09-10_11-51-13xdn_5i34 -&gt; None\nalgo-1-y2ayw_1  | 2019-09-10 11:51:13.941718: I tensorflow\/core\/common_runtime\/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | LocalMultiGPUOptimizer devices ['\/cpu:0']\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to save.\nalgo-1-y2ayw_1  | No assets to save.\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to write.\nalgo-1-y2ayw_1  | No assets to write.\nalgo-1-y2ayw_1  | INFO:tensorflow:SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | Saved TensorFlow serving model!\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000f352d985b807ca399460941fe2264899.\n\nalgo-1-y2ayw_1  | 2019-09-10 11:51:20,075 sagemaker-containers INFO\n\n Reporting training SUCCESS\n\ntmpwwb4b358_algo-1-y2ayw_1 exited with code 0\n\nAborting on container exit...\nFailed to delete: \/tmp\/tmpwwb4b358\/algo-1-y2ayw Please remove it manually.\n\n===== Job Complete =====\n<\/code><\/pre>\n\n<p>This time I make edits in all 3 files. Your environment, training script &amp; the Jupyter notebook but it turns out that there isn't a need to define custom models for your custom environment. However, that remains viable. And you're right, the main cause of the issue is still in the obs space.<\/p>\n\n<p>I set <code>self.price<\/code> to be a 1D numpy array to make it talk better with Ray RLlib. The creation of the custom environment in the training script was done in a simpler way as shown below. As for the notebook, I used version 0.5.3 instead of 0.6.5 for toolkit_version &amp; the training is done in local mode (in the docker container on the Sagemaker Jupyter notebook instance, still on AWS) with CPU only. However, it will also work with any ML instance (e.g ml.m4.xlarge) with GPU.<\/p>\n\n<p>The entire package along with all dependencies is in <a href=\"https:\/\/github.com\/ChuaCheowHuan\/sagemaker_Ray_RLlib_custom_env\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The edited env:<\/p>\n\n<pre><code># new\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n# end new\n\n\nfrom enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\nimport numpy as np\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        # original obs space:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space initially suggested:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space suggested in this edit:\n        self.observation_space = spaces.Box(np.array([0.0]), np.array([1000.0]), dtype=np.float32)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n\n        next_state = self.price\n        print('(self.price).shape =', (self.price).shape)\n        #next_state = self.observation_space.sample()\n\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n        #self.observation_space.value = np.random.rand()\n        #return self.observation_space.sample()\n\n        self.price = np.random.rand(1)\n\n        print('reset -&gt; (self.price).shape = ', (self.price).shape)\n\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        #print('price b =', self.price)\n        print('price b =', self.price[0])\n        #print('price b =', self.price[[0]])\n        #self.price += PRICE_ADJUSTMENT[action]\n        self.price[0] += PRICE_ADJUSTMENT[action]\n        #self.price[[0]] += PRICE_ADJUSTMENT[action]\n        #print('price a =', self.price)\n        print('price a =', self.price[0])\n        #print('price a =', self.price[[0]])\n\n    #def _get_reward(self, price):\n    def _get_reward(self):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        #self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        #return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    \"\"\"\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40.0}\n    )\n    env = gym.make('ArrivalSim-v0')\n    \"\"\"\n    env = ArrivalSim(30.0)\n\n    val = env.reset()\n    print('val.shape = ', val.shape)\n\n    for _ in range(5):\n        print('env.observation_space =', env.observation_space)\n        act = env.action_space.sample()\n        print('\\nact =', act)\n        next_state, reward, done, _ = env.step(act)  # take a random action\n        print('next_state = ', next_state)\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n<\/code><\/pre>\n\n<p>The edited training script:<\/p>\n\n<pre><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nimport ray.rllib.agents.a3c as a3c\nimport ray.rllib.agents.ppo as ppo\nfrom ray.tune.registry import register_env\nfrom mod_op_env import ArrivalSim\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\"\"\"\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\"\"\"\ndef create_environment(env_config):\n    price = 30.0\n    # This import must happen inside the method so that worker processes import this code\n    from mod_op_env import ArrivalSim\n    return ArrivalSim(price)\n\n\nclass MyLauncher(SageMakerRayLauncher):\n    def __init__(self):        \n        super(MyLauncher, self).__init__()\n        self.num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))\n        self.hosts_info = json.loads(os.environ.get(\"SM_RESOURCE_CONFIG\"))[\"hosts\"]\n        self.num_total_gpus = self.num_gpus * len(self.hosts_info)\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"training_iteration\": 3,\n            },\n\n            \"local_dir\": \"\/opt\/ml\/model\/\",\n            \"checkpoint_freq\" : 3,\n\n            \"config\": {                                \n              #\"num_workers\": max(self.num_total_gpus-1, 1),\n              \"num_workers\": max(self.num_cpus-1, 1),\n              #\"use_gpu_for_workers\": False,\n              \"train_batch_size\": 128, #5,\n              \"sample_batch_size\": 32, #1,\n              \"gpu_fraction\": 0.3,\n              \"optimizer\": {\n                \"grads_per_step\": 10\n              },\n            },\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 1), \"extra_cpu\": 0},\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 0),\n            #                    \"extra_cpu\": max(self.num_cpus-1, 1)},\n            \"trial_resources\": {\"cpu\": 1,\n                                \"extra_cpu\": max(self.num_cpus-1, 1)},              \n          }\n        }\n\nif __name__ == \"__main__\":\n    os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n    os.environ[\"LANG\"] = \"C.UTF-8\"\n    os.environ[\"RAY_USE_XRAY\"] = \"1\"\n    print(ppo.DEFAULT_CONFIG)\n    MyLauncher().train_main()\n\n<\/code><\/pre>\n\n<p>The notebook code:<\/p>\n\n<pre><code>!\/bin\/bash .\/setup.sh\n\nfrom time import gmtime, strftime\nimport sagemaker \nrole = sagemaker.get_execution_role()\n\nsage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3:\/\/{}\/'.format(s3_bucket)\nprint(\"S3 bucket path: {}\".format(s3_output_path))\n\njob_name_prefix = 'ArrivalSim'\n\nfrom sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n\nestimator = RLEstimator(entry_point=\"mod_op_train.py\", # Our launcher code\n                        source_dir='src', # Directory where the supporting files are at. All of this will be\n                                          # copied into the container.\n                        dependencies=[\"common\/sagemaker_rl\"], # some other utils files.\n                        toolkit=RLToolkit.RAY, # We want to run using the Ray toolkit against the ray container image.\n                        framework=RLFramework.TENSORFLOW, # The code is in tensorflow backend.\n                        toolkit_version='0.5.3', # Toolkit version. This will also choose an apporpriate tf version.                                               \n                        #toolkit_version='0.6.5', # Toolkit version. This will also choose an apporpriate tf version.                        \n                        role=role, # The IAM role that we created at the begining.\n                        #train_instance_type=\"ml.m4.xlarge\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_type=\"local\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_count=1, # Single instance will also work, but running distributed makes things \n                                                # fast, particularly in the case of multiple rollout training.\n                        output_path=s3_output_path, # The path where we can expect our trained model.\n                        base_job_name=job_name_prefix, # This is the name we setup above to be to track our job.\n                        hyperparameters = {      # Some hyperparameters for Ray toolkit to operate.\n                          \"s3_bucket\": s3_bucket,\n                          \"rl.training.stop.training_iteration\": 2, # Number of iterations.\n                          \"rl.training.checkpoint_freq\": 2,\n                        },\n                        #metric_definitions=metric_definitions, # This will bring all the logs out into the notebook.\n                    )\n\nestimator.fit()\n<\/code><\/pre>",
        "Answer_comment_count":19.0,
        "Answer_creation_time":1567460466780,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1567160893503,
        "Answer_last_edit_time":1568837974432,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57724414",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to make the inputs and model have the same shape (rllib ray reinforcement learning); Content: i have a mismatch in shapes between inputs and the model of my reinforcement learning project. i have been closely following the aws examples, specifically the cartpole example. however i have built my own custom environment. what i am struggling to understand is how to change my environment so that it is able to work with the prebuilt ray rlestimator. here is the code for the environment: from enum import enum import math import gym from gym import error, spaces, utils, wrappers from gym.utils import seeding from gym.envs.registration import register from gym.spaces import discrete, box import numpy as np # from float_space import floatspace def sigmoid_price_fun(x, maxcust, gamma): return maxcust \/ (1 + math.exp(gamma * max(0, x))) class actions(enum): decrease_price = 0 increase_price = 1 hold = 2 price_adjustment = { actions.decrease_price: -0.25, actions.increase_price: 0.25, actions.hold: 0 } class arrivalsim(gym.env): \"\"\" simple environment for price optimising rl learner. \"\"\" def __init__(self, price): \"\"\" parameters ---------- price : float the initial price to use. \"\"\" super().__init__() self.price = price self.revenue = 0 self.action_space = discrete(3) # [0, 1, 2] #increase or decrease self.observation_space = box(np.array(0.0),np.array(1000)) # self.observation_space = floatspace(price) def step(self, action): \"\"\" enacts the specified action in the environment. returns the new price, reward, whether we're finished and an empty dict for compatibility with gym's interface. \"\"\" self._take_action(actions(action)) next_state = self.price # next_state = self.observation_space.sample() reward = self._get_reward() done = false if next_state < 0 or reward == 0: done = true print(next_state, reward, done, {}) return np.array(next_state), reward, done, {} def reset(self): \"\"\" resets the environment, selecting a random initial price. returns the price. \"\"\" # self.observation_space.value = np.random.rand() # return self.observation_space.sample() self.price = np.random.rand() return self.price def _take_action(self, action): # self.observation_space.value += price_adjustment[action] self.price += price_adjustment[action] def _get_reward(self,price): # price = self.observation_space.value # return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0) self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0) return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0) # def render(self, mode='human'): # super().render(mode) def testenv(): register( id='arrivalsim-v0', entry_point='env:arrivalsim', kwargs= {'price' : 40} ) env = gym.make('arrivalsim-v0') env.reset() for _ in range(20): test = env.action_space.sample() print(test) print(env.observation_space) env.step(test) # take a random action env.close() if __name__ =='__main__': testenv() here is the training script import json import os import gym import ray from ray.tune import run_experiments from ray.tune.registry import register_env from gym.envs.registration import register from _rl.ray_launcher import raylauncher def create_environment(env_config): import gym # from gym.spaces import space from gym.envs.registration import register # this import must happen inside the method so that worker processes import this code register( id='arrivalsim-v0', entry_point='env:arrivalsim', kwargs= {'price' : 40} ) return gym.make('arrivalsim-v0') class mylauncher(raylauncher): def register_env_creator(self): register_env(\"arrivalsim-v0\", create_environment) def get_experiment_config(self): return { \"training\": { \"env\": \"arrivalsim-v0\", \"run\": \"ppo\", \"stop\": { \"episode_reward_mean\": 5000, }, \"config\": { \"gamma\": 0.995, \"kl_coeff\": 1.0, \"num_sgd_iter\": 10, \"lr\": 0.0001, \"sgd_minibatch_size\": 32768, \"train_batch_size\": 320000, \"monitor\": false, # record videos. \"model\": { \"free_log_std\": false }, \"use_gae\": false, \"num_workers\": (self.num_cpus-1), \"num_gpus\": self.num_gpus, \"batch_mode\": \"complete_episodes\" } } } if __name__ == \"__main__\": mylauncher().train_main() here is the code i run in jupyter: metric_definitions = rlestimator.default_metric_definitions(rltoolkit.ray) environment = env = { '_requirements': 'requirements.txt', # path relative to `source_dir` below. } estimator = rlestimator(entry_point=\"train.py\", source_dir='.', toolkit=rltoolkit.ray, toolkit_version='0.6.5', framework=rlframework.tensorflow, dependencies=[\"_rl\"], # image_name='price-response-ray-cpu', role=role, # train_instance_type=\"ml.c5.2xlarge\", train_instance_type='local', train_instance_count=1, # output_path=s3_output_path, # base_job_name=job_name_prefix, metric_definitions=metric_definitions # hyperparameters={ # attention scientists! you can override any ray algorithm parameter here: #\"rl.training.config.horizon\": 5000, #\"rl.training.config.num_sgd_iter\": 10, #} ) estimator.fit(wait=true) job_name = estimator.latest_training_job.job_name print(\"training job: %s\" % job_name) the error message i have been receiving has been the following: algo-1-dxwxx_1 | == status == algo-1-dxwxx_1 | using fifo scheduling algorithm. algo-1-dxwxx_1 | resources requested: 0\/3 cpus, 0\/0 gpus algo-1-dxwxx_1 | memory usage on this node: 1.1\/4.1 gb algo-1-dxwxx_1 | algo-1-dxwxx_1 | == status == algo-1-dxwxx_1 | using fifo scheduling algorithm. algo-1-dxwxx_1 | resources requested: 2\/3 cpus, 0\/0 gpus algo-1-dxwxx_1 | memory usage on this node: 1.4\/4.1 gb algo-1-dxwxx_1 | result logdir: \/opt\/ml\/output\/intermediate\/training algo-1-dxwxx_1 | number of trials: 1 ({'running': 1}) algo-1-dxwxx_1 | running trials: algo-1-dxwxx_1 | - ppo_arrivalsim-v0_0: running algo-1-dxwxx_1 | algo-1-dxwxx_1 | (pid=72) 2019-08-30 09:35:13,030 warning ppo.py:172 -- fyi: by default, the value function will not share layers with the policy model ('vf_share_layers': false). algo-1-dxwxx_1 | 2019-08-30 09:35:13,063 error trial_runner.py:460 -- error processing event. algo-1-dxwxx_1 | traceback (most recent call last): algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trial_runner.py\", line 409, in _process_trial algo-1-dxwxx_1 | result = self.trial_executor.fetch_result(trial) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/ray_trial_executor.py\", line 314, in fetch_result algo-1-dxwxx_1 | result = ray.get(trial_future[0]) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/worker.py\", line 2316, in get algo-1-dxwxx_1 | raise value algo-1-dxwxx_1 | ray.exceptions.raytaskerror: ray_worker (pid=72, host=b9b15d495b68) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 83, in __init__ algo-1-dxwxx_1 | restored, num_outputs, options) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 135, in _build_layers_v2 algo-1-dxwxx_1 | raise notimplementederror algo-1-dxwxx_1 | notimplementederror algo-1-dxwxx_1 | algo-1-dxwxx_1 | during handling of the above exception, another exception occurred: algo-1-dxwxx_1 | algo-1-dxwxx_1 | ray_worker (pid=72, host=b9b15d495b68) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 276, in __init__ algo-1-dxwxx_1 | trainable.__init__(self, config, logger_creator) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trainable.py\", line 88, in __init__ algo-1-dxwxx_1 | self._setup(copy.deepcopy(self.config)) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 373, in _setup algo-1-dxwxx_1 | self._init() algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo.py\", line 77, in _init algo-1-dxwxx_1 | self.env_creator, self._policy_graph) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 506, in make_local_evaluator algo-1-dxwxx_1 | extra_config or {})) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 714, in _make_evaluator algo-1-dxwxx_1 | async_remote_worker_envs=config[\"async_remote_worker_envs\"]) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 288, in __init__ algo-1-dxwxx_1 | self._build_policy_map(policy_dict, policy_config) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 661, in _build_policy_map algo-1-dxwxx_1 | policy_map[name] = cls(obs_space, act_space, merged_conf) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo_policy_graph.py\", line 176, in __init__ algo-1-dxwxx_1 | seq_lens=existing_seq_lens) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 215, in get_model algo-1-dxwxx_1 | seq_lens) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 255, in _get_model algo-1-dxwxx_1 | num_outputs, options) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 86, in __init__ algo-1-dxwxx_1 | input_dict[\"obs\"], num_outputs, options) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/fcnet.py\", line 37, in _build_layers algo-1-dxwxx_1 | scope=label) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/framework\/python\/ops\/arg_scope.py\", line 182, in func_with_args algo-1-dxwxx_1 | return func(*args, **current_args) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/layers\/python\/layers\/layers.py\", line 1854, in fully_connected algo-1-dxwxx_1 | outputs = layer.apply(inputs) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 817, in apply algo-1-dxwxx_1 | return self.__call__(inputs, *args, **kwargs) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/layers\/base.py\", line 374, in __call__ algo-1-dxwxx_1 | outputs = super(layer, self).__call__(inputs, *args, **kwargs) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 730, in __call__ algo-1-dxwxx_1 | self._assert_input_compatibility(inputs) algo-1-dxwxx_1 | file \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 1493, in _assert_input_compatibility algo-1-dxwxx_1 | str(x.shape.as_list())) algo-1-dxwxx_1 | valueerror: input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. full shape received: [none] algo-1-dxwxx_1 | algo-1-dxwxx_1 | 2019-08-30 09:35:13,064 info ray_trial_executor.py:178 -- destroying actor for trial ppo_arrivalsim-v0_0. if your trainable is slow to initialize, consider setting reuse_actors=true to reduce actor creation overheads. algo-1-dxwxx_1 | 2019-08-30 09:35:13,076 info trial_runner.py:497 -- attempting to recover trial state from last checkpoint. algo-1-dxwxx_1 | (pid=72) 2019-08-30 09:35:13,041 info policy_evaluator.py:278 -- creating policy evaluation worker 0 on cpu (please ignore any cuda init errors) i am not sure how to change the input the environment gives to the model or the models setup itself. it seems the documentations are quite obscure. i have a hunch that problem lies with the observation and action spaces here is the reference to the original aws project example: https:\/\/github.com\/awslabs\/amazon--examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with mismatched shapes between the inputs and model of their reinforcement learning project, and is struggling to understand how to change their environment to work with the prebuilt Ray RLestimator.",
        "Question_preprocessed_content":"Title: how to make the inputs and model have the same shape ; Content: i have a mismatch in shapes between inputs and the model of my reinforcement learning project. i have been closely following the aws examples, specifically the cartpole example. however i have built my own custom environment. what i am struggling to understand is how to change my environment so that it is able to work with the prebuilt ray rlestimator. here is the code for the environment here is the training script here is the code i run in jupyter the error message i have been receiving has been the following i am not sure how to change the input the environment gives to the model or the models setup itself. it seems the documentations are quite obscure. i have a hunch that problem lies with the observation and action spaces here is the reference to the original aws project example",
        "Answer_original_content":"possible reason: the error message: valueerror: input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. full shape received: [none] your original environment obs space is self.observation_space = box(np.array(0.0),np.array(1000)). displaying the shape of your environment obs space gives: print(box(np.array(0.0), np.array(1000), dtype=np.float32).shape) = () this could be indicated by full shape received: [none] in the error message. if you pass the shape (1,1) into np.zeros, you get the expected min_ndim=2: x = np.zeros((1, 1)) print(x) [[0.]] print(x.ndim) 2 suggested solution: i assume that you want your environment obs space to range from 0.0 to 1000.0 as indicated by the self.price = np.random.rand() in your reset function. try using the following for your environment obs space: self.observation_space = box(0.0, 1000.0, shape=(1,1), dtype=np.float32) i hope that by setting the box with an explicit shape helps.",
        "Answer_original_content_gpt_summary":"The solution to the user's challenge with mismatched shapes between the inputs and model of their reinforcement learning project is to set the environment obs space to range from 0.0 to 1000.0 using the following code: self.observation_space = box(0.0, 1000.0, shape=(1,1), dtype=np.float32).",
        "Answer_preprocessed_content":"possible reason the error message your original environment obs space is . displaying the shape of your environment obs space gives this could be indicated by in the error message. if you pass the shape into , you get the expected suggested solution i assume that you want your environment obs space to range from to as indicated by the in your function. try using the following for your environment obs space i hope that by setting the with an explicit helps. edit i have modified your training script. this modification includes new imports, custom model class, model registration & addition of registered custom model to config. for readability, only sections added are shown below. the entire modified training script is available in this gist. please run with the proposed obs space as describe above. new additional imports custom model class registered & add custom model edit to show that it works, truncated output from this time i make edits in all files. your environment, training script & the jupyter notebook but it turns out that there isn't a need to define custom models for your custom environment. however, that remains viable. and you're right, the main cause of the issue is still in the obs space. i set to be a d numpy array to make it talk better with ray rllib. the creation of the custom environment in the training script was done in a simpler way as shown below. as for the notebook, i used version instead of for & the training is done in local mode with cpu only. however, it will also work with any ml instance with gpu. the entire package along with all dependencies is in here. the edited env the edited training script the notebook code"
    },
    {
        "Question_id":null,
        "Question_title":"ERROR: The provided hyperparameter space cannot be interpreted",
        "Question_body":"I'm training a KNN classifier from sklearn, and I want to use BayersianParameterSampling for hypertununing parameters. I have this code:\n\n run_config = ScriptRunConfig(\n     source_directory='.', script='train.py', arguments=['--input-data', input_ds.as_named_input('data')], \n     environment=_env, compute_target=cluster\n )\n    \n hyper_params = BayesianParameterSampling(parameter_space={\n     '--n_neighbors': choice(range(5, 11)),\n     '--weights': choice('uniform', 'distance'),\n     '--leaf_size': choice(range(30, 101)),\n     '--p': choice(1, 2)\n })\n    \n hd_config = HyperDriveConfig(\n     run_config=run_config, hyperparameter_sampling=hyper_params, policy=None, \n     primary_metric_name='AUC', primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n     max_total_runs=80, max_concurrent_runs=2)\n    \n experiment = Experiment(ws, 'churn-hyperdrive')\n hyperdrive_run = experiment.submit(hd_config)\n    \n hyperdrive_run.wait_for_completion(show_output=True)\n\n\n\nWhen I submit the experiment, I get an error saying there's something wrong with the parameter space I have. I'm passing the parameters as arguments to a simple script, train.py, which only parses the args, sets the values in the KNN classifier, logs a few metrics, and saves the model.\n\nWhat am I doing wrong here? I've went over everything multiple times, and I don't think there's a mistake. The error I'm getting:\n\n\n\n \"<START>[2022-03-05T13:56:17.215074][API][INFO]Experiment created<END>\\n\"\"<START>[2022-03-05T13:56:18.343781][GENERATOR][ERROR]Exception in creating bayesian optimization: ArgumentException:\\n\\tMessage: Got an invalid parameter space for [Random sampling]: [The provided hyperparameter space cannot be interpreted.]\\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Got an invalid parameter space for [Random sampling]: [The provided hyperparameter space cannot be interpreted.]\\\",\\n        \\\"inner_error\\\": {\\n            \\\"code\\\": \\\"BadArgument\\\",\\n            \\\"inner_error\\\": {\\n                \\\"code\\\": \\\"ArgumentInvalid\\\"\\n            }\\n        }\\n    }\\n}.<END>\\n\"\"<START>[2022-03-05T13:56:18.343632][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space<END>\\n\"",
        "Question_answer_count":2,
        "Question_comment_count":4.0,
        "Question_creation_time":1646489041427,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/760594\/error-the-provided-hyperparameter-space-cannot-be.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-07T12:55:32.057Z",
                "Answer_score":1,
                "Answer_body":"Removing the weights parameter solves it. I'm not sure why, as the model I used does take a weights param.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-10T17:11:51.467Z",
                "Answer_score":0,
                "Answer_body":"@jjk1993 After checking with the product group they have suggested an odd workaround that might work for you. Could you please switch \"uniform\" and \"distance\" in the weights parameter and check if it works?\n\nEx:\n\n hyper_params = BayesianParameterSampling(parameter_space={\n      '--n_neighbors': choice(range(5, 11)),\n      '--weights': choice('distance', 'uniform'),\n      '--leaf_size': choice(range(30, 101)),\n      '--p': choice(1, 2)\n  })\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: error: the provided hyperparameter space cannot be interpreted; Content: i'm training a knn classifier from sklearn, and i want to use bayersianparametersampling for hypertununing parameters. i have this code: run_config = scriptrunconfig( source_directory='.', script='train.py', arguments=['--input-data', input_ds.as_named_input('data')], environment=_env, compute_target=cluster ) hyper_params = bayesianparametersampling(parameter_space={ '--n_neighbors': choice(range(5, 11)), '--weights': choice('uniform', 'distance'), '--leaf_size': choice(range(30, 101)), '--p': choice(1, 2) }) hd_config = hyperdriveconfig( run_config=run_config, hyperparameter_sampling=hyper_params, policy=none, primary_metric_name='auc', primary_metric_goal=primarymetricgoal.maximize, max_total_runs=80, max_concurrent_runs=2) experiment = experiment(ws, 'churn-hyperdrive') hyperdrive_run = experiment.submit(hd_config) hyperdrive_run.wait_for_completion(show_output=true) when i submit the experiment, i get an error saying there's something wrong with the parameter space i have. i'm passing the parameters as arguments to a simple script, train.py, which only parses the args, sets the values in the knn classifier, logs a few metrics, and saves the model. what am i doing wrong here? i've went over everything multiple times, and i don't think there's a mistake. the error i'm getting: \"[2022-03-05t13:56:17.215074][api][info]experiment created\\n\"\"[2022-03-05t13:56:18.343781][generator][error]exception in creating bayesian optimization: argumentexception:\\n\\tmessage: got an invalid parameter space for [random sampling]: [the provided hyperparameter space cannot be interpreted.]\\n\\tinnerexception none\\n\\terrorresponse \\n{\\n \\\"error\\\": {\\n \\\"code\\\": \\\"usererror\\\",\\n \\\"message\\\": \\\"got an invalid parameter space for [random sampling]: [the provided hyperparameter space cannot be interpreted.]\\\",\\n \\\"inner_error\\\": {\\n \\\"code\\\": \\\"badargument\\\",\\n \\\"inner_error\\\": {\\n \\\"code\\\": \\\"argumentinvalid\\\"\\n }\\n }\\n }\\n}.\\n\"\"[2022-03-05t13:56:18.343632][generator][info]trying to sample '2' jobs from the hyperparameter space\\n\"",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with their hyperparameter space, resulting in an error message stating that the provided hyperparameter space cannot be interpreted.",
        "Question_preprocessed_content":"Title: error the provided hyperparameter space cannot be interpreted; Content: i'm training a knn classifier from sklearn, and i want to use bayersianparametersampling for hypertununing parameters. i have this code scriptrunconfig , choice , choice , choice , choice hyperdriveconfig experiment experiment when i submit the experiment, i get an error saying there's something wrong with the parameter space i have. i'm passing the parameters as arguments to a simple script, which only parses the args, sets the values in the knn classifier, logs a few metrics, and saves the model. what am i doing wrong here? i've went over everything multiple times, and i don't think there's a mistake. the error i'm getting in creating bayesian optimization got an invalid parameter space for to sample ' ' jobs from the hyperparameter",
        "Answer_original_content":"removing the weights parameter solves it. i'm not sure why, as the model i used does take a weights param. @jjk1993 after checking with the product group they have suggested an odd workaround that might work for you. could you please switch \"uniform\" and \"distance\" in the weights parameter and check if it works? ex: hyper_params = bayesianparametersampling(parameter_space={ '--n_neighbors': choice(range(5, 11)), '--weights': choice('distance', 'uniform'), '--leaf_size': choice(range(30, 101)), '--p': choice(1, 2) }) if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"The solution to the error message regarding the hyperparameter space is to remove the weights parameter. However, if the weights parameter is necessary, an odd workaround suggested by the product group is to switch \"uniform\" and \"distance\" in the weights parameter.",
        "Answer_preprocessed_content":"removing the weights parameter solves it. i'm not sure why, as the model i used does take a weights param. after checking with the product group they have suggested an odd workaround that might work for you. could you please switch uniform and distance in the weights parameter and check if it works? ex choice , choice , choice , choice if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":52278613.0,
        "Question_title":"Add custom packages to Azure Machine Learing Studio",
        "Question_body":"<p>I need to use the function tsCV on azure machine learning studio to evaluate models of forecast, but i got the error <\/p>\n\n<pre><code>could not find function \"tsCV\n<\/code><\/pre>\n\n<p>I'm trying to update the forecast package, but no package are loaded.\nI followed this tutorial\n<a href=\"http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html\" rel=\"noreferrer\">http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html<\/a>\nand \n<a href=\"https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/\" rel=\"noreferrer\">https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/<\/a>\nbut i dont get the same result.\nNo packages are load.<\/p>\n\n<p>I need an example of a package with R code that works o Azure ML or an update of forecast package to use tsCV function.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1536677172073,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":404.0,
        "Owner_creation_time":1515518171123,
        "Owner_last_access_time":1657119408507,
        "Owner_reputation":1108.0,
        "Owner_up_votes":33.0,
        "Owner_down_votes":2.0,
        "Owner_views":183.0,
        "Answer_body":"<p>I have installed the latest version of the forecast package and here are the steps I followed during the installation. <\/p>\n\n<ol>\n<li>Download latest version of CRAN<\/li>\n<li>Be sure that tsCV is working locally<\/li>\n<li>Zip all the dependencies + forecast package<\/li>\n<li>Zip all the generated zips together and upload it to the AMLStudio<\/li>\n<li>Run the following code:<\/li>\n<\/ol>\n\n<blockquote>\n<pre><code>install.packages(\"src\/glue.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/assertthat.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fansi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/utf8.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/labeling.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/munsell.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/R6.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/cli.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/crayon.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/pillar.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/xts.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/TTR.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/curl.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/digest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/gtable.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lazyeval.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/plyr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/reshape2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/rlang.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/scales.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tibble.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/viridisLite.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/withr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quadprog.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quantmod.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/colorspace.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fracdiff.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggplot2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lmtest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/magrittr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/Rcpp.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/timeDate.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tseries.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/urca.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/uroot.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/zoo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RcppArmadillo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/forecast.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(forecast, lib.loc=\".\", verbose=TRUE)\nfar2 &lt;- function(x, h){forecast(Arima(x, order=c(2,0,0)), h=h)}\ne &lt;- tsCV(lynx, far2, h=1)\n<\/code><\/pre>\n<\/blockquote>\n\n<p><a href=\"https:\/\/drive.google.com\/open?id=10Bj0RGCmRFrRECLQrVc26nbx3T-bNSL6\" rel=\"nofollow noreferrer\">Here is the zip I have generated:<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/bbowH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bbowH.png\" alt=\"My experiment\"><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1537192338792,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52278613",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: add custom packages to azure machine learing studio; Content: i need to use the function tscv on studio to evaluate models of forecast, but i got the error could not find function \"tscv i'm trying to update the forecast package, but no package are loaded. i followed this tutorial http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html and https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/ but i dont get the same result. no packages are load. i need an example of a package with r code that works o or an update of forecast package to use tscv function.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in adding custom packages to Azure Machine Learning Studio in order to use the 'tscv' function to evaluate models of forecast.",
        "Question_preprocessed_content":"Title: add custom packages to azure machine learing studio; Content: i need to use the function tscv on studio to evaluate models of forecast, but i got the error i'm trying to update the forecast package, but no package are loaded. i followed this tutorial and but i dont get the same result. no packages are load. i need an example of a package with r code that works o or an update of forecast package to use tscv function.",
        "Answer_original_content":"i have installed the latest version of the forecast package and here are the steps i followed during the installation. download latest version of cran be sure that tscv is working locally zip all the dependencies + forecast package zip all the generated zips together and upload it to the amlstudio run the following code: install.packages(\"src\/glue.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/stringi.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/assertthat.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/fansi.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/utf8.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/stringr.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/labeling.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/munsell.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/r6.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/rcolorbrewer.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/cli.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/crayon.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/pillar.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/xts.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/ttr.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/curl.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/digest.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/gtable.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/lazyeval.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/plyr.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/reshape2.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/rlang.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/scales.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/tibble.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/viridislite.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/withr.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/quadprog.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/quantmod.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/colorspace.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/fracdiff.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/ggplot2.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/lmtest.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/magrittr.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/rcpp.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/timedate.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/tseries.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/urca.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/uroot.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/zoo.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/rcpparmadillo.zip\", lib = \".\", repos = null, verbose = true) install.packages(\"src\/forecast.zip\", lib = \".\", repos = null, verbose = true) library(forecast, lib.loc=\".\", verbose=true) far2 <- function(x, h){forecast(arima(x, order=c(2,0,0)), h=h)} e <- tscv(lynx, far2, h=1) here is the zip i have generated:",
        "Answer_original_content_gpt_summary":"The answer provides a step-by-step guide to installing the latest version of the forecast package and its dependencies in Azure Machine Learning Studio. The solution involves downloading the latest version of cran, ensuring that tscv is working locally, zipping all the dependencies and forecast package, and uploading them to AML Studio. The answer also includes a list of code to install the dependencies and forecast package and a generated zip file.",
        "Answer_preprocessed_content":"i have installed the latest version of the forecast package and here are the steps i followed during the installation. download latest version of cran be sure that tscv is working locally zip all the dependencies + forecast package zip all the generated zips together and upload it to the amlstudio run the following code here is the zip i have generated"
    },
    {
        "Question_id":73663585.0,
        "Question_title":"Add Security groups in Amazon SageMaker for distributed training jobs",
        "Question_body":"<p>We would like to enforce specific security groups to be set on the SageMaker training jobs (XGBoost in script mode).\nHowever, distributed training, in this case, won\u2019t work out of the box, since the containers need to communicate with each other. What are the minimum inbound\/outbound rules (ports) that we need to specify for training jobs so that they can communicate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1662733216593,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":19.0,
        "Owner_creation_time":1662621266503,
        "Owner_last_access_time":1663966999636,
        "Owner_reputation":48.0,
        "Owner_up_votes":2.0,
        "Owner_down_votes":0.0,
        "Owner_views":6.0,
        "Answer_body":"<p>setting up training in VPC including specifying security groups is documented here:\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups<\/a><\/p>\n<p>Normally you would allow all communication between the training nodes. To do this you specify the security group source and destination to the name of the security group itself, and allow all IPv4 traffic. If you want to figure out what ports are used, you could: 1\/ define the permissive security group. 2\/ Turn on VPC flow logs 3\/ run training. 4\/ examine VPC Flow logs 5\/ update the security group only to the required ports.<\/p>\n<p>I must say restricting communication between the training nodes might be an extreme, so I would challenge the customer why it's really needed, as all nodes carry the same job, have the same IAM role, and are transiate by nature.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1662835019252,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73663585",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: add security groups in for distributed training jobs; Content: we would like to enforce specific security groups to be set on the training jobs (xgboost in script mode). however, distributed training, in this case, won\u2019t work out of the box, since the containers need to communicate with each other. what are the minimum inbound\/outbound rules (ports) that we need to specify for training jobs so that they can communicate?",
        "Question_original_content_gpt_summary":"The user is looking for the minimum inbound\/outbound rules (ports) that need to be specified for distributed training jobs so that the containers can communicate.",
        "Question_preprocessed_content":"Title: add security groups in for distributed training jobs; Content: we would like to enforce specific security groups to be set on the training jobs . however, distributed training, in this case, wont work out of the box, since the containers need to communicate with each other. what are the minimum rules that we need to specify for training jobs so that they can communicate?",
        "Answer_original_content":"setting up training in vpc including specifying security groups is documented here:https:\/\/docs.aws.amazon.com\/\/latest\/dg\/train-vpc.html#train-vpc-groups normally you would allow all communication between the training nodes. to do this you specify the security group source and destination to the name of the security group itself, and allow all ipv4 traffic. if you want to figure out what ports are used, you could: 1\/ define the permissive security group. 2\/ turn on vpc flow logs 3\/ run training. 4\/ examine vpc flow logs 5\/ update the security group only to the required ports. i must say restricting communication between the training nodes might be an extreme, so i would challenge the customer why it's really needed, as all nodes carry the same job, have the same iam role, and are transiate by nature.",
        "Answer_original_content_gpt_summary":"The answer suggests that for distributed training jobs, it is recommended to set up training in VPC and specify security groups to allow all communication between the training nodes. If the user wants to figure out what ports are used, they can define a permissive security group, turn on VPC flow logs, run training, examine VPC flow logs, and update the security group only to the required ports. The answer also challenges the need to restrict communication between the training nodes as all nodes carry the same job, have the same IAM role, and are transient by nature.",
        "Answer_preprocessed_content":"setting up training in vpc including specifying security groups is documented normally you would allow all communication between the training nodes. to do this you specify the security group source and destination to the name of the security group itself, and allow all ipv traffic. if you want to figure out what ports are used, you could \/ define the permissive security group. \/ turn on vpc flow logs \/ run training. \/ examine vpc flow logs \/ update the security group only to the required ports. i must say restricting communication between the training nodes might be an extreme, so i would challenge the customer why it's really needed, as all nodes carry the same job, have the same iam role, and are transiate by nature."
    },
    {
        "Question_id":null,
        "Question_title":"Azure Automated ML Model Deployment to Online Endpoint Stuck in Transitioning",
        "Question_body":"I have a model produced by Azure's Automated ML service. I am following this tutorial (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-automl-endpoint?tabs=Studio) to deploy to an online endpoint via the portal. Currently, the endpoint is successfully created but the actual deployment fails. It hangs for around 2.5 hours before crashing on a timeout error. The deployment logs are empty.\n\nOther posts suggest that this might be due to some kind of issue in the configuration file or specification of dependencies. However, those files are being generated and provided by azure. I am at a loss for how to proceed. Thought about trying to deploy from the CLI but the ml extension fails to install citing version conflicts in Docker. Any advice on what might be going wrong or ways to investigate this would be greatly appreciated!",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1650137939707,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/814684\/azure-automated-ml-model-deployment-to-online-endp.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-11T10:24:50.27Z",
                "Answer_score":0,
                "Answer_body":"@SamMcKinleyHinson-6915 Thanks, Can you try this notebook for deployment and if that works for you (it should), compare with your code?\nhttps:\/\/github.com\/CESARDELATORRE\/Easy-AutoML-MLOps\/blob\/master\/notebooks\/5-automl-model-service-deployment-and-inference\/automl-model-service-deployment-and-inference-safe-driver-classifier.ipynb\n\nYou can also use the notebook with a simple AutoML remote run, but you might need to change the name of the model when registering it in the Workspace since it\u2019s a different name to what the deployment notebook is using:\nhttps:\/\/github.com\/CESARDELATORRE\/Easy-AutoML-MLOps\/blob\/master\/notebooks\/3-automl-remote-compute-run\/automl-remote-compute-run-safe-driver-classifier.ipynb",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure automated ml model deployment to online endpoint stuck in transitioning; Content: i have a model produced by azure's automated ml service. i am following this tutorial (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-automl-endpoint?tabs=studio) to deploy to an online endpoint via the portal. currently, the endpoint is successfully created but the actual deployment fails. it hangs for around 2.5 hours before crashing on a timeout error. the deployment logs are empty. other posts suggest that this might be due to some kind of issue in the configuration file or specification of dependencies. however, those files are being generated and provided by azure. i am at a loss for how to proceed. thought about trying to deploy from the cli but the ml extension fails to install citing version conflicts in docker. any advice on what might be going wrong or ways to investigate this would be greatly appreciated!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with deploying an Azure Automated ML model to an online endpoint, which is stuck in transitioning and crashing after a timeout error.",
        "Question_preprocessed_content":"Title: azure automated ml model deployment to online endpoint stuck in transitioning; Content: i have a model produced by azure's automated ml service. i am following this tutorial to deploy to an online endpoint via the portal. currently, the endpoint is successfully created but the actual deployment fails. it hangs for around hours before crashing on a timeout error. the deployment logs are empty. other posts suggest that this might be due to some kind of issue in the configuration file or specification of dependencies. however, those files are being generated and provided by azure. i am at a loss for how to proceed. thought about trying to deploy from the cli but the ml extension fails to install citing version conflicts in docker. any advice on what might be going wrong or ways to investigate this would be greatly appreciated!",
        "Answer_original_content":"@sammckinleyhinson-6915 thanks, can you try this notebook for deployment and if that works for you (it should), compare with your code? https:\/\/github.com\/cesardelatorre\/easy-automl-mlops\/blob\/master\/notebooks\/5-automl-model-service-deployment-and-inference\/automl-model-service-deployment-and-inference-safe-driver-classifier.ipynb you can also use the notebook with a simple automl remote run, but you might need to change the name of the model when registering it in the workspace since its a different name to what the deployment notebook is using: https:\/\/github.com\/cesardelatorre\/easy-automl-mlops\/blob\/master\/notebooks\/3-automl-remote-compute-run\/automl-remote-compute-run-safe-driver-classifier.ipynb",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of deploying an Azure Automated ML model to an online endpoint that is stuck in transitioning and crashing after a timeout error are: trying a deployment notebook from a GitHub repository and comparing it with the user's code, and using a notebook with a simple automl remote run, but changing the name of the model when registering it in the workspace since it has a different name than the deployment notebook.",
        "Answer_preprocessed_content":"thanks, can you try this notebook for deployment and if that works for you , compare with your code? you can also use the notebook with a simple automl remote run, but you might need to change the name of the model when registering it in the workspace since its a different name to what the deployment notebook is using"
    },
    {
        "Question_id":70838510.0,
        "Question_title":"Is it possible to request a Vertex AI endpoint from another GCP project?",
        "Question_body":"<p>I trained a model on GCP Vertex AI, and deployed it on an endpoint.<\/p>\n<p>I am able to execute predictions from a sample to my model with this python code <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python<\/a><\/p>\n<p>It works within my GCP project.<\/p>\n<p>My question is, is it possible to request this endpoint from another GCP project ? If I set a service account and set IAM role in both projects ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1643047926007,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":417.0,
        "Owner_creation_time":1598873976143,
        "Owner_last_access_time":1663530379503,
        "Owner_reputation":140.0,
        "Owner_up_votes":9.0,
        "Owner_down_votes":0.0,
        "Owner_views":7.0,
        "Answer_body":"<p>Yes it is possible. For example you have Project A and Project B, assuming that Project A hosts the model.<\/p>\n<ul>\n<li><p>Add service account of Project B in Project A and provide at least <code>roles\/aiplatform.user<\/code> predefined role. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\">predefined roles<\/a> and look for <code>roles\/aiplatform.user<\/code> to see complete roles it contains.<\/p>\n<\/li>\n<li><p>This role contains <strong>aiplatform.endpoints.<\/strong>* and <strong>aiplatform.batchPredictionJobs.<\/strong>* as these are the roles needed to run predictions.<\/p>\n<blockquote>\n<p>See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/iam-permissions\" rel=\"nofollow noreferrer\">IAM permissions for Vertex AI<\/a><\/p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Resource<\/th>\n<th>Operation<\/th>\n<th>Permissions needed<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>batchPredictionJobs<\/td>\n<td>Create a batchPredictionJob<\/td>\n<td>aiplatform.batchPredictionJobs.create (permission needed on the parent resource)<\/td>\n<\/tr>\n<tr>\n<td>endpoints<\/td>\n<td>Predict an endpoint<\/td>\n<td>aiplatform.endpoints.predict (permission needed on the endpoint resource)<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<\/div><\/blockquote>\n<\/li>\n<\/ul>\n<p>With this set up, Project B will be able to use the model in Project A to run predictions.<\/p>\n<p>NOTE: Just make sure that the script of Project B points to the resources in Project A like <code>project_id<\/code> and <code>endpoint_id<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1643080918688,
        "Answer_score":1.0,
        "Owner_location":"Versailles, France",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1643081751060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70838510",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is it possible to request a endpoint from another gcp project?; Content: i trained a model on gcp , and deployed it on an endpoint. i am able to execute predictions from a sample to my model with this python code https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/online-predictions-automl#aiplatform_predict_image_classification_sample-python it works within my gcp project. my question is, is it possible to request this endpoint from another gcp project ? if i set a service account and set iam role in both projects ?",
        "Question_original_content_gpt_summary":"The user is trying to determine if it is possible to request an endpoint from another GCP project using a service account and IAM role.",
        "Question_preprocessed_content":"Title: is it possible to request a endpoint from another gcp project?; Content: i trained a model on gcp , and deployed it on an endpoint. i am able to execute predictions from a sample to my model with this python code it works within my gcp project. my question is, is it possible to request this endpoint from another gcp project ? if i set a service account and set iam role in both projects ?",
        "Answer_original_content":"yes it is possible. for example you have project a and project b, assuming that project a hosts the model. add service account of project b in project a and provide at least roles\/aiplatform.user predefined role. see predefined roles and look for roles\/aiplatform.user to see complete roles it contains. this role contains aiplatform.endpoints.* and aiplatform.batchpredictionjobs.* as these are the roles needed to run predictions. see iam permissions for resource operation permissions needed batchpredictionjobs create a batchpredictionjob aiplatform.batchpredictionjobs.create (permission needed on the parent resource) endpoints predict an endpoint aiplatform.endpoints.predict (permission needed on the endpoint resource) with this set up, project b will be able to use the model in project a to run predictions. note: just make sure that the script of project b points to the resources in project a like project_id and endpoint_id.",
        "Answer_original_content_gpt_summary":"Possible solutions to the question of whether it is possible to request an endpoint from another GCP project using a service account and IAM role are provided in the answer. The answer suggests adding the service account of project B in project A and providing at least the roles\/aiplatform.user predefined role. This role contains the necessary roles to run predictions, such as aiplatform.endpoints.* and aiplatform.batchpredictionjobs.*. The answer also provides the IAM permissions for resource operation permissions needed to create a batchpredictionjob and predict an endpoint. With this setup, project B will be able to use the model in project A to run predictions, provided that the script of project B points to the resources in project A like project_id and endpoint_id.",
        "Answer_preprocessed_content":"yes it is possible. for example you have project a and project b, assuming that project a hosts the model. add service account of project b in project a and provide at least predefined role. see predefined roles and look for to see complete roles it contains. this role contains and as these are the roles needed to run predictions. see iam permissions for resource operation permissions needed batchpredictionjobs create a batchpredictionjob endpoints predict an endpoint with this set up, project b will be able to use the model in project a to run predictions. note just make sure that the script of project b points to the resources in project a like and ."
    },
    {
        "Question_id":56260720.0,
        "Question_title":"Import custom modules in Amazon Sagemaker Jupyter notebook",
        "Question_body":"<p>I want to import a custom module in my jupyter notebook in Sagemaker. Trying the import from Untitled1.ipynb I have tried two different structures. The first one is:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tJb5l.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tJb5l.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Inside \"package folder\" there were the files \"cross_validation.py\" and \"<strong>init<\/strong>.py\". The followings commands have been tried:<\/p>\n\n<pre><code>from package import cross_validation\nimport package.cross_validation\n<\/code><\/pre>\n\n<p>The second one is<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/eplmc.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eplmc.png\" alt=\"emak\"><\/a><\/p>\n\n<p>and I have coded  <code>import cross_validation<\/code><\/p>\n\n<p>In both cases I get no error at all when importing, but I can't use the class inside the module because I receive the error name <code>Class_X is not defined<\/code><\/p>\n\n<p>I also have restarted the notebook, just in case and it still not working. How could I make it?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1558540323460,
        "Question_favorite_count":1.0,
        "Question_score":5.0,
        "Question_view_count":6033.0,
        "Owner_creation_time":1523298968403,
        "Owner_last_access_time":1663934452963,
        "Owner_reputation":1754.0,
        "Owner_up_votes":396.0,
        "Owner_down_votes":76.0,
        "Owner_views":197.0,
        "Answer_body":"<p>You can add a <code>__init__.py<\/code> file to your <code>package<\/code> directory to make it a Python package. Then you will be import the modules from the package inside your Jupyter notebook<\/p>\n\n<pre><code>\/home\/ec2-user\/SageMaker\n    -- Notebook.ipynb \n    -- mypackage\n        -- __init__.py\n        -- mymodule.py\n<\/code><\/pre>\n\n<p>Contents of Notebook.ipynb<\/p>\n\n<pre><code>from mypackage.mymodule import SomeClass, SomeOtherClass\n<\/code><\/pre>\n\n<p>For more details, see <a href=\"https:\/\/docs.python.org\/3\/tutorial\/modules.html#packages\" rel=\"nofollow noreferrer\">https:\/\/docs.python.org\/3\/tutorial\/modules.html#packages<\/a><\/p>\n\n<p>Thanks for using Amazon SageMaker!<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1558571220172,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56260720",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: import custom modules in jupyter notebook; Content: i want to import a custom module in my jupyter notebook in . trying the import from untitled1.ipynb i have tried two different structures. the first one is: inside \"package folder\" there were the files \"cross_validation.py\" and \"init.py\". the followings commands have been tried: from package import cross_validation import package.cross_validation the second one is and i have coded import cross_validation in both cases i get no error at all when importing, but i can't use the class inside the module because i receive the error name class_x is not defined i also have restarted the notebook, just in case and it still not working. how could i make it?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in importing a custom module into their Jupyter Notebook, as they are unable to use the class inside the module due to an error.",
        "Question_preprocessed_content":"Title: import custom modules in jupyter notebook; Content: i want to import a custom module in my jupyter notebook in . trying the import from i have tried two different structures. the first one is inside package folder there were the files and the followings commands have been tried the second one is and i have coded in both cases i get no error at all when importing, but i can't use the class inside the module because i receive the error name i also have restarted the notebook, just in case and it still not working. how could i make it?",
        "Answer_original_content":"you can add a __init__.py file to your package directory to make it a python package. then you will be import the modules from the package inside your jupyter notebook \/home\/ec2-user\/ -- notebook.ipynb -- mypackage -- __init__.py -- mymodule.py contents of notebook.ipynb from mypackage.mymodule import someclass, someotherclass for more details, see https:\/\/docs.python.org\/3\/tutorial\/modules.html#packages thanks for using !",
        "Answer_original_content_gpt_summary":"The solution to the user's challenge of importing a custom module into their Jupyter Notebook is to add a __init__.py file to their package directory to make it a python package. This will allow them to import the modules from the package inside their Jupyter Notebook. They can then use the imported classes in their notebook. The user can refer to the Python documentation for more details on packages.",
        "Answer_preprocessed_content":"you can add a file to your directory to make it a python package. then you will be import the modules from the package inside your jupyter notebook contents of for more details, see thanks for using !"
    },
    {
        "Question_id":null,
        "Question_title":"Different tensorflow version using azure ml portal vs VS Code",
        "Question_body":"I am using azure ML to run an python code in a jupyter notebook using tensorflow. I'm using \"Python 3.6 - Azure ML\" and an instance of azure compute to run it. It's showing tensorflow version as 2.1.0 .When I run the same code using VS Code connected to azure portal remotely, it's shows tensorflow version as 2.5.0. Why this discrepency?\n\nAlso I need tensorflow version 2.3.0 or up but there is not way for me to upgrade that using azure ml portal. Any help will be appreciated. Thank you!",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1627517892667,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/493634\/different-tensorflow-version-using-azure-ml-portal.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-29T09:56:36.073Z",
                "Answer_score":0,
                "Answer_body":"@MJ-1993 I think the discrepancy is because the notebook is using a different environment with the kernel Python 3.6 - Azure ML. You can install the required version of tensorflow or any package with pip install from a new cell in the notebook.\n\n!pip install tensorflow==2.3.0\n\nThis should help you use this environment from the notebook. Thanks.",
                "Answer_comment_count":4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: different tensorflow version using portal vs vs code; Content: i am using to run an python code in a jupyter notebook using tensorflow. i'm using \"python 3.6 - \" and an instance of azure compute to run it. it's showing tensorflow version as 2.1.0 .when i run the same code using vs code connected to azure portal remotely, it's shows tensorflow version as 2.5.0. why this discrepency? also i need tensorflow version 2.3.0 or up but there is not way for me to upgrade that using portal. any help will be appreciated. thank you!",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with different tensorflow versions when running a python code in a jupyter notebook using tensorflow on an instance of Azure Compute, and is unable to upgrade the version using the portal.",
        "Question_preprocessed_content":"Title: different tensorflow version using portal vs vs code; Content: i am using to run an python code in a jupyter notebook using tensorflow. i'm using python and an instance of azure compute to run it. it's showing tensorflow version as .when i run the same code using vs code connected to azure portal remotely, it's shows tensorflow version as why this discrepency? also i need tensorflow version or up but there is not way for me to upgrade that using portal. any help will be appreciated. thank you!",
        "Answer_original_content":"@mj-1993 i think the discrepancy is because the notebook is using a different environment with the kernel python 3.6 - . you can install the required version of tensorflow or any package with pip install from a new cell in the notebook. !pip install tensorflow==2.3.0 this should help you use this environment from the notebook. thanks.",
        "Answer_original_content_gpt_summary":"Possible solution: The user can install the required version of tensorflow or any package with pip install from a new cell in the notebook. For example, they can use \"!pip install tensorflow==2.3.0\" to install tensorflow version 2.3.0. This should help them use the desired environment from the notebook.",
        "Answer_preprocessed_content":"i think the discrepancy is because the notebook is using a different environment with the kernel python . you can install the required version of tensorflow or any package with pip install from a new cell in the notebook. !pip install this should help you use this environment from the notebook. thanks."
    },
    {
        "Question_id":58751122.0,
        "Question_title":"How to disable logs in Kedro",
        "Question_body":"<p>I have been unsuccessful in disabling kedro logs.  I have tried adding <code>disable_existing_loggers: True<\/code> to the logging.yml file as well as <code>disable:True<\/code> to all of the existing logs and it still appears to be saving log files.  Any suggestions?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1573137628147,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":410.0,
        "Owner_creation_time":1479159384132,
        "Owner_last_access_time":1663361584220,
        "Owner_reputation":513.0,
        "Owner_up_votes":123.0,
        "Owner_down_votes":0.0,
        "Owner_views":113.0,
        "Answer_body":"<p>If you want <code>kedro<\/code> to stop logging you can override the <code>_setup_logging<\/code> in <code>ProjectContext<\/code> in <code>src\/&lt;package-name&gt;\/run.py<\/code> as per the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/07_logging.html#configure-logging\" rel=\"nofollow noreferrer\">documentation<\/a>. For example:<\/p>\n\n<pre><code>class ProjectContext(KedroContext):\n    \"\"\"Users can override the remaining methods from the parent class here, or create new ones\n    (e.g. as required by plugins)\n\n    \"\"\"\n\n    project_name = \"&lt;PACKGE-NAME&gt;\"\n    project_version = \"0.15.4\"\n\n    def _get_pipelines(self) -&gt; Dict[str, Pipeline]:\n        return create_pipelines()\n\n    def _setup_logging(self) -&gt; None:\n        import logging\n        logging.disable()\n<\/code><\/pre>\n\n<p>If you want it to still log to the console, but not save to <code>logs\/info.log<\/code> then you can do <code>def _setup_logging(self) -&gt; None: pass<\/code>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1573141908796,
        "Answer_score":3.0,
        "Owner_location":"Illinois, United States",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1573142581883,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58751122",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to disable logs in ; Content: i have been unsuccessful in disabling logs. i have tried adding disable_existing_loggers: true to the logging.yml file as well as disable:true to all of the existing logs and it still appears to be saving log files. any suggestions?",
        "Question_original_content_gpt_summary":"The user is struggling to disable logs in their logging.yml file despite attempting to add the necessary code.",
        "Question_preprocessed_content":"Title: how to disable logs in ; Content: i have been unsuccessful in disabling logs. i have tried adding to the file as well as to all of the existing logs and it still appears to be saving log files. any suggestions?",
        "Answer_original_content":"if you want to stop logging you can override the _setup_logging in projectcontext in src\/<package-name>\/run.py as per the documentation. for example: class projectcontext(context): \"\"\"users can override the remaining methods from the parent class here, or create new ones (e.g. as required by plugins) \"\"\" project_name = \"<packge-name>\" project_version = \"0.15.4\" def _get_pipelines(self) -> dict[str, pipeline]: return create_pipelines() def _setup_logging(self) -> none: import logging logging.disable() if you want it to still log to the console, but not save to logs\/info.log then you can do def _setup_logging(self) -> none: pass.",
        "Answer_original_content_gpt_summary":"Possible solutions to disable logs in the logging.yml file are to override the _setup_logging method in the projectcontext file and add the necessary code to disable logging. Another solution is to modify the _setup_logging method to pass and still log to the console but not save to logs\/info.log.",
        "Answer_preprocessed_content":"if you want to stop logging you can override the in in as per the documentation. for example if you want it to still log to the console, but not save to then you can do ."
    },
    {
        "Question_id":null,
        "Question_title":"Does Vertex AI support labels for counting?",
        "Question_body":"I have an image where I have to do a multi-label classification and additionally count the number of a specific item in each image. I'm trying to setup a labeling task so I can enter a continuous number (0-100 for example), but there doesn't seem to be support for it.  Additionally, does the labeling have capabilities to pre-choose a \"default\"  value? Does anyone have an idea?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1635151620000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":404.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-Vertex-AI-support-labels-for-counting\/td-p\/173840\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nAs you have rightly mentioned, Vertex AI does not presently support object counting in an image. Please feel free to reach out to submit a feature request via the issue tracker link[0] to the Vertex AI product team about such implementations.\n\nAs mentioned in this article[1], there are only three ways to assign labels to your training data items:\n\n-- Add the data items to your dataset with their labels already assigned, for example using a commercially available dataset\n-- Assign labels to the data items using the Cloud Console\n-- Request to have human labelers add labels to the data items\n\nAt this time, none of these options provide ways to pre-choose a \"default\" value. May be there may be workarounds to explore using your own human labelers via your instructions.\n\n\n[0]https:\/\/developers.google.com\/issue-tracker\/#public_users\n[1]https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-labeling-job"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: does support labels for counting?; Content: i have an image where i have to do a multi-label classification and additionally count the number of a specific item in each image. i'm trying to setup a labeling task so i can enter a continuous number (0-100 for example), but there doesn't seem to be support for it. additionally, does the labeling have capabilities to pre-choose a \"default\" value? does anyone have an idea?",
        "Question_original_content_gpt_summary":"The user is trying to set up a labeling task to enter a continuous number for a multi-label classification and count the number of a specific item in each image, and is wondering if the labeling has capabilities to pre-choose a \"default\" value.",
        "Question_preprocessed_content":"Title: does support labels for counting?; Content: i have an image where i have to do a classification and additionally count the number of a specific item in each image. i'm trying to setup a labeling task so i can enter a continuous number , but there doesn't seem to be support for it. additionally, does the labeling have capabilities to a default value? does anyone have an idea?",
        "Answer_original_content":"hello, as you have rightly mentioned, does not presently support object counting in an image. please feel free to reach out to submit a feature request via the issue tracker link[0] to the product team about such implementations. as mentioned in this article[1], there are only three ways to assign labels to your training data items: -- add the data items to your dataset with their labels already assigned, for example using a commercially available dataset -- assign labels to the data items using the cloud console -- request to have human labelers add labels to the data items at this time, none of these options provide ways to pre-choose a \"default\" value. may be there may be workarounds to explore using your own human labelers via your instructions. [0]https:\/\/developers.google.com\/issue-tracker\/#public_users [1]https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/data-labeling-job",
        "Answer_original_content_gpt_summary":"The answer states that the labeling task does not presently support object counting in an image, and suggests submitting a feature request to the product team for such implementations. The answer also mentions three ways to assign labels to training data items, but none of them provide ways to pre-choose a \"default\" value. The answer suggests exploring workarounds using your own human labelers via your instructions.",
        "Answer_preprocessed_content":"hello, as you have rightly mentioned, does not presently support object counting in an image. please feel free to reach out to submit a feature request via the issue tracker link to the product team about such implementations. as mentioned in this article , there are only three ways to assign labels to your training data items add the data items to your dataset with their labels already assigned, for example using a commercially available dataset assign labels to the data items using the cloud console request to have human labelers add labels to the data items at this time, none of these options provide ways to a default value. may be there may be workarounds to explore using your own human labelers via your instructions."
    },
    {
        "Question_id":71509160.0,
        "Question_title":"AzureML list huge amount of files",
        "Question_body":"<p>I have directory in AzureML notebook in which I have 300k files and need to list their names.\nApproach below works but takes 1.5h to execute:<\/p>\n<pre><code>from os import listdir\nfrom os.path import isfile, join\nmypath = &quot;.\/temp\/&quot;\ndocsOnDisk = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n<\/code><\/pre>\n<p>What is the azure way to quickly list those files? (both notebook and this directory is in FileShare).<\/p>\n<p>I am also aware that the approach below will give some gain, but still it is not the azure way to do this.<\/p>\n<pre><code>docsOnDisk = [f.name for f in scandir(mypath) ] # shall be 2-20x faster\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1647505772703,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":102.0,
        "Owner_creation_time":1567151674136,
        "Owner_last_access_time":1663771775580,
        "Owner_reputation":304.0,
        "Owner_up_votes":221.0,
        "Owner_down_votes":1.0,
        "Owner_views":35.0,
        "Answer_body":"<p>Try using glob module and filter method instead of list comprehension.<\/p>\n<pre><code>import glob\nfrom os.path import isfile\nmypath = &quot;.\/temp\/*&quot;\ndocsOnDisk = glob.glob(mypath)\nverified_docsOnDisk = list(filter(lambda x:isfile(x), docsOnDisk))\n<\/code><\/pre>\n<p>glob should give only existing files. Its not needed to verify them by using isfile(). But still if you need to try it out then you can use filter method instead of list comprehension. To skip verification, you can comment last line.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648651666183,
        "Answer_score":1.0,
        "Owner_location":"Krak\u00f3w, Poland",
        "Question_last_edit_time":1647506520003,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71509160",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: list huge amount of files; Content: i have directory in notebook in which i have 300k files and need to list their names. approach below works but takes 1.5h to execute: from os import listdir from os.path import isfile, join mypath = \".\/temp\/\" docsondisk = [f for f in listdir(mypath) if isfile(join(mypath, f))] what is the azure way to quickly list those files? (both notebook and this directory is in fileshare). i am also aware that the approach below will give some gain, but still it is not the azure way to do this. docsondisk = [f.name for f in scandir(mypath) ] # shall be 2-20x faster",
        "Question_original_content_gpt_summary":"The user is facing a challenge of needing to list the names of 300k files in a directory on their notebook, and is looking for an Azure-based solution to do this quickly.",
        "Question_preprocessed_content":"Title: list huge amount of files; Content: i have directory in notebook in which i have k files and need to list their names. approach below works but takes to execute what is the azure way to quickly list those files? . i am also aware that the approach below will give some gain, but still it is not the azure way to do this.",
        "Answer_original_content":"try using glob module and filter method instead of list comprehension. import glob from os.path import isfile mypath = \".\/temp\/*\" docsondisk = glob.glob(mypath) verified_docsondisk = list(filter(lambda x:isfile(x), docsondisk)) glob should give only existing files. its not needed to verify them by using isfile(). but still if you need to try it out then you can use filter method instead of list comprehension. to skip verification, you can comment last line.",
        "Answer_original_content_gpt_summary":"The answer suggests using the glob module and filter method instead of list comprehension to quickly list the names of 300k files in a directory on their notebook. The glob module should give only existing files, so it's not necessary to verify them using isfile(). However, if verification is needed, the user can use the filter method instead of list comprehension. To skip verification, the user can comment out the last line.",
        "Answer_preprocessed_content":"try using glob module and filter method instead of list comprehension. glob should give only existing files. its not needed to verify them by using isfile . but still if you need to try it out then you can use filter method instead of list comprehension. to skip verification, you can comment last line."
    },
    {
        "Question_id":null,
        "Question_title":"Deep Learning VM Config to connect to Google Colab",
        "Question_body":"Morning to allIm trying to connect a google colab file to a Google Deep Learning VM with any results. My guess is that I need to configure something inside the VM or the google console but not sure how to do so.I get the error that you will find in image 1 that says:\"The VM requested does not exist. Check out our guide to set up GCE VMs in Colab\"and has a the next link in which theres not much info on how to solve the situation:  https:\/\/research.google.com\/colaboratory\/marketplace.htmlOn image 2 and 3 you will find the info that I add to the colab file that is the same as the VM configuration that you will fins on image 3.What I\u00b4m doing wrong? Do I need to asing special permits to the VM? Any comments or advice is more than appreciatedImage 1Image 2\nImage 3",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1652954760000,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":298.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deep-Learning-VM-Config-to-connect-to-Google-Colab\/td-p\/424603\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-20T17:55:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Did you confirm that the VM does exist in the project you typed down? If so, is its name correctly spelled and is the VM turned on?"
            },
            {
                "Answer_creation_time":"2022-06-02T02:50:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I solve the issus by changing VM configuration(decrease or increase ram for exemple)"
            },
            {
                "Answer_creation_time":"2022-08-30T16:39:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi All,\n\nI too am experiencing the same issue as the original poster\u00a0@holguinmora\u00a0\n\nWould be very helpful to know the steps\/configuration to overcome this issue\u00a0@bentalla"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deep learning vm config to connect to google colab; Content: morning to allim trying to connect a google colab file to a google deep learning vm with any results. my guess is that i need to configure something inside the vm or the google console but not sure how to do so.i get the error that you will find in image 1 that says:\"the vm requested does not exist. check out our guide to set up gce vms in colab\"and has a the next link in which theres not much info on how to solve the situation: https:\/\/research.google.com\/colaboratory\/marketplace.htmlon image 2 and 3 you will find the info that i add to the colab file that is the same as the vm configuration that you will fins on image 3.what i\u00b4m doing wrong? do i need to asing special permits to the vm? any comments or advice is more than appreciatedimage 1image 2 image 3",
        "Question_original_content_gpt_summary":"The user is encountering challenges in connecting a Google Colab file to a Google Deep Learning VM, and is unsure of how to configure the VM or the Google Console to do so.",
        "Question_preprocessed_content":"Title: deep learning vm config to connect to google colab; Content: morning to allim trying to connect a google colab file to a google deep learning vm with any results. my guess is that i need to configure something inside the vm or the google console but not sure how to do get the error that you will find in image that says the vm requested does not exist. check out our guide to set up gce vms in colab and has a the next link in which theres not much info on how to solve the situation image and you will find the info that i add to the colab file that is the same as the vm configuration that you will fins on image im doing wrong? do i need to asing special permits to the vm? any comments or advice is more than appreciatedimage image image",
        "Answer_original_content":"did you confirm that the vm does exist in the project you typed down? if so, is its name correctly spelled and is the vm turned on? i solve the issus by changing vm configuration(decrease or increase ram for exemple) hi all, i too am experiencing the same issue as the original poster@holguinmora would be very helpful to know the steps\/configuration to overcome this issue@bentalla",
        "Answer_original_content_gpt_summary":"Possible solutions mentioned in the answer include confirming the existence of the VM in the project and ensuring that its name is spelled correctly and turned on. Additionally, changing the VM configuration, such as increasing or decreasing RAM, may help resolve the issue. However, the answer does not provide specific steps or configurations to overcome the issue.",
        "Answer_preprocessed_content":"did you confirm that the vm does exist in the project you typed down? if so, is its name correctly spelled and is the vm turned on? i solve the issus by changing vm configuration hi all, i too am experiencing the same issue as the original poster would be very helpful to know the to overcome this issue"
    },
    {
        "Question_id":null,
        "Question_title":"How do I create a resource group when creating a workspace?",
        "Question_body":"I am going through the Azure AI training and need to create a workspace under machine learning. When it asks me to select a resource group there are no options. When I want to create a new resource group it says I dont have permissions under my subscription. What do I need to do?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1638162185497,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/643801\/how-do-i-create-a-resource-group-when-creating-a-w.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-29T05:11:17.963Z",
                "Answer_score":1,
                "Answer_body":"I believe you need to be a subscription-level owner or contributor.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i create a resource group when creating a workspace?; Content: i am going through the azure ai training and need to create a workspace under machine learning. when it asks me to select a resource group there are no options. when i want to create a new resource group it says i dont have permissions under my subscription. what do i need to do?",
        "Question_original_content_gpt_summary":"The user is encountering difficulty creating a resource group when creating a workspace in Azure AI training.",
        "Question_preprocessed_content":"Title: how do i create a resource group when creating a workspace?; Content: i am going through the azure ai training and need to create a workspace under machine learning. when it asks me to select a resource group there are no options. when i want to create a new resource group it says i dont have permissions under my subscription. what do i need to do?",
        "Answer_original_content":"i believe you need to be a subscription-level owner or contributor.",
        "Answer_original_content_gpt_summary":"Possible solutions to the difficulty encountered when creating a resource group for a workspace in Azure AI training are to ensure that the user has subscription-level owner or contributor access.",
        "Answer_preprocessed_content":"i believe you need to be a owner or contributor."
    },
    {
        "Question_id":null,
        "Question_title":"How to access the data of specific step in the dashboard of wandb.ai?",
        "Question_body":"<p>I am using the dashboard of wandb and want to access the specific step data. However, I can only obtain the metric throughout the whole training step. For example, I wanna get the best mAP in COCO evaluation, and corresponding AP50, AP70 and other evaluation metrics.<\/p>\n<p>Need help. Thanks for your help!<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_creation_time":1646403651386,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":178.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-access-the-data-of-specific-step-in-the-dashboard-of-wandb-ai\/2017",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-09T00:51:13.714Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/djh\">@djh<\/a>,<\/p>\n<p>Could you share the workspace for which you are looking to compute these metrics? The context of the workspace would help me understand your request better.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-09T01:21:03.575Z",
                "Answer_body":"<p>The request is simple, I just cannot find how to access the entire record of one specific step in the dashboard. For example, I have recorded the log metric (loss, acc, .etc.) at each step, but in the dashboard, I can only see the line chart for loss or acc varying from step. My real demand is to access the log information at a certain step. Thanks for your reply!<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-10T21:45:39.535Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/djh\">@djh<\/a>,<\/p>\n<p>This should be possible through our <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/weave#introduction\">Weave Tables<\/a>. The <code>runs.history<\/code> metric holds all the step information and the weave table will let you hold it in a Tabular Format.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-14T21:39:10.199Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/djh\">@djh<\/a>,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<\/p>\n<p>Weights &amp; Biases<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-15T02:44:21.815Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a> , thanks for your instructions, Weave Tables meets my expectation.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-14T02:44:29.536Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to access the data of specific step in the dashboard of .ai?; Content: i am using the dashboard of and want to access the specific step data. however, i can only obtain the metric throughout the whole training step. for example, i wanna get the best map in coco evaluation, and corresponding ap50, ap70 and other evaluation metrics. need help. thanks for your help!",
        "Question_original_content_gpt_summary":"The user is trying to access the data of a specific step in the dashboard of .ai, but is only able to obtain the metric throughout the whole training step.",
        "Question_preprocessed_content":"Title: how to access the data of specific step in the dashboard of ; Content: i am using the dashboard of and want to access the specific step data. however, i can only obtain the metric throughout the whole training step. for example, i wanna get the best map in coco evaluation, and corresponding ap , ap and other evaluation metrics. need help. thanks for your help!",
        "Answer_original_content":"hi @djh, could you share the workspace for which you are looking to compute these metrics? the context of the workspace would help me understand your request better. thanks, ramit the request is simple, i just cannot find how to access the entire record of one specific step in the dashboard. for example, i have recorded the log metric (loss, acc, .etc.) at each step, but in the dashboard, i can only see the line chart for loss or acc varying from step. my real demand is to access the log information at a certain step. thanks for your reply! hi @djh, this should be possible through our weave tables. the runs.history metric holds all the step information and the weave table will let you hold it in a tabular format. thanks, ramit hi @djh, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi @ramit_goolry , thanks for your instructions, weave tables meets my expectation. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The user is trying to access the log information of a specific step in the dashboard of .ai, but is only able to obtain the metric throughout the whole training step. The solution to this problem is to use weave tables, which holds all the step information in a tabular format.",
        "Answer_preprocessed_content":"hi could you share the workspace for which you are looking to compute these metrics? the context of the workspace would help me understand your request better. thanks, ramit the request is simple, i just cannot find how to access the entire record of one specific step in the dashboard. for example, i have recorded the log metric at each step, but in the dashboard, i can only see the line chart for loss or acc varying from step. my real demand is to access the log information at a certain step. thanks for your reply! hi this should be possible through our weave tables. the metric holds all the step information and the weave table will let you hold it in a tabular format. thanks, ramit hi we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi , thanks for your instructions, weave tables meets my expectation. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Differences between dvc repro and dvc status",
        "Question_body":"<p>Is it guaranteed that if <code>dvc status Dvcfile<\/code> shows: nothing to reproduce, the <code>dvc repro Dvcfile<\/code> will not do anything ?<\/p>\n<p>For my use case, it looks like <code>dvc repro<\/code> (even when there is nothing to reproduce) takes more time to run than <code>dvc status<\/code>. I could then switch to calling <code>dvc status<\/code> to find out if there\u2019s nothing to reproduce and have a faster pipeline.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1537891906524,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":736.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/differences-between-dvc-repro-and-dvc-status\/101",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-26T18:02:53.181Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tmain\">@tmain<\/a> !<\/p>\n<p>Sorry for a late response.<\/p>\n<blockquote>\n<p>Is it guaranteed that if  <code>dvc status Dvcfile<\/code>  shows: nothing to reproduce, the  <code>dvc repro Dvcfile<\/code>  will not do anything ?<\/p>\n<\/blockquote>\n<p>Yes, it is indeed guaranteed.<\/p>\n<blockquote>\n<p>For my use case, it looks like  <code>dvc repro<\/code>  (even when there is nothing to reproduce) takes more time to run than  <code>dvc status<\/code> . I could then switch to calling  <code>dvc status<\/code>  to find out if there\u2019s nothing to reproduce and have a faster pipeline.<\/p>\n<\/blockquote>\n<p>Repro works in a bit different way than the status does so there might be differences in processing time. Sorry, I\u2019m not sure I follow, could you please elaborate on what you mean by <code>switch<\/code>?<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":11.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-26T18:17:13.571Z",
                "Answer_body":"<p>By <code>switch<\/code> I meant the following.<\/p>\n<p>In order to find out whether I need to repro something, I used to call <code>dvc repro Dvcfile --dry<\/code> and parse the output of that command. If I got <code>Nothing to reproduce<\/code>, I would continue without calling <code>dvc repro Dvcfile<\/code>.<\/p>\n<p>Now instead, I can do <code>dvc status Dvcfile<\/code>, find out whether there is something to reproduce, and call <code>dvc repro Dvcfile<\/code> if there is.<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-26T19:09:08.118Z",
                "Answer_body":"<p>Ah, got it. Thank you for clarifying! Indeed, <code>dvc status<\/code> was intended to be used  precisely for checking if anything in your pipeline has changed and requires reproduction. And <code>repro --dry<\/code> is actually intended for visualizing the order and list of the commands that will be run in order to reproduce the pipeline <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: differences between repro and status; Content: is it guaranteed that if status file shows: nothing to reproduce, the repro file will not do anything ? for my use case, it looks like repro (even when there is nothing to reproduce) takes more time to run than status. i could then switch to calling status to find out if there\u2019s nothing to reproduce and have a faster pipeline.",
        "Question_original_content_gpt_summary":"The user is facing a challenge of determining whether it is more efficient to use the repro or status file to determine if there is nothing to reproduce, as the repro file takes more time to run than the status file.",
        "Question_preprocessed_content":"Title: differences between repro and status; Content: is it guaranteed that if shows nothing to reproduce, the will not do anything ? for my use case, it looks like takes more time to run than . i could then switch to calling to find out if theres nothing to reproduce and have a faster pipeline.",
        "Answer_original_content":"hi @tmain ! sorry for a late response. is it guaranteed that if status file shows: nothing to reproduce, the repro file will not do anything ? yes, it is indeed guaranteed. for my use case, it looks like repro (even when there is nothing to reproduce) takes more time to run than status . i could then switch to calling status to find out if theres nothing to reproduce and have a faster pipeline. repro works in a bit different way than the status does so there might be differences in processing time. sorry, im not sure i follow, could you please elaborate on what you mean by switch? thanks, ruslan by switch i meant the following. in order to find out whether i need to repro something, i used to call repro file --dry and parse the output of that command. if i got nothing to reproduce, i would continue without calling repro file. now instead, i can do status file, find out whether there is something to reproduce, and call repro file if there is. ah, got it. thank you for clarifying! indeed, status was intended to be used precisely for checking if anything in your pipeline has changed and requires reproduction. and repro --dry is actually intended for visualizing the order and list of the commands that will be run in order to reproduce the pipeline",
        "Answer_original_content_gpt_summary":"The answer suggests that the user can switch from using the repro file to using the status file to determine if there is nothing to reproduce, as the repro file takes more time to run than the status file. The user can call the status file to find out if there is anything to reproduce and then call the repro file if there is. The status file is intended to be used for checking if anything in the pipeline has changed and requires reproduction, while the repro --dry is intended for visualizing the order and list of the commands that will be run to reproduce the pipeline.",
        "Answer_preprocessed_content":"hi ! sorry for a late response. is it guaranteed that if shows nothing to reproduce, the will not do anything ? yes, it is indeed guaranteed. for my use case, it looks like takes more time to run than . i could then switch to calling to find out if theres nothing to reproduce and have a faster pipeline. repro works in a bit different way than the status does so there might be differences in processing time. sorry, im not sure i follow, could you please elaborate on what you mean by ? thanks, ruslan by i meant the following. in order to find out whether i need to repro something, i used to call and parse the output of that command. if i got , i would continue without calling . now instead, i can do , find out whether there is something to reproduce, and call if there is. ah, got it. thank you for clarifying! indeed, was intended to be used precisely for checking if anything in your pipeline has changed and requires reproduction. and is actually intended for visualizing the order and list of the commands that will be run in order to reproduce the pipeline"
    },
    {
        "Question_id":null,
        "Question_title":"Using DVC outside git",
        "Question_body":"<p>Hello here,<br>\nI am grateful on what dvc offers so far in data management. I am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with dvc considering  the data folder is not a git\/dvc working directory?,  such that you can <code>dvc pull<\/code> from any project I wish to work on. The segmentation is added with every new labeling.<\/p>",
        "Question_answer_count":10,
        "Question_comment_count":null,
        "Question_creation_time":1640005495644,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":417.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-outside-git\/1014",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-20T14:35:22.915Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"kongkip\" data-post=\"1\" data-topic=\"1014\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/b5e925\/40.png\" class=\"avatar\"> kongkip:<\/div>\n<blockquote>\n<p>I am grateful on what dvc offers so far in data management. I am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with dvc considering the data folder is not a git\/dvc working directory?, such that you can <code>dvc pull<\/code> from any project I wish to work on. The segmentation is added with every new labeling.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hi, are you looking for a data registry in which you can <code>dvc import<\/code> the data to any projects you want?<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon-32x32.png?v=dfbc4a93a926127fc4495e9d640409f8\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data Version Control \u00b7 DVC<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries#data-registries\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data Registries<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-20T19:42:15.684Z",
                "Answer_body":"<p>Checked this out but I don\u2019t think it will solve my issue.  To provide clarity the raw images are originally stored in AWS S3, then the labeling tool accesses the  image URLs  which are then embedded as images on the UI where one labels and generates the segmentations. For this case this is not a DVC repo yet, so how can the data  be managed with DVC since there are no commands run  locally such as <code>dvc add<\/code> or <code>dvc push<\/code>.<\/p>",
                "Answer_score":11.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-20T19:47:21.592Z",
                "Answer_body":"<p>To understand, data registry will work only if I start locally  creating a git repo on the data and pushing  it to the cloud with <code>dvc push<\/code>.  The way dvc also stores the data on the cloud storage  will make it hard for the labeling tool which is a JS based app.<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T02:31:38.393Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"kongkip\" data-post=\"3\" data-topic=\"1014\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/k\/b5e925\/40.png\" class=\"avatar\"> kongkip:<\/div>\n<blockquote>\n<p>Checked this out but I don\u2019t think it will solve my issue. To provide clarity the raw images are originally stored in AWS S3, then the labeling tool accesses the image URLs which are then embedded as images on the UI where one labels and generates the segmentations. For this case this is not a DVC repo yet, so how can the data be managed with DVC since there are no commands run locally such as <code>dvc add<\/code> or <code>dvc push<\/code> .<\/p>\n<\/blockquote>\n<\/aside>\n<p>OK, thanks for your clarification. In my opinion, if you want to use DVC to track the data version. You might still need a dvc+git repo to store your DVC version files. Even if you are using some advanced features like <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data#setting-up-an-external-cache\" rel=\"noopener nofollow ugc\">eternal data<\/a> or some hacking method like using DVC\u2019s Repo API (<code>Repo.add<\/code>, <code>Repo.push<\/code>, <code>Repo.pull<\/code>). They all need a local repo to operate.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T03:45:33.230Z",
                "Answer_body":"<p>We are working on a new tool in DVC ecosystem that sounds like a better solution.<\/p>\n<p>Do you mind describing your workflow in a greater detail?<\/p>",
                "Answer_score":11.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T05:59:37.410Z",
                "Answer_body":"<p>ok sure, let me drop a diagram here<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/b56a697206e5c68a87e3ca08bec449bf44085260.png\" data-download-href=\"\/uploads\/short-url\/pSSpkBNUetLrn5DnmyIJm641KWA.png?dl=1\" title=\"Data Pipeline Flow Chart.drawio (1)\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/b56a697206e5c68a87e3ca08bec449bf44085260.png\" alt=\"Data Pipeline Flow Chart.drawio (1)\" data-base62-sha1=\"pSSpkBNUetLrn5DnmyIJm641KWA\" width=\"597\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/b56a697206e5c68a87e3ca08bec449bf44085260_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Data Pipeline Flow Chart.drawio (1)<\/span><span class=\"informations\">608\u00d7509 17.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The final storage is the  dvc remote storage which can be exported to any project<\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T07:30:25.073Z",
                "Answer_body":"<p>what do you use csv for?<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T08:59:08.134Z",
                "Answer_body":"<p>it contains points for the labels e.g, ellipse, circles and polygons which are used to generate image segmenations.<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-21T16:14:37.498Z",
                "Answer_body":"<p>Just to clarify - your images and labels live in s3 and you consider this location to be immutable (in a sense that you can guarantee they will not be accidentally deleted or moved as you keep adding new labels).<\/p>\n<p>If this is the case, we can provide tracking of labels versions and datasets (defined as collections of pointers) with our new tool.<\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-11T10:09:32.930Z",
                "Answer_body":"<p>This sounds nice, update me once you do have it ready or in beta.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: using outside git; Content: hello here, i am grateful on what offers so far in data management. i am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with considering the data folder is not a git\/ working directory?, such that you can pull from any project i wish to work on. the segmentation is added with every new labeling.",
        "Question_original_content_gpt_summary":"The user is looking for a way to manage a data set stored in the cloud storage, while not using a git\/working directory.",
        "Question_preprocessed_content":"Title: using outside git; Content: hello here, i am grateful on what offers so far in data management. i am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with considering the data folder is not a working directory?, such that you can from any project i wish to work on. the segmentation is added with every new labeling.",
        "Answer_original_content":"kongkip: i am grateful on what offers so far in data management. i am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with considering the data folder is not a git\/ working directory?, such that you can pull from any project i wish to work on. the segmentation is added with every new labeling. hi, are you looking for a data registry in which you can import the data to any projects you want? data version control data registries open-source version control system for data science and machine learning projects. git-like experience to organize your data, models, and experiments. checked this out but i dont think it will solve my issue. to provide clarity the raw images are originally stored in aws s3, then the labeling tool accesses the image urls which are then embedded as images on the ui where one labels and generates the segmentations. for this case this is not a repo yet, so how can the data be managed with since there are no commands run locally such as add or push. to understand, data registry will work only if i start locally creating a git repo on the data and pushing it to the cloud with push. the way also stores the data on the cloud storage will make it hard for the labeling tool which is a js based app. kongkip: checked this out but i dont think it will solve my issue. to provide clarity the raw images are originally stored in aws s3, then the labeling tool accesses the image urls which are then embedded as images on the ui where one labels and generates the segmentations. for this case this is not a repo yet, so how can the data be managed with since there are no commands run locally such as add or push . ok, thanks for your clarification. in my opinion, if you want to use to track the data version. you might still need a +git repo to store your version files. even if you are using some advanced features like eternal data or some hacking method like using s repo api (repo.add, repo.push, repo.pull). they all need a local repo to operate. we are working on a new tool in ecosystem that sounds like a better solution. do you mind describing your workflow in a greater detail? ok sure, let me drop a diagram here data pipeline flow chart.drawio (1)608509 17.6 kb the final storage is the remote storage which can be exported to any project what do you use csv for? it contains points for the labels e.g, ellipse, circles and polygons which are used to generate image segmenations. just to clarify - your images and labels live in s3 and you consider this location to be immutable (in a sense that you can guarantee they will not be accidentally deleted or moved as you keep adding new labels). if this is the case, we can provide tracking of labels versions and datasets (defined as collections of pointers) with our new tool. this sounds nice, update me once you do have it ready or in beta.",
        "Answer_original_content_gpt_summary":"Possible solutions extracted from the answer are:\n\n- Using a data registry to import data to any projects.\n- Using data version control and data registries with an open-source version control system for data science and machine learning projects.\n- Creating a git repo on the data and pushing it to the cloud with push.\n- Using a new tool in the ecosystem that provides tracking of labels versions and datasets.",
        "Answer_preprocessed_content":"kongkip i am grateful on what offers so far in data management. i am working on a labeling tool for image segmentation with the image labels stored in the cloud storage after generation, my question is, is it possible to manage this data set with considering the data folder is not a working directory?, such that you can from any project i wish to work on. the segmentation is added with every new labeling. hi, are you looking for a data registry in which you can the data to any projects you want? data version control data registries version control system for data science and machine learning projects. experience to organize your data, models, and experiments. checked this out but i dont think it will solve my issue. to provide clarity the raw images are originally stored in aws s , then the labeling tool accesses the image urls which are then embedded as images on the ui where one labels and generates the segmentations. for this case this is not a repo yet, so how can the data be managed with since there are no commands run locally such as or . to understand, data registry will work only if i start locally creating a git repo on the data and pushing it to the cloud with . the way also stores the data on the cloud storage will make it hard for the labeling tool which is a js based app. kongkip checked this out but i dont think it will solve my issue. to provide clarity the raw images are originally stored in aws s , then the labeling tool accesses the image urls which are then embedded as images on the ui where one labels and generates the segmentations. for this case this is not a repo yet, so how can the data be managed with since there are no commands run locally such as or . ok, thanks for your clarification. in my opinion, if you want to use to track the data version. you might still need a +git repo to store your version files. even if you are using some advanced features like eternal data or some hacking method like using s repo api . they all need a local repo to operate. we are working on a new tool in ecosystem that sounds like a better solution. do you mind describing your workflow in a greater detail? ok sure, let me drop a diagram here data pipeline flow kb the final storage is the remote storage which can be exported to any project what do you use csv for? it contains points for the labels ellipse, circles and polygons which are used to generate image segmenations. just to clarify your images and labels live in s and you consider this location to be immutable . if this is the case, we can provide tracking of labels versions and datasets with our new tool. this sounds nice, update me once you do have it ready or in beta."
    },
    {
        "Question_id":null,
        "Question_title":"DVC assigns user permissions to folders in the ssh repository",
        "Question_body":"<p>Hi everybody. I need some help with a dvc repo. I have a very simple repository with a couple of dirs shared. The repo has a remote in an ssh server owned and managed by our team. As a repo folder in the ssh-server I created a folder (dvc.cache) owned by a unix group \u2018dvc-group\u2019.  The problem is that when some user pushes files to the repo (with dvc push) and this requires the creation of a new repo folder (e.g., dvc.cache\/fa), this folder is created with writing permissions only for the user, not for the dvc-group. This causes that when other users try to push modifications occuring in that folder he\/she gets an [Errno 13] Permission denied, as expected.<br>\nMy problem is that I couldn\u2019t find a way to fix this other than manually logging into the server an changing the permissions of these folders.<br>\nAny ideas how to fix this for good with DVC?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1632174009351,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":676.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-assigns-user-permissions-to-folders-in-the-ssh-repository\/898",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-21T07:17:20.510Z",
                "Answer_body":"<p>I guess you need UNIX special permissions for your repository<br>\n$ chmod g+s test<\/p>\n<blockquote>\n<p>When used on a directory, instead, the setgid bit alters the standard behavior so that the group of the files created inside said directory, will not be that of the user who created them, but that of the parent directory itself. This is often used to ease the sharing of files (files will be modifiable by all the users that are part of said group). Just like the setuid, the setgid bit can easily be spotted (in this case on a test directory):<\/p>\n<\/blockquote>",
                "Answer_score":11.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: assigns user permissions to folders in the ssh repository; Content: hi everybody. i need some help with a repo. i have a very simple repository with a couple of dirs shared. the repo has a remote in an ssh server owned and managed by our team. as a repo folder in the ssh-server i created a folder (.cache) owned by a unix group \u2018-group\u2019. the problem is that when some user pushes files to the repo (with push) and this requires the creation of a new repo folder (e.g., .cache\/fa), this folder is created with writing permissions only for the user, not for the -group. this causes that when other users try to push modifications occuring in that folder he\/she gets an [errno 13] permission denied, as expected. my problem is that i couldn\u2019t find a way to fix this other than manually logging into the server an changing the permissions of these folders. any ideas how to fix this for good with ? thanks!",
        "Question_original_content_gpt_summary":"The user is facing a challenge of assigning user permissions to folders in the ssh repository so that when other users try to push modifications, they do not get an [errno 13] permission denied error.",
        "Question_preprocessed_content":"Title: assigns user permissions to folders in the ssh repository; Content: hi everybody. i need some help with a repo. i have a very simple repository with a couple of dirs shared. the repo has a remote in an ssh server owned and managed by our team. as a repo folder in the i created a folder owned by a unix group the problem is that when some user pushes files to the repo and this requires the creation of a new repo folder , this folder is created with writing permissions only for the user, not for the this causes that when other users try to push modifications occuring in that folder gets an permission denied, as expected. my problem is that i couldnt find a way to fix this other than manually logging into the server an changing the permissions of these folders. any ideas how to fix this for good with ? thanks!",
        "Answer_original_content":"i guess you need unix special permissions for your repository $ chmod g+s test when used on a directory, instead, the setgid bit alters the standard behavior so that the group of the files created inside said directory, will not be that of the user who created them, but that of the parent directory itself. this is often used to ease the sharing of files (files will be modifiable by all the users that are part of said group). just like the setuid, the setgid bit can easily be spotted (in this case on a test directory):",
        "Answer_original_content_gpt_summary":"The solution to the challenge of assigning user permissions to folders in the ssh repository is to use Unix special permissions. Specifically, the user should use the command \"chmod g+s\" on the directory to set the setgid bit, which alters the standard behavior so that the group of the files created inside said directory will be that of the parent directory itself. This makes it easier to share files and allows all users in the group to modify them.",
        "Answer_preprocessed_content":"i guess you need unix special permissions for your repository $ chmod g+s test when used on a directory, instead, the setgid bit alters the standard behavior so that the group of the files created inside said directory, will not be that of the user who created them, but that of the parent directory itself. this is often used to ease the sharing of files . just like the setuid, the setgid bit can easily be spotted"
    },
    {
        "Question_id":60329363.0,
        "Question_title":"How to process huge datasets in kedro",
        "Question_body":"<p>I have pretty big (~200Gb, ~20M lines) raw jsonl dataset. I need to extract important properties from there and store the intermediate dataset in csv for further conversion into something like HDF5, parquet, etc. Obviously, I can't use <code>JSONDataSet<\/code> for loading raw dataset, because it utilizes <code>pandas.read_json<\/code> under the hood, and using pandas for the dataset of such size sounds like a bad idea. So I'm thinking about reading the raw dataset line by line, process and append processed data line by line to the intermediate dataset.<\/p>\n\n<p>What I can't understand is how to make this compatible with <code>AbstractDataSet<\/code> with its <code>_load<\/code> and <code>_save<\/code> methods.<\/p>\n\n<p>P.S. I understand I can move this out of kedro's context, and introduce preprocessed dataset as a raw one, but that kinda breaks the whole idea of complete pipelines. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1582237049943,
        "Question_favorite_count":null,
        "Question_score":6.0,
        "Question_view_count":656.0,
        "Owner_creation_time":1324477592580,
        "Owner_last_access_time":1663840044463,
        "Owner_reputation":1315.0,
        "Owner_up_votes":85.0,
        "Owner_down_votes":5.0,
        "Owner_views":91.0,
        "Answer_body":"<p>Try to use pyspark to leverage lazy evaluation and batch execution. \nSparkDataSet is implemented in kedro.contib.io.spark_data_set<\/p>\n\n<p>Sample catalog config for jsonl:<\/p>\n\n<pre><code>your_dataset_name:   \n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: \"\\file_path\"\n  file_format: json\n  load_args:\n    multiline: True\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1582275656936,
        "Answer_score":4.0,
        "Owner_location":null,
        "Question_last_edit_time":1583417458307,
        "Answer_last_edit_time":1582292099020,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60329363",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to process huge datasets in ; Content: i have pretty big (~200gb, ~20m lines) raw jsonl dataset. i need to extract important properties from there and store the intermediate dataset in csv for further conversion into something like hdf5, parquet, etc. obviously, i can't use jsondataset for loading raw dataset, because it utilizes pandas.read_json under the hood, and using pandas for the dataset of such size sounds like a bad idea. so i'm thinking about reading the raw dataset line by line, process and append processed data line by line to the intermediate dataset. what i can't understand is how to make this compatible with abstractdataset with its _load and _save methods. p.s. i understand i can move this out of 's context, and introduce preprocessed dataset as a raw one, but that kinda breaks the whole idea of complete pipelines.",
        "Question_original_content_gpt_summary":"The user is facing a challenge of processing a large (~200gb, ~20m lines) raw JSONL dataset and storing the intermediate dataset in CSV for further conversion into something like HDF5 or Parquet, while also making it compatible with the AbstractDataset's _load and _save methods.",
        "Question_preprocessed_content":"Title: how to process huge datasets in ; Content: i have pretty big raw jsonl dataset. i need to extract important properties from there and store the intermediate dataset in csv for further conversion into something like hdf , parquet, etc. obviously, i can't use for loading raw dataset, because it utilizes under the hood, and using pandas for the dataset of such size sounds like a bad idea. so i'm thinking about reading the raw dataset line by line, process and append processed data line by line to the intermediate dataset. what i can't understand is how to make this compatible with with its and methods. i understand i can move this out of 's context, and introduce preprocessed dataset as a raw one, but that kinda breaks the whole idea of complete pipelines.",
        "Answer_original_content":"try to use pyspark to leverage lazy evaluation and batch execution. sparkdataset is implemented in .contib.io.spark_data_set sample catalog config for jsonl: your_dataset_name: type: .contrib.io.pyspark.sparkdataset filepath: \"\\file_path\" file_format: json load_args: multiline: true",
        "Answer_original_content_gpt_summary":"The solution to processing a large raw JSONL dataset and storing it in CSV for further conversion into something like HDF5 or Parquet, while also making it compatible with the AbstractDataset's _load and _save methods, is to use pyspark to leverage lazy evaluation and batch execution. The sparkdataset is implemented in .contib.io.spark_data_set sample catalog config for jsonl. The user can define their dataset name, file path, file format, and load arguments, including multiline.",
        "Answer_preprocessed_content":"try to use pyspark to leverage lazy evaluation and batch execution. sparkdataset is implemented in sample catalog config for jsonl"
    },
    {
        "Question_id":52003180.0,
        "Question_title":"How to understand output from a Multiclass Neural Network",
        "Question_body":"<p>Built a flow in Azure ML using a Neural network Multiclass module (for settings see picture). \n <a href=\"https:\/\/i.stack.imgur.com\/6xTYY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6xTYY.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Some more info about the Multiclass:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/lLUVC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lLUVC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The data flow is simple, split of 80\/20. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/k72ZX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/k72ZX.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Preparation of the data is made before it goes into Azure. Data looks like this: <a href=\"https:\/\/i.stack.imgur.com\/SoSUa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SoSUa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My problem comes when I want to make sense of the output and if possible transform\/calculate the output to probabilities. Output looks like this: <a href=\"https:\/\/i.stack.imgur.com\/kBZOM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kBZOM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My question: If scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? And how sure can I be that actual outcome will be a 1?<\/p>\n\n<p>Can I safely assume that a scored probabilities of 0.80 = 80% chance of outcome? Or what type of outcomes should I watch out for?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6.0,
        "Question_creation_time":1535108970810,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":263.0,
        "Owner_creation_time":1466013021616,
        "Owner_last_access_time":1550758296712,
        "Owner_reputation":103.0,
        "Owner_up_votes":6.0,
        "Owner_down_votes":0.0,
        "Owner_views":54.0,
        "Answer_body":"<p>To start with, your are in a <em>binary<\/em> classification setting, not in a multi-class one (we normally use this term when number of classes > 2).<\/p>\n\n<blockquote>\n  <p>If scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? <\/p>\n<\/blockquote>\n\n<p>In <em>practice<\/em>, the scored probabilities are routinely interpreted as the <em>confidence<\/em> of the model; so, in this example, we would say that your model has 60% confidence that the particular sample belongs to class 1 (and, complementary, 40% confidence that it belongs to class 0).<\/p>\n\n<blockquote>\n  <p>And how sure can I be that actual outcome will be a 1?<\/p>\n<\/blockquote>\n\n<p>If you don't have any alternate means of computing such outcomes yourself (e.g. a different model), I cannot see how this question is different from your previous one.<\/p>\n\n<blockquote>\n  <p>Can I safely assume that a scored probabilities of 0.80 = 80% chance of outcome? <\/p>\n<\/blockquote>\n\n<p>This is the kind of statement that would drive a professional statistician mad; nevertheless, the clarifications above regarding the confidence should be enough for your purposes (they are enough indeed for ML practitioners).<\/p>\n\n<p>My answer in <a href=\"https:\/\/stackoverflow.com\/questions\/51367755\/predict-classes-or-class-probabilities\/51423325#51423325\">Predict classes or class probabilities?<\/a> should also be helpful.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1535111519160,
        "Answer_score":3.0,
        "Owner_location":"Malm\u00f6, Sverige",
        "Question_last_edit_time":1535126329423,
        "Answer_last_edit_time":1535111877860,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52003180",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to understand output from a multiclass neural network; Content: built a flow in using a neural network multiclass module (for settings see picture). some more info about the multiclass: the data flow is simple, split of 80\/20. preparation of the data is made before it goes into azure. data looks like this: my problem comes when i want to make sense of the output and if possible transform\/calculate the output to probabilities. output looks like this: my question: if scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? and how sure can i be that actual outcome will be a 1? can i safely assume that a scored probabilities of 0.80 = 80% chance of outcome? or what type of outcomes should i watch out for?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in understanding the output from a multiclass neural network and transforming\/calculating the output to probabilities.",
        "Question_preprocessed_content":"Title: how to understand output from a multiclass neural network; Content: built a flow in using a neural network multiclass module . some more info about the multiclass the data flow is simple, split of preparation of the data is made before it goes into azure. data looks like this my problem comes when i want to make sense of the output and if possible the output to probabilities. output looks like this my question if scored probabilities output for my model is and scored labels , how sure is the model of the scored labels ? and how sure can i be that actual outcome will be a ? can i safely assume that a scored probabilities of % chance of outcome? or what type of outcomes should i watch out for?",
        "Answer_original_content":"to start with, your are in a binary classification setting, not in a multi-class one (we normally use this term when number of classes > 2). if scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? in practice, the scored probabilities are routinely interpreted as the confidence of the model; so, in this example, we would say that your model has 60% confidence that the particular sample belongs to class 1 (and, complementary, 40% confidence that it belongs to class 0). and how sure can i be that actual outcome will be a 1? if you don't have any alternate means of computing such outcomes yourself (e.g. a different model), i cannot see how this question is different from your previous one. can i safely assume that a scored probabilities of 0.80 = 80% chance of outcome? this is the kind of statement that would drive a professional statistician mad; nevertheless, the clarifications above regarding the confidence should be enough for your purposes (they are enough indeed for ml practitioners). my answer in predict classes or class probabilities? should also be helpful.",
        "Answer_original_content_gpt_summary":"The answer suggests that the user is in a binary classification setting and that the scored probabilities output from the model can be interpreted as the confidence of the model. The answer also clarifies that the scored probabilities cannot be used to predict the actual outcome with certainty and that it is not appropriate to assume that a scored probability of 0.80 equals an 80% chance of outcome. The answer recommends referring to a previous answer for more information on predicting classes or class probabilities.",
        "Answer_preprocessed_content":"to start with, your are in a binary classification setting, not in a one . if scored probabilities output for my model is and scored labels , how sure is the model of the scored labels ? in practice, the scored probabilities are routinely interpreted as the confidence of the model; so, in this example, we would say that your model has % confidence that the particular sample belongs to class . and how sure can i be that actual outcome will be a ? if you don't have any alternate means of computing such outcomes yourself , i cannot see how this question is different from your previous one. can i safely assume that a scored probabilities of % chance of outcome? this is the kind of statement that would drive a professional statistician mad; nevertheless, the clarifications above regarding the confidence should be enough for your purposes . my answer in predict classes or class probabilities? should also be helpful."
    },
    {
        "Question_id":null,
        "Question_title":"How to use parameter weight_column_name in AutoMLConfig Class for Automated Azure MAchine Learning",
        "Question_body":"Hi everybody,\ncould you please help with the weight_column_Name in the AutomLconfig how is used,and how should the syntax be for multiple columns? We are trying to run various different experiments with specific weights in some columns that we consider more serious. Can somebody provide an example of this parameter in use?\n\nKind Regards",
        "Question_answer_count":2,
        "Question_comment_count":2.0,
        "Question_creation_time":1642428938500,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/698115\/how-to-use-parameter-weight-column-name-in-automlc.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-15T17:32:25.97Z",
                "Answer_score":0,
                "Answer_body":"how do I add this setting with the autoML UI?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-15T17:42:44.437Z",
                "Answer_score":0,
                "Answer_body":"how do I set this in the auto ML UI?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to use parameter weight_column_name in automlconfig class for automated ; Content: hi everybody, could you please help with the weight_column_name in the automlconfig how is used,and how should the syntax be for multiple columns? we are trying to run various different experiments with specific weights in some columns that we consider more serious. can somebody provide an example of this parameter in use? kind regards",
        "Question_original_content_gpt_summary":"The user is seeking assistance with understanding how to use the parameter weight_column_name in the automlconfig class for automated machine learning, and how to use the syntax for multiple columns.",
        "Question_preprocessed_content":"Title: how to use parameter in automlconfig class for automated ; Content: hi everybody, could you please help with the in the automlconfig how is used,and how should the syntax be for multiple columns? we are trying to run various different experiments with specific weights in some columns that we consider more serious. can somebody provide an example of this parameter in use? kind regards",
        "Answer_original_content":"how do i add this setting with the automl ui? how do i set this in the auto ml ui?",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer. The answer only repeats the question in a slightly different way.",
        "Answer_preprocessed_content":"how do i add this setting with the automl ui? how do i set this in the auto ml ui?"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning Errors",
        "Question_body":"Hello, I am trying to run some of the sample notebooks from Microsoft Docs for Azure Machine Learning. I am running into the following error and cannot find a workaround though this appears to be a common error that others have also encountered with no workaround. cannot import name 'AzureMLAggregatedException' from 'azureml.exceptions'",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1614300692297,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/289691\/azure-machine-learning-errors.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-05T02:51:15.36Z",
                "Answer_score":0,
                "Answer_body":"Hi,\nIt is the explanation dashboards and the fairness dashboards and it cannot find the Lime library or its methods.\nRegards,\nAmy",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: errors; Content: hello, i am trying to run some of the sample notebooks from microsoft docs for . i am running into the following error and cannot find a workaround though this appears to be a common error that others have also encountered with no workaround. cannot import name 'aggregatedexception' from '.exceptions'",
        "Question_original_content_gpt_summary":"The user is encountering an error while trying to run sample notebooks from Microsoft docs and is unable to find a workaround for the \"aggregatedexception\" import error.",
        "Question_preprocessed_content":"Title: errors; Content: hello, i am trying to run some of the sample notebooks from microsoft docs for . i am running into the following error and cannot find a workaround though this appears to be a common error that others have also encountered with no workaround. cannot import name 'aggregatedexception' from",
        "Answer_original_content":"hi, it is the explanation dashboards and the fairness dashboards and it cannot find the lime library or its methods. regards, amy",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer for the \"aggregatedexception\" import error. The answer seems to be addressing a different issue related to the inability to find the lime library or its methods while running explanation and fairness dashboards.",
        "Answer_preprocessed_content":"hi, it is the explanation dashboards and the fairness dashboards and it cannot find the lime library or its methods. regards, amy"
    },
    {
        "Question_id":59833019.0,
        "Question_title":"memory error while writing a large dataframe to S3 AWS",
        "Question_body":"<p>I have created a dataframe with the following shape using amazon sagemaker.<\/p>\n\n<pre><code>10612611 rows \u00d7 4 columns\n<\/code><\/pre>\n\n<p>All are numeric values.\n When I am trying to write this dataframe into my S3 bucket as follows, I get memory error.<\/p>\n\n<pre><code>bytes_to_write = df.to_csv(None).encode()\nwith s3.open('aws-athena-query-results-xxxxxxx\/query_result\/xx.csv','wb') as f:\n    f.write(bytes_to_write)\n<\/code><\/pre>\n\n<blockquote>\n  <p>MemoryError:<\/p>\n<\/blockquote>\n\n<p>I am using <strong>ml.t2.medium<\/strong> for sagemaker instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1579570132453,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":414.0,
        "Owner_creation_time":1497488105307,
        "Owner_last_access_time":1663902465110,
        "Owner_reputation":1237.0,
        "Owner_up_votes":214.0,
        "Owner_down_votes":5.0,
        "Owner_views":376.0,
        "Answer_body":"<p>I solved this issue by changing the instance type from <strong>ml.t2.medium<\/strong> to <strong>ml.t2.2xlarge<\/strong> and it worked perfectly.<\/p>\n\n<p>The original issue was with the RAM of the instance type and not with S3.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1579577379867,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59833019",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: memory error while writing a large dataframe to s3 aws; Content: i have created a dataframe with the following shape using . 10612611 rows \u00d7 4 columns all are numeric values. when i am trying to write this dataframe into my s3 bucket as follows, i get memory error. bytes_to_write = df.to_csv(none).encode() with s3.open('aws-athena-query-results-xxxxxxx\/query_result\/xx.csv','wb') as f: f.write(bytes_to_write) memoryerror: i am using ml.t2.medium for instance.",
        "Question_original_content_gpt_summary":"The user is encountering a MemoryError while attempting to write a large dataframe (10612611 rows \u00d7 4 columns) of numeric values to an S3 bucket using an ml.t2.medium instance.",
        "Question_preprocessed_content":"Title: memory error while writing a large dataframe to s aws; Content: i have created a dataframe with the following shape using . all are numeric values. when i am trying to write this dataframe into my s bucket as follows, i get memory error. memoryerror i am using for instance.",
        "Answer_original_content":"i solved this issue by changing the instance type from ml.t2.medium to ml.t2.2xlarge and it worked perfectly. the original issue was with the ram of the instance type and not with s3.",
        "Answer_original_content_gpt_summary":"The solution to the MemoryError encountered while writing a large dataframe to an S3 bucket using an ml.t2.medium instance is to change the instance type to ml.t2.2xlarge, which has more RAM. The issue is not with S3 but with the RAM of the instance type.",
        "Answer_preprocessed_content":"i solved this issue by changing the instance type from to and it worked perfectly. the original issue was with the ram of the instance type and not with s ."
    },
    {
        "Question_id":null,
        "Question_title":"How to continually update a model with new data",
        "Question_body":"<p>I have a reinforcement learning use case with the following initial steps:<\/p>\n<ol>\n<li>Construct initial episodes<\/li>\n<li>Add episodes to initial experience replay buffer<\/li>\n<li>Train initial model<\/li>\n<\/ol>\n<p>Then the following steps which repeat every day:<\/p>\n<ol start=\"4\">\n<li>Construct most recent time steps<\/li>\n<li>Add new time steps to existing experience replay buffer<\/li>\n<li>Fine-tune existing model<\/li>\n<li>Compare existing model and fine-tuned model and keep the one that\u2019s best<\/li>\n<\/ol>\n<p>What would be the best way to do this using DVC? Is it even possible given that circular dependencies are not allowed?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1644516069762,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":173.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-continually-update-a-model-with-new-data\/1055",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-11T01:35:31.741Z",
                "Answer_body":"<p>You can use <code>persist: true<\/code> to prevent outputs from being deleted during stage execution (see <a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#description\">https:\/\/dvc.org\/doc\/command-reference\/repro#description<\/a>), so you can feed in new data each day and have the subsequent stages read from their existing outputs before writing out newly modified versions.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-14T17:10:19.893Z",
                "Answer_body":"<p>Thanks! I think that should work<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to continually update a model with new data; Content: i have a reinforcement learning use case with the following initial steps: construct initial episodes add episodes to initial experience replay buffer train initial model then the following steps which repeat every day: construct most recent time steps add new time steps to existing experience replay buffer fine-tune existing model compare existing model and fine-tuned model and keep the one that\u2019s best what would be the best way to do this using ? is it even possible given that circular dependencies are not allowed?",
        "Question_original_content_gpt_summary":"The user is looking for the best way to continually update a reinforcement learning model with new data while avoiding circular dependencies.",
        "Question_preprocessed_content":"Title: how to continually update a model with new data; Content: i have a reinforcement learning use case with the following initial steps construct initial episodes add episodes to initial experience replay buffer train initial model then the following steps which repeat every day construct most recent time steps add new time steps to existing experience replay buffer existing model compare existing model and model and keep the one thats best what would be the best way to do this using ? is it even possible given that circular dependencies are not allowed?",
        "Answer_original_content":"you can use persist: true to prevent outputs from being deleted during stage execution (see https:\/\/.org\/doc\/command-reference\/repro#description), so you can feed in new data each day and have the subsequent stages read from their existing outputs before writing out newly modified versions. thanks! i think that should work",
        "Answer_original_content_gpt_summary":"The solution to continually update a reinforcement learning model with new data while avoiding circular dependencies is to use persist: true to prevent outputs from being deleted during stage execution. This allows for feeding in new data each day and having subsequent stages read from their existing outputs before writing out newly modified versions.",
        "Answer_preprocessed_content":"you can use to prevent outputs from being deleted during stage execution , so you can feed in new data each day and have the subsequent stages read from their existing outputs before writing out newly modified versions. thanks! i think that should work"
    },
    {
        "Question_id":null,
        "Question_title":"Why does Jupyter Notebooks not recognize changes in my .py files?",
        "Question_body":"Hello, When working locally, I usually put routine tasks inside functions held in a .py file and import those.  When I need to make a change, I change the function, reimport and moving on with the main script.   GCP's jupyter instance does not recognize when I make the change an re-import the function.  I have to restart the kernel each time.  Is there a way around this?",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1652095440000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":20.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Why-does-Jupyter-Notebooks-not-recognize-changes-in-my-py-files\/td-p\/421489\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-09T11:24:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello,\u00a0\n\nWhen working locally, I usually put routine tasks inside functions held in a .py file and import those.\u00a0 When I need to make a change, I change the function, reimport and moving on with the main script.\u00a0\u00a0\n\n\u00a0\n\nGCP's jupyter instance does not recognize when I make the change an re-import the function.\u00a0 I have to restart the kernel each time.\u00a0 Is there a way around this?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why does jupyter notebooks not recognize changes in my .py files?; Content: hello, when working locally, i usually put routine tasks inside functions held in a .py file and import those. when i need to make a change, i change the function, reimport and moving on with the main script. gcp's jupyter instance does not recognize when i make the change an re-import the function. i have to restart the kernel each time. is there a way around this?",
        "Question_original_content_gpt_summary":"The user is encountering an issue where Jupyter Notebooks does not recognize changes made to their .py files, requiring them to restart the kernel each time.",
        "Question_preprocessed_content":"Title: why does jupyter notebooks not recognize changes in my .py files?; Content: hello, when working locally, i usually put routine tasks inside functions held in a .py file and import those. when i need to make a change, i change the function, reimport and moving on with the main script. gcp's jupyter instance does not recognize when i make the change an the function. i have to restart the kernel each time. is there a way around this?",
        "Answer_original_content":"hello, when working locally, i usually put routine tasks inside functions held in a .py file and import those. when i need to make a change, i change the function, reimport and moving on with the main script. gcp's jupyter instance does not recognize when i make the change an re-import the function. i have to restart the kernel each time. is there a way around this?",
        "Answer_original_content_gpt_summary":"The possible solution to the issue of Jupyter Notebooks not recognizing changes made to .py files is to put routine tasks inside functions held in a .py file and import those. When a change is made to the function, it can be reimported and the main script can continue without having to restart the kernel. However, this solution may not work for Jupyter instances on GCP as they may not recognize the changes even after reimporting the function.",
        "Answer_preprocessed_content":"hello, when working locally, i usually put routine tasks inside functions held in a .py file and import those. when i need to make a change, i change the function, reimport and moving on with the main script. gcp's jupyter instance does not recognize when i make the change an the function. i have to restart the kernel each time. is there a way around this?"
    },
    {
        "Question_id":50113374.0,
        "Question_title":"What is a trait in Azure ML matchbox recommender?",
        "Question_body":"<p>Azure Machine Learning has an item called <code>Train Matchbox Recommender<\/code>. It can be configured with a <code>Number of traits<\/code>. Unfortunately, the documentation does not describe what such a trait is.<\/p>\n\n<p>What are traits? Is this related to <a href=\"https:\/\/en.wikipedia.org\/wiki\/Latent_variable\" rel=\"nofollow noreferrer\">latent variables<\/a>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1525162879793,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":859.0,
        "Owner_creation_time":1290750251812,
        "Owner_last_access_time":1663965312343,
        "Owner_reputation":55864.0,
        "Owner_up_votes":1621.0,
        "Owner_down_votes":31.0,
        "Owner_views":4960.0,
        "Answer_body":"<p><a href=\"http:\/\/apprize.info\/microsoft\/azure_1\/9.html\" rel=\"nofollow noreferrer\">This<\/a> page may have better descriptions on it.<\/p>\n\n<p>Basically, traits are the features the algorithm will learn about each user related to each item. For example, in the <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Recommender-Restaurant-ratings-2\" rel=\"nofollow noreferrer\">restaurant ratings recommender<\/a> traits could include a user's birth year, if they're a student or working professional, martial status, etc.<\/p>\n\n<p>Hope that helps!<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1525171563376,
        "Answer_score":1.0,
        "Owner_location":"Nimes, France",
        "Question_last_edit_time":1526046231816,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50113374",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what is a trait in matchbox recommender?; Content: has an item called train matchbox recommender. it can be configured with a number of traits. unfortunately, the documentation does not describe what such a trait is. what are traits? is this related to latent variables?",
        "Question_original_content_gpt_summary":"The user is encountering difficulty understanding what a trait is in the Matchbox Recommender and whether it is related to latent variables.",
        "Question_preprocessed_content":"Title: what is a trait in matchbox recommender?; Content: has an item called . it can be configured with a . unfortunately, the documentation does not describe what such a trait is. what are traits? is this related to latent variables?",
        "Answer_original_content":"this page may have better descriptions on it. basically, traits are the features the algorithm will learn about each user related to each item. for example, in the restaurant ratings recommender traits could include a user's birth year, if they're a student or working professional, martial status, etc. hope that helps!",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Visit the page suggested for better descriptions on traits in the Matchbox Recommender.\n- Understand that traits are the features the algorithm learns about each user related to each item.\n- Consider examples of traits in the restaurant ratings recommender, such as a user's birth year, student or working professional status, and martial status. \n\nSummary: The answer provides possible solutions to the user's difficulty in understanding traits in the Matchbox Recommender. The solutions include visiting a suggested page for better descriptions, understanding that traits are features learned by the algorithm, and considering examples of traits in a restaurant ratings recommender.",
        "Answer_preprocessed_content":"this page may have better descriptions on it. basically, traits are the features the algorithm will learn about each user related to each item. for example, in the restaurant ratings recommender traits could include a user's birth year, if they're a student or working professional, martial status, etc. hope that helps!"
    },
    {
        "Question_id":null,
        "Question_title":"Sweep over pre-training, then sweep over finetuning",
        "Question_body":"<p>I\u2019d like to pre-train a model under several different conditions, then finetune each of those resulting models. The simple way to do this would be two separate sweeps. But then I need to manually start the second one. Is there a way to combine these into one single sweep?<\/p>\n<p>With a bash script, I can simply call pre-train and finetune in sequence, passing the respective arguments via a config file. Is it somehow possible to tell the respective scripts that they are part of the same sweep and should therefore use certain parameters?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1646399505042,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":164.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-over-pre-training-then-sweep-over-finetuning\/2015",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-20T20:53:36.012Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/sgerard\">@sgerard<\/a>,<\/p>\n<p>I don\u2019t believe so. A sweep is inherently tied to its config, so there is no way to have 2 different configs - one for pre-training and one for fine-tuning.<\/p>\n<p>The only good way to do this currently is to have 2 sweeps to do this.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-27T19:16:58.009Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/sgerard\">@sgerard<\/a>,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-02T16:27:18.273Z",
                "Answer_body":"<p>Hi Sebastian, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-19T20:53:49.540Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sweep over pre-training, then sweep over finetuning; Content: i\u2019d like to pre-train a model under several different conditions, then finetune each of those resulting models. the simple way to do this would be two separate sweeps. but then i need to manually start the second one. is there a way to combine these into one single sweep? with a bash script, i can simply call pre-train and finetune in sequence, passing the respective arguments via a config file. is it somehow possible to tell the respective scripts that they are part of the same sweep and should therefore use certain parameters?",
        "Question_original_content_gpt_summary":"The user is looking for a way to combine two separate sweeps into one single sweep in order to pre-train a model under several different conditions and then finetune each of those resulting models.",
        "Question_preprocessed_content":"Title: sweep over then sweep over finetuning; Content: id like to a model under several different conditions, then finetune each of those resulting models. the simple way to do this would be two separate sweeps. but then i need to manually start the second one. is there a way to combine these into one single sweep? with a bash script, i can simply call and finetune in sequence, passing the respective arguments via a config file. is it somehow possible to tell the respective scripts that they are part of the same sweep and should therefore use certain parameters?",
        "Answer_original_content":"hi @sgerard, i dont believe so. a sweep is inherently tied to its config, so there is no way to have 2 different configs - one for pre-training and one for fine-tuning. the only good way to do this currently is to have 2 sweeps to do this. thanks, ramit hi @sgerard, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi sebastian, since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer to the question. The answer is a follow-up message from a support team member to the user who had previously contacted them.",
        "Answer_preprocessed_content":"hi i dont believe so. a sweep is inherently tied to its config, so there is no way to have different configs one for and one for the only good way to do this currently is to have sweeps to do this. thanks, ramit hi we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi sebastian, since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Integrate Azure ML Model with Power BI by using Python",
        "Question_body":"Dear Sir\/ Madam,\nWhile I was trying to integrate Azure ML Model(classic) with Power BI I searched for Python script but was not able to find any code or video regarding the integration through Python.\n\nIf there is any way to integrate them please share.\n\nRegards,\nNikhil.",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1612444605970,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/258899\/integrate-azure-ml-model-with-power-bi-by-using-py.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-06T02:56:39.573Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nThanks for reaching out to us. I am not sure what kind of Python script you are looking for.\n\nBut I do have posted the guidance of how to integrate machine learning model with PowerBI below. Please let me know more about your requirement so that we can help to find out the best guidance.\n\nhttps:\/\/docs.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: integrate model with power bi by using python; Content: dear sir\/ madam, while i was trying to integrate model(classic) with power bi i searched for python script but was not able to find any code or video regarding the integration through python. if there is any way to integrate them please share. regards, nikhil.",
        "Question_original_content_gpt_summary":"The user, Nikhil, is trying to integrate a classic model with Power BI using Python, but is having difficulty finding any code or video to help with the process.",
        "Question_preprocessed_content":"Title: integrate model with power bi by using python; Content: dear sir\/ madam, while i was trying to integrate model with power bi i searched for python script but was not able to find any code or video regarding the integration through python. if there is any way to integrate them please share. regards, nikhil.",
        "Answer_original_content":"hello, thanks for reaching out to us. i am not sure what kind of python script you are looking for. but i do have posted the guidance of how to integrate machine learning model with powerbi below. please let me know more about your requirement so that we can help to find out the best guidance. https:\/\/docs.microsoft.com\/en-us\/power-bi\/connect-data\/service-aml-integrate regards, yutong",
        "Answer_original_content_gpt_summary":"The answer provides a link to Microsoft's guidance on integrating machine learning models with Power BI. The user is encouraged to provide more information about their specific requirements so that they can receive more tailored guidance.",
        "Answer_preprocessed_content":"hello, thanks for reaching out to us. i am not sure what kind of python script you are looking for. but i do have posted the guidance of how to integrate machine learning model with powerbi below. please let me know more about your requirement so that we can help to find out the best guidance. regards, yutong"
    },
    {
        "Question_id":69689266.0,
        "Question_title":"How to set a tag at the experiment level in MLFlow",
        "Question_body":"<p>I can see that an experiment in MLFlow can have tags (like runs can have tags).\nI'm able to set a run's tag using <code>mlflow.set_tag<\/code>, but how do I set it for an experiment?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1635000756820,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":1047.0,
        "Owner_creation_time":1244984040076,
        "Owner_last_access_time":1663968051750,
        "Owner_reputation":13408.0,
        "Owner_up_votes":306.0,
        "Owner_down_votes":12.0,
        "Owner_views":687.0,
        "Answer_body":"<p>If you look into the Python API, the very <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html\" rel=\"nofollow noreferrer\">first example<\/a> in <code>mlflow.tracking package<\/code> that shows how to create the <code>MLflowClient<\/code> is really showing how to tag experiment using the <code>client.set_experiment_tag<\/code> function (<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_experiment_tag\" rel=\"nofollow noreferrer\">doc<\/a>):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow.tracking import MlflowClient\n\n# Create an experiment with a name that is unique and case sensitive.\nclient = MlflowClient()\nexperiment_id = client.create_experiment(&quot;Social NLP Experiments&quot;)\nclient.set_experiment_tag(experiment_id, &quot;nlp.framework&quot;, &quot;Spark NLP&quot;)\n<\/code><\/pre>\n<p>you can also set it for model version with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.client.MlflowClient.set_model_version_tag\" rel=\"nofollow noreferrer\">set_model_version_tag<\/a> function, and for registered model with <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.client.html#mlflow.tracking.MlflowClient.set_registered_model_tag\" rel=\"nofollow noreferrer\">set_registered_model_tag<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1635012053310,
        "Answer_score":3.0,
        "Owner_location":"New York, NY",
        "Question_last_edit_time":1635005765867,
        "Answer_last_edit_time":1663958792436,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69689266",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to set a tag at the experiment level in ; Content: i can see that an experiment in can have tags (like runs can have tags). i'm able to set a run's tag using .set_tag, but how do i set it for an experiment?",
        "Question_original_content_gpt_summary":"The user is trying to figure out how to set a tag at the experiment level in .",
        "Question_preprocessed_content":"Title: how to set a tag at the experiment level in ; Content: i can see that an experiment in can have tags . i'm able to set a run's tag using , but how do i set it for an experiment?",
        "Answer_original_content":"if you look into the python api, the very first example in .tracking package that shows how to create the client is really showing how to tag experiment using the client.set_experiment_tag function (doc): from .tracking import client # create an experiment with a name that is unique and case sensitive. client = client() experiment_id = client.create_experiment(\"social nlp experiments\") client.set_experiment_tag(experiment_id, \"nlp.framework\", \"spark nlp\") you can also set it for model version with set_model_version_tag function, and for registered model with set_registered_model_tag.",
        "Answer_original_content_gpt_summary":"The answer suggests that to set a tag at the experiment level in .tracking package, the user can use the client.set_experiment_tag function from the python api. The example provided shows how to create an experiment with a unique and case-sensitive name, and set a tag for it. Additionally, the answer mentions that tags can also be set for model versions and registered models using set_model_version_tag and set_registered_model_tag functions respectively.",
        "Answer_preprocessed_content":"if you look into the python api, the very first example in that shows how to create the is really showing how to tag experiment using the function you can also set it for model version with function, and for registered model with"
    },
    {
        "Question_id":null,
        "Question_title":"PermissionDeniedError when trying to save a Tensorflow model checkpoint",
        "Question_body":"Hi,\n\nWe are using the Azure Machine Learning Studio to run pipelines in which computer vision models are trained using Tensorflow (v2.4.0).\nOur input data (images & annotations) are stored on our Azure Blob Storage account.\nThe saved models are also saved to the same Azure Blob Storage account\n\nWe have several different pipelines (for different projects) that all worked perfectly fine for the last months... up until last week.\nEvery pipeline (that worked before) results in the exact same error now.\nEverything (image loading, preprocessing, augmentations, ...) works fine.\nThe training step starts and the first epoch is trained.\nHowever after the first epoch is done training, the error occurs.\n\ntensorflow\/core\/framework\/op_kernel.cc:1763] OP_REQUIRES failed at save_restore_v2_ops.cc:157 : Permission denied: \/mnt\/azureml\/cr\/j\/0ddbaa5dfd4243c4bd18feabd6037209\/cap\/data-capability\/wd\/output_84f84eab_univision_ai\/pc_ds_2021_v3\/refinement-reg\/v2\/results\/128_c1720fb5-81ed-45aa-a823-a1fa5ef1a8d1\/export\/saved_model\/variables\/variables_temp\/part-00000-of-00001.data-00000-of-00001.tempstate5255274353572806690; Read-only file system\n...\nEpoch 00001: val_loss improved from inf to 0.10273, saving model to \/mnt\/azureml\/cr\/j\/0ddbaa5dfd4243c4bd18feabd6037209\/cap\/data-capability\/wd\/output_84f84eab_univision_ai\/pc_ds_2021_v3\/refinement-reg\/v2\/results\/128_c1720fb5-81ed-45aa-a823-a1fa5ef1a8d1\/export\/saved_model Cleaning up all outstanding Run operations, waiting 300.0 seconds 2 items cleaning up... Cleanup took 0.19626808166503906 seconds\n...\ntensorflow.python.framework.errors_impl.PermissionDeniedError: \/mnt\/azureml\/cr\/j\/0ddbaa5dfd4243c4bd18feabd6037209\/cap\/data-capability\/wd\/output_84f84eab_univision_ai\/pc_ds_2021_v3\/refinement-reg\/v2\/results\/128_c1720fb5-81ed-45aa-a823-a1fa5ef1a8d1\/export\/saved_model\/variables\/variables_temp\/part-00000-of-00001.data-00000-of-00001.tempstate5255274353572806690; Read-only file system [Op:SaveV2]\n\nWe get a PermissionDeniedError while trying to save a temporary file, apparently because it's a read-only file system.\nIf we take a look at this temporary file in the Azure storage account there is nothing that points out it would be read-only.\nThere's also no difference in settings between this file and other files that we were able to read\/write.\n\nI have already been able to find what direction to search in.\nIn the training pipeline step we have always been using Model Checkpoint (if the validation is better than the current saved checkpoint, the model checkpoint is saved).\nBy deleting the Model Checkpoint callback, the error does not occur.\nThis is not a solution of course, as we do need these model checkpoints.\nIt does show however that it has something to do with these model checkpoints.\n\nI am not sure what else I can try to solve this issue.\n\nKind regards",
        "Question_answer_count":2,
        "Question_comment_count":12.0,
        "Question_creation_time":1643818966130,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/720040\/permissiondeniederror-when-trying-to-save-a-tensor.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-08T13:36:39.523Z",
                "Answer_score":1,
                "Answer_body":"I just had a call with a Support Engineer of Microsoft and they solved the issue.\nApparently they are implementing a new version of AzureML Compute Runtime.\nMy problem was solved by reversing to an older version of this.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-25T01:39:46.117Z",
                "Answer_score":2,
                "Answer_body":"Hi @MichielVANACKER-8830, thank you for the report.\nThis issue is indeed caused by the new mounting solution in the new runtime. While being way faster and more performant it turned out there is a missing support for a rename operation on the mounted drive. Tensorflow saves model checkpoint to a temp file and then attempts to rename it and fails. Error message is very misleading and was in part a reason why it took us so long to root cause it.\n\nWe are disabling new mounting solution on our side and will reenable it once we validate support. Thank you for your patience and do not hesitate to reach out if you see any future data access issues.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":18.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: permissiondeniederror when trying to save a tensorflow model checkpoint; Content: hi, we are using the studio to run pipelines in which computer vision models are trained using tensorflow (v2.4.0). our input data (images & annotations) are stored on our azure blob storage account. the saved models are also saved to the same azure blob storage account we have several different pipelines (for different projects) that all worked perfectly fine for the last months... up until last week. every pipeline (that worked before) results in the exact same error now. everything (image loading, preprocessing, augmentations, ...) works fine. the training step starts and the first epoch is trained. however after the first epoch is done training, the error occurs. tensorflow\/core\/framework\/op_kernel.cc:1763] op_requires failed at save_restore_v2_ops.cc:157 : permission denied: \/mnt\/\/cr\/j\/0ddbaa5dfd4243c4bd18feabd6037209\/cap\/data-capability\/wd\/output_84f84eab_univision_ai\/pc_ds_2021_v3\/refinement-reg\/v2\/results\/128_c1720fb5-81ed-45aa-a823-a1fa5ef1a8d1\/export\/saved_model\/variables\/variables_temp\/part-00000-of-00001.data-00000-of-00001.tempstate5255274353572806690; read-only file system ... epoch 00001: val_loss improved from inf to 0.10273, saving model to \/mnt\/\/cr\/j\/0ddbaa5dfd4243c4bd18feabd6037209\/cap\/data-capability\/wd\/output_84f84eab_univision_ai\/pc_ds_2021_v3\/refinement-reg\/v2\/results\/128_c1720fb5-81ed-45aa-a823-a1fa5ef1a8d1\/export\/saved_model cleaning up all outstanding run operations, waiting 300.0 seconds 2 items cleaning up... cleanup took 0.19626808166503906 seconds ... tensorflow.python.framework.errors_impl.permissiondeniederror: \/mnt\/\/cr\/j\/0ddbaa5dfd4243c4bd18feabd6037209\/cap\/data-capability\/wd\/output_84f84eab_univision_ai\/pc_ds_2021_v3\/refinement-reg\/v2\/results\/128_c1720fb5-81ed-45aa-a823-a1fa5ef1a8d1\/export\/saved_model\/variables\/variables_temp\/part-00000-of-00001.data-00000-of-00001.tempstate5255274353572806690; read-only file system [op:savev2] we get a permissiondeniederror while trying to save a temporary file, apparently because it's a read-only file system. if we take a look at this temporary file in the azure storage account there is nothing that points out it would be read-only. there's also no difference in settings between this file and other files that we were able to read\/write. i have already been able to find what direction to search in. in the training pipeline step we have always been using model checkpoint (if the validation is better than the current saved checkpoint, the model checkpoint is saved). by deleting the model checkpoint callback, the error does not occur. this is not a solution of course, as we do need these model checkpoints. it does show however that it has something to do with these model checkpoints. i am not sure what else i can try to solve this issue. kind regards",
        "Question_original_content_gpt_summary":"The user is encountering a PermissionDeniedError when trying to save a TensorFlow model checkpoint, which appears to be due to a read-only file system.",
        "Question_preprocessed_content":"Title: permissiondeniederror when trying to save a tensorflow model checkpoint; Content: hi, we are using the studio to run pipelines in which computer vision models are trained using tensorflow . our input data are stored on our azure blob storage account. the saved models are also saved to the same azure blob storage account we have several different pipelines that all worked perfectly fine for the last up until last week. every pipeline results in the exact same error now. everything works fine. the training step starts and the first epoch is trained. however after the first epoch is done training, the error occurs. failed at permission denied file system epoch improved from inf to saving model to cleaning up all outstanding run operations, waiting seconds items cleaning cleanup took seconds file system we get a permissiondeniederror while trying to save a temporary file, apparently because it's a file system. if we take a look at this temporary file in the azure storage account there is nothing that points out it would be there's also no difference in settings between this file and other files that we were able to i have already been able to find what direction to search in. in the training pipeline step we have always been using model checkpoint . by deleting the model checkpoint callback, the error does not occur. this is not a solution of course, as we do need these model checkpoints. it does show however that it has something to do with these model checkpoints. i am not sure what else i can try to solve this issue. kind regards",
        "Answer_original_content":"i just had a call with a support engineer of microsoft and they solved the issue. apparently they are implementing a new version of compute runtime. my problem was solved by reversing to an older version of this. hi @michielvanacker-8830, thank you for the report. this issue is indeed caused by the new mounting solution in the new runtime. while being way faster and more performant it turned out there is a missing support for a rename operation on the mounted drive. tensorflow saves model checkpoint to a temp file and then attempts to rename it and fails. error message is very misleading and was in part a reason why it took us so long to root cause it. we are disabling new mounting solution on our side and will reenable it once we validate support. thank you for your patience and do not hesitate to reach out if you see any future data access issues.",
        "Answer_original_content_gpt_summary":"Possible solutions to the PermissionDeniedError encountered when trying to save a TensorFlow model checkpoint due to a read-only file system are to reverse to an older version of the compute runtime or to wait for the new mounting solution to be validated for support. The error message is misleading and the issue is caused by a missing support for a rename operation on the mounted drive. The support team is disabling the new mounting solution and will re-enable it once validated.",
        "Answer_preprocessed_content":"i just had a call with a support engineer of microsoft and they solved the issue. apparently they are implementing a new version of compute runtime. my problem was solved by reversing to an older version of this. hi thank you for the report. this issue is indeed caused by the new mounting solution in the new runtime. while being way faster and more performant it turned out there is a missing support for a rename operation on the mounted drive. tensorflow saves model checkpoint to a temp file and then attempts to rename it and fails. error message is very misleading and was in part a reason why it took us so long to root cause it. we are disabling new mounting solution on our side and will reenable it once we validate support. thank you for your patience and do not hesitate to reach out if you see any future data access issues."
    },
    {
        "Question_id":null,
        "Question_title":"Wandb has no attribute login error",
        "Question_body":"<p>For some reason, I\u2019m getting an error when I run wandb in a notebook that used to run fine. My error is:<\/p>\n<p>AttributeError                            Traceback (most recent call last)<br>\nInput In [11], in &lt;cell line: 1&gt;()<br>\n----&gt; 1 wandb.login(key=\u2018xxx\u2019)<\/p>\n<p>AttributeError: module \u2018wandb\u2019 has no attribute \u2018login\u2019<\/p>\n<p>Has anyone else encountered this?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1663548044682,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":356.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-has-no-attribute-login-error\/3146",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-19T01:04:58.020Z",
                "Answer_body":"<p>I was able to resolve this by uninstalling wandb and reinstalling the latest package. I\u2019m still not sure what caused it to begin with.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-19T20:36:51.987Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/havocy28\">@havocy28<\/a> , thank-you for letting us know your issue resolved with a reinstall. It\u2019s difficult to say why that error occurred, however, if it does repeat, please reach out again and we\u2019ll take a closer look.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-18T20:37:31.865Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: has no attribute login error; Content: for some reason, i\u2019m getting an error when i run in a notebook that used to run fine. my error is: attributeerror traceback (most recent call last) input in [11], in <cell line: 1>() ----> 1 .login(key=\u2018xxx\u2019) attributeerror: module \u2018\u2019 has no attribute \u2018login\u2019 has anyone else encountered this?",
        "Question_original_content_gpt_summary":"The user has encountered an AttributeError when running a notebook that used to run fine, with the error message \"AttributeError: module '' has no attribute 'login'.\"",
        "Question_preprocessed_content":"Title: has no attribute login error; Content: for some reason, im getting an error when i run in a notebook that used to run fine. my error is attributeerror traceback input in , in attributeerror module has no attribute login has anyone else encountered this?",
        "Answer_original_content":"i was able to resolve this by uninstalling and reinstalling the latest package. im still not sure what caused it to begin with. hi @havocy28 , thank-you for letting us know your issue resolved with a reinstall. its difficult to say why that error occurred, however, if it does repeat, please reach out again and well take a closer look. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The solution to the AttributeError issue is to uninstall and reinstall the latest package. The cause of the error is unknown, but if it repeats, the user should reach out for further assistance.",
        "Answer_preprocessed_content":"i was able to resolve this by uninstalling and reinstalling the latest package. im still not sure what caused it to begin with. hi , for letting us know your issue resolved with a reinstall. its difficult to say why that error occurred, however, if it does repeat, please reach out again and well take a closer look. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Get best model from artifacts",
        "Question_body":"<p>Hi,<\/p>\n<p>I guess this use-case is common but I cannot figure it out\u2026<\/p>\n<p>I would like to log one model per run, and in the end, be able to load the best overall model for production.<\/p>\n<p>So like :<br>\nRun 1,2,3,4,5\u2026<\/p>\n<pre><code class=\"lang-auto\">run.log_artifact(my_model_artifact)\n<\/code><\/pre>\n<p>Production:<\/p>\n<pre><code class=\"lang-auto\">artifact = api.artifact.get_best_of_all_my_runs()\n<\/code><\/pre>\n<p>For now my solution is :<\/p>\n<pre><code class=\"lang-auto\">Runs : \nartifact.save() # with the same name so only one artifact for all runs\n\nProduction\nartifact = api.artifact(\"entity\/project\/artifact:alias\") # Get the only model (which also should be the best)\n<\/code><\/pre>\n<p>Thanks in advance for any help.<br>\nhave a great day<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1634323325709,
        "Question_favorite_count":null,
        "Question_score":5.0,
        "Question_view_count":322.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/get-best-model-from-artifacts\/992",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-18T23:37:22.999Z",
                "Answer_body":"<p>Hi there! You can do this in a couple of ways but you don\u2019t have to limit yourself to logging just one artifact. A common flow would be to log a model checkpoint every but then to also log a \u201cbest model\u201d artifact. Since artifacts are versioned you don\u2019t have to worry about renaming the new \u201cbest model\u201d artifact. Then at the end of your run you not only have an artifact history of your model at each of the checkpoints but also a versioned history of all the best models.<\/p>",
                "Answer_score":97.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-19T17:22:20.423Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a>,<\/p>\n<p>Thanks a lot for the reply !<\/p>\n<p>I indeed do this and have my <code>best model<\/code> for each run and it works great.<\/p>\n<p>Then I can see all my runs in the <code>Table<\/code> Tab, with their hyperparameters, and a column for the score\/accuracy.<\/p>\n<p>However my questions is : Could I get the best model among all my runs so I can use it in production ?<\/p>\n<p>Let say I have one parameter P :<br>\nI run Run1 with P1, get Accuracy A1 \/\/ Run 2 with P2 get Accuracy A2 \/\/ Run3 with P3 get Accurary A3<\/p>\n<p>Then in the table I can see Run1, Run2 and Run3 each with logs \/ metrics \/ models \/ Parameters.<\/p>\n<p>I would like to be able to do in my production\/inference code :<br>\n\u201cSelect best Accuracy model from Run1, Run2, Run3 so I can use the best model in inference mode in production\u201d<\/p>\n<p>I hope my explanation was clear\u2026<br>\nThanks a lot for your time.,<\/p>\n<p>Have a great day<\/p>",
                "Answer_score":2.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-22T19:28:10.719Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ierezell\">@ierezell<\/a>,<\/p>\n<p>I think I understand your question a little better. We are currently working on a feature that will make exactly what you\u2019re asking for super clean and easy <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>In the meantime the most straightforward way of going about this would be to save the performance of all of the metrics you care about to the <code>run.summary<\/code> of the run that produced that model. Then from the <code>api<\/code> you can query all the runs in the project and select the best run using the metrics you set in the summary. Then you can get the download the best model and use it as an input to your production code.<\/p>\n<p>Here is a <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#download-the-best-model-file-from-a-sweep\">related example<\/a> querying the best model from a sweep but the process will be slightly different for you use case.<\/p>",
                "Answer_score":22.4,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-10-25T14:07:41.400Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a>,<\/p>\n<p>Thanks for the reply, this is indeed what I was looking for ! <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>I will be glad to test the new feature (even in beta) but I\u2019m sure you will communicate when it will be available.<br>\nElse I was thinking about using the API and you just confirmed it so I will be headed for that.<\/p>\n<p>Have a great day.<\/p>",
                "Answer_score":47.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: get best model from artifacts; Content: hi, i guess this use-case is common but i cannot figure it out\u2026 i would like to log one model per run, and in the end, be able to load the best overall model for production. so like : run 1,2,3,4,5\u2026 run.log_artifact(my_model_artifact) production: artifact = api.artifact.get_best_of_all_my_runs() for now my solution is : runs : artifact.save() # with the same name so only one artifact for all runs production artifact = api.artifact(\"entity\/project\/artifact:alias\") # get the only model (which also should be the best) thanks in advance for any help. have a great day",
        "Question_original_content_gpt_summary":"The user is looking for a way to log one model per run and be able to load the best overall model for production.",
        "Question_preprocessed_content":"Title: get best model from artifacts; Content: hi, i guess this is common but i cannot figure it out i would like to log one model per run, and in the end, be able to load the best overall model for production. so like run , , , , production for now my solution is thanks in advance for any help. have a great day",
        "Answer_original_content":"hi @ierezell, i think i understand your question a little better. we are currently working on a feature that will make exactly what youre asking for super clean and easy in the meantime the most straightforward way of going about this would be to save the performance of all of the metrics you care about to the run.summary of the run that produced that model. then from the api you can query all the runs in the project and select the best run using the metrics you set in the summary. then you can get the download the best model and use it as an input to your production code. here is a related example querying the best model from a sweep but the process will be slightly different for you use case.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. Wait for a feature that will make logging one model per run and loading the best overall model for production super clean and easy.\n2. Save the performance of all the metrics that matter to the run.summary of the run that produced that model.\n3. Query all the runs in the project and select the best run using the metrics set in the summary.\n4. Download the best model and use it as an input to the production code.",
        "Answer_preprocessed_content":"hi i think i understand your question a little better. we are currently working on a feature that will make exactly what youre asking for super clean and easy in the meantime the most straightforward way of going about this would be to save the performance of all of the metrics you care about to the of the run that produced that model. then from the you can query all the runs in the project and select the best run using the metrics you set in the summary. then you can get the download the best model and use it as an input to your production code. here is a related example querying the best model from a sweep but the process will be slightly different for you use case."
    },
    {
        "Question_id":71499094.0,
        "Question_title":"Python AzureML Hello world - Can't find module azureml",
        "Question_body":"<p>Python 3.10, Pip install azureml-sdk 1.39.0.<br \/>\nEnvironments: Win10 PS, VS2022, and a docker image- all same results . Pip show shows the azureml-core package.<\/p>\n<p>Simple (I thought) script, but it can't find &quot;azureml.core&quot;   No module named azureml is the error.\nHow do I make it &quot;find&quot; it? I'm new at python so it could be syntax.<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Experiment, Environment, Model,Dataset,Datastore,ScriptRunConfig\n     \n    # check core SDK version number\n    print(&quot;Azure ML SDK Version: &quot;, azureml.core.VERSION)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1647441787870,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":310.0,
        "Owner_creation_time":1263312456836,
        "Owner_last_access_time":1662141819340,
        "Owner_reputation":49.0,
        "Owner_up_votes":3.0,
        "Owner_down_votes":0.0,
        "Owner_views":15.0,
        "Answer_body":"<p>azureml python sdk does not support py3.10 yet, AutoML sdk supports py&lt;=3.8.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1648177993987,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71499094",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: python hello world - can't find module ; Content: python 3.10, pip install -sdk 1.39.0. environments: win10 ps, vs2022, and a docker image- all same results . pip show shows the -core package. simple (i thought) script, but it can't find \".core\" no module named is the error. how do i make it \"find\" it? i'm new at python so it could be syntax. import os from .core import workspace, experiment, environment, model,dataset,datastore,scriptrunconfig # check core sdk version number print(\" sdk version: \", .core.version)",
        "Question_original_content_gpt_summary":"The user is encountering challenges with a simple Python script, where they are unable to make the script \"find\" the \".core\" module, resulting in a \"no module named\" error.",
        "Question_preprocessed_content":"Title: python hello world can't find module ; Content: python pip install environments win ps, vs , and a docker image all same results . pip show shows the package. simple script, but it can't find no module named is the error. how do i make it find it? i'm new at python so it could be syntax.",
        "Answer_original_content":"python sdk does not support py3.10 yet, automl sdk supports py<=3.8.",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's challenge with a Python script that cannot find the \".core\" module are not explicitly mentioned in the answer. However, the answer suggests that the issue may be related to the Python version being used. Specifically, the Python SDK does not support version 3.10 yet, and the AutoML SDK only supports versions up to 3.8. Therefore, the user may need to downgrade their Python version to resolve the issue.",
        "Answer_preprocessed_content":"python sdk does not support yet, automl sdk supports"
    },
    {
        "Question_id":null,
        "Question_title":"Run dataset.register method multiple Times",
        "Question_body":"Hi\n\nI am using the following code to register the dataset.\ndataset.register(\nworkspace = ws,\nname = dataset_name,\ntags = tags,\ncreate_new_version = True,\ndescription = \"\"\n)\n\nWhen I use the above for the first time too create a dataset , a new version is creation which is version 1. But when I re-run the same command again, one more new version is not created like version 2 is not created. Why? For the second-run my Data is same, Tags are same.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1653460894327,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/863226\/run-datasetregister-method-multiple-times.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-26T03:03:28.543Z",
                "Answer_score":1,
                "Answer_body":"@KrishnamohanNadimpalli-6337 Thanks for the question. Dataset is a reference to the files to your data in storage.\n\nPlease follow the document to register dataset as version. Still facing an issue add more details about the error.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: run dataset.register method multiple times; Content: hi i am using the following code to register the dataset. dataset.register( workspace = ws, name = dataset_name, tags = tags, create_new_version = true, description = \"\" ) when i use the above for the first time too create a dataset , a new version is creation which is version 1. but when i re-run the same command again, one more new version is not created like version 2 is not created. why? for the second-run my data is same, tags are same.",
        "Question_original_content_gpt_summary":"The user is encountering an issue where running the dataset.register method multiple times does not create a new version of the dataset.",
        "Question_preprocessed_content":"Title: run method multiple times; Content: hi i am using the following code to register the dataset. workspace ws, name tags tags, true, description when i use the above for the first time too create a dataset , a new version is creation which is version . but when i the same command again, one more new version is not created like version is not created. why? for the my data is same, tags are same.",
        "Answer_original_content":"@krishnamohannadimpalli-6337 thanks for the question. dataset is a reference to the files to your data in storage. please follow the document to register dataset as version. still facing an issue add more details about the error.",
        "Answer_original_content_gpt_summary":"Possible solutions to the issue of not being able to create a new version of the dataset when running the dataset.register method multiple times are:\n\n1. Follow the document to register the dataset as a version.\n2. Provide more details about the error if the issue persists.",
        "Answer_preprocessed_content":"thanks for the question. dataset is a reference to the files to your data in storage. please follow the document to register dataset as version. still facing an issue add more details about the error."
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML for SAP ERP",
        "Question_body":"I am trying to figure out about standard connectors between SAP ERP product and Azure ML especially for NLP scenarios. Can you please suggest on this.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1664541861543,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1030800\/azure-ml-for-sap-erp.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-30T13:01:37.547Z",
                "Answer_score":0,
                "Answer_body":"@Divya-0887 Thanks for the question. Here is the blog that could help and nlp recipes.\nhttps:\/\/blogs.sap.com\/2022\/08\/03\/azure-machine-learning-triggering-calculations-ml-in-sap-data-warehouse-cloud\/",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: for sap erp; Content: i am trying to figure out about standard connectors between sap erp product and especially for nlp scenarios. can you please suggest on this.",
        "Question_original_content_gpt_summary":"The user is trying to figure out standard connectors between SAP ERP and NLP scenarios.",
        "Question_preprocessed_content":"Title: for sap erp; Content: i am trying to figure out about standard connectors between sap erp product and especially for nlp scenarios. can you please suggest on this.",
        "Answer_original_content":"@divya-0887 thanks for the question. here is the blog that could help and nlp recipes. https:\/\/blogs.sap.com\/2022\/08\/03\/azure-machine-learning-triggering-calculations-ml-in-sap-data-warehouse-cloud\/",
        "Answer_original_content_gpt_summary":"The answer provides a link to a blog that could potentially help the user figure out standard connectors between SAP ERP and NLP scenarios. The blog specifically discusses triggering calculations and machine learning in SAP Data Warehouse Cloud using Azure Machine Learning.",
        "Answer_preprocessed_content":"thanks for the question. here is the blog that could help and nlp recipes."
    },
    {
        "Question_id":null,
        "Question_title":"Pipeline failed to deploy model: \"service_account cannot be specified for deploying AutoML models\"",
        "Question_body":"I made a pipeline that almost mirrors step 6 of Intro to Vertex Pipelines which has managed to get past every step up until the model deployment side of things. The code snippet for my model deploy op is here:   And the associated error message in the logs for the deployment part of the pipeline was:RuntimeError: Failed to create the resource. Error: {'code': 400, 'message': 'service_account cannot be specified for deploying AutoML Models.', 'status': 'FAILED_PRECONDITION'} Does it have to do with a specific permission I need to give my service account? I don't know how to interpret this error.  ",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1649678700000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Pipeline-failed-to-deploy-model-quot-service-account-cannot-be\/td-p\/412645\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-18T14:14:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"You are sure you can specify a service account here like this? Do you have any reference that uses service account here?\n\n[1] https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-..."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pipeline failed to deploy model: \"service_account cannot be specified for deploying automl models\"; Content: i made a pipeline that almost mirrors step 6 of intro to vertex pipelines which has managed to get past every step up until the model deployment side of things. the code snippet for my model deploy op is here: and the associated error message in the logs for the deployment part of the pipeline was:runtimeerror: failed to create the resource. error: {'code': 400, 'message': 'service_account cannot be specified for deploying automl models.', 'status': 'failed_precondition'} does it have to do with a specific permission i need to give my service account? i don't know how to interpret this error.",
        "Question_original_content_gpt_summary":"The user encountered a challenge with their pipeline failing to deploy a model due to an error message stating \"service_account cannot be specified for deploying automl models.\".",
        "Question_preprocessed_content":"Title: pipeline failed to deploy model cannot be specified for deploying automl models ; Content: i made a pipeline that almost mirrors step of intro to vertex pipelines which has managed to get past every step up until the model deployment side of things. the code snippet for my model deploy op is here and the associated error message in the logs for the deployment part of the pipeline was runtimeerror failed to create the resource. error does it have to do with a specific permission i need to give my service account? i don't know how to interpret this error.",
        "Answer_original_content":"you are sure you can specify a service account here like this? do you have any reference that uses service account here? [1] https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-...",
        "Answer_original_content_gpt_summary":"Possible solution: The answer suggests checking if specifying a service account is allowed in the pipeline deployment process. The provided reference link may also provide guidance on how to build an AutoML pipeline using Vertex Pipelines.",
        "Answer_preprocessed_content":"you are sure you can specify a service account here like this? do you have any reference that uses service account here?"
    },
    {
        "Question_id":72975189.0,
        "Question_title":"How to build a simple spark-test-app.jar to test AWS SageMaker SparkJarProcessing",
        "Question_body":"<p>Does anyone know a repo that shows what a simple HelloWorld java or scala code would look like to build the jar that could be executed using the AWS SageMaker <strong>SparkJarProcessing<\/strong> class?<\/p>\n<p>Readthedocs (<a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/spark_distributed_data_processing\/sagemaker-spark-processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_processing\/spark_distributed_data_processing\/sagemaker-spark-processing.html<\/a>) mentions:<\/p>\n<p>&quot;In the next example, you\u2019ll take a Spark application jar (located in .\/code\/spark-test-app.jar)...&quot;<\/p>\n<p>My question is how does the source code look like for this jar (spark-test-app.jar)?<\/p>\n<p>I tried building a simple Java project jar<\/p>\n<p>src&gt;com.test&gt;HW.java:<\/p>\n<pre><code>\npublic class HW {\n    public static void main(String[] args) {\n        System.out.printf(&quot;hello world!&quot;);\n    }\n}\n<\/code><\/pre>\n<p>and running it inside SageMaker Notebook conda_python3 kernel using<\/p>\n<pre><code>from sagemaker.spark.processing import SparkJarProcessor\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\nprint(role)\n\nspark_processor = SparkJarProcessor(\n    base_job_name=&quot;sm-spark-java&quot;,\n    framework_version=&quot;3.1&quot;,\n    role=role,\n    instance_count=2,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    max_runtime_in_seconds=1200,\n)\n\nspark_processor.run(\n    submit_app=&quot;.\/SparkJarProcessing-1.0-SNAPSHOT.jar&quot;,\n    submit_class=&quot;com.test.HW&quot;,\n    arguments=[&quot;--input&quot;, &quot;abc&quot;],\n    logs=True,\n)\n<\/code><\/pre>\n<p>But end up getting an error:\nCould not execute HW class.<\/p>\n<p>Any sample source code for <strong>spark-test-app.jar<\/strong> would be highly appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1657772745600,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":49.0,
        "Owner_creation_time":1600969850140,
        "Owner_last_access_time":1662936777792,
        "Owner_reputation":51.0,
        "Owner_up_votes":10.0,
        "Owner_down_votes":0.0,
        "Owner_views":7.0,
        "Answer_body":"<p>To answer your question, the source code of that class looks like:<\/p>\n<pre><code>package com.amazonaws.sagemaker.spark.test;\n\nimport java.lang.invoke.SerializedLambda;\nimport org.apache.commons.cli.CommandLineParser;\nimport org.apache.commons.cli.ParseException;\nimport org.apache.commons.cli.HelpFormatter;\nimport org.apache.commons.cli.Option;\nimport org.apache.commons.cli.BasicParser;\nimport org.apache.commons.cli.Options;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.commons.cli.CommandLine;\nimport org.apache.spark.sql.types.DataTypes;\nimport org.apache.commons.lang3.StringUtils;\nimport java.util.List;\nimport org.apache.spark.sql.SparkSession;\n\npublic class HelloJavaSparkApp\n{\n    public static void main(final String[] args) {\n        System.out.println(&quot;Hello World, this is Java-Spark!&quot;);\n        final CommandLine parsedArgs = parseArgs(args);\n        final String inputPath = parsedArgs.getOptionValue(&quot;input&quot;);\n        final String outputPath = parsedArgs.getOptionValue(&quot;output&quot;);\n        final SparkSession spark = SparkSession.builder().appName(&quot;Hello Spark App&quot;).getOrCreate();\n        System.out.println(&quot;Got a Spark session with version: &quot; + spark.version());\n        System.out.println(&quot;Reading input from: &quot; + inputPath);\n        final Dataset salesDF = spark.read().json(inputPath);\n        salesDF.printSchema();\n        salesDF.createOrReplaceTempView(&quot;sales&quot;);\n        final Dataset topDF = spark.sql(&quot;SELECT date, sale FROM sales WHERE sale &gt; 750 SORT BY sale DESC&quot;);\n        topDF.show();\n        final Dataset avgDF = salesDF.groupBy(&quot;date&quot;, new String[0]).avg(new String[0]).orderBy(&quot;date&quot;, new String[0]);\n        System.out.println(&quot;Collected average sales: &quot; + StringUtils.join((Object[])new List[] { avgDF.collectAsList() }));\n        spark.sqlContext().udf().register(&quot;double&quot;, n -&gt; n + n, DataTypes.LongType);\n        final Dataset saleDoubleDF = salesDF.selectExpr(new String[] { &quot;date&quot;, &quot;sale&quot;, &quot;double(sale) as sale_double&quot; }).orderBy(&quot;date&quot;, new String[] { &quot;sale&quot; });\n        saleDoubleDF.show();\n        System.out.println(&quot;Writing output to: &quot; + outputPath);\n        saleDoubleDF.coalesce(1).write().json(outputPath);\n        spark.stop();\n    }\n    \n    private static CommandLine parseArgs(final String[] args) {\n        final Options options = new Options();\n        final CommandLineParser parser = (CommandLineParser)new BasicParser();\n        final Option input = new Option(&quot;i&quot;, &quot;input&quot;, true, &quot;input path&quot;);\n        input.setRequired(true);\n        options.addOption(input);\n        final Option output = new Option(&quot;o&quot;, &quot;output&quot;, true, &quot;output path&quot;);\n        output.setRequired(true);\n        options.addOption(output);\n        try {\n            return parser.parse(options, args);\n        }\n        catch (ParseException e) {\n            new HelpFormatter().printHelp(&quot;HelloScalaSparkApp --input \/opt\/ml\/input\/foo --output \/opt\/ml\/output\/bar&quot;, options);\n            throw new RuntimeException((Throwable)e);\n        }\n    }\n}\n<\/code><\/pre>\n<p>At the same time, I have created a simple example that shows how to run an hello world app <a href=\"https:\/\/github.com\/giuseppeporcelli\/sagemaker-misc-examples\/blob\/main\/spark-jar-example.ipynb\" rel=\"nofollow noreferrer\">here<\/a>. Please note that I have run that example on Amazon SageMaker Studio Notebooks, using the Data Science 1.0 kernel.<\/p>\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658238283580,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72975189",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to build a simple spark-test-app.jar to test sparkjarprocessing; Content: does anyone know a repo that shows what a simple helloworld java or scala code would look like to build the jar that could be executed using the sparkjarprocessing class? readthedocs (https:\/\/-examples.readthedocs.io\/en\/latest\/_processing\/spark_distributed_data_processing\/-spark-processing.html) mentions: \"in the next example, you\u2019ll take a spark application jar (located in .\/code\/spark-test-app.jar)...\" my question is how does the source code look like for this jar (spark-test-app.jar)? i tried building a simple java project jar src>com.test>hw.java: public class hw { public static void main(string[] args) { system.out.printf(\"hello world!\"); } } and running it inside notebook conda_python3 kernel using from .spark.processing import sparkjarprocessor from import get_execution_role role = get_execution_role() print(role) spark_processor = sparkjarprocessor( base_job_name=\"sm-spark-java\", framework_version=\"3.1\", role=role, instance_count=2, instance_type=\"ml.m5.xlarge\", max_runtime_in_seconds=1200, ) spark_processor.run( submit_app=\".\/sparkjarprocessing-1.0-snapshot.jar\", submit_class=\"com.test.hw\", arguments=[\"--input\", \"abc\"], logs=true, ) but end up getting an error: could not execute hw class. any sample source code for spark-test-app.jar would be highly appreciated!",
        "Question_original_content_gpt_summary":"The user is encountering challenges in building a simple spark-test-app.jar to test sparkjarprocessing and is looking for a sample source code for the jar.",
        "Question_preprocessed_content":"Title: how to build a simple to test sparkjarprocessing; Content: does anyone know a repo that shows what a simple helloworld java or scala code would look like to build the jar that could be executed using the sparkjarprocessing class? readthedocs mentions in the next example, youll take a spark application jar my question is how does the source code look like for this jar ? i tried building a simple java project jar and running it inside notebook kernel using but end up getting an error could not execute hw class. any sample source code for would be highly appreciated!",
        "Answer_original_content":"to answer your question, the source code of that class looks like: package com.amazonaws..spark.test; import java.lang.invoke.serializedlambda; import org.apache.commons.cli.commandlineparser; import org.apache.commons.cli.parseexception; import org.apache.commons.cli.helpformatter; import org.apache.commons.cli.option; import org.apache.commons.cli.basicparser; import org.apache.commons.cli.options; import org.apache.spark.sql.dataset; import org.apache.commons.cli.commandline; import org.apache.spark.sql.types.datatypes; import org.apache.commons.lang3.stringutils; import java.util.list; import org.apache.spark.sql.sparksession; public class hellojavasparkapp { public static void main(final string[] args) { system.out.println(\"hello world, this is java-spark!\"); final commandline parsedargs = parseargs(args); final string inputpath = parsedargs.getoptionvalue(\"input\"); final string outputpath = parsedargs.getoptionvalue(\"output\"); final sparksession spark = sparksession.builder().appname(\"hello spark app\").getorcreate(); system.out.println(\"got a spark session with version: \" + spark.version()); system.out.println(\"reading input from: \" + inputpath); final dataset salesdf = spark.read().json(inputpath); salesdf.printschema(); salesdf.createorreplacetempview(\"sales\"); final dataset topdf = spark.sql(\"select date, sale from sales where sale > 750 sort by sale desc\"); topdf.show(); final dataset avgdf = salesdf.groupby(\"date\", new string[0]).avg(new string[0]).orderby(\"date\", new string[0]); system.out.println(\"collected average sales: \" + stringutils.join((object[])new list[] { avgdf.collectaslist() })); spark.sqlcontext().udf().register(\"double\", n -> n + n, datatypes.longtype); final dataset saledoubledf = salesdf.selectexpr(new string[] { \"date\", \"sale\", \"double(sale) as sale_double\" }).orderby(\"date\", new string[] { \"sale\" }); saledoubledf.show(); system.out.println(\"writing output to: \" + outputpath); saledoubledf.coalesce(1).write().json(outputpath); spark.stop(); } private static commandline parseargs(final string[] args) { final options options = new options(); final commandlineparser parser = (commandlineparser)new basicparser(); final option input = new option(\"i\", \"input\", true, \"input path\"); input.setrequired(true); options.addoption(input); final option output = new option(\"o\", \"output\", true, \"output path\"); output.setrequired(true); options.addoption(output); try { return parser.parse(options, args); } catch (parseexception e) { new helpformatter().printhelp(\"helloscalasparkapp --input \/opt\/ml\/input\/foo --output \/opt\/ml\/output\/bar\", options); throw new runtimeexception((throwable)e); } } } at the same time, i have created a simple example that shows how to run an hello world app here. please note that i have run that example on studio notebooks, using the data science 1.0 kernel. hope this helps.",
        "Answer_original_content_gpt_summary":"The answer provides the source code for a Java Spark application that reads input from a JSON file, performs some operations on the data, and writes the output to a JSON file. Additionally, the answer includes a link to a simple example of running a \"hello world\" app on Studio Notebooks using the Data Science 1.0 kernel.",
        "Answer_preprocessed_content":"to answer your question, the source code of that class looks like at the same time, i have created a simple example that shows how to run an hello world app here. please note that i have run that example on studio notebooks, using the data science kernel. hope this helps."
    },
    {
        "Question_id":56364828.0,
        "Question_title":"Azure ML - Train a model on segments of the data-set",
        "Question_body":"<p>I could really use some help!<\/p>\n\n<p>The company I work for is made up of 52 very different businesses so I can't predict at the company level but instead need to predict business by business then roll up the result to give company wide prediction.<\/p>\n\n<p>I have written an ML model in studio.azureml.net\nIt works great with a 0.947 Coefficient of Determination, but this is for 1 of the businesses.\nI now need to train the model for the other 51.<\/p>\n\n<p>Is there a way to do this in a single ML model rather than having to create 52 very similar models?<\/p>\n\n<p>Any help would be much appreciated !!!<\/p>\n\n<p>Kind Regards\nMartin<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1559146852583,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":123.0,
        "Owner_creation_time":1300461675980,
        "Owner_last_access_time":1662999574123,
        "Owner_reputation":189.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":26.0,
        "Answer_body":"<p>You can use Ensembles, combining several models to improve predictions. The most direct is stacking when the outputs of all the models are trained on the entire dataset. \nThe method that, I think, corresponds the best to your problem is bagging (bootstrap aggregation). You need to divide the training set into different subsets (each corresponding to a certain business), then train a different model on each subset and combine the result of each classifier. \nAnother way is boosting but it is difficult to implement in Azure ML. \nYou can see an example in <a href=\"https:\/\/gallery.azure.ai\/Experiment\/b6b09fc0c26047e6b4c733ab78a86498\" rel=\"nofollow noreferrer\">Azure ML Gallery<\/a>. <\/p>\n\n<p>Quote from book:<\/p>\n\n<blockquote>\n  <p>Stacking and bagging can be easily implemented in Azure Machine\n  Learning, but other ensemble methods are more difficult. Also, it\n  turns out to be very tedious to implement in Azure Machine Learning an\n  ensemble of, say, more than five models. The experiment is filled with\n  modules and is quite difficult to maintain. Sometimes it is worthwhile\n  to use any ensemble method available in R or Python. Adding more\n  models to an ensemble written in a script can be as trivial as\n  changing a number in the code, instead of copying and pasting modules\n  into the experiment.<\/p>\n<\/blockquote>\n\n<p>You may also have a look at <a href=\"http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\" rel=\"nofollow noreferrer\">sklearn (Python)<\/a> and caret (R) documentation for further details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559897214220,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1560930522380,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56364828",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - train a model on segments of the data-set; Content: i could really use some help! the company i work for is made up of 52 very different businesses so i can't predict at the company level but instead need to predict business by business then roll up the result to give company wide prediction. i have written an ml model in studio..net it works great with a 0.947 coefficient of determination, but this is for 1 of the businesses. i now need to train the model for the other 51. is there a way to do this in a single ml model rather than having to create 52 very similar models? any help would be much appreciated !!! kind regards martin",
        "Question_original_content_gpt_summary":"The user Martin is facing a challenge of training a model on segments of the data-set for the 52 different businesses of the company he works for, and is looking for a way to do this in a single ML model rather than having to create 52 very similar models.",
        "Question_preprocessed_content":"Title: train a model on segments of the ; Content: i could really use some help! the company i work for is made up of very different businesses so i can't predict at the company level but instead need to predict business by business then roll up the result to give company wide prediction. i have written an ml model in it works great with a coefficient of determination, but this is for of the businesses. i now need to train the model for the other . is there a way to do this in a single ml model rather than having to create very similar models? any help would be much appreciated !!! kind regards martin",
        "Answer_original_content":"you can use ensembles, combining several models to improve predictions. the most direct is stacking when the outputs of all the models are trained on the entire dataset. the method that, i think, corresponds the best to your problem is bagging (bootstrap aggregation). you need to divide the training set into different subsets (each corresponding to a certain business), then train a different model on each subset and combine the result of each classifier. another way is boosting but it is difficult to implement in . you can see an example in gallery. quote from book: stacking and bagging can be easily implemented in azure machine learning, but other ensemble methods are more difficult. also, it turns out to be very tedious to implement in an ensemble of, say, more than five models. the experiment is filled with modules and is quite difficult to maintain. sometimes it is worthwhile to use any ensemble method available in r or python. adding more models to an ensemble written in a script can be as trivial as changing a number in the code, instead of copying and pasting modules into the experiment. you may also have a look at sklearn (python) and caret (r) documentation for further details.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of training a single ML model on segments of data for 52 different businesses are ensembles, such as stacking, bagging, and boosting. Bagging is recommended for this specific problem, where the training set is divided into subsets for each business, and a different model is trained on each subset, and the results are combined. Ensembles can be implemented in Azure Machine Learning, but other methods may be more difficult and tedious to maintain. Sklearn (Python) and Caret (R) documentation can provide further details.",
        "Answer_preprocessed_content":"you can use ensembles, combining several models to improve predictions. the most direct is stacking when the outputs of all the models are trained on the entire dataset. the method that, i think, corresponds the best to your problem is bagging . you need to divide the training set into different subsets , then train a different model on each subset and combine the result of each classifier. another way is boosting but it is difficult to implement in . you can see an example in gallery. quote from book stacking and bagging can be easily implemented in azure machine learning, but other ensemble methods are more difficult. also, it turns out to be very tedious to implement in an ensemble of, say, more than five models. the experiment is filled with modules and is quite difficult to maintain. sometimes it is worthwhile to use any ensemble method available in r or python. adding more models to an ensemble written in a script can be as trivial as changing a number in the code, instead of copying and pasting modules into the experiment. you may also have a look at sklearn and caret documentation for further details."
    },
    {
        "Question_id":null,
        "Question_title":"Train machine learning model using reserved instance",
        "Question_body":"Hi.\n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks, Stefan",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1641871148701,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":151.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-14T08:13:53.228Z",
                "Answer_score":0,
                "Answer_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\nUsing the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample Blazingtext-instances, Deepar-instances\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: train machine learning model using reserved instance; Content: hi. is it possible to train a machine learning model with using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? i'm familiar with local mode, but i understand this is not supported when using machine learning estimators. appreciate any suggestions for how to make the model training process in go faster when using machine learning estimators. thanks, stefan",
        "Question_original_content_gpt_summary":"The user is looking for a way to speed up the process of training a machine learning model using machine learning estimators on a reserved instance, instead of provisioning a new instance every time.",
        "Question_preprocessed_content":"Title: train machine learning model using reserved instance; Content: hi. is it possible to train a machine learning model with using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? i'm familiar with local mode, but i understand this is not supported when using machine learning estimators. appreciate any suggestions for how to make the model training process in go faster when using machine learning estimators. thanks, stefan",
        "Answer_original_content":"as of today, it's not possible to train a machine learning model with using a reserved instance that is already up and running instead of provisioning a new instance. the service team is currently working on it, unfortunately i don't have an eta as to when the feature will be released. local mode is supported for frameworks images (tensorflow, mxnet, chainer, pytorch, and scikit-learn) and images you supply yourself. using the python sdk 2.72.3 documentation if you want to train built-in algorithm models simply faster, you should check the recommendation in the document. example blazingtext-instances, deepar-instances if the algorithm supports it, one can also try using pipe mode or fastfile mode. these offer some fast training job startup time. accelerate-model-training-using-faster-pipe-mode-on-amazon-",
        "Answer_original_content_gpt_summary":"Possible solutions mentioned in the answer are:\n- Currently, it's not possible to train a machine learning model using a reserved instance that is already up and running instead of provisioning a new instance.\n- The service team is working on this feature, but there is no ETA available.\n- Local mode is supported for certain frameworks and images, which can help speed up training.\n- Check the recommendations in the Python SDK documentation to train built-in algorithm models faster.\n- Consider using blazingtext-instances or deepar-instances if the algorithm supports it.\n- Try using pipe mode or fastfile mode to offer faster training job startup time.",
        "Answer_preprocessed_content":"as of today, it's not possible to train a machine learning model with using a reserved instance that is already up and running instead of provisioning a new instance. the service team is currently working on it, unfortunately i don't have an eta as to when the feature will be released. local mode is supported for frameworks images and images you supply yourself. using the python sdk documentation if you want to train algorithm models simply faster, you should check the recommendation in the document. example if the algorithm supports it, one can also try using pipe mode or fastfile mode. these offer some fast training job startup time."
    },
    {
        "Question_id":61683506.0,
        "Question_title":"Azure ML: how to access logs of a failed Model deployment",
        "Question_body":"<p>I'm deploying a Keras model that is failing with the error below. The exception says that I can retrieve the logs by running \"print(service.get_logs())\", but that's giving me empty results. I am deploying the model from my AzureNotebook and I'm using the same \"service\" var to retrieve the logs. <\/p>\n\n<p>Also, how can i retrieve the logs from the container instance? I'm deploying to an AKS compute cluster I created. Sadly, the docs link in the exception also doesnt detail how to retrieve these logs.<\/p>\n\n<pre><code>More information can be found using '.get_logs()' Error: \n<\/code><\/pre>\n\n<pre><code>{   \"code\":\n\"KubernetesDeploymentFailed\",   \"statusCode\": 400,   \"message\":\n\"Kubernetes Deployment failed\",   \"details\": [\n    {\n      \"code\": \"CrashLoopBackOff\",\n      \"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\\nPlease check\nthe logs for your container instance: my-model-service. From\nthe AML SDK, you can run print(service.get_logs()) if you have service\nobject to fetch the logs. \\nYou can also try to run image\nmlwks.azurecr.io\/azureml\/azureml_3c0c34b65cf18c8644e8d745943ab7d2:latest\nlocally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails\nfor more information.\"\n    }   ] }\n<\/code><\/pre>\n\n<p>UPDATE<\/p>\n\n<p>Here's my code to deploy the model:<\/p>\n\n<pre><code>environment = Environment('my-environment')\nenvironment.python.conda_dependencies = CondaDependencies.create(pip_packages=[\"azureml-defaults\",\"azureml-dataprep[pandas,fuse]\",\"tensorflow\", \"keras\", \"matplotlib\"])\nservice_name = 'my-model-service'\n\n# Remove any existing service under the same name.\ntry:\n    Webservice(ws, service_name).delete()\nexcept WebserviceException:\n    pass\n\ninference_config = InferenceConfig(entry_script='score.py', environment=environment)\ncomp = ComputeTarget(workspace=ws, name=\"ml-inference-dev\")\nservice = Model.deploy(workspace=ws,\n                       name=service_name,\n                       models=[model],\n                       inference_config=inference_config,\n                       deployment_target=comp \n                      )\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n\n<p>And my score.py<\/p>\n\n<pre><code>import joblib\nimport numpy as np\nimport os\n\nimport keras\n\nfrom keras.models import load_model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\n\ndef init():\n    global model\n\n    model_path = Model.get_model_path('model.h5')\n    model = load_model(model_path)\n    model = keras.models.load_model(model_path)\n\n\n# The run() method is called each time a request is made to the scoring API.\n#\n# Shown here are the optional input_schema and output_schema decorators\n# from the inference-schema pip package. Using these decorators on your\n# run() method parses and validates the incoming payload against\n# the example input you provide here. This will also generate a Swagger\n# API document for your web service.\n@input_schema('data', NumpyParameterType(np.array([[0.1, 1.2, 2.3, 3.4, 4.5, 5.6, 6.7, 7.8, 8.9, 9.0]])))\n@output_schema(NumpyParameterType(np.array([4429.929236457418])))\ndef run(data):\n\n    return [123] #test\n<\/code><\/pre>\n\n<p><strong>Update 2:<\/strong><\/p>\n\n<p>Here is a screencap of the endpoint page. Is it normal for the CPU to be .1? Also, when i hit the swagger url in the browser, i get the error: \"No ready replicas for service doc-classify-env-service\"<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Uvrfx.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Uvrfx.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Update 3<\/strong>\nAfter finally getting to the container logs, it turns out that it was choking  with this error on my score.py<\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError: No module named 'inference_schema'<\/p>\n<\/blockquote>\n\n<p>I then ran a test that commented out the refs for \"input_schema\" and \"output_schema\" and also simplified my pip_packages and the REST endpoint come up! I was also able to get a prediction out of the model. <\/p>\n\n<pre><code>pip_packages=[\"azureml-defaults\",\"tensorflow\", \"keras\"])\n<\/code><\/pre>\n\n<p>So my question is, how should I have my pip_packages for the scoring file to utilize the inference_schema decorators? I'm assuming I need to include azureml-sdk[auotml] pip package, but when i do so, the image creation fails and I see several dependency conflicts.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1588954967507,
        "Question_favorite_count":1.0,
        "Question_score":1.0,
        "Question_view_count":1564.0,
        "Owner_creation_time":1330016065408,
        "Owner_last_access_time":1662160983830,
        "Owner_reputation":1704.0,
        "Owner_up_votes":61.0,
        "Owner_down_votes":7.0,
        "Owner_views":232.0,
        "Answer_body":"<p>Try retrieving your service from the workspace directly <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ws.webservices[service_name].get_logs()\n<\/code><\/pre>\n\n<p>Also, I found deploying an image as an endpoint to be easier than inference+deploy model (depending on your use case) <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>my_image = Image(ws, name='test', version='26')  \nservice = AksWebservice.deploy_from_image(ws, \"test1\", my_image, deployment_config, aks_target)\n<\/code><\/pre>",
        "Answer_comment_count":13.0,
        "Answer_creation_time":1588964687420,
        "Answer_score":3.0,
        "Owner_location":null,
        "Question_last_edit_time":1589409955880,
        "Answer_last_edit_time":1588970402070,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61683506",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: : how to access logs of a failed model deployment; Content: i'm deploying a keras model that is failing with the error below. the exception says that i can retrieve the logs by running \"print(service.get_logs())\", but that's giving me empty results. i am deploying the model from my azurenotebook and i'm using the same \"service\" var to retrieve the logs. also, how can i retrieve the logs from the container instance? i'm deploying to an aks compute cluster i created. sadly, the docs link in the exception also doesnt detail how to retrieve these logs. more information can be found using '.get_logs()' error: { \"code\": \"kubernetesdeploymentfailed\", \"statuscode\": 400, \"message\": \"kubernetes deployment failed\", \"details\": [ { \"code\": \"crashloopbackoff\", \"message\": \"your container application crashed. this may be caused by errors in your scoring file's init() function.\\nplease check the logs for your container instance: my-model-service. from the aml sdk, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nyou can also try to run image mlwks.azurecr.io\/\/_3c0c34b65cf18c8644e8d745943ab7d2:latest locally. please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\" } ] } update here's my code to deploy the model: environment = environment('my-environment') environment.python.conda_dependencies = condadependencies.create(pip_packages=[\"-defaults\",\"-dataprep[pandas,fuse]\",\"tensorflow\", \"keras\", \"matplotlib\"]) service_name = 'my-model-service' # remove any existing service under the same name. try: webservice(ws, service_name).delete() except webserviceexception: pass inference_config = inferenceconfig(entry_script='score.py', environment=environment) comp = computetarget(workspace=ws, name=\"ml-inference-dev\") service = model.deploy(workspace=ws, name=service_name, models=[model], inference_config=inference_config, deployment_target=comp ) service.wait_for_deployment(show_output=true) and my score.py import joblib import numpy as np import os import keras from keras.models import load_model from inference_schema.schema_decorators import input_schema, output_schema from inference_schema.parameter_types.numpy_parameter_type import numpyparametertype def init(): global model model_path = model.get_model_path('model.h5') model = load_model(model_path) model = keras.models.load_model(model_path) # the run() method is called each time a request is made to the scoring api. # # shown here are the optional input_schema and output_schema decorators # from the inference-schema pip package. using these decorators on your # run() method parses and validates the incoming payload against # the example input you provide here. this will also generate a swagger # api document for your web service. @input_schema('data', numpyparametertype(np.array([[0.1, 1.2, 2.3, 3.4, 4.5, 5.6, 6.7, 7.8, 8.9, 9.0]]))) @output_schema(numpyparametertype(np.array([4429.929236457418]))) def run(data): return [123] #test update 2: here is a screencap of the endpoint page. is it normal for the cpu to be .1? also, when i hit the swagger url in the browser, i get the error: \"no ready replicas for service doc-classify-env-service\" update 3 after finally getting to the container logs, it turns out that it was choking with this error on my score.py modulenotfounderror: no module named 'inference_schema' i then ran a test that commented out the refs for \"input_schema\" and \"output_schema\" and also simplified my pip_packages and the rest endpoint come up! i was also able to get a prediction out of the model. pip_packages=[\"-defaults\",\"tensorflow\", \"keras\"]) so my question is, how should i have my pip_packages for the scoring file to utilize the inference_schema decorators? i'm assuming i need to include -sdk[auotml] pip package, but when i do so, the image creation fails and i see several dependency conflicts.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with accessing logs of a failed model deployment, retrieving logs from a container instance, and configuring the pip_packages to utilize the inference_schema decorators.",
        "Question_preprocessed_content":"Title: how to access logs of a failed model deployment; Content: i'm deploying a keras model that is failing with the error below. the exception says that i can retrieve the logs by running but that's giving me empty results. i am deploying the model from my azurenotebook and i'm using the same service var to retrieve the logs. also, how can i retrieve the logs from the container instance? i'm deploying to an aks compute cluster i created. sadly, the docs link in the exception also doesnt detail how to retrieve these logs. update here's my code to deploy the model and my update here is a screencap of the endpoint page. is it normal for the cpu to be . ? also, when i hit the swagger url in the browser, i get the error no ready replicas for service update after finally getting to the container logs, it turns out that it was choking with this error on my modulenotfounderror no module named i then ran a test that commented out the refs for and and also simplified my and the rest endpoint come up! i was also able to get a prediction out of the model. so my question is, how should i have my for the scoring file to utilize the decorators? i'm assuming i need to include pip package, but when i do so, the image creation fails and i see several dependency conflicts.",
        "Answer_original_content":"try retrieving your service from the workspace directly ws.webservices[service_name].get_logs() also, i found deploying an image as an endpoint to be easier than inference+deploy model (depending on your use case) my_image = image(ws, name='test', version='26') service = akswebservice.deploy_from_image(ws, \"test1\", my_image, deployment_config, aks_target)",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include: \n1. Try retrieving the service logs directly from the workspace using the code \"ws.webservices[service_name].get_logs()\". \n2. Consider deploying an image as an endpoint instead of using inference+deploy model, depending on the use case. \n3. Use the code \"my_image = image(ws, name='test', version='26')\" and \"service = akswebservice.deploy_from_image(ws, \"test1\", my_image, deployment_config, aks_target)\" to deploy an image as an endpoint.",
        "Answer_preprocessed_content":"try retrieving your service from the workspace directly also, i found deploying an image as an endpoint to be easier than inference+deploy model"
    },
    {
        "Question_id":null,
        "Question_title":"Recommendations on handling the models",
        "Question_body":"Hi,\n\nI'm currently experimenting with ML.NET.\nThe goal is to be able to forecast values for future months based on historical data.\n\n\n\n\nExample scenario:\n\nLet's say a company named XYZ has 10 stores across United States and each branch has it's own historical sales data for the past year.\n\nIf I want to forecast the Net income for each store for the next 6-12 months, does that mean we'll have 1 model for each store since each store has its own set of data?\n\nIf yes, do you have a recommendation on how to organize these models? Are all the models part of the same zip file?\n\nAlso, for any given day, new data comes in for each store, is there a way to automatically add these new data to the model's knowledge?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1657703130580,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/924897\/recommendations-on-handling-the-models.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-14T03:26:58.537Z",
                "Answer_score":0,
                "Answer_body":"@Francis-5653 Thanks for the question. We would recommend using Azure ML for Recommendations.Azure Machine Learning (AML) makes it easy to train, operate, and manage hundreds or even thousands of models. This repo will walk you through the end to end process of creating a many models solution from training to scoring to monitoring.\nThis scenario (MANY MODELS) is very common most of all for Time Series Forecasting. Here is the sample for retails use case.\nhttps:\/\/github.com\/microsoft\/solution-accelerator-many-models\/",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: recommendations on handling the models; Content: hi, i'm currently experimenting with ml.net. the goal is to be able to forecast values for future months based on historical data. example scenario: let's say a company named xyz has 10 stores across united states and each branch has it's own historical sales data for the past year. if i want to forecast the net income for each store for the next 6-12 months, does that mean we'll have 1 model for each store since each store has its own set of data? if yes, do you have a recommendation on how to organize these models? are all the models part of the same zip file? also, for any given day, new data comes in for each store, is there a way to automatically add these new data to the model's knowledge?",
        "Question_original_content_gpt_summary":"The user is seeking advice on how to handle multiple models for forecasting future values based on historical data for multiple stores, and how to incorporate new data into the models.",
        "Question_preprocessed_content":"Title: recommendations on handling the models; Content: hi, i'm currently experimenting with the goal is to be able to forecast values for future months based on historical data. example scenario let's say a company named xyz has stores across united states and each branch has it's own historical sales data for the past year. if i want to forecast the net income for each store for the next months, does that mean we'll have model for each store since each store has its own set of data? if yes, do you have a recommendation on how to organize these models? are all the models part of the same zip file? also, for any given day, new data comes in for each store, is there a way to automatically add these new data to the model's knowledge?",
        "Answer_original_content":"@francis-5653 thanks for the question. we would recommend using for recommendations. (aml) makes it easy to train, operate, and manage hundreds or even thousands of models. this repo will walk you through the end to end process of creating a many models solution from training to scoring to monitoring. this scenario (many models) is very common most of all for time series forecasting. here is the sample for retails use case. https:\/\/github.com\/microsoft\/solution-accelerator-many-models\/",
        "Answer_original_content_gpt_summary":"The answer suggests using Azure Machine Learning (AML) to train, operate, and manage multiple models for time series forecasting. The solution accelerator for many models provides an end-to-end process for creating and monitoring many models. A sample for retail use case is also provided in the given GitHub repository.",
        "Answer_preprocessed_content":"thanks for the question. we would recommend using for machine learning makes it easy to train, operate, and manage hundreds or even thousands of models. this repo will walk you through the end to end process of creating a many models solution from training to scoring to monitoring. this scenario is very common most of all for time series forecasting. here is the sample for retails use case."
    },
    {
        "Question_id":null,
        "Question_title":"Consume scoreing api in excel",
        "Question_body":"I created a new azure automl experiment and deployed it to an endpoint and can access the scoring URI via postman but how do I consume it in excel? Classic ml studio had the excel addin you can use but I don't see the same for URIs created and deployed from an automl experiment.\n\n\n\n\nThis Microsoft Developer video has a demo of exactly what I'm looking to do around the 32 min mark.\nhttps:\/\/youtu.be\/9FGuf55_Xtk?t=1915",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1611073754767,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/236781\/consume-scoreing-api-in-excel.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-20T12:52:45.4Z",
                "Answer_score":1,
                "Answer_body":"@steves-4330 Thanks for the question, Have a look here:\nhttps:\/\/github.com\/retkowsky\/AzureML_Excel\n\nThere is an Excel macro in the Excel file that call an Azure ML service deployed model.\nThere is a quick description of the process in the Word document available in this repo.\nYou can find here as well the Python notebook for creating & deploying the model. No autoML in it but not a big deal to adapt.\n\nPlease try the Consume web services portion Azure ML documentation? That could help you get started.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-04-29T16:09:45.88Z",
                "Answer_score":0,
                "Answer_body":"@steves-4330 @ramr-msft\n\nHi Steve did it work for you? I love this \"new\" Azure ML studio but its integration with Excel is not good. Microsoft seems to nudge us into Power BI.\n\nAlso it would be good to see some examples of web services that work with an Azure Machine Learning model rest endpoint. I would like to see practical examples.\n\nThank you",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: consume scoreing api in excel; Content: i created a new azure automl experiment and deployed it to an endpoint and can access the scoring uri via postman but how do i consume it in excel? classic ml studio had the excel addin you can use but i don't see the same for uris created and deployed from an automl experiment. this microsoft developer video has a demo of exactly what i'm looking to do around the 32 min mark. https:\/\/youtu.be\/9fguf55_xtk?t=1915",
        "Question_original_content_gpt_summary":"The user is looking for a way to consume a scoring API in Excel created from an Azure AutoML experiment.",
        "Question_preprocessed_content":"Title: consume scoreing api in excel; Content: i created a new azure automl experiment and deployed it to an endpoint and can access the scoring uri via postman but how do i consume it in excel? classic ml studio had the excel addin you can use but i don't see the same for uris created and deployed from an automl experiment. this microsoft developer video has a demo of exactly what i'm looking to do around the min mark.",
        "Answer_original_content":"@steves-4330 thanks for the question, have a look here: https:\/\/github.com\/retkowsky\/_excel there is an excel macro in the excel file that call an service deployed model. there is a quick description of the process in the word document available in this repo. you can find here as well the python notebook for creating & deploying the model. no automl in it but not a big deal to adapt. please try the consume web services portion documentation? that could help you get started.",
        "Answer_original_content_gpt_summary":"Possible solutions to consume a scoring API in Excel created from an Azure AutoML experiment are available in the GitHub repository https:\/\/github.com\/retkowsky\/_excel. The repository contains an Excel macro that calls a deployed model service, a Python notebook for creating and deploying the model, and a Word document with a quick description of the process. The documentation on consuming web services could also be helpful in getting started.",
        "Answer_preprocessed_content":"thanks for the question, have a look here there is an excel macro in the excel file that call an service deployed model. there is a quick description of the process in the word document available in this repo. you can find here as well the python notebook for creating & deploying the model. no automl in it but not a big deal to adapt. please try the consume web services portion documentation? that could help you get started."
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning Studio Designer (preview) - Selective module execution",
        "Question_body":"I find that I am often inserting or modifying one module in a flow, and needing to edit several later items in the DAG,\n\nBut since the prior module is now invalidated, It asks me to SUBMIT and run the full experiment, so that I can do things like select columns.\n\nIs there not a way to disable modules? or selectively execute just a subset of modules?\n\nI am using the preview versions of the ML studio.",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1593464903017,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/41135\/azure-machine-learning-studio-designer-preview-sel.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-01T11:32:32.75Z",
                "Answer_score":0,
                "Answer_body":"@StevenGutfreund-9039 Thanks for the feedback.Currently We don't support selected node run yet in Designer. BTW pipeline has the capability to use previous run result if the input data hasn't changed. You can confirm whether a step is reused or not from the recycle icon as shown below:\n\n\n\n\nPlease share your feedback from the ml.azure.com portal to prioritize this feature.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: studio designer (preview) - selective module execution; Content: i find that i am often inserting or modifying one module in a flow, and needing to edit several later items in the dag, but since the prior module is now invalidated, it asks me to submit and run the full experiment, so that i can do things like select columns. is there not a way to disable modules? or selectively execute just a subset of modules? i am using the preview versions of the ml studio.",
        "Question_original_content_gpt_summary":"The user is encountering challenges when attempting to selectively execute a subset of modules in a flow, as the prior module is invalidated and requires the full experiment to be submitted and run.",
        "Question_preprocessed_content":"Title: studio designer selective module execution; Content: i find that i am often inserting or modifying one module in a flow, and needing to edit several later items in the dag, but since the prior module is now invalidated, it asks me to submit and run the full experiment, so that i can do things like select columns. is there not a way to disable modules? or selectively execute just a subset of modules? i am using the preview versions of the ml studio.",
        "Answer_original_content":"@stevengutfreund-9039 thanks for the feedback.currently we don't support selected node run yet in designer. btw pipeline has the capability to use previous run result if the input data hasn't changed. you can confirm whether a step is reused or not from the recycle icon as shown below: please share your feedback from the ml.azure.com portal to prioritize this feature.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n- Currently, selective execution of a subset of modules in a flow is not supported in designer.\n- Pipeline has the capability to use previous run result if the input data hasn't changed.\n- The user can confirm whether a step is reused or not from the recycle icon.\n- The user can share their feedback from the ml.azure.com portal to prioritize the feature of selective execution of a subset of modules in a flow.",
        "Answer_preprocessed_content":"thanks for the we don't support selected node run yet in designer. btw pipeline has the capability to use previous run result if the input data hasn't changed. you can confirm whether a step is reused or not from the recycle icon as shown below please share your feedback from the portal to prioritize this feature."
    },
    {
        "Question_id":null,
        "Question_title":"Hypersweep metric with a dictionary hierarchy",
        "Question_body":"<p>I am creating hyper parameter sweeps of a linear regression problem, which has gone without a hitch so far. However, I am working to improve my skills with wand. In particular, I logged dictionaries of dictionaries. Here is an example:<\/p>\n<pre><code class=\"lang-python\"> wandb.log({\n    'epoch': epoch,     \n    'train':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch},                  \n    'valid':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch}\n  } , step=epoch, commit=True)\n<\/code><\/pre>\n<p>The metric should be the training loss. How does one specify it when using dictionary hierarchies? Have I done it correctly below?<\/p>\n<pre><code class=\"lang-python\"># docs: https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration\nsweep_config3 = {\n    'name' : 'broad_sweep', \n    'method' : 'random',\n    'metric' : {\n                'name': {'train': 'loss'},\n                'goal': 'minimize',\n               },\n    'parameters' : {\n        'lr' : {\n            'distribution': 'log_uniform_values',\n            'min': 1.e-3, \n            'max': 1.e-1},\n        'batch_size' : { 'value': 32 },\n        'optim' : { 'value': 'adamw' },\n        'nb_layers' : { 'values': [0, 2, 4] },\n        'pts_layer': { 'values': [5, 10, 30] },\n        'nb_epochs': { 'value': 200},\n    }\n}\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1659466475629,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":54.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hypersweep-metric-with-a-dictionary-hierarchy\/2839",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-04T23:52:12.738Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a> , for your initial block of code, you can log dictionary of dictionaries just fine in a single run. However you cannot set a sweep up to optimize for multiple metrics at the same time. This will results in the following error message when attempting to do so:<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\nwandb: WARNING To avoid this, please fix the sweep config schema violations below:\nwandb: WARNING   Violation 1. {'loss': None, 'train': None} is not of type 'string'\n<\/code><\/pre>\n<p>You will have to pass a single value in <code>metric<\/code> within your sweep config.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-05T02:40:29.269Z",
                "Answer_body":"<p>Thank you, <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> . I do understand the point you make. But what if in the example above, I wish the metric to be <code>min_loss<\/code>, defined under <code>train<\/code>?<\/p>\n<pre><code class=\"lang-python\"> wandb.log({\n    'epoch': epoch,     \n    'train':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch},                  \n    'valid':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch}\n  } , step=epoch, commit=True)\n<\/code><\/pre>\n<p>Thanks!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-04T02:40:54.441Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: hypersweep metric with a dictionary hierarchy; Content: i am creating hyper parameter sweeps of a linear regression problem, which has gone without a hitch so far. however, i am working to improve my skills with wand. in particular, i logged dictionaries of dictionaries. here is an example: .log({ 'epoch': epoch, 'train':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch}, 'valid':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch} } , step=epoch, commit=true) the metric should be the training loss. how does one specify it when using dictionary hierarchies? have i done it correctly below? # docs: https:\/\/docs..ai\/guides\/sweeps\/configuration sweep_config3 = { 'name' : 'broad_sweep', 'method' : 'random', 'metric' : { 'name': {'train': 'loss'}, 'goal': 'minimize', }, 'parameters' : { 'lr' : { 'distribution': 'log_uniform_values', 'min': 1.e-3, 'max': 1.e-1}, 'batch_size' : { 'value': 32 }, 'optim' : { 'value': 'adamw' }, 'nb_layers' : { 'values': [0, 2, 4] }, 'pts_layer': { 'values': [5, 10, 30] }, 'nb_epochs': { 'value': 200}, } }",
        "Question_original_content_gpt_summary":"The user is encountering challenges with specifying a metric when using a dictionary hierarchy in a hyperparameter sweep of a linear regression problem.",
        "Question_preprocessed_content":"Title: hypersweep metric with a dictionary hierarchy; Content: i am creating hyper parameter sweeps of a linear regression problem, which has gone without a hitch so far. however, i am working to improve my skills with wand. in particular, i logged dictionaries of dictionaries. here is an example the metric should be the training loss. how does one specify it when using dictionary hierarchies? have i done it correctly below?",
        "Answer_original_content":"hi @erlebacher , for your initial block of code, you can log dictionary of dictionaries just fine in a single run. however you cannot set a sweep up to optimize for multiple metrics at the same time. this will results in the following error message when attempting to do so: : warning malformed sweep config detected! this may cause your sweep to behave in unexpected ways. : warning to avoid this, please fix the sweep config schema violations below: : warning violation 1. {'loss': none, 'train': none} is not of type 'string' you will have to pass a single value in metric within your sweep config. thank you, @mohammadbakir . i do understand the point you make. but what if in the example above, i wish the metric to be min_loss, defined under train? .log({ 'epoch': epoch, 'train':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch}, 'valid':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch} } , step=epoch, commit=true) thanks! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The answer suggests that the user cannot set up a hyperparameter sweep to optimize for multiple metrics at the same time. The solution is to pass a single value in the metric within the sweep config. The user can log a dictionary of dictionaries in a single run, but they need to ensure that the metric is a single value.",
        "Answer_preprocessed_content":"hi , for your initial block of code, you can log dictionary of dictionaries just fine in a single run. however you cannot set a sweep up to optimize for multiple metrics at the same time. this will results in the following error message when attempting to do so you will have to pass a single value in within your sweep config. thank you, . i do understand the point you make. but what if in the example above, i wish the metric to be , defined under ? thanks! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":63248562.0,
        "Question_title":"How to handle a .csv input for use in Tensorflow Serving batch transform?",
        "Question_body":"<p><strong>Information:<\/strong>\nI am loading an existing trained model.tar.gz from an S3 bucket, and want to perform a batch transform with a .csv containing the input data. The data.csv is structured in such a way that reading it into a pandas DataFrame gives me rows of complete prediction inputs.<\/p>\nNotes:\n<ul>\n<li>This is done on Amazon Sagemaker using the Python SDK<\/li>\n<li>BATCH_TRANSFORM_INPUT is the path to data.csv.<\/li>\n<li>I'm able to load the contents inside model.tar.gz and use them for inference on my local machine using tensorflow, and the logs show <code>2020-08-04 13:35:01.123557: I tensorflow_serving\/core\/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}<\/code>so the model seems to have been trained and saved properly.<\/li>\n<li>The data.csv is in the exact same format as the training data, which means one row per &quot;prediction&quot; where all columns in that row represents the different features.<\/li>\n<li>Changing the argument strategy to 'MultiRecord' gives the same error<\/li>\n<li>[path in s3] is a substitute for the real path as i don't want to reveal any bucket information.<\/li>\n<li>TensorFlow ModelServer: 2.0.0+dev.sha.ab786af<\/li>\n<li>TensorFlow Library: 2.0.2<\/li>\n<\/ul>\n<p>Where 1-5 are features, the file data.csv looks like:<\/p>\n<pre><code>+------+-------------------------+---------+----------+---------+----------+----------+\n| UNIT | TS                      | 1       | 2        | 3       | 4        | 5        |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:01:00.000 | 1.81766 | 0.178043 | 1.33607 | 25.42162 | 12.85445 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:02:00.000 | 1.81673 | 0.178168 | 1.30159 | 25.48204 | 12.87305 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:03:00.000 | 1.8155  | 0.176242 | 1.38399 | 25.35309 | 12.47222 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:04:00.000 | 1.81530 | 0.176398 | 1.39781 | 25.18216 | 12.16837 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:05:00.000 | 1.81505 | 0.151682 | 1.38451 | 25.22351 | 12.41623 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n<\/code><\/pre>\n<p>inference.py currently looks like:<\/p>\n<pre><code>def input_handler(data, context):\n    import pandas as pd\n    if context.request_content_type == 'text\/csv':\n        payload = pd.read_csv(data)\n        instance = [{&quot;dataset&quot;: payload}]\n        return json.dumps({&quot;instances&quot;: instance})\n    else:\n        _return_error(416, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or 'Unknown'))\n<\/code><\/pre>\n<h3>The problem:<\/h3>\n<p>When the following code runs in my jupyter Notebook:<\/p>\n<pre><code>sagemaker_model = Model(model_data = '[path in s3]\/savedmodel\/model.tar.gz'),  \n                        sagemaker_session=sagemaker_session,\n                        role = role,\n                        framework_version='2.0',\n                        entry_point = os.path.join('training', 'inference.py')\n                        )\n\ntf_serving_transformer = sagemaker_model.transformer(instance_count=1,\n                                                     instance_type='ml.p2.xlarge',\n                                                     max_payload=1,\n                                                     output_path=BATCH_TRANSFORM_OUTPUT_DIR,\n                                                     strategy='SingleRecord')\n\n\ntf_serving_transformer.transform(data=BATCH_TRANSFORM_INPUT, data_type='S3Prefix', content_type='text\/csv')\ntf_serving_transformer.wait()\n<\/code><\/pre>\n<p>The model seems to get loaded, but I end up with the following error:\n<code>2020-08-04T09:54:27.415:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=1, BatchStrategy=SINGLE_RECORD 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: ClientError: 400 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv:  2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: Message: 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: { &quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: \\&quot;\\&quot; Type: String is not of expected type: float&quot; } <\/code><\/p>\n<p>Error more clearly:<\/p>\n<p><strong>ClientError: 400\nMessage: {&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float&quot;}<\/strong><\/p>\n<p>If i understand this error correctly, something is wrong with the way my data is structured, so that sagemaker fails to deliver the input data to the TFS model. I suppose there is some &quot;input handling&quot; missing in my inference.py. Maybe the csv data has to somehow be translated into a compatible JSON, for TFS to use it? What exactly has to be done in input_handler() ?<\/p>\n<p>I appreciate all help, and am sorry for this confusing case. If there is any additional information needed, please ask and I'll gladly provide what I can.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1596549653107,
        "Question_favorite_count":1.0,
        "Question_score":2.0,
        "Question_view_count":774.0,
        "Owner_creation_time":1596548853443,
        "Owner_last_access_time":1597839282567,
        "Owner_reputation":41.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":"<p><strong>Solution:<\/strong> The problem was solved by saving the dataframe as .csv using the arguments header=False, index=False. This makes the saved csv not include the dataframe indexing labels. TFS accepted a clean .csv with only float values (without labels). I assume the error message <em>Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float<\/em> refers to the first cell in the csv, which if the csv was exported with labels is just an empty cell. When it got an empty string instead of a float value it got confused.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1596696668183,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":1596696699176,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63248562",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to handle a .csv input for use in tensorflow serving batch transform?; Content: information: i am loading an existing trained model.tar.gz from an s3 bucket, and want to perform a batch transform with a .csv containing the input data. the data.csv is structured in such a way that reading it into a pandas dataframe gives me rows of complete prediction inputs. notes: this is done on using the python sdk batch_transform_input is the path to data.csv. i'm able to load the contents inside model.tar.gz and use them for inference on my local machine using tensorflow, and the logs show 2020-08-04 13:35:01.123557: i tensorflow_serving\/core\/loader_harness.cc:87] successfully loaded servable version {name: model version: 1}so the model seems to have been trained and saved properly. the data.csv is in the exact same format as the training data, which means one row per \"prediction\" where all columns in that row represents the different features. changing the argument strategy to 'multirecord' gives the same error [path in s3] is a substitute for the real path as i don't want to reveal any bucket information. tensorflow modelserver: 2.0.0+dev.sha.ab786af tensorflow library: 2.0.2 where 1-5 are features, the file data.csv looks like: +------+-------------------------+---------+----------+---------+----------+----------+ | unit | ts | 1 | 2 | 3 | 4 | 5 | +------+-------------------------+---------+----------+---------+----------+----------+ | 110 | 2018-01-01 00:01:00.000 | 1.81766 | 0.178043 | 1.33607 | 25.42162 | 12.85445 | +------+-------------------------+---------+----------+---------+----------+----------+ | 110 | 2018-01-01 00:02:00.000 | 1.81673 | 0.178168 | 1.30159 | 25.48204 | 12.87305 | +------+-------------------------+---------+----------+---------+----------+----------+ | 110 | 2018-01-01 00:03:00.000 | 1.8155 | 0.176242 | 1.38399 | 25.35309 | 12.47222 | +------+-------------------------+---------+----------+---------+----------+----------+ | 110 | 2018-01-01 00:04:00.000 | 1.81530 | 0.176398 | 1.39781 | 25.18216 | 12.16837 | +------+-------------------------+---------+----------+---------+----------+----------+ | 110 | 2018-01-01 00:05:00.000 | 1.81505 | 0.151682 | 1.38451 | 25.22351 | 12.41623 | +------+-------------------------+---------+----------+---------+----------+----------+ inference.py currently looks like: def input_handler(data, context): import pandas as pd if context.request_content_type == 'text\/csv': payload = pd.read_csv(data) instance = [{\"dataset\": payload}] return json.dumps({\"instances\": instance}) else: _return_error(416, 'unsupported content type \"{}\"'.format(context.request_content_type or 'unknown')) the problem: when the following code runs in my jupyter notebook: _model = model(model_data = '[path in s3]\/savedmodel\/model.tar.gz'), _session=_session, role = role, framework_version='2.0', entry_point = os.path.join('training', 'inference.py') ) tf_serving_transformer = _model.transformer(instance_count=1, instance_type='ml.p2.xlarge', max_payload=1, output_path=batch_transform_output_dir, strategy='singlerecord') tf_serving_transformer.transform(data=batch_transform_input, data_type='s3prefix', content_type='text\/csv') tf_serving_transformer.wait() the model seems to get loaded, but i end up with the following error: 2020-08-04t09:54:27.415:[ logs]: maxconcurrenttransforms=1, maxpayloadinmb=1, batchstrategy=single_record 2020-08-04t09:54:27.503:[ logs]: [path in s3]\/data.csv: clienterror: 400 2020-08-04t09:54:27.503:[ logs]: [path in s3]\/data.csv: 2020-08-04t09:54:27.503:[ logs]: [path in s3]\/data.csv: message: 2020-08-04t09:54:27.503:[ logs]: [path in s3]\/data.csv: { \"error\": \"failed to process element: 0 of 'instances' list. error: invalid argument: json value: \\\"\\\" type: string is not of expected type: float\" } error more clearly: clienterror: 400 message: {\"error\": \"failed to process element: 0 of 'instances' list. error: invalid argument: json value: \"\" type: string is not of expected type: float\"} if i understand this error correctly, something is wrong with the way my data is structured, so that fails to deliver the input data to the tfs model. i suppose there is some \"input handling\" missing in my inference.py. maybe the csv data has to somehow be translated into a compatible json, for tfs to use it? what exactly has to be done in input_handler() ? i appreciate all help, and am sorry for this confusing case. if there is any additional information needed, please ask and i'll gladly provide what i can.",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to perform a batch transform with a .csv containing the input data for a trained model.tar.gz loaded from an s3 bucket, and is looking for help in understanding what needs to be done in the input_handler() to translate the csv data into a compatible json for TensorFlow Serving to use.",
        "Question_preprocessed_content":"Title: how to handle a .csv input for use in tensorflow serving batch transform?; Content: information i am loading an existing trained from an s bucket, and want to perform a batch transform with a .csv containing the input data. the is structured in such a way that reading it into a pandas dataframe gives me rows of complete prediction inputs. notes this is done on using the python sdk is the path to i'm able to load the contents inside and use them for inference on my local machine using tensorflow, and the logs show so the model seems to have been trained and saved properly. the is in the exact same format as the training data, which means one row per prediction where all columns in that row represents the different features. changing the argument strategy to 'multirecord' gives the same error path in s is a substitute for the real path as i don't want to reveal any bucket information. tensorflow modelserver tensorflow library where are features, the file looks like currently looks like the problem when the following code runs in my jupyter notebook the model seems to get loaded, but i end up with the following error error more clearly clienterror message if i understand this error correctly, something is wrong with the way my data is structured, so that fails to deliver the input data to the tfs model. i suppose there is some input handling missing in my maybe the csv data has to somehow be translated into a compatible json, for tfs to use it? what exactly has to be done in ? i appreciate all help, and am sorry for this confusing case. if there is any additional information needed, please ask and i'll gladly provide what i can.",
        "Answer_original_content":"solution: the problem was solved by saving the dataframe as .csv using the arguments header=false, index=false. this makes the saved csv not include the dataframe indexing labels. tfs accepted a clean .csv with only float values (without labels). i assume the error message invalid argument: json value: \"\" type: string is not of expected type: float refers to the first cell in the csv, which if the csv was exported with labels is just an empty cell. when it got an empty string instead of a float value it got confused.",
        "Answer_original_content_gpt_summary":"The solution to the problem of encountering an error while performing a batch transform with a .csv containing input data for a trained model.tar.gz loaded from an s3 bucket is to save the dataframe as .csv using the arguments header=false, index=false. This makes the saved csv not include the dataframe indexing labels. TensorFlow Serving accepts a clean .csv with only float values (without labels). The error message \"invalid argument: json value: \"\" type: string is not of expected type: float\" refers to the first cell in the csv, which if the csv was exported with labels is just an empty cell. When it got an empty string instead of a float value it got confused.",
        "Answer_preprocessed_content":"solution the problem was solved by saving the dataframe as .csv using the arguments header false, index false. this makes the saved csv not include the dataframe indexing labels. tfs accepted a clean .csv with only float values . i assume the error message invalid argument json value type string is not of expected type float refers to the first cell in the csv, which if the csv was exported with labels is just an empty cell. when it got an empty string instead of a float value it got confused."
    },
    {
        "Question_id":62949488.0,
        "Question_title":"AMLS Experiment run stuck in status \"Running\"",
        "Question_body":"<p>I made an Azure Machine Learning Service Experiment run and logged neural network losses with Jupyter Notebook. Logging worked fine and NN training completed as it should. However, the experiment is stuck in the running status. Shutting down the compute resources does not shut down the Experiment run and I cannot cancel it from the Experiment panel. In addition, the run does not have any log-files.<\/p>\n<p>Has anyone had the same behavior? Run has now lasted for over 24 hours.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/KzAoS.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KzAoS.jpg\" alt=\"AMLS Experiment run\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1594972260903,
        "Question_favorite_count":0.0,
        "Question_score":2.0,
        "Question_view_count":887.0,
        "Owner_creation_time":1563874553356,
        "Owner_last_access_time":1663957374736,
        "Owner_reputation":881.0,
        "Owner_up_votes":1241.0,
        "Owner_down_votes":14.0,
        "Owner_views":106.0,
        "Answer_body":"<p>this totally happens from time to time. it is certainly frustrating especially because the &quot;Cancel&quot; button it grayed out. You can use either the CLI or Python SDK  to cancel the run.<\/p>\n<h2>SDK<\/h2>\n<h3>&gt;= 1.16.0<\/h3>\n<p>As of version <code>1.16.0<\/code> you no longer an <code>Experiment<\/code> object is no longer needed. Instead you can access using the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#get-workspace--run-id-&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\"><code>Run<\/code><\/a> or <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace(class)?view=azure-ml-py#get-run-run-id-&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\"><code>Workspace<\/code><\/a> objects directly<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Experiment, Run, VERSION\nprint(&quot;SDK version:&quot;, VERSION)\n\nws = Workspace.from_config()\n\nrun = ws.get_run('YOUR_RUN_ID')\nrun = Run().get(ws, 'YOUR_RUN_ID') # also works\nrun.cancel()\n<\/code><\/pre>\n<h3>&lt; 1.16.0<\/h3>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Experiment, Run, VERSION\nprint(&quot;SDK version:&quot;, VERSION)\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = 'YOUR_EXP_NAME')\n\nrun = Run(exp, run_id='YOUR STEP RUN ID')\n\nrun.cancel() # or run.fail()\n<\/code><\/pre>\n<h1>CLI<\/h1>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-azure-machine-learning-cli#install-the-extension\" rel=\"nofollow noreferrer\">More CLI details here<\/a><\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>az login\naz ml run cancel --run YOUR_RUN_ID\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1595004339510,
        "Answer_score":5.0,
        "Owner_location":"Helsinki, Finland",
        "Question_last_edit_time":1594989303640,
        "Answer_last_edit_time":1604912331752,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62949488",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: amls experiment run stuck in status \"running\"; Content: i made an service experiment run and logged neural network losses with jupyter notebook. logging worked fine and nn training completed as it should. however, the experiment is stuck in the running status. shutting down the compute resources does not shut down the experiment run and i cannot cancel it from the experiment panel. in addition, the run does not have any log-files. has anyone had the same behavior? run has now lasted for over 24 hours.",
        "Question_original_content_gpt_summary":"The user is experiencing an issue with an Azure Machine Learning Service experiment run that is stuck in the \"running\" status and cannot be cancelled or shut down, lasting for over 24 hours.",
        "Question_preprocessed_content":"Title: amls experiment run stuck in status running ; Content: i made an service experiment run and logged neural network losses with jupyter notebook. logging worked fine and nn training completed as it should. however, the experiment is stuck in the running status. shutting down the compute resources does not shut down the experiment run and i cannot cancel it from the experiment panel. in addition, the run does not have any has anyone had the same behavior? run has now lasted for over hours.",
        "Answer_original_content":"this totally happens from time to time. it is certainly frustrating especially because the \"cancel\" button it grayed out. you can use either the cli or python sdk to cancel the run. sdk >= 1.16.0 as of version 1.16.0 you no longer an experiment object is no longer needed. instead you can access using the run or workspace objects directly from .core import workspace, experiment, run, version print(\"sdk version:\", version) ws = workspace.from_config() run = ws.get_run('your_run_id') run = run().get(ws, 'your_run_id') # also works run.cancel() < 1.16.0 from .core import workspace, experiment, run, version print(\"sdk version:\", version) ws = workspace.from_config() exp = experiment(workspace = ws, name = 'your_exp_name') run = run(exp, run_id='your step run id') run.cancel() # or run.fail() cli more cli details here az login az ml run cancel --run your_run_id",
        "Answer_original_content_gpt_summary":"Possible solutions to the issue of an Azure Machine Learning Service experiment run stuck in the \"running\" status and cannot be cancelled or shut down are to use either the CLI or Python SDK to cancel the run. For SDK version 1.16.0 and above, an experiment object is no longer needed, and instead, you can access the run or workspace objects directly. For versions below 1.16.0, you can use the experiment object to cancel the run. Additionally, you can use the CLI command \"az ml run cancel --run your_run_id\" to cancel the run.",
        "Answer_preprocessed_content":"this totally happens from time to time. it is certainly frustrating especially because the cancel button it grayed out. you can use either the cli or python sdk to cancel the run. sdk as of version you no longer an object is no longer needed. instead you can access using the or objects directly cli more cli details here"
    },
    {
        "Question_id":null,
        "Question_title":"Lambda Function to invoke sagemaker endpoint",
        "Question_body":"Hi AWS, I need to create a lambda function that will invoke the SageMaker endpoint that will send a text description for which it will return a generated image.\n\nThe endpoint is generated using HuggingFace model.\n\nI need your help with the code and the steps to obtain it.\n\nThanks Arjun Goel",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1657965474513,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":107.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU33wE3pnRS9Om2yfVt4EIAg\/lambda-function-to-invoke-sagemaker-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-17T16:20:21.662Z",
                "Answer_score":0,
                "Answer_body":"There is a blog that shows how you can invoke a Sagemaker endpoint from a lambda function - https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-16T18:28:31.470Z",
                "Answer_score":0,
                "Answer_body":"Yes I have gone through the approach as mentioned by you above but the problem is I am not a Machine Learning Expert so it will take me sometime to write inference for Text-to-Image. If you have python code for that it would be great as I need to complete that on urgent basis.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: lambda function to invoke endpoint; Content: hi aws, i need to create a lambda function that will invoke the endpoint that will send a text description for which it will return a generated image. the endpoint is generated using huggingface model. i need your help with the code and the steps to obtain it. thanks arjun goel",
        "Question_original_content_gpt_summary":"The user Arjun Goel is seeking help to create a Lambda function to invoke an endpoint that will send a text description and return a generated image, using a Huggingface model.",
        "Question_preprocessed_content":"Title: lambda function to invoke endpoint; Content: hi aws, i need to create a lambda function that will invoke the endpoint that will send a text description for which it will return a generated image. the endpoint is generated using huggingface model. i need your help with the code and the steps to obtain it. thanks arjun goel",
        "Answer_original_content":"there is a blog that shows how you can invoke a endpoint from a lambda function - https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon--model-endpoint-using-amazon-api-gateway-and-aws-lambda\/ yes i have gone through the approach as mentioned by you above but the problem is i am not a machine learning expert so it will take me sometime to write inference for text-to-image. if you have python code for that it would be great as i need to complete that on urgent basis.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n- There is a blog that shows how to invoke an endpoint from a Lambda function.\n- The user has gone through the approach mentioned in the question, but they are not a machine learning expert, so it will take them some time to write inference for text-to-image.\n- The user is requesting Python code for text-to-image inference to complete the task on an urgent basis.",
        "Answer_preprocessed_content":"there is a blog that shows how you can invoke a endpoint from a lambda function yes i have gone through the approach as mentioned by you above but the problem is i am not a machine learning expert so it will take me sometime to write inference for if you have python code for that it would be great as i need to complete that on urgent basis."
    },
    {
        "Question_id":69751254.0,
        "Question_title":"Submitting multiple runs to the same node on AzureML",
        "Question_body":"<p>I want to perform hyperparameter search using AzureML. My models are small (around 1GB) thus I would like to run multiple models on the same GPU\/node to save costs but I do not know how to achieve this.<\/p>\n<p>The way I currently submit jobs is the following (resulting in one training run per GPU\/node):<\/p>\n<pre><code>experiment = Experiment(workspace, experiment_name)\nconfig = ScriptRunConfig(source_directory=&quot;.\/src&quot;,\n                         script=&quot;train.py&quot;,\n                         compute_target=&quot;gpu_cluster&quot;,\n                         environment=&quot;env_name&quot;,\n                         arguments=[&quot;--args args&quot;])\nrun = experiment.submit(config)\n<\/code><\/pre>\n<p><code>ScriptRunConfig<\/code> can be provided with a <code>distributed_job_config<\/code>. I tried to use <code>MpiConfiguration<\/code> there but if this is done the run fails due to an MPI error that reads as if the cluster is configured to only allow one run per node:<\/p>\n<blockquote>\n<pre><code>Open RTE detected a bad parameter in hostfile: [...]\nThe max_slots parameter is less than the slots parameter:\nslots = 3\nmax_slots = 1\n[...] ORTE_ERROR_LOG: Bad Parameter in file util\/hostfile\/hostfile.c at line 407\n<\/code><\/pre>\n<\/blockquote>\n<p>Using <code>HyperDriveConfig<\/code> also defaults to submitting one run to one GPU and additionally providing a <code>MpiConfiguration<\/code> leads to the same error as shown above.<\/p>\n<p>I guess I could always rewrite my train script to train multiple models in parallel, s.t. each <code>run<\/code> wraps multiple trainings. I would like to avoid this option though, because then logging and checkpoint writes become increasingly messy and it would require a large refactor of the train pipeline. Also this functionality seems so basic that I hope there is a way to do this gracefully. Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1635412142523,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":364.0,
        "Owner_creation_time":1396607378876,
        "Owner_last_access_time":1657293849527,
        "Owner_reputation":107.0,
        "Owner_up_votes":2.0,
        "Owner_down_votes":0.0,
        "Owner_views":17.0,
        "Answer_body":"<p>Use Run.create_children method which will start child runs that are \u201clocal\u201d to the parent run, and don\u2019t need authentication.<\/p>\n<p>For AMLcompute max_concurrent_runs map to maximum number of nodes that will be used to run  a hyperparameter tuning run.\nSo there would be 1 execution per node.<\/p>\n<p>single service deployed but you can load multiple model versions in the init then the score function, depending on the request\u2019s param, uses particular model version to score.\nor with the new ML Endpoints (Preview).\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints\" rel=\"nofollow noreferrer\">What are endpoints (preview) - Azure Machine Learning | Microsoft Docs<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1635511999763,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1635512880996,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69751254",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: submitting multiple runs to the same node on ; Content: i want to perform hyperparameter search using . my models are small (around 1gb) thus i would like to run multiple models on the same gpu\/node to save costs but i do not know how to achieve this. the way i currently submit jobs is the following (resulting in one training run per gpu\/node): experiment = experiment(workspace, experiment_name) config = scriptrunconfig(source_directory=\".\/src\", script=\"train.py\", compute_target=\"gpu_cluster\", environment=\"env_name\", arguments=[\"--args args\"]) run = experiment.submit(config) scriptrunconfig can be provided with a distributed_job_config. i tried to use mpiconfiguration there but if this is done the run fails due to an mpi error that reads as if the cluster is configured to only allow one run per node: open rte detected a bad parameter in hostfile: [...] the max_slots parameter is less than the slots parameter: slots = 3 max_slots = 1 [...] orte_error_log: bad parameter in file util\/hostfile\/hostfile.c at line 407 using hyperdriveconfig also defaults to submitting one run to one gpu and additionally providing a mpiconfiguration leads to the same error as shown above. i guess i could always rewrite my train script to train multiple models in parallel, s.t. each run wraps multiple trainings. i would like to avoid this option though, because then logging and checkpoint writes become increasingly messy and it would require a large refactor of the train pipeline. also this functionality seems so basic that i hope there is a way to do this gracefully. any ideas?",
        "Question_original_content_gpt_summary":"The user is encountering challenges when attempting to submit multiple runs to the same node on Azure Machine Learning, as the current configuration results in one training run per GPU\/node.",
        "Question_preprocessed_content":"Title: submitting multiple runs to the same node on ; Content: i want to perform hyperparameter search using . my models are small thus i would like to run multiple models on the same to save costs but i do not know how to achieve this. the way i currently submit jobs is the following can be provided with a . i tried to use there but if this is done the run fails due to an mpi error that reads as if the cluster is configured to only allow one run per node using also defaults to submitting one run to one gpu and additionally providing a leads to the same error as shown above. i guess i could always rewrite my train script to train multiple models in parallel, each wraps multiple trainings. i would like to avoid this option though, because then logging and checkpoint writes become increasingly messy and it would require a large refactor of the train pipeline. also this functionality seems so basic that i hope there is a way to do this gracefully. any ideas?",
        "Answer_original_content":"use run.create_children method which will start child runs that are local to the parent run, and dont need authentication. for amlcompute max_concurrent_runs map to maximum number of nodes that will be used to run a hyperparameter tuning run. so there would be 1 execution per node. single service deployed but you can load multiple model versions in the init then the score function, depending on the requests param, uses particular model version to score. or with the new ml endpoints (preview). what are endpoints (preview) - | microsoft docs",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of submitting multiple runs to the same node on Azure Machine Learning are to use the run.create_children method to start child runs that are local to the parent run, and to set the max_concurrent_runs parameter to limit the number of nodes used for a hyperparameter tuning run. Another solution is to deploy a single service that can load multiple model versions and use a specific version to score based on the request parameter. Alternatively, the new ml endpoints (preview) can be used.",
        "Answer_preprocessed_content":"use method which will start child runs that are local to the parent run, and dont need authentication. for amlcompute map to maximum number of nodes that will be used to run a hyperparameter tuning run. so there would be execution per node. single service deployed but you can load multiple model versions in the init then the score function, depending on the requests param, uses particular model version to score. or with the new ml endpoints . what are endpoints microsoft docs"
    },
    {
        "Question_id":57172147.0,
        "Question_title":"'no SavedModel bundles found!' on tensorflow_hub model deployment to AWS SageMaker",
        "Question_body":"<p>I attempting to deploy the universal-sentence-encoder model to a aws Sagemaker endpoint and am getting the error <code>raise ValueError('no SavedModel bundles found!')<\/code><\/p>\n\n<p>I have shown my code below, I have a feeling that one of my paths is incorrect<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sagemaker import get_execution_role\nfrom sagemaker.tensorflow.serving import Model\n\ndef tfhub_to_savedmodel(model_name,uri):\n    tfhub_uri = uri\n    model_path = 'encoder_model\/' + model_name\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        module = hub.Module(tfhub_uri) \n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n\n        # define the model outputs\n        # we want the class ids and probabilities for the top 3 classes\n        logits = module(inputs['text'])\n        outputs = {\n            'vector': logits,\n        }\n\n        # export the model\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n\n\nsagemaker_role = get_execution_role()\n\n!tar -C \"$PWD\" -czf encoder.tar.gz encoder_model\/\nmodel_data = Session().upload_data(path='encoder.tar.gz',key_prefix='model')\n\nenv = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'universal-sentence-encoder-large'}\n\nmodel = Model(model_data=model_data, role=sagemaker_role, framework_version=1.12, env=env)\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1563915680943,
        "Question_favorite_count":null,
        "Question_score":5.0,
        "Question_view_count":2621.0,
        "Owner_creation_time":1531840489147,
        "Owner_last_access_time":1636067734716,
        "Owner_reputation":425.0,
        "Owner_up_votes":12.0,
        "Owner_down_votes":1.0,
        "Owner_views":92.0,
        "Answer_body":"<p>I suppose you started from this example? <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_serving_container<\/a><\/p>\n\n<p>It looks like you're not saving the TF Serving bundle properly: the model version number is missing, because of this line:<\/p>\n\n<pre><code>model_path = 'encoder_model\/' + model_name\n<\/code><\/pre>\n\n<p>Replacing it with this should fix your problem:<\/p>\n\n<pre><code>model_path = '{}\/{}\/00000001'.format('encoder_model\/', model_name)\n<\/code><\/pre>\n\n<p>Your model artefact should look like this (I used the model in the notebook above):<\/p>\n\n<pre><code>mobilenet\/\nmobilenet\/mobilenet_v2_140_224\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/saved_model.pb\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.data-00000-of-00001\nmobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.index\n<\/code><\/pre>\n\n<p>Then, upload to S3 and deploy.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1563978858283,
        "Answer_score":6.0,
        "Owner_location":"Berkeley, CA, USA",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57172147",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: 'no savedmodel bundles found!' on tensorflow_hub model deployment to ; Content: i attempting to deploy the universal-sentence-encoder model to a endpoint and am getting the error raise valueerror('no savedmodel bundles found!') i have shown my code below, i have a feeling that one of my paths is incorrect import tensorflow as tf import tensorflow_hub as hub import numpy as np from import get_execution_role from .tensorflow.serving import model def tfhub_to_savedmodel(model_name,uri): tfhub_uri = uri model_path = 'encoder_model\/' + model_name with tf.session(graph=tf.graph()) as sess: module = hub.module(tfhub_uri) input_params = module.get_input_info_dict() dtype = input_params['text'].dtype shape = input_params['text'].get_shape() # define the model inputs inputs = {'text': tf.placeholder(dtype, shape, 'text')} # define the model outputs # we want the class ids and probabilities for the top 3 classes logits = module(inputs['text']) outputs = { 'vector': logits, } # export the model sess.run([tf.global_variables_initializer(), tf.tables_initializer()]) tf.saved_model.simple_save( sess, model_path, inputs=inputs, outputs=outputs) return model_path _role = get_execution_role() !tar -c \"$pwd\" -czf encoder.tar.gz encoder_model\/ model_data = session().upload_data(path='encoder.tar.gz',key_prefix='model') env = {'_tfs_default_model_name': 'universal-sentence-encoder-large'} model = model(model_data=model_data, role=_role, framework_version=1.12, env=env) predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to deploy a universal-sentence-encoder model to an endpoint.",
        "Question_preprocessed_content":"Title: 'no savedmodel bundles found!' on model deployment to ; Content: i attempting to deploy the model to a endpoint and am getting the error i have shown my code below, i have a feeling that one of my paths is incorrect",
        "Answer_original_content":"i suppose you started from this example? https:\/\/github.com\/awslabs\/amazon--examples\/tree\/master\/-python-sdk\/tensorflow_serving_container it looks like you're not saving the tf serving bundle properly: the model version number is missing, because of this line: model_path = 'encoder_model\/' + model_name replacing it with this should fix your problem: model_path = '{}\/{}\/00000001'.format('encoder_model\/', model_name) your model artefact should look like this (i used the model in the notebook above): mobilenet\/ mobilenet\/mobilenet_v2_140_224\/ mobilenet\/mobilenet_v2_140_224\/00000001\/ mobilenet\/mobilenet_v2_140_224\/00000001\/saved_model.pb mobilenet\/mobilenet_v2_140_224\/00000001\/variables\/ mobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.data-00000-of-00001 mobilenet\/mobilenet_v2_140_224\/00000001\/variables\/variables.index then, upload to s3 and deploy.",
        "Answer_original_content_gpt_summary":"Possible solutions to the error encountered when deploying a universal-sentence-encoder model to an endpoint include properly saving the tf serving bundle with the correct model version number and ensuring that the model artifact is structured correctly. The answer suggests replacing a line of code to fix the problem and provides an example of how the model artifact should be structured. Finally, the answer recommends uploading the model to S3 and deploying it.",
        "Answer_preprocessed_content":"i suppose you started from this example? it looks like you're not saving the tf serving bundle properly the model version number is missing, because of this line replacing it with this should fix your problem your model artefact should look like this then, upload to s and deploy."
    },
    {
        "Question_id":null,
        "Question_title":"How to install wandb on a docker image for arm?",
        "Question_body":"<p>My docker building failed at the <code>RUN <\/code><\/p>\n<p>with:<\/p>\n<pre><code class=\"lang-auto\">(meta_learning) brandomiranda~ \u276f docker build -f ~\/iit-term-synthesis\/Dockerfile_arm -t brandojazz\/iit-term-synthesis:test_arm ~\/iit-term-synthesis\/\n\n[+] Building 184.7s (20\/28)\n =&gt; [internal] load build definition from Dockerfile_arm                                                                                           0.0s\n =&gt; =&gt; transferring dockerfile: 41B                                                                                                                0.0s\n =&gt; [internal] load .dockerignore                                                                                                                  0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                    0.0s\n =&gt; [internal] load metadata for docker.io\/continuumio\/miniconda3:latest                                                                           0.0s\n =&gt; [ 1\/24] FROM docker.io\/continuumio\/miniconda3                                                                                                  0.0s\n =&gt; https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main                                                                                     0.3s\n =&gt; CACHED [ 2\/24] RUN apt-get update   &amp;&amp; apt-get install -y --no-install-recommends     ssh     git     m4     libgmp-dev     opam     wget      0.0s\n =&gt; CACHED [ 3\/24] RUN useradd -m bot                                                                                                              0.0s\n =&gt; CACHED [ 4\/24] WORKDIR \/home\/bot                                                                                                               0.0s\n =&gt; CACHED [ 5\/24] ADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json                                                     0.0s\n =&gt; CACHED [ 6\/24] RUN opam init --disable-sandboxing                                                                                              0.0s\n =&gt; CACHED [ 7\/24] RUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda                     0.0s\n =&gt; CACHED [ 8\/24] RUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1                                                          0.0s\n =&gt; CACHED [ 9\/24] RUN eval $(opam env)                                                                                                            0.0s\n =&gt; CACHED [10\/24] RUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released                                                               0.0s\n =&gt; CACHED [11\/24] RUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released                                  0.0s\n =&gt; CACHED [12\/24] RUN opam update --all                                                                                                           0.0s\n =&gt; CACHED [13\/24] RUN opam pin add -y coq 8.11.0                                                                                                  0.0s\n =&gt; [14\/24] RUN opam install -y coq-serapi                                                                                                       176.3s\n =&gt; [15\/24] RUN eval $(opam env)                                                                                                                   0.2s\n =&gt; ERROR [16\/24] RUN pip install wandb --upgrade                                                                                                  8.0s\n------\n &gt; [16\/24] RUN pip install wandb --upgrade:\n#20 0.351 Defaulting to user installation because normal site-packages is not writeable\n#20 0.637 Collecting wandb\n#20 0.986   Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n#20 1.365 Requirement already satisfied: setuptools in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (61.2.0)\n#20 1.366 Requirement already satisfied: six&gt;=1.13.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (1.16.0)\n#20 1.409 Collecting promise&lt;3,&gt;=2.0\n#20 1.472   Downloading promise-2.3.tar.gz (19 kB)\n#20 2.087 Collecting PyYAML\n#20 2.154   Downloading PyYAML-6.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (731 kB)\n#20 2.431 Collecting protobuf&lt;4.0dev,&gt;=3.12.0\n#20 2.492   Downloading protobuf-3.20.1-cp39-cp39-manylinux2014_aarch64.whl (917 kB)\n#20 2.648 Collecting setproctitle\n#20 2.706   Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30 kB)\n#20 2.763 Collecting Click!=8.0.0,&gt;=7.0\n#20 2.818   Downloading click-8.1.3-py3-none-any.whl (96 kB)\n#20 2.902 Collecting sentry-sdk&gt;=1.0.0\n#20 2.962   Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n#20 3.112 Collecting psutil&gt;=5.0.0\n#20 3.172   Downloading psutil-5.9.2.tar.gz (479 kB)\n#20 3.871 Collecting pathtools\n#20 3.937   Downloading pathtools-0.1.2.tar.gz (11 kB)\n#20 4.431 Collecting shortuuid&gt;=0.5.0\n#20 4.509   Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n#20 4.512 Requirement already satisfied: requests&lt;3,&gt;=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (2.27.1)\n#20 4.568 Collecting docker-pycreds&gt;=0.4.0\n#20 4.636   Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n#20 4.695 Collecting GitPython&gt;=1.0.0\n#20 4.781   Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n#20 4.834 Collecting gitdb&lt;5,&gt;=4.0.1\n#20 4.892   Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n#20 4.934 Collecting smmap&lt;6,&gt;=3.0.1\n#20 4.992   Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n#20 5.005 Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (1.26.8)\n#20 5.005 Requirement already satisfied: certifi&gt;=2017.4.17 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2021.10.8)\n#20 5.006 Requirement already satisfied: idna&lt;4,&gt;=2.5 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.3)\n#20 5.006 Requirement already satisfied: charset-normalizer~=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2.0.4)\n#20 5.075 Collecting urllib3&lt;1.27,&gt;=1.21.1\n#20 5.135   Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n#20 5.172 Building wheels for collected packages: promise, psutil, pathtools\n#20 5.172   Building wheel for promise (setup.py): started\n#20 5.851   Building wheel for promise (setup.py): finished with status 'done'\n#20 5.852   Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=6de0373376d2a8e995959e6173507e13cba502c79b648b5884b1eac45d1ec9ae\n#20 5.852   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/e1\/e8\/83\/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n#20 5.854   Building wheel for psutil (setup.py): started\n#20 6.226   Building wheel for psutil (setup.py): finished with status 'error'\n#20 6.226   ERROR: Command errored out with exit status 1:\n#20 6.226    command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-4y62c4eb\n#20 6.226        cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 6.226   Complete output (45 lines):\n#20 6.226   running bdist_wheel\n#20 6.226   running build\n#20 6.226   running build_py\n#20 6.226   creating build\n#20 6.226   creating build\/lib.linux-aarch64-3.9\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   running build_ext\n#20 6.226   building 'psutil._psutil_linux' extension\n#20 6.226   creating build\/temp.linux-aarch64-3.9\n#20 6.226   creating build\/temp.linux-aarch64-3.9\/psutil\n#20 6.226   gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 6.226   ----------------------------------------\n#20 6.226   ERROR: Failed building wheel for psutil\n#20 6.226   Running setup.py clean for psutil\n#20 6.550   Building wheel for pathtools (setup.py): started\n#20 7.135   Building wheel for pathtools (setup.py): finished with status 'done'\n#20 7.135   Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8e205a0f68c9c7a3c0107d1cc40d94f1d2843c78270217378dcbe98212958b82\n#20 7.135   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/b7\/0a\/67\/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n#20 7.136 Successfully built promise pathtools\n#20 7.136 Failed to build psutil\n#20 7.195 Installing collected packages: smmap, urllib3, gitdb, shortuuid, setproctitle, sentry-sdk, PyYAML, psutil, protobuf, promise, pathtools, GitPython, docker-pycreds, Click, wandb\n#20 7.262   WARNING: The script shortuuid is installed in '\/home\/bot\/.local\/bin' which is not on PATH.\n#20 7.262   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n#20 7.345     Running setup.py install for psutil: started\n#20 7.727     Running setup.py install for psutil: finished with status 'error'\n#20 7.727     ERROR: Command errored out with exit status 1:\n#20 7.727      command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil\n#20 7.727          cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 7.727     Complete output (47 lines):\n#20 7.727     running install\n#20 7.727     \/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n#20 7.727       warnings.warn(\n#20 7.727     running build\n#20 7.727     running build_py\n#20 7.727     creating build\n#20 7.727     creating build\/lib.linux-aarch64-3.9\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     running build_ext\n#20 7.727     building 'psutil._psutil_linux' extension\n#20 7.727     creating build\/temp.linux-aarch64-3.9\n#20 7.727     creating build\/temp.linux-aarch64-3.9\/psutil\n#20 7.727     gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 7.727     ----------------------------------------\n#20 7.728 ERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil Check the logs for full command output.\n------\nexecutor failed running [\/bin\/sh -c pip install wandb --upgrade]: exit code: 1\n<\/code><\/pre>\n<p>why?<\/p>\n<p>Docker file so far:<\/p>\n<pre><code class=\"lang-auto\">FROM continuumio\/miniconda3\n\nRUN apt-get update \\\n  &amp;&amp; apt-get install -y --no-install-recommends \\\n    ssh \\\n    git \\\n    m4 \\\n    libgmp-dev \\\n    opam \\\n    wget \\\n    ca-certificates \\\n    rsync \\\n    strace\n\nRUN useradd -m bot\nWORKDIR \/home\/bot\nUSER bot\n\n## https:\/\/stackoverflow.com\/questions\/73642349\/how-to-have-miniconda-work-properly-with-docker-especially-naming-my-conda-en\n#RUN wget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh  \\\n#    &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b -f\n#ENV PATH=\"\/home\/bot\/miniconda3\/bin:${PATH}\"\n#RUN conda create -n pycoq python=3.9 -y\n## somehow this \"works\" but conda isn't fully aware of this. Fix later?\n#ENV PATH=\"\/home\/bot\/miniconda3\/envs\/pycoq\/bin:${PATH}\"\n\nADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json\n\n# -- setup opam like VP's PyCoq\nRUN opam init --disable-sandboxing\n# compiler + '_' + coq_serapi + '.' + coq_serapi_pin\nRUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda\nRUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1\nRUN eval $(opam env)\n\nRUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released\n# RUN opam pin add -y coq 8.11.0\n# ['opam', 'repo', '--all-switches', 'add', '--set-default', 'coq-released', 'https:\/\/coq.inria.fr\/opam\/released']\nRUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released\nRUN opam update --all\nRUN opam pin add -y coq 8.11.0\n\n#RUN opam install -y --switch ocaml-variants.4.07.1+flambda_coq-serapi_coq-serapi_8.11.0+0.11.1 coq-serapi 8.11.0+0.11.1\nRUN opam install -y coq-serapi\n\nRUN eval $(opam env)\n\n# makes sure depedencies for pycoq are installed once already in the docker image\nENV WANDB_API_KEY=\"SECRET\"\nRUN pip install wandb --upgrade\n<\/code><\/pre>\n<aside class=\"onebox stackexchange\" data-onebox-src=\"https:\/\/stackoverflow.com\/questions\/73642527\/how-to-install-wandb-on-a-docker-image-for-arm\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/stackoverflow.com\/questions\/73642527\/how-to-install-wandb-on-a-docker-image-for-arm\" target=\"_blank\" rel=\"noopener nofollow ugc\">stackoverflow.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img alt=\"Charlie Parker\" src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e9c0a0caedbda92f5ad9bc087e52e143936f9f5.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n  <\/a>\n\n<h4>\n  <a href=\"https:\/\/stackoverflow.com\/questions\/73642527\/how-to-install-wandb-on-a-docker-image-for-arm\" target=\"_blank\" rel=\"noopener nofollow ugc\">How to install wandb on a docker image for arm?<\/a>\n<\/h4>\n\n<div class=\"tags\">\n  <strong>python, linux, docker, anaconda<\/strong>\n<\/div>\n\n<div class=\"date\">\n  asked by\n  \n  <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    Charlie Parker\n  <\/a>\n  on <a href=\"https:\/\/stackoverflow.com\/questions\/73642527\/how-to-install-wandb-on-a-docker-image-for-arm\" target=\"_blank\" rel=\"noopener nofollow ugc\">12:07AM - 08 Sep 22 UTC<\/a>\n<\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1662595762611,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":780.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-install-wandb-on-a-docker-image-for-arm\/3080",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-09T21:12:35.448Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/brando\">@brando<\/a> , thank-you for writing in. There are several errors that appear, but primarily the <code>exit code 1<\/code> could be attributed to you building the container using the base image cached layers. As a preliminary step, can you please try to build the container again and use the command <code>--no-cache<\/code> when doing so. Please let me know what results from this.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-16T05:19:31.238Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/brando\">@brando<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-15T05:20:19.823Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to install on a docker image for arm?; Content: my docker building failed at the run with: (meta_learning) brandomiranda~ \u276f docker build -f ~\/iit-term-synthesis\/dockerfile_arm -t brandojazz\/iit-term-synthesis:test_arm ~\/iit-term-synthesis\/ [+] building 184.7s (20\/28) => [internal] load build definition from dockerfile_arm 0.0s => => transferring dockerfile: 41b 0.0s => [internal] load .dockerignore 0.0s => => transferring context: 2b 0.0s => [internal] load metadata for docker.io\/continuumio\/miniconda3:latest 0.0s => [ 1\/24] from docker.io\/continuumio\/miniconda3 0.0s => https:\/\/api.github.com\/repos\/ibm\/pycoq\/git\/refs\/heads\/main 0.3s => cached [ 2\/24] run apt-get update && apt-get install -y --no-install-recommends ssh git m4 libgmp-dev opam wget 0.0s => cached [ 3\/24] run useradd -m bot 0.0s => cached [ 4\/24] workdir \/home\/bot 0.0s => cached [ 5\/24] add https:\/\/api.github.com\/repos\/ibm\/pycoq\/git\/refs\/heads\/main version.json 0.0s => cached [ 6\/24] run opam init --disable-sandboxing 0.0s => cached [ 7\/24] run opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda 0.0s => cached [ 8\/24] run opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 0.0s => cached [ 9\/24] run eval $(opam env) 0.0s => cached [10\/24] run opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released 0.0s => cached [11\/24] run opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released 0.0s => cached [12\/24] run opam update --all 0.0s => cached [13\/24] run opam pin add -y coq 8.11.0 0.0s => [14\/24] run opam install -y coq-serapi 176.3s => [15\/24] run eval $(opam env) 0.2s => error [16\/24] run pip install --upgrade 8.0s ------ > [16\/24] run pip install --upgrade: #20 0.351 defaulting to user installation because normal site-packages is not writeable #20 0.637 collecting #20 0.986 downloading -0.13.2-py2.py3-none-any.whl (1.8 mb) #20 1.365 requirement already satisfied: setuptools in \/opt\/conda\/lib\/python3.9\/site-packages (from ) (61.2.0) #20 1.366 requirement already satisfied: six>=1.13.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from ) (1.16.0) #20 1.409 collecting promise<3,>=2.0 #20 1.472 downloading promise-2.3.tar.gz (19 kb) #20 2.087 collecting pyyaml #20 2.154 downloading pyyaml-6.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (731 kb) #20 2.431 collecting protobuf<4.0dev,>=3.12.0 #20 2.492 downloading protobuf-3.20.1-cp39-cp39-manylinux2014_aarch64.whl (917 kb) #20 2.648 collecting setproctitle #20 2.706 downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30 kb) #20 2.763 collecting click!=8.0.0,>=7.0 #20 2.818 downloading click-8.1.3-py3-none-any.whl (96 kb) #20 2.902 collecting sentry-sdk>=1.0.0 #20 2.962 downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kb) #20 3.112 collecting psutil>=5.0.0 #20 3.172 downloading psutil-5.9.2.tar.gz (479 kb) #20 3.871 collecting pathtools #20 3.937 downloading pathtools-0.1.2.tar.gz (11 kb) #20 4.431 collecting shortuuid>=0.5.0 #20 4.509 downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kb) #20 4.512 requirement already satisfied: requests<3,>=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from ) (2.27.1) #20 4.568 collecting docker-pycreds>=0.4.0 #20 4.636 downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kb) #20 4.695 collecting gitpython>=1.0.0 #20 4.781 downloading gitpython-3.1.27-py3-none-any.whl (181 kb) #20 4.834 collecting gitdb<5,>=4.0.1 #20 4.892 downloading gitdb-4.0.9-py3-none-any.whl (63 kb) #20 4.934 collecting smmap<6,>=3.0.1 #20 4.992 downloading smmap-5.0.0-py3-none-any.whl (24 kb) #20 5.005 requirement already satisfied: urllib3<1.27,>=1.21.1 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests<3,>=2.0.0->) (1.26.8) #20 5.005 requirement already satisfied: certifi>=2017.4.17 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests<3,>=2.0.0->) (2021.10.8) #20 5.006 requirement already satisfied: idna<4,>=2.5 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests<3,>=2.0.0->) (3.3) #20 5.006 requirement already satisfied: charset-normalizer~=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests<3,>=2.0.0->) (2.0.4) #20 5.075 collecting urllib3<1.27,>=1.21.1 #20 5.135 downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kb) #20 5.172 building wheels for collected packages: promise, psutil, pathtools #20 5.172 building wheel for promise (setup.py): started #20 5.851 building wheel for promise (setup.py): finished with status 'done' #20 5.852 created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=6de0373376d2a8e995959e6173507e13cba502c79b648b5884b1eac45d1ec9ae #20 5.852 stored in directory: \/home\/bot\/.cache\/pip\/wheels\/e1\/e8\/83\/ddea66100678d139b14bc87692ece57c6a2a937956d2532608 #20 5.854 building wheel for psutil (setup.py): started #20 6.226 building wheel for psutil (setup.py): finished with status 'error' #20 6.226 error: command errored out with exit status 1: #20 6.226 command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.stringio('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-4y62c4eb #20 6.226 cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/ #20 6.226 complete output (45 lines): #20 6.226 running bdist_wheel #20 6.226 running build #20 6.226 running build_py #20 6.226 creating build #20 6.226 creating build\/lib.linux-aarch64-3.9 #20 6.226 creating build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/_psosx.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/_psbsd.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/_common.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/_pswindows.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/_psposix.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/__init__.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/_compat.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/_pslinux.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/_pssunos.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 copying psutil\/_psaix.py -> build\/lib.linux-aarch64-3.9\/psutil #20 6.226 creating build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/__main__.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_process.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_aix.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_misc.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_bsd.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_linux.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/runner.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/__init__.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_connections.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_unicode.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_windows.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_contracts.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_sunos.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_testutils.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_osx.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_memleaks.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_posix.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 copying psutil\/tests\/test_system.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 6.226 running build_ext #20 6.226 building 'psutil._psutil_linux' extension #20 6.226 creating build\/temp.linux-aarch64-3.9 #20 6.226 creating build\/temp.linux-aarch64-3.9\/psutil #20 6.226 gcc -pthread -b \/opt\/conda\/compiler_compat -wno-unused-result -wsign-compare -dndebug -o2 -wall -fpic -o2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -i\/opt\/conda\/include -fpic -o2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fpic -dpsutil_posix=1 -dpsutil_sizeof_pid_t=4 -dpsutil_version=592 -dpsutil_linux=1 -i\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o #20 6.226 gcc: error: .2-a+fp16+rcpc+dotprod+crypto: no such file or directory #20 6.226 gcc: error: .2-a+fp16+rcpc+dotprod+crypto: no such file or directory #20 6.226 gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019? #20 6.226 gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019? #20 6.226 error: command '\/usr\/bin\/gcc' failed with exit code 1 #20 6.226 ---------------------------------------- #20 6.226 error: failed building wheel for psutil #20 6.226 running setup.py clean for psutil #20 6.550 building wheel for pathtools (setup.py): started #20 7.135 building wheel for pathtools (setup.py): finished with status 'done' #20 7.135 created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8e205a0f68c9c7a3c0107d1cc40d94f1d2843c78270217378dcbe98212958b82 #20 7.135 stored in directory: \/home\/bot\/.cache\/pip\/wheels\/b7\/0a\/67\/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05 #20 7.136 successfully built promise pathtools #20 7.136 failed to build psutil #20 7.195 installing collected packages: smmap, urllib3, gitdb, shortuuid, setproctitle, sentry-sdk, pyyaml, psutil, protobuf, promise, pathtools, gitpython, docker-pycreds, click, #20 7.262 warning: the script shortuuid is installed in '\/home\/bot\/.local\/bin' which is not on path. #20 7.262 consider adding this directory to path or, if you prefer to suppress this warning, use --no-warn-script-location. #20 7.345 running setup.py install for psutil: started #20 7.727 running setup.py install for psutil: finished with status 'error' #20 7.727 error: command errored out with exit status 1: #20 7.727 command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.stringio('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil #20 7.727 cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/ #20 7.727 complete output (47 lines): #20 7.727 running install #20 7.727 \/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/command\/install.py:34: setuptoolsdeprecationwarning: setup.py install is deprecated. use build and pip and other standards-based tools. #20 7.727 warnings.warn( #20 7.727 running build #20 7.727 running build_py #20 7.727 creating build #20 7.727 creating build\/lib.linux-aarch64-3.9 #20 7.727 creating build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/_psosx.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/_psbsd.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/_common.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/_pswindows.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/_psposix.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/__init__.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/_compat.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/_pslinux.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/_pssunos.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 copying psutil\/_psaix.py -> build\/lib.linux-aarch64-3.9\/psutil #20 7.727 creating build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/__main__.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_process.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_aix.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_misc.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_bsd.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_linux.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/runner.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/__init__.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_connections.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_unicode.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_windows.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_contracts.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_sunos.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_testutils.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_osx.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_memleaks.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_posix.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 copying psutil\/tests\/test_system.py -> build\/lib.linux-aarch64-3.9\/psutil\/tests #20 7.727 running build_ext #20 7.727 building 'psutil._psutil_linux' extension #20 7.727 creating build\/temp.linux-aarch64-3.9 #20 7.727 creating build\/temp.linux-aarch64-3.9\/psutil #20 7.727 gcc -pthread -b \/opt\/conda\/compiler_compat -wno-unused-result -wsign-compare -dndebug -o2 -wall -fpic -o2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -i\/opt\/conda\/include -fpic -o2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fpic -dpsutil_posix=1 -dpsutil_sizeof_pid_t=4 -dpsutil_version=592 -dpsutil_linux=1 -i\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o #20 7.727 gcc: error: .2-a+fp16+rcpc+dotprod+crypto: no such file or directory #20 7.727 gcc: error: .2-a+fp16+rcpc+dotprod+crypto: no such file or directory #20 7.727 gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019? #20 7.727 gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019? #20 7.727 error: command '\/usr\/bin\/gcc' failed with exit code 1 #20 7.727 ---------------------------------------- #20 7.728 error: command errored out with exit status 1: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.stringio('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil check the logs for full command output. ------ executor failed running [\/bin\/sh -c pip install --upgrade]: exit code: 1 why? docker file so far: from continuumio\/miniconda3 run apt-get update \\ && apt-get install -y --no-install-recommends \\ ssh \\ git \\ m4 \\ libgmp-dev \\ opam \\ wget \\ ca-certificates \\ rsync \\ strace run useradd -m bot workdir \/home\/bot user bot ## https:\/\/stackoverflow.com\/questions\/73642349\/how-to-have-miniconda-work-properly-with-docker-especially-naming-my-conda-en #run wget https:\/\/repo.anaconda.com\/miniconda\/miniconda3-latest-linux-x86_64.sh \\ # && bash miniconda3-latest-linux-x86_64.sh -b -f #env path=\"\/home\/bot\/miniconda3\/bin:${path}\" #run conda create -n pycoq python=3.9 -y ## somehow this \"works\" but conda isn't fully aware of this. fix later? #env path=\"\/home\/bot\/miniconda3\/envs\/pycoq\/bin:${path}\" add https:\/\/api.github.com\/repos\/ibm\/pycoq\/git\/refs\/heads\/main version.json # -- setup opam like vp's pycoq run opam init --disable-sandboxing # compiler + '_' + coq_serapi + '.' + coq_serapi_pin run opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda run opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 run eval $(opam env) run opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released # run opam pin add -y coq 8.11.0 # ['opam', 'repo', '--all-switches', 'add', '--set-default', 'coq-released', 'https:\/\/coq.inria.fr\/opam\/released'] run opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released run opam update --all run opam pin add -y coq 8.11.0 #run opam install -y --switch ocaml-variants.4.07.1+flambda_coq-serapi_coq-serapi_8.11.0+0.11.1 coq-serapi 8.11.0+0.11.1 run opam install -y coq-serapi run eval $(opam env) # makes sure depedencies for pycoq are installed once already in the docker image env _api_key=\"secret\" run pip install --upgrade https:\/\/community..ai\/t\/how-to-install--on-a-docker-image-for-arm\/3080",
        "Question_original_content_gpt_summary":"The user encountered a challenge while attempting to install a docker image for ARM.",
        "Question_preprocessed_content":"Title: how to install on a docker image for arm?; Content: my docker building failed at the with why? docker file so far how to install on a docker image for arm? python, linux, docker, anaconda asked by charlie parker on am sep utc",
        "Answer_original_content":"hi @brando , thank-you for writing in. there are several errors that appear, but primarily the exit code 1 could be attributed to you building the container using the base image cached layers. as a preliminary step, can you please try to build the container again and use the command --no-cache when doing so. please let me know what results from this. hi @brando , since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The solution suggested in the answer is to rebuild the container using the command --no-cache to avoid errors related to cached layers.",
        "Answer_preprocessed_content":"hi , for writing in. there are several errors that appear, but primarily the could be attributed to you building the container using the base image cached layers. as a preliminary step, can you please try to build the container again and use the command when doing so. please let me know what results from this. hi , since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Sweep creation down? (Resolved)",
        "Question_body":"<p>Hello, I thought you\u2019d like to know that creating a sweep seems to be broken for me. I get a new sweep ID, but going to the URL in question gets a 404, and it doesn\u2019t appear in the sweep list either. I\u2019ve tried via CLI and browser.<\/p>\n<p>Edit: Seems to be back up!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1655321334078,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":128.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-creation-down-resolved\/2615",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-16T08:46:47.497Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/dcx\">@dcx<\/a>, glad it\u2019s working now for you. Please let us know if you experience any issues like this again.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-15T08:46:55.391Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sweep creation down? (resolved); Content: hello, i thought you\u2019d like to know that creating a sweep seems to be broken for me. i get a new sweep id, but going to the url in question gets a 404, and it doesn\u2019t appear in the sweep list either. i\u2019ve tried via cli and browser. edit: seems to be back up!",
        "Question_original_content_gpt_summary":"The user encountered an issue with creating a sweep, where they received a 404 error and the sweep did not appear in the sweep list.",
        "Question_preprocessed_content":"Title: sweep creation down? ; Content: hello, i thought youd like to know that creating a sweep seems to be broken for me. i get a new sweep id, but going to the url in question gets a , and it doesnt appear in the sweep list either. ive tried via cli and browser. edit seems to be back up!",
        "Answer_original_content":"hey @dcx, glad its working now for you. please let us know if you experience any issues like this again. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer. The response is a general message acknowledging that the issue has been resolved and asking the user to report any future issues.",
        "Answer_preprocessed_content":"hey glad its working now for you. please let us know if you experience any issues like this again. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Best practice for handling large data",
        "Question_body":"<p>I am working with a 25GB raw dataset which when processed to be prepared for training produces another 10+GB file. Using <code>dvc push<\/code> and <code>dvc pull<\/code> with s3 takes a significant amount of time to move data through the network. Is there a way to keep the data only in s3? or use a flag that chooses when to pull or push but that knows that a stage has run and does not need reruning?<\/p>\n<p>What would be the recommended way of working with the data both locally and using a remote?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1618304859684,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":1023.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-handling-large-data\/721",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-14T17:48:40.772Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/nsorros\">@nsorros<\/a>,<\/p>\n<p>Can you please clarify why you need to push\/pull data constantly? Remote storage is an \u201coptional\u201d (thought very common) feature, meaning that you can do your entire pipeline locally and only sync to\/from remote once (in a while).<\/p>\n<blockquote>\n<p>Is there a way to keep the data only in s3?<\/p>\n<\/blockquote>\n<p>DVC can track data externally without ever downloading it, yes. But it\u2019s a very advanced\/tricky feature that we need to redesign so not something I would definitely recommend\u2026 See <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\">https:\/\/dvc.org\/doc\/user-guide\/managing-external-data<\/a>.<\/p>\n<blockquote>\n<p>or use a flag that chooses when to pull or push but that knows that a stage has run<\/p>\n<\/blockquote>\n<p>This is the part I\u2019m not getting. When do you want push\/pull to happen? If you can share more details on your workflow we can probably give you a better answer.<\/p>\n<blockquote>\n<p>recommended way of working with the data both locally and using a remote?<\/p>\n<\/blockquote>\n<p>To recap, remote storage is meant for sharing and backing up raw data, results, models, etc. But if you\u2019re using a network connection to sync it then it\u2019s probably not a good idea to rely on it for day-to-day pipeline executions involving very large files. You may be interested in things like <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\">https:\/\/dvc.org\/doc\/use-cases\/shared-development-server<\/a> or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage<\/a> but again, depends on your goals.<\/p>\n<p>Thanks<\/p>",
                "Answer_score":88.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-15T15:50:05.478Z",
                "Answer_body":"<p>Sorry for the confusion, what I am thinking is a flow where the code and data lives in 2 places, an EC2 instance and locally and synced through GitHub and DVC. This is our current setup for most projects. We use an EC2 instance to train when needed, otherwise we train locally.<\/p>\n<p>So in this setup sometimes I develop locally but want to train in the EC2 instance. Ideally I do not want to run dvc pull and get all data locally, I want to be able to exclude some due to the size of the files. And same for other members of the team, if they go to my project and want to test a pipeline works, I do not want them to have to download 25-100GB of data with dvc pull. Ideally I want to include a sample dataset that they can use in a sample pipeline or something. I might use that as well for local development.<\/p>\n<p>If that flow does not make sense and a better solution is to never get the data through dvc pull both locally and in the ec2 instance and always treat them as external data then I guess that is my answer. This problem should be more prominent as the data and the models get larger I would assume.<\/p>\n<p>Hope this makes sense, I understand it might be confusing, very curious to hear more about how you would think about it and I am happy to provide more context.<\/p>",
                "Answer_score":53.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-15T18:46:42.080Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"nsorros\" data-post=\"3\" data-topic=\"721\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/nsorros\/40\/104_2.png\" class=\"avatar\"> nsorros:<\/div>\n<blockquote>\n<p>We use an EC2 instance to train when needed, otherwise we train locally.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Got it.<\/p>\n<aside class=\"quote no-group\" data-username=\"nsorros\" data-post=\"3\" data-topic=\"721\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/nsorros\/40\/104_2.png\" class=\"avatar\"> nsorros:<\/div>\n<blockquote>\n<p>I do not want to run dvc pull and get all data locally, I want to be able to exclude some<\/p>\n<\/blockquote>\n<\/aside>\n<p>You can give specific targets to <code>dvc pull<\/code>. With the <code>--glob<\/code> option you can even use wildcards and other patterns. See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/pull\">https:\/\/dvc.org\/doc\/command-reference\/pull<\/a> (Same for <code>dvc push<\/code>)<\/p>\n<aside class=\"quote no-group\" data-username=\"nsorros\" data-post=\"3\" data-topic=\"721\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/nsorros\/40\/104_2.png\" class=\"avatar\"> nsorros:<\/div>\n<blockquote>\n<p>Ideally I want to include a sample dataset that they can use in a sample pipeline or something.<\/p>\n<\/blockquote>\n<\/aside>\n<p>You can make a separate Git branch in your repo, put the sample there, <code>dvc commit\/repro<\/code> to update DVC metafiles, and <code>git+dvc push<\/code> it. They can then get that branch and <code>dvc pull<\/code> will download the smaller sample.<\/p>\n<p>Does that make sense or am I misunderstanding the situation? Thanks<\/p>",
                "Answer_score":83.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-16T07:29:39.531Z",
                "Answer_body":"<p>Hi Jorge,<\/p>\n<p>That makes sense. I guess I missed the glob option which seems to do what I am looking for.<\/p>\n<p>Interesting idea about the separate branch, thanks for the suggestion.<\/p>\n<p>Super helpful and again thanks for the quick response.<\/p>",
                "Answer_score":17.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-16T18:37:16.567Z",
                "Answer_body":"<p>Glad I could help!<\/p>\n<blockquote>\n<p>Post must be at least 20 characters<\/p>\n<\/blockquote>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: best practice for handling large data; Content: i am working with a 25gb raw dataset which when processed to be prepared for training produces another 10+gb file. using push and pull with s3 takes a significant amount of time to move data through the network. is there a way to keep the data only in s3? or use a flag that chooses when to pull or push but that knows that a stage has run and does not need reruning? what would be the recommended way of working with the data both locally and using a remote?",
        "Question_original_content_gpt_summary":"The user is facing a challenge of how to efficiently handle a large dataset of 25GB and a processed dataset of 10+GB while using S3 to push and pull data through the network.",
        "Question_preprocessed_content":"Title: best practice for handling large data; Content: i am working with a gb raw dataset which when processed to be prepared for training produces another +gb file. using and with s takes a significant amount of time to move data through the network. is there a way to keep the data only in s ? or use a flag that chooses when to pull or push but that knows that a stage has run and does not need reruning? what would be the recommended way of working with the data both locally and using a remote?",
        "Answer_original_content":"hey @nsorros, can you please clarify why you need to push\/pull data constantly? remote storage is an optional (thought very common) feature, meaning that you can do your entire pipeline locally and only sync to\/from remote once (in a while). is there a way to keep the data only in s3? can track data externally without ever downloading it, yes. but its a very advanced\/tricky feature that we need to redesign so not something i would definitely recommend see https:\/\/.org\/doc\/user-guide\/managing-external-data. or use a flag that chooses when to pull or push but that knows that a stage has run this is the part im not getting. when do you want push\/pull to happen? if you can share more details on your workflow we can probably give you a better answer. recommended way of working with the data both locally and using a remote? to recap, remote storage is meant for sharing and backing up raw data, results, models, etc. but if youre using a network connection to sync it then its probably not a good idea to rely on it for day-to-day pipeline executions involving very large files. you may be interested in things like https:\/\/.org\/doc\/use-cases\/shared-development-server or https:\/\/.org\/doc\/command-reference\/add#example-transfer-to-remote-storage but again, depends on your goals. thanks sorry for the confusion, what i am thinking is a flow where the code and data lives in 2 places, an ec2 instance and locally and synced through github and . this is our current setup for most projects. we use an ec2 instance to train when needed, otherwise we train locally. so in this setup sometimes i develop locally but want to train in the ec2 instance. ideally i do not want to run pull and get all data locally, i want to be able to exclude some due to the size of the files. and same for other members of the team, if they go to my project and want to test a pipeline works, i do not want them to have to download 25-100gb of data with pull. ideally i want to include a sample dataset that they can use in a sample pipeline or something. i might use that as well for local development. if that flow does not make sense and a better solution is to never get the data through pull both locally and in the ec2 instance and always treat them as external data then i guess that is my answer. this problem should be more prominent as the data and the models get larger i would assume. hope this makes sense, i understand it might be confusing, very curious to hear more about how you would think about it and i am happy to provide more context. nsorros: we use an ec2 instance to train when needed, otherwise we train locally. got it. nsorros: i do not want to run pull and get all data locally, i want to be able to exclude some you can give specific targets to pull. with the --glob option you can even use wildcards and other patterns. see https:\/\/.org\/doc\/command-reference\/pull (same for push) nsorros: ideally i want to include a sample dataset that they can use in a sample pipeline or something. you can make a separate git branch in your repo, put the sample there, commit\/repro to update metafiles, and git+ push it. they can then get that branch and pull will download the smaller sample. does that make sense or am i misunderstanding the situation? thanks hi jorge, that makes sense. i guess i missed the glob option which seems to do what i am looking for. interesting idea about the separate branch, thanks for the suggestion. super helpful and again thanks for the quick response. glad i could help! post must be at least 20 characters",
        "Answer_original_content_gpt_summary":"Possible solutions to efficiently handle a large dataset of 25GB and a processed dataset of 10+GB while using S3 to push and pull data through the network include: \n- Keeping the data only in S3 and tracking data externally without ever downloading it (an advanced\/tricky feature)\n- Using a flag that chooses when to pull or push but that knows that a stage has run\n- Giving specific targets to pull with the --glob option (can even use wildcards and other patterns)\n- Making a separate git branch in the repo for a smaller sample dataset that can be used in a sample pipeline or something.",
        "Answer_preprocessed_content":"hey can you please clarify why you need to data constantly? remote storage is an optional feature, meaning that you can do your entire pipeline locally and only sync remote once . is there a way to keep the data only in s ? can track data externally without ever downloading it, yes. but its a very feature that we need to redesign so not something i would definitely recommend see or use a flag that chooses when to pull or push but that knows that a stage has run this is the part im not getting. when do you want to happen? if you can share more details on your workflow we can probably give you a better answer. recommended way of working with the data both locally and using a remote? to recap, remote storage is meant for sharing and backing up raw data, results, models, etc. but if youre using a network connection to sync it then its probably not a good idea to rely on it for pipeline executions involving very large files. you may be interested in things like or but again, depends on your goals. thanks sorry for the confusion, what i am thinking is a flow where the code and data lives in places, an ec instance and locally and synced through github and . this is our current setup for most projects. we use an ec instance to train when needed, otherwise we train locally. so in this setup sometimes i develop locally but want to train in the ec instance. ideally i do not want to run pull and get all data locally, i want to be able to exclude some due to the size of the files. and same for other members of the team, if they go to my project and want to test a pipeline works, i do not want them to have to download of data with pull. ideally i want to include a sample dataset that they can use in a sample pipeline or something. i might use that as well for local development. if that flow does not make sense and a better solution is to never get the data through pull both locally and in the ec instance and always treat them as external data then i guess that is my answer. this problem should be more prominent as the data and the models get larger i would assume. hope this makes sense, i understand it might be confusing, very curious to hear more about how you would think about it and i am happy to provide more context. nsorros we use an ec instance to train when needed, otherwise we train locally. got it. nsorros i do not want to run pull and get all data locally, i want to be able to exclude some you can give specific targets to . with the option you can even use wildcards and other patterns. see nsorros ideally i want to include a sample dataset that they can use in a sample pipeline or something. you can make a separate git branch in your repo, put the sample there, to update metafiles, and it. they can then get that branch and will download the smaller sample. does that make sense or am i misunderstanding the situation? thanks hi jorge, that makes sense. i guess i missed the glob option which seems to do what i am looking for. interesting idea about the separate branch, thanks for the suggestion. super helpful and again thanks for the quick response. glad i could help! post must be at least characters"
    },
    {
        "Question_id":null,
        "Question_title":"How can I access a FileDataset without filling local disk?",
        "Question_body":"Hello!\n\nI'm setting up a pipeline for machine learning. Reading the docs I understood that when I pass my FileDataset as_mount it is mounted, similar to mounting an external drive. However, three hours into my training, my pipeline crashed, out of storage. It seems that as_mount actually is downloading per file, rather than the entire Dataset, but still uses local disk space. Is this correct? If so, how can I train on a FileDataset that is too large for any of the available compute options?\n\nDavid",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1628529752433,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/506820\/how-can-i-access-a-filedataset-without-filling-loc.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-10T12:31:26.553Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. When you mount, only the data files used by your script are loaded at the time of processing. When you download, all files referenced by the dataset will be downloaded to the compute target. If your data size exceeds the compute disk size, we recommend mounting (which reads only a subset of the data). Based on your post, you may need to use a larger compute instance to handle your workload. Please review mount vs download documentation. Hope this helps.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i access a filedataset without filling local disk?; Content: hello! i'm setting up a pipeline for machine learning. reading the docs i understood that when i pass my filedataset as_mount it is mounted, similar to mounting an external drive. however, three hours into my training, my pipeline crashed, out of storage. it seems that as_mount actually is downloading per file, rather than the entire dataset, but still uses local disk space. is this correct? if so, how can i train on a filedataset that is too large for any of the available compute options? david",
        "Question_original_content_gpt_summary":"The user David is encountering challenges with accessing a filedataset without filling local disk space.",
        "Question_preprocessed_content":"Title: how can i access a filedataset without filling local disk?; Content: hello! i'm setting up a pipeline for machine learning. reading the docs i understood that when i pass my filedataset it is mounted, similar to mounting an external drive. however, three hours into my training, my pipeline crashed, out of storage. it seems that actually is downloading per file, rather than the entire dataset, but still uses local disk space. is this correct? if so, how can i train on a filedataset that is too large for any of the available compute options? david",
        "Answer_original_content":"hi, thanks for reaching out. when you mount, only the data files used by your script are loaded at the time of processing. when you download, all files referenced by the dataset will be downloaded to the compute target. if your data size exceeds the compute disk size, we recommend mounting (which reads only a subset of the data). based on your post, you may need to use a larger compute instance to handle your workload. please review mount vs download documentation. hope this helps.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n- Mounting the file dataset instead of downloading it to save local disk space.\n- Using a larger compute instance to handle the workload if the data size exceeds the compute disk size. \n- Reviewing the documentation on mount vs download for more information.",
        "Answer_preprocessed_content":"hi, thanks for reaching out. when you mount, only the data files used by your script are loaded at the time of processing. when you download, all files referenced by the dataset will be downloaded to the compute target. if your data size exceeds the compute disk size, we recommend mounting . based on your post, you may need to use a larger compute instance to handle your workload. please review mount vs download documentation. hope this helps."
    },
    {
        "Question_id":null,
        "Question_title":"Add custom column in table reports the variance of a metric per group",
        "Question_body":"<p>Hello everyone,<\/p>\n<p>I\u2019 am using table view of wandb to compare different groups of multiple runs. Even thought table reports the mean of each group (e.x. training accuracy), do not reports the variance (std dev) of group\u2019s accuracies . Is there any way to add this per-group metric in my table view?<\/p>\n<p>Thank you in advance!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1645009645094,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":109.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/add-custom-column-in-table-reports-the-variance-of-a-metric-per-group\/1938",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-16T23:35:11.580Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/eakirtas\">@eakirtas<\/a>,<br>\nThank you for the question! Unfortunately there is no easy way to do this currently. Accessing the runs via the API and programmatically calculating the standard deviation by group is possible but not an efficient solution.<\/p>\n<p>If you\u2019d like I can put in a feature request to try to make this available in the UI? If so, what statistics would you like to see available? Just standard deviation or anything else?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-22T15:51:56.614Z",
                "Answer_body":"<p>Hi, <a class=\"mention\" href=\"\/u\/eakirtas\">@eakirtas<\/a>,<br>\nI just wanted to follow up and see if this answered your question or if there is anything I can help with?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-23T15:52:13.317Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: add custom column in table reports the variance of a metric per group; Content: hello everyone, i\u2019 am using table view of to compare different groups of multiple runs. even thought table reports the mean of each group (e.x. training accuracy), do not reports the variance (std dev) of group\u2019s accuracies . is there any way to add this per-group metric in my table view? thank you in advance!",
        "Question_original_content_gpt_summary":"The user is looking for a way to add a custom column to their table view that reports the variance of a metric per group.",
        "Question_preprocessed_content":"Title: add custom column in table reports the variance of a metric per group; Content: hello everyone, i am using table view of to compare different groups of multiple runs. even thought table reports the mean of each group , do not reports the variance of groups accuracies . is there any way to add this metric in my table view? thank you in advance!",
        "Answer_original_content":"hi @eakirtas, thank you for the question! unfortunately there is no easy way to do this currently. accessing the runs via the api and programmatically calculating the standard deviation by group is possible but not an efficient solution. if youd like i can put in a feature request to try to make this available in the ui? if so, what statistics would you like to see available? just standard deviation or anything else? thank you, nate hi, @eakirtas, i just wanted to follow up and see if this answered your question or if there is anything i can help with? thank you, nate this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There is no easy way to add a custom column to a table view that reports the variance of a metric per group. One possible solution is to access the runs via the API and programmatically calculate the standard deviation by group, but this is not an efficient solution. The answerer suggests putting in a feature request to make this available in the UI and asks for clarification on what statistics the user would like to see available.",
        "Answer_preprocessed_content":"hi thank you for the question! unfortunately there is no easy way to do this currently. accessing the runs via the api and programmatically calculating the standard deviation by group is possible but not an efficient solution. if youd like i can put in a feature request to try to make this available in the ui? if so, what statistics would you like to see available? just standard deviation or anything else? thank you, nate hi, i just wanted to follow up and see if this answered your question or if there is anything i can help with? thank you, nate this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"How do I do probabilistic logging?",
        "Question_body":"<p>I\u2019m currently doing all my logging like this:<\/p>\n<pre><code class=\"lang-auto\">wandb.log(\n{ \"loss\": loss, \"step\": step }, commit=should_commit())\n# step increments with each batch\n\ndef should_commit():\n    return random.randint(0, 100) == 0\n<\/code><\/pre>\n<p>The goal is to only sync metrics to the server once every 100 or so calls to log. I\u2019m doing this because otherwise my training speed is halved, if I just do the default.<\/p>\n<p>But, I notice this isn\u2019t working. My charts look very odd for some reason.<br>\nWhat is the correct way to only sync all my metrics every so often?<\/p>\n<p>To be clear, I want the end result to be such that all my metrics are on the server for every time-step, however, I want to do the syncing infrequently instead of every time I call <code>wandb.log<\/code>, because that\u2019s slow.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1657753781053,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":65.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-i-do-probabilistic-logging\/2747",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-15T21:29:56.145Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/vroomerify\">@vroomerify<\/a> ,<\/p>\n<p>Our global step must always increase and we will automatically increment it if you don\u2019t specify it or set <code>commit=False<\/code> in <code>wandb.init<\/code>. If you want to log metrics against multiple x-axis you can log those axis as separate metrics, I.E.<\/p>\n<p>wandb.log({\u201closs\u201d: 0.3, \u201cbatch\u201d: 1000, \u201cepoch\u201d: 2})<\/p>\n<p>Then you can choose a custom x-axis in the ui to be whatever you\u2019ve logged against your metrics.<\/p>\n<p>Additionally, we allow for Stepwise and Incremental Logging, see <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#stepwise-and-incremental-logging\">here<\/a> for more details on how to do so.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-22T06:34:10.759Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/vroomerify\">@vroomerify<\/a> ,  since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-20T06:34:47.378Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i do probabilistic logging?; Content: i\u2019m currently doing all my logging like this: .log( { \"loss\": loss, \"step\": step }, commit=should_commit()) # step increments with each batch def should_commit(): return random.randint(0, 100) == 0 the goal is to only sync metrics to the server once every 100 or so calls to log. i\u2019m doing this because otherwise my training speed is halved, if i just do the default. but, i notice this isn\u2019t working. my charts look very odd for some reason. what is the correct way to only sync all my metrics every so often? to be clear, i want the end result to be such that all my metrics are on the server for every time-step, however, i want to do the syncing infrequently instead of every time i call .log, because that\u2019s slow.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with probabilistic logging, where they are attempting to sync metrics to the server once every 100 calls to log, but are noticing that their charts look odd.",
        "Question_preprocessed_content":"Title: how do i do probabilistic logging?; Content: im currently doing all my logging like this the goal is to only sync metrics to the server once every or so calls to log. im doing this because otherwise my training speed is halved, if i just do the default. but, i notice this isnt working. my charts look very odd for some reason. what is the correct way to only sync all my metrics every so often? to be clear, i want the end result to be such that all my metrics are on the server for every however, i want to do the syncing infrequently instead of every time i call , because thats slow.",
        "Answer_original_content":"@vroomerify , our global step must always increase and we will automatically increment it if you dont specify it or set commit=false in .init. if you want to log metrics against multiple x-axis you can log those axis as separate metrics, i.e. .log({loss: 0.3, batch: 1000, epoch: 2}) then you can choose a custom x-axis in the ui to be whatever youve logged against your metrics. additionally, we allow for stepwise and incremental logging, see here for more details on how to do so. @vroomerify , since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include: ensuring that the global step always increases, logging metrics against multiple x-axis as separate metrics, and using stepwise and incremental logging. The answer also suggests checking out a link for more details on how to implement these solutions.",
        "Answer_preprocessed_content":", our global step must always increase and we will automatically increment it if you dont specify it or set in . if you want to log metrics against multiple you can log those axis as separate metrics, batch , epoch then you can choose a custom in the ui to be whatever youve logged against your metrics. additionally, we allow for stepwise and incremental logging, see here for more details on how to do so. , since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":54154455.0,
        "Question_title":"How do you invoke a sagemaker xgboost endpoint from a chalice app?",
        "Question_body":"<p>I built a chalice web-app that is hosted in an s3 bucket and calls an xgboost endpoint. I keep getting an error when I invoke the model through the web-app. When I looked into the Lambda log files I discovered my input is not properly decoding. <code>input_text = app.current_request.raw_body.decode()<\/code> What would be the correct code to decode the input from binary so I can pass in a regular string to my endpoint?<\/p>\n\n<p>Here is the error:<\/p>\n\n<p>botocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message \"could not convert string to float: user_input=1%\". <\/p>\n\n<p>Here is my index.html file:<\/p>\n\n<pre><code>&lt;html&gt;\n&lt;head&gt;&lt;\/head&gt;\n&lt;body&gt;\n&lt;form method=\"post\" action=\"&lt;chalice_deployed_http&gt;\"&gt;\n\n&lt;input type=\"text\" name=\"user_input\"&gt;&lt;br&gt;\n\n&lt;input type=\"submit\" value=\"Submit\"&gt;\n&lt;\/form&gt;\n&lt;\/body&gt;\n&lt;\/html&gt;\n<\/code><\/pre>\n\n<p>Here is my app.py file:<\/p>\n\n<pre><code>try:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n\nfrom io import BytesIO\nimport csv\nimport sys, os, base64, datetime, hashlib, hmac\nfrom chalice import Chalice, NotFoundError, BadRequestError\nimport boto3\n\n\napp = Chalice(app_name='&lt;name_of_chalice_app&gt;')\napp.debug = True\n\nsagemaker = boto3.client('sagemaker-runtime')\n\n@app.route('\/', methods=['POST'], content_types=['application\/x-www-form-urlencoded'])\ndef handle_data():\n    input_text = app.current_request.raw_body.decode()\n\n    res = sagemaker.invoke_endpoint(\n                    EndpointName='&lt;endpoint_name&gt;',\n                    Body=input_text,\n                    ContentType='text\/csv',\n                    Accept='Accept'\n                )\n    return res['Body'].read().decode()[0]\n<\/code><\/pre>\n\n<p>I should be able to pass in a string like this:<\/p>\n\n<p>'1,4,26,0.076923077,2,3,1,0.611940299,0.7818181820000001,0.40376569,0.571611506,0.12,12,1,0.0,2,1.0,1,2,6,3,1,1,1,1,1,3,1,0.000666667,1,1,2,2,-1.0,0.490196078,-1.0,0.633928571,6.0,145,2,2,1,3,2,2,1,3,2,3,3,-1.0,1,3,1,1,2,1,2,3,1,3,3,1,3,2,3,-1.0,3,3,1,2,2,1,3,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0.3497921158934803,0'<\/p>\n\n<p>and get output like this:<\/p>\n\n<p>'5'<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sb5Nw.jpg\" rel=\"nofollow noreferrer\">When I run it in a jupyter notebook it works.<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1547243089770,
        "Question_favorite_count":0.0,
        "Question_score":0.0,
        "Question_view_count":805.0,
        "Owner_creation_time":1546566576912,
        "Owner_last_access_time":1614803818790,
        "Owner_reputation":25.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":11.0,
        "Answer_body":"<p>This worked:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>    input_text = app.current_request.raw_body\n    d = parse_qs(input_text)\n    lst = d[b'user_input'][0].decode()\n    res = sagemaker.invoke_endpoint(\n                    EndpointName='&lt;name-of-SageMaker-Endpoint&gt;',\n                    Body=lst,\n                    ContentType='text\/csv',\n                    Accept='Accept'\n                )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1547657753670,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1547479657316,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54154455",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do you invoke a xgboost endpoint from a chalice app?; Content: i built a chalice web-app that is hosted in an s3 bucket and calls an xgboost endpoint. i keep getting an error when i invoke the model through the web-app. when i looked into the lambda log files i discovered my input is not properly decoding. input_text = app.current_request.raw_body.decode() what would be the correct code to decode the input from binary so i can pass in a regular string to my endpoint? here is the error: botocore.errorfactory.modelerror: an error occurred (modelerror) when calling the invokeendpoint operation: received client error (415) from model with message \"could not convert string to float: user_input=1%\". here is my index.html file: <html> <head><\/head> <body> <form method=\"post\" action=\"<chalice_deployed_http>\"> <input type=\"text\" name=\"user_input\"><br> <input type=\"submit\" value=\"submit\"> <\/form> <\/body> <\/html> here is my app.py file: try: from stringio import stringio except importerror: from io import stringio from io import bytesio import csv import sys, os, base64, datetime, hashlib, hmac from chalice import chalice, notfounderror, badrequesterror import boto3 app = chalice(app_name='<name_of_chalice_app>') app.debug = true = boto3.client('-runtime') @app.route('\/', methods=['post'], content_types=['application\/x-www-form-urlencoded']) def handle_data(): input_text = app.current_request.raw_body.decode() res = .invoke_endpoint( endpointname='<endpoint_name>', body=input_text, contenttype='text\/csv', accept='accept' ) return res['body'].read().decode()[0] i should be able to pass in a string like this: '1,4,26,0.076923077,2,3,1,0.611940299,0.7818181820000001,0.40376569,0.571611506,0.12,12,1,0.0,2,1.0,1,2,6,3,1,1,1,1,1,3,1,0.000666667,1,1,2,2,-1.0,0.490196078,-1.0,0.633928571,6.0,145,2,2,1,3,2,2,1,3,2,3,3,-1.0,1,3,1,1,2,1,2,3,1,3,3,1,3,2,3,-1.0,3,3,1,2,2,1,3,3,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,0.3497921158934803,0' and get output like this: '5' when i run it in a jupyter notebook it works.",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to invoke an XGBoost endpoint from a Chalice web-app, due to an issue with decoding the input from binary.",
        "Question_preprocessed_content":"Title: how do you invoke a xgboost endpoint from a chalice app?; Content: i built a chalice that is hosted in an s bucket and calls an xgboost endpoint. i keep getting an error when i invoke the model through the when i looked into the lambda log files i discovered my input is not properly decoding. what would be the correct code to decode the input from binary so i can pass in a regular string to my endpoint? here is the error an error occurred when calling the invokeendpoint operation received client error from model with message could not convert string to float here is my file here is my file i should be able to pass in a string like this and get output like this ' ' when i run it in a jupyter notebook it works.",
        "Answer_original_content":"this worked: input_text = app.current_request.raw_body d = parse_qs(input_text) lst = d[b'user_input'][0].decode() res = .invoke_endpoint( endpointname='<name-of--endpoint>', body=lst, contenttype='text\/csv', accept='accept' )",
        "Answer_original_content_gpt_summary":"The solution to the error encountered when invoking an XGBoost endpoint from a Chalice web-app due to an issue with decoding the input from binary is to use the provided code snippet. The code snippet involves parsing the input text, decoding it, and invoking the endpoint with the decoded input.",
        "Answer_preprocessed_content":"this worked"
    },
    {
        "Question_id":null,
        "Question_title":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Question_body":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1607093437353,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/186789\/does-automl-support-optimizing-convolutional-neura.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-04T21:28:34.597Z",
                "Answer_score":0,
                "Answer_body":"AutoML doesn't currently support CNNs publicly, it's on our roadmap and it will come with optimizations across different parameters, so stay tuned. Hope this helps.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: does automl support optimizing convolutional neural network over the number of layers and pool layer parameters?; Content: does automl support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Question_original_content_gpt_summary":"The user is inquiring about whether Automl supports optimizing Convolutional Neural Networks over the number of layers and pool layer parameters.",
        "Question_preprocessed_content":"Title: does automl support optimizing convolutional neural network over the number of layers and pool layer parameters?; Content: does automl support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Answer_original_content":"automl doesn't currently support cnns publicly, it's on our roadmap and it will come with optimizations across different parameters, so stay tuned. hope this helps.",
        "Answer_original_content_gpt_summary":"Summary: The answer states that Automl does not currently support optimizing Convolutional Neural Networks (CNNs) publicly, but it is on their roadmap and will come with optimizations across different parameters in the future. No specific solutions are provided at this time, but the user is encouraged to stay tuned for updates.",
        "Answer_preprocessed_content":"automl doesn't currently support cnns publicly, it's on our roadmap and it will come with optimizations across different parameters, so stay tuned. hope this helps."
    },
    {
        "Question_id":null,
        "Question_title":"Log multiple variables at the same plot",
        "Question_body":"<p>Hello there, I would like to log two different variables to show up on the same chart. I use the following:<\/p>\n<pre><code class=\"lang-auto\">for I in range(100):\n    var1 = something\n    var2 = something_else\n\n    wandb.log({\"var1\":something, \"var2\":something_else})\n<\/code><\/pre>\n<p>but for some reason, the dashboard shows it as two separate plots. I went through the documentation I see this:<br>\nMultiple metrics on one chart: Log multiple metrics in the same call to wandb.log, like this:<\/p>\n<p><code>wandb.log({\"acc'\": 0.9, \"loss\": 0.1})<\/code><\/p>\n<p>and they will both be available to plot against in the UI.<\/p>\n<p>Can anyone help me on this? I tried many hacks like insertring a list in the place of the variable, or a dict of dicts, etc\u2026<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1653185144304,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":83.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/log-multiple-variables-at-the-same-plot\/2474",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-23T10:54:25.875Z",
                "Answer_body":"<p>Hey there, you\u2019ll need to edit one of the charts to include the other metrics manually in the UI. I\u2019ll edit the documentation to make this more clear. Here is an example:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c0855646bcca0e8b6f621e2da9d9a2e57f80b7e9.gif\" alt=\"May-23-2022 14-53-44\" data-base62-sha1=\"rt7ly31TOAnaAoc2J0N9HA5UFJf\" width=\"690\" height=\"282\" class=\"animated\"><\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-22T10:55:12.901Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: log multiple variables at the same plot; Content: hello there, i would like to log two different variables to show up on the same chart. i use the following: for i in range(100): var1 = something var2 = something_else .log({\"var1\":something, \"var2\":something_else}) but for some reason, the dashboard shows it as two separate plots. i went through the documentation i see this: multiple metrics on one chart: log multiple metrics in the same call to .log, like this: .log({\"acc'\": 0.9, \"loss\": 0.1}) and they will both be available to plot against in the ui. can anyone help me on this? i tried many hacks like insertring a list in the place of the variable, or a dict of dicts, etc\u2026 thanks!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with logging multiple variables at the same plot, despite attempting various hacks to make it work.",
        "Question_preprocessed_content":"Title: log multiple variables at the same plot; Content: hello there, i would like to log two different variables to show up on the same chart. i use the following but for some reason, the dashboard shows it as two separate plots. i went through the documentation i see this multiple metrics on one chart log multiple metrics in the same call to like this and they will both be available to plot against in the ui. can anyone help me on this? i tried many hacks like insertring a list in the place of the variable, or a dict of dicts, etc thanks!",
        "Answer_original_content":"hey there, youll need to edit one of the charts to include the other metrics manually in the ui. ill edit the documentation to make this more clear. here is an example: this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Edit one of the charts to include the other metrics manually in the UI. \n\nSummary: The answer suggests manually editing one of the charts to include the other metrics in the UI as a possible solution to the challenge of logging multiple variables at the same plot. The documentation will also be updated to make this clearer.",
        "Answer_preprocessed_content":"hey there, youll need to edit one of the charts to include the other metrics manually in the ui. ill edit the documentation to make this more clear. here is an example this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"PySpark on Sagemaker - force stop of the execution for all cells",
        "Question_body":"Hi, I am using Jupyter Notebook on SageMaker with PySpark kernel. I want to implement some data checks so that the processing of the notebook is stopped if some conditions is meet. For Python kernel one can simply use raise statement as in the example below. If the notebook was run using \"Run All\" button then the code below would stop not only the execution of the error cell, but it would not proceed to the next cells.\n\nif type(age) is not int:\n    raise TypeError(\"Age must be an integer\")\nelif age < 0:\n    raise ValueError(\"Sorry you can't be born in the future\")\n\n\nHowever, if we use the same piece of code with PySpark kernel, execution of the error cell would be stopped but the notebook would still proceed with the execution of the next cells (as \"Run All\" was used). So in fact calculations would be proceeded even if age is not integer or is < 0.\n\nHow to force PySpark kernel to stop executing the entire notebook on error, just as it is the case for Python kernel?\n\nAdding a print screen with output notebook after \"Run All\":",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1661935002270,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":39.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW0rkjqaNS7aNKZV3mUnFGw\/py-spark-on-sagemaker-force-stop-of-the-execution-for-all-cells",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-01T10:46:43.032Z",
                "Answer_score":0,
                "Answer_body":"Hello\n\nThe functionality you have explained here is available when using the PySpark Kernel within a SageMaker Jupyter Notebook.\n\nWhen raising an error within a cell, if that error is flagged in the run time, the notebook will not continue to run the following cells even when using the Run All Option.\n\nWhen an error is met, the notebook will output the information of this error and will stop executing.\n\nI have tested this in a SageMaker Jupyter Notebook Instance with a SparkMagic PySpark Kernel and found the above to be true.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pyspark on - force stop of the execution for all cells; Content: hi, i am using jupyter notebook on with pyspark kernel. i want to implement some data checks so that the processing of the notebook is stopped if some conditions is meet. for python kernel one can simply use raise statement as in the example below. if the notebook was run using \"run all\" button then the code below would stop not only the execution of the error cell, but it would not proceed to the next cells. if type(age) is not int: raise typeerror(\"age must be an integer\") elif age < 0: raise valueerror(\"sorry you can't be born in the future\") however, if we use the same piece of code with pyspark kernel, execution of the error cell would be stopped but the notebook would still proceed with the execution of the next cells (as \"run all\" was used). so in fact calculations would be proceeded even if age is not integer or is < 0. how to force pyspark kernel to stop executing the entire notebook on error, just as it is the case for python kernel? adding a print screen with output notebook after \"run all\":",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with pyspark kernel where the execution of the notebook is not stopped when an error is encountered, unlike with python kernel, and is looking for a way to force pyspark kernel to stop executing the entire notebook on error.",
        "Question_preprocessed_content":"Title: pyspark on force stop of the execution for all cells; Content: hi, i am using jupyter notebook on with pyspark kernel. i want to implement some data checks so that the processing of the notebook is stopped if some conditions is meet. for python kernel one can simply use raise statement as in the example below. if the notebook was run using run all button then the code below would stop not only the execution of the error cell, but it would not proceed to the next cells. if type is not int raise typeerror elif age raise valueerror however, if we use the same piece of code with pyspark kernel, execution of the error cell would be stopped but the notebook would still proceed with the execution of the next cells . so in fact calculations would be proceeded even if age is not integer or is . how to force pyspark kernel to stop executing the entire notebook on error, just as it is the case for python kernel? adding a print screen with output notebook after run all",
        "Answer_original_content":"hello the functionality you have explained here is available when using the pyspark kernel within a jupyter notebook. when raising an error within a cell, if that error is flagged in the run time, the notebook will not continue to run the following cells even when using the run all option. when an error is met, the notebook will output the information of this error and will stop executing. i have tested this in a jupyter notebook instance with a sparkmagic pyspark kernel and found the above to be true.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of the pyspark kernel not stopping execution of the entire notebook on error is to use the pyspark kernel within a Jupyter notebook. When an error is raised within a cell, the notebook will not continue to run the following cells even when using the \"run all\" option. The notebook will output the information of the error and stop executing. This solution has been tested and found to be true.",
        "Answer_preprocessed_content":"hello the functionality you have explained here is available when using the pyspark kernel within a jupyter notebook. when raising an error within a cell, if that error is flagged in the run time, the notebook will not continue to run the following cells even when using the run all option. when an error is met, the notebook will output the information of this error and will stop executing. i have tested this in a jupyter notebook instance with a sparkmagic pyspark kernel and found the above to be true."
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to invert the logged images?",
        "Question_body":"<p>Hi,<\/p>\n<p>is it possible to invert my logged images ? Or is there maybe a way to do that in pytorch ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1660223927393,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":110.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-invert-the-logged-images\/2907",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-15T21:10:22.932Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/sophie_jo\">@sophie_jo<\/a> ,<\/p>\n<p>There isn\u2019t an ability to invert images directly from the UI. You are correct that this can be done in <a href=\"https:\/\/pytorch.org\/vision\/stable\/generated\/torchvision.transforms.functional.invert.html\" rel=\"noopener nofollow ugc\">pytorch<\/a>.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-14T21:10:34.462Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is it possible to invert the logged images?; Content: hi, is it possible to invert my logged images ? or is there maybe a way to do that in pytorch ?",
        "Question_original_content_gpt_summary":"The user is asking if it is possible to invert their logged images, or if there is a way to do so in PyTorch.",
        "Question_preprocessed_content":"Title: is it possible to invert the logged images?; Content: hi, is it possible to invert my logged images ? or is there maybe a way to do that in pytorch ?",
        "Answer_original_content":"hi @sophie_jo , there isnt an ability to invert images directly from the ui. you are correct that this can be done in pytorch. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Inverting images cannot be done directly from the UI, but it can be done in PyTorch.",
        "Answer_preprocessed_content":"hi , there isnt an ability to invert images directly from the ui. you are correct that this can be done in pytorch. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":54171261.0,
        "Question_title":"ClientError: train channel is not specified with AWS object_detection_augmented_manifest_training using ground truth images",
        "Question_body":"<p>I have completed a labelling job in AWS ground truth and started working on the notebook template for object detection.<\/p>\n\n<p>I have 2 manifests which has 293 labeled images for birds in a train and validation set like this:<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\",\"Bird-Label-Train\":{\"workerId\":XXXXXXXX,\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1612,\"top\":841,\"label\":\"Blackbird\",\"left\":1276,\"height\":757}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"Bird-Label-Train-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"bird-label-train\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-16T17:28:23+0000\"}}\n<\/code><\/pre>\n\n<p>Below are the parameters I am using for the notebook instance:<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 5\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"1\",\n         \"mini_batch_size\": \"16\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n \"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_train_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_validation_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I would end up with this being printed after running my ml.p3.2xlarge instance:<\/p>\n\n<pre><code>InProgress Starting\nInProgress Starting\nInProgress Starting\nInProgress Training\nFailed Failed\n<\/code><\/pre>\n\n<p>Followed by this error message: \n<strong>'ClientError: train channel is not specified.'<\/strong><\/p>\n\n<p>Does anyone have any thoughts for how I can get this running with no errors? Any help is much apreciated!<\/p>\n\n<p><strong>Successful run:<\/strong> Below is the paramaters that were used, along with the Augmented Manifest JSON Objects for a successful run.<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 50\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"3\",\n         \"mini_batch_size\": \"1\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_train_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": attribute_names # NB. This must correspond to the JSON field names in your **TRAIN** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_validation_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": [\"source-ref\",\"ValidateBird\"] # NB. This must correspond to the JSON field names in your **VALIDATION** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Training Augmented Manifest File generated during the running of the training job<\/p>\n\n<pre><code>Line 1\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_1.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1613,\"top\":840,\"height\":766,\"left\":1293}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:21:29.829003\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 2\n{\"source-ref\":\"s3:\/\/xxxxx\/Train\/Blackbird_2.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":897,\"top\":665,\"height\":1601,\"left\":1598}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:22:34.502274\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 3\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_3.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1040,\"top\":509,\"height\":1695,\"left\":1548}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:20:26.660164\",\"type\":\"groundtruth\/object-detection\"}}\n<\/code><\/pre>\n\n<p>I then unzip the model.tar file to get the following files:hyperparams.JSON, model_algo_1-0000.params and model_algo_1-symbol<\/p>\n\n<p>hyperparams.JSON looks like this:<\/p>\n\n<pre><code>{\"label_width\": \"350\", \"early_stopping_min_epochs\": \"10\", \"epochs\": \"5\", \"overlap_threshold\": \"0.5\", \"lr_scheduler_factor\": \"0.1\", \"_num_kv_servers\": \"auto\", \"weight_decay\": \"0.0005\", \"mini_batch_size\": \"1\", \"use_pretrained_model\": \"1\", \"freeze_layer_pattern\": \"\", \"lr_scheduler_step\": \"3,6\", \"early_stopping\": \"False\", \"early_stopping_patience\": \"5\", \"momentum\": \"0.9\", \"num_training_samples\": \"11\", \"optimizer\": \"rmsprop\", \"_tuning_objective_metric\": \"\", \"early_stopping_tolerance\": \"0.0\", \"learning_rate\": \"0.001\", \"kv_store\": \"device\", \"nms_threshold\": \"0.45\", \"num_classes\": \"1\", \"base_network\": \"resnet-50\", \"nms_topk\": \"400\", \"_kvstore\": \"device\", \"image_shape\": \"300\"}\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1547399704507,
        "Question_favorite_count":1.0,
        "Question_score":3.0,
        "Question_view_count":1312.0,
        "Owner_creation_time":1547398724312,
        "Owner_last_access_time":1565032201830,
        "Owner_reputation":41.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":6.0,
        "Answer_body":"<p>Thank you again for your help. All of which were valid in helping me get further. Having received a response on the AWS forum pages, I finally got it working.<\/p>\n\n<p>I understood that my JSON was slightly different to the augmented manifest training guide. Having gone back to basics, I created another labelling job, but used the 'Bounding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!<\/p>\n\n<p>As my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!<\/p>\n\n<p>i.e.<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}} \n<\/code><\/pre>\n\n<p>The original mapping was 0:'Bird' for all images through the labelling job.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_creation_time":1549801330896,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1551005141750,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54171261",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: clienterror: train channel is not specified with aws object_detection_augmented_manifest_training using ground truth images; Content: i have completed a labelling job in aws ground truth and started working on the notebook template for object detection. i have 2 manifests which has 293 labeled images for birds in a train and validation set like this: {\"source-ref\":\"s3:\/\/xxxxxxx\/train\/blackbird_1.jpg\",\"bird-label-train\":{\"workerid\":xxxxxxxx,\"imagesource\":{\"s3uri\":\"s3:\/\/xxxxxxx\/train\/blackbird_1.jpg\"},\"boxesinfo\":{\"annotatedresult\":{\"boundingboxes\":[{\"width\":1612,\"top\":841,\"label\":\"blackbird\",\"left\":1276,\"height\":757}],\"inputimageproperties\":{\"width\":3872,\"height\":2592}}}},\"bird-label-train-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"bird-label-train\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-16t17:28:23+0000\"}} below are the parameters i am using for the notebook instance: training_params = \\ { \"algorithmspecification\": { \"trainingimage\": training_image, # nb. this is one of the named constants defined in the first cell. \"traininginputmode\": \"pipe\" }, \"rolearn\": role, \"outputdataconfig\": { \"s3outputpath\": s3_output_path }, \"resourceconfig\": { \"instancecount\": 1, \"instancetype\": \"ml.p3.2xlarge\", \"volumesizeingb\": 5 }, \"trainingjobname\": job_name, \"hyperparameters\": { # nb. these hyperparameters are at the user's discretion and are beyond the scope of this demo. \"base_network\": \"resnet-50\", \"use_pretrained_model\": \"1\", \"num_classes\": \"1\", \"mini_batch_size\": \"16\", \"epochs\": \"5\", \"learning_rate\": \"0.001\", \"lr_scheduler_step\": \"3,6\", \"lr_scheduler_factor\": \"0.1\", \"optimizer\": \"rmsprop\", \"momentum\": \"0.9\", \"weight_decay\": \"0.0005\", \"overlap_threshold\": \"0.5\", \"nms_threshold\": \"0.45\", \"image_shape\": \"300\", \"label_width\": \"350\", \"num_training_samples\": str(num_training_samples) }, \"stoppingcondition\": { \"maxruntimeinseconds\": 86400 }, \"inputdataconfig\": [ { \"channelname\": \"train\", \"datasource\": { \"s3datasource\": { \"s3datatype\": \"augmentedmanifestfile\", # nb. augmented manifest \"s3uri\": s3_train_data_path, \"s3datadistributiontype\": \"fullyreplicated\", \"attributenames\": [\"source-ref\",\"bird-label-train\"] # nb. this must correspond to the json field names in your augmented manifest. } }, \"contenttype\": \"image\/jpeg\", \"recordwrappertype\": \"none\", \"compressiontype\": \"none\" }, { \"channelname\": \"validation\", \"datasource\": { \"s3datasource\": { \"s3datatype\": \"augmentedmanifestfile\", # nb. augmented manifest \"s3uri\": s3_validation_data_path, \"s3datadistributiontype\": \"fullyreplicated\", \"attributenames\": [\"source-ref\",\"bird-label\"] # nb. this must correspond to the json field names in your augmented manifest. } }, \"contenttype\": \"image\/jpeg\", \"recordwrappertype\": \"none\", \"compressiontype\": \"none\" } ] i would end up with this being printed after running my ml.p3.2xlarge instance: inprogress starting inprogress starting inprogress starting inprogress training failed failed followed by this error message: 'clienterror: train channel is not specified.' does anyone have any thoughts for how i can get this running with no errors? any help is much apreciated! successful run: below is the paramaters that were used, along with the augmented manifest json objects for a successful run. training_params = \\ { \"algorithmspecification\": { \"trainingimage\": training_image, # nb. this is one of the named constants defined in the first cell. \"traininginputmode\": \"pipe\" }, \"rolearn\": role, \"outputdataconfig\": { \"s3outputpath\": s3_output_path }, \"resourceconfig\": { \"instancecount\": 1, \"instancetype\": \"ml.p3.2xlarge\", \"volumesizeingb\": 50 }, \"trainingjobname\": job_name, \"hyperparameters\": { # nb. these hyperparameters are at the user's discretion and are beyond the scope of this demo. \"base_network\": \"resnet-50\", \"use_pretrained_model\": \"1\", \"num_classes\": \"3\", \"mini_batch_size\": \"1\", \"epochs\": \"5\", \"learning_rate\": \"0.001\", \"lr_scheduler_step\": \"3,6\", \"lr_scheduler_factor\": \"0.1\", \"optimizer\": \"rmsprop\", \"momentum\": \"0.9\", \"weight_decay\": \"0.0005\", \"overlap_threshold\": \"0.5\", \"nms_threshold\": \"0.45\", \"image_shape\": \"300\", \"label_width\": \"350\", \"num_training_samples\": str(num_training_samples) }, \"stoppingcondition\": { \"maxruntimeinseconds\": 86400 }, \"inputdataconfig\": [ { \"channelname\": \"train\", \"datasource\": { \"s3datasource\": { \"s3datatype\": \"augmentedmanifestfile\", # nb. augmented manifest \"s3uri\": s3_train_data_path, \"s3datadistributiontype\": \"fullyreplicated\", \"attributenames\": attribute_names # nb. this must correspond to the json field names in your **train** augmented manifest. } }, \"contenttype\": \"application\/x-recordio\", \"recordwrappertype\": \"recordio\", \"compressiontype\": \"none\" }, { \"channelname\": \"validation\", \"datasource\": { \"s3datasource\": { \"s3datatype\": \"augmentedmanifestfile\", # nb. augmented manifest \"s3uri\": s3_validation_data_path, \"s3datadistributiontype\": \"fullyreplicated\", \"attributenames\": [\"source-ref\",\"validatebird\"] # nb. this must correspond to the json field names in your **validation** augmented manifest. } }, \"contenttype\": \"application\/x-recordio\", \"recordwrappertype\": \"recordio\", \"compressiontype\": \"none\" } ] } training augmented manifest file generated during the running of the training job line 1 {\"source-ref\":\"s3:\/\/xxxxx\/train\/blackbird_1.jpg\",\"trainbird\":{\"annotations\":[{\"class_id\":0,\"width\":1613,\"top\":840,\"height\":766,\"left\":1293}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"trainbird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09t14:21:29.829003\",\"type\":\"groundtruth\/object-detection\"}} line 2 {\"source-ref\":\"s3:\/\/xxxxx\/train\/blackbird_2.jpg\",\"trainbird\":{\"annotations\":[{\"class_id\":0,\"width\":897,\"top\":665,\"height\":1601,\"left\":1598}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"trainbird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09t14:22:34.502274\",\"type\":\"groundtruth\/object-detection\"}} line 3 {\"source-ref\":\"s3:\/\/xxxxx\/train\/blackbird_3.jpg\",\"trainbird\":{\"annotations\":[{\"class_id\":0,\"width\":1040,\"top\":509,\"height\":1695,\"left\":1548}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"trainbird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09t14:20:26.660164\",\"type\":\"groundtruth\/object-detection\"}} i then unzip the model.tar file to get the following files:hyperparams.json, model_algo_1-0000.params and model_algo_1-symbol hyperparams.json looks like this: {\"label_width\": \"350\", \"early_stopping_min_epochs\": \"10\", \"epochs\": \"5\", \"overlap_threshold\": \"0.5\", \"lr_scheduler_factor\": \"0.1\", \"_num_kv_servers\": \"auto\", \"weight_decay\": \"0.0005\", \"mini_batch_size\": \"1\", \"use_pretrained_model\": \"1\", \"freeze_layer_pattern\": \"\", \"lr_scheduler_step\": \"3,6\", \"early_stopping\": \"false\", \"early_stopping_patience\": \"5\", \"momentum\": \"0.9\", \"num_training_samples\": \"11\", \"optimizer\": \"rmsprop\", \"_tuning_objective_metric\": \"\", \"early_stopping_tolerance\": \"0.0\", \"learning_rate\": \"0.001\", \"kv_store\": \"device\", \"nms_threshold\": \"0.45\", \"num_classes\": \"1\", \"base_network\": \"resnet-50\", \"nms_topk\": \"400\", \"_kvstore\": \"device\", \"image_shape\": \"300\"}",
        "Question_original_content_gpt_summary":"The user encountered a challenge with their AWS Object Detection Augmented Manifest Training, resulting in a 'clienterror: train channel is not specified' error message.",
        "Question_preprocessed_content":"Title: clienterror train channel is not specified with aws using ground truth images; Content: i have completed a labelling job in aws ground truth and started working on the notebook template for object detection. i have manifests which has labeled images for birds in a train and validation set like this below are the parameters i am using for the notebook instance i would end up with this being printed after running my instance followed by this error message 'clienterror train channel is not does anyone have any thoughts for how i can get this running with no errors? any help is much apreciated! successful run below is the paramaters that were used, along with the augmented manifest json objects for a successful run. training augmented manifest file generated during the running of the training job i then unzip the file to get the following and looks like this",
        "Answer_original_content":"thank you again for your help. all of which were valid in helping me get further. having received a response on the aws forum pages, i finally got it working. i understood that my json was slightly different to the augmented manifest training guide. having gone back to basics, i created another labelling job, but used the 'bounding box' type as opposed to the 'custom - bounding box template'. my output matched what was expected. this ran with no errors! as my purpose was to have multiple labels, i was able to edit the files and mapping of my output manifests, which also worked! i.e. {\"source-ref\":\"s3:\/\/xxxxx\/blackbird_15.jpg\",\"validatebird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"validatebird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09t14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}} {\"source-ref\":\"s3:\/\/xxxx\/pigeon_19.jpg\",\"validatebird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"validatebird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09t14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}} the original mapping was 0:'bird' for all images through the labelling job.",
        "Answer_original_content_gpt_summary":"The user encountered a 'clienterror: train channel is not specified' error message while using AWS Object Detection Augmented Manifest Training. They received help from the AWS forum pages and found that their JSON was slightly different from the augmented manifest training guide. They created another labeling job using the 'bounding box' type instead of the 'custom - bounding box template', which resulted in the expected output with no errors. They were able to edit the files and mapping of their output manifests to have multiple labels.",
        "Answer_preprocessed_content":"thank you again for your help. all of which were valid in helping me get further. having received a response on the aws forum pages, i finally got it working. i understood that my json was slightly different to the augmented manifest training guide. having gone back to basics, i created another labelling job, but used the 'bounding box' type as opposed to the 'custom bounding box template'. my output matched what was expected. this ran with no errors! as my purpose was to have multiple labels, i was able to edit the files and mapping of my output manifests, which also worked! the original mapping was 'bird' for all images through the labelling job."
    },
    {
        "Question_id":72732616.0,
        "Question_title":"Can't find scoring.py when using PythonScriptStep() in Databricks",
        "Question_body":"<p>We are defining in Databricks a PythonScriptStep(). When using PythonScriptStep() within our pipeline script we can't find the scoring.py file.<\/p>\n<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>We getting the following error message:<\/p>\n<pre><code>Step [Scoring_Step]: script not found at: \/databricks\/driver\/scoring.py. Make sure to specify an appropriate source_directory on the Step or default_source_directory on the Pipeline.\n<\/code><\/pre>\n<p>For some reason Databricks is searching for the file in '\/databricks\/driver\/' instead of the folder we entered.<\/p>\n<p>There is also the way to use DatabricksStep() instead of PythonScriptStep(), but because of specific reasons we need to use the PythonSriptStep() class.<\/p>\n<p>Could anybody help us with this specific problem?<\/p>\n<p>Thank you very much for any help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1655997421433,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Owner_creation_time":1544598969960,
        "Owner_last_access_time":1663828260856,
        "Owner_reputation":37.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":21.0,
        "Answer_body":"<pre><code>scoring_step = PythonScriptStep(\n    name=&quot;Scoring_Step&quot;,\n    source_directory=os.getenv(&quot;DATABRICKS_NOTEBOOK_PATH&quot;, &quot;\/Users\/USER_NAME\/source_directory&quot;),\n    script_name=&quot;.\/scoring.py&quot;,\n    arguments=[&quot;--input_dataset&quot;, ds_consumption],\n    compute_target=pipeline_cluster,\n    runconfig=pipeline_run_config,\n    allow_reuse=False)\n<\/code><\/pre>\n<p>Change the above code block with below code block. It will resolve the error<\/p>\n<pre><code>data_ref = OutputFileDatasetConfig(\n    name='data_ref',\n    destination=(ds, '\/data')\n).as_upload()\n\n\ndata_prep_step = PythonScriptStep(\n    name='data_prep',\n    script_name='pipeline_steps\/data_prep.py',\n    source_directory='\/.',\n    arguments=[\n        '--main_path', main_ref,\n        '--data_ref_folder', data_ref\n                ],\n    inputs=[main_ref, data_ref],\n    outputs=[data_ref],\n    runconfig=arbitrary_run_config,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p>Reference link for the <a href=\"https:\/\/scoring_step%20=%20PythonScriptStep(%20%20%20%20%20name=%22Scoring_Step%22,%20%20%20%20%20source_directory=os.getenv(%22DATABRICKS_NOTEBOOK_PATH%22,%20%22\/Users\/USER_NAME\/source_directory%22),%20%20%20%20%20script_name=%22.\/scoring.py%22,%20%20%20%20%20arguments=%5B%22--input_dataset%22,%20ds_consumption%5D,%20%20%20%20%20compute_target=pipeline_cluster,%20%20%20%20%20runconfig=pipeline_run_config,%20%20%20%20%20allow_reuse=False)\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656324774632,
        "Answer_score":0.0,
        "Owner_location":"Germany",
        "Question_last_edit_time":1656039942703,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72732616",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can't find scoring.py when using pythonscriptstep() in databricks; Content: we are defining in databricks a pythonscriptstep(). when using pythonscriptstep() within our pipeline script we can't find the scoring.py file. scoring_step = pythonscriptstep( name=\"scoring_step\", source_directory=os.getenv(\"databricks_notebook_path\", \"\/users\/user_name\/source_directory\"), script_name=\".\/scoring.py\", arguments=[\"--input_dataset\", ds_consumption], compute_target=pipeline_cluster, runconfig=pipeline_run_config, allow_reuse=false) we getting the following error message: step [scoring_step]: script not found at: \/databricks\/driver\/scoring.py. make sure to specify an appropriate source_directory on the step or default_source_directory on the pipeline. for some reason databricks is searching for the file in '\/databricks\/driver\/' instead of the folder we entered. there is also the way to use databricksstep() instead of pythonscriptstep(), but because of specific reasons we need to use the pythonsriptstep() class. could anybody help us with this specific problem? thank you very much for any help!",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with using the pythonscriptstep() class in Databricks, as the script is not being found in the specified source directory.",
        "Question_preprocessed_content":"Title: can't find when using pythonscriptstep in databricks; Content: we are defining in databricks a pythonscriptstep . when using pythonscriptstep within our pipeline script we can't find the file. we getting the following error message for some reason databricks is searching for the file in instead of the folder we entered. there is also the way to use databricksstep instead of pythonscriptstep , but because of specific reasons we need to use the pythonsriptstep class. could anybody help us with this specific problem? thank you very much for any help!",
        "Answer_original_content":"scoring_step = pythonscriptstep( name=\"scoring_step\", source_directory=os.getenv(\"databricks_notebook_path\", \"\/users\/user_name\/source_directory\"), script_name=\".\/scoring.py\", arguments=[\"--input_dataset\", ds_consumption], compute_target=pipeline_cluster, runconfig=pipeline_run_config, allow_reuse=false) change the above code block with below code block. it will resolve the error data_ref = outputfiledatasetconfig( name='data_ref', destination=(ds, '\/data') ).as_upload() data_prep_step = pythonscriptstep( name='data_prep', script_name='pipeline_steps\/data_prep.py', source_directory='\/.', arguments=[ '--main_path', main_ref, '--data_ref_folder', data_ref ], inputs=[main_ref, data_ref], outputs=[data_ref], runconfig=arbitrary_run_config, allow_reuse=false ) reference link for the documentation",
        "Answer_original_content_gpt_summary":"The answer suggests changing the code block to resolve the error encountered while using the pythonscriptstep() class in Databricks. The suggested solution involves using data_ref and data_prep_step to upload and prepare data respectively, with the help of specific arguments and configurations. The reference link to the documentation is also provided.",
        "Answer_preprocessed_content":"change the above code block with below code block. it will resolve the error reference link for the documentation"
    },
    {
        "Question_id":null,
        "Question_title":"Explaining a model in AzureML Studio",
        "Question_body":"Hi,\n\nThere is an issue when I try to explain a Time Series model created with AzureML Studio, with the AutoML service.\n\nWhen the proccess finishes and the best model is automatically explained, I'd like to see a chart with the information of \"Predicted Values vs True Values\" but I can\u00b4t find anything similar. Then I realized that when I click on the explanation of the model, the next message is showed:\n\n\"Las estad\u00edsticas de rendimiento del modelo requieren que se proporcionen los resultados verdaderos adem\u00e1s de los previstos.\"\n\nTranslated: \"The performance statistics of the model require to provide true values in addition to the predicted ones.\"\n\nHow can I provide the model with the true values? I think it should be done automatically by the process, isn\u00b4t it? I mean, true values is part of the dataset, it is in fact the target column.\n\nFurthermore, I tried to create a classification model and I don\u00b4t have that problem there.\n\nAnyone could help me? Without a chart where I can compare true vs predicted values, the model doesn\u00b4t make sense.\n\n\n\n\nThank you very much in advance.",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1623017811457,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/423931\/explicacion-de-un-modelo-en-azureml-studio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-08T03:19:11.683Z",
                "Answer_score":0,
                "Answer_body":"@FernndezCalvoAlberto-0353 Thanks, For forecasting experiment the predicted vs. true chart plots the relationship between the target feature (true\/actual values) and the model's predictions. Please follow the document for Predicted vs. true charts.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-understand-automated-ml#prerequisites",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: explaining a model in studio; Content: hi, there is an issue when i try to explain a time series model created with studio, with the automl service. when the proccess finishes and the best model is automatically explained, i'd like to see a chart with the information of \"predicted values vs true values\" but i can\u00b4t find anything similar. then i realized that when i click on the explanation of the model, the next message is showed: \"las estad\u00edsticas de rendimiento del modelo requieren que se proporcionen los resultados verdaderos adem\u00e1s de los previstos.\" translated: \"the performance statistics of the model require to provide true values in addition to the predicted ones.\" how can i provide the model with the true values? i think it should be done automatically by the process, isn\u00b4t it? i mean, true values is part of the dataset, it is in fact the target column. furthermore, i tried to create a classification model and i don\u00b4t have that problem there. anyone could help me? without a chart where i can compare true vs predicted values, the model doesn\u00b4t make sense. thank you very much in advance.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge when attempting to explain a time series model created with Studio, as the process requires the user to provide true values in addition to the predicted ones in order to generate a chart comparing the two.",
        "Question_preprocessed_content":"Title: explaining a model in studio; Content: hi, there is an issue when i try to explain a time series model created with studio, with the automl service. when the proccess finishes and the best model is automatically explained, i'd like to see a chart with the information of predicted values vs true values but i cant find anything similar. then i realized that when i click on the explanation of the model, the next message is showed las estadsticas de rendimiento del modelo requieren que se proporcionen los resultados verdaderos adems de los translated the performance statistics of the model require to provide true values in addition to the predicted how can i provide the model with the true values? i think it should be done automatically by the process, isnt it? i mean, true values is part of the dataset, it is in fact the target column. furthermore, i tried to create a classification model and i dont have that problem there. anyone could help me? without a chart where i can compare true vs predicted values, the model doesnt make sense. thank you very much in advance.",
        "Answer_original_content":"@fernndezcalvoalberto-0353 thanks, for forecasting experiment the predicted vs. true chart plots the relationship between the target feature (true\/actual values) and the model's predictions. please follow the document for predicted vs. true charts. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-understand-automated-ml#prerequisites",
        "Answer_original_content_gpt_summary":"The answer suggests that the user should follow the provided document for generating predicted vs. true charts in order to compare the target feature and the model's predictions. The document provides prerequisites for creating these charts.",
        "Answer_preprocessed_content":"thanks, for forecasting experiment the predicted vs. true chart plots the relationship between the target feature and the model's predictions. please follow the document for predicted vs. true charts."
    },
    {
        "Question_id":null,
        "Question_title":"Experiment on Azure Machine Learning services is not starting.",
        "Question_body":"I run an experiment in the azure ml service using an associated VM, but the experiment status is as follows:\n\nJob runstatus is NotStarted\n\nID execution 75ff42c9-2fbd-48c2-beec-b72aa38f1d00",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1620678944347,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/389811\/experiment-on-azure-machine-learning-services-is-n.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-11T01:51:49.837Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nThanks for reaching out to us. I just check on my side and there is no problem for me.\n\nAre you referring below two guidance to train and deploy your pipeline?\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\n\nIs there any guidance you are following so that we can help investigate more?\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-11T14:46:42.987Z",
                "Answer_score":0,
                "Answer_body":"Hi,\nI'm not using the designer or Azure ML Studio, I'm building the pipeline trough code in python.\n\nI'm followig this tutorial:\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\n\nI have a Remote VM ( (ubuntu 18.04)) which has a VPN connected, I'm trying to use this VM to execute a script but In the pipeline it does not initiate.\n\n\n\n\n\n\nIn addition to this, any of the experiment I have sent were completed. In the image we can clearly see that it says duration 16 h until i have to cancel the experiment. whereas the script only have to print a string for testing",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: experiment on services is not starting.; Content: i run an experiment in the service using an associated vm, but the experiment status is as follows: job runstatus is notstarted id execution 75ff42c9-2fbd-48c2-beec-b72aa38f1d00",
        "Question_original_content_gpt_summary":"The user encountered a challenge where their experiment on services was not starting.",
        "Question_preprocessed_content":"Title: experiment on services is not starting.; Content: i run an experiment in the service using an associated vm, but the experiment status is as follows job runstatus is notstarted id execution",
        "Answer_original_content":"hello, thanks for reaching out to us. i just check on my side and there is no problem for me. are you referring below two guidance to train and deploy your pipeline? https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy is there any guidance you are following so that we can help investigate more? regards, yutong hi, i'm not using the designer or studio, i'm building the pipeline trough code in python. i'm followig this tutorial: https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/master\/how-to-use-\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb i have a remote vm ( (ubuntu 18.04)) which has a vpn connected, i'm trying to use this vm to execute a script but in the pipeline it does not initiate. in addition to this, any of the experiment i have sent were completed. in the image we can clearly see that it says duration 16 h until i have to cancel the experiment. whereas the script only have to print a string for testing",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. Check if the user is following the correct guidance to train and deploy their pipeline.\n2. Investigate if there is any other guidance the user is following.\n3. Check if the user is using the correct tools to build the pipeline.\n4. Verify if the user's remote VM is properly connected to the VPN.\n5. Check if the user's script is correct and properly configured.\n6. Investigate why the experiment is not initiating and why it is taking too long to complete.",
        "Answer_preprocessed_content":"hello, thanks for reaching out to us. i just check on my side and there is no problem for me. are you referring below two guidance to train and deploy your pipeline? is there any guidance you are following so that we can help investigate more? regards, yutong hi, i'm not using the designer or studio, i'm building the pipeline trough code in python. i'm followig this tutorial i have a remote vm which has a vpn connected, i'm trying to use this vm to execute a script but in the pipeline it does not initiate. in addition to this, any of the experiment i have sent were completed. in the image we can clearly see that it says duration h until i have to cancel the experiment. whereas the script only have to print a string for testing"
    },
    {
        "Question_id":59983062.0,
        "Question_title":"TSV as input to sagemaker",
        "Question_body":"<p>Is there any way to use a tsv instead of a csv as the input into sagemaker's autopilot ?<\/p>\n\n<p>Currently I'm inputting the data as such:<\/p>\n\n<pre><code>input_data_config = [{\n      'DataSource': {\n        'S3DataSource': {\n          'S3DataType': 'S3Prefix',\n          'S3Uri': 's3:\/\/{}\/{}\/train'.format(bucket,prefix)\n        }\n      },\n      'TargetAttributeName': 'sentiment'\n    }\n  ]\n<\/code><\/pre>\n\n<p>this seems to work file for .csv files but fails for my .tsv files.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1580377511313,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":204.0,
        "Owner_creation_time":1479115407580,
        "Owner_last_access_time":1662241113300,
        "Owner_reputation":2944.0,
        "Owner_up_votes":177.0,
        "Owner_down_votes":79.0,
        "Owner_views":381.0,
        "Answer_body":"<p>I am a developer at AWS SageMaker. Autopilot currently only supports CSV data. While we are working on extending the support to more file formats: JSON, TSV, etc, this might be something that you can try to convert your .tsv file to .csv:<\/p>\n\n<pre><code>import csv\n\n# read tab-delimited file\nwith open('yourfile.tsv','rb') as fin:\n    cr = csv.reader(fin, delimiter='\\t')\n    filecontents = [line for line in cr]\n\n# write comma-delimited file (comma is the default delimiter)\nwith open('yourfile.csv','wb') as fou:\n    cw = csv.writer(fou, quotechar='', quoting=csv.QUOTE_NONE)\n    cw.writerows(filecontents)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>\n\n<p>Ref: <a href=\"https:\/\/stackoverflow.com\/questions\/5590631\/how-to-convert-a-tab-separated-file-to-csv-format\">How to convert a tab separated file to CSV format?<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1580778531436,
        "Answer_score":1.0,
        "Owner_location":"Earth",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59983062",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tsv as input to ; Content: is there any way to use a tsv instead of a csv as the input into 's autopilot ? currently i'm inputting the data as such: input_data_config = [{ 'datasource': { 's3datasource': { 's3datatype': 's3prefix', 's3uri': 's3:\/\/{}\/{}\/train'.format(bucket,prefix) } }, 'targetattributename': 'sentiment' } ] this seems to work file for .csv files but fails for my .tsv files.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with using a tsv instead of a csv as the input into 's autopilot.",
        "Question_preprocessed_content":"Title: tsv as input to ; Content: is there any way to use a tsv instead of a csv as the input into 's autopilot ? currently i'm inputting the data as such this seems to work file for .csv files but fails for my .tsv files.",
        "Answer_original_content":"i am a developer at . autopilot currently only supports csv data. while we are working on extending the support to more file formats: json, tsv, etc, this might be something that you can try to convert your .tsv file to .csv: import csv # read tab-delimited file with open('yourfile.tsv','rb') as fin: cr = csv.reader(fin, delimiter='\\t') filecontents = [line for line in cr] # write comma-delimited file (comma is the default delimiter) with open('yourfile.csv','wb') as fou: cw = csv.writer(fou, quotechar='', quoting=csv.quote_none) cw.writerows(filecontents) hope this helps. ref: how to convert a tab separated file to csv format?",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of using a tsv instead of a csv as input into 's autopilot are to either wait for the extension of support to more file formats or to convert the .tsv file to .csv using the provided code snippet. The code snippet involves reading the tab-delimited file and writing it as a comma-delimited file.",
        "Answer_preprocessed_content":"i am a developer at . autopilot currently only supports csv data. while we are working on extending the support to more file formats json, tsv, etc, this might be something that you can try to convert your .tsv file to .csv hope this helps. ref how to convert a tab separated file to csv format?"
    },
    {
        "Question_id":null,
        "Question_title":"Secure Azure Machine Learning REST Endpoints (deployed in ACI) with TLS",
        "Question_body":"We have developed and deployed machine learning models in AML Studio. The models were deployed using ACI and we have REST endpoints that we can make calls to successfully. Next thing that I need to do is to secure the endpoints using TLS. I am going through the following article:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-web-service#enable\n\nThe article suggests that I need to get a domain and then update our DNS point to the IP address of scoring endpoint. I have a subdomain ready to use but as for the IP address, I can't work out where I would get the IP address of the scoring endpoint and how I would even be able to map this to the endpoint as the current endpoint do not contain and IP address and look nothing like the example in the article.\n\nURIs currently look like the following:\nhttp:\/\/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx.northeurope.azurecontainer.io\/score\n\nAnyone able to help with this one please as it's a little confusing and I can't find any guidance online anywhere?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1615991231360,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/318807\/secure-azure-machine-learning-rest-endpoints-deplo.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-18T03:31:43.613Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nYou can do it according to DNS.\n\nA \u201cURL\u201d is a full specification to a page. For example:\n\nhttp:\/\/example.com\/this_is_example.html is a URL. It has three parts:\n\nThe protocol specifier: http:\n\nThe domain name: example.com\n\nThe page location: \/this_is_example.html\n\nThe protocol specifies the port that will be used. http, for example, is\nport 80. ftp uses ports 20 and 21. SMTP, the mail sending protocol, is usually\non port 25. You can actually find the full list of \u201cofficial\u201d ports here.\n\nIt\u2019s only the domain name that has an IP address associated with it. So that\u2019s what you would be looking up.\n\nMy approach is to use the \u201cping\u201d command in a Windows command prompt. For\nexample:\n\nC:\\>ping example.com\n\nThen you can get it.\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: secure rest endpoints (deployed in aci) with tls; Content: we have developed and deployed machine learning models in aml studio. the models were deployed using aci and we have rest endpoints that we can make calls to successfully. next thing that i need to do is to secure the endpoints using tls. i am going through the following article: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-web-service#enable the article suggests that i need to get a domain and then update our dns point to the ip address of scoring endpoint. i have a subdomain ready to use but as for the ip address, i can't work out where i would get the ip address of the scoring endpoint and how i would even be able to map this to the endpoint as the current endpoint do not contain and ip address and look nothing like the example in the article. uris currently look like the following: http:\/\/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx.northeurope.azurecontainer.io\/score anyone able to help with this one please as it's a little confusing and i can't find any guidance online anywhere?",
        "Question_original_content_gpt_summary":"The user is facing challenges in securing their REST endpoints deployed in ACI with TLS, as they are unable to work out where to get the IP address of the scoring endpoint and how to map it to the endpoint.",
        "Question_preprocessed_content":"Title: secure rest endpoints with tls; Content: we have developed and deployed machine learning models in aml studio. the models were deployed using aci and we have rest endpoints that we can make calls to successfully. next thing that i need to do is to secure the endpoints using tls. i am going through the following article the article suggests that i need to get a domain and then update our dns point to the ip address of scoring endpoint. i have a subdomain ready to use but as for the ip address, i can't work out where i would get the ip address of the scoring endpoint and how i would even be able to map this to the endpoint as the current endpoint do not contain and ip address and look nothing like the example in the article. uris currently look like the following anyone able to help with this one please as it's a little confusing and i can't find any guidance online anywhere?",
        "Answer_original_content":"hello, you can do it according to dns. a url is a full specification to a page. for example: http:\/\/example.com\/this_is_example.html is a url. it has three parts: the protocol specifier: http: the domain name: example.com the page location: \/this_is_example.html the protocol specifies the port that will be used. http, for example, is port 80. ftp uses ports 20 and 21. smtp, the mail sending protocol, is usually on port 25. you can actually find the full list of official ports here. its only the domain name that has an ip address associated with it. so thats what you would be looking up. my approach is to use the ping command in a windows command prompt. for example: c:\\>ping example.com then you can get it. regards, yutong",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of securing REST endpoints deployed in ACI with TLS are to use DNS to map the IP address of the scoring endpoint, and to use the ping command in a Windows command prompt to look up the IP address associated with the domain name.",
        "Answer_preprocessed_content":"hello, you can do it according to dns. a url is a full specification to a page. for example is a url. it has three parts the protocol specifier http the domain name the page location the protocol specifies the port that will be used. http, for example, is port . ftp uses ports and . smtp, the mail sending protocol, is usually on port . you can actually find the full list of official ports here. its only the domain name that has an ip address associated with it. so thats what you would be looking up. my approach is to use the ping command in a windows command prompt. for example then you can get it. regards, yutong"
    },
    {
        "Question_id":50576929.0,
        "Question_title":"Replacing values in dataset within Azure Machine Learning Studio",
        "Question_body":"<p>In Azure Machine Learning studio I need to convert a column of data that has three categorical values 'yes', 'no' and 'maybe', and wish to combine the 'no' and 'maybe' values as just 'no'. <\/p>\n\n<p>I can do this easily using SQL, R, or Python but for these purposes I need to show if it is possible to do this without using these languages. I can't seem to find a way to do this. <\/p>\n\n<p>Does anyone have any ideas? I'm fine if the answer is no but I don't want to say it's not possible if it is. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1527572052450,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":484.0,
        "Owner_creation_time":1367782461047,
        "Owner_last_access_time":1574911198152,
        "Owner_reputation":1647.0,
        "Owner_up_votes":365.0,
        "Owner_down_votes":8.0,
        "Owner_views":321.0,
        "Answer_body":"<p>It can be done! :)<\/p>\n\n<p>You would just use the \"Group Categorical Values\" module. Choose the column that has the data you want to group, and you can set the values like the following:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UGhrR.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What's going on here is that the default, which will get used if the other levels aren't caught, is set to \"yes\". Then when any values are \"no\", or \"maybe\", it gets grouped into a category of \"no\".<\/p>\n\n<p>However, this will error unless you make that column a categorical type, so you would need to use the \"Edit Metadata\" module to do that.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/45s2Q.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The example I used is <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Replace-Values-in-Dataset\" rel=\"nofollow noreferrer\">published to the gallery<\/a>, if you need to reference it.<\/p>\n\n<p>If you need more info, just let me know.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1527597469392,
        "Answer_score":3.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50576929",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: replacing values in dataset within studio; Content: in studio i need to convert a column of data that has three categorical values 'yes', 'no' and 'maybe', and wish to combine the 'no' and 'maybe' values as just 'no'. i can do this easily using sql, r, or python but for these purposes i need to show if it is possible to do this without using these languages. i can't seem to find a way to do this. does anyone have any ideas? i'm fine if the answer is no but i don't want to say it's not possible if it is.",
        "Question_original_content_gpt_summary":"The user is looking for a way to replace values in a dataset within Studio without using SQL, R, or Python.",
        "Question_preprocessed_content":"Title: replacing values in dataset within studio; Content: in studio i need to convert a column of data that has three categorical values 'yes', 'no' and 'maybe', and wish to combine the 'no' and 'maybe' values as just 'no'. i can do this easily using sql, r, or python but for these purposes i need to show if it is possible to do this without using these languages. i can't seem to find a way to do this. does anyone have any ideas? i'm fine if the answer is no but i don't want to say it's not possible if it is.",
        "Answer_original_content":"it can be done! :) you would just use the \"group categorical values\" module. choose the column that has the data you want to group, and you can set the values like the following: what's going on here is that the default, which will get used if the other levels aren't caught, is set to \"yes\". then when any values are \"no\", or \"maybe\", it gets grouped into a category of \"no\". however, this will error unless you make that column a categorical type, so you would need to use the \"edit metadata\" module to do that. the example i used is published to the gallery, if you need to reference it. if you need more info, just let me know.",
        "Answer_original_content_gpt_summary":"The answer suggests using the \"group categorical values\" module in Studio to replace values in a dataset without using SQL, R, or Python. The user needs to choose the column that has the data they want to group and set the values accordingly. They also need to make sure to make the column a categorical type using the \"edit metadata\" module to avoid errors. An example is available in the gallery for reference.",
        "Answer_preprocessed_content":"it can be done! you would just use the group categorical values module. choose the column that has the data you want to group, and you can set the values like the following what's going on here is that the default, which will get used if the other levels aren't caught, is set to yes . then when any values are no , or maybe , it gets grouped into a category of no . however, this will error unless you make that column a categorical type, so you would need to use the edit metadata module to do that. the example i used is published to the gallery, if you need to reference it. if you need more info, just let me know."
    },
    {
        "Question_id":56353814.0,
        "Question_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Question_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1559108619707,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1826.0,
        "Owner_creation_time":1559099281007,
        "Owner_last_access_time":1624868926128,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559329920120,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56353814",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: batch transform job results in \"internalservererror\" with data file >100mb; Content: i'm using in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). i used a tensorflow model in script mode, my script being very similar to the one i used as reference: https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py the training reported success and i was able to deploy a model for batch transformation. the transform job works fine when i input just a few samples (say, [10,24,11]), but it returns an internalservererror when i input more samples for prediction (for example, [30000, 24, 11], which size is >100mb). here is the error: --------------------------------------------------------------------------- valueerror traceback (most recent call last) <ipython-input-6-0c46f7563389> in <module>() 32 33 # then wait until transform job is completed ---> 34 tf_transformer.wait() ~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/\/transformer.py in wait(self) 133 def wait(self): 134 self._ensure_last_transform_job() --> 135 self.latest_transform_job.wait() 136 137 def _ensure_last_transform_job(self): ~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/\/transformer.py in wait(self) 207 208 def wait(self): --> 209 self._session.wait_for_transform_job(self.job_name) 210 211 @staticmethod ~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/\/session.py in wait_for_transform_job(self, job, poll) 893 \"\"\" 894 desc = _wait_until(lambda: _transform_job_status(self._client, job), poll) --> 895 self._check_job_status(job, desc, 'transformjobstatus') 896 return desc 897 ~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/\/session.py in _check_job_status(self, job, desc, status_key_name) 915 reason = desc.get('failurereason', '(no reason provided)') 916 job_type = status_key_name.replace('jobstatus', ' job') --> 917 raise valueerror('error for {} {}: {} reason: {}'.format(job_type, job, status, reason)) 918 919 def wait_for_endpoint(self, endpoint, poll=5): valueerror: error for transform job tensorflow-batch-transform-2019-05-29-02-56-00-477: failed reason: internalservererror: we encountered an internal error. please try again. i tried to use both singlerecord and multirecord parameters when deploying the model but the result was the same, so i decided to keep multirecord. my transformer looks like that: transformer = tf_estimator.transformer( instance_count=1, instance_type='ml.m4.xlarge', max_payload = 100, assemble_with = 'line', strategy='multirecord' ) at first i was using a json file as input for the transform job, and it threw the error : too much data for max payload size so next i tried the jsonlines format (the .npy format is not supported as far as i understand), thinking that jsonlines could get split by line and thus avoid the size error, but that's where i got the internalservererror. here is the related code: #convert test_x to jsonlines and save test_x_list = test_x.tolist() file_path ='data_cnn_test\/test_x.jsonl' file_name='test_x.jsonl' with jsonlines.open(file_path, 'w') as writer: writer.write(test_x_list) input_key = 'batch_transform_tf\/input\/{}'.format(file_name) output_key = 'batch_transform_tf\/output' test_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key) test_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key) s3.upload_file(file_path, bucket, input_key) # initialize the transformer object tf_transformer = .transformer.transformer( base_transform_job_name='tensorflow-batch-transform', model_name='-tensorflow-scriptmode-2019-05-29-02-46-36-162', instance_count=1, instance_type='ml.c4.2xlarge', output_path=test_output_location, assemble_with = 'line' ) # start the transform job tf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='line') the list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so i would like to return 30000 predictions. i suspect my jsonlines file isn't being split by line and is of course too big to be processed in one batch, which throws the error, but i don't understand why it doesn't get split correctly. i am using the default output_fn and input_fn (i did not re-write those functions in my script). any insight on what i could be doing wrong would be greatly appreciated.",
        "Question_original_content_gpt_summary":"The user is encountering an \"internalservererror\" when attempting to perform a batch transformation job with a data file larger than 100mb, despite attempting to use the jsonlines format to split the file into smaller batches.",
        "Question_preprocessed_content":"Title: batch transform job results in internalservererror with data file mb; Content: i'm using in order to perform binary classification on time series, each sample being a numpy array of shape . i used a tensorflow model in script mode, my script being very similar to the one i used as reference the training reported success and i was able to deploy a model for batch transformation. the transform job works fine when i input just a few samples , but it returns an when i input more samples for prediction . here is the error i tried to use both singlerecord and multirecord parameters when deploying the model but the result was the same, so i decided to keep multirecord. my transformer looks like that at first i was using a json file as input for the transform job, and it threw the error so next i tried the jsonlines format , thinking that jsonlines could get split by line and thus avoid the size error, but that's where i got the . here is the related code the list named has a shape , which corresponds to samples so i would like to return predictions. i suspect my jsonlines file isn't being split by line and is of course too big to be processed in one batch, which throws the error, but i don't understand why it doesn't get split correctly. i am using the default and . any insight on what i could be doing wrong would be greatly appreciated.",
        "Answer_original_content":"i assume this is a duplicate of this aws forum post: https:\/\/forums.aws.amazon.com\/thread.jspa?threadid=303810&tstart=0 anyway, for completeness i'll answer here as well. the issue is that you are serializing your dataset incorrectly when converting it into jsonlines: test_x_list = test_x.tolist() ... with jsonlines.open(file_path, 'w') as writer: writer.write(test_x_list) what the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume. i suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset: test_x_list = test_x.tolist() ... with jsonlines.open(file_path, 'w') as writer: for sample in test_x_list: writer.write(sample) if one sample at a time is too slow you can also play around with the max_concurrent_transforms, strategy, and max_payload parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. see https:\/\/.readthedocs.io\/en\/latest\/transformer.html and https:\/\/docs.aws.amazon.com\/\/latest\/dg\/api_createtransformjob.html for additional detail on what these parameters do.",
        "Answer_original_content_gpt_summary":"The solution to the \"internalservererror\" encountered when attempting to perform a batch transformation job with a data file larger than 100mb is to change the code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset. If one sample at a time is too slow, the user can also play around with the max_concurrent_transforms, strategy, and max_payload parameters to be able to batch the data as well as run concurrent transforms if their algorithm can run in parallel. Additionally, the user can split the data into multiple files and run transformations with more than just one node.",
        "Answer_preprocessed_content":"i assume this is a duplicate of this aws forum post anyway, for completeness i'll answer here as well. the issue is that you are serializing your dataset incorrectly when converting it into jsonlines what the above is doing is creating a very large containing your full dataset which is too big for single inference call to consume. i suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset if one sample at a time is too slow you can also play around with the , , and parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel also, of course, you can split the data into multiple files and run transformations with more than just one node. see and for additional detail on what these parameters do."
    },
    {
        "Question_id":null,
        "Question_title":"AWS Ground Truth Plus Available Medical Expertise",
        "Question_body":"On Dec 1, 2021, AWS put out a press release regarding SageMaker Ground Truth Plus that contained the statement:\n\nTo get started, customers simply point Amazon SageMaker Ground Truth Plus to their data source in Amazon Simple Storage Service (Amazon S3) and provide their specific labeling requirements (e.g. instructions for how medical experts should label anomalies in radiology images of lungs).\n\nCan AWS provide medical experts for labeling medical data? Or am I misinterpreting this statement and the services included in this \"turnkey\" solution. (BTW, I've already built and tested a custom segmentation task for SageMaker Ground Truth and am looking for \"expert\" labeling.)",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1668530819224,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":28.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1vINii7DRkGK-4klJOq7wA\/aws-ground-truth-plus-available-medical-expertise",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-15T19:01:33.076Z",
                "Answer_score":0,
                "Answer_body":"That's a poor choice of example on their part; the FAQ for the service explicitly states: \"Currently, Amazon SageMaker Ground Truth Plus is not a HIPAA eligible service.\"",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: aws ground truth plus available medical expertise; Content: on dec 1, 2021, aws put out a press release regarding ground truth plus that contained the statement: to get started, customers simply point ground truth plus to their data source in amazon simple storage service (amazon s3) and provide their specific labeling requirements (e.g. instructions for how medical experts should label anomalies in radiology images of lungs). can aws provide medical experts for labeling medical data? or am i misinterpreting this statement and the services included in this \"turnkey\" solution. (btw, i've already built and tested a custom segmentation task for ground truth and am looking for \"expert\" labeling.)",
        "Question_original_content_gpt_summary":"The user is questioning whether AWS can provide medical experts for labeling medical data as part of their \"turnkey\" Ground Truth Plus solution.",
        "Question_preprocessed_content":"Title: aws ground truth plus available medical expertise; Content: on dec , , aws put out a press release regarding ground truth plus that contained the statement to get started, customers simply point ground truth plus to their data source in amazon simple storage service and provide their specific labeling requirements . can aws provide medical experts for labeling medical data? or am i misinterpreting this statement and the services included in this turnkey solution.",
        "Answer_original_content":"that's a poor choice of example on their part; the faq for the service explicitly states: \"currently, ground truth plus is not a hipaa eligible service.\"",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer to the user's question about whether AWS can provide medical experts for labeling medical data as part of their \"turnkey\" Ground Truth Plus solution. The answer only points out that the example given by the user is not relevant as the service is not HIPAA eligible.",
        "Answer_preprocessed_content":"that's a poor choice of example on their part; the faq for the service explicitly states currently, ground truth plus is not a hipaa eligible"
    },
    {
        "Question_id":null,
        "Question_title":"Easiest way to load the best model checkpoint after training w\/ pytorch lightning",
        "Question_body":"<p>I have a notebook based on <a href=\"http:\/\/wandb.me\/lit-colab\" rel=\"noopener nofollow ugc\"> Supercharge your Training with PyTorch Lightning + Weights &amp; Biases<\/a> and I\u2019m wondering what the easiest approach to load a model with the best checkpoint after training finishes.<\/p>\n<p>I\u2019m assuming that after training the \u201cmodel\u201d instance will just have the weights of the most recent epoch, which might not be the most accurate model (in case it started overfitting etc).<\/p>\n<p>Specifically I was looking for an easy way to get the directory where the checkpoints artifacts are stored, which in my case look like this: <code>.\/MnistKaggle\/1vzsgin6\/checkpoints<\/code>, where <code>1vzsgin6<\/code> is the run id auto-generated by wandb.<\/p>\n<p>One (clunky) way to do it would be:<\/p>\n<pre><code class=\"lang-auto\">wandb_logger = WandbLogger(project=\"MnistKaggle\")\ncheckpoint_dir_path = None\n\ndef my_after_save_checkpoint(checkpoint):\n  checkpoint_dir_path = checkpoint.dirpath\n\nwandb_logger.after_save_checkpoint = my_after_save_checkpoint\n\n# Now find the checkpoint file in the checkpoint_dir_path directory and load the model from that.\n<\/code><\/pre>\n<p>Is there an easier way?  I was sorta expecting the <code>WandbLogger<\/code> object to have an easy method like <code>get_save_checkpoint_dirpath()<\/code>, but I\u2019m not seeing anything.<\/p>\n<p>Thanks in advance for any help!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1667348143889,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":357.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/easiest-way-to-load-the-best-model-checkpoint-after-training-w-pytorch-lightning\/3365",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-02T23:00:11.968Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tleyden\">@tleyden<\/a> , happy to help. Please review the following <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/extensions\/generated\/pytorch_lightning.loggers.WandbLogger.html#:~:text=(model)-,Log%20model%20checkpoints,-Log%20model%20checkpoints\" rel=\"noopener nofollow ugc\">resource<\/a> on model checkpointing and retrieval.<\/p>\n<p>A common flow would be to log a model checkpoint as in the example then to also log a \u201cbest model\u201d artifact. Since artifacts are versioned you don\u2019t have to worry about renaming the new \u201cbest model\u201d artifact. Then at the end of your run you not only have an artifact history of your model at each of the checkpoints but also a versioned history of all the best models.<\/p>",
                "Answer_score":45.4,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-11-07T18:37:27.390Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tleyden\">@tleyden<\/a>  since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-07T19:00:32.227Z",
                "Answer_body":"<p>Thanks for the tip about the \u201clatest\/best\u201d aliases, I hadn\u2019t seen that.  So if I understand correctly, this would be downloading the model checkpoint locally via the API - which is somewhat redundant since I assume it\u2019s already saved locally, but it provides more control in terms of being able to specify those aliases.<\/p>",
                "Answer_score":10.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-06T19:00:41.754Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":15.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: easiest way to load the best model checkpoint after training w\/ pytorch lightning; Content: i have a notebook based on supercharge your training with pytorch lightning + and i\u2019m wondering what the easiest approach to load a model with the best checkpoint after training finishes. i\u2019m assuming that after training the \u201cmodel\u201d instance will just have the weights of the most recent epoch, which might not be the most accurate model (in case it started overfitting etc). specifically i was looking for an easy way to get the directory where the checkpoints artifacts are stored, which in my case look like this: .\/mnistkaggle\/1vzsgin6\/checkpoints, where 1vzsgin6 is the run id auto-generated by . one (clunky) way to do it would be: _logger = logger(project=\"mnistkaggle\") checkpoint_dir_path = none def my_after_save_checkpoint(checkpoint): checkpoint_dir_path = checkpoint.dirpath _logger.after_save_checkpoint = my_after_save_checkpoint # now find the checkpoint file in the checkpoint_dir_path directory and load the model from that. is there an easier way? i was sorta expecting the logger object to have an easy method like get_save_checkpoint_dirpath(), but i\u2019m not seeing anything. thanks in advance for any help!",
        "Question_original_content_gpt_summary":"The user is looking for the easiest way to load the best model checkpoint after training with PyTorch Lightning, and is wondering if there is an easier way than manually searching for the directory where the checkpoints artifacts are stored.",
        "Question_preprocessed_content":"Title: easiest way to load the best model checkpoint after training w\/ pytorch lightning; Content: i have a notebook based on supercharge your training with pytorch lightning + and im wondering what the easiest approach to load a model with the best checkpoint after training finishes. im assuming that after training the model instance will just have the weights of the most recent epoch, which might not be the most accurate model . specifically i was looking for an easy way to get the directory where the checkpoints artifacts are stored, which in my case look like this , where is the run id by . one way to do it would be is there an easier way? i was sorta expecting the object to have an easy method like , but im not seeing anything. thanks in advance for any help!",
        "Answer_original_content":"hi @tleyden , happy to help. please review the following resource on model checkpointing and retrieval. a common flow would be to log a model checkpoint as in the example then to also log a best model artifact. since artifacts are versioned you dont have to worry about renaming the new best model artifact. then at the end of your run you not only have an artifact history of your model at each of the checkpoints but also a versioned history of all the best models.",
        "Answer_original_content_gpt_summary":"The answer suggests that the user can log a model checkpoint and a best model artifact during training with PyTorch Lightning. This will create a versioned history of all the best models, making it easier to retrieve the best model checkpoint without manually searching for the directory where the checkpoints artifacts are stored.",
        "Answer_preprocessed_content":"hi , happy to help. please review the following resource on model checkpointing and retrieval. a common flow would be to log a model checkpoint as in the example then to also log a best model artifact. since artifacts are versioned you dont have to worry about renaming the new best model artifact. then at the end of your run you not only have an artifact history of your model at each of the checkpoints but also a versioned history of all the best models."
    },
    {
        "Question_id":null,
        "Question_title":"Custom HTML with three.js for interactive 3D asset visualization",
        "Question_body":"<p>Hi, I am working on a 3D reconstruction task that returns multiple 3D meshes in my visualization. The custom W&amp;B 3D visualization is too bright and it is not helpful for tasks that require to visualize texture; also, it seems to reduce the triangle counts, but triangle counts are important for 3D vision.<\/p>\n<p>It seems that W&amp;B supports uploading custom HTML.  I am wondering if it is possible to:<\/p>\n<ol>\n<li>Upload my predicted 3D meshes for each step.<\/li>\n<li>Display my 3D meshes using a custom HTML that using <a href=\"https:\/\/threejs.org\/examples\/#webgl_animation_keyframes\" rel=\"noopener nofollow ugc\">three.js<\/a> to allow me interact with it.<\/li>\n<\/ol>\n<p>Potential blockers:<\/p>\n<ul>\n<li>I assume <code>wandb.Object3D<\/code> is used with <code>wandb.log<\/code> to upload my 3D prediction. In my HTML, how do I get access to the uploaded meshes?<\/li>\n<li>Similarly, how do I upload and get access to a js library in the HTML? (the paths)<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1650291491993,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":297.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/custom-html-with-three-js-for-interactive-3d-asset-visualization\/2258",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-01T14:29:04.480Z",
                "Answer_body":"<p>Alosm curious about this.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-30T14:29:55.774Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: custom html with three.js for interactive 3d asset visualization; Content: hi, i am working on a 3d reconstruction task that returns multiple 3d meshes in my visualization. the custom w&b 3d visualization is too bright and it is not helpful for tasks that require to visualize texture; also, it seems to reduce the triangle counts, but triangle counts are important for 3d vision. it seems that w&b supports uploading custom html. i am wondering if it is possible to: upload my predicted 3d meshes for each step. display my 3d meshes using a custom html that using three.js to allow me interact with it. potential blockers: i assume .object3d is used with .log to upload my 3d prediction. in my html, how do i get access to the uploaded meshes? similarly, how do i upload and get access to a js library in the html? (the paths)",
        "Question_original_content_gpt_summary":"The user is facing challenges with customizing their 3D asset visualization with three.js, including accessing uploaded meshes and uploading and accessing a JS library in the HTML.",
        "Question_preprocessed_content":"Title: custom html with for interactive d asset visualization; Content: hi, i am working on a d reconstruction task that returns multiple d meshes in my visualization. the custom w&b d visualization is too bright and it is not helpful for tasks that require to visualize texture; also, it seems to reduce the triangle counts, but triangle counts are important for d vision. it seems that w&b supports uploading custom html. i am wondering if it is possible to upload my predicted d meshes for each step. display my d meshes using a custom html that using to allow me interact with it. potential blockers i assume is used with to upload my d prediction. in my html, how do i get access to the uploaded meshes? similarly, how do i upload and get access to a js library in the html?",
        "Answer_original_content":"alosm curious about this. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer as it seems to be a response from a forum moderator indicating that the thread has been closed due to inactivity.",
        "Answer_preprocessed_content":"alosm curious about this. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Running wandb server behind a jupyterhub proxy",
        "Question_body":"<p>i\u2019m trying to proxy the wandb \u201clocal\u201d server that runs under docker behind jupyterhub. (Playing with an on-prem install to do labs in the new full stack-dl course). Jupyterhub has a proxy server which lets one obtain an app at an arbitrary port show up at a given url.  The wandb server wants a particular url, but to be behind jupyterhub i need the base url for the wandb server to be something like <a href=\"http:\/\/hostname\/hub\/user-redirect\/proxy\/8080\" rel=\"noopener nofollow ugc\">http:\/\/hostname\/hub\/user-redirect\/proxy\/8080<\/a>. Is it possible to set a base url as an option in the docker container\/arguments to the local web server in the container there?<\/p>",
        "Question_answer_count":13,
        "Question_comment_count":null,
        "Question_creation_time":1660524858007,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":134.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/running-wandb-server-behind-a-jupyterhub-proxy\/2930",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-17T19:38:35.091Z",
                "Answer_body":"<p>Hi Rahul!<\/p>\n<p>You can set a port on wandb local as <code>wandb local start --port PORT<\/code> in order to set a port for your local instance. Does that fulfill your usecase?<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-23T04:56:48.724Z",
                "Answer_body":"<p>Hi Rahul,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-26T18:10:34.242Z",
                "Answer_body":"<p>Hi Rahul, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-30T13:06:16.160Z",
                "Answer_body":"<p>I actually do this\u2026the problem is not in the port. I can even get to the port but the html wont form. This is because the wandb server hardcodes (i think) the URLS in the docker server. So I was wondering if there was a variable such as <code>baseurl<\/code> that i can supply to the container (possibly rebuilding it) which will let me provide a prefix-url to the wandb server.  Now whatever leading string the jupyterhub proxy provides in the URL can be incorporated<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-01T17:00:22.700Z",
                "Answer_body":"<p>Hi Ramit,<\/p>\n<p>Any thoughts? Where I am trying to get this to work (a healthcare company) has strict data egress restrictions, and proxying through jupyterhub rather than exposing the port is what we need. Basically i need a way to change the base URL on the wandb server of this is at all possible\u2026<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-01T21:27:22.416Z",
                "Answer_body":"<p>Not sure exactly what your issue is, but I had a similar issue when I was testing out self-hosting. I was using a SOCKS proxy and while I could connect to the server I couldn\u2019t login as I kept getting stuck in a retry loop. What did work for me was using SSH port forwarding, but I ended up not self hosting in the end, so I never found a proper solution.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-02T14:54:23.565Z",
                "Answer_body":"<p>Its worse for me, the CSS and JS wont load. The wandb server is expecting a particular URL, but because jupyter-server-proxy adds a <a href=\"https:\/\/jupyterhubhost\/user\/proxy\/PORTNUM\/\" rel=\"noopener nofollow ugc\">https:\/\/jupyterhubhost\/user\/proxy\/PORTNUM\/<\/a> to the beginning of the URLS, and wandb server expects an absolute url (i think) nothing is properly visible\u2026<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-02T17:57:24.239Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/rahuldave\">@rahuldave<\/a>,<\/p>\n<p>Apologies about the delay here, you would want to start your server like this in that case:<\/p>\n<pre><code class=\"lang-auto\">wandb server start --env HOST=&lt;HOST&gt;:&lt;PORT&gt;\n<\/code><\/pre>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":10.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-06T00:25:01.695Z",
                "Answer_body":"<p>Not sure I made my use case clear: i am not trying to start wandb server at a particular port. That proxying is taken care of by jupyterhub. What i am trying to do iis to have the base URL of the server something like: \u201c<a href=\"https:\/\/jupyterhubhost\/user\/proxy\/PORTNUM\/\" rel=\"noopener nofollow ugc\">https:\/\/jupyterhubhost\/user\/proxy\/PORTNUM\/<\/a>\u201d because thats what jupyter gives me. The internal linlks in wandb server are expecting the top level to be \u201c\/js\u201d or \u201c\/css\u201d for example, but i am stuck with the base url of the form \u201c\/user\/proxy\/PORTNUM\/\u201d so that the css url for example is \u201c\/user\/proxy\/PORTNUM\/css\u201d<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-15T20:54:26.454Z",
                "Answer_body":"<p>Hey Rahul,<\/p>\n<p>The HOST I specified on the previous comment is the base URL for the server. Is there any errors\/odd behaviour you see in particular when running this command? It would be great if you could share a few screenshots \/ a screen recording so that we can understand what\u2019s happening on your machine.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-21T09:34:31.493Z",
                "Answer_body":"<p>Hi Rahul,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-26T19:43:43.145Z",
                "Answer_body":"<p>Hi Rahul, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-14T20:55:14.384Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: running server behind a jupyterhub proxy; Content: i\u2019m trying to proxy the \u201clocal\u201d server that runs under docker behind jupyterhub. (playing with an on-prem install to do labs in the new full stack-dl course). jupyterhub has a proxy server which lets one obtain an app at an arbitrary port show up at a given url. the server wants a particular url, but to be behind jupyterhub i need the base url for the server to be something like http:\/\/hostname\/hub\/user-redirect\/proxy\/8080. is it possible to set a base url as an option in the docker container\/arguments to the local web server in the container there?",
        "Question_original_content_gpt_summary":"The user is trying to proxy a \"local\" server running under Docker behind JupyterHub, but is having difficulty setting the base URL for the server to be http:\/\/hostname\/hub\/user-redirect\/proxy\/8080.",
        "Question_preprocessed_content":"Title: running server behind a jupyterhub proxy; Content: im trying to proxy the local server that runs under docker behind jupyterhub. . jupyterhub has a proxy server which lets one obtain an app at an arbitrary port show up at a given url. the server wants a particular url, but to be behind jupyterhub i need the base url for the server to be something like is it possible to set a base url as an option in the docker to the local web server in the container there?",
        "Answer_original_content":"hi rahul! you can set a port on local as local start --port port in order to set a port for your local instance. does that fulfill your usecase? thanks, ramit hi rahul, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi rahul, since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! i actually do thisthe problem is not in the port. i can even get to the port but the html wont form. this is because the server hardcodes (i think) the urls in the docker server. so i was wondering if there was a variable such as baseurl that i can supply to the container (possibly rebuilding it) which will let me provide a prefix-url to the server. now whatever leading string the jupyterhub proxy provides in the url can be incorporated hi ramit, any thoughts? where i am trying to get this to work (a healthcare company) has strict data egress restrictions, and proxying through jupyterhub rather than exposing the port is what we need. basically i need a way to change the base url on the server of this is at all possible not sure exactly what your issue is, but i had a similar issue when i was testing out self-hosting. i was using a socks proxy and while i could connect to the server i couldnt login as i kept getting stuck in a retry loop. what did work for me was using ssh port forwarding, but i ended up not self hosting in the end, so i never found a proper solution. its worse for me, the css and js wont load. the server is expecting a particular url, but because jupyter-server-proxy adds a https:\/\/jupyterhubhost\/user\/proxy\/portnum\/ to the beginning of the urls, and server expects an absolute url (i think) nothing is properly visible hey @rahuldave, apologies about the delay here, you would want to start your server like this in that case: server start --env host=<host>:<port> thanks, ramit not sure i made my use case clear: i am not trying to start server at a particular port. that proxying is taken care of by jupyterhub. what i am trying to do iis to have the base url of the server something like: https:\/\/jupyterhubhost\/user\/proxy\/portnum\/ because thats what jupyter gives me. the internal linlks in server are expecting the top level to be \/js or \/css for example, but i am stuck with the base url of the form \/user\/proxy\/portnum\/ so that the css url for example is \/user\/proxy\/portnum\/css hey rahul, the host i specified on the previous comment is the base url for the server. is there any errors\/odd behaviour you see in particular when running this command? it would be great if you could share a few screenshots \/ a screen recording so that we can understand whats happening on your machine. thanks, ramit hi rahul, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi rahul, since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no clear solutions provided in the answer. The user is having difficulty setting the base URL for a server running under Docker behind JupyterHub. They are looking for a way to change the base URL on the server, possibly by supplying a variable such as baseurl to the container. The support team suggests starting the server with a specific host and port, but this does not address the user's issue. The support team also requests screenshots or a screen recording to better understand the problem. However, the conversation ends without a clear resolution.",
        "Answer_preprocessed_content":"hi rahul! you can set a port on local as in order to set a port for your local instance. does that fulfill your usecase? thanks, ramit hi rahul, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi rahul, since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! i actually do thisthe problem is not in the port. i can even get to the port but the html wont form. this is because the server hardcodes the urls in the docker server. so i was wondering if there was a variable such as that i can supply to the container which will let me provide a to the server. now whatever leading string the jupyterhub proxy provides in the url can be incorporated hi ramit, any thoughts? where i am trying to get this to work has strict data egress restrictions, and proxying through jupyterhub rather than exposing the port is what we need. basically i need a way to change the base url on the server of this is at all possible not sure exactly what your issue is, but i had a similar issue when i was testing out i was using a socks proxy and while i could connect to the server i couldnt login as i kept getting stuck in a retry loop. what did work for me was using ssh port forwarding, but i ended up not self hosting in the end, so i never found a proper solution. its worse for me, the css and js wont load. the server is expecting a particular url, but because adds a to the beginning of the urls, and server expects an absolute url nothing is properly visible hey apologies about the delay here, you would want to start your server like this in that case thanks, ramit not sure i made my use case clear i am not trying to start server at a particular port. that proxying is taken care of by jupyterhub. what i am trying to do iis to have the base url of the server something like because thats what jupyter gives me. the internal linlks in server are expecting the top level to be or for example, but i am stuck with the base url of the form so that the css url for example is hey rahul, the host i specified on the previous comment is the base url for the server. is there any behaviour you see in particular when running this command? it would be great if you could share a few screenshots \/ a screen recording so that we can understand whats happening on your machine. thanks, ramit hi rahul, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi rahul, since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":60637170.0,
        "Question_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Question_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2.0,
        "Question_creation_time":1583933260433,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":1681.0,
        "Owner_creation_time":1360655430743,
        "Owner_last_access_time":1663784892907,
        "Owner_reputation":2947.0,
        "Owner_up_votes":297.0,
        "Owner_down_votes":16.0,
        "Owner_views":355.0,
        "Answer_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1584005785480,
        "Answer_score":-2.0,
        "Owner_location":"Belgium",
        "Question_last_edit_time":1584005920356,
        "Answer_last_edit_time":1584011988083,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to pass arguments to scoring file when deploying a model in ; Content: i am deploying a trained model to an aci endpoint on , using the python sdk. i have created my score.py file, but i would like that file to be called with an argument being passed (just like with a training file) that i can interpret using argparse. however, i don't seem to find how i can pass arguments this is the code i have to create the inferenceconfig environment and which obviously does not work. should i fall back on using the extra docker file steps or so? from .core.conda_dependencies import condadependencies from .core.environment import environment from .core.model import inferenceconfig env = environment('my_hosted_environment') env.python.conda_dependencies = condadependencies.create( conda_packages=['scikit-learn'], pip_packages=['-defaults']) scoring_script = 'score.py --model_name ' + model_name inference_config = inferenceconfig(entry_script=scoring_script, environment=env) adding the score.py for reference on how i'd love to use the arguments in that script: #removed imports import argparse def init(): global model parser = argparse.argumentparser(description=\"load sklearn model\") parser.add_argument('--model_name', dest=\"model_name\", required=true) args, _ = parser.parse_known_args() model_path = model.get_model_path(model_name=args.model_name) model = joblib.load(model_path) def run(raw_data): try: data = json.loads(raw_data)['data'] data = np.array(data) result = model.predict(data) return result.tolist() except exception as e: result = str(e) return result interested to hear your thoughts",
        "Question_original_content_gpt_summary":"The user is encountering challenges in passing arguments to a scoring file when deploying a model in Azure Machine Learning.",
        "Question_preprocessed_content":"Title: how to pass arguments to scoring file when deploying a model in ; Content: i am deploying a trained model to an aci endpoint on , using the python sdk. i have created my file, but i would like that file to be called with an argument being passed that i can interpret using . however, i don't seem to find how i can pass arguments this is the code i have to create the inferenceconfig environment and which obviously does not work. should i fall back on using the extra docker file steps or so? adding the for reference on how i'd love to use the arguments in that script interested to hear your thoughts",
        "Answer_original_content":"how to deploy using environments can be found here model-register-and-deploy.ipynb . inferenceconfig class accepts source_directory and entry_script parameters, where source_directory is a path to the folder that contains all files(score.py and any other additional files) to create the image. this multi-model-register-and-deploy.ipynb has code snippets on how to create inferenceconfig with source_directory and entry_script. from .core.webservice import webservice from .core.model import inferenceconfig from .core.environment import environment myenv = environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\") inference_config = inferenceconfig(entry_script=\"score.py\", environment=myenv) service = model.deploy(workspace=ws, name='sklearn-mnist-svc', models=[model], inference_config=inference_config, deployment_config=aciconfig) service.wait_for_deployment(show_output=true) print(service.scoring_uri)",
        "Answer_original_content_gpt_summary":"The answer suggests that the user can deploy using environments and provides a link to a notebook with instructions. It also mentions the use of the inferenceconfig class, which accepts source_directory and entry_script parameters. The answer provides a link to another notebook with code snippets on how to create inferenceconfig with source_directory and entry_script. Additionally, the answer includes code snippets for creating an environment and deploying the model.",
        "Answer_preprocessed_content":"how to deploy using environments can be found here . inferenceconfig class accepts and parameters, where is a path to the folder that contains all and any other additional files to create the image. this has code snippets on how to create inferenceconfig with and"
    },
    {
        "Question_id":73133746.0,
        "Question_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Question_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1658906440657,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":51.0,
        "Owner_creation_time":1658906023852,
        "Owner_last_access_time":1663921457750,
        "Owner_reputation":152.0,
        "Owner_up_votes":15.0,
        "Owner_down_votes":1.0,
        "Owner_views":11.0,
        "Answer_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1658964743500,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1660220920907,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: fb-prophet, apache spark in colab and \/ lambda; Content: i am using google-colab for creating a model by using fbprophet and i am try to use apache spark in the google-colab itself. now can i upload this google-colab notebook in \/lambda for free (without charge for apache spark and only charge for )?",
        "Question_original_content_gpt_summary":"The user is attempting to use Google Colab to create a model with fbprophet and Apache Spark, and is wondering if they can upload the Colab notebook to Lambda for free without incurring charges for Apache Spark.",
        "Question_preprocessed_content":"Title: apache spark in colab and \/ lambda; Content: i am using for creating a model by using fbprophet and i am try to use apache spark in the itself. now can i upload this notebook in for free ?",
        "Answer_original_content":"in short, you can upload the notebook without any issue into . few things to keep in mind if you are using the pyspark library in colab and running spark locally, you should be able to do the same by installing necessary pyspark libs in studio kernels. here you will only pay for the underlying compute for the notebook instance. if you are experimenting then i would recommend you to use https:\/\/studiolab..aws\/ to create a free account and try things out. if you had a separate spark cluster setup then you may need a similar setup in aws using emr so that you can connect to the cluster to execute the job.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n- You can upload the Colab notebook to Lambda for free without any issue.\n- If you are using the pyspark library in Colab and running Spark locally, you can do the same by installing necessary pyspark libs in Studio kernels. Here you will only pay for the underlying compute for the notebook instance.\n- If you are experimenting, it is recommended to use https:\/\/studiolab.aws\/ to create a free account and try things out.\n- If you had a separate Spark cluster setup, then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.",
        "Answer_preprocessed_content":"in short, you can upload the notebook without any issue into . few things to keep in mind if you are using the pyspark library in colab and running spark locally, you should be able to do the same by installing necessary pyspark libs in studio kernels. here you will only pay for the underlying compute for the notebook instance. if you are experimenting then i would recommend you to use to create a free account and try things out. if you had a separate spark cluster setup then you may need a similar setup in aws using emr so that you can connect to the cluster to execute the job."
    },
    {
        "Question_id":72073763.0,
        "Question_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Question_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5.0,
        "Question_creation_time":1651375335713,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":358.0,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583236,
        "Owner_reputation":14749.0,
        "Owner_up_votes":641.0,
        "Owner_down_votes":62.0,
        "Owner_views":968.0,
        "Answer_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652802212843,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: gcp vertex pipeline - why kfp.v2.dsl.output as function arguments work without being provided?; Content: why kfp.v2.dsl.output as function argument works without being provided? i am following create and run ml pipelines with vertex pipelines! jupyter notebook example from gcp. the function classif_model_eval_metrics takes metrics: output[metrics] and metricsc: output[classificationmetrics] which have no default values. @component( base_image=\"gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest\", output_component_file=\"tables_eval_component.yaml\", # optional: you can use this to load the component later packages_to_install=[\"google-cloud-aiplatform\"], ) def classif_model_eval_metrics( project: str, location: str, # \"us-central1\", api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\", thresholds_dict_str: str, model: input[model], metrics: output[metrics], # no default value set, hence must be mandatory metricsc: output[classificationmetrics], # no default value set, hence must be mandatory ) -> namedtuple(\"outputs\", [(\"dep_decision\", str)]): # full code at the bottom. hence those arguments should be mandatory, but the function is called without those arguments. model_eval_task = classif_model_eval_metrics( project, gcp_region, api_endpoint, thresholds_dict_str, training_op.outputs[\"model\"], # <--- no arguments for ``metrics: output[metrics]``` and ```metricsc: output[classificationmetrics]``` ) the entire pipeline code is below. @kfp.dsl.pipeline(name=\"automl-tab-beans-training-v2\", pipeline_root=pipeline_root) def pipeline( bq_source: str = \"bq:\/\/aju-dev-demos.beans.beans1\", display_name: str = display_name, project: str = project_id, gcp_region: str = \"us-central1\", api_endpoint: str = \"us-central1-aiplatform.googleapis.com\", thresholds_dict_str: str = '{\"auroc\": 0.95}', ): dataset_create_op = gcc_aip.tabulardatasetcreateop( project=project, display_name=display_name, bq_source=bq_source ) training_op = gcc_aip.automltabulartrainingjobrunop( project=project, display_name=display_name, optimization_prediction_type=\"classification\", budget_milli_node_hours=1000, column_transformations=columns, dataset=dataset_create_op.outputs[\"dataset\"], target_column=\"class\", ) model_eval_task = classif_model_eval_metrics( project, gcp_region, api_endpoint, thresholds_dict_str, training_op.outputs[\"model\"], # <--- no arguments for ``metrics: output[metrics]``` and ```metricsc: output[classificationmetrics]``` ) why does it work and what are metrics: output[metrics] and metricsc: output[classificationmetrics] of type kfp.v2.dsl.output? classif_model_eval_metrics function code from kfp.v2.dsl import ( dataset, model, output, input, outputpath, classificationmetrics, metrics, component ) @component( base_image=\"gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest\", output_component_file=\"tables_eval_component.yaml\", # optional: you can use this to load the component later packages_to_install=[\"google-cloud-aiplatform\"], ) def classif_model_eval_metrics( project: str, location: str, # \"us-central1\", api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\", thresholds_dict_str: str, model: input[model], metrics: output[metrics], metricsc: output[classificationmetrics], ) -> namedtuple(\"outputs\", [(\"dep_decision\", str)]): # return parameter. \"\"\"renders evaluation metrics for an automl tabular classification model. retrieves the classification model evaluation and render the roc and confusion matrix for the model. determine whether the model is sufficiently accurate to deploy. \"\"\" import json import logging from google.cloud import aiplatform # fetch model eval info def get_eval_info(client, model_name): from google.protobuf.json_format import messagetodict response = client.list_model_evaluations(parent=model_name) metrics_list = [] metrics_string_list = [] for evaluation in response: metrics = messagetodict(evaluation._pb.metrics) metrics_str = json.dumps(metrics) metrics_list.append(metrics) metrics_string_list.append(metrics_str) return ( evaluation.name, metrics_list, metrics_string_list, ) def classification_thresholds_check(metrics_dict, thresholds_dict): for k, v in thresholds_dict.items(): if k in [\"auroc\", \"auprc\"]: # higher is better if metrics_dict[k] < v: # if under threshold, don't deploy return false return true def log_metrics(metrics_list, metricsc): test_confusion_matrix = metrics_list[0][\"confusionmatrix\"] logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"]) # log the roc curve fpr = [], tpr = [], thresholds = [] for item in metrics_list[0][\"confidencemetrics\"]: fpr.append(item.get(\"falsepositiverate\", 0.0)) tpr.append(item.get(\"recall\", 0.0)) thresholds.append(item.get(\"confidencethreshold\", 0.0)) metricsc.log_roc_curve(fpr, tpr, thresholds) # log the confusion matrix annotations = [] for item in test_confusion_matrix[\"annotationspecs\"]: annotations.append(item[\"displayname\"]) metricsc.log_confusion_matrix( annotations, test_confusion_matrix[\"rows\"], ) # log textual metrics info as well for metric in metrics_list[0].keys(): if metric != \"confidencemetrics\": val_string = json.dumps(metrics_list[0][metric]) metrics.log_metric(metric, val_string) # metrics.metadata[\"model_type\"] = \"automl tabular classification\" aiplatform.init(project=project) client = aiplatform.gapic.modelserviceclient(client_options={\"api_endpoint\": api_endpoint}) eval_name, metrics_list, metrics_str_list = get_eval_info( client, model.uri.replace(\"aiplatform:\/\/v1\/\", \"\") ) log_metrics(metrics_list, metricsc) thresholds_dict = json.loads(thresholds_dict_str) return (\"true\",) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (\"false\", )",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with the gcp vertex pipeline, where the function classif_model_eval_metrics takes arguments of type kfp.v2.dsl.output as function arguments, yet the function is called without those arguments.",
        "Question_preprocessed_content":"Title: gcp vertex pipeline why as function arguments work without being provided?; Content: why as function argument works without being provided? i am following create and run ml pipelines with vertex pipelines! jupyter notebook example from gcp. the function takes and which have no default values. hence those arguments should be mandatory, but the function is called without those arguments. the entire pipeline code is below. why does it work and what are and of type ? function code",
        "Answer_original_content":"the custom component is defined as a python function with a @kfp.v2.dsl.component decorator. the @component decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification. the component function, classif_model_eval_metrics, has some input parameters. the model parameter is an input kfp.v2.dsl.model artifact. the two function args, metrics and metricsc, are component outputs, in this case of types metrics and classificationmetrics. theyre not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component. @component( base_image=\"gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest\", output_component_file=\"tables_eval_component.yaml\", packages_to_install=[\"google-cloud-aiplatform\"], ) def classif_model_eval_metrics( project: str, location: str, # \"us-central1\", api_endpoint: str, # \"us-central1-aiplatform.googleapis.com\", thresholds_dict_str: str, model: input[model], metrics: output[metrics], metricsc: output[classificationmetrics], ) for example, in the function below, were calling metricsc.log_roc_curve() and metricsc.log_confusion_matrix() to render these visualizations in the pipelines ui. these output params become component outputs when the component is compiled, and can be consumed by other pipeline steps. def log_metrics(metrics_list, metricsc): ... metricsc.log_roc_curve(fpr, tpr, thresholds) ... metricsc.log_confusion_matrix( annotations, test_confusion_matrix[\"rows\"], ) for more information you can refer to this document.",
        "Answer_original_content_gpt_summary":"The solution to the challenge with the gcp vertex pipeline is to ensure that the function classif_model_eval_metrics takes arguments of type kfp.v2.dsl.output as function arguments. The custom component is defined as a python function with a @kfp.v2.dsl.component decorator, which specifies three optional arguments: the base container image to use, any packages to install, and the yaml file to which to write the component specification. The component function, classif_model_eval_metrics, has some input parameters, and the two function args, metrics and metricsc, are component outputs, which can be used in the component. These output params become component outputs when the component is compiled and can be consumed by other pipeline steps.",
        "Answer_preprocessed_content":"the custom component is defined as a python function with a decorator. the decorator specifies three optional arguments the base container image to use; any packages to install; and the yaml file to which to write the component specification. the component function, , has some input parameters. the model parameter is an input . the two function args, and , are component outputs, in this case of types metrics and classificationmetrics. theyre not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component. for example, in the function below, were calling and to render these visualizations in the pipelines ui. these output params become component outputs when the component is compiled, and can be consumed by other pipeline steps. for more information you can refer to this document."
    },
    {
        "Question_id":67186515.0,
        "Question_title":"Installing modules inside python .py file",
        "Question_body":"<p>I am deploying a custom pytorch model on AWS sagemaker, Following <a href=\"https:\/\/github.com\/abiodunjames\/MachineLearning\/blob\/master\/DeployYourModelToSageMaker\/inference.py\" rel=\"nofollow noreferrer\">this<\/a> tutorial.\nIn my case I have few dependencies to install some modules.<\/p>\n<p>I need pycocotools in my inference.py script. I can easily install pycocotool inside a separate notebook using this bash command,<\/p>\n<p><code>%%bash<\/code><\/p>\n<p><code>pip -g install pycocotools<\/code><\/p>\n<p>But when I create my endpoint for deployment, I get this error that pycocotools in not defined.\nI need pycocotools inside my inference.py script. How I can install this inside a .py file<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1618953881933,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":167.0,
        "Owner_creation_time":1567880532003,
        "Owner_last_access_time":1661706476487,
        "Owner_reputation":137.0,
        "Owner_up_votes":22.0,
        "Owner_down_votes":0.0,
        "Owner_views":100.0,
        "Answer_body":"<p>At the beginning of inference.py add these lines:<\/p>\n<pre><code>from subprocess import check_call, run, CalledProcessError\nimport sys\nimport os\n\n# Since it is likely that you're going to run inference.py multiple times, this avoids reinstalling the same package:\nif not os.environ.get(&quot;INSTALL_SUCCESS&quot;):\n    \n    try:\n        check_call(\n        [ sys.executable, &quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    except CalledProcessError:\n        run(\n        [&quot;pip&quot;, &quot;install&quot;, &quot;pycocotools&quot;,]\n        )\n    os.environ[&quot;INSTALL_SUCCESS&quot;] = &quot;True&quot;\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1618965088403,
        "Answer_score":1.0,
        "Owner_location":"Lahore, Pakistan",
        "Question_last_edit_time":1618961452447,
        "Answer_last_edit_time":1619882971870,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67186515",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: installing modules inside python .py file; Content: i am deploying a custom pytorch model on , following this tutorial. in my case i have few dependencies to install some modules. i need pycocotools in my inference.py script. i can easily install pycocotool inside a separate notebook using this bash command, %%bash pip -g install pycocotools but when i create my endpoint for deployment, i get this error that pycocotools in not defined. i need pycocotools inside my inference.py script. how i can install this inside a .py file",
        "Question_original_content_gpt_summary":"The user is encountering challenges with installing modules inside a .py file for deployment of a custom PyTorch model.",
        "Question_preprocessed_content":"Title: installing modules inside python .py file; Content: i am deploying a custom pytorch model on , following this tutorial. in my case i have few dependencies to install some modules. i need pycocotools in my script. i can easily install pycocotool inside a separate notebook using this bash command, but when i create my endpoint for deployment, i get this error that pycocotools in not defined. i need pycocotools inside my script. how i can install this inside a .py file",
        "Answer_original_content":"at the beginning of inference.py add these lines: from subprocess import check_call, run, calledprocesserror import sys import os # since it is likely that you're going to run inference.py multiple times, this avoids reinstalling the same package: if not os.environ.get(\"install_success\"): try: check_call( [ sys.executable, \"pip\", \"install\", \"pycocotools\",] ) except calledprocesserror: run( [\"pip\", \"install\", \"pycocotools\",] ) os.environ[\"install_success\"] = \"true\"",
        "Answer_original_content_gpt_summary":"The solution to the challenge of installing modules inside a .py file for deployment of a custom PyTorch model is to add the provided lines of code at the beginning of the inference.py file. These lines of code will ensure that the required package is installed only once and avoid reinstalling it multiple times.",
        "Answer_preprocessed_content":"at the beginning of add these lines"
    },
    {
        "Question_id":71284125.0,
        "Question_title":"GCP Vertex AI Training: Auto-packaged Custom Training Job Yields Huge Docker Image",
        "Question_body":"<p>I am trying to run a Custom Training Job in Google Cloud Platform's Vertex AI Training service.<\/p>\n<p>The job is based on <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/pytorch-google-cloud-how-train-and-tune-pytorch-models-vertex-ai\" rel=\"nofollow noreferrer\">a tutorial from Google that fine-tunes a pre-trained BERT model<\/a> (from HuggingFace).<\/p>\n<p>When I use the <code>gcloud<\/code> CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:<\/p>\n<pre><code>$BASE_GPU_IMAGE=&quot;us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest&quot;\n$BUCKET_NAME = &quot;my-bucket&quot;\n\ngcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=&quot;--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier&quot; `\n--worker-pool-spec=&quot;machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task&quot;\n<\/code><\/pre>\n<p>... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.<\/p>\n<p>Granted <a href=\"https:\/\/console.cloud.google.com\/gcr\/images\/deeplearning-platform-release\/GLOBAL\/pytorch-gpu.1-7?tag=nightly-2021-03-28\" rel=\"nofollow noreferrer\">the base image is around 6.5GB<\/a> but <strong>where do the additional &gt;10GB come from and is there a way for me to avoid this &quot;image bloat&quot;?<\/strong><\/p>\n<p>Please note that my job loads the training data using the <code>datasets<\/code> Python package at run time and AFAIK does not include it in the auto-packaged docker image.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1645958992960,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":478.0,
        "Owner_creation_time":1225618447787,
        "Owner_last_access_time":1663751971780,
        "Owner_reputation":15116.0,
        "Owner_up_votes":890.0,
        "Owner_down_votes":56.0,
        "Owner_views":1182.0,
        "Answer_body":"<p>The image size shown in the UI is the virtual size of the image. It is the compressed total image size that will be downloaded over the network. Once the image is pulled, it will be extracted and the resulting size will be bigger. In this case, the <a href=\"https:\/\/console.cloud.google.com\/artifacts\/docker\/vertex-ai\/us\/training\/pytorch-gpu.1-7\/sha256:0d990ebc4fc376880fd3ee375015e43594450d11d9791ac203cf2d044871917f\" rel=\"nofollow noreferrer\">PyTorch image's virtual size<\/a> is 6.8 GB while the actual size is 17.9 GB.<\/p>\n<p>Also, when a <code>docker push<\/code> command is executed, the progress bars show the uncompressed size. The actual amount of data that\u2019s pushed will be <a href=\"https:\/\/docs.docker.com\/engine\/reference\/commandline\/push\/#extended-description\" rel=\"nofollow noreferrer\">compressed before sending<\/a>, so the uploaded size will not be reflected by the progress bar.<\/p>\n<p>To cut down the size of the docker image, custom containers can be used. Here, only the necessary components can be configured which would result in a smaller docker image. More information on custom containers <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/containers-overview\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646123694692,
        "Answer_score":1.0,
        "Owner_location":"Tel Aviv, Israel",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71284125",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: gcp training: auto-packaged custom training job yields huge docker image; Content: i am trying to run a custom training job in google cloud platform's training service. the job is based on a tutorial from google that fine-tunes a pre-trained bert model (from huggingface). when i use the gcloud cli tool to auto-package my training code into a docker image and deploy it to the training service like so: $base_gpu_image=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\" $bucket_name = \"my-bucket\" gcloud ai custom-jobs create ` --region=us-central1 ` --display-name=fine_tune_bert ` --args=\"--job_dir=$bucket_name,--num-epochs=2,--model-name=finetuned-bert-classifier\" ` --worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=nvidia_tesla_v100,executor-image-uri=$base_gpu_image,local-package-path=.,python-module=trainer.task\" ... i end up with a docker image that is roughly 18gb (!) and takes a very long time to upload to the gcp registry. granted the base image is around 6.5gb but where do the additional >10gb come from and is there a way for me to avoid this \"image bloat\"? please note that my job loads the training data using the datasets python package at run time and afaik does not include it in the auto-packaged docker image.",
        "Question_original_content_gpt_summary":"The user encountered a challenge of a huge docker image (18GB) when auto-packaging their custom training job for Google Cloud Platform's training service, with the additional >10GB coming from an unknown source.",
        "Question_preprocessed_content":"Title: gcp training custom training job yields huge docker image; Content: i am trying to run a custom training job in google cloud platform's training service. the job is based on a tutorial from google that a bert model . when i use the cli tool to my training code into a docker image and deploy it to the training service like so i end up with a docker image that is roughly gb and takes a very long time to upload to the gcp registry. granted the base image is around but where do the additional gb come from and is there a way for me to avoid this image bloat ? please note that my job loads the training data using the python package at run time and afaik does not include it in the docker image.",
        "Answer_original_content":"the image size shown in the ui is the virtual size of the image. it is the compressed total image size that will be downloaded over the network. once the image is pulled, it will be extracted and the resulting size will be bigger. in this case, the pytorch image's virtual size is 6.8 gb while the actual size is 17.9 gb. also, when a docker push command is executed, the progress bars show the uncompressed size. the actual amount of data thats pushed will be compressed before sending, so the uploaded size will not be reflected by the progress bar. to cut down the size of the docker image, custom containers can be used. here, only the necessary components can be configured which would result in a smaller docker image. more information on custom containers here.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of a huge docker image (18GB) when auto-packaging a custom training job for Google Cloud Platform's training service are: \n\n1. The image size shown in the UI is the virtual size of the image, which is the compressed total image size that will be downloaded over the network. Once the image is pulled, it will be extracted, and the resulting size will be bigger. \n\n2. To cut down the size of the docker image, custom containers can be used. Here, only the necessary components can be configured, which would result in a smaller docker image. \n\n3. More information on custom containers can be found in the provided link.",
        "Answer_preprocessed_content":"the image size shown in the ui is the virtual size of the image. it is the compressed total image size that will be downloaded over the network. once the image is pulled, it will be extracted and the resulting size will be bigger. in this case, the pytorch image's virtual size is gb while the actual size is gb. also, when a command is executed, the progress bars show the uncompressed size. the actual amount of data thats pushed will be compressed before sending, so the uploaded size will not be reflected by the progress bar. to cut down the size of the docker image, custom containers can be used. here, only the necessary components can be configured which would result in a smaller docker image. more information on custom containers here."
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to make parametric plots?",
        "Question_body":"<p>Hello, I would like to make plots such as the ones that can be seen in this video at this timestamp (59:59): <a href=\"https:\/\/youtu.be\/XL07WEc2TRI?t=3599\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Stanford Seminar - Information Theory of Deep Learning - YouTube<\/a><\/p>\n<p>Basically, I have two variables (let\u2019s say X and Y) that are measured at each layer and epoch, and I would like to have a unified plot where each layer is represented as a parametric curve. Connecting points in a same epoch by neighboring layers would be a plus but that\u2019s optional.<br>\nSo for each epoch and layer, I would like to plot a point at coordinates (X,Y) connected to the corresponding previous point of the previous epoch. If possible, I would like to color each point according to the epoch so that we can see the progression.<\/p>\n<p>I tried to plot a line series like this:<\/p>\n<pre><code class=\"lang-python\">wandb.log({\"XY\": wandb.plot.line_series(self.layers_x, self.layers_y, self.layer_names,\n                                        \"XY by layer and epoch\", \"X\")}, step=step)\n<\/code><\/pre>\n<p>But there are three issues with this:<\/p>\n<ol>\n<li>The points of the curves aren\u2019t connected in the correct order, it seems they are implicitly connected according to their sorted X values. So the resulting curves are incorrect, even if I can guess the true shapes they should have.<\/li>\n<li>I haven\u2019t managed to get point coloring according to the epoch number, and I had to manually modify the plot in the dashboard so that I had all curves correctly displayed in the same plot. I had to use custom plots but I am not familiar with these. I also don\u2019t know how to set the display name of the y axis which is \u201cy\u201d by default.<\/li>\n<li>I have to manually keep track of the table values, if possible I would like to log the values for each step normally, like any other value like the accuracy at each epoch.<\/li>\n<\/ol>\n<p>So, is it possible to make such plots? Thank you.<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":null,
        "Question_creation_time":1656316710393,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":214.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-make-parametric-plots\/2662",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-29T16:33:25.375Z",
                "Answer_body":"<p>Hi Ben,<\/p>\n<p>Thanks for being in touch with this interesting question. So that I can get a clear picture of what you are trying to achieve would you be able to share a link to the workplace where you are trying to plot this? Look forward to hearing from you. ~Frida<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-01T10:54:53.813Z",
                "Answer_body":"<p>Hi Frida,<\/p>\n<p>You can see the issue here <a href=\"https:\/\/wandb.ai\/bencrulis\/mnist%20custom%202?workspace=user-bencrulis\">https:\/\/wandb.ai\/bencrulis\/mnist%20custom%202?workspace=user-bencrulis<\/a><\/p>\n<p>As you can see in the custom chart, the line corresponding to layer_8 should arc from left to right then right to left while going up at all times, but the first point is connected to the last and make the trajectory incorrect.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-05T12:28:16.977Z",
                "Answer_body":"<p>Hi Ben,<\/p>\n<p>I think that weave plots will likely give you much closer functionality to the image at the timestamp of the video that you shared. I have attached a screengrab of this, alongside steps that you would need to take, you can see an arc where the colour is each layer and x and y values on a scatter follow the same logic as you have shared. To reproduce this you would need to add a panel, and use Weave to select \u2018Merge Tables Plot\u2019. set the x dim and y dim, this will colour by step.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-06T09:52:18.708Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>The Weave plot is indeed a easier to read, but I didn\u2019t manage to get the correct coloring of the points. I guess this is because the epoch variable isn\u2019t in the table.<br>\nChanging the style to \u201cline\u201d instead of \u201cpoint\u201d also displays the line incorrectly, I guess the available plots weren\u2019t designed to do this.<\/p>\n<p>Well in any case thank you, I will adapt.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-06T14:16:13.038Z",
                "Answer_body":"<p>Hey Ben thanks for being back in touch and always happy to help. I think that this would make an interesting use case, and yes I did notice the same behaviour when adding a line. I will log this as a feature request for your (line group by) functionality, where the order of precedence is not dictated by the x axis). One interesting point of using weave is that you can use feature embedding such as PCA (principal component analysis) which I notice produces some interesting results for this and may add value\/insight to your use case.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-07T14:14:23.819Z",
                "Answer_body":"<p>Hey Ben, Updating you that I have created a feature request for you on this. Let me know if you need any further help just now.<\/p>",
                "Answer_score":15.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-04T09:53:09.474Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is it possible to make parametric plots?; Content: hello, i would like to make plots such as the ones that can be seen in this video at this timestamp (59:59): stanford seminar - information theory of deep learning - youtube basically, i have two variables (let\u2019s say x and y) that are measured at each layer and epoch, and i would like to have a unified plot where each layer is represented as a parametric curve. connecting points in a same epoch by neighboring layers would be a plus but that\u2019s optional. so for each epoch and layer, i would like to plot a point at coordinates (x,y) connected to the corresponding previous point of the previous epoch. if possible, i would like to color each point according to the epoch so that we can see the progression. i tried to plot a line series like this: .log({\"xy\": .plot.line_series(self.layers_x, self.layers_y, self.layer_names, \"xy by layer and epoch\", \"x\")}, step=step) but there are three issues with this: the points of the curves aren\u2019t connected in the correct order, it seems they are implicitly connected according to their sorted x values. so the resulting curves are incorrect, even if i can guess the true shapes they should have. i haven\u2019t managed to get point coloring according to the epoch number, and i had to manually modify the plot in the dashboard so that i had all curves correctly displayed in the same plot. i had to use custom plots but i am not familiar with these. i also don\u2019t know how to set the display name of the y axis which is \u201cy\u201d by default. i have to manually keep track of the table values, if possible i would like to log the values for each step normally, like any other value like the accuracy at each epoch. so, is it possible to make such plots? thank you.",
        "Question_original_content_gpt_summary":"The user is looking for a way to make parametric plots with two variables, connected points in the same epoch, and coloring according to the epoch, while also being able to log the values for each step and set the display name of the y axis.",
        "Question_preprocessed_content":"Title: is it possible to make parametric plots?; Content: hello, i would like to make plots such as the ones that can be seen in this video at this timestamp stanford seminar information theory of deep learning youtube basically, i have two variables that are measured at each layer and epoch, and i would like to have a unified plot where each layer is represented as a parametric curve. connecting points in a same epoch by neighboring layers would be a plus but thats optional. so for each epoch and layer, i would like to plot a point at coordinates connected to the corresponding previous point of the previous epoch. if possible, i would like to color each point according to the epoch so that we can see the progression. i tried to plot a line series like this but there are three issues with this the points of the curves arent connected in the correct order, it seems they are implicitly connected according to their sorted x values. so the resulting curves are incorrect, even if i can guess the true shapes they should have. i havent managed to get point coloring according to the epoch number, and i had to manually modify the plot in the dashboard so that i had all curves correctly displayed in the same plot. i had to use custom plots but i am not familiar with these. i also dont know how to set the display name of the y axis which is y by default. i have to manually keep track of the table values, if possible i would like to log the values for each step normally, like any other value like the accuracy at each epoch. so, is it possible to make such plots? thank you.",
        "Answer_original_content":"hi ben, thanks for being in touch with this interesting question. so that i can get a clear picture of what you are trying to achieve would you be able to share a link to the workplace where you are trying to plot this? look forward to hearing from you. ~frida hi frida, you can see the issue here https:\/\/.ai\/bencrulis\/mnist%20custom%202?workspace=user-bencrulis as you can see in the custom chart, the line corresponding to layer_8 should arc from left to right then right to left while going up at all times, but the first point is connected to the last and make the trajectory incorrect. hi ben, i think that weave plots will likely give you much closer functionality to the image at the timestamp of the video that you shared. i have attached a screengrab of this, alongside steps that you would need to take, you can see an arc where the colour is each layer and x and y values on a scatter follow the same logic as you have shared. to reproduce this you would need to add a panel, and use weave to select merge tables plot. set the x dim and y dim, this will colour by step. hi, the weave plot is indeed a easier to read, but i didnt manage to get the correct coloring of the points. i guess this is because the epoch variable isnt in the table. changing the style to line instead of point also displays the line incorrectly, i guess the available plots werent designed to do this. well in any case thank you, i will adapt. hey ben thanks for being back in touch and always happy to help. i think that this would make an interesting use case, and yes i did notice the same behaviour when adding a line. i will log this as a feature request for your (line group by) functionality, where the order of precedence is not dictated by the x axis). one interesting point of using weave is that you can use feature embedding such as pca (principal component analysis) which i notice produces some interesting results for this and may add value\/insight to your use case. hey ben, updating you that i have created a feature request for you on this. let me know if you need any further help just now. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The user is looking for a way to make parametric plots with two variables, connected points in the same epoch, and coloring according to the epoch, while also being able to log the values for each step and set the display name of the y axis. The answer suggests using weave plots to achieve the desired functionality. However, the user encountered issues with coloring the points and displaying the line correctly. The answer suggests logging a feature request for the desired functionality and also mentions the use of feature embedding such as PCA to add value\/insight to the use case.",
        "Answer_preprocessed_content":"hi ben, thanks for being in touch with this interesting question. so that i can get a clear picture of what you are trying to achieve would you be able to share a link to the workplace where you are trying to plot this? look forward to hearing from you. frida hi frida, you can see the issue here as you can see in the custom chart, the line corresponding to should arc from left to right then right to left while going up at all times, but the first point is connected to the last and make the trajectory incorrect. hi ben, i think that weave plots will likely give you much closer functionality to the image at the timestamp of the video that you shared. i have attached a screengrab of this, alongside steps that you would need to take, you can see an arc where the colour is each layer and x and y values on a scatter follow the same logic as you have shared. to reproduce this you would need to add a panel, and use weave to select merge tables plot. set the x dim and y dim, this will colour by step. hi, the weave plot is indeed a easier to read, but i didnt manage to get the correct coloring of the points. i guess this is because the epoch variable isnt in the table. changing the style to line instead of point also displays the line incorrectly, i guess the available plots werent designed to do this. well in any case thank you, i will adapt. hey ben thanks for being back in touch and always happy to help. i think that this would make an interesting use case, and yes i did notice the same behaviour when adding a line. i will log this as a feature request for your functionality, where the order of precedence is not dictated by the x axis . one interesting point of using weave is that you can use feature embedding such as pca which i notice produces some interesting results for this and may add to your use case. hey ben, updating you that i have created a feature request for you on this. let me know if you need any further help just now. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Designer: Export Code",
        "Question_body":"Is there an option to export the Azure ML Designer to code so we can copy between workspaces?",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1646150939773,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/755142\/azure-ml-designer-export-code.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-02T06:20:05.287Z",
                "Answer_score":1,
                "Answer_body":"Hi, this feature is currently not supported as mentioned on this thread. However, it's on the roadmap.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-11-10T18:23:47.713Z",
                "Answer_score":0,
                "Answer_body":"Is the roadmap public? When is this feature planning on being released?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: designer: export code; Content: is there an option to export the designer to code so we can copy between workspaces?",
        "Question_original_content_gpt_summary":"The user is looking for an option to export the Designer to code so they can copy between workspaces.",
        "Question_preprocessed_content":"Title: designer export code; Content: is there an option to export the designer to code so we can copy between workspaces?",
        "Answer_original_content":"hi, this feature is currently not supported as mentioned on this thread. however, it's on the roadmap.",
        "Answer_original_content_gpt_summary":"Possible solutions: None.\n\nSummary: The user is looking for an option to export the Designer to code, but currently, there is no such feature available. However, it is on the roadmap for future development.",
        "Answer_preprocessed_content":"hi, this feature is currently not supported as mentioned on this thread. however, it's on the roadmap."
    },
    {
        "Question_id":null,
        "Question_title":"Set MLflow Tracking to only track in your Azure Machine Learning workspace",
        "Question_body":"Hi Team,\n\nAs per below link\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-mlflow-azure-databricks?tabs=custom\n\nAnd section: Set MLflow Tracking to only track in your Azure Machine Learning workspace\n\nWe can set ML Flow Tracking URI to Azure ML in Databricks but i am getting below error upon setting and trying to create experiement\n\n\n\n\nUnsupportedModelRegistryStoreURIException: Model registry functionality is unavailable; got unsupported URI 'azureml:\/\/eastus.api.azureml.ms\/mlflow\/v1.0\/subscriptions\/60732f07-e07d-4492-b2cc-e43155932aca\/resourceGroups\/RG-BHOGA\/providers\/Microsoft.MachineLearningServices\/workspaces\/[REDACTED]' for model registry data storage. Supported URI schemes are: ['', 'file', 'databricks', 'http', 'https', 'postgresql', 'mysql', 'sqlite', 'mssql']. See https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations.\n\nCould you please advise what has to be done?",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1656381127797,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/905516\/set-mlflow-tracking-to-only-track-in-your-azure-ma.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-28T16:55:09.987Z",
                "Answer_score":0,
                "Answer_body":"@bhogasenareddykalakata-6505 Thanks for the question. Can you please share the notebook that you are trying. Here is the sample for ml flow tracking.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-26T18:41:36.383Z",
                "Answer_score":0,
                "Answer_body":"@bhogasenareddykalakata-6505, you probably forgot to install the plugin azureml-mlflow. This is required to interpret the URI and that's why you get the unsupported URI error.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":18.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: set tracking to only track in your workspace; Content: hi team, as per below link https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use--azure-databricks?tabs=custom and section: set tracking to only track in your workspace we can set ml flow tracking uri to in databricks but i am getting below error upon setting and trying to create experiement unsupportedmodelregistrystoreuriexception: model registry functionality is unavailable; got unsupported uri ':\/\/eastus.api..ms\/\/v1.0\/subscriptions\/60732f07-e07d-4492-b2cc-e43155932aca\/resourcegroups\/rg-bhoga\/providers\/microsoft.machinelearningservices\/workspaces\/[redacted]' for model registry data storage. supported uri schemes are: ['', 'file', 'databricks', 'http', 'https', 'postgresql', 'mysql', 'sqlite', 'mssql']. see https:\/\/www..org\/docs\/latest\/tracking.html#storage for how to run an server against one of the supported backend storage locations. could you please advise what has to be done?",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to set MLFlow tracking to only track in their workspace.",
        "Question_preprocessed_content":"Title: set tracking to only track in your workspace; Content: hi team, as per below link and section set tracking to only track in your workspace we can set ml flow tracking uri to in databricks but i am getting below error upon setting and trying to create experiement unsupportedmodelregistrystoreuriexception model registry functionality is unavailable; got unsupported uri for model registry data storage. supported uri schemes are . see for how to run an server against one of the supported backend storage locations. could you please advise what has to be done?",
        "Answer_original_content":"@bhogasenareddykalakata-6505 thanks for the question. can you please share the notebook that you are trying. here is the sample for ml flow tracking. @bhogasenareddykalakata-6505, you probably forgot to install the plugin -mlflow. this is required to interpret the uri and that's why you get the unsupported uri error.",
        "Answer_original_content_gpt_summary":"Possible solution: The user may have forgotten to install the plugin -mlflow, which is required to interpret the uri and avoid the unsupported uri error.",
        "Answer_preprocessed_content":"thanks for the question. can you please share the notebook that you are trying. here is the sample for ml flow tracking. you probably forgot to install the plugin this is required to interpret the uri and that's why you get the unsupported uri error."
    },
    {
        "Question_id":null,
        "Question_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Question_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n\nSageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\nAmazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only. Is this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1605279993000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sage-maker-training-on-amazon-f-sx-for-lustre",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-13T15:26:19.000Z",
                "Answer_score":0,
                "Answer_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\nFSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\nFSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1]. [1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i achieve the least-access secure networking for training on amazon fsx for lustre?; Content: i'm trying to figure out a minimally permissive yet operational network configuration for training to train on data from amazon fsx for lustre. my understanding is that both the file system and the instance can have their own security groups and that fsx uses tcp on ports 988 and 1021-1023. therefore, i think a good network configuration for using with fsx is the following: ec2 equipped with the security group sm-sg that allows inbound only with tcp on 988 and 1021-1023 from fsx-sg only. amazon fsx equipped with the security group fsx-sg that allows outbound only with tcp on 988 and 1021-1023 towards sm-sg only. is this configuration enough for the training to work? do fsx and need other ports and sources to be opened to operate normally?",
        "Question_original_content_gpt_summary":"The user is trying to figure out a secure network configuration for training on data from Amazon FSx for Lustre, and is unsure if the proposed configuration is enough for the training to work.",
        "Question_preprocessed_content":"Title: how do i achieve the secure networking for training on amazon fsx for lustre?; Content: i'm trying to figure out a minimally permissive yet operational network configuration for training to train on data from amazon fsx for lustre. my understanding is that both the file system and the instance can have their own security groups and that fsx uses tcp on ports and therefore, i think a good network configuration for using with fsx is the following ec equipped with the security group that allows inbound only with tcp on and from only. amazon fsx equipped with the security group that allows outbound only with tcp on and towards only. is this configuration enough for the training to work? do fsx and need other ports and sources to be opened to operate normally?",
        "Answer_original_content":"for the security group for amazon fsx (example: fsx-sg), you need to add the following additional rules: fsx-sg needs inbound access from the security group for (example: sm-sg). the instance needs to initiate a connection to the amazon fsx file system, which is an inbound tcp packet to fsx. fsx-sg needs inbound and outbound access to itself. this is because, amazon fsx for lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to communicate with one another. for more information on the minimum set of rules required for fsx-sg, see [file system access control with amazon vpc][1]. [1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/lustreguide\/limit-access-security-groups.html",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include adding additional rules to the security group for Amazon FSx, allowing inbound access from the security group for the instance, allowing the instance to initiate a connection to the Amazon FSx file system, and allowing inbound and outbound access to itself. The answer also provides a link to more information on the minimum set of rules required for the security group.",
        "Answer_preprocessed_content":"for the security group for amazon fsx , you need to add the following additional rules needs inbound access from the security group for . the instance needs to initiate a connection to the amazon fsx file system, which is an inbound tcp packet to fsx. needs inbound and outbound access to itself. this is because, amazon fsx for lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to communicate with one another. for more information on the minimum set of rules required for see ."
    },
    {
        "Question_id":null,
        "Question_title":"Run.history() returns different values on almost each call",
        "Question_body":"<p>I recently started using the <code>wandb.Api()<\/code> in order not to manually download all the Charts in <code>.csv<\/code> format.<\/p>\n<p>The problem is that I cannot get consistent results, most of the times that I call the API  in a jupyter-notebook I get different results.<\/p>\n<p>I have made public one of my dashboards to tackle this issue. Here is a screenshot with a reproducible example:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" data-download-href=\"\/uploads\/short-url\/8x7Rm9lNkSyg4pi6edKG0wNOxgE.png?dl=1\" title=\"2022-05-16-165542_647x517_scrot\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c.png\" alt=\"2022-05-16-165542_647x517_scrot\" data-base62-sha1=\"8x7Rm9lNkSyg4pi6edKG0wNOxgE\" width=\"625\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3bd006338d2541c672c4bf4c2f5e60aa6144e60c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">2022-05-16-165542_647x517_scrot<\/span><span class=\"informations\">647\u00d7517 50.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>In order to obtain the <code>csv_val_f1<\/code> variable one just needs to download the <code>Val F1<\/code> chart. Two things can be seen here:<\/p>\n<ol>\n<li>Multiple runs of the same code produce different results<\/li>\n<li>The maximum value obtained by the API differs from the maximum value obtained by manually downloading the <code>.csv<\/code> version of the Chart.<\/li>\n<\/ol>\n<p>Any ideas on what I\u2019m missing?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1652713229122,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":502.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/run-history-returns-different-values-on-almost-each-call\/2431",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-27T06:59:45.338Z",
                "Answer_body":"<p>I am experiencing the same issue.<br>\nI wanted to generate matplotlib plots using the API, but run.history() shows different results at each call so that it is impossible to reproduce the wandb charts .<\/p>",
                "Answer_score":12.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-27T16:47:31.375Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jaeheelee\">@jaeheelee<\/a> and <a class=\"mention\" href=\"\/u\/carloshernandezp\">@carloshernandezp<\/a>,<br>\nI believe you are seeing this because we sample the data points when you call <code>run.history()<\/code>. You can use <code>run.scan_history()<\/code> if you would like to have the entire history returned. <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#sampling\">Here<\/a> is some more information on this.<\/p>\n<p>Let me know if this solves the issue for you.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":27.2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-05-28T12:52:43.563Z",
                "Answer_body":"<p>Yes, this solved my problem. Thank you <a class=\"mention\" href=\"\/u\/nathank\">@nathank<\/a> !<\/p>",
                "Answer_score":22.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-30T09:41:55.473Z",
                "Answer_body":"<p>This indeed solved the issue. Thank you for answering!<\/p>",
                "Answer_score":16.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-29T09:42:40.908Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: run.history() returns different values on almost each call; Content: i recently started using the .api() in order not to manually download all the charts in .csv format. the problem is that i cannot get consistent results, most of the times that i call the api in a jupyter-notebook i get different results. i have made public one of my dashboards to tackle this issue. here is a screenshot with a reproducible example: 2022-05-16-165542_647x517_scrot647\u00d7517 50.3 kb in order to obtain the csv_val_f1 variable one just needs to download the val f1 chart. two things can be seen here: multiple runs of the same code produce different results the maximum value obtained by the api differs from the maximum value obtained by manually downloading the .csv version of the chart. any ideas on what i\u2019m missing?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with the .api() function, where multiple runs of the same code produce different results and the maximum value obtained by the api differs from the maximum value obtained by manually downloading the .csv version of the chart.",
        "Question_preprocessed_content":"Title: returns different values on almost each call; Content: i recently started using the in order not to manually download all the charts in format. the problem is that i cannot get consistent results, most of the times that i call the api in a i get different results. i have made public one of my dashboards to tackle this issue. here is a screenshot with a reproducible example kb in order to obtain the variable one just needs to download the chart. two things can be seen here multiple runs of the same code produce different results the maximum value obtained by the api differs from the maximum value obtained by manually downloading the version of the chart. any ideas on what im missing?",
        "Answer_original_content":"hi @jaeheelee and @carloshernandezp, i believe you are seeing this because we sample the data points when you call run.history(). you can use run.scan_history() if you would like to have the entire history returned. here is some more information on this. let me know if this solves the issue for you. thank you, nate",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of obtaining different results from the .api() function and the .csv version of the chart include using the run.scan_history() function to obtain the entire history and understanding that the data points are sampled when calling run.history().",
        "Answer_preprocessed_content":"hi and i believe you are seeing this because we sample the data points when you call . you can use if you would like to have the entire history returned. here is some more information on this. let me know if this solves the issue for you. thank you, nate"
    },
    {
        "Question_id":null,
        "Question_title":"Error 404: AciDeploymentFailed",
        "Question_body":"Hello,\n\nI am trying to deploy a machine learning model through an ACI (Azure Container Instances) service. I am working in Python and I followed the following code (from the official documentation : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli) :\n\nfrom azureml.core import Workspace\n\n from azureml.core import Workspace\n    \n # Connect to workspace\n ws = Workspace(subscription_id=\"my-subscription-id\",\n                resource_group=\"my-ressource-group-name\",\n                workspace_name=\"my-workspace-name\")\n    \n from azureml.core.model import Model\n    \n model = Model.register(workspace = ws,\n                        model_path= 'model.pkl',\n                        model_name = 'my-model',\n                        description = 'my-description')\n    \n    \n    \n %%writefile score.py\n    \n import os\n import dill\n import joblib\n    \n def init():\n     global model\n     # Get the path where the deployed model can be found\n     model_path = os.getenv('AZUREML_MODEL_DIR')\n    \n     # Load existing model\n     model = joblib.load('model.pkl')\n    \n # Handle request to the service\n def run(data):\n     try:\n         # Pick out the text property of the JSON request\n         # Expected JSON details {\"text\": \"some text to evaluate\"}\n         data = json.loads(data)\n         prediction = model.predict(data['text'])\n         return prediction\n     except Exception as e:\n         error = str(e)\n         return error\n    \n    \n from azureml.core.environment import Environment\n    \n # Name environment and call requirements file\n # requirements: numpy, tensorflow\n myenv = Environment.from_pip_requirements(name = 'myenv', file_path = 'requirements.txt')\n    \n from azureml.core.model import InferenceConfig\n    \n # Create inference configuration\n inference_config = InferenceConfig(environment=myenv, entry_script='score.py')\n    \n from azureml.core.webservice import AciWebservice #AksWebservice\n    \n # Set the virtual machine capabilities\n deployment_config = AciWebservice.deploy_configuration(cpu_cores = 0.5, memory_gb = 3)\n    \n    \n from azureml.core.model import Model\n    \n # Deploy ML model (Azure Container Instances)\n service = Model.deploy(workspace=ws,\n                        name='my-service-name',\n                        models=[model],\n                        inference_config=inference_config,\n                        deployment_config=deployment_config)\n    \n service.wait_for_deployment(show_output = True)\n\n\n\n\n\nI succeded once with the previous code. I noticed that the Model.deploy created a container registry with a specific name 6e07ce2cc4ac4838b42d35cda8d38616.\nThe API was working well and I wanted to deploy an other model from scratch. I deleted the service and model from Azure ML Studio and the container registry from Azure ressources.\n\nUnfortunately I am not able to deploy again anything.\n\nFor the last step (the Model.deploy step), I have the following error message :\n\nService deployment polling reached non-successful terminal state, current service state: Unhealthy\nOperation ID: 46243f9b-3833-4650-8d47-3ac54a39dc5e\nMore information can be found here: https:\/\/machinelearnin2812599115.blob.core.windows.net\/azureml\/ImageLogs\/46245f8b-3833-4659-8d47-3ac54a39dc5e\/build.log?sv=2019-07-07&sr=b&sig=45kgNS4sbSZrQH%2Fp29Rhxzb7qC5Nf1hJ%2BLbRDpXJolk%3D&st=2021-10-25T17%3A20%3A49Z&se=2021-10-27T01%3A24%3A49Z&sp=r\nError:\n{\n\"code\": \"AciDeploymentFailed\",\n\"statusCode\": 404,\n\"message\": \"No definition exists for Environment with Name: myenv Version: Autosave_2021-10-25T17:24:43Z_b1d066bf Reason: Container > registry 6e07ce2cc4ac4838b42d35cda8d38616.azurecr.io not found. If private link is enabled in workspace, please verify ACR is part of private > link and retry..\",\n\"details\": []\n}\n\nI do not understand why the first time a new container registry was well created, but now it seems that it is sought (the message is saying that container registry identified by name 6e07ce2cc4ac4838b42d35cda8d38616 is missing). I never found where I can force the creation of a new container registry ressource in Python, neither specify a name for it in AciWebservice.deploy_configuration or Model.deploy.\n\nI tried to create the container registry by hand, but this time, this is the container that cannot be created. The output is the folloiwing :\n\nTips: You can try get_logs(): https:\/\/aka.ms\/debugimage#dockerlog or local deployment: https:\/\/aka.ms\/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\nRunning\n2021-10-25 19:25:10+02:00 Creating Container Registry if not exists.\n2021-10-25 19:25:10+02:00 Registering the environment.\n2021-10-25 19:25:13+02:00 Building image..\n2021-10-25 19:30:45+02:00 Generating deployment configuration.\n2021-10-25 19:30:46+02:00 Submitting deployment to compute.\nFailed\n\nService deployment polling reached non-successful terminal state, current service state: Unhealthy\nOperation ID: 93780de6-7662-40d8-ab9e-4e1556ef880f\nCurrent sub-operation type not known, more logs unavailable.\nError:\n{\n\"code\": \"InaccessibleImage\",\n\"statusCode\": 400,\n\"message\": \"ACI Service request failed. Reason: The image '6e07ce2cc4ac4838b42d35cda8d38616.azurecr.io\/azureml\/azureml_684133370d8916c87f6230d213976ca5' in container group 'my-service-name-LM4HbqzEBEi0LTXNqNOGFQ' is not accessible. Please check the image and registry credential.. Refer to https:\/\/docs.microsoft.com\/azure\/container-registry\/container-registry-authentication#admin-account and make sure Admin user is enabled for your container registry.\"\n}\n\nI tried to follow the recommandation of the last message saying to set Admin user enabled for the container registry. Unfortunately the same error message appears again and I am stuck here...\n\nDoes anyone could help me omving on with this? The best solution would be I think to delete totally this 6e07ce2cc4ac4838b42d35cda8d38616 container registry but I can't find where the reference is set so Model.deploy always fall to find it.\n\nAn other solution would be to force Model.deploy to generate a new container registry, but I could find how to make that.\n\nI need your help !",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1635187234100,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/603277\/error-404-acideploymentfailed.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-26T09:08:27.543Z",
                "Answer_score":0,
                "Answer_body":"@FlorianSurmont-4431 The azure container registry is actually created while creating your workspace. Deleting the container registry or any of its dependent resources like storage account, Keyvault and app insights will actually cause the workspace to behave inconsistently.\n\nSince you have already deleted the registry and tried to attach a new one, it looks like the keys of this dependent resource are not synced with the workspace. Ideally during create of a workspace you can use an existing registry with the following command.\n\n az ml workspace create -w <workspace-name>\n                        -g <resource-group-name>\n                        --container-registry \"\/subscriptions\/<service-GUID>\/resourceGroups\/<resource-group-name>\/providers\/Microsoft.ContainerRegistry\/registries\/<acr-name>\"\n\n\n\nSince the workspace is already available you can use the az ml workspace update command instead to set the registry and then sync the keys.\n\n  az ml workspace sync-keys -w <workspace-name> -g <resource-group-name>\n\n\n\nI have worked with another user with a similar issue before but they did not enable admin access on registry. Since you have already done so, I think the above steps should help to sync the registry with your workspace and you can try to create or update a model from your experiment.\n\nI hope this can help.\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-26T10:29:45.23Z",
                "Answer_score":0,
                "Answer_body":"Thank you very much for your answer. I do understand when I made my mistake, and what happens, thank you.\n\nUnfortunately I am not able to run the command you suggest:\n\n  az ml workspace sync-keys -w machinelearning -g oc-ingenieur-ia\n\nIndeed, I am not able to find <workspace-name> from the CLI. The error message is the following:\n\nProjectSystemException:\nMessage: Workspace not found.\nInnerException None\nErrorResponse\n{\n\"error\": {\n\"message\": \"Workspace not found.\"\n}\n}\n\n\n\n\nWhen I run\n\n az ml workspace list\n\nThe result is empty : [].\n\nThat is quite weird, because I do have it created in the portal web interface.\n\nWhen I try to find the resource group name with command:\n\n az group list --subscription <my-subscription-id>\n\nI do have the result:\n\n[\n{\n\"id\": \"\/subscriptions\/<my-subscription-id>\/resourceGroups\/OC-ingenieur-IA\",\n\"location\": \"francecentral\",\n\"managedBy\": null,\n\"name\": \"OC-ingenieur-IA\",\n\"properties\": {\n\"provisioningState\": \"Succeeded\"\n},\n\"tags\": {},\n\"type\": \"Microsoft.Resources\/resourceGroups\"\n}\n]\n\nBut when I run either:\n\n az ml workspace list --resource-group OC-ingenieur-IA\n\nor\n\n az ml workspace list --resource-group oc-ingenieur-ia\n\nI have the following error:\n\nProjectSystemException:\nMessage: Workspaces not found.\nInnerException None\nErrorResponse\n{\n\"error\": {\n\"message\": \"Workspaces not found.\"\n}\n}\n\n\n\n\nMoreover if I try to import the workspace from Python:\n\n ws = Workspace(subscription_id=\"my-subscription-id\",\n                resource_group=\"oc-ingenieur-ia\",\n                workspace_name=\"machinelearning\")\n\nThe Python object iswell created and its name attribute is \"machinelearning\" as expected. I can interact with it to create a model with Model.register that shows up in Azure portal.\n\nAny idea of what is going on?",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: error 404: acideploymentfailed; Content: hello, i am trying to deploy a machine learning model through an aci (azure container instances) service. i am working in python and i followed the following code (from the official documentation : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli) : from .core import workspace from .core import workspace # connect to workspace ws = workspace(subscription_id=\"my-subscription-id\", resource_group=\"my-ressource-group-name\", workspace_name=\"my-workspace-name\") from .core.model import model model = model.register(workspace = ws, model_path= 'model.pkl', model_name = 'my-model', description = 'my-description') %%writefile score.py import os import dill import joblib def init(): global model # get the path where the deployed model can be found model_path = os.getenv('_model_dir') # load existing model model = joblib.load('model.pkl') # handle request to the service def run(data): try: # pick out the text property of the json request # expected json details {\"text\": \"some text to evaluate\"} data = json.loads(data) prediction = model.predict(data['text']) return prediction except exception as e: error = str(e) return error from .core.environment import environment # name environment and call requirements file # requirements: numpy, tensorflow myenv = environment.from_pip_requirements(name = 'myenv', file_path = 'requirements.txt') from .core.model import inferenceconfig # create inference configuration inference_config = inferenceconfig(environment=myenv, entry_script='score.py') from .core.webservice import aciwebservice #akswebservice # set the virtual machine capabilities deployment_config = aciwebservice.deploy_configuration(cpu_cores = 0.5, memory_gb = 3) from .core.model import model # deploy ml model (azure container instances) service = model.deploy(workspace=ws, name='my-service-name', models=[model], inference_config=inference_config, deployment_config=deployment_config) service.wait_for_deployment(show_output = true) i succeded once with the previous code. i noticed that the model.deploy created a container registry with a specific name 6e07ce2cc4ac4838b42d35cda8d38616. the api was working well and i wanted to deploy an other model from scratch. i deleted the service and model from studio and the container registry from azure ressources. unfortunately i am not able to deploy again anything. for the last step (the model.deploy step), i have the following error message : service deployment polling reached non-successful terminal state, current service state: unhealthy operation id: 46243f9b-3833-4650-8d47-3ac54a39dc5e more information can be found here: https:\/\/machinelearnin2812599115.blob.core.windows.net\/\/imagelogs\/46245f8b-3833-4659-8d47-3ac54a39dc5e\/build.log?sv=2019-07-07&sr=b&sig=45kgns4sbszrqh%2fp29rhxzb7qc5nf1hj%2blbrdpxjolk%3d&st=2021-10-25t17%3a20%3a49z&se=2021-10-27t01%3a24%3a49z&sp=r error: { \"code\": \"acideploymentfailed\", \"statuscode\": 404, \"message\": \"no definition exists for environment with name: myenv version: autosave_2021-10-25t17:24:43z_b1d066bf reason: container > registry 6e07ce2cc4ac4838b42d35cda8d38616.azurecr.io not found. if private link is enabled in workspace, please verify acr is part of private > link and retry..\", \"details\": [] } i do not understand why the first time a new container registry was well created, but now it seems that it is sought (the message is saying that container registry identified by name 6e07ce2cc4ac4838b42d35cda8d38616 is missing). i never found where i can force the creation of a new container registry ressource in python, neither specify a name for it in aciwebservice.deploy_configuration or model.deploy. i tried to create the container registry by hand, but this time, this is the container that cannot be created. the output is the folloiwing : tips: you can try get_logs(): https:\/\/aka.ms\/debugimage#dockerlog or local deployment: https:\/\/aka.ms\/debugimage#debug-locally to debug if deployment takes longer than 10 minutes. running 2021-10-25 19:25:10+02:00 creating container registry if not exists. 2021-10-25 19:25:10+02:00 registering the environment. 2021-10-25 19:25:13+02:00 building image.. 2021-10-25 19:30:45+02:00 generating deployment configuration. 2021-10-25 19:30:46+02:00 submitting deployment to compute. failed service deployment polling reached non-successful terminal state, current service state: unhealthy operation id: 93780de6-7662-40d8-ab9e-4e1556ef880f current sub-operation type not known, more logs unavailable. error: { \"code\": \"inaccessibleimage\", \"statuscode\": 400, \"message\": \"aci service request failed. reason: the image '6e07ce2cc4ac4838b42d35cda8d38616.azurecr.io\/\/_684133370d8916c87f6230d213976ca5' in container group 'my-service-name-lm4hbqzebei0ltxnqnogfq' is not accessible. please check the image and registry credential.. refer to https:\/\/docs.microsoft.com\/azure\/container-registry\/container-registry-authentication#admin-account and make sure admin user is enabled for your container registry.\" } i tried to follow the recommandation of the last message saying to set admin user enabled for the container registry. unfortunately the same error message appears again and i am stuck here... does anyone could help me omving on with this? the best solution would be i think to delete totally this 6e07ce2cc4ac4838b42d35cda8d38616 container registry but i can't find where the reference is set so model.deploy always fall to find it. an other solution would be to force model.deploy to generate a new container registry, but i could find how to make that. i need your help !",
        "Question_original_content_gpt_summary":"The user is encountering challenges with deploying a machine learning model through an Azure Container Instances (ACI) service, as they are unable to delete the existing container registry and create a new one.",
        "Question_preprocessed_content":"Title: error acideploymentfailed; Content: hello, i am trying to deploy a machine learning model through an aci service. i am working in python and i followed the following code from import workspace from import workspace connect to workspace ws from import model model ws, description %%writefile import os import dill import joblib def init global model get the path where the deployed model can be found load existing model model handle request to the service def run try pick out the text property of the json request expected json details data prediction return prediction except exception as e error str return error from import environment name environment and call requirements file requirements numpy, tensorflow myenv 'myenv', from import inferenceconfig create inference configuration inferenceconfig from import aciwebservice akswebservice set the virtual machine capabilities from import model deploy ml model service models model , true i succeded once with the previous code. i noticed that the created a container registry with a specific name e ce cc ac b d cda d . the api was working well and i wanted to deploy an other model from scratch. i deleted the service and model from studio and the container registry from azure ressources. unfortunately i am not able to deploy again anything. for the last step , i have the following error message service deployment polling reached terminal state, current service state unhealthy operation id more information can be found here error code acideploymentfailed , statuscode , message no definition exists for environment with name myenv version reason container registry not found. if private link is enabled in workspace, please verify acr is part of private link and details i do not understand why the first time a new container registry was well created, but now it seems that it is sought . i never found where i can force the creation of a new container registry ressource in python, neither specify a name for it in or i tried to create the container registry by hand, but this time, this is the container that cannot be created. the output is the folloiwing tips you can try or local deployment to debug if deployment takes longer than minutes. running + creating container registry if not exists. + registering the environment. + building + generating deployment configuration. + submitting deployment to compute. failed service deployment polling reached terminal state, current service state unhealthy operation id current type not known, more logs unavailable. error code inaccessibleimage , statuscode , message aci service request failed. reason the image in container group is not accessible. please check the image and registry refer to and make sure admin user is enabled for your container i tried to follow the recommandation of the last message saying to set admin user enabled for the container registry. unfortunately the same error message appears again and i am stuck does anyone could help me omving on with this? the best solution would be i think to delete totally this e ce cc ac b d cda d container registry but i can't find where the reference is set so always fall to find it. an other solution would be to force to generate a new container registry, but i could find how to make that. i need your help !",
        "Answer_original_content":"@floriansurmont-4431 the azure container registry is actually created while creating your workspace. deleting the container registry or any of its dependent resources like storage account, keyvault and app insights will actually cause the workspace to behave inconsistently. since you have already deleted the registry and tried to attach a new one, it looks like the keys of this dependent resource are not synced with the workspace. ideally during create of a workspace you can use an existing registry with the following command. az ml workspace create -w -g --container-registry \"\/subscriptions\/\/resourcegroups\/\/providers\/microsoft.containerregistry\/registries\/\" since the workspace is already available you can use the az ml workspace update command instead to set the registry and then sync the keys. az ml workspace sync-keys -w -g i have worked with another user with a similar issue before but they did not enable admin access on registry. since you have already done so, i think the above steps should help to sync the registry with your workspace and you can try to create or update a model from your experiment. i hope this can help. if an answer is helpful, please click on or upvote which might help other community members reading this thread. thank you very much for your answer. i do understand when i made my mistake, and what happens, thank you. unfortunately i am not able to run the command you suggest: az ml workspace sync-keys -w machinelearning -g oc-ingenieur-ia indeed, i am not able to find from the cli. the error message is the following: projectsystemexception: message: workspace not found. innerexception none errorresponse { \"error\": { \"message\": \"workspace not found.\" } } when i run az ml workspace list the result is empty : []. that is quite weird, because i do have it created in the portal web interface. when i try to find the resource group name with command: az group list --subscription i do have the result: [ { \"id\": \"\/subscriptions\/\/resourcegroups\/oc-ingenieur-ia\", \"location\": \"francecentral\", \"managedby\": null, \"name\": \"oc-ingenieur-ia\", \"properties\": { \"provisioningstate\": \"succeeded\" }, \"tags\": {}, \"type\": \"microsoft.resources\/resourcegroups\" } ] but when i run either: az ml workspace list --resource-group oc-ingenieur-ia or az ml workspace list --resource-group oc-ingenieur-ia i have the following error: projectsystemexception: message: workspaces not found. innerexception none errorresponse { \"error\": { \"message\": \"workspaces not found.\" } } moreover if i try to import the workspace from python: ws = workspace(subscription_id=\"my-subscription-id\", resource_group=\"oc-ingenieur-ia\", workspace_name=\"machinelearning\") the python object iswell created and its name attribute is \"machinelearning\" as expected. i can interact with it to create a model with model.register that shows up in azure portal. any idea of what is going on?",
        "Answer_original_content_gpt_summary":"The answer suggests that deleting the container registry or any of its dependent resources like storage account, keyvault, and app insights will cause the workspace to behave inconsistently. The solution proposed is to use the az ml workspace update command to set the registry and then sync the keys. The user is also advised to check if they have admin access on the registry. The user is facing issues with finding the workspace and is advised to check if the workspace is available in the portal web interface. The user is also advised to import the workspace from Python and interact with it to create a model with model.register that shows up in the Azure portal.",
        "Answer_preprocessed_content":"the azure container registry is actually created while creating your workspace. deleting the container registry or any of its dependent resources like storage account, keyvault and app insights will actually cause the workspace to behave inconsistently. since you have already deleted the registry and tried to attach a new one, it looks like the keys of this dependent resource are not synced with the workspace. ideally during create of a workspace you can use an existing registry with the following command. az ml workspace create w g since the workspace is already available you can use the az ml workspace update command instead to set the registry and then sync the keys. az ml workspace w g i have worked with another user with a similar issue before but they did not enable admin access on registry. since you have already done so, i think the above steps should help to sync the registry with your workspace and you can try to create or update a model from your experiment. i hope this can help. if an answer is helpful, please click on or upvote which might help other community members reading this thread. thank you very much for your answer. i do understand when i made my mistake, and what happens, thank you. unfortunately i am not able to run the command you suggest az ml workspace w machinelearning g indeed, i am not able to find from the cli. the error message is the following projectsystemexception message workspace not found. innerexception none errorresponse error when i run az ml workspace list the result is empty . that is quite weird, because i do have it created in the portal web interface. when i try to find the resource group name with command az group list i do have the result id location francecentral , managedby null, name properties , tags , type but when i run either az ml workspace list or az ml workspace list i have the following error projectsystemexception message workspaces not found. innerexception none errorresponse error moreover if i try to import the workspace from python ws the python object iswell created and its name attribute is machinelearning as expected. i can interact with it to create a model with that shows up in azure portal. any idea of what is going on?"
    },
    {
        "Question_id":68709594.0,
        "Question_title":"Why does my sagemaker model need a larger instance for real-time inference",
        "Question_body":"<p>I have a sagemaker endpoint that serves a Random Forest Classifier SKLearn model. This model predicts the kind of user someone is based on there interaction on a website. <strong>I have had two releases for this model<\/strong>.<\/p>\n<p>The first model had 4 kinds of user: <code>'user-type-1', 'user-type-2', 'user-type-3', 'other'<\/code><\/p>\n<p>The second release differed from the first in that there was more training data used and there were 10 types of user <code>'user-type-1', 'user-type-2', 'user-type-3', 'user-type-4', 'user-type-5', 'user-type-6', 'user-type-7', 'user-type-8', 'user-type-9','other'<\/code>. To be more specific, in the first model many users got predicted as <code>'other'<\/code> whereas in the second model many of these users were in one of the new categories.<\/p>\n<p><strong>The parameters for the random forest was the same in both models<\/strong>.<\/p>\n<p><strong>Question:<\/strong> The second model uses a lot more memory than the first and I cannot figure out why. Both models are of similar size and the same number of calls to the endpoint are being made under the second release. Any ideas why I might need a larger instance with more memory in the second model than the first?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1628500559870,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":52.0,
        "Owner_creation_time":1521737124656,
        "Owner_last_access_time":1663943374747,
        "Owner_reputation":738.0,
        "Owner_up_votes":22.0,
        "Owner_down_votes":7.0,
        "Owner_views":69.0,
        "Answer_body":"<p>The <code>values<\/code> attribute of classification-type SkLearn tree models is essentially a data matrix <code>(n_leaves, n_classes)<\/code> (probability distributions associated with each leaf node).<\/p>\n<p>In the first experiment this matrix has 4 columns, in the second experiment it has 10 columns - a 2.5X increase in size.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1628505835360,
        "Answer_score":1.0,
        "Owner_location":"Milton Keynes",
        "Question_last_edit_time":1628501268236,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68709594",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why does my model need a larger instance for real-time inference; Content: i have a endpoint that serves a random forest classifier sklearn model. this model predicts the kind of user someone is based on there interaction on a website. i have had two releases for this model. the first model had 4 kinds of user: 'user-type-1', 'user-type-2', 'user-type-3', 'other' the second release differed from the first in that there was more training data used and there were 10 types of user 'user-type-1', 'user-type-2', 'user-type-3', 'user-type-4', 'user-type-5', 'user-type-6', 'user-type-7', 'user-type-8', 'user-type-9','other'. to be more specific, in the first model many users got predicted as 'other' whereas in the second model many of these users were in one of the new categories. the parameters for the random forest was the same in both models. question: the second model uses a lot more memory than the first and i cannot figure out why. both models are of similar size and the same number of calls to the endpoint are being made under the second release. any ideas why i might need a larger instance with more memory in the second model than the first?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with their random forest classifier sklearn model, which predicts the kind of user someone is based on their interaction on a website, as the second model requires a larger instance with more memory than the first, despite having the same parameters and similar size.",
        "Question_preprocessed_content":"Title: why does my model need a larger instance for inference; Content: i have a endpoint that serves a random forest classifier sklearn model. this model predicts the kind of user someone is based on there interaction on a website. i have had two releases for this model. the first model had kinds of user the second release differed from the first in that there was more training data used and there were types of user . to be more specific, in the first model many users got predicted as whereas in the second model many of these users were in one of the new categories. the parameters for the random forest was the same in both models. question the second model uses a lot more memory than the first and i cannot figure out why. both models are of similar size and the same number of calls to the endpoint are being made under the second release. any ideas why i might need a larger instance with more memory in the second model than the first?",
        "Answer_original_content":"the values attribute of classification-type sklearn tree models is essentially a data matrix (n_leaves, n_classes) (probability distributions associated with each leaf node). in the first experiment this matrix has 4 columns, in the second experiment it has 10 columns - a 2.5x increase in size.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge with the random forest classifier sklearn model are to either increase the instance size and memory capacity or to reduce the number of columns in the data matrix associated with each leaf node. The second model requires a larger instance with more memory than the first because it has a data matrix with 10 columns, which is 2.5 times larger than the data matrix of the first model that has only 4 columns.",
        "Answer_preprocessed_content":"the attribute of sklearn tree models is essentially a data matrix . in the first experiment this matrix has columns, in the second experiment it has columns a increase in size."
    },
    {
        "Question_id":47921875.0,
        "Question_title":"Accessing files in Mongodb",
        "Question_body":"<p>I am using sacred package in python, this allows to keep track of computational experiments i'm running. sacred allows to add observer (<code>mongodb<\/code>) which stores all sorts of information regarding the experiment (<code>configuration<\/code>, <code>source files<\/code> etc).\n<code>sacred<\/code> allows to add artifacts to the db bt using <code>sacred.Experiment.add_artifact(PATH_TO_FILE).<\/code><\/p>\n\n<p>This command essentially adds the file to the DB.<\/p>\n\n<p>I'm using MongoDB compass, I can access the experiment information and see that an artifact has been added. it contains two fields:\n'<code>name<\/code>' and '<code>file_id<\/code>' which contains an <code>ObjectId<\/code>. (see image)<\/p>\n\n<p>I am attempting to access the stored file itself. i have noticed that under my db there is an additional <code>sub-db<\/code> called <code>fs.files<\/code> in it i can filter to find my <code>ObjectId<\/code> but it does not seem to allow me to access to content of the file itself.<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/SBg8m.png\" alt=\"object id under .files\"><\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/B7ymG.png\" alt=\"file_id under artifact\/object\"><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1513848655213,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":2457.0,
        "Owner_creation_time":1468845236196,
        "Owner_last_access_time":1654806454288,
        "Owner_reputation":55.0,
        "Owner_up_votes":11.0,
        "Owner_down_votes":0.0,
        "Owner_views":16.0,
        "Answer_body":"<p>MongoDB file storage is handled by \"GridFS\" which basically splits up files in chunks and stores them in a collection (fs.files).<\/p>\n\n<p>Tutorial to access: <a href=\"http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html\" rel=\"nofollow noreferrer\">http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1513855746928,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1525602466710,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47921875",
        "Tool":"Sacred",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: accessing files in mongodb; Content: i am using package in python, this allows to keep track of computational experiments i'm running. allows to add observer (mongodb) which stores all sorts of information regarding the experiment (configuration, source files etc). allows to add artifacts to the db bt using .experiment.add_artifact(path_to_file). this command essentially adds the file to the db. i'm using mongodb compass, i can access the experiment information and see that an artifact has been added. it contains two fields: 'name' and 'file_id' which contains an objectid. (see image) i am attempting to access the stored file itself. i have noticed that under my db there is an additional sub-db called fs.files in it i can filter to find my objectid but it does not seem to allow me to access to content of the file itself.",
        "Question_original_content_gpt_summary":"The user is encountering challenges accessing the stored files in MongoDB, as they are unable to access the content of the file itself despite being able to filter to find the associated object ID.",
        "Question_preprocessed_content":"Title: accessing files in mongodb; Content: i am using package in python, this allows to keep track of computational experiments i'm running. allows to add observer which stores all sorts of information regarding the experiment . allows to add artifacts to the db bt using this command essentially adds the file to the db. i'm using mongodb compass, i can access the experiment information and see that an artifact has been added. it contains two fields '' and '' which contains an . i am attempting to access the stored file itself. i have noticed that under my db there is an additional called in it i can filter to find my but it does not seem to allow me to access to content of the file itself.",
        "Answer_original_content":"mongodb file storage is handled by \"gridfs\" which basically splits up files in chunks and stores them in a collection (fs.files). tutorial to access: http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html",
        "Answer_original_content_gpt_summary":"The solution to accessing stored files in MongoDB is to use \"gridfs,\" which splits up files into chunks and stores them in a collection called \"fs.files.\" A tutorial on how to access these files can be found at http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html.",
        "Answer_preprocessed_content":"mongodb file storage is handled by gridfs which basically splits up files in chunks and stores them in a collection . tutorial to access"
    },
    {
        "Question_id":null,
        "Question_title":"Why can't I delete multiple runs in W&B at once",
        "Question_body":"<p>As shown in the picture below, I selected multiple runs but still can\u2019t hit delete, which was ok before, did I set something wrong?<br>\nThank you for your help!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe.jpeg\" data-download-href=\"\/uploads\/short-url\/1tihGWyfSAk2zp9Y2O4VmZiNf3g.jpeg?dl=1\" title=\"20211218195831\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe_2_690x283.jpeg\" alt=\"20211218195831\" data-base62-sha1=\"1tihGWyfSAk2zp9Y2O4VmZiNf3g\" width=\"690\" height=\"283\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe_2_690x283.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe_2_1035x424.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">20211218195831<\/span><span class=\"informations\">1072\u00d7441 33.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1640127473795,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":205.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/why-cant-i-delete-multiple-runs-in-w-b-at-once\/1585",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-22T13:30:12.386Z",
                "Answer_body":"<p>Hey there, this is a known issue that we are working on right now. I\u2019ll notify you once this is fixed.<\/p>",
                "Answer_score":11.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-28T17:01:19.641Z",
                "Answer_body":"<p>Hey there, can you check if the issue still persists?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-30T01:10:49.998Z",
                "Answer_body":"<p>I\u2019m sorry I just saw your reply, this problem has been solved,  thanks!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-28T01:11:08.297Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why can't i delete multiple runs in w&b at once; Content: as shown in the picture below, i selected multiple runs but still can\u2019t hit delete, which was ok before, did i set something wrong? thank you for your help! 202112181958311072\u00d7441 33.1 kb",
        "Question_original_content_gpt_summary":"The user is unable to delete multiple runs in W&B, despite having selected them, which was previously possible.",
        "Question_preprocessed_content":"Title: why can't i delete multiple runs in w&b at once; Content: as shown in the picture below, i selected multiple runs but still cant hit delete, which was ok before, did i set something wrong? thank you for your help! kb",
        "Answer_original_content":"hey there, this is a known issue that we are working on right now. ill notify you once this is fixed. hey there, can you check if the issue still persists? im sorry i just saw your reply, this problem has been solved, thanks! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There was a known issue with W&B where users were unable to delete multiple runs despite selecting them. The issue has been fixed and the user is advised to check if the problem still persists. The topic was automatically closed after 60 days and new replies are no longer allowed.",
        "Answer_preprocessed_content":"hey there, this is a known issue that we are working on right now. ill notify you once this is fixed. hey there, can you check if the issue still persists? im sorry i just saw your reply, this problem has been solved, thanks! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":70929123.0,
        "Question_title":"AzureMLCompute job failed with `FailedLoginToImageRegistry`",
        "Question_body":"<p>I've been trying to send a train job through azure ml python sdk with:<\/p>\n<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig \n\nif __name__ == &quot;__main__&quot;:\n    ws = Workspace.from_config()\n    experiment = Experiment(workspace=ws, name='ConstructionTopicsModel')\n\n    config = ScriptRunConfig(source_directory='.\/',\n                         script='src\/azureml\/train.py',\n                         arguments=None,\n                         compute_target='ComputeTargetName',\n                         )\n\n    env = ws.environments['test-env']\n    config.run_config.environment = env\n    run = experiment.submit(config)\n    \n    run.wait_for_completion(show_output=True)\n\n    aml_url = run.get_portal_url()\n    print(aml_url)\n<\/code><\/pre>\n<p>But I was getting the <code>ServiceError<\/code> message:<\/p>\n<pre><code>AzureMLCompute job failed. FailedLoginToImageRegistry: Unable to login to docker image repo\nReason: Failed to login to the docker registry\nerror: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nserviceURL: 7ac86b04d6564d36aa80ae2ad090582c.azurecr.io\nReason: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.\n<\/code><\/pre>\n<p>I also tried using the azure cli without success, same error message<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1643645330913,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":202.0,
        "Owner_creation_time":1589293508567,
        "Owner_last_access_time":1663681781072,
        "Owner_reputation":833.0,
        "Owner_up_votes":9.0,
        "Owner_down_votes":9.0,
        "Owner_views":55.0,
        "Answer_body":"<p>The only way I've found so far to make this work, was to run it on a terminal of the compute-target itself. That's how the docker error goes away. Trying to run the experiment from a terminal of a different compute instance raises the exception.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1643645330912,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70929123",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: compute job failed with `failedlogintoimageregistry`; Content: i've been trying to send a train job through python sdk with: from .core import workspace, experiment, scriptrunconfig if __name__ == \"__main__\": ws = workspace.from_config() experiment = experiment(workspace=ws, name='constructiontopicsmodel') config = scriptrunconfig(source_directory='.\/', script='src\/\/train.py', arguments=none, compute_target='computetargetname', ) env = ws.environments['test-env'] config.run_config.environment = env run = experiment.submit(config) run.wait_for_completion(show_output=true) aml_url = run.get_portal_url() print(aml_url) but i was getting the serviceerror message: compute job failed. failedlogintoimageregistry: unable to login to docker image repo reason: failed to login to the docker registry error: warning! using --password via the cli is insecure. use --password-stdin. error saving credentials: error storing credentials - err: exit status 1, out: `cannot autolaunch d-bus without x11 $display` serviceurl: 7ac86b04d6564d36aa80ae2ad090582c.azurecr.io reason: warning! using --password via the cli is insecure. use --password-stdin. error saving credentials: error storing credentials - err: exit status 1, out: `cannot autolaunch d-bus without x11 $display` info: failed to setup runtime for job execution: job environment preparation failed on 10.0.0.5 with err exit status 1. i also tried using the azure cli without success, same error message",
        "Question_original_content_gpt_summary":"The user encountered a challenge with a compute job failing due to an error message related to logging into a docker image repository.",
        "Question_preprocessed_content":"Title: compute job failed with ; Content: i've been trying to send a train job through python sdk with but i was getting the message i also tried using the azure cli without success, same error message",
        "Answer_original_content":"the only way i've found so far to make this work, was to run it on a terminal of the compute-target itself. that's how the docker error goes away. trying to run the experiment from a terminal of a different compute instance raises the exception.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of a compute job failing due to a docker error message related to logging into a repository is to run the job on the terminal of the compute-target itself. Running the experiment from a terminal of a different compute instance will raise an exception.",
        "Answer_preprocessed_content":"the only way i've found so far to make this work, was to run it on a terminal of the itself. that's how the docker error goes away. trying to run the experiment from a terminal of a different compute instance raises the exception."
    },
    {
        "Question_id":65102618.0,
        "Question_title":"struggling to install python package via amazon sagemaker",
        "Question_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1606886205577,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1148.0,
        "Owner_creation_time":1474967309676,
        "Owner_last_access_time":1661467549403,
        "Owner_reputation":59.0,
        "Owner_up_votes":31.0,
        "Owner_down_votes":0.0,
        "Owner_views":66.0,
        "Answer_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1644193786212,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1606887753310,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: struggling to install python package via ; Content: i am using and trying to install gaapi4py package via anaconda python3 notebook. so far i've tried the following commands: %conda install gaapi4py and conda install gaapi4py got same error: collecting package metadata (current_repodata.json): failed condahttperror: http 000 connection failed for url <https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json> elapsed: - an http error occurred when trying to retrieve this url. http errors are often intermittent, and a simple retry will get you on your way. 'https:\/\/conda.anaconda.org\/conda-forge\/linux-64' note: you may need to restart the kernel to use updated packages. alternatively i've tried the below but it failed as well: pip install gaapi4py error text: warning: retrying (retry(total=4, connect=none, read=none, redirect=none, status=none)) after connection broken by 'newconnectionerror('<pip._vendor.urllib3.connection.verifiedhttpsconnection object at 0x7f657c803c50>: failed to establish a new connection: [errno 101] network is unreachable',)': \/simple\/gaapi4py\/ warning: retrying (retry(total=3, connect=none, read=none, redirect=none, status=none)) after connection broken by 'newconnectionerror('<pip._vendor.urllib3.connection.verifiedhttpsconnection object at 0x7f657c8035f8>: failed to establish a new connection: [errno 101] network is unreachable',)': \/simple\/gaapi4py\/ warning: retrying (retry(total=2, connect=none, read=none, redirect=none, status=none)) after connection broken by 'newconnectionerror('<pip._vendor.urllib3.connection.verifiedhttpsconnection object at 0x7f657c803550>: failed to establish a new connection: [errno 101] network is unreachable',)': \/simple\/gaapi4py\/ warning: retrying (retry(total=1, connect=none, read=none, redirect=none, status=none)) after connection broken by 'newconnectionerror('<pip._vendor.urllib3.connection.verifiedhttpsconnection object at 0x7f657c803400>: failed to establish a new connection: [errno 101] network is unreachable',)': \/simple\/gaapi4py\/ warning: retrying (retry(total=0, connect=none, read=none, redirect=none, status=none)) after connection broken by 'newconnectionerror('<pip._vendor.urllib3.connection.verifiedhttpsconnection object at 0x7f657c803358>: failed to establish a new connection: [errno 101] network is unreachable',)': \/simple\/gaapi4py\/ error: could not find a version that satisfies the requirement gaapi4py (from versions: none) error: no matching distribution found for gaapi4py warning: you are using pip version 20.0.2; however, version 20.3 is available. you should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command. note: you may need to restart the kernel to use updated packages. what am i doing wrong? all previous packages worked well. upd: tried also as recommended in amazon book: import sys !{sys.executable} -m pip install gaapi4py and import sys !conda install -y --prefix {sys.prefix} gaapi4py both didn't work neither, getting same errors as above.",
        "Question_original_content_gpt_summary":"The user is struggling to install the gaapi4py package via Anaconda Python3 notebook, despite trying multiple commands and receiving various errors.",
        "Question_preprocessed_content":"Title: struggling to install python package via ; Content: i am using and trying to install gaapi py package via anaconda python notebook. so far i've tried the following commands and alternatively i've tried the below but it failed as well error text what am i doing wrong? all previous packages worked well. upd tried also as recommended in amazon book and both didn't work neither, getting same errors as above.",
        "Answer_original_content":"after talking back-in-forth with our it department i figured out that custom libraries installation was blocked for security reasons.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Contact the IT department to request permission to install custom libraries.\n- Use a different package or library that is already installed and approved for use.",
        "Answer_preprocessed_content":"after talking with our it department i figured out that custom libraries installation was blocked for security reasons."
    },
    {
        "Question_id":73592371.0,
        "Question_title":"start, monitor and define script of SageMaker processing job from local machine",
        "Question_body":"<p>I am looking at <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, which makes all sense. Let us focus on this bit of code:<\/p>\n<pre><code>from sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    inputs=[\n        ProcessingInput(source=&quot;s3:\/\/your-bucket\/path\/to\/your\/data&quot;, destination=&quot;\/opt\/ml\/processing\/input&quot;),\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n\npreprocessing_job_description = sklearn_processor.jobs[-1].describe() \n<\/code><\/pre>\n<p>Here preprocessing.py has to be obviously in the cloud. I am curious, could one also put scripts into an S3 bucket and trigger the job remotely. I can easily to this with hyper parameter optimisation, which does not require dedicated scripts though as I use an OOTB training image.<\/p>\n<p>In this case I can fire off the job like so:<\/p>\n<pre><code>tuning_job_name = &quot;amazing-hpo-job-&quot; + strftime(&quot;%d-%H-%M-%S&quot;, gmtime())\n\nsmclient = boto3.Session().client(&quot;sagemaker&quot;)\nsmclient.create_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name,\n    HyperParameterTuningJobConfig=tuning_job_config,\n    TrainingJobDefinition=training_job_definition\n)\n<\/code><\/pre>\n<p>and then monitor the job's progress:<\/p>\n<pre><code>smclient = boto3.Session().client(&quot;sagemaker&quot;)\n\ntuning_job_result = smclient.describe_hyper_parameter_tuning_job(\n    HyperParameterTuningJobName=tuning_job_name\n)\n\nstatus = tuning_job_result[&quot;HyperParameterTuningJobStatus&quot;]\nif status != &quot;Completed&quot;:\n    print(&quot;Reminder: the tuning job has not been completed.&quot;)\n\njob_count = tuning_job_result[&quot;TrainingJobStatusCounters&quot;][&quot;Completed&quot;]\nprint(&quot;%d training jobs have completed&quot; % job_count)\n\nobjective = tuning_job_result[&quot;HyperParameterTuningJobConfig&quot;][&quot;HyperParameterTuningJobObjective&quot;]\nis_minimize = objective[&quot;Type&quot;] != &quot;Maximize&quot;\nobjective_name = objective[&quot;MetricName&quot;]\n\nif tuning_job_result.get(&quot;BestTrainingJob&quot;, None):\n    print(&quot;Best model found so far:&quot;)\n    pprint(tuning_job_result[&quot;BestTrainingJob&quot;])\nelse:\n    print(&quot;No training jobs have reported results yet.&quot;) \n<\/code><\/pre>\n<p>I would think starting and monitoring a SageMaker processing job from a local machine should be possible as with an HPO job but what about the script(s)? Ideally I would like to develop and test them locally and the run remotely. Hope this makes sense?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1662209372887,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779312,
        "Owner_reputation":15705.0,
        "Owner_up_votes":2171.0,
        "Owner_down_votes":91.0,
        "Owner_views":2150.0,
        "Answer_body":"<p>Im not sure I understand the comparison to a Tuning Job.<\/p>\n<p>Based on what you have described, in this case the <code>preprocessing.py<\/code> is actually stored locally. The SageMaker SDK will upload it to S3 for the remote Processing Job to access it. I suggest launching the Job and then taking a look at the inputs in the SageMaker Console.<\/p>\n<p>If you wanted to test the Processing Job locally you can do so using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a>. This will basically imitate the Job locally which aids in debugging the script before kicking off a remote Processing Job. Kindly note docker is required to make use of Local Mode.<\/p>\n<p>Example code for local mode:<\/p>\n<pre><code>from sagemaker.local import LocalSession\nfrom sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n\nsagemaker_session = LocalSession()\nsagemaker_session.config = {'local': {'local_code': True}}\n\n# For local training a dummy role will be sufficient\nrole = 'arn:aws:iam::111111111111:role\/service-role\/AmazonSageMaker-ExecutionRole-20200101T000001'\n\nprocessor = ScriptProcessor(command=['python3'],\n                    image_uri='sagemaker-scikit-learn-processing-local',\n                    role=role,\n                    instance_count=1,\n                    instance_type='local')\n\nprocessor.run(code='processing_script.py',\n                    inputs=[ProcessingInput(\n                        source='.\/input_data\/',\n                        destination='\/opt\/ml\/processing\/input_data\/')],\n                    outputs=[ProcessingOutput(\n                        output_name='word_count_data',\n                        source='\/opt\/ml\/processing\/processed_data\/')],\n                    arguments=['job-type', 'word-count']\n                    )\n\npreprocessing_job_description = processor.jobs[-1].describe()\noutput_config = preprocessing_job_description['ProcessingOutputConfig']\n\nprint(output_config)\n\nfor output in output_config['Outputs']:\n    if output['OutputName'] == 'word_count_data':\n        word_count_data_file = output['S3Output']['S3Uri']\n\nprint('Output file is located on: {}'.format(word_count_data_file))\n\n\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_creation_time":1662504468407,
        "Answer_score":1.0,
        "Owner_location":"Somewhere",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73592371",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: start, monitor and define script of processing job from local machine; Content: i am looking at this, which makes all sense. let us focus on this bit of code: from .processing import processinginput, processingoutput sklearn_processor.run( code=\"preprocessing.py\", inputs=[ processinginput(source=\"s3:\/\/your-bucket\/path\/to\/your\/data\", destination=\"\/opt\/ml\/processing\/input\"), ], outputs=[ processingoutput(output_name=\"train_data\", source=\"\/opt\/ml\/processing\/train\"), processingoutput(output_name=\"test_data\", source=\"\/opt\/ml\/processing\/test\"), ], arguments=[\"--train-test-split-ratio\", \"0.2\"], ) preprocessing_job_description = sklearn_processor.jobs[-1].describe() here preprocessing.py has to be obviously in the cloud. i am curious, could one also put scripts into an s3 bucket and trigger the job remotely. i can easily to this with hyper parameter optimisation, which does not require dedicated scripts though as i use an ootb training image. in this case i can fire off the job like so: tuning_job_name = \"amazing-hpo-job-\" + strftime(\"%d-%h-%m-%s\", gmtime()) smclient = boto3.session().client(\"\") smclient.create_hyper_parameter_tuning_job( hyperparametertuningjobname=tuning_job_name, hyperparametertuningjobconfig=tuning_job_config, trainingjobdefinition=training_job_definition ) and then monitor the job's progress: smclient = boto3.session().client(\"\") tuning_job_result = smclient.describe_hyper_parameter_tuning_job( hyperparametertuningjobname=tuning_job_name ) status = tuning_job_result[\"hyperparametertuningjobstatus\"] if status != \"completed\": print(\"reminder: the tuning job has not been completed.\") job_count = tuning_job_result[\"trainingjobstatuscounters\"][\"completed\"] print(\"%d training jobs have completed\" % job_count) objective = tuning_job_result[\"hyperparametertuningjobconfig\"][\"hyperparametertuningjobobjective\"] is_minimize = objective[\"type\"] != \"maximize\" objective_name = objective[\"metricname\"] if tuning_job_result.get(\"besttrainingjob\", none): print(\"best model found so far:\") pprint(tuning_job_result[\"besttrainingjob\"]) else: print(\"no training jobs have reported results yet.\") i would think starting and monitoring a processing job from a local machine should be possible as with an hpo job but what about the script(s)? ideally i would like to develop and test them locally and the run remotely. hope this makes sense?",
        "Question_original_content_gpt_summary":"The user is looking for a way to start, monitor, and define a script of processing job from a local machine, and is curious if they can put scripts into an S3 bucket and trigger the job remotely.",
        "Question_preprocessed_content":"Title: start, monitor and define script of processing job from local machine; Content: i am looking at this, which makes all sense. let us focus on this bit of code here has to be obviously in the cloud. i am curious, could one also put scripts into an s bucket and trigger the job remotely. i can easily to this with hyper parameter optimisation, which does not require dedicated scripts though as i use an ootb training image. in this case i can fire off the job like so and then monitor the job's progress i would think starting and monitoring a processing job from a local machine should be possible as with an hpo job but what about the script ? ideally i would like to develop and test them locally and the run remotely. hope this makes sense?",
        "Answer_original_content":"im not sure i understand the comparison to a tuning job. based on what you have described, in this case the preprocessing.py is actually stored locally. the sdk will upload it to s3 for the remote processing job to access it. i suggest launching the job and then taking a look at the inputs in the console. if you wanted to test the processing job locally you can do so using local mode. this will basically imitate the job locally which aids in debugging the script before kicking off a remote processing job. kindly note docker is required to make use of local mode. example code for local mode: from .local import localsession from .processing import scriptprocessor, processinginput, processingoutput _session = localsession() _session.config = {'local': {'local_code': true}} # for local training a dummy role will be sufficient role = 'arn:aws:iam::111111111111:role\/service-role\/amazon-executionrole-20200101t000001' processor = scriptprocessor(command=['python3'], image_uri='-scikit-learn-processing-local', role=role, instance_count=1, instance_type='local') processor.run(code='processing_script.py', inputs=[processinginput( source='.\/input_data\/', destination='\/opt\/ml\/processing\/input_data\/')], outputs=[processingoutput( output_name='word_count_data', source='\/opt\/ml\/processing\/processed_data\/')], arguments=['job-type', 'word-count'] ) preprocessing_job_description = processor.jobs[-1].describe() output_config = preprocessing_job_description['processingoutputconfig'] print(output_config) for output in output_config['outputs']: if output['outputname'] == 'word_count_data': word_count_data_file = output['s3output']['s3uri'] print('output file is located on: {}'.format(word_count_data_file))",
        "Answer_original_content_gpt_summary":"The answer does not provide any solutions to the user's question. Instead, it clarifies that the preprocessing script is stored locally and can be uploaded to an S3 bucket for remote processing. The answer also suggests using local mode to test the processing job before launching it remotely. The example code for local mode is also provided.",
        "Answer_preprocessed_content":"im not sure i understand the comparison to a tuning job. based on what you have described, in this case the is actually stored locally. the sdk will upload it to s for the remote processing job to access it. i suggest launching the job and then taking a look at the inputs in the console. if you wanted to test the processing job locally you can do so using local mode. this will basically imitate the job locally which aids in debugging the script before kicking off a remote processing job. kindly note docker is required to make use of local mode. example code for local mode"
    },
    {
        "Question_id":null,
        "Question_title":"Why I am logging same plot all over again?",
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to log plt plot as wandb.Image in my sweep, but I have an issue. Wandb.log will log only one first one and then it logs it all over again. Can you help me guys? Pasting my log plot code.<\/p>\n<pre><code class=\"lang-python\">        for name, index in zip(names, indexes):\n            print(index)\n            pca_values = PCA().fit_transform(np.append(y_pred[:,index], y_valid[:,index],0))\n            c_map = [\"red\"] * y_pred.shape[0] + [\"green\"] * y_valid.shape[0]\n\n\n            plt.scatter(pca_values[:,0], pca_values[:,1],c=c_map, s=400,alpha=0.3)\n            for i in range(pca_values.shape[0]):\n                label = f\"P-{i}\" if i &lt; y_pred.shape[0] else str(i - y_pred.shape[0])\n                plt.text(pca_values[i,0], pca_values[i,1], label, ha=\"center\", va=\"center\", color='black')\n            plt.grid('minor')\n            plt.title(name)\n\n            wandb.log(\n                {\n                    f\"{name}_plot\" : wandb.Image(plt)\n                }\n            )\n<\/code><\/pre>\n<p>Can someone help me? It is possible that I have an issue not understanding plt correctly\u2026<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1676364432266,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":41.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/why-i-am-logging-same-plot-all-over-again\/3878",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-14T16:27:25.047Z",
                "Answer_body":"<p>Hi William!<\/p>\n<p>When you are talking about plt, are you talking about the <code>matplotlib<\/code> library?<\/p>\n<p>Cheers,<br>\nArtsiom<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-15T14:37:10.639Z",
                "Answer_body":"<p>yes, I am using matplotlib.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-21T16:04:31.627Z",
                "Answer_body":"<p>Hi William!<br>\nAlthough <code>maltplotlib<\/code> does create an image of the graph, we do have an integration with their library to where you should be able to log it directly like this from our documentation:<\/p>\n<pre><code>import matplotlib.pyplot as pltplt.plot([1, 2, 3, 4])plt.ylabel(\"some interesting numbers\")wandb.log({\"chart\": plt})\n<\/code><\/pre>\n<p>Also, I would advise you to check if your step\/global step is going up if you are running into an issue with your plot being overwritten.<\/p>",
                "Answer_score":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why i am logging same plot all over again?; Content: hi, i am trying to log plt plot as .image in my sweep, but i have an issue. .log will log only one first one and then it logs it all over again. can you help me guys? pasting my log plot code. for name, index in zip(names, indexes): print(index) pca_values = pca().fit_transform(np.append(y_pred[:,index], y_valid[:,index],0)) c_map = [\"red\"] * y_pred.shape[0] + [\"green\"] * y_valid.shape[0] plt.scatter(pca_values[:,0], pca_values[:,1],c=c_map, s=400,alpha=0.3) for i in range(pca_values.shape[0]): label = f\"p-{i}\" if i < y_pred.shape[0] else str(i - y_pred.shape[0]) plt.text(pca_values[i,0], pca_values[i,1], label, ha=\"center\", va=\"center\", color='black') plt.grid('minor') plt.title(name) .log( { f\"{name}_plot\" : .image(plt) } ) can someone help me? it is possible that i have an issue not understanding plt correctly",
        "Question_original_content_gpt_summary":"The user is encountering an issue with logging a plt plot as an .image in a sweep, as the .log command only logs the first plot and then logs it all over again.",
        "Question_preprocessed_content":"Title: why i am logging same plot all over again?; Content: hi, i am trying to log plt plot as in my sweep, but i have an issue. will log only one first one and then it logs it all over again. can you help me guys? pasting my log plot code. can someone help me? it is possible that i have an issue not understanding plt correctly",
        "Answer_original_content":"hi william! when you are talking about plt, are you talking about the matplotlib library? cheers, artsiom yes, i am using matplotlib. hi william! although maltplotlib does create an image of the graph, we do have an integration with their library to where you should be able to log it directly like this from our documentation: import matplotlib.pyplot as pltplt.plot([1, 2, 3, 4])plt.ylabel(\"some interesting numbers\").log({\"chart\": plt}) also, i would advise you to check if your step\/global step is going up if you are running into an issue with your plot being overwritten.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. Use the integration with the matplotlib library to log the plot directly.\n2. Check if the step\/global step is going up to avoid overwriting the plot.",
        "Answer_preprocessed_content":"hi william! when you are talking about plt, are you talking about the library? cheers, artsiom yes, i am using matplotlib. hi william! although does create an image of the graph, we do have an integration with their library to where you should be able to log it directly like this from our documentation also, i would advise you to check if your step is going up if you are running into an issue with your plot being overwritten."
    },
    {
        "Question_id":54797698.0,
        "Question_title":"SageMaker delete Models and Endpoint configurations with python API",
        "Question_body":"<p>I've tried deleting\/recreating endpoints with the same name, and wasted a lot of time before I realized that changes do not get applied unless you also delete the corresponding Model and Endpoint configuration so that new ones can be created with that name. <\/p>\n\n<p>Is there a way with the sagemaker python api to delete all three instead of just the endpoint?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1550711269467,
        "Question_favorite_count":1.0,
        "Question_score":4.0,
        "Question_view_count":4760.0,
        "Owner_creation_time":1361339272692,
        "Owner_last_access_time":1663965928400,
        "Owner_reputation":6281.0,
        "Owner_up_votes":430.0,
        "Owner_down_votes":17.0,
        "Owner_views":958.0,
        "Answer_body":"<p>It looks like AWS is currently in the process of supporting model deletion via API with <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/647\" rel=\"nofollow noreferrer\" title=\"sagemaker-python-sdk\/pull\/647\">this<\/a> pull request. <\/p>\n\n<p>For the time being Amazon's only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\" rel=\"nofollow noreferrer\" title=\"docs.aws.amazon.com\/sagemaker\">recommendation<\/a> is to delete everything via the console. <\/p>\n\n<p>If this is critical to your system you can probably manage everything via Cloud Formation and create\/delete services containing your Sagemaker models and endpoints.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1550714082127,
        "Answer_score":2.0,
        "Owner_location":"NYC",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54797698",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: delete models and endpoint configurations with python api; Content: i've tried deleting\/recreating endpoints with the same name, and wasted a lot of time before i realized that changes do not get applied unless you also delete the corresponding model and endpoint configuration so that new ones can be created with that name. is there a way with the python api to delete all three instead of just the endpoint?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with deleting\/recreating endpoints with the same name, and is looking for a way to delete all three (model, endpoint configuration, and endpoint) with the Python API.",
        "Question_preprocessed_content":"Title: delete models and endpoint configurations with python api; Content: i've tried endpoints with the same name, and wasted a lot of time before i realized that changes do not get applied unless you also delete the corresponding model and endpoint configuration so that new ones can be created with that name. is there a way with the python api to delete all three instead of just the endpoint?",
        "Answer_original_content":"it looks like aws is currently in the process of supporting model deletion via api with this pull request. for the time being amazon's only recommendation is to delete everything via the console. if this is critical to your system you can probably manage everything via cloud formation and create\/delete services containing your models and endpoints.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Wait for AWS to support model deletion via API with the pull request.\n- Delete everything via the console.\n- Manage everything via cloud formation and create\/delete services containing models and endpoints.",
        "Answer_preprocessed_content":"it looks like aws is currently in the process of supporting model deletion via api with this pull request. for the time being amazon's only recommendation is to delete everything via the console. if this is critical to your system you can probably manage everything via cloud formation and services containing your models and endpoints."
    },
    {
        "Question_id":63138835.0,
        "Question_title":"How to save models trained locally in Amazon SageMaker?",
        "Question_body":"<p>I'm trying to use a local training job in SageMaker.<\/p>\n<p>Following this AWS notebook (<a href=\"http:\/\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb<\/a>) I was able to train and predict locally.<\/p>\n<p>There is any way to train locally and save the trained model in the Amazon SageMaker Training Job section?\nOtherwise, how can I properly save trained models I trained using local mode?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1595954217667,
        "Question_favorite_count":1.0,
        "Question_score":7.0,
        "Question_view_count":3195.0,
        "Owner_creation_time":1464391892936,
        "Owner_last_access_time":1658153265652,
        "Owner_reputation":2243.0,
        "Owner_up_votes":497.0,
        "Owner_down_votes":32.0,
        "Owner_views":148.0,
        "Answer_body":"<p>There is no way to have your local mode training jobs appear in the AWS console. The intent of local mode is to allow for faster iteration\/debugging before using SageMaker for training your model.<\/p>\n<p>You can create SageMaker Models from local model artifacts. Compress your model artifacts into a <code>.tar.gz<\/code> file, upload that file to S3, and then create the Model (with the SDK or in the console).<\/p>\n<p>Documentation:<\/p>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker<\/a><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1596039571220,
        "Answer_score":2.0,
        "Owner_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Question_last_edit_time":1595959142728,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63138835",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to save models trained locally in ?; Content: i'm trying to use a local training job in . following this aws notebook (https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb) i was able to train and predict locally. there is any way to train locally and save the trained model in the training job section? otherwise, how can i properly save trained models i trained using local mode?",
        "Question_original_content_gpt_summary":"The user is trying to use a local training job in AWS and is looking for a way to save the trained model in the training job section or to properly save trained models trained using local mode.",
        "Question_preprocessed_content":"Title: how to save models trained locally in ?; Content: i'm trying to use a local training job in . following this aws notebook i was able to train and predict locally. there is any way to train locally and save the trained model in the training job section? otherwise, how can i properly save trained models i trained using local mode?",
        "Answer_original_content":"there is no way to have your local mode training jobs appear in the aws console. the intent of local mode is to allow for faster iteration\/debugging before using for training your model. you can create models from local model artifacts. compress your model artifacts into a .tar.gz file, upload that file to s3, and then create the model (with the sdk or in the console). documentation: https:\/\/.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon- https:\/\/docs.aws.amazon.com\/\/latest\/apireference\/api_createmodel.html",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n- There is no way to have local mode training jobs appear in the AWS console.\n- Local mode is intended for faster iteration\/debugging before using for training your model.\n- You can create models from local model artifacts.\n- Compress your model artifacts into a .tar.gz file, upload that file to S3, and then create the model (with the SDK or in the console).\n\nIn summary, the user cannot save the trained model in the training job section using local mode in AWS. However, they can create models from local model artifacts and upload them to S3 to create the model using the SDK or console.",
        "Answer_preprocessed_content":"there is no way to have your local mode training jobs appear in the aws console. the intent of local mode is to allow for faster before using for training your model. you can create models from local model artifacts. compress your model artifacts into a file, upload that file to s , and then create the model . documentation"
    },
    {
        "Question_id":null,
        "Question_title":"How to make use of Labelled Images in AzureML Designer?",
        "Question_body":"Hello! I have been labeling images in AzureML Data Labeling. I wish to use this labelled set in Designer, to prototype some model ideas. However, I cannot get this to work. Any output (Dataset, COCO or csv) seems not to be compatible with \"Convert to Image Directory\".\n\nMy question is quite similar to one asked over a year ago - https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/32203\/how-to-use-labeled-image-datasets-to-perform-an-im.html - the answer suggests a result was imminent. Has there been any update?\n\nIf there is not one individual module capable of parsing this information, is there a way to use multiple modules to import the data?\n\nThanks!\n\n\n\n\nEDIT: The problem is also discussed here:\nhttps:\/\/docs.microsoft.com\/en-us\/answers\/questions\/194940\/how-to-use-azuremldataset.html\nIs there a cleaner solution yet? Or is downloading it, converting to pandas, then reuploading the best thing to do?",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1628873927937,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/513343\/how-to-make-use-of-labelled-images-in-azureml-desi.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-02T15:30:10.037Z",
                "Answer_score":1,
                "Answer_body":"Hello @andrewblance-4822\n\nSorry I didn't hear anything from product team, what I can do now is I will continue following up with them to see any new ways for this and also I can help you enable a support ticket to escalate this issue for more awareness. Please let me know if you do not have a support plan, I can help you to enable a one time free ticket regarding to this issue. Thanks.\n\nRegards,\nYutong",
                "Answer_comment_count":4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to make use of labelled images in designer?; Content: hello! i have been labeling images in data labeling. i wish to use this labelled set in designer, to prototype some model ideas. however, i cannot get this to work. any output (dataset, coco or csv) seems not to be compatible with \"convert to image directory\". my question is quite similar to one asked over a year ago - https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/32203\/how-to-use-labeled-image-datasets-to-perform-an-im.html - the answer suggests a result was imminent. has there been any update? if there is not one individual module capable of parsing this information, is there a way to use multiple modules to import the data? thanks! edit: the problem is also discussed here: https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/194940\/how-to-use-dataset.html is there a cleaner solution yet? or is downloading it, converting to pandas, then reuploading the best thing to do?",
        "Question_original_content_gpt_summary":"The user is struggling to use labelled images in Designer to prototype model ideas, and is looking for an update on a solution suggested over a year ago.",
        "Question_preprocessed_content":"Title: how to make use of labelled images in designer?; Content: hello! i have been labeling images in data labeling. i wish to use this labelled set in designer, to prototype some model ideas. however, i cannot get this to work. any output seems not to be compatible with convert to image directory . my question is quite similar to one asked over a year ago the answer suggests a result was imminent. has there been any update? if there is not one individual module capable of parsing this information, is there a way to use multiple modules to import the data? thanks! edit the problem is also discussed here is there a cleaner solution yet? or is downloading it, converting to pandas, then reuploading the best thing to do?",
        "Answer_original_content":"hello @andrewblance-4822 sorry i didn't hear anything from product team, what i can do now is i will continue following up with them to see any new ways for this and also i can help you enable a support ticket to escalate this issue for more awareness. please let me know if you do not have a support plan, i can help you to enable a one time free ticket regarding to this issue. thanks. regards, yutong",
        "Answer_original_content_gpt_summary":"The answer does not provide any specific solution to the user's problem of using labelled images in Designer to prototype model ideas. However, the responder offers to follow up with the product team to see if there are any new ways to address the issue. Additionally, they offer to enable a support ticket to escalate the issue for more awareness and can help the user enable a one-time free ticket if they do not have a support plan.",
        "Answer_preprocessed_content":"hello sorry i didn't hear anything from product team, what i can do now is i will continue following up with them to see any new ways for this and also i can help you enable a support ticket to escalate this issue for more awareness. please let me know if you do not have a support plan, i can help you to enable a one time free ticket regarding to this issue. thanks. regards, yutong"
    },
    {
        "Question_id":63518174.0,
        "Question_title":"using gunicorn for nested folders",
        "Question_body":"<p>I'm new to gunicorn and heroku so I would appreciate any help. I want to deploy my python Dash app on to heroku and I know I need a Procfile. The thing is that my project structure uses the Kedro structure and my structure looks like this:<\/p>\n<pre><code>myproject\n    .... # Kedro-generated files\n    src\/\n        package1\/\n            package2\/\n                __init__.py\n                index.py\n    Procfile\n<\/code><\/pre>\n<p>index.py is a Dash application like so<\/p>\n<pre><code>#imports up here\n\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\nserver = app.server\n\n.......  # main code chunk\n\nif __name__ == '__main__':\napp.run_server(debug=True)\n<\/code><\/pre>\n<p>Currently, my Procfile looks like this:<\/p>\n<pre><code>web: gunicorn src frontend.index:app\n<\/code><\/pre>\n<p>My project uploads to heroku just fine but I'm getting this error in my log:<\/p>\n<pre><code>2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 941, in _find_and_load_unlocked\n2020-08-21T06:46:46.433935+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n2020-08-21T06:46:46.433936+00:00 app[web.1]: File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 953, in _find_and_load_unlocked\n2020-08-21T06:46:46.433962+00:00 app[web.1]: ModuleNotFoundError: No module named 'frontend'\n2020-08-21T06:46:46.434082+00:00 app[web.1]: [2020-08-21 06:46:46 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:46:46.464346+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.464367+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 202, in run\n2020-08-21T06:46:46.464715+00:00 app[web.1]: self.manage_workers()\n2020-08-21T06:46:46.464732+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 545, in manage_workers\n2020-08-21T06:46:46.465049+00:00 app[web.1]: self.spawn_workers()\n2020-08-21T06:46:46.465054+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 617, in spawn_workers\n2020-08-21T06:46:46.465412+00:00 app[web.1]: time.sleep(0.1 * random.random())\n2020-08-21T06:46:46.465417+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.465617+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.465622+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.465905+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.465950+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.465964+00:00 app[web.1]: \n2020-08-21T06:46:46.465965+00:00 app[web.1]: During handling of the above exception, another exception occurred:\n2020-08-21T06:46:46.465965+00:00 app[web.1]: \n2020-08-21T06:46:46.465969+00:00 app[web.1]: Traceback (most recent call last):\n2020-08-21T06:46:46.465969+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n2020-08-21T06:46:46.466103+00:00 app[web.1]: sys.exit(run())\n2020-08-21T06:46:46.466107+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n2020-08-21T06:46:46.466254+00:00 app[web.1]: WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n2020-08-21T06:46:46.466258+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n2020-08-21T06:46:46.466464+00:00 app[web.1]: super().run()\n2020-08-21T06:46:46.466470+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n2020-08-21T06:46:46.466601+00:00 app[web.1]: Arbiter(self).run()\n2020-08-21T06:46:46.466606+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n2020-08-21T06:46:46.466790+00:00 app[web.1]: self.halt(reason=inst.reason, exit_status=inst.exit_status)\n2020-08-21T06:46:46.466794+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n2020-08-21T06:46:46.467031+00:00 app[web.1]: self.stop()\n2020-08-21T06:46:46.467032+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n2020-08-21T06:46:46.467262+00:00 app[web.1]: time.sleep(0.1)\n2020-08-21T06:46:46.467267+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n2020-08-21T06:46:46.467468+00:00 app[web.1]: self.reap_workers()\n2020-08-21T06:46:46.467469+00:00 app[web.1]: File &quot;\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n2020-08-21T06:46:46.467750+00:00 app[web.1]: raise HaltServer(reason, self.WORKER_BOOT_ERROR)\n2020-08-21T06:46:46.467754+00:00 app[web.1]: gunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n2020-08-21T06:46:46.559947+00:00 heroku[web.1]: Process exited with status 1\n2020-08-21T06:46:46.610907+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:47:03.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:49:12.915422+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=781ff03f-db0d-40ad-996f-1d25ff3fd026 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:49:13.357185+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=51bff951-d0fa-4e01-ba38-60f44cbe373b fwd=&quot;18.217.223.118&quot; dyno= connect= service= status=503 bytes= protocol=http\n2020-08-21T06:49:13.955353+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=632cc0a9-e052-43c9-a90c-62f99dfbba5c fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n<\/code><\/pre>\n<pre><code>2020-08-21T06:52:18.372623+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:32.487313+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:34.595212+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:34.595933+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:17241 (4)\n2020-08-21T06:52:34.596051+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:34.600183+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:34.603725+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.603887+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:34.626625+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:52:34.629877+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:34.629978+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:52:34.733270+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:34.733356+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:34.800675+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:34.837697+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:52:34.839731+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:52:49.188229+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index`\n2020-08-21T06:52:50.000000+00:00 app[api]: Build succeeded\n2020-08-21T06:52:51.154243+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:52:51.154956+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:46031 (4)\n2020-08-21T06:52:51.155075+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:52:51.158999+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:52:51.162147+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:52:51.162261+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:52:51.189291+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:52:51.189375+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:52:51.249579+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:52:51.281288+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:53:27.313026+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/&quot; host=protected-coast-07061.herokuapp.com request_id=67b1f83d-37a8-4ad1-b522-2e7cc0bd7b7d fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:53:28.196639+00:00 heroku[router]: at=error code=H10 desc=&quot;App crashed&quot; method=GET path=&quot;\/favicon.ico&quot; host=protected-coast-07061.herokuapp.com request_id=0d13d80e-ca9b-4857-970e-47cfcf602017 fwd=&quot;115.66.91.134&quot; dyno= connect= service= status=503 bytes= protocol=https\n2020-08-21T06:57:12.000000+00:00 app[api]: Build started by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Deploy 1f77e9e8 by user \n2020-08-21T06:58:51.667324+00:00 app[api]: Release v12 created by user \n2020-08-21T06:58:51.832220+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T06:59:07.062252+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T06:59:10.383357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T06:59:10.384213+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:54641 (4)\n2020-08-21T06:59:10.384357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Using worker: sync\n2020-08-21T06:59:10.388913+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T06:59:10.392276+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.392426+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T06:59:10.403239+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Booting worker with pid: 11\n2020-08-21T06:59:10.407880+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T06:59:10.408006+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [INFO] Worker exiting (pid: 11)\n2020-08-21T06:59:10.525402+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T06:59:10.525558+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [INFO] Reason: App failed to load.\n2020-08-21T06:59:10.607473+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T06:59:11.643239+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T06:59:54.000000+00:00 app[api]: Build succeeded\n2020-08-21T07:08:53.300472+00:00 heroku[web.1]: State changed from crashed to starting\n2020-08-21T07:09:16.319403+00:00 heroku[web.1]: Starting process with command `gunicorn src frontend.index:app`\n2020-08-21T07:09:19.182910+00:00 heroku[web.1]: Process exited with status 4\n2020-08-21T07:09:19.228761+00:00 heroku[web.1]: State changed from starting to crashed\n2020-08-21T07:09:19.057971+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Starting gunicorn 20.0.4\n2020-08-21T07:09:19.058760+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Listening at: http:\/\/0.0.0.0:25408 (4)\n2020-08-21T07:09:19.058888+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Using worker: sync\n2020-08-21T07:09:19.063236+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Booting worker with pid: 10\n2020-08-21T07:09:19.066629+00:00 app[web.1]: Failed to find attribute 'application' in 'src'.\n2020-08-21T07:09:19.066758+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [INFO] Worker exiting (pid: 10)\n2020-08-21T07:09:19.102247+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Shutting down: Master\n2020-08-21T07:09:19.102349+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [INFO] Reason: App failed to load.\n<\/code><\/pre>\n<p>Apologies, I am new to this so I am not sure where to even start with the debugging as well. To summarise: I think my gunicorn is not firing as my line may be wrong; and I am not sure what is causing my app to not launch. How do I solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1597994063423,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":586.0,
        "Owner_creation_time":1597993100672,
        "Owner_last_access_time":1613368758792,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":"<p>The logic of gunicorn is the following:\n<code>.<\/code> (dot) for directories, <code>:<\/code> (column) for objects defined inside a file.<\/p>\n<p>Assuming the given structure, you should have something like this:<\/p>\n<pre><code>$ cat Procfile\nweb: gunicorn src.package1.package2.index:app\n<\/code><\/pre>\n<p>[EDIT] If you get an error, you should consider using <code>server<\/code> instead of <code>app<\/code>. As an example, these files are from <a href=\"https:\/\/gitlab.com\/qmeeus\/datathon\" rel=\"nofollow noreferrer\">one of my old projects<\/a> (also a Dash app):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># app.py\n\nimport flask\nfrom src import dashboard\n\nserver = flask.Flask(__name__)\nserver.secret_key = os.environ.get('secret_key', str(randint(0, 1000000)))\napp = dashboard.main(server)\n\nif __name__ == '__main__':\n    app.server.run(debug=True, threaded=True)\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code># Procfile\nweb: gunicorn app:server --timeout 300\n<\/code><\/pre>\n<pre class=\"lang-sh prettyprint-override\"><code>$ ls *\nProcfile app.py\n\nsrc:\nconfig.py  dashboard.py ...\n \n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1597994468467,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1597995299663,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63518174",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: using gunicorn for nested folders; Content: i'm new to gunicorn and heroku so i would appreciate any help. i want to deploy my python dash app on to heroku and i know i need a procfile. the thing is that my project structure uses the structure and my structure looks like this: myproject .... # -generated files src\/ package1\/ package2\/ __init__.py index.py procfile index.py is a dash application like so #imports up here app = dash.dash(__name__, external_stylesheets=external_stylesheets) server = app.server ....... # main code chunk if __name__ == '__main__': app.run_server(debug=true) currently, my procfile looks like this: web: gunicorn src frontend.index:app my project uploads to heroku just fine but i'm getting this error in my log: 2020-08-21t06:46:46.433935+00:00 app[web.1]: file \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked 2020-08-21t06:46:46.433935+00:00 app[web.1]: file \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed 2020-08-21t06:46:46.433936+00:00 app[web.1]: file \"<frozen importlib._bootstrap>\", line 994, in _gcd_import 2020-08-21t06:46:46.433936+00:00 app[web.1]: file \"<frozen importlib._bootstrap>\", line 971, in _find_and_load 2020-08-21t06:46:46.433936+00:00 app[web.1]: file \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked 2020-08-21t06:46:46.433962+00:00 app[web.1]: modulenotfounderror: no module named 'frontend' 2020-08-21t06:46:46.434082+00:00 app[web.1]: [2020-08-21 06:46:46 +0000] [11] [info] worker exiting (pid: 11) 2020-08-21t06:46:46.464346+00:00 app[web.1]: traceback (most recent call last): 2020-08-21t06:46:46.464367+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 202, in run 2020-08-21t06:46:46.464715+00:00 app[web.1]: self.manage_workers() 2020-08-21t06:46:46.464732+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 545, in manage_workers 2020-08-21t06:46:46.465049+00:00 app[web.1]: self.spawn_workers() 2020-08-21t06:46:46.465054+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 617, in spawn_workers 2020-08-21t06:46:46.465412+00:00 app[web.1]: time.sleep(0.1 * random.random()) 2020-08-21t06:46:46.465417+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 242, in handle_chld 2020-08-21t06:46:46.465617+00:00 app[web.1]: self.reap_workers() 2020-08-21t06:46:46.465622+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 525, in reap_workers 2020-08-21t06:46:46.465905+00:00 app[web.1]: raise haltserver(reason, self.worker_boot_error) 2020-08-21t06:46:46.465950+00:00 app[web.1]: gunicorn.errors.haltserver: <haltserver 'worker failed to boot.' 3> 2020-08-21t06:46:46.465964+00:00 app[web.1]: 2020-08-21t06:46:46.465965+00:00 app[web.1]: during handling of the above exception, another exception occurred: 2020-08-21t06:46:46.465965+00:00 app[web.1]: 2020-08-21t06:46:46.465969+00:00 app[web.1]: traceback (most recent call last): 2020-08-21t06:46:46.465969+00:00 app[web.1]: file \"\/app\/.heroku\/python\/bin\/gunicorn\", line 8, in <module> 2020-08-21t06:46:46.466103+00:00 app[web.1]: sys.exit(run()) 2020-08-21t06:46:46.466107+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/wsgiapp.py\", line 58, in run 2020-08-21t06:46:46.466254+00:00 app[web.1]: wsgiapplication(\"%(prog)s [options] [app_module]\").run() 2020-08-21t06:46:46.466258+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py\", line 228, in run 2020-08-21t06:46:46.466464+00:00 app[web.1]: super().run() 2020-08-21t06:46:46.466470+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/app\/base.py\", line 72, in run 2020-08-21t06:46:46.466601+00:00 app[web.1]: arbiter(self).run() 2020-08-21t06:46:46.466606+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 229, in run 2020-08-21t06:46:46.466790+00:00 app[web.1]: self.halt(reason=inst.reason, exit_status=inst.exit_status) 2020-08-21t06:46:46.466794+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 342, in halt 2020-08-21t06:46:46.467031+00:00 app[web.1]: self.stop() 2020-08-21t06:46:46.467032+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 393, in stop 2020-08-21t06:46:46.467262+00:00 app[web.1]: time.sleep(0.1) 2020-08-21t06:46:46.467267+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 242, in handle_chld 2020-08-21t06:46:46.467468+00:00 app[web.1]: self.reap_workers() 2020-08-21t06:46:46.467469+00:00 app[web.1]: file \"\/app\/.heroku\/python\/lib\/python3.6\/site-packages\/gunicorn\/arbiter.py\", line 525, in reap_workers 2020-08-21t06:46:46.467750+00:00 app[web.1]: raise haltserver(reason, self.worker_boot_error) 2020-08-21t06:46:46.467754+00:00 app[web.1]: gunicorn.errors.haltserver: <haltserver 'worker failed to boot.' 3> 2020-08-21t06:46:46.559947+00:00 heroku[web.1]: process exited with status 1 2020-08-21t06:46:46.610907+00:00 heroku[web.1]: state changed from starting to crashed 2020-08-21t06:47:03.000000+00:00 app[api]: build succeeded 2020-08-21t06:49:12.915422+00:00 heroku[router]: at=error code=h10 desc=\"app crashed\" method=get path=\"\/\" host=protected-coast-07061.herokuapp.com request_id=781ff03f-db0d-40ad-996f-1d25ff3fd026 fwd=\"115.66.91.134\" dyno= connect= service= status=503 bytes= protocol=https 2020-08-21t06:49:13.357185+00:00 heroku[router]: at=error code=h10 desc=\"app crashed\" method=get path=\"\/\" host=protected-coast-07061.herokuapp.com request_id=51bff951-d0fa-4e01-ba38-60f44cbe373b fwd=\"18.217.223.118\" dyno= connect= service= status=503 bytes= protocol=http 2020-08-21t06:49:13.955353+00:00 heroku[router]: at=error code=h10 desc=\"app crashed\" method=get path=\"\/favicon.ico\" host=protected-coast-07061.herokuapp.com request_id=632cc0a9-e052-43c9-a90c-62f99dfbba5c fwd=\"115.66.91.134\" dyno= connect= service= status=503 bytes= protocol=https 2020-08-21t06:52:18.372623+00:00 heroku[web.1]: state changed from crashed to starting 2020-08-21t06:52:32.487313+00:00 heroku[web.1]: starting process with command `gunicorn src frontend.index` 2020-08-21t06:52:34.595212+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [info] starting gunicorn 20.0.4 2020-08-21t06:52:34.595933+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [info] listening at: http:\/\/0.0.0.0:17241 (4) 2020-08-21t06:52:34.596051+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [info] using worker: sync 2020-08-21t06:52:34.600183+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [info] booting worker with pid: 10 2020-08-21t06:52:34.603725+00:00 app[web.1]: failed to find attribute 'application' in 'src'. 2020-08-21t06:52:34.603887+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [10] [info] worker exiting (pid: 10) 2020-08-21t06:52:34.626625+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [info] booting worker with pid: 11 2020-08-21t06:52:34.629877+00:00 app[web.1]: failed to find attribute 'application' in 'src'. 2020-08-21t06:52:34.629978+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [11] [info] worker exiting (pid: 11) 2020-08-21t06:52:34.733270+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [info] shutting down: master 2020-08-21t06:52:34.733356+00:00 app[web.1]: [2020-08-21 06:52:34 +0000] [4] [info] reason: app failed to load. 2020-08-21t06:52:34.800675+00:00 heroku[web.1]: process exited with status 4 2020-08-21t06:52:34.837697+00:00 heroku[web.1]: state changed from starting to crashed 2020-08-21t06:52:34.839731+00:00 heroku[web.1]: state changed from crashed to starting 2020-08-21t06:52:49.188229+00:00 heroku[web.1]: starting process with command `gunicorn src frontend.index` 2020-08-21t06:52:50.000000+00:00 app[api]: build succeeded 2020-08-21t06:52:51.154243+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [info] starting gunicorn 20.0.4 2020-08-21t06:52:51.154956+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [info] listening at: http:\/\/0.0.0.0:46031 (4) 2020-08-21t06:52:51.155075+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [info] using worker: sync 2020-08-21t06:52:51.158999+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [info] booting worker with pid: 10 2020-08-21t06:52:51.162147+00:00 app[web.1]: failed to find attribute 'application' in 'src'. 2020-08-21t06:52:51.162261+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [10] [info] worker exiting (pid: 10) 2020-08-21t06:52:51.189291+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [info] shutting down: master 2020-08-21t06:52:51.189375+00:00 app[web.1]: [2020-08-21 06:52:51 +0000] [4] [info] reason: app failed to load. 2020-08-21t06:52:51.249579+00:00 heroku[web.1]: process exited with status 4 2020-08-21t06:52:51.281288+00:00 heroku[web.1]: state changed from starting to crashed 2020-08-21t06:53:27.313026+00:00 heroku[router]: at=error code=h10 desc=\"app crashed\" method=get path=\"\/\" host=protected-coast-07061.herokuapp.com request_id=67b1f83d-37a8-4ad1-b522-2e7cc0bd7b7d fwd=\"115.66.91.134\" dyno= connect= service= status=503 bytes= protocol=https 2020-08-21t06:53:28.196639+00:00 heroku[router]: at=error code=h10 desc=\"app crashed\" method=get path=\"\/favicon.ico\" host=protected-coast-07061.herokuapp.com request_id=0d13d80e-ca9b-4857-970e-47cfcf602017 fwd=\"115.66.91.134\" dyno= connect= service= status=503 bytes= protocol=https 2020-08-21t06:57:12.000000+00:00 app[api]: build started by user 2020-08-21t06:58:51.667324+00:00 app[api]: deploy 1f77e9e8 by user 2020-08-21t06:58:51.667324+00:00 app[api]: release v12 created by user 2020-08-21t06:58:51.832220+00:00 heroku[web.1]: state changed from crashed to starting 2020-08-21t06:59:07.062252+00:00 heroku[web.1]: starting process with command `gunicorn src frontend.index:app` 2020-08-21t06:59:10.383357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [info] starting gunicorn 20.0.4 2020-08-21t06:59:10.384213+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [info] listening at: http:\/\/0.0.0.0:54641 (4) 2020-08-21t06:59:10.384357+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [info] using worker: sync 2020-08-21t06:59:10.388913+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [info] booting worker with pid: 10 2020-08-21t06:59:10.392276+00:00 app[web.1]: failed to find attribute 'application' in 'src'. 2020-08-21t06:59:10.392426+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [10] [info] worker exiting (pid: 10) 2020-08-21t06:59:10.403239+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [info] booting worker with pid: 11 2020-08-21t06:59:10.407880+00:00 app[web.1]: failed to find attribute 'application' in 'src'. 2020-08-21t06:59:10.408006+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [11] [info] worker exiting (pid: 11) 2020-08-21t06:59:10.525402+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [info] shutting down: master 2020-08-21t06:59:10.525558+00:00 app[web.1]: [2020-08-21 06:59:10 +0000] [4] [info] reason: app failed to load. 2020-08-21t06:59:10.607473+00:00 heroku[web.1]: process exited with status 4 2020-08-21t06:59:11.643239+00:00 heroku[web.1]: state changed from starting to crashed 2020-08-21t06:59:54.000000+00:00 app[api]: build succeeded 2020-08-21t07:08:53.300472+00:00 heroku[web.1]: state changed from crashed to starting 2020-08-21t07:09:16.319403+00:00 heroku[web.1]: starting process with command `gunicorn src frontend.index:app` 2020-08-21t07:09:19.182910+00:00 heroku[web.1]: process exited with status 4 2020-08-21t07:09:19.228761+00:00 heroku[web.1]: state changed from starting to crashed 2020-08-21t07:09:19.057971+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [info] starting gunicorn 20.0.4 2020-08-21t07:09:19.058760+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [info] listening at: http:\/\/0.0.0.0:25408 (4) 2020-08-21t07:09:19.058888+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [info] using worker: sync 2020-08-21t07:09:19.063236+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [info] booting worker with pid: 10 2020-08-21t07:09:19.066629+00:00 app[web.1]: failed to find attribute 'application' in 'src'. 2020-08-21t07:09:19.066758+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [10] [info] worker exiting (pid: 10) 2020-08-21t07:09:19.102247+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [info] shutting down: master 2020-08-21t07:09:19.102349+00:00 app[web.1]: [2020-08-21 07:09:19 +0000] [4] [info] reason: app failed to load. apologies, i am new to this so i am not sure where to even start with the debugging as well. to summarise: i think my gunicorn is not firing as my line may be wrong; and i am not sure what is causing my app to not launch. how do i solve this issue?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with using gunicorn to deploy a Python Dash app on Heroku, due to the project structure using nested folders and an incorrect Procfile.",
        "Question_preprocessed_content":"Title: using gunicorn for nested folders; Content: i'm new to gunicorn and heroku so i would appreciate any help. i want to deploy my python dash app on to heroku and i know i need a procfile. the thing is that my project structure uses the structure and my structure looks like this is a dash application like so currently, my procfile looks like this my project uploads to heroku just fine but i'm getting this error in my log apologies, i am new to this so i am not sure where to even start with the debugging as well. to summarise i think my gunicorn is not firing as my line may be wrong; and i am not sure what is causing my app to not launch. how do i solve this issue?",
        "Answer_original_content":"the logic of gunicorn is the following: . (dot) for directories, : (column) for objects defined inside a file. assuming the given structure, you should have something like this: $ cat procfile web: gunicorn src.package1.package2.index:app [edit] if you get an error, you should consider using server instead of app. as an example, these files are from one of my old projects (also a dash app): # app.py import flask from src import dashboard server = flask.flask(__name__) server.secret_key = os.environ.get('secret_key', str(randint(0, 1000000))) app = dashboard.main(server) if __name__ == '__main__': app.server.run(debug=true, threaded=true) # procfile web: gunicorn app:server --timeout 300 $ ls * procfile app.py src: config.py dashboard.py ...",
        "Answer_original_content_gpt_summary":"The solution to the challenge of using gunicorn to deploy a Python Dash app on Heroku with nested folders and an incorrect Procfile is to use the correct syntax for the Procfile. The syntax for gunicorn is to use a dot for directories and a colon for objects defined inside a file. The Procfile should look like this: \"web: gunicorn src.package1.package2.index:app\". If an error occurs, consider using \"server\" instead of \"app\".",
        "Answer_preprocessed_content":"the logic of gunicorn is the following for directories, for objects defined inside a file. assuming the given structure, you should have something like this edit if you get an error, you should consider using instead of . as an example, these files are from one of my old projects"
    },
    {
        "Question_id":null,
        "Question_title":"Voice\/Speech to Text Train Model",
        "Question_body":"Hi.\n\nSo I would like to create a model that 'listens' to audio from movies\/podcasts (with subtitles) then returns the text transcript from it. Problem is, it's in a language not supported by Azure (or most of the big cloud providers). How would I go about and, from scratch, build a model that is trained on the audio from a new language? The input audio all will have subtitles or captions.\n\nI tried Azure ML studio but I couldn't create datasets with audio files. Not sure if I missed something there. Also tried Speech studio but it only supports a select number of languages. Would that be possible at all?\n\nAny suggestions would be appreciated. Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1627330507383,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/490113\/voicespeech-to-text-train-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-04T23:39:40.253Z",
                "Answer_score":0,
                "Answer_body":"@NathanCarns-0092 Yes, you are correct, to develop a model for speech to text we need a deep learning model here. This is out of the scope of Azure Machine Learning Studio(classic). But I think Azure Machine Learning service should support it, please refer to this: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-deep-learning-vs-machine-learning#machine-translation\n\nI have found one post which may help: https:\/\/towardsdatascience.com\/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706\n\nMoreover, I have forwarded your feedback to see any plan here for Nigerian in Azure.\n\nThanks.\nYutong",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: voice\/speech to text train model; Content: hi. so i would like to create a model that 'listens' to audio from movies\/podcasts (with subtitles) then returns the text transcript from it. problem is, it's in a language not supported by azure (or most of the big cloud providers). how would i go about and, from scratch, build a model that is trained on the audio from a new language? the input audio all will have subtitles or captions. i tried studio but i couldn't create datasets with audio files. not sure if i missed something there. also tried speech studio but it only supports a select number of languages. would that be possible at all? any suggestions would be appreciated. thanks.",
        "Question_original_content_gpt_summary":"The user is looking to create a model that can convert audio from movies\/podcasts in a language not supported by Azure or other cloud providers into text transcripts, and is unsure of how to go about building such a model from scratch.",
        "Question_preprocessed_content":"Title: to text train model; Content: hi. so i would like to create a model that 'listens' to audio from then returns the text transcript from it. problem is, it's in a language not supported by azure . how would i go about and, from scratch, build a model that is trained on the audio from a new language? the input audio all will have subtitles or captions. i tried studio but i couldn't create datasets with audio files. not sure if i missed something there. also tried speech studio but it only supports a select number of languages. would that be possible at all? any suggestions would be appreciated. thanks.",
        "Answer_original_content":"@nathancarns-0092 yes, you are correct, to develop a model for speech to text we need a deep learning model here. this is out of the scope of studio(classic). but i think service should support it, please refer to this: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-deep-learning-vs-machine-learning#machine-translation i have found one post which may help: https:\/\/towardsdatascience.com\/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706 moreover, i have forwarded your feedback to see any plan here for nigerian in azure. thanks. yutong",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include using a deep learning model to develop a speech-to-text model, referring to Microsoft's documentation on deep learning versus machine learning, and checking out a post on automatic speech recognition. The answer also mentions forwarding the user's feedback to see if there are any plans for Nigerian language support in Azure.",
        "Answer_preprocessed_content":"yes, you are correct, to develop a model for speech to text we need a deep learning model here. this is out of the scope of studio . but i think service should support it, please refer to this i have found one post which may help moreover, i have forwarded your feedback to see any plan here for nigerian in azure. thanks. yutong"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Jupyter notebook",
        "Question_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1606707121000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-30T04:05:24.000Z",
                "Answer_score":0,
                "Answer_body":"Some useful points:\n\nThe typical arguments of cloud vs local will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\nSageMaker notebooks already run in an explicit IAM context (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run sagemaker.get_execution_role()\nPre-built environments for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\nLinux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\nIf you started using SageMaker Studio, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: jupyter notebook; Content: what are the advantages of using jupyter instance instead of running it locally? is there a special integration with that we lose it if we do not use jupyer instance?",
        "Question_original_content_gpt_summary":"The user is questioning the advantages of using a Jupyter instance instead of running it locally, and whether there is a special integration with Jupyter that would be lost if they do not use the instance.",
        "Question_preprocessed_content":"Title: jupyter notebook; Content: what are the advantages of using jupyter instance instead of running it locally? is there a special integration with that we lose it if we do not use jupyer instance?",
        "Answer_original_content":"some useful points: the typical arguments of cloud vs local will apply (as with e.g. cloud9, workspaces, etc): can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc. notebooks already run in an explicit iam context (via assigned execution role) - so you don't need to log in e.g. as you would through the cli on local machine... can just run .get_execution_role() pre-built environments for a range of use-cases (e.g. generic data science, tensorflow, pytorch, mxnet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop. linux-based environments, which typically makes for a shorter path to production code than mac\/windows. if you started using studio, then yes there are some native integrations such as the uis for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
        "Answer_original_content_gpt_summary":"Possible solutions mentioned in the answer are: \n- Using a Jupyter instance in the cloud can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, and right-size the environment for what workloads you need to do on a given day.\n- Notebooks already run in an explicit IAM context, so you don't need to log in as you would through the CLI on a local machine.\n- Pre-built environments for a range of use-cases with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance.\n- Linux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\n- If you started using studio, then there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring, easy sharing of notebook snapshots, and whatever else might be announced over the next couple of weeks.",
        "Answer_preprocessed_content":"some useful points the typical arguments of cloud vs local will apply can your work from the lifetime of your laptop, keep things running when your local machine is shut down, the environment for what workloads you need to do on a given day, etc. notebooks already run in an explicit iam context so you don't need to log in as you would through the cli on local can just run environments for a range of with libraries already installed, and easy of the environment by stopping & starting the instance no more environment soup on your local laptop. environments, which typically makes for a shorter path to production code than if you started using studio, then yes there are some native integrations such as the uis for experiment tracking and endpoint easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks."
    },
    {
        "Question_id":null,
        "Question_title":"Internal server error while deploying scoring endpoint",
        "Question_body":"Dear Azure community,\n\nUnfortunately, I am currently failing to deploy a scoring endpoint via ml studio or via the azure cli. I always get an internal server error (500). It's not because of the qutoas (I already requested additional ones).\n\nThe specific configurations and models would be those from the Azure Samples on GitHub (azureml-examples\\cli\\endpoints\\online\\model-1\\onlinescoring\\score.py and azureml-examples\\cli\\endpoints\\online\\managed\\sample\\endpoint.yml and blue-deployment.yml\n\nMy own configurations are based on this 1:1. Both configurations are not a problem locally and run smoothly. The server error only comes when deploying online.\n\nUnfortunately I don't have any further error information from the 500.\n\nRegion: West Europe\nSubscription: Visual Studio Enterprise - MPN\n\nHas anyone ever had this problem? Could someone please help me here?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1638274714127,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/646024\/internal-server-error-while-deploying-scoring-endp.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-30T16:33:17.527Z",
                "Answer_score":0,
                "Answer_body":"@ChristophGattermayr-6000 Thanks for the details, Please share details of your experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end? This option is available from the top right hand corner of the portal by clicking the smiley face, Please select the option Microsoft can email you about the feedback along with a screen shot so our service team can lookup and advise through email.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: internal server error while deploying scoring endpoint; Content: dear azure community, unfortunately, i am currently failing to deploy a scoring endpoint via ml studio or via the azure cli. i always get an internal server error (500). it's not because of the qutoas (i already requested additional ones). the specific configurations and models would be those from the azure samples on github (-examples\\cli\\endpoints\\online\\model-1\\onlinescoring\\score.py and -examples\\cli\\endpoints\\online\\managed\\sample\\endpoint.yml and blue-deployment.yml my own configurations are based on this 1:1. both configurations are not a problem locally and run smoothly. the server error only comes when deploying online. unfortunately i don't have any further error information from the 500. region: west europe subscription: visual studio enterprise - mpn has anyone ever had this problem? could someone please help me here?",
        "Question_original_content_gpt_summary":"The user is encountering an internal server error (500) while attempting to deploy a scoring endpoint via ML Studio or Azure CLI, despite the fact that their configurations and models are based on Azure samples and run smoothly locally.",
        "Question_preprocessed_content":"Title: internal server error while deploying scoring endpoint; Content: dear azure community, unfortunately, i am currently failing to deploy a scoring endpoint via ml studio or via the azure cli. i always get an internal server error . it's not because of the qutoas . the specific configurations and models would be those from the azure samples on github and and my own configurations are based on this . both configurations are not a problem locally and run smoothly. the server error only comes when deploying online. unfortunately i don't have any further error information from the . region west europe subscription visual studio enterprise mpn has anyone ever had this problem? could someone please help me here?",
        "Answer_original_content":"@christophgattermayr-6000 thanks for the details, please share details of your experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end? this option is available from the top right hand corner of the portal by clicking the smiley face, please select the option microsoft can email you about the feedback along with a screen shot so our service team can lookup and advise through email.",
        "Answer_original_content_gpt_summary":"The possible solution suggested in the answer is to share the details of the experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end. This can be done by clicking on the smiley face in the top right-hand corner of the portal and selecting the option to allow Microsoft to email the user about the feedback along with a screenshot. The service team will then advise through email.",
        "Answer_preprocessed_content":"thanks for the details, please share details of your experiment and issue from the portal for a service engineer to lookup the issue from the this option is available from the top right hand corner of the portal by clicking the smiley face, please select the option microsoft can email you about the feedback along with a screen shot so our service team can lookup and advise through email."
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML compute not able to access Azure MLS workspace blob( not in vnet) during automl experiment execution",
        "Question_body":"Hi Team,\n\nI'm trying to run the automl code from the examples (https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/how-to-use-azureml\/automated-machine-learning\/regression-explanation-featurization) in Azure MLS which is not in virtual network. While running the experiment, it is getting failed with the below error.\n\nAzureMLCompute job failed.\nBFSMountError: Unable to mount blob fuse file system\nInfo: Could not mount Azure Blob Container azureml-blobstore-xxxx at workspaceblobstore: Unauthorized. Cannot access the storage account with the given account key. Please verify that the account key is valid.\nInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.4 with err exit status 1.\n\nNot sure why the AzureML is not able to access its own blobstorage to place the model artifacts.\nThe AzureML and the workspace blob both are not in virtual network.\n\nWorkarounds tried:\n1) Tried to register the workspace blob container (azureml-blobstore-<ID>) as per the link here (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data), but still getting the same error.\n\nNote: The workspace blob storage keys are synced and can able to access the notebooks and data in AzureML, Is this causing the issue?\n\nAs per the ticket :- https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/35043\/azure-machine-learning-resync-keys-not-working-no.html\n\nAre the storage keys cached in the storage connection strings at the backend ? however the error message is different, in the reference ticket it says not able to access the resource, but in my case it is not able mount to the azure-ml-<ID> container.\n\nCould you please help on it.\n\nThanks in advance.",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1631627630167,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/551634\/azure-ml-compute-not-able-to-access-azure-mls-work.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-14T21:51:14.653Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nCould you please let me know where you try to do that? I tried in my Azure Machine Learning Notebook in the studio and everything works fine for me.\n\nPlease let me know more details and let's see if we can figure this out here. If we need more environment details, I will recommend raising a ticket for this.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-15T04:40:21.273Z",
                "Answer_score":0,
                "Answer_body":"Thanks for your response.\n\nI'm able to connect to workspace and create the compute cluster, I'm getting the error while submitting the experiment on the AML compute.\n\nIt is throwing unauthorized error while trying to place the Automl artifacts folder in the AML workspace default blobstore.\n\n\n\n\n\n\n\nI tired explicitly registering the AML workspace blob container (azureml-blobstore-<ID>) as per the link here (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data), but still getting the same error.\n\nIs it because of resyncing azure blob keys with azure MLS?",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: compute not able to access s workspace blob( not in vnet) during automl experiment execution; Content: hi team, i'm trying to run the automl code from the examples (https:\/\/github.com\/azure\/machinelearningnotebooks\/tree\/master\/how-to-use-\/automated-machine-learning\/regression-explanation-featurization) in s which is not in virtual network. while running the experiment, it is getting failed with the below error. compute job failed. bfsmounterror: unable to mount blob fuse file system info: could not mount azure blob container -blobstore-xxxx at workspaceblobstore: unauthorized. cannot access the storage account with the given account key. please verify that the account key is valid. info: failed to setup runtime for job execution: job environment preparation failed on 10.0.0.4 with err exit status 1. not sure why the is not able to access its own blobstorage to place the model artifacts. the and the workspace blob both are not in virtual network. workarounds tried: 1) tried to register the workspace blob container (-blobstore-) as per the link here (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data), but still getting the same error. note: the workspace blob storage keys are synced and can able to access the notebooks and data in , is this causing the issue? as per the ticket :- https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/35043\/azure-machine-learning-resync-keys-not-working-no.html are the storage keys cached in the storage connection strings at the backend ? however the error message is different, in the reference ticket it says not able to access the resource, but in my case it is not able mount to the azure-ml- container. could you please help on it. thanks in advance.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where their compute is unable to access their workspace blob (not in a virtual network) during an Automl experiment execution, despite trying various workarounds.",
        "Question_preprocessed_content":"Title: compute not able to access s workspace blob during automl experiment execution; Content: hi team, i'm trying to run the automl code from the examples in s which is not in virtual network. while running the experiment, it is getting failed with the below error. compute job failed. bfsmounterror unable to mount blob fuse file system info could not mount azure blob container at workspaceblobstore unauthorized. cannot access the storage account with the given account key. please verify that the account key is valid. info failed to setup runtime for job execution job environment preparation failed on with err exit status . not sure why the is not able to access its own blobstorage to place the model artifacts. the and the workspace blob both are not in virtual network. workarounds tried tried to register the workspace blob container as per the link here , but still getting the same error. note the workspace blob storage keys are synced and can able to access the notebooks and data in , is this causing the issue? as per the ticket are the storage keys cached in the storage connection strings at the backend ? however the error message is different, in the reference ticket it says not able to access the resource, but in my case it is not able mount to the container. could you please help on it. thanks in advance.",
        "Answer_original_content":"hello, could you please let me know where you try to do that? i tried in my notebook in the studio and everything works fine for me. please let me know more details and let's see if we can figure this out here. if we need more environment details, i will recommend raising a ticket for this. regards, yutong thanks for your response. i'm able to connect to workspace and create the compute cluster, i'm getting the error while submitting the experiment on the aml compute. it is throwing unauthorized error while trying to place the automl artifacts folder in the aml workspace default blobstore. i tired explicitly registering the aml workspace blob container (-blobstore-) as per the link here (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data), but still getting the same error. is it because of resyncing azure blob keys with s?",
        "Answer_original_content_gpt_summary":"There are no specific solutions provided in the answer. The responder is asking for more details and suggesting raising a ticket if needed.",
        "Answer_preprocessed_content":"hello, could you please let me know where you try to do that? i tried in my notebook in the studio and everything works fine for me. please let me know more details and let's see if we can figure this out here. if we need more environment details, i will recommend raising a ticket for this. regards, yutong thanks for your response. i'm able to connect to workspace and create the compute cluster, i'm getting the error while submitting the experiment on the aml compute. it is throwing unauthorized error while trying to place the automl artifacts folder in the aml workspace default blobstore. i tired explicitly registering the aml workspace blob container as per the link here , but still getting the same error. is it because of resyncing azure blob keys with s?"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker Model Spend",
        "Question_body":"If I deploy a SageMaker model, am I incurring hosting charges even while no one is accessing my model?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1592312639000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":71.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlNS8ujYmQqePwWS-mgso3Q\/sage-maker-model-spend",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-16T13:24:24.000Z",
                "Answer_score":0,
                "Answer_body":"When you deploy a SageMaker model, it deploys it behind a SageMaker endpoint for real-time inference. You are charged by the second for on-demand ML hosting. Check the model deployment section of each region on the SageMaker Pricing page. In some use cases, you can save on inference cost by hosting several models behind the same endpoint (check this blog post).",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: model spend; Content: if i deploy a model, am i incurring hosting charges even while no one is accessing my model?",
        "Question_original_content_gpt_summary":"The user is wondering if they will incur hosting charges for a deployed model even if no one is accessing it.",
        "Question_preprocessed_content":"Title: model spend; Content: if i deploy a model, am i incurring hosting charges even while no one is accessing my model?",
        "Answer_original_content":"when you deploy a model, it deploys it behind a endpoint for real-time inference. you are charged by the second for on-demand ml hosting. check the model deployment section of each region on the pricing page. in some use cases, you can save on inference cost by hosting several models behind the same endpoint (check this blog post).",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Check the model deployment section of each region on the pricing page to understand the hosting charges.\n- Consider hosting several models behind the same endpoint to save on inference cost.",
        "Answer_preprocessed_content":"when you deploy a model, it deploys it behind a endpoint for inference. you are charged by the second for ml hosting. check the model deployment section of each region on the pricing page. in some use cases, you can save on inference cost by hosting several models behind the same endpoint ."
    },
    {
        "Question_id":58541260.0,
        "Question_title":"Difference between git-lfs and dvc",
        "Question_body":"<p>What is the difference between these two? We used git-lfs in my previous job and we are starting to use dvc alongside git in my current one. They both place some kind of index instead of file and can be downloaded on demand. Has dvc some improvements over the former one?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1571919586097,
        "Question_favorite_count":5.0,
        "Question_score":27.0,
        "Question_view_count":6255.0,
        "Owner_creation_time":1373630643248,
        "Owner_last_access_time":1664025275128,
        "Owner_reputation":382.0,
        "Owner_up_votes":185.0,
        "Owner_down_votes":1.0,
        "Owner_views":11.0,
        "Answer_body":"<p>DVC is a better replacement for <code>git-lfs<\/code>. <\/p>\n\n<p>Unlike git-lfs, DVC doesn't require installing a dedicated server; It can be used on-premises (NAS, SSH, for example) or with any major cloud provider (S3, Google Cloud, Azure).<\/p>\n\n<p>For more information: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning\" rel=\"noreferrer\">https:\/\/dvc.org\/doc\/use-cases\/data-and-model-files-versioning<\/a><\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1571925277763,
        "Answer_score":10.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58541260",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: difference between git-lfs and ; Content: what is the difference between these two? we used git-lfs in my previous job and we are starting to use alongside git in my current one. they both place some kind of index instead of file and can be downloaded on demand. has some improvements over the former one?",
        "Question_original_content_gpt_summary":"The user is seeking to understand the differences between git-lfs and , and whether the latter has any improvements over the former.",
        "Question_preprocessed_content":"Title: difference between and ; Content: what is the difference between these two? we used in my previous job and we are starting to use alongside git in my current one. they both place some kind of index instead of file and can be downloaded on demand. has some improvements over the former one?",
        "Answer_original_content":"is a better replacement for git-lfs. unlike git-lfs, doesn't require installing a dedicated server; it can be used on-premises (nas, ssh, for example) or with any major cloud provider (s3, google cloud, azure). for more information: https:\/\/.org\/doc\/use-cases\/data-and-model-files-versioning",
        "Answer_original_content_gpt_summary":"The answer suggests that is a better replacement for git-lfs as it does not require a dedicated server and can be used on-premises or with major cloud providers. More information can be found at https:\/\/.org\/doc\/use-cases\/data-and-model-files-versioning.",
        "Answer_preprocessed_content":"is a better replacement for . unlike doesn't require installing a dedicated server; it can be used or with any major cloud provider . for more information"
    },
    {
        "Question_id":null,
        "Question_title":"Notebook Instance Types for SageMaker Studio",
        "Question_body":"Within SageMaker Studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon-sagemaker-studio\/). However, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.\n\nIs there a way to change to other instance types for SageMaker Studio? For SageMaker Notebook Instances, I know you can change to many other types of instances, but I am not sure how to do it for SageMaker Studio.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1593107198000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":343.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOd5vfn4FRjGvGjac4d00PQ\/notebook-instance-types-for-sage-maker-studio",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-25T18:02:17.000Z",
                "Answer_score":0,
                "Answer_body":"The instance types you are seeing are Fast Launch Instances ( which are instance types designed to launch in under two minutes).\n\nIn order to see all the types of instances, click on the switch on top of the instance type list that says \"Fast Launch\", that should display the rest of available instances.\n\nHere is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks.html\n\nHope it helps!",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: notebook instance types for studio; Content: within studio, you can change instance types (see screenshots here:https:\/\/aws.amazon.com\/blogs\/machine-learning\/learn-how-to-select-ml-instances-on-the-fly-in-amazon--studio\/). however, this seems to only support changing to: ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large. is there a way to change to other instance types for studio? for notebook instances, i know you can change to many other types of instances, but i am not sure how to do it for studio.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in changing instance types for Amazon Studio, as the available options are limited to ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large.",
        "Question_preprocessed_content":"Title: notebook instance types for studio; Content: within studio, you can change instance types . however, this seems to only support changing to and is there a way to change to other instance types for studio? for notebook instances, i know you can change to many other types of instances, but i am not sure how to do it for studio.",
        "Answer_original_content":"the instance types you are seeing are fast launch instances ( which are instance types designed to launch in under two minutes). in order to see all the types of instances, click on the switch on top of the instance type list that says \"fast launch\", that should display the rest of available instances. here is additional info about fast launch instances: https:\/\/docs.aws.amazon.com\/\/latest\/dg\/notebooks.html hope it helps!",
        "Answer_original_content_gpt_summary":"The solution to the user's challenge in changing instance types for Amazon Studio is to click on the switch on top of the instance type list that says \"fast launch\" to display all available instances. The available instance types are not limited to ml.t3.medium, ml.g4dn.xlarge, ml.m5.large, and ml.c5.large. The user can also refer to the provided link for additional information about fast launch instances.",
        "Answer_preprocessed_content":"the instance types you are seeing are fast launch instances . in order to see all the types of instances, click on the switch on top of the instance type list that says fast launch , that should display the rest of available instances. here is additional info about fast launch instances hope it helps!"
    },
    {
        "Question_id":null,
        "Question_title":"Local run vs remote run dependencies",
        "Question_body":"<p>I have run some operations on remotes successfully in the past. However, there was always a some discrepancy between the imports for local and remote runs that I needed to fix by trial and error.<\/p>\n<p>In my current setup, I switched from flags as global variables in the training script to flags in config.yml files. And I\u2019m unable to make it work on remotes.<\/p>\n<p><strong>Project structure<\/strong><br>\nProject:<\/p>\n<ul>\n<li>[some folders]<\/li>\n<li>datasets \u2192 module, contains data loaders + their config.yml files<\/li>\n<li>zoo \u2192 guild Home for local runs<\/li>\n<li>models \u2192 model definitions<\/li>\n<li>\n<ul>\n<li>guild.yml<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>abstract_model.py<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>conv_lstm \u2192 model I want to run<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>model.py \u2192 model definition<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>train.py \u2192 training script<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<li>\n<ul>\n<li>\n<ul>\n<li>config.yml \u2192 flags<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p><strong>Guild file<\/strong><\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\"># Standard convolutional LSTM\n- model: conv_lstm\n  description: Convolutional LSTM\n  operations:\n    train_local:\n      description: Train Convolutional LSTM\n      sourcecode:\n        - conv_lstm\/train.py\n        - conv_lstm\/model.py\n        - abstract_model.py\n      requires:\n        - config: conv_lstm\/config.yml\n        - file: ..\/datasets\/\n      main: conv_lstm\/train\n      flags-dest: config:conv_lstm\/config.yml\n      flags-import: all\n      flags:\n        epochs: 100\n        dataset_args:\n          - dataset_name: ucsd\n            batch_size: 2\n      output-scalars:\n        train_loss: 'Train mse: (\\value)'\n        test_acc: 'Test mse: (\\value)'\n    train_remote:\n      description: Train Convolutional LSTM on remote\n      sourcecode:\n        - conv_lstm\/train.py\n        - conv_lstm\/model.py\n        - abstract_model.py\n      requires:\n        - config: conv_lstm\/config.yml\n        - file: ..\/datasets\/\n      main: conv_lstm\/train\n      flags-dest: config:conv_lstm\/config.yml\n      flags-import: all\n      flags:\n        optimizer: Adam\n        loss: mse\n        learning_rate: 0.001\n        epochs: 100\n        dev: True\n        gpus: [7]\n        dataset_args:\n          - dataset_name: ucsd\n            batch_size: 2\n            train_path: ~\/data\/ucsd\/UCSDped1\/Train\/\n            test_path: ~\/data\/ucsd\/UCSDped1\/Test\/\n      output-scalars:\n        train_loss: 'Train mse: (\\value)'\n        test_acc: 'Test mse: (\\value)'\n<\/code><\/pre>\n<p><strong>Training script<\/strong><\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">sys.path.append('..\/')\nsys.path.append('..\/..\/datasets')\n# Tensorflow logging level\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n\nimport tensorflow as tf\nimport yaml\nfrom model import ConvLSTM\nfrom datasets.data_loader import DataLoader\n\n\n# Load the model configuration\nclass Config(object):\n    def __init__(self, filename):\n        self.__dict__.update(yaml.safe_load(open(filename)))\n\n\nconfig = Config(\"config.yml\")\n\n(...)\n<\/code><\/pre>\n<p><strong>Current situation &amp; error<\/strong><br>\nI\u2019m able to run \u2018conv_lstm:train_local\u2019 without any issues, and everything works as expected. However, almost the same configuration, with a few flags changed, fails to run on remote.<\/p>\n<p>Issue 1: I cannot see any evidence of the config.yml file being copied to the remote<br>\nIssue 2: the remote run fails to find the main training script, even though it works locally.<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild -H \/home\/bleporowski\/Projects\/mad\/zoo run conv_lstm:train_remote --remote [remote_name] --gpus 7\nYou are about to run conv_lstm:train_remote as a batch (1 trial) on [remote_name]\n  dataset_args: [{batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/UCSDped1\/Test\/, train_path: ~\/data\/ucsd\/UCSDped1\/Train\/}]\n  dev: yes\n  epochs: 100\n  gpus: [7]\n  learning_rate: 0.001\n  loss: mse\n  optimizer: Adam\nContinue? (Y\/n) y\nBuilding package\npackage src: \/home\/bleporowski\/Projects\/mad\/models\npackage dist: \/tmp\/guild-remote-stage-eq7ahi7e\nrunning clean\nremoving 'build\/lib' (and everything under it)\nremoving 'build\/bdist.linux-x86_64' (and everything under it)\n'build\/scripts-3.8' does not exist -- can't clean it\nremoving 'build'\nrunning bdist_wheel\nrunning build\nrunning build_py\npackage init file '\/home\/bleporowski\/Projects\/mad\/models\/__init__.py' not found (or not a regular file)\ncreating build\ncreating build\/lib\ncreating build\/lib\/conv_lstm\ncopying \/home\/bleporowski\/Projects\/mad\/models\/abstract_model.py -&gt; build\/lib\/conv_lstm\ncopying \/home\/bleporowski\/Projects\/mad\/models\/guild.yml -&gt; build\/lib\/conv_lstm\ninstalling to build\/bdist.linux-x86_64\/wheel\nrunning install\nrunning install_lib\ncreating build\/bdist.linux-x86_64\ncreating build\/bdist.linux-x86_64\/wheel\ncreating build\/bdist.linux-x86_64\/wheel\/conv_lstm\ncopying build\/lib\/conv_lstm\/guild.yml -&gt; build\/bdist.linux-x86_64\/wheel\/conv_lstm\ncopying build\/lib\/conv_lstm\/abstract_model.py -&gt; build\/bdist.linux-x86_64\/wheel\/conv_lstm\nrunning install_egg_info\nrunning egg_info\nwriting conv_lstm.egg-info\/PKG-INFO\nwriting dependency_links to conv_lstm.egg-info\/dependency_links.txt\nwriting entry points to conv_lstm.egg-info\/entry_points.txt\nwriting namespace_packages to conv_lstm.egg-info\/namespace_packages.txt\nwriting top-level names to conv_lstm.egg-info\/top_level.txt\nreading manifest file 'conv_lstm.egg-info\/SOURCES.txt'\nwriting manifest file 'conv_lstm.egg-info\/SOURCES.txt'\nCopying conv_lstm.egg-info to build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0-py3.8.egg-info\nrunning install_scripts\ncreating build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0.dist-info\/WHEEL\ncreating '\/tmp\/guild-remote-stage-eq7ahi7e\/conv_lstm-0.0.0-py2.py3-none-any.whl' and adding 'build\/bdist.linux-x86_64\/wheel' to it\nadding 'conv_lstm\/abstract_model.py'\nadding 'conv_lstm\/guild.yml'\nadding 'conv_lstm-0.0.0.dist-info\/METADATA'\nadding 'conv_lstm-0.0.0.dist-info\/PACKAGE'\nadding 'conv_lstm-0.0.0.dist-info\/WHEEL'\nadding 'conv_lstm-0.0.0.dist-info\/entry_points.txt'\nadding 'conv_lstm-0.0.0.dist-info\/namespace_packages.txt'\nadding 'conv_lstm-0.0.0.dist-info\/top_level.txt'\nadding 'conv_lstm-0.0.0.dist-info\/RECORD'\nremoving build\/bdist.linux-x86_64\/wheel\nInitializing remote run\nCopying package\nsending incremental file list\nconv_lstm-0.0.0-py2.py3-none-any.whl\n\nsent 3,558 bytes  received 35 bytes  1,437.20 bytes\/sec\ntotal size is 3,424  speedup is 0.95\nInstalling package and its dependencies\nProcessing .\/conv_lstm-0.0.0-py2.py3-none-any.whl\nInstalling collected packages: conv-lstm\nSuccessfully installed conv-lstm-0.0.0\nStarting conv_lstm:train_remote on charybdis as 8a26ca399039412fb31c7791d293b507\nWARNING: [Errno 2] No such file or directory: 'conv_lstm\/config.yml'\nWARNING: [Errno 2] No such file or directory: 'conv_lstm\/config.yml'\nWARNING: cannot import flags from conv_lstm\/train: No module named conv_lstm\/train\nWARNING: cannot import flags from conv_lstm\/train: No module named conv_lstm\/train\nINFO: [guild] Running trial 05afef0858f74b4198af160c6d904e2e: conv-lstm\/conv_lstm:train_remote (dataset_args={batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/UCSDped1\/Test\/, train_path: ~\/data\/ucsd\/UCSDped1\/Train\/}, dev=yes, epochs=100, gpus=7, learning_rate=0.001, loss=mse, optimizer=Adam)\nINFO: [guild] Resolving config:conv_lstm\/config.yml dependency\nERROR: [guild] Trial 05afef0858f74b4198af160c6d904e2e exited with an error: (1) run failed because a dependency was not met: could not resolve 'config:conv_lstm\/config.yml' in config:conv_lstm\/config.yml resource: cannot find source file 'conv_lstm\/config.yml'\nRun 8a26ca399039412fb31c7791d293b507 stopped with a status of 'completed'\n\n<\/code><\/pre>\n<p>Do remote runs require everything to become a module, with \u2018<strong>init<\/strong>.py\u2019? Or should the guild file be in a different location?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1645549217715,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":198.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/local-run-vs-remote-run-dependencies\/812",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-03T17:17:20.121Z",
                "Answer_body":"<p>Sorry for the late reply here! I thought someone had replied to this but I had my messages crossed. Taking a look now.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-03T17:49:58.131Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cptpirx\">@CptPirx<\/a>,<\/p>\n<p>We think your issue is the lack of a data-files entry: <a href=\"https:\/\/my.guild.ai\/t\/packages\/223\" class=\"inline-onebox\">Packages<\/a><\/p>\n<p>Garrett and I discussed the unintuitive behavior that explicitly listed sourcecode and files are not being included. We\u2019ll file a feature request for straightening this out, so that the data-files entry is only necessary for files not explicitly listed in other fields.<\/p>\n<p>EDIT: issue filed at <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/323\" class=\"inline-onebox\">Package explicitly listed sourcecode and config entries without data-files \u00b7 Issue #323 \u00b7 guildai\/guildai \u00b7 GitHub<\/a><\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T16:00:50.556Z",
                "Answer_body":"<p>After adding appropriate data-files entries in the package, as suggested, everything works.<\/p>\n<p>Thank you both for quick help, as always  <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":51.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T16:05:40.637Z",
                "Answer_body":"<p>I\u2019ll echo <a class=\"mention\" href=\"\/u\/msarahan\">@msarahan<\/a> with a mea culpa \u2014 Guild should figure that stuff out and not require the extra work. We\u2019ll get that cleaned up! Thanks for your patience.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: local run vs remote run dependencies; Content: i have run some operations on remotes successfully in the past. however, there was always a some discrepancy between the imports for local and remote runs that i needed to fix by trial and error. in my current setup, i switched from flags as global variables in the training script to flags in config.yml files. and i\u2019m unable to make it work on remotes. project structure project: [some folders] datasets \u2192 module, contains data loaders + their config.yml files zoo \u2192 guild home for local runs models \u2192 model definitions guild.yml abstract_model.py conv_lstm \u2192 model i want to run model.py \u2192 model definition train.py \u2192 training script config.yml \u2192 flags guild file # standard convolutional lstm - model: conv_lstm description: convolutional lstm operations: train_local: description: train convolutional lstm sourcecode: - conv_lstm\/train.py - conv_lstm\/model.py - abstract_model.py requires: - config: conv_lstm\/config.yml - file: ..\/datasets\/ main: conv_lstm\/train flags-dest: config:conv_lstm\/config.yml flags-import: all flags: epochs: 100 dataset_args: - dataset_name: ucsd batch_size: 2 output-scalars: train_loss: 'train mse: (\\value)' test_acc: 'test mse: (\\value)' train_remote: description: train convolutional lstm on remote sourcecode: - conv_lstm\/train.py - conv_lstm\/model.py - abstract_model.py requires: - config: conv_lstm\/config.yml - file: ..\/datasets\/ main: conv_lstm\/train flags-dest: config:conv_lstm\/config.yml flags-import: all flags: optimizer: adam loss: mse learning_rate: 0.001 epochs: 100 dev: true gpus: [7] dataset_args: - dataset_name: ucsd batch_size: 2 train_path: ~\/data\/ucsd\/ucsdped1\/train\/ test_path: ~\/data\/ucsd\/ucsdped1\/test\/ output-scalars: train_loss: 'train mse: (\\value)' test_acc: 'test mse: (\\value)' training script sys.path.append('..\/') sys.path.append('..\/..\/datasets') # tensorflow logging level os.environ[\"tf_cpp_min_log_level\"] = \"2\" import tensorflow as tf import yaml from model import convlstm from datasets.data_loader import dataloader # load the model configuration class config(object): def __init__(self, filename): self.__dict__.update(yaml.safe_load(open(filename))) config = config(\"config.yml\") (...) current situation & error i\u2019m able to run \u2018conv_lstm:train_local\u2019 without any issues, and everything works as expected. however, almost the same configuration, with a few flags changed, fails to run on remote. issue 1: i cannot see any evidence of the config.yml file being copied to the remote issue 2: the remote run fails to find the main training script, even though it works locally. guild -h \/home\/bleporowski\/projects\/mad\/zoo run conv_lstm:train_remote --remote [remote_name] --gpus 7 you are about to run conv_lstm:train_remote as a batch (1 trial) on [remote_name] dataset_args: [{batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/ucsdped1\/test\/, train_path: ~\/data\/ucsd\/ucsdped1\/train\/}] dev: yes epochs: 100 gpus: [7] learning_rate: 0.001 loss: mse optimizer: adam continue? (y\/n) y building package package src: \/home\/bleporowski\/projects\/mad\/models package dist: \/tmp\/guild-remote-stage-eq7ahi7e running clean removing 'build\/lib' (and everything under it) removing 'build\/bdist.linux-x86_64' (and everything under it) 'build\/scripts-3.8' does not exist -- can't clean it removing 'build' running bdist_wheel running build running build_py package init file '\/home\/bleporowski\/projects\/mad\/models\/__init__.py' not found (or not a regular file) creating build creating build\/lib creating build\/lib\/conv_lstm copying \/home\/bleporowski\/projects\/mad\/models\/abstract_model.py -> build\/lib\/conv_lstm copying \/home\/bleporowski\/projects\/mad\/models\/guild.yml -> build\/lib\/conv_lstm installing to build\/bdist.linux-x86_64\/wheel running install running install_lib creating build\/bdist.linux-x86_64 creating build\/bdist.linux-x86_64\/wheel creating build\/bdist.linux-x86_64\/wheel\/conv_lstm copying build\/lib\/conv_lstm\/guild.yml -> build\/bdist.linux-x86_64\/wheel\/conv_lstm copying build\/lib\/conv_lstm\/abstract_model.py -> build\/bdist.linux-x86_64\/wheel\/conv_lstm running install_egg_info running egg_info writing conv_lstm.egg-info\/pkg-info writing dependency_links to conv_lstm.egg-info\/dependency_links.txt writing entry points to conv_lstm.egg-info\/entry_points.txt writing namespace_packages to conv_lstm.egg-info\/namespace_packages.txt writing top-level names to conv_lstm.egg-info\/top_level.txt reading manifest file 'conv_lstm.egg-info\/sources.txt' writing manifest file 'conv_lstm.egg-info\/sources.txt' copying conv_lstm.egg-info to build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0-py3.8.egg-info running install_scripts creating build\/bdist.linux-x86_64\/wheel\/conv_lstm-0.0.0.dist-info\/wheel creating '\/tmp\/guild-remote-stage-eq7ahi7e\/conv_lstm-0.0.0-py2.py3-none-any.whl' and adding 'build\/bdist.linux-x86_64\/wheel' to it adding 'conv_lstm\/abstract_model.py' adding 'conv_lstm\/guild.yml' adding 'conv_lstm-0.0.0.dist-info\/metadata' adding 'conv_lstm-0.0.0.dist-info\/package' adding 'conv_lstm-0.0.0.dist-info\/wheel' adding 'conv_lstm-0.0.0.dist-info\/entry_points.txt' adding 'conv_lstm-0.0.0.dist-info\/namespace_packages.txt' adding 'conv_lstm-0.0.0.dist-info\/top_level.txt' adding 'conv_lstm-0.0.0.dist-info\/record' removing build\/bdist.linux-x86_64\/wheel initializing remote run copying package sending incremental file list conv_lstm-0.0.0-py2.py3-none-any.whl sent 3,558 bytes received 35 bytes 1,437.20 bytes\/sec total size is 3,424 speedup is 0.95 installing package and its dependencies processing .\/conv_lstm-0.0.0-py2.py3-none-any.whl installing collected packages: conv-lstm successfully installed conv-lstm-0.0.0 starting conv_lstm:train_remote on charybdis as 8a26ca399039412fb31c7791d293b507 warning: [errno 2] no such file or directory: 'conv_lstm\/config.yml' warning: [errno 2] no such file or directory: 'conv_lstm\/config.yml' warning: cannot import flags from conv_lstm\/train: no module named conv_lstm\/train warning: cannot import flags from conv_lstm\/train: no module named conv_lstm\/train info: [guild] running trial 05afef0858f74b4198af160c6d904e2e: conv-lstm\/conv_lstm:train_remote (dataset_args={batch_size: 2, dataset_name: ucsd, test_path: ~\/data\/ucsd\/ucsdped1\/test\/, train_path: ~\/data\/ucsd\/ucsdped1\/train\/}, dev=yes, epochs=100, gpus=7, learning_rate=0.001, loss=mse, optimizer=adam) info: [guild] resolving config:conv_lstm\/config.yml dependency error: [guild] trial 05afef0858f74b4198af160c6d904e2e exited with an error: (1) run failed because a dependency was not met: could not resolve 'config:conv_lstm\/config.yml' in config:conv_lstm\/config.yml resource: cannot find source file 'conv_lstm\/config.yml' run 8a26ca399039412fb31c7791d293b507 stopped with a status of 'completed' do remote runs require everything to become a module, with \u2018init.py\u2019? or should the guild file be in a different location?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with discrepancies between local and remote runs, and is unable to make their current setup work on remotes due to issues with the config.yml file and the main training script.",
        "Question_preprocessed_content":"Title: local run vs remote run dependencies; Content: i have run some operations on remotes successfully in the past. however, there was always a some discrepancy between the imports for local and remote runs that i needed to fix by trial and error. in my current setup, i switched from flags as global variables in the training script to flags in files. and im unable to make it work on remotes. project structure project some folders datasets module, contains data loaders + their files zoo guild home for local runs models model definitions model i want to run model definition training script flags guild file training script current situation & error im able to run without any issues, and everything works as expected. however, almost the same configuration, with a few flags changed, fails to run on remote. issue i cannot see any evidence of the file being copied to the remote issue the remote run fails to find the main training script, even though it works locally. do remote runs require everything to become a module, with or should the guild file be in a different location?",
        "Answer_original_content":"sorry for the late reply here! i thought someone had replied to this but i had my messages crossed. taking a look now. hi @cptpirx, we think your issue is the lack of a data-files entry: packages garrett and i discussed the unintuitive behavior that explicitly listed sourcecode and files are not being included. well file a feature request for straightening this out, so that the data-files entry is only necessary for files not explicitly listed in other fields. edit: issue filed at package explicitly listed sourcecode and config entries without data-files issue #323 guildai\/guildai github after adding appropriate data-files entries in the package, as suggested, everything works. thank you both for quick help, as always ill echo @msarahan with a mea culpa guild should figure that stuff out and not require the extra work. well get that cleaned up! thanks for your patience.",
        "Answer_original_content_gpt_summary":"The solution to the user's issue with discrepancies between local and remote runs is to add a data-files entry in the package. This will resolve the issues with the config.yml file and the main training script. The user was able to resolve the issue by adding the appropriate data-files entries in the package. The team will file a feature request to straighten out the unintuitive behavior of explicitly listed sourcecode and files not being included. The team will work on cleaning up the issue so that the data-files entry is only necessary for files not explicitly listed in other fields.",
        "Answer_preprocessed_content":"sorry for the late reply here! i thought someone had replied to this but i had my messages crossed. taking a look now. hi we think your issue is the lack of a entry packages garrett and i discussed the unintuitive behavior that explicitly listed sourcecode and files are not being included. well file a feature request for straightening this out, so that the entry is only necessary for files not explicitly listed in other fields. edit issue filed at package explicitly listed sourcecode and config entries without issue github after adding appropriate entries in the package, as suggested, everything works. thank you both for quick help, as always ill echo with a mea culpa guild should figure that stuff out and not require the extra work. well get that cleaned up! thanks for your patience."
    },
    {
        "Question_id":70771911.0,
        "Question_title":"How to use PipelineParameter in DatabricksStep (Python)",
        "Question_body":"<p>I've created an AML Pipeline with a single DatabricksStep. I've need to pass a parameter to the Databricks notebook when I run the published pipeline.<\/p>\n<p>When I run the published pipeline, the Databricks steps always take the default value of the PipelineParameter, no matter what value I choose when I submit the pipeline.<\/p>\n<p>Here the code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>start_date_param = PipelineParameter(name=&quot;StartDate&quot;, default_value='2022-01-19')\n\n# Define data ingestion step\ndata_loading_step = DatabricksStep(\n        name=&quot;Data Loading&quot;,\n        existing_cluster_id=db_cluster_id,\n        notebook_path=data_loading_path,\n        run_name=&quot;Loading raw data&quot;,\n        notebook_params={\n            'StartDate': start_date_param,\n        },\n        compute_target=dbricks_compute,\n        instance_pool_id=instance_pool_id,\n        num_workers=num_workers,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>And this is the Databricks notebook:<\/p>\n<pre><code>dbutils.widgets.text(&quot;StartDate&quot;, &quot;&quot;, &quot;StartDate(YYYY-MM-DD)&quot;)\n<\/code><\/pre>\n<p>The default value of StartDate is 2022-01-19.Even though I set the StartDate parameter to '2021-01-19' the Databricks notebook still takes 2022-01-19 as StartDate.<\/p>\n<p>What am I doing wrong?<\/p>\n<p>Thanks for any help,<\/p>\n<p>G<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1642600664140,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":175.0,
        "Owner_creation_time":1547219189528,
        "Owner_last_access_time":1648043983012,
        "Owner_reputation":31.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":"<p>I've found the solution of the problem. I hope it can be of help to someone.\nIt works if you set all the parameter name and widget name in lower case.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>start_date_param = PipelineParameter(name=&quot;start_date&quot;, default_value='2022-01-19')\n\n# Define data ingestion step\ndata_loading_step = DatabricksStep(\n        name=&quot;Data Loading&quot;,\n        existing_cluster_id=db_cluster_id,\n        notebook_path=data_loading_path,\n        run_name=&quot;Loading raw data&quot;,\n        notebook_params={\n            'start_date': start_date_param,\n        },\n        compute_target=dbricks_compute,\n        instance_pool_id=instance_pool_id,\n        num_workers=num_workers,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<pre><code>dbutils.widgets.text(&quot;start_date&quot;, &quot;&quot;, &quot;start_date(YYYY-MM-DD)&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1642668696820,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70771911",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to use pipelineparameter in databricksstep (python); Content: i've created an aml pipeline with a single databricksstep. i've need to pass a parameter to the databricks notebook when i run the published pipeline. when i run the published pipeline, the databricks steps always take the default value of the pipelineparameter, no matter what value i choose when i submit the pipeline. here the code: start_date_param = pipelineparameter(name=\"startdate\", default_value='2022-01-19') # define data ingestion step data_loading_step = databricksstep( name=\"data loading\", existing_cluster_id=db_cluster_id, notebook_path=data_loading_path, run_name=\"loading raw data\", notebook_params={ 'startdate': start_date_param, }, compute_target=dbricks_compute, instance_pool_id=instance_pool_id, num_workers=num_workers, allow_reuse=false ) and this is the databricks notebook: dbutils.widgets.text(\"startdate\", \"\", \"startdate(yyyy-mm-dd)\") the default value of startdate is 2022-01-19.even though i set the startdate parameter to '2021-01-19' the databricks notebook still takes 2022-01-19 as startdate. what am i doing wrong? thanks for any help, g",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with using a pipelineparameter in a DatabricksStep to pass a parameter to a Databricks notebook when running a published pipeline.",
        "Question_preprocessed_content":"Title: how to use pipelineparameter in databricksstep ; Content: i've created an aml pipeline with a single databricksstep. i've need to pass a parameter to the databricks notebook when i run the published pipeline. when i run the published pipeline, the databricks steps always take the default value of the pipelineparameter, no matter what value i choose when i submit the pipeline. here the code and this is the databricks notebook the default value of startdate is though i set the startdate parameter to the databricks notebook still takes as startdate. what am i doing wrong? thanks for any help, g",
        "Answer_original_content":"i've found the solution of the problem. i hope it can be of help to someone. it works if you set all the parameter name and widget name in lower case. start_date_param = pipelineparameter(name=\"start_date\", default_value='2022-01-19') # define data ingestion step data_loading_step = databricksstep( name=\"data loading\", existing_cluster_id=db_cluster_id, notebook_path=data_loading_path, run_name=\"loading raw data\", notebook_params={ 'start_date': start_date_param, }, compute_target=dbricks_compute, instance_pool_id=instance_pool_id, num_workers=num_workers, allow_reuse=false ) dbutils.widgets.text(\"start_date\", \"\", \"start_date(yyyy-mm-dd)\")",
        "Answer_original_content_gpt_summary":"The solution to the challenge of using a pipeline parameter in a DatabricksStep to pass a parameter to a Databricks notebook when running a published pipeline is to set all the parameter name and widget name in lower case. Additionally, the code snippet provided shows how to define a data ingestion step and set the notebook parameters.",
        "Answer_preprocessed_content":"i've found the solution of the problem. i hope it can be of help to someone. it works if you set all the parameter name and widget name in lower case."
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker inference on inf1 no opencv",
        "Question_body":"I am trying to deploy Pytorch model on ml.inf1.xlarge instance. Image: 301217895009.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-neo-pytorch:1.5.1-inf-py3 My python code using some oepncv functions, and when I am trying to run the infernce I got the following error: ModuleNotFoundError: No module named 'cv2'\n\nI tried to add opencv-python-headless to requirements.txt, but then I got another error ImportError: libgthread-2.0.so.0: cannot open shared object file\n\nHow I can use opencv with the ml.inf1 instances?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1649704234930,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":123.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUG9fHLN3TNGbXjRmuwRBcA\/sagemaker-inference-on-inf-1-no-opencv",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-11T21:49:47.674Z",
                "Answer_score":0,
                "Answer_body":"When running Neo inference on Sagemaker, it\u2019s possible to now use the Deep Learning Containers (DLC) provided by AWS. It\u2019s also unnecessary to remain on Pytorch 1.5 when using the DLC images. In our latest released image, OpenCV is pre-installed along with a more recent version of AWS Neuron SDK. Here\u2019s a link to the latest docker image for Pytorch 1.10 + Neuron SDK 1.17: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.10\/py3\/sdk1.17.1\/Dockerfile.neuron#L83\n\nIf you\u2019re unable to easily move to the latest Pytorch+Neuron DLC, the dockerfile link may help with resolving installation errors of OpenCV into your container.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: inference on inf1 no opencv; Content: i am trying to deploy pytorch model on ml.inf1.xlarge instance. image: 301217895009.dkr.ecr.us-west-2.amazonaws.com\/-neo-pytorch:1.5.1-inf-py3 my python code using some oepncv functions, and when i am trying to run the infernce i got the following error: modulenotfounderror: no module named 'cv2' i tried to add opencv-python-headless to requirements.txt, but then i got another error importerror: libgthread-2.0.so.0: cannot open shared object file how i can use opencv with the ml.inf1 instances?",
        "Question_original_content_gpt_summary":"The user is encountering challenges deploying a PyTorch model on an ml.inf1.xlarge instance, including errors related to OpenCV and libgthread-2.0.so.0.",
        "Question_preprocessed_content":"Title: inference on inf no opencv; Content: i am trying to deploy pytorch model on instance. image my python code using some oepncv functions, and when i am trying to run the infernce i got the following error modulenotfounderror no module named 'cv ' i tried to add to but then i got another error importerror cannot open shared object file how i can use opencv with the instances?",
        "Answer_original_content":"when running neo inference on , its possible to now use the deep learning containers (dlc) provided by aws. its also unnecessary to remain on pytorch 1.5 when using the dlc images. in our latest released image, opencv is pre-installed along with a more recent version of aws neuron sdk. heres a link to the latest docker image for pytorch 1.10 + neuron sdk 1.17: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.10\/py3\/sdk1.17.1\/dockerfile.neuron#l83 if youre unable to easily move to the latest pytorch+neuron dlc, the dockerfile link may help with resolving installation errors of opencv into your container.",
        "Answer_original_content_gpt_summary":"Possible solutions to challenges encountered when deploying a PyTorch model on an ml.inf1.xlarge instance include using the deep learning containers (DLC) provided by AWS, which come with pre-installed OpenCV and a more recent version of AWS Neuron SDK. It is also unnecessary to remain on PyTorch 1.5 when using the DLC images. If unable to move to the latest PyTorch+Neuron DLC, the Dockerfile link provided may help with resolving installation errors of OpenCV into the container.",
        "Answer_preprocessed_content":"when running neo inference on , its possible to now use the deep learning containers provided by aws. its also unnecessary to remain on pytorch when using the dlc images. in our latest released image, opencv is along with a more recent version of aws neuron sdk. heres a link to the latest docker image for pytorch + neuron sdk if youre unable to easily move to the latest pytorch+neuron dlc, the dockerfile link may help with resolving installation errors of opencv into your container."
    },
    {
        "Question_id":36249716.0,
        "Question_title":"Automating Azure Machine Learning",
        "Question_body":"<p>Is there a way of automating the calls to the Azure Machine Learning Service (AML)? <\/p>\n\n<p>I\u2019ve created the web service from AML. Now I have to do the calls the automated way. I\u2019m trying to build a system, that connects to a Raspberry Pi for sensor data and gets a prediction from the ML service to be saved with the data itself. <\/p>\n\n<p>Is there something in Azure to automate this or should I do it within the application?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1459095760807,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":341.0,
        "Owner_creation_time":1459065054367,
        "Owner_last_access_time":1459181119923,
        "Owner_reputation":3.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":"<p>I'm assuming you've created the webservice from the experiment and asking about the consumption of the webservice. You can consume the webservice from anything that can do an API call to the endpoint. I don't know the exact architecture of your solution but take a look at this as it might suit your scenario. <\/p>\n\n<p>Stream analytics on Azure has a new feature called Functions(just a heads-up, its still in preview) that can automate the usage of deployed ML services from your account.Since you are trying to gather info from IoT devices, you might use <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/event-hubs-csharp-ephcs-getstarted\/\" rel=\"nofollow\">Event Hubs<\/a> or <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/iot-hub-csharp-csharp-getstarted\/\" rel=\"nofollow\">IoT Hubs<\/a> to get the data and process it using Stream Analytics and during the process you can use the Webservice as Function in SA to achieve on-the-go ML results.<\/p>\n\n<p>Usage is relatively simple if you are familiar with Stream Analytics or SQL queries in general.This <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link<\/a> shows the step by step implementation and the usage is below;<\/p>\n\n<pre><code>    WITH subquery AS (  \n    SELECT text, \"webservicealias\"(text) as result from input  \n    )  \n\n    Select text, result.[Score]  \n    Into output  \n    From subquery  \n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>\n\n<p>Mert<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1459099456052,
        "Answer_score":4.0,
        "Owner_location":null,
        "Question_last_edit_time":1459199614030,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36249716",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: automating ; Content: is there a way of automating the calls to the service (aml)? i\u2019ve created the web service from aml. now i have to do the calls the automated way. i\u2019m trying to build a system, that connects to a raspberry pi for sensor data and gets a prediction from the ml service to be saved with the data itself. is there something in azure to automate this or should i do it within the application?",
        "Question_original_content_gpt_summary":"The user is trying to build a system that connects to a Raspberry Pi for sensor data and gets a prediction from an ML service to be saved with the data itself, and is looking for a way to automate the calls to the service.",
        "Question_preprocessed_content":"Title: automating ; Content: is there a way of automating the calls to the service ? ive created the web service from aml. now i have to do the calls the automated way. im trying to build a system, that connects to a raspberry pi for sensor data and gets a prediction from the ml service to be saved with the data itself. is there something in azure to automate this or should i do it within the application?",
        "Answer_original_content":"i'm assuming you've created the webservice from the experiment and asking about the consumption of the webservice. you can consume the webservice from anything that can do an api call to the endpoint. i don't know the exact architecture of your solution but take a look at this as it might suit your scenario. stream analytics on azure has a new feature called functions(just a heads-up, its still in preview) that can automate the usage of deployed ml services from your account.since you are trying to gather info from iot devices, you might use event hubs or iot hubs to get the data and process it using stream analytics and during the process you can use the webservice as function in sa to achieve on-the-go ml results. usage is relatively simple if you are familiar with stream analytics or sql queries in general.this link shows the step by step implementation and the usage is below; with subquery as ( select text, \"webservicealias\"(text) as result from input ) select text, result.[score] into output from subquery hope this helps! mert",
        "Answer_original_content_gpt_summary":"Possible solutions to automate calls to an ML service for sensor data on a Raspberry Pi include using Stream Analytics on Azure with the new Functions feature (still in preview) to automate the usage of deployed ML services from your account. You can gather data from IoT devices using Event Hubs or IoT Hubs and process it using Stream Analytics, using the ML service as a function in SA to achieve on-the-go ML results. Usage is relatively simple if you are familiar with Stream Analytics or SQL queries in general.",
        "Answer_preprocessed_content":"i'm assuming you've created the webservice from the experiment and asking about the consumption of the webservice. you can consume the webservice from anything that can do an api call to the endpoint. i don't know the exact architecture of your solution but take a look at this as it might suit your scenario. stream analytics on azure has a new feature called functions that can automate the usage of deployed ml services from your you are trying to gather info from iot devices, you might use event hubs or iot hubs to get the data and process it using stream analytics and during the process you can use the webservice as function in sa to achieve ml results. usage is relatively simple if you are familiar with stream analytics or sql queries in link shows the step by step implementation and the usage is below; hope this helps! mert"
    },
    {
        "Question_id":null,
        "Question_title":"Get image id from deployed azureml Model?",
        "Question_body":"About a year ago when testing azure.train.automl models' deployments with AciContainer and AciWebServices the endpoint details also displayed an Image ID which our web services team could pull and deploy.\n\nNow with azure.core==1.19.0 the Image ID is not automatically produced.\n\nHow do I generate the Image ID for them to pull the image?",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1611789072743,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/247987\/get-image-id-from-deployed-azureml-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-28T09:50:42.463Z",
                "Answer_score":0,
                "Answer_body":"@MathBarbarian-1293 Thanks, Model.Resgister(Register Model > Create Image > Create Webservice) is old approach, you can use the latest SDK with Model.deploy to deploy the model. the Inference Config Class has a spot for environment which works with the Model.deploy method.\nModel.download will give you the model file\nModel.package will give you the model packaged into a docker image\n\nPlease follow the below steps to Deploy the best Model.\nDeploy the model registered in the previous slide, to Azure Container Instance (ACI) as a Web Service\nThere are 4 steps involved in model deployment\u200b\n\nStep 9.1 \u2013 Create scoring script\u200b\n\nStep 9.2 \u2013 Create environment file\u200b\n\nStep 9.3 \u2013 Create configuration file\u200b\n\nStep 9.4 \u2013 Deploy to ACI!\u200b\n\n\n\n\nhttps:\/\/github.com\/danielsc\/dogbreeds\/blob\/master\/webservice-test.ipynb\nhttps:\/\/github.com\/retkowsky\/AzureML_Excel\/blob\/master\/Boston%20ML%20ACI.ipynb",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: get image id from deployed model?; Content: about a year ago when testing azure.train.automl models' deployments with acicontainer and aciwebservices the endpoint details also displayed an image id which our web services team could pull and deploy. now with azure.core==1.19.0 the image id is not automatically produced. how do i generate the image id for them to pull the image?",
        "Question_original_content_gpt_summary":"The user is trying to figure out how to generate an image ID for their web services team to pull and deploy from an Azure.train.automl model deployment.",
        "Question_preprocessed_content":"Title: get image id from deployed model?; Content: about a year ago when testing models' deployments with acicontainer and aciwebservices the endpoint details also displayed an image id which our web services team could pull and deploy. now with the image id is not automatically produced. how do i generate the image id for them to pull the image?",
        "Answer_original_content":"@mathbarbarian-1293 thanks, model.resgister(register model > create image > create webservice) is old approach, you can use the latest sdk with model.deploy to deploy the model. the inference config class has a spot for environment which works with the model.deploy method. model.download will give you the model file model.package will give you the model packaged into a docker image please follow the below steps to deploy the best model. deploy the model registered in the previous slide, to azure container instance (aci) as a web service there are 4 steps involved in model deployment step 9.1 create scoring script step 9.2 create environment file step 9.3 create configuration file step 9.4 deploy to aci! https:\/\/github.com\/danielsc\/dogbreeds\/blob\/master\/webservice-test.ipynb https:\/\/github.com\/retkowsky\/_excel\/blob\/master\/boston%20ml%20aci.ipynb",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include using the latest SDK with model.deploy to deploy the model, using the inference config class with the model.deploy method, downloading the model file with model.download, packaging the model into a docker image with model.package, and following the four steps involved in model deployment to deploy the best model. The steps include creating a scoring script, creating an environment file, creating a configuration file, and deploying to Azure Container Instance (ACI) as a web service. The answer also provides links to two Jupyter notebooks for reference.",
        "Answer_preprocessed_content":"thanks, model create image create webservice is old approach, you can use the latest sdk with to deploy the model. the inference config class has a spot for environment which works with the method. will give you the model file will give you the model packaged into a docker image please follow the below steps to deploy the best model. deploy the model registered in the previous slide, to azure container instance as a web service there are steps involved in model deployment step create scoring script step create environment file step create configuration file step deploy to aci!"
    },
    {
        "Question_id":31863977.0,
        "Question_title":"What's the original name of Microsoft Azure Machine Learning?",
        "Question_body":"<p>In this talk <a href=\"https:\/\/channel9.msdn.com\/Events\/Ignite\/2015\/BRK3550\" rel=\"nofollow\">https:\/\/channel9.msdn.com\/Events\/Ignite\/2015\/BRK3550<\/a> the speaker mentions that the Azure ML service originally had a different name.<\/p>\n\n<p>My stack and google searches have shown no results so far.\nDoes anyone know it by chance?<\/p>\n\n<p>Edit: The fragment i'm talking about starts at 56:00 and is a couple of seconds long.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3.0,
        "Question_creation_time":1438888294060,
        "Question_favorite_count":1.0,
        "Question_score":1.0,
        "Question_view_count":220.0,
        "Owner_creation_time":1437760185463,
        "Owner_last_access_time":1609936218083,
        "Owner_reputation":33.0,
        "Owner_up_votes":3.0,
        "Owner_down_votes":0.0,
        "Owner_views":24.0,
        "Answer_body":"<p>Look for <strong>Project Passau<\/strong>. Here's one of the references, in <a href=\"https:\/\/visualstudiomagazine.com\/articles\/2014\/09\/01\/azure-machine-learning-studio.aspx\" rel=\"nofollow noreferrer\">Visual Studio Magazine<\/a> or another one <a href=\"https:\/\/blogs.msdn.microsoft.com\/mspowerutilities\/2014\/07\/08\/harnessing-the-power-of-big-data-with-cloud-predictive-analytics-introducing-azure-machine-learning\/\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1512165883856,
        "Answer_score":0.0,
        "Owner_location":"Belgium",
        "Question_last_edit_time":1445831744236,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31863977",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what's the original name of ?; Content: in this talk https:\/\/channel9.msdn.com\/events\/ignite\/2015\/brk3550 the speaker mentions that the service originally had a different name. my stack and google searches have shown no results so far. does anyone know it by chance? edit: the fragment i'm talking about starts at 56:00 and is a couple of seconds long.",
        "Question_original_content_gpt_summary":"The user is trying to find out the original name of Microsoft, but has been unsuccessful in their searches so far.",
        "Question_preprocessed_content":"Title: what's the original name of ?; Content: in this talk the speaker mentions that the service originally had a different name. my stack and google searches have shown no results so far. does anyone know it by chance? edit the fragment i'm talking about starts at and is a couple of seconds long.",
        "Answer_original_content":"look for project passau. here's one of the references, in visual studio magazine or another one here.",
        "Answer_original_content_gpt_summary":"Possible solutions to find the original name of Microsoft are to search for \"project passau\" and look for references in Visual Studio Magazine or other sources.",
        "Answer_preprocessed_content":"look for project passau. here's one of the references, in visual studio magazine or another one here."
    },
    {
        "Question_id":null,
        "Question_title":"AML Pipeline as an Artifacts in Azure Devops CI\/CD",
        "Question_body":"I have a script that has multiple steps to preprocess, train, register model, and publish an AML pipeline.\n\nHere is the code for AML pipeline:\n\n\n\n step_sequence = StepSequence(steps=[step1, step2,step3])\n     pipeline = Pipeline(workspace=ws, steps=step_sequence)\n    \n     pipeline.validate()\n    \n     # Submit a pipeline\n     pipeline.submit(experiment_name=e.experiment_name_train)\n     print(\"Pipeline submitted for execution.\")\n    \n     # Publish a pipeline\n     published_pipeline = pipeline.publish(\n         name='SomeName',\n         description=\"some Desc\",\n         version=e.build_id\n     )\n     #Publish the pipeline to its versioned endpoint URI\n     publish_pipeline_to_endpoint(ws, published_pipeline)\n\n\n\nIn CI pipeline, we want publish the AML pipeline as an artifact so that it can be used in CD pipeline and deploy the same AML pipeline in test and prod.\n\nNot just a model, we want to deploy whole AML pipeline.\n\nIs there a way to do this?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1645623751260,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/747636\/aml-pipeline-as-an-artifacts-in-azure-devops-cicd.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-24T15:50:51.947Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. Please review the following resources:\n\nAzure Pipelines for CI\/CD\n\n\nSamples (MLOps and MLOpsPython)",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: aml pipeline as an artifacts in azure devops ci\/cd; Content: i have a script that has multiple steps to preprocess, train, register model, and publish an aml pipeline. here is the code for aml pipeline: step_sequence = stepsequence(steps=[step1, step2,step3]) pipeline = pipeline(workspace=ws, steps=step_sequence) pipeline.validate() # submit a pipeline pipeline.submit(experiment_name=e.experiment_name_train) print(\"pipeline submitted for execution.\") # publish a pipeline published_pipeline = pipeline.publish( name='somename', description=\"some desc\", version=e.build_id ) #publish the pipeline to its versioned endpoint uri publish_pipeline_to_endpoint(ws, published_pipeline) in ci pipeline, we want publish the aml pipeline as an artifact so that it can be used in cd pipeline and deploy the same aml pipeline in test and prod. not just a model, we want to deploy whole aml pipeline. is there a way to do this?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in publishing an Azure Machine Learning (AML) pipeline as an artifact in Azure DevOps CI\/CD in order to deploy the same AML pipeline in test and production environments.",
        "Question_preprocessed_content":"Title: aml pipeline as an artifacts in azure devops ; Content: i have a script that has multiple steps to preprocess, train, register model, and publish an aml pipeline. here is the code for aml pipeline stepsequence pipeline pipeline submit a pipeline print publish a pipeline name 'somename', description some desc , publish the pipeline to its versioned endpoint uri in ci pipeline, we want publish the aml pipeline as an artifact so that it can be used in cd pipeline and deploy the same aml pipeline in test and prod. not just a model, we want to deploy whole aml pipeline. is there a way to do this?",
        "Answer_original_content":"hi, thanks for reaching out. please review the following resources: azure pipelines for ci\/cd samples (mlops and mlopspython)",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of publishing an Azure Machine Learning pipeline as an artifact in Azure DevOps CI\/CD are available in the Azure Pipelines for CI\/CD samples, specifically the MLOps and MLOpsPython resources.",
        "Answer_preprocessed_content":"hi, thanks for reaching out. please review the following resources azure pipelines for samples"
    },
    {
        "Question_id":null,
        "Question_title":"How to customize Advanced Legend to display a string",
        "Question_body":"<p>Hello W&amp;B community!<br>\nFor a custom project within our company, we attempt to compare various models across a list of category.<br>\nWe therefore want to visualize, for each version of our model:<\/p>\n<ul>\n<li>for each category (a string)<\/li>\n<li>a score (float in 0,1).<\/li>\n<\/ul>\n<p>Here is how we log scores in W&amp;B (simplified for easy understanding):<\/p>\n<pre><code class=\"lang-auto\">    scores = [0.3, 0.5, 0.6, ...]\n    categories = ['dog', 'cat', 'fish', ...]\n    for k, (val, name) in enumerate(zip(scores, categories)):\n        wandb.log({\n                    f\"score\": val,\n                    f\"category\": name,\n                    f\"id\": k\n                })\n<\/code><\/pre>\n<p>Here is the graph associated with this code:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/e\/ebc8c5a33c454d8dbd51c5ae856934adb624544d.jpeg\" data-download-href=\"\/uploads\/short-url\/xDQmaIYqXkpCtcwGLMp3ch9QtmR.jpeg?dl=1\" title=\"Screenshot from 2022-12-05 14-29-41\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ebc8c5a33c454d8dbd51c5ae856934adb624544d_2_690x373.jpeg\" alt=\"Screenshot from 2022-12-05 14-29-41\" data-base62-sha1=\"xDQmaIYqXkpCtcwGLMp3ch9QtmR\" width=\"690\" height=\"373\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ebc8c5a33c454d8dbd51c5ae856934adb624544d_2_690x373.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ebc8c5a33c454d8dbd51c5ae856934adb624544d_2_1035x559.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ebc8c5a33c454d8dbd51c5ae856934adb624544d_2_1380x746.jpeg 2x\" data-dominant-color=\"F6F8F8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2022-12-05 14-29-41<\/span><span class=\"informations\">1389\u00d7751 103 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Now as you can see in the picture: For each version (td_ver), we display the \u201cscore\u201d (y) for each \u201cid\u201d (x). When we hover over the score itself, we do not want to show the \u201cid\u201d, but the \u201cname\u201d<br>\n(In the image, instead of displaying \u201c25\u201d highlighted in red, we want to display \u201cfish\u201d.<\/p>\n<p>I assume this change needs to happen in the \u201cAdvanced Legend\u201d section, but I could not find the correct spelling for that formula.<\/p>\n<pre><code class=\"lang-auto\">original:\n [[ ${x}: ${y} (${original})]] ${run:displayName}\n\nnew:\n???\n<\/code><\/pre>\n<p>Thanks a lot for your help!<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":null,
        "Question_creation_time":1670218529261,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":172.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-customize-advanced-legend-to-display-a-string\/3498",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-07T01:02:51.662Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/roland_roms\">@roland_roms<\/a> , happy to help. To update your legend to read \u201cfish\u201d instead of the \u201cid\u201d (x), use the following<\/p>\n<p><code> [[ fish: ${y} ]] ${run:displayName}<\/code><\/p>\n<p>Please let me know if you have any questions.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-07T01:58:12.803Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> , thank you for your answer!<\/p>\n<p>Sorry if my question wasn\u2019t asked properly:  I do not want the string \u201cfish\u201d to be displayed, but the name of the category.<br>\nAs you can see in the code I shared previously, each score is associated with a category: (scores[1] is for the categories[1], etc).<br>\nWith your solution, it would replace all occurrences of the ID ( ${x} ) by fish.<br>\nWhat I need is to have the first score with the first category (cat), the second score with the second category (dog), and so on\u2026<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-09T20:53:46.329Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/roland_roms\">@roland_roms<\/a> , appreciate the clarification. If you were to include your class categories in the run configuration, then you can reference that in your legend, see <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/line-plot\/reference#legend\">here<\/a>. This would look like<\/p>\n<p><code>[[ ${config:category}: ${y} ]] ${run:displayName}<\/code><\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-12T03:08:21.753Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"mohammadbakir\" data-post=\"4\" data-topic=\"3498\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/a9a28c\/40.png\" class=\"avatar\"> mohammadbakir:<\/div>\n<blockquote>\n<p>you were to include your class categories in the run configuration, then you can reference that in<\/p>\n<\/blockquote>\n<\/aside>\n<p>Thank you for your answer, but it does not work.<br>\nFollowing your advice, I added categories to the config, and tried to call it in various ways in Advanced legend:<\/p>\n<pre><code class=\"lang-python\">import wandb\n\nscores = [0.3, 0.5, 0.6]\ncategories = ['dog', 'cat', 'fish']\n\nconfig = {\"categories\": categories,\n            \"id_cat\": {0: \"dog\", 1:\"cat\", 2:\"fish\"}}\n\nwandb.init(project=\"dummy_test\", name=\"dummy_demo\", config=config)\nfor k, (val, name) in enumerate(zip(scores, categories)):\n    wandb.log({\n                \"score\": val,\n                \"category\": name,\n                \"categories\": name,\n                \"id_cat\": name,\n                \"id\": k\n            })\nwandb.finish()\n<\/code><\/pre>\n<p>As you can see, I tried to add it in various ways (categories or id_cat) and to call them in various ways inside the advanced legend. It never worked properly.<\/p>\n<p>Regarding the documentation you shared <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/line-plot\/reference#ignore-outliers\">here<\/a>, two points are to be highlighted:<\/p>\n<blockquote>\n<p>You can set value inside<code>[[ ]]<\/code> to display point specific values in the crosshair when hovering over a chart<br>\nAnd<\/p>\n<\/blockquote>\n<blockquote>\n<p>Supported values inside [<span class=\"chcklst-box fa fa-square-o fa-fw\"><\/span>] are as follows: \u2026<br>\n\u201cconfig\u201d is not part of these supported values. summary is also not supported. Therefore, I don\u2019t think it is possible to display it this way.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/e\/ead33e06bdd6b38ebfc3bf256f570c7c02ddac64.jpeg\" data-download-href=\"\/uploads\/short-url\/xvmjnEuXd20NUT3wOUv65YK0pi4.jpeg?dl=1\" title=\"Screenshot from 2022-12-12 12-07-53\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ead33e06bdd6b38ebfc3bf256f570c7c02ddac64_2_690x230.jpeg\" alt=\"Screenshot from 2022-12-12 12-07-53\" data-base62-sha1=\"xvmjnEuXd20NUT3wOUv65YK0pi4\" width=\"690\" height=\"230\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ead33e06bdd6b38ebfc3bf256f570c7c02ddac64_2_690x230.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ead33e06bdd6b38ebfc3bf256f570c7c02ddac64_2_1035x345.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/e\/ead33e06bdd6b38ebfc3bf256f570c7c02ddac64_2_1380x460.jpeg 2x\" data-dominant-color=\"FAFAF9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2022-12-12 12-07-53<\/span><span class=\"informations\">1833\u00d7612 84.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<\/blockquote>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-12T03:34:37.674Z",
                "Answer_body":"<p>I have found a temporary solution, that gives a similar result (but not exactly the same).<\/p>\n<ol>\n<li>I run a first time, logging only categories:<\/li>\n<\/ol>\n<pre><code class=\"lang-python\">import wandb\n\nscores = [0.3, 0.5, 0.6]\ncategories = ['dog', 'cat', 'fish']\n\nwandb.init(project=\"dummy_test\", name=\"categories\")\nfor k, (val, name) in enumerate(zip(scores, categories)):\n    wandb.log({\n                # \"score\": val,\n                \"category\": name\n            })\nwandb.finish()\n<\/code><\/pre>\n<ol start=\"2\">\n<li>I can then run wandb logs without the categories, but with the scores (I run a first time with the version_1 scores, then a second time with version_2 scores, etc):<\/li>\n<\/ol>\n<pre><code class=\"lang-python\">import wandb\n\nscores = [0.3, 0.5, 0.6]\ncategories = ['dog', 'cat', 'fish']\n\nwandb.init(project=\"dummy_test\", name=\"version_1\")\nfor k, (val, name) in enumerate(zip(scores, categories)):\n    wandb.log({\n                \"score\": val,\n                # \"category\": name\n            })\nwandb.finish()\n<\/code><\/pre>\n<ol start=\"3\">\n<li>In wandb display pannel, I create a new line plot with the following Data as Y:<\/li>\n<\/ol>\n<ul>\n<li>\u201ccategory\u201d (Added manually via \u201cregular expression\u201d)<\/li>\n<li>score<\/li>\n<\/ul>\n<ol start=\"4\">\n<li>In Advanced Legend, I set up the formula as:<\/li>\n<\/ol>\n<blockquote>\n<p>[[ ${x}: ${y} ]] ${run:displayName}<\/p>\n<\/blockquote>\n<p>It gives me the following results:<br>\n(top: first point appearing as \u201cdog\u201d. bottom: second point appearing as \u201ccat\u201d)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/5\/5c2daa076374d2fba03b113aa392b82d794efd22.jpeg\" data-download-href=\"\/uploads\/short-url\/d9rLgnww2wkjwIxLwLxWGS1x4SS.jpeg?dl=1\" title=\"Screenshot from 2022-12-12 12-33-48\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/5c2daa076374d2fba03b113aa392b82d794efd22_2_690x416.jpeg\" alt=\"Screenshot from 2022-12-12 12-33-48\" data-base62-sha1=\"d9rLgnww2wkjwIxLwLxWGS1x4SS\" width=\"690\" height=\"416\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/5c2daa076374d2fba03b113aa392b82d794efd22_2_690x416.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/5c2daa076374d2fba03b113aa392b82d794efd22_2_1035x624.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/5\/5c2daa076374d2fba03b113aa392b82d794efd22.jpeg 2x\" data-dominant-color=\"FBFBF9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2022-12-12 12-33-48<\/span><span class=\"informations\">1302\u00d7785 107 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-13T01:48:44.904Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/roland_roms\">@roland_roms<\/a> , please see <a href=\"https:\/\/wandb.ai\/mohammadbakir\/Legend-Test?workspace=user-mohammadbakir\">this project example<\/a> that produces the following chart.  I used<\/p>\n<p><code>[[ ${x}: ${y} ]] ${run:displayName}:${config:animal}<\/code> to customize the legend and include animal types. Is this what you intend to accomplish?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/c\/c9e65e451857ca3dcc0ae8f53abd2820af307193.png\" data-download-href=\"\/uploads\/short-url\/sO5x3MvHl2n7R0aeubOwsUZoBZF.png?dl=1\" title=\"CustomLegend\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/c\/c9e65e451857ca3dcc0ae8f53abd2820af307193_2_690x227.png\" alt=\"CustomLegend\" data-base62-sha1=\"sO5x3MvHl2n7R0aeubOwsUZoBZF\" width=\"690\" height=\"227\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/c\/c9e65e451857ca3dcc0ae8f53abd2820af307193_2_690x227.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/c\/c9e65e451857ca3dcc0ae8f53abd2820af307193_2_1035x340.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/c\/c9e65e451857ca3dcc0ae8f53abd2820af307193_2_1380x454.png 2x\" data-dominant-color=\"F7F8F8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">CustomLegend<\/span><span class=\"informations\">1821\u00d7600 101 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport random\n\ncategories = [\"cat\",\"dog\",\"frog\"]\n\nfor animal_type in categories:\n    config = {\"animal\": animal_type}\n\n    run1 = wandb.init(\n        project=\"Legend-Test\",\n        config=config,\n    )\n\n    epochs = 10\n    offset = random.random() \/ 5\n    for epoch in range(epochs):\n        acc = 1 - 2 ** -epoch - random.random()  - offset \n        run1.log({\"acc\": acc})\n\n    run1.finish()\n<\/code><\/pre>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-13T02:15:47.560Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> . Thank you so much for your time and answer!<br>\nYour solution would be great in a different scenario, but it\u2019s not what I aim for.<\/p>\n<ul>\n<li>In your example, there is one line for each category (you have one run for \u201ccat\u201d, one for \u201cdog\u201d, etc). With multiple values for each one of them.<\/li>\n<li>What I have is one value per category. But multiple types of runs.<\/li>\n<\/ul>\n<p>Let\u2019s say that I want to compare two kinds of models (Yolov4 or Yolov5) for finding animals in images.<br>\nI train my model, then test yolov4 on 100 images of dogs, 100 images of cats, etc. Same for Yolov5.<br>\nI then get an average score for yolov4 on cats. then a score for yolov4 on dogs, etc. Same for yolov5.<\/p>\n<p>Now I want to compare both models: first line are the scores for yolov4 (score for cats, then for dogs, then for frog). Second line are the same scores, but for yolov5.<\/p>\n<p>Now each point of the line is for a category (cats, dogs, frogs). With the current system, I can see that \u201cfor the third point, yolov4 is better than yolov5\u201d. But I cannot say what the third point is (cats, dogs, frogs, or something else).<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-16T23:44:58.807Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/roland_roms\">@roland_roms<\/a> , thank you for the additional info and clarification. This is not doable natively via wandb and your proposed work around solution is the closest currently. As our charts use <a href=\"https:\/\/vega.github.io\/vega\/\" rel=\"noopener nofollow ugc\">Vega<\/a>, you may be able to accomplish this using <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. Please do let me know if you have any other questions.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-14T23:45:45.542Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to customize advanced legend to display a string; Content: hello w&b community! for a custom project within our company, we attempt to compare various models across a list of category. we therefore want to visualize, for each version of our model: for each category (a string) a score (float in 0,1). here is how we log scores in w&b (simplified for easy understanding): scores = [0.3, 0.5, 0.6, ...] categories = ['dog', 'cat', 'fish', ...] for k, (val, name) in enumerate(zip(scores, categories)): .log({ f\"score\": val, f\"category\": name, f\"id\": k }) here is the graph associated with this code: screenshot from 2022-12-05 14-29-411389751 103 kb now as you can see in the picture: for each version (td_ver), we display the score (y) for each id (x). when we hover over the score itself, we do not want to show the id, but the name (in the image, instead of displaying 25 highlighted in red, we want to display fish. i assume this change needs to happen in the advanced legend section, but i could not find the correct spelling for that formula. original: [[ ${x}: ${y} (${original})]] ${run:displayname} new: ??? thanks a lot for your help!",
        "Question_original_content_gpt_summary":"The user is attempting to customize an advanced legend to display a string instead of a numerical value when hovering over a score.",
        "Question_preprocessed_content":"Title: how to customize advanced legend to display a string; Content: hello w&b community! for a custom project within our company, we attempt to compare various models across a list of category. we therefore want to visualize, for each version of our model for each category a score . here is how we log scores in w&b here is the graph associated with this code screenshot from kb now as you can see in the picture for each version , we display the score for each id . when we hover over the score itself, we do not want to show the id, but the name in the image, instead of displaying highlighted in red, we want to display fish. i assume this change needs to happen in the advanced legend section, but i could not find the correct spelling for that formula. thanks a lot for your help!",
        "Answer_original_content":"hi @roland_roms , happy to help. to update your legend to read fish instead of the id (x), use the following [[ fish: ${y} ]] ${run:displayname} please let me know if you have any questions. hello @mohammadbakir , thank you for your answer! sorry if my question wasnt asked properly: i do not want the string fish to be displayed, but the name of the category. as you can see in the code i shared previously, each score is associated with a category: (scores[1] is for the categories[1], etc). with your solution, it would replace all occurrences of the id ( ${x} ) by fish. what i need is to have the first score with the first category (cat), the second score with the second category (dog), and so on hi @roland_roms , appreciate the clarification. if you were to include your class categories in the run configuration, then you can reference that in your legend, see here. this would look like [[ ${config:category}: ${y} ]] ${run:displayname} mohammadbakir: you were to include your class categories in the run configuration, then you can reference that in thank you for your answer, but it does not work. following your advice, i added categories to the config, and tried to call it in various ways in advanced legend: import scores = [0.3, 0.5, 0.6] categories = ['dog', 'cat', 'fish'] config = {\"categories\": categories, \"id_cat\": {0: \"dog\", 1:\"cat\", 2:\"fish\"}} .init(project=\"dummy_test\", name=\"dummy_demo\", config=config) for k, (val, name) in enumerate(zip(scores, categories)): .log({ \"score\": val, \"category\": name, \"categories\": name, \"id_cat\": name, \"id\": k }) .finish() as you can see, i tried to add it in various ways (categories or id_cat) and to call them in various ways inside the advanced legend. it never worked properly. regarding the documentation you shared here, two points are to be highlighted: you can set value inside[[ ]] to display point specific values in the crosshair when hovering over a chart and supported values inside [] are as follows: config is not part of these supported values. summary is also not supported. therefore, i dont think it is possible to display it this way. screenshot from 2022-12-12 12-07-531833612 84.3 kb i have found a temporary solution, that gives a similar result (but not exactly the same). i run a first time, logging only categories: import scores = [0.3, 0.5, 0.6] categories = ['dog', 'cat', 'fish'] .init(project=\"dummy_test\", name=\"categories\") for k, (val, name) in enumerate(zip(scores, categories)): .log({ # \"score\": val, \"category\": name }) .finish() i can then run logs without the categories, but with the scores (i run a first time with the version_1 scores, then a second time with version_2 scores, etc): import scores = [0.3, 0.5, 0.6] categories = ['dog', 'cat', 'fish'] .init(project=\"dummy_test\", name=\"version_1\") for k, (val, name) in enumerate(zip(scores, categories)): .log({ \"score\": val, # \"category\": name }) .finish() in display pannel, i create a new line plot with the following data as y: category (added manually via regular expression) score in advanced legend, i set up the formula as: [[ ${x}: ${y} ]] ${run:displayname} it gives me the following results: (top: first point appearing as dog. bottom: second point appearing as cat) screenshot from 2022-12-12 12-33-481302785 107 kb hi @roland_roms , please see this project example that produces the following chart. i used [[ ${x}: ${y} ]] ${run:displayname}:${config:animal} to customize the legend and include animal types. is this what you intend to accomplish? customlegend1821600 101 kb import import random categories = [\"cat\",\"dog\",\"frog\"] for animal_type in categories: config = {\"animal\": animal_type} run1 = .init( project=\"legend-test\", config=config, ) epochs = 10 offset = random.random() \/ 5 for epoch in range(epochs): acc = 1 - 2 ** -epoch - random.random() - offset run1.log({\"acc\": acc}) run1.finish() hi @mohammadbakir . thank you so much for your time and answer! your solution would be great in a different scenario, but its not what i aim for. in your example, there is one line for each category (you have one run for cat, one for dog, etc). with multiple values for each one of them. what i have is one value per category. but multiple types of runs. lets say that i want to compare two kinds of models (yolov4 or yolov5) for finding animals in images. i train my model, then test yolov4 on 100 images of dogs, 100 images of cats, etc. same for yolov5. i then get an average score for yolov4 on cats. then a score for yolov4 on dogs, etc. same for yolov5. now i want to compare both models: first line are the scores for yolov4 (score for cats, then for dogs, then for frog). second line are the same scores, but for yolov5. now each point of the line is for a category (cats, dogs, frogs). with the current system, i can see that for the third point, yolov4 is better than yolov5. but i cannot say what the third point is (cats, dogs, frogs, or something else). hi @roland_roms , thank you for the additional info and clarification. this is not doable natively via and your proposed work around solution is the closest currently. as our charts use vega, you may be able to accomplish this using custom charts. please do let me know if you have any other questions. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The user wants to customize an advanced legend to display the name of the category instead of a numerical value when hovering over a score. One possible solution is to include the class categories in the run configuration and reference it in the legend using the formula [[ ${config:category}: ${y} ]] ${run:displayname}. However, this solution did not work for the user. Another temporary solution is to log the categories separately and then log the scores with a reference to the categories. The user can then create a line plot with the category as y and score as x and use the formula [[ ${x}: ${y} ]] ${run:displayname} in the advanced legend. The support team suggests that this is the closest solution currently available and that it is not possible to display the category name natively via the advanced legend. They also suggest exploring custom charts using Vega.",
        "Answer_preprocessed_content":"hi , happy to help. to update your legend to read fish instead of the id , use the following please let me know if you have any questions. hello , thank you for your answer! sorry if my question wasnt asked properly i do not want the string fish to be displayed, but the name of the category. as you can see in the code i shared previously, each score is associated with a category . with your solution, it would replace all occurrences of the id by fish. what i need is to have the first score with the first category , the second score with the second category , and so on hi , appreciate the clarification. if you were to include your class categories in the run configuration, then you can reference that in your legend, see here. this would look like mohammadbakir you were to include your class categories in the run configuration, then you can reference that in thank you for your answer, but it does not work. following your advice, i added categories to the config, and tried to call it in various ways in advanced legend as you can see, i tried to add it in various ways and to call them in various ways inside the advanced legend. it never worked properly. regarding the documentation you shared here, two points are to be highlighted you can set value inside to display point specific values in the crosshair when hovering over a chart and supported values inside are as follows config is not part of these supported values. summary is also not supported. therefore, i dont think it is possible to display it this way. screenshot from kb i have found a temporary solution, that gives a similar result . i run a first time, logging only categories i can then run logs without the categories, but with the scores in display pannel, i create a new line plot with the following data as y category score in advanced legend, i set up the formula as $ $ $ it gives me the following results top first point appearing as dog. bottom second point appearing as cat screenshot from kb hi , please see this project example that produces the following chart. i used to customize the legend and include animal types. is this what you intend to accomplish? customlegend kb hi . thank you so much for your time and answer! your solution would be great in a different scenario, but its not what i aim for. in your example, there is one line for each category . with multiple values for each one of them. what i have is one value per category. but multiple types of runs. lets say that i want to compare two kinds of models for finding animals in images. i train my model, then test yolov on images of dogs, images of cats, etc. same for yolov . i then get an average score for yolov on cats. then a score for yolov on dogs, etc. same for yolov . now i want to compare both models first line are the scores for yolov . second line are the same scores, but for yolov . now each point of the line is for a category . with the current system, i can see that for the third point, yolov is better than yolov . but i cannot say what the third point is . hi , thank you for the additional info and clarification. this is not doable natively via and your proposed work around solution is the closest currently. as our charts use vega, you may be able to accomplish this using custom charts. please do let me know if you have any other questions. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Archive \/ Share a snapshot of a DVC remote",
        "Question_body":"<p>Dear DVC team,<\/p>\n<p>Thank you for the great work and coming with an agnostic approach for DVC!<\/p>\n<p>It seems that a) one could create a tar.gz of the directory used as a DVC remote and b) that someone else could unpack this directory somewhere else and use it as a local remote.<\/p>\n<p><strong>Is there any counter-argument for distributing an archive of a DVC remote?<\/strong><\/p>\n<p>I have seen that it seems fine according to <a href=\"https:\/\/discuss.dvc.org\/t\/copying-a-dvc-repository\/213\" class=\"inline-onebox\">Copying a dvc repository<\/a>. But it was in another context (filesystem specificities).<\/p>\n<p>Why archiving a DVC remote? <strong>The idea is to share publicly a snapshot of a DVC remote.<\/strong><\/p>\n<p>One way could be to <code>dvc push<\/code> to a specific public remote when someone would want to create a snapshot. However, this implies two things one might not want. First, this requires a dedicated public server. Second, this prevents from having a DOI for the snapshot.<\/p>\n<p><strong>Would you have a better suggestion than distributing an archive to share a snapshot of a DVC remote?<\/strong><\/p>\n<p>Thanks,<br>\nPierre-Alexandre<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1613033068998,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":341.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/archive-share-a-snapshot-of-a-dvc-remote\/664",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-11T09:17:52.641Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/pafonta\">@pafonta<\/a>!<br>\nThank you for sharing your thoughts on the DVC.<\/p>\n<p>As to the point you have been making:<br>\nWhat is your use case? What kind of remote are you using?<br>\nIt seems to me that in many cases snapshotting the remote or the cache could be achieved by <code>zip<\/code>-ping or <code>tar<\/code>-ing the <code>cache<\/code>\/<code>remote<\/code> directory. Do you think we need a special command for that?<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-11T10:05:17.340Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a>!<\/p>\n<p>Thank you for your prompt reply.<\/p>\n<aside class=\"quote no-group\" data-username=\"Paffciu\" data-post=\"2\" data-topic=\"664\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/p\/d07c76\/40.png\" class=\"avatar\"> Paffciu:<\/div>\n<blockquote>\n<p>It seems to me that in many cases snapshotting the remote or the cache could be achieved by <code>zip<\/code> -ping or <code>tar<\/code> -ing the <code>cache<\/code> \/ <code>remote<\/code> directory.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Great!<\/p>\n<p>In this case, this is a SSH remote.<\/p>\n<p>The use case is from academia. In a nutshell. One uses DVC for a ML project which has public milestones. People should be able to reproduce experiments \/ models for each milestones. But in-between, evolutions should be private. Also, each snapshot should get a DOI (<a href=\"https:\/\/en.wikipedia.org\/wiki\/Digital_object_identifier\" rel=\"noopener nofollow ugc\">Wikipedia<\/a>) to be a companion to, for example, a scientific paper. I hope it clarifies.<\/p>\n<p>Regarding getting a special DVC command for snapshots, I guess the use case above is common in academia or will become common in the 1-3 years horizon. Also, for other types of remote or extremely huge tracked data and models, it could be tricky to do it by hand. So, I think that it could become handy for DVC to have such feature. I hope it could help make DVC even greater.<\/p>\n<p>Have a good day!<\/p>",
                "Answer_score":116.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-11T10:12:14.365Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/pafonta\">@pafonta<\/a><br>\nThank you very much for clearing this up.<br>\nYour use case makes perfect sense, and need to have ability to \u201cofficialy\u201d prepare snapshot sounds reasonable in case of assigining DOI. Could I ask you to create feature request for that on our github? We try to keep development discussions there so that we have a single place of discussion history.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-11T15:18:59.522Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> If it could help the community, here is the <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5450\" rel=\"noopener nofollow ugc\">GitHub issue<\/a>.<\/p>",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: archive \/ share a snapshot of a remote; Content: dear team, thank you for the great work and coming with an agnostic approach for ! it seems that a) one could create a tar.gz of the directory used as a remote and b) that someone else could unpack this directory somewhere else and use it as a local remote. is there any counter-argument for distributing an archive of a remote? i have seen that it seems fine according to copying a repository. but it was in another context (filesystem specificities). why archiving a remote? the idea is to share publicly a snapshot of a remote. one way could be to push to a specific public remote when someone would want to create a snapshot. however, this implies two things one might not want. first, this requires a dedicated public server. second, this prevents from having a doi for the snapshot. would you have a better suggestion than distributing an archive to share a snapshot of a remote? thanks, pierre-alexandre",
        "Question_original_content_gpt_summary":"The user is looking for a way to archive and share a snapshot of a remote, while avoiding the need for a dedicated public server and the lack of a DOI for the snapshot.",
        "Question_preprocessed_content":"Title: archive \/ share a snapshot of a remote; Content: dear team, thank you for the great work and coming with an agnostic approach for ! it seems that a one could create a of the directory used as a remote and b that someone else could unpack this directory somewhere else and use it as a local remote. is there any for distributing an archive of a remote? i have seen that it seems fine according to copying a repository. but it was in another context . why archiving a remote? the idea is to share publicly a snapshot of a remote. one way could be to to a specific public remote when someone would want to create a snapshot. however, this implies two things one might not want. first, this requires a dedicated public server. second, this prevents from having a doi for the snapshot. would you have a better suggestion than distributing an archive to share a snapshot of a remote? thanks,",
        "Answer_original_content":"hello @pafonta! thank you for sharing your thoughts on the . as to the point you have been making: what is your use case? what kind of remote are you using? it seems to me that in many cases snapshotting the remote or the cache could be achieved by zip-ping or tar-ing the cache\/remote directory. do you think we need a special command for that? hello @paffciu! thank you for your prompt reply. paffciu: it seems to me that in many cases snapshotting the remote or the cache could be achieved by zip -ping or tar -ing the cache \/ remote directory. great! in this case, this is a ssh remote. the use case is from academia. in a nutshell. one uses for a ml project which has public milestones. people should be able to reproduce experiments \/ models for each milestones. but in-between, evolutions should be private. also, each snapshot should get a doi (wikipedia) to be a companion to, for example, a scientific paper. i hope it clarifies. regarding getting a special command for snapshots, i guess the use case above is common in academia or will become common in the 1-3 years horizon. also, for other types of remote or extremely huge tracked data and models, it could be tricky to do it by hand. so, i think that it could become handy for to have such feature. i hope it could help make even greater. have a good day! @pafonta thank you very much for clearing this up. your use case makes perfect sense, and need to have ability to officialy prepare snapshot sounds reasonable in case of assigining doi. could i ask you to create feature request for that on our github? we try to keep development discussions there so that we have a single place of discussion history. @paffciu if it could help the community, here is the github issue.",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's problem of archiving and sharing a snapshot of a remote while avoiding the need for a dedicated public server and the lack of a DOI for the snapshot include zipping or tarring the cache\/remote directory, creating a special command for snapshots, and creating a feature request on Github for the ability to officially prepare a snapshot with a DOI. The use case for this is in academia, where people need to reproduce experiments\/models for public milestones while keeping evolutions private.",
        "Answer_preprocessed_content":"hello thank you for sharing your thoughts on the . as to the point you have been making what is your use case? what kind of remote are you using? it seems to me that in many cases snapshotting the remote or the cache could be achieved by ping or ing the \/ directory. do you think we need a special command for that? hello thank you for your prompt reply. paffciu it seems to me that in many cases snapshotting the remote or the cache could be achieved by ping or ing the \/ directory. great! in this case, this is a ssh remote. the use case is from academia. in a nutshell. one uses for a ml project which has public milestones. people should be able to reproduce experiments \/ models for each milestones. but evolutions should be private. also, each snapshot should get a doi to be a companion to, for example, a scientific paper. i hope it clarifies. regarding getting a special command for snapshots, i guess the use case above is common in academia or will become common in the years horizon. also, for other types of remote or extremely huge tracked data and models, it could be tricky to do it by hand. so, i think that it could become handy for to have such feature. i hope it could help make even greater. have a good day! thank you very much for clearing this up. your use case makes perfect sense, and need to have ability to officialy prepare snapshot sounds reasonable in case of assigining doi. could i ask you to create feature request for that on our github? we try to keep development discussions there so that we have a single place of discussion history. if it could help the community, here is the github issue."
    },
    {
        "Question_id":null,
        "Question_title":"DVC support for the local storage",
        "Question_body":"<p>HI<br>\nCan we add local storage for DVC. like (i dont want to store it on s3 or gcp, need  to only point to local storage)<br>\nex :<\/p>\n<ol>\n<li>dvc add file:\\\\ xxx.xx.xx.x\\images\\annex\\dvc-storage<br>\nor<\/li>\n<li>dvc add X:\/annex\/dvc-storage\/data.xml ( local storage)<br>\nAfter trying above option. i am getting error.<br>\nInitialization error: Config file error: Unsupported URL.<br>\nPlease provide an appropriate solution or syntax<\/li>\n<\/ol>\n<p>Note : storage is tyron.   the storage location is mounted to window or on linux.<\/p>\n<p>With ref : <a href=\"https:\/\/discuss.dvc.org\/t\/does-dvc-fit-in-a-local-area-network-infrastucture-where-git-repos-are-not-in-the-computing-server\/24\/3\" class=\"inline-onebox\">Does DVC fit in a Local Area Network infrastucture where git repos are not in the computing server?<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1534489981766,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":2069.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-support-for-the-local-storage\/71",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2018-08-17T08:47:28.936Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jagdeesh\">@jagdeesh<\/a> !<\/p>\n<p>If I\u2019ve understood your scenario correctly from the previous post, you want to use mounted <code>dvc-storage<\/code> directory as a local remote for your dvc projects, so you and your team could <code>dvc push\/pull<\/code> from\/to it the cache for your projects, right? If so, you should use this command(I assume you are running Windows):<\/p>\n<pre><code class=\"lang-auto\">$ dvc remote add myremote X:\\\\annex\\dvc-storage -d\n<\/code><\/pre>\n<p>which will tell dvc where the remote storage is located and after that you could simply use<\/p>\n<pre><code class=\"lang-auto\">$ dvc push # to upload your cache to the local storage\n$ dvc pull # to download cache from the local storage\n<\/code><\/pre>\n<p>Our <a href=\"https:\/\/dvc.org\/doc\/get-started\">\u201cget started\u201d guide<\/a> actually goes through the process of setting up a dvc project with a \u201clocal remote storage\u201d, which looks to be precisely what you are trying to accomplish. Please take a look at it, it should be useful. Feel free to followup with any questions, we are always happy to help.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":148.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-08-17T09:18:53.739Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>!.<\/p>\n<p>Thanks for the quick reply it resolved my issue with small changes.<\/p>\n<p>Url stored in the config<br>\ndvc  add X:\\annex\\dvc-storage store in config file as<br>\nX:\\annex\\dvc-storage<\/p>\n<p>with changes ;<br>\ndvc  add X:\\annex\\dvc-storage   in config file  (\\) it double back slash.<br>\nX:\\annex\\dvc-storage<\/p>\n<p>Thank tyou<\/p>",
                "Answer_score":48.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: support for the local storage; Content: hi can we add local storage for . like (i dont want to store it on s3 or gcp, need to only point to local storage) ex : add file:\\\\ xxx.xx.xx.x\\images\\annex\\-storage or add x:\/annex\/-storage\/data.xml ( local storage) after trying above option. i am getting error. initialization error: config file error: unsupported url. please provide an appropriate solution or syntax note : storage is tyron. the storage location is mounted to window or on linux. with ref : does fit in a local area network infrastucture where git repos are not in the computing server?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with adding local storage for their data, and is receiving an initialization error when trying to do so.",
        "Question_preprocessed_content":"Title: support for the local storage; Content: hi can we add local storage for . like ex add or add after trying above option. i am getting error. initialization error config file error unsupported url. please provide an appropriate solution or syntax note storage is tyron. the storage location is mounted to window or on linux. with ref does fit in a local area network infrastucture where git repos are not in the computing server?",
        "Answer_original_content":"hi @jagdeesh ! if ive understood your scenario correctly from the previous post, you want to use mounted -storage directory as a local remote for your projects, so you and your team could push\/pull from\/to it the cache for your projects, right? if so, you should use this command(i assume you are running windows): $ remote add myremote x:\\\\annex\\-storage -d which will tell where the remote storage is located and after that you could simply use $ push # to upload your cache to the local storage $ pull # to download cache from the local storage our get started guide actually goes through the process of setting up a project with a local remote storage, which looks to be precisely what you are trying to accomplish. please take a look at it, it should be useful. feel free to followup with any questions, we are always happy to help. thanks, ruslan hi @kupruser!. thanks for the quick reply it resolved my issue with small changes. url stored in the config add x:\\annex\\-storage store in config file as x:\\annex\\-storage with changes ; add x:\\annex\\-storage in config file (\\) it double back slash. x:\\annex\\-storage thank tyou",
        "Answer_original_content_gpt_summary":"The solution to the user's challenge with adding local storage for their data is to use the \"remote add\" command to specify the location of the remote storage, and then use the \"push\" and \"pull\" commands to upload and download cache to and from the local storage. The user can also refer to the get started guide for setting up a project with a local remote storage. Additionally, the user can store the URL in the config file as x:\\annex\\-storage with changes and add x:\\annex\\-storage in the config file.",
        "Answer_preprocessed_content":"hi ! if ive understood your scenario correctly from the previous post, you want to use mounted directory as a local remote for your projects, so you and your team could it the cache for your projects, right? if so, you should use this command which will tell where the remote storage is located and after that you could simply use our get started guide actually goes through the process of setting up a project with a local remote storage, which looks to be precisely what you are trying to accomplish. please take a look at it, it should be useful. feel free to followup with any questions, we are always happy to help. thanks, ruslan hi thanks for the quick reply it resolved my issue with small changes. url stored in the config add store in config file as with changes ; add in config file it double back slash. thank tyou"
    },
    {
        "Question_id":56418684.0,
        "Question_title":"Possible to access the internal representation of a neural network trained in Azure Machine Learning Service or Azure Machine Learning Studio?",
        "Question_body":"<p>I'm working with data scientists who would like to gain insight and understanding of the neural network models that they train using the visual interfaces in Azure Machine Learning Studio\/Service. Is it possible to dump out and inspect the internal representation of a neural network model? Is there a way that I could write code that accesses the nodes and weights of a trained neural network in order to visualize the network as a graph structure? Or if Azure Machine Learning Studio\/Service doesn't support this I'd appreciate advice on a different machine learning framework that might be more appropriate for this kind of analysis.<\/p>\n\n<p>Things I have tried:<\/p>\n\n<ul>\n<li>Train Model outputs an ILearnerDotNet (AML Studio) or Model (AML Service). I looked for items to drag into the workspace where I could write custom code such as Execute Python Script. They seem to accept datasets, but not ILearnerDotNet\/Model as input.<\/li>\n<li>I wasn't able to locate documentation about the ILearnerDotNet\/Model interfaces.<\/li>\n<li>Selecting the Train Model output offers the option to Save as Trained Model. This creates a trained model object and that would help me reference the trained model in other places, but I didn't find a way to use this to get at its internals.<\/li>\n<\/ul>\n\n<p>I'm new to the Azure Machine Learning landscape, and could use some help with how to get started on how to access this data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1559506858340,
        "Question_favorite_count":1.0,
        "Question_score":2.0,
        "Question_view_count":167.0,
        "Owner_creation_time":1427643193808,
        "Owner_last_access_time":1663710082516,
        "Owner_reputation":45.0,
        "Owner_up_votes":8.0,
        "Owner_down_votes":0.0,
        "Owner_views":7.0,
        "Answer_body":"<p>Quote from Azure ML Exam reference:<\/p>\n\n<blockquote>\n  <p>By default, the architecture of neural networks is limited to a single\n  hidden layer with sigmoid as the activation function and softmax in\n  the last layer. You can change this in the properties of the model,\n  opening the Hidden layer specification dropdown list, and selecting a\n  Custom definition script. A text box will appear in which you will be\n  able to insert a Net# script. This script language allows you to\n  define neural networks architectures.<\/p>\n<\/blockquote>\n\n<p>For instance, if you want to create a two layer network, you may put the following code.<\/p>\n\n<pre><code>input Picture [28, 28];\nhidden H1 [200] from Picture all;\nhidden H2 [200] from H1 all;\noutput Result [10] softmax from H2 all;\n<\/code><\/pre>\n\n<p>Nevertheless, with Net# you will face certain limitations as, it does not accept regularization (neither L2 nor dropout). Also, there is no ReLU activation that are\ncommonly used in deep learning due to their benefits in backpropagation. You cannot modify the batch size of the Stochastic Gradient Descent (SGD). Besides that, you cannot use other optimization algorithms. You can use SGD with momentum, but not others like Adam, or RMSprop. You cannot define recurrent or recursive neural networks.<\/p>\n\n<p>Another great tool is CNTK (Cognitive Toolkit) that allows you defining your computational graph and create a fully customizable model.\nQuote from documentation<\/p>\n\n<blockquote>\n  <p>It is a Microsoft open source deep learning toolkit. Like other deep\n  learning tools, CNTK is based on the construction of computational\n  graphs and their optimization using automatic differentiation. The\n  toolkit is highly optimized and scales efficiently (from CPU, to GPU,\n  to multiple machines). CNTK is also very portable and flexible; you\n  can use it with programming languages like Python, C#, or C++, but you\n  can also use a model description language called BrainScript.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1559896269912,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56418684",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: possible to access the internal representation of a neural network trained in service or studio?; Content: i'm working with data scientists who would like to gain insight and understanding of the neural network models that they train using the visual interfaces in studio\/service. is it possible to dump out and inspect the internal representation of a neural network model? is there a way that i could write code that accesses the nodes and weights of a trained neural network in order to visualize the network as a graph structure? or if studio\/service doesn't support this i'd appreciate advice on a different machine learning framework that might be more appropriate for this kind of analysis. things i have tried: train model outputs an ilearnerdotnet (aml studio) or model (aml service). i looked for items to drag into the workspace where i could write custom code such as execute python script. they seem to accept datasets, but not ilearnerdotnet\/model as input. i wasn't able to locate documentation about the ilearnerdotnet\/model interfaces. selecting the train model output offers the option to save as trained model. this creates a trained model object and that would help me reference the trained model in other places, but i didn't find a way to use this to get at its internals. i'm new to the landscape, and could use some help with how to get started on how to access this data.",
        "Question_original_content_gpt_summary":"The user is looking for a way to access the internal representation of a neural network model trained in Studio\/Service in order to visualize the network as a graph structure.",
        "Question_preprocessed_content":"Title: possible to access the internal representation of a neural network trained in service or studio?; Content: i'm working with data scientists who would like to gain insight and understanding of the neural network models that they train using the visual interfaces in is it possible to dump out and inspect the internal representation of a neural network model? is there a way that i could write code that accesses the nodes and weights of a trained neural network in order to visualize the network as a graph structure? or if doesn't support this i'd appreciate advice on a different machine learning framework that might be more appropriate for this kind of analysis. things i have tried train model outputs an ilearnerdotnet or model . i looked for items to drag into the workspace where i could write custom code such as execute python script. they seem to accept datasets, but not as input. i wasn't able to locate documentation about the interfaces. selecting the train model output offers the option to save as trained model. this creates a trained model object and that would help me reference the trained model in other places, but i didn't find a way to use this to get at its internals. i'm new to the landscape, and could use some help with how to get started on how to access this data.",
        "Answer_original_content":"quote from exam reference: by default, the architecture of neural networks is limited to a single hidden layer with sigmoid as the activation function and softmax in the last layer. you can change this in the properties of the model, opening the hidden layer specification dropdown list, and selecting a custom definition script. a text box will appear in which you will be able to insert a net# script. this script language allows you to define neural networks architectures. for instance, if you want to create a two layer network, you may put the following code. input picture [28, 28]; hidden h1 [200] from picture all; hidden h2 [200] from h1 all; output result [10] softmax from h2 all; nevertheless, with net# you will face certain limitations as, it does not accept regularization (neither l2 nor dropout). also, there is no relu activation that are commonly used in deep learning due to their benefits in backpropagation. you cannot modify the batch size of the stochastic gradient descent (sgd). besides that, you cannot use other optimization algorithms. you can use sgd with momentum, but not others like adam, or rmsprop. you cannot define recurrent or recursive neural networks. another great tool is cntk (cognitive toolkit) that allows you defining your computational graph and create a fully customizable model. quote from documentation it is a microsoft open source deep learning toolkit. like other deep learning tools, cntk is based on the construction of computational graphs and their optimization using automatic differentiation. the toolkit is highly optimized and scales efficiently (from cpu, to gpu, to multiple machines). cntk is also very portable and flexible; you can use it with programming languages like python, c#, or c++, but you can also use a model description language called brainscript.",
        "Answer_original_content_gpt_summary":"Possible solutions to access the internal representation of a neural network model trained in Studio\/Service and visualize it as a graph structure are:\n\n1. Change the architecture of the neural network model by selecting a custom definition script in the hidden layer specification dropdown list and inserting a net# script to define the neural network architecture. However, there are certain limitations with net# such as no regularization, no relu activation, no modification of batch size, and no use of other optimization algorithms.\n\n2. Use cntk (cognitive toolkit), a Microsoft open source deep learning toolkit, to define a computational graph and create a fully customizable model. Cntk is highly optimized, scales efficiently, and is portable and flexible. It can be used with programming languages like Python, C#, or C++ or a model description language called brainscript.",
        "Answer_preprocessed_content":"quote from exam reference by default, the architecture of neural networks is limited to a single hidden layer with sigmoid as the activation function and softmax in the last layer. you can change this in the properties of the model, opening the hidden layer specification dropdown list, and selecting a custom definition script. a text box will appear in which you will be able to insert a net script. this script language allows you to define neural networks architectures. for instance, if you want to create a two layer network, you may put the following code. nevertheless, with net you will face certain limitations as, it does not accept regularization . also, there is no relu activation that are commonly used in deep learning due to their benefits in backpropagation. you cannot modify the batch size of the stochastic gradient descent . besides that, you cannot use other optimization algorithms. you can use sgd with momentum, but not others like adam, or rmsprop. you cannot define recurrent or recursive neural networks. another great tool is cntk that allows you defining your computational graph and create a fully customizable model. quote from documentation it is a microsoft open source deep learning toolkit. like other deep learning tools, cntk is based on the construction of computational graphs and their optimization using automatic differentiation. the toolkit is highly optimized and scales efficiently . cntk is also very portable and flexible; you can use it with programming languages like python, c , or c++, but you can also use a model description language called brainscript."
    },
    {
        "Question_id":null,
        "Question_title":"Account storage not being freed?",
        "Question_body":"<p>I deleted all my projects in order to free up my account storage, but the usage dashboard page still shows it as being used. 36gb of the 100gb available to be precise. Isn\u2019t it supposed to be reclaimed after the projects\/artifacts are deleted?<\/p>\n<p>I used all this space just by uploading some Driverless AI (<a href=\"https:\/\/www.h2o.ai\/products\/h2o-driverless-ai\/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">H2O Driverless AI | H2O.ai<\/a>) models and temporary files I was experimenting with. But if I certainly won\u2019t continue doing this if this space is gone for good.<\/p>\n<p>I can\u2019t really complain as I\u2019ve a free account, but I wonder if you are charging your paying customers for deleted files too\u2026<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_creation_time":1637707347325,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":228.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/account-storage-not-being-freed\/1375",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-25T13:11:02.558Z",
                "Answer_body":"<p>Hey there,<\/p>\n<p>The storage should be reclaimed. This appears to be a bug on our end. Can you tell me your username so I can take a look?<\/p>\n<p>Best,<br>\nArman<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-27T01:17:23.798Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/armanharutyunyan\">@armanharutyunyan<\/a>, my account is <code>ogoid<\/code>, thanks.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-10T05:12:02.603Z",
                "Answer_body":"<p>Hello! Our engineers are aware of the issue and will be working to ensure the storage you see matches the actual amount of storage you occupy.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-23T14:57:11.387Z",
                "Answer_body":"<p>Hello Andreas,<\/p>\n<p>The issue with the incorrectly displayed used storage has been fixed. I have double checked your account to ensure all is proper. Let me know if you still are facing any issues.<\/p>\n<p>Regards,<br>\nAnish<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-23T14:58:20.713Z",
                "Answer_body":"<p>Hello Andreas,<\/p>\n<p>There actually still seems to be lingering issues which I have escalated to our engineers. Ignore the last message.<\/p>\n<p>Regards,<br>\nAnish Shah<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-26T01:18:10.436Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: account storage not being freed?; Content: i deleted all my projects in order to free up my account storage, but the usage dashboard page still shows it as being used. 36gb of the 100gb available to be precise. isn\u2019t it supposed to be reclaimed after the projects\/artifacts are deleted? i used all this space just by uploading some driverless ai (h2o driverless ai | h2o.ai) models and temporary files i was experimenting with. but if i certainly won\u2019t continue doing this if this space is gone for good. i can\u2019t really complain as i\u2019ve a free account, but i wonder if you are charging your paying customers for deleted files too\u2026",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where their account storage is not being freed up after deleting all their projects, despite the usage dashboard page still showing 36gb of the 100gb available being used.",
        "Question_preprocessed_content":"Title: account storage not being freed?; Content: i deleted all my projects in order to free up my account storage, but the usage dashboard page still shows it as being used. gb of the gb available to be precise. isnt it supposed to be reclaimed after the are deleted? i used all this space just by uploading some driverless ai models and temporary files i was experimenting with. but if i certainly wont continue doing this if this space is gone for good. i cant really complain as ive a free account, but i wonder if you are charging your paying customers for deleted files too",
        "Answer_original_content":"hey there, the storage should be reclaimed. this appears to be a bug on our end. can you tell me your username so i can take a look? best, arman @armanharutyunyan, my account is ogoid, thanks. hello! our engineers are aware of the issue and will be working to ensure the storage you see matches the actual amount of storage you occupy. hello andreas, the issue with the incorrectly displayed used storage has been fixed. i have double checked your account to ensure all is proper. let me know if you still are facing any issues. regards, anish hello andreas, there actually still seems to be lingering issues which i have escalated to our engineers. ignore the last message. regards, anish shah this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- The user can provide their username to the support team to investigate the issue.\n- The support team acknowledges the issue and assures the user that their engineers are working on it.\n- The support team confirms that the issue has been fixed and double-checks the user's account to ensure everything is proper.\n- The support team informs the user that there are still lingering issues and escalates the matter to their engineers.",
        "Answer_preprocessed_content":"hey there, the storage should be reclaimed. this appears to be a bug on our end. can you tell me your username so i can take a look? best, arman my account is , thanks. hello! our engineers are aware of the issue and will be working to ensure the storage you see matches the actual amount of storage you occupy. hello andreas, the issue with the incorrectly displayed used storage has been fixed. i have double checked your account to ensure all is proper. let me know if you still are facing any issues. regards, anish hello andreas, there actually still seems to be lingering issues which i have escalated to our engineers. ignore the last message. regards, anish shah this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Realtime endpoint deploy 'xxx' stayed in progress",
        "Question_body":"Hi everyone,\n\nI'm trying to deploy my real time inference, but after a while not happened to it. I didn't got any error or message and nothing happens. (See the following picture).\n\nWhat I did:\n\nI created a pipeline in designer, it's valid and be submitted very well.\n\n\nAfter submit is complete, I created a \"Real-time inference pipeline\" via button in menu!\n\n\n\n\nAnd finally I tried to deploy it, but nothing happened. (For deploy I tried both of Azure Kubernetes Service and Azure Container Instance)",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1661074944827,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/975300\/realtime-endpoint-deploy-39xxx39-stayed-in-progres.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-22T22:20:19.56Z",
                "Answer_score":0,
                "Answer_body":"Hello @ImranShams-7088\n\nYou have been enabled for one-time Free Technical Support. To create the support request, please do the following as detailed below. It may take up to 1 hour for the free support ticket enablement to go through.\n\n\u2022 Go to the Health Advisory section within the Azure Portal: https:\/\/aka.ms\/healthadvisories\n\u2022 Select the Issue Name \"You have been enabled for one-time Free Technical Support\"\n\u2022 Details will populate below in the Summary Tab within the reading pane and you can click on the link \"Create a Support Request\" to the right of the message\n\nOnce created, please share the ticket number with me. Thanks.\n\nRegards,\nYutong",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: realtime endpoint deploy 'xxx' stayed in progress; Content: hi everyone, i'm trying to deploy my real time inference, but after a while not happened to it. i didn't got any error or message and nothing happens. (see the following picture). what i did: i created a pipeline in designer, it's valid and be submitted very well. after submit is complete, i created a \"real-time inference pipeline\" via button in menu! and finally i tried to deploy it, but nothing happened. (for deploy i tried both of azure kubernetes service and azure container instance)",
        "Question_original_content_gpt_summary":"The user encountered challenges with deploying their real-time inference pipeline, as the process stayed in progress without any errors or messages.",
        "Question_preprocessed_content":"Title: realtime endpoint deploy 'xxx' stayed in progress; Content: hi everyone, i'm trying to deploy my real time inference, but after a while not happened to it. i didn't got any error or message and nothing happens. . what i did i created a pipeline in designer, it's valid and be submitted very well. after submit is complete, i created a inference pipeline via button in menu! and finally i tried to deploy it, but nothing happened.",
        "Answer_original_content":"hello @imranshams-7088 you have been enabled for one-time free technical support. to create the support request, please do the following as detailed below. it may take up to 1 hour for the free support ticket enablement to go through. go to the health advisory section within the azure portal: https:\/\/aka.ms\/healthadvisories select the issue name \"you have been enabled for one-time free technical support\" details will populate below in the summary tab within the reading pane and you can click on the link \"create a support request\" to the right of the message once created, please share the ticket number with me. thanks. regards, yutong",
        "Answer_original_content_gpt_summary":"The answer does not provide any solutions to the user's challenge with deploying their real-time inference pipeline. Instead, it offers instructions on how to create a support request for one-time free technical support through the Azure portal.",
        "Answer_preprocessed_content":"hello you have been enabled for free technical support. to create the support request, please do the following as detailed below. it may take up to hour for the free support ticket enablement to go through. go to the health advisory section within the azure portal select the issue name you have been enabled for free technical support details will populate below in the summary tab within the reading pane and you can click on the link create a support request to the right of the message once created, please share the ticket number with me. thanks. regards, yutong"
    },
    {
        "Question_id":null,
        "Question_title":"Pipeline depending on multiple of the same operation",
        "Question_body":"<p>I have the following setup:<\/p>\n<pre><code class=\"lang-yaml\">- model: my_model\n  operations:\n    train: \n      main: scripts.train\n      flags:\n        a: 1\n    evaluate: \n      main: scripts.evaluate\n      requires:\n          - operation: train\n    train_evaluate:\n      flags:\n        a: 1\n      steps:\n      - run: train a=${a}\n      - run: evaluate\n    compare_evaluate:\n      main: scripts.compare\n      requires:\n        - train_evaluate_run_1\n        - train_evaluate_run_2\n    compare:\n      steps:\n      - run: train_evaluate a=1\n      - run: train_evaluate a=2\n      - run: compare_evaluate # HERE\nresources:\n  train_evaluate_run_1:\n    - operation: train_evaluate\n      name: train_evaluate_run_1\n  train_evaluate_run_2:\n    - operation: train_evaluate\n      name: train_evaluate_run_2\n<\/code><\/pre>\n<p>How to tell guild to resolve two different resources in the <code>compare_evaluate<\/code> run (with the # HERE tag)?<\/p>\n<p>The goal is to have a script\/notebook that takes as input two <code>train_evaluate<\/code> runs and compares them using custom plotting etc. and have all that specified in a single pipeline.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1655386885562,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":112.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/pipeline-depending-on-multiple-of-the-same-operation\/891",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-17T07:30:59.793Z",
                "Answer_body":"<p>Would you consider creating an addition pipeline to include the two different runs?<\/p>\n<pre><code class=\"lang-yaml\">op:\n  ...\n\nwrapper:\n  steps:\n    - op [flags 1]\n    - op [flags 2]\n\ncompare: \n  ...\n  requires:\n    - operation: wrapper\n      select: \n        - file: op\/XXX\n        - file: op_2\/XXX # guild automatically rename operations by adding _[#] \n                         # if there are multiple steps with the same operation\n\npipeline: \n  steps:\n    - wrapper\n    - compare\n<\/code><\/pre>\n<p>I am not 100% this would work but here\u2019s a direction for you to try.<\/p>",
                "Answer_score":21.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-11T18:47:08.755Z",
                "Answer_body":"<p>Tried it out and it works. Thx for your suggestion!<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pipeline depending on multiple of the same operation; Content: i have the following setup: - model: my_model operations: train: main: scripts.train flags: a: 1 evaluate: main: scripts.evaluate requires: - operation: train train_evaluate: flags: a: 1 steps: - run: train a=${a} - run: evaluate compare_evaluate: main: scripts.compare requires: - train_evaluate_run_1 - train_evaluate_run_2 compare: steps: - run: train_evaluate a=1 - run: train_evaluate a=2 - run: compare_evaluate # here resources: train_evaluate_run_1: - operation: train_evaluate name: train_evaluate_run_1 train_evaluate_run_2: - operation: train_evaluate name: train_evaluate_run_2 how to tell guild to resolve two different resources in the compare_evaluate run (with the # here tag)? the goal is to have a script\/notebook that takes as input two train_evaluate runs and compares them using custom plotting etc. and have all that specified in a single pipeline.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in which they need to tell Guild to resolve two different resources in the compare_evaluate run in order to have a script\/notebook that takes as input two train_evaluate runs and compares them using custom plotting.",
        "Question_preprocessed_content":"Title: pipeline depending on multiple of the same operation; Content: i have the following setup how to tell guild to resolve two different resources in the run ? the goal is to have a that takes as input two runs and compares them using custom plotting etc. and have all that specified in a single pipeline.",
        "Answer_original_content":"would you consider creating an addition pipeline to include the two different runs? op: ... wrapper: steps: - op [flags 1] - op [flags 2] compare: ... requires: - operation: wrapper select: - file: op\/xxx - file: op_2\/xxx # guild automatically rename operations by adding _[#] # if there are multiple steps with the same operation pipeline: steps: - wrapper - compare i am not 100% this would work but heres a direction for you to try. tried it out and it works. thx for your suggestion!",
        "Answer_original_content_gpt_summary":"The possible solution suggested in the answer is to create an additional pipeline that includes the two different runs. The pipeline would consist of two steps: a wrapper and a compare step. The wrapper step would include the two different runs, and the compare step would compare them using custom plotting. The answer suggests that this solution may work, but it is not guaranteed.",
        "Answer_preprocessed_content":"would you consider creating an addition pipeline to include the two different runs? i am not % this would work but heres a direction for you to try. tried it out and it works. thx for your suggestion!"
    },
    {
        "Question_id":null,
        "Question_title":"Help for Azure ML Studio experiment mapping",
        "Question_body":"I am learning Azure ML (Studio) and please help me for below scenarios,\nI have a bank customer data having column labelled as customer age, family members (1,2,3 &4), credit card (Yes \/no) Personal Loan (Yes \/no), education (1. Undergrad 2. Graduate 3. Advanced\/professional).\n\nHow to filter age column and find number of customers less than 45 years in % of total number of customers?\n\n\nAlso need % customers who are having credit card as well Personal loan?\n\n\nwhich education category of customers are more prone to subscribe to personal loan?\n\n\nHow to do visual analysis?\n\n\nHow to calculate correlation between 2 columns?\n\nThanks in advance for your guidance. and incase tag to wrong group please guide to appropriate group",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1620716372030,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/390316\/help-for-azure-ml-studio-experiment-mapping.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-11T23:07:34.347Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. I am assuming you are using Azure ML Studio Designer to work on this scenario. Please review response below:\n\n\n\n\nHow to filter age column and find number of customers less than 45 years in % of total number of customers? As part of your data preparations step, you can use available data transformation modules such as apply sql transformation and apply math operation to perform data transformations.\n\nAlso need % customers who are having credit card as well Personal loan? Similar to 1 above.\n\nWhich education category of customers are more prone to subscribe to personal loan? This seems to be a multi-classification problem where you'd want to predict several categories. AML Studio has the following modules and algorithms for predicting classes. For future reference, this document helps you identify which algorithm to select based on the ML scenario.\n\nHow to do visual analysis? With Azure ML Designer, you can Right Click on a module and select Visualize to visualize dataset output or results.\n\nHow to calculate correlation between 2 columns? You can use Filter Based Feature Selection to identify the columns in your input dataset that have the greatest predictive power. The module includes correlation methods such as Pearson correlation and Chi-Squared.\n\n\n\n\nAlso, here are some useful resources to help get you started:\n\nAzure Machine Learning Documentation.\n\n\nSample tutorials are available in Designer (newer drag and drop interface. Click Designer, select More Samples) and Classic (older drag and drop interface, via Azure AI Gallery, although some modules may not be available in designer, but a great starting point as well).\n\nHope this helps!",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: help for studio experiment mapping; Content: i am learning (studio) and please help me for below scenarios, i have a bank customer data having column labelled as customer age, family members (1,2,3 &4), credit card (yes \/no) personal loan (yes \/no), education (1. undergrad 2. graduate 3. advanced\/professional). how to filter age column and find number of customers less than 45 years in % of total number of customers? also need % customers who are having credit card as well personal loan? which education category of customers are more prone to subscribe to personal loan? how to do visual analysis? how to calculate correlation between 2 columns? thanks in advance for your guidance. and incase tag to wrong group please guide to appropriate group",
        "Question_original_content_gpt_summary":"The user is seeking guidance on how to filter an age column, calculate correlation between two columns, and do visual analysis in order to answer questions about the bank customer data they have, such as the percentage of customers with credit cards and personal loans, and which education category is more prone to personal loan subscription.",
        "Question_preprocessed_content":"Title: help for studio experiment mapping; Content: i am learning and please help me for below scenarios, i have a bank customer data having column labelled as customer age, family members , credit card personal loan , education . how to filter age column and find number of customers less than years in % of total number of customers? also need % customers who are having credit card as well personal loan? which education category of customers are more prone to subscribe to personal loan? how to do visual analysis? how to calculate correlation between columns? thanks in advance for your guidance. and incase tag to wrong group please guide to appropriate group",
        "Answer_original_content":"hi, thanks for reaching out. i am assuming you are using studio designer to work on this scenario. please review response below: how to filter age column and find number of customers less than 45 years in % of total number of customers? as part of your data preparations step, you can use available data transformation modules such as apply sql transformation and apply math operation to perform data transformations. also need % customers who are having credit card as well personal loan? similar to 1 above. which education category of customers are more prone to subscribe to personal loan? this seems to be a multi-classification problem where you'd want to predict several categories. aml studio has the following modules and algorithms for predicting classes. for future reference, this document helps you identify which algorithm to select based on the ml scenario. how to do visual analysis? with designer, you can right click on a module and select visualize to visualize dataset output or results. how to calculate correlation between 2 columns? you can use filter based feature selection to identify the columns in your input dataset that have the greatest predictive power. the module includes correlation methods such as pearson correlation and chi-squared. also, here are some useful resources to help get you started: documentation. sample tutorials are available in designer (newer drag and drop interface. click designer, select more samples) and classic (older drag and drop interface, via azure ai gallery, although some modules may not be available in designer, but a great starting point as well). hope this helps!",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include using data transformation modules to filter the age column and find the percentage of customers less than 45 years old, using the same method to find the percentage of customers with credit cards and personal loans, using AML Studio's modules and algorithms for multi-classification problems to determine which education category is more prone to personal loan subscription, using the \"visualize\" option in Designer to do visual analysis, and using filter-based feature selection to calculate correlation between two columns. The answer also provides additional resources for further guidance.",
        "Answer_preprocessed_content":"hi, thanks for reaching out. i am assuming you are using studio designer to work on this scenario. please review response below how to filter age column and find number of customers less than years in % of total number of customers? as part of your data preparations step, you can use available data transformation modules such as apply sql transformation and apply math operation to perform data transformations. also need % customers who are having credit card as well personal loan? similar to above. which education category of customers are more prone to subscribe to personal loan? this seems to be a problem where you'd want to predict several categories. aml studio has the following modules and algorithms for predicting classes. for future reference, this document helps you identify which algorithm to select based on the ml scenario. how to do visual analysis? with designer, you can right click on a module and select visualize to visualize dataset output or results. how to calculate correlation between columns? you can use filter based feature selection to identify the columns in your input dataset that have the greatest predictive power. the module includes correlation methods such as pearson correlation and also, here are some useful resources to help get you started documentation. sample tutorials are available in designer and classic . hope this helps!"
    },
    {
        "Question_id":70202848.0,
        "Question_title":"Is it possible to \"apt install\" in SageMaker Studio Lab?",
        "Question_body":"<p>I have started using SageMaker Studio Lab.<\/p>\n<p>When I run &quot;apt install xvfb&quot; in SageMaker Studio Lab Notebook, I get the following error.<\/p>\n<pre><code>!apt install xvfb\n\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\n<\/code><\/pre>\n<p>Then I tried with sudo, but the sudo command was not installed.<\/p>\n<pre><code>!sudo apt install xvfb\n\n\/usr\/bin\/sh: 1: sudo: not found\n<\/code><\/pre>\n<p>Can you please tell me how to solve this problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1638461318997,
        "Question_favorite_count":1.0,
        "Question_score":1.0,
        "Question_view_count":1174.0,
        "Owner_creation_time":1481861289276,
        "Owner_last_access_time":1643180283947,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":"<p>To date Studio Lab doesn't support package installs that require root access. It does support packages installable via pip and conda. You can do that either in your notebook with the %, rather than the !, or you can do that via opening a terminal.<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html<\/a><\/li>\n<\/ul>\n<p>If you'd like to open an issue you're welcome to do that on our repository right here:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md<\/a><\/li>\n<\/ul>\n<p>Thanks for trying out Studio Lab!<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1638468298390,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70202848",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is it possible to \"apt install\" in studio lab?; Content: i have started using studio lab. when i run \"apt install xvfb\" in studio lab notebook, i get the following error. !apt install xvfb e: could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: permission denied) e: unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root? then i tried with sudo, but the sudo command was not installed. !sudo apt install xvfb \/usr\/bin\/sh: 1: sudo: not found can you please tell me how to solve this problem?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with attempting to \"apt install\" in studio lab, as they are receiving permission denied errors and the sudo command is not installed.",
        "Question_preprocessed_content":"Title: is it possible to apt install in studio lab?; Content: i have started using studio lab. when i run apt install xvfb in studio lab notebook, i get the following error. then i tried with sudo, but the sudo command was not installed. can you please tell me how to solve this problem?",
        "Answer_original_content":"to date studio lab doesn't support package installs that require root access. it does support packages installable via pip and conda. you can do that either in your notebook with the %, rather than the !, or you can do that via opening a terminal. https:\/\/docs.aws.amazon.com\/\/latest\/dg\/studio-lab-use-manage.html if you'd like to open an issue you're welcome to do that on our repository right here: https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/issue_template\/bug-report-for--studio-lab.md thanks for trying out studio lab!",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's challenge with \"apt install\" in studio lab are to use packages installable via pip and conda, which can be done in the notebook with the % command or by opening a terminal. However, studio lab does not support package installs that require root access. The user can also open an issue on the AWS repository if they encounter any issues.",
        "Answer_preprocessed_content":"to date studio lab doesn't support package installs that require root access. it does support packages installable via pip and conda. you can do that either in your notebook with the %, rather than the !, or you can do that via opening a terminal. if you'd like to open an issue you're welcome to do that on our repository right here thanks for trying out studio lab!"
    },
    {
        "Question_id":null,
        "Question_title":"Pytorch sync_tensorboard help",
        "Question_body":"<p>I\u2019m having some issues getting W&amp;B to sync with Tensorboard in PyTorch. According to this <a href=\"https:\/\/github.com\/wandb\/client\/issues\/493\" rel=\"noopener nofollow ugc\">issue<\/a> and the docs, I should initializing <code>SummaryWriter<\/code> after W&amp;B  <code>init<\/code> possibly using <code>wandb.tensorboard.patch<\/code>. So far I haven\u2019t been able to get this to work with either <code>torch.utils.tensorboard<\/code> or <code>tensorboardX<\/code> and with or without the patch. Not sure if this is a bug or I\u2019m missing something. Thanks.<\/p>\n<p>Windows 11<br>\nPython 3.8.8<br>\nwandb 0.12.4<br>\ntorch 1.9.1<\/p>\n<pre><code class=\"lang-auto\"> wandb.tensorboard.patch(root_logdir=\"logs\")\n wandb.init(config=hyperparameter_defaults, project=f\"ppo_{env_name}_torch\", sync_tensorboard=True, save_code=True, name=run_name)\n config = wandb.config\n writer = SummaryWriter(f\"logs\")\n<\/code><\/pre>",
        "Question_answer_count":8,
        "Question_comment_count":null,
        "Question_creation_time":1634500284534,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":633.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/pytorch-sync-tensorboard-help\/1017",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-19T15:30:08.699Z",
                "Answer_body":"<p>Hey there, taking a look!<\/p>",
                "Answer_score":18.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-26T12:59:29.832Z",
                "Answer_body":"<p>Hey there, are you getting an error? If so could you send the stack trace?<\/p>",
                "Answer_score":18.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-26T14:02:42.372Z",
                "Answer_body":"<p>I don\u2019t get any errors. The code runs and logs to Tensorboard, but nothing gets logged to W&amp;B. Here\u2019s an example run:<\/p>\n<p>dazzling-microwave-39: <a href=\"https:\/\/wandb.ai\/tims457\/mbrl\/runs\/8x3u2qoi\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>",
                "Answer_score":3.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-28T17:17:55.706Z",
                "Answer_body":"<p>Could you share the debug logs for this run? They are in the run directory relative to your script.<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-29T23:55:11.620Z",
                "Answer_body":"<p>debug.log<\/p>\n<pre><code class=\"lang-auto\">2021-10-26 07:58:59,137 INFO    MainThread:33572 [wandb_setup.py:_flush():71] setting env: {}\n2021-10-26 07:58:59,137 INFO    MainThread:33572 [wandb_setup.py:_flush():71] setting login settings: {}\n2021-10-26 07:58:59,138 INFO    MainThread:33572 [wandb_init.py:_log_setup():357] Logging user logs to C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\logs\\debug.log\n2021-10-26 07:58:59,138 INFO    MainThread:33572 [wandb_init.py:_log_setup():358] Logging internal logs to C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\logs\\debug-internal.log\n2021-10-26 07:58:59,139 INFO    MainThread:33572 [wandb_init.py:init():390] calling init triggers\n2021-10-26 07:58:59,139 INFO    MainThread:33572 [wandb_init.py:init():395] wandb.init called with sweep_config: {}\nconfig: {'gamma': 0.99, 'gae_lambda': 0.95, 'eps': 0.2, 'batch_size': 128, 'timesteps': 2000, 'n_epochs': 40, 'c1': 0.5, 'c2': 0.001, 'actor_lr': 0.0003, 'critic_lr': 0.001, 'decay_lr': True, 'lr_decay': 1, 'epochs': 150, 'test_freq': 20, 'reward_threshold': 150}\n2021-10-26 07:58:59,139 INFO    MainThread:33572 [wandb_init.py:init():435] starting backend\n2021-10-26 07:58:59,139 INFO    MainThread:33572 [backend.py:_multiprocessing_setup():94] multiprocessing start_methods=spawn, using: spawn\n2021-10-26 07:58:59,151 INFO    MainThread:33572 [backend.py:ensure_launched():198] starting backend process...\n2021-10-26 07:58:59,266 INFO    MainThread:33572 [backend.py:ensure_launched():203] started backend process with pid: 16652\n2021-10-26 07:58:59,267 INFO    MainThread:33572 [wandb_init.py:init():444] backend started and connected\n2021-10-26 07:58:59,267 INFO    MainThread:33572 [wandb_init.py:init():503] updated telemetry\n2021-10-26 07:58:59,346 INFO    MainThread:33572 [wandb_init.py:init():533] communicating current version\n2021-10-26 07:59:01,269 INFO    MainThread:33572 [wandb_init.py:init():538] got version response \n2021-10-26 07:59:01,269 INFO    MainThread:33572 [wandb_init.py:init():548] communicating run to backend with 30 second timeout\n2021-10-26 07:59:01,362 INFO    MainThread:33572 [wandb_init.py:init():576] starting run threads in backend\n2021-10-26 07:59:06,373 INFO    MainThread:33572 [wandb_run.py:_console_start():1693] atexit reg\n2021-10-26 07:59:06,374 INFO    MainThread:33572 [wandb_run.py:_redirect():1567] redirect: SettingsConsole.WRAP\n2021-10-26 07:59:06,374 INFO    MainThread:33572 [wandb_run.py:_redirect():1604] Wrapping output streams.\n2021-10-26 07:59:06,377 INFO    MainThread:33572 [wandb_run.py:_redirect():1628] Redirects installed.\n2021-10-26 07:59:06,377 INFO    MainThread:33572 [wandb_init.py:init():603] run started, returning control to user process\n2021-10-26 07:59:06,377 INFO    MainThread:33572 [wandb_watch.py:watch():43] Watching\n2021-10-26 07:59:06,378 INFO    MainThread:33572 [wandb_run.py:_tensorboard_callback():984] tensorboard callback: logs, None\n2021-10-26 08:01:02,573 INFO    MainThread:33572 [wandb_run.py:_atexit_cleanup():1663] got exitcode: 255\n2021-10-26 08:01:02,575 INFO    MainThread:33572 [wandb_run.py:_restore():1635] restore\n2021-10-26 08:01:02,833 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: file_counts {\n  wandb_count: 2\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 41021\n  total_bytes: 41021\n}\n\n2021-10-26 08:01:07,935 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: None\n2021-10-26 08:01:09,929 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: file_counts {\n  wandb_count: 2\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 41021\n  total_bytes: 41021\n}\n\n2021-10-26 08:01:10,043 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: file_counts {\n  wandb_count: 2\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 41021\n  total_bytes: 41021\n}\n\n2021-10-26 08:01:10,560 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: file_counts {\n  wandb_count: 6\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 41021\n  total_bytes: 66539\n}\n\n2021-10-26 08:01:10,667 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: file_counts {\n  wandb_count: 7\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 41021\n  total_bytes: 88164\n}\n\n2021-10-26 08:01:10,769 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: file_counts {\n  wandb_count: 7\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 88164\n  total_bytes: 88164\n}\n\n2021-10-26 08:01:10,871 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: file_counts {\n  wandb_count: 7\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 88164\n  total_bytes: 88164\n}\n\n2021-10-26 08:01:10,972 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: file_counts {\n  wandb_count: 7\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 88164\n  total_bytes: 88164\n}\n\n2021-10-26 08:01:11,104 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: file_counts {\n  wandb_count: 7\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 88164\n  total_bytes: 88164\n}\n\n2021-10-26 08:01:11,319 INFO    MainThread:33572 [wandb_run.py:_wait_for_finish():1793] got exit ret: done: true\nexit_result {\n}\nfile_counts {\n  wandb_count: 7\n  other_count: 1\n}\npusher_stats {\n  uploaded_bytes: 88164\n  total_bytes: 88164\n}\nlocal_info {\n}\n\n2021-10-26 08:01:12,789 INFO    MainThread:33572 [wandb_run.py:_append_history():2011] rendering history\n2021-10-26 08:01:12,789 INFO    MainThread:33572 [wandb_run.py:_append_summary():1966] rendering summary\n2021-10-26 08:01:12,789 INFO    MainThread:33572 [wandb_run.py:_append_files():2061] logging synced files\n\n<\/code><\/pre>",
                "Answer_score":8.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-29T23:55:44.926Z",
                "Answer_body":"<p>debug-internal.log<\/p>\n<pre><code class=\"lang-auto\">2021-10-26 07:59:01,207 INFO    MainThread:16652 [internal.py:wandb_internal():88] W&amp;B internal server running at pid: 16652, started at: 2021-10-26 07:59:01.207113\n2021-10-26 07:59:01,209 INFO    WriterThread:16652 [datastore.py:open_for_write():77] open: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\run-8x3u2qoi.wandb\n2021-10-26 07:59:01,209 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: check_version\n2021-10-26 07:59:01,213 DEBUG   SenderThread:16652 [sender.py:send():185] send: header\n2021-10-26 07:59:01,213 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: check_version\n2021-10-26 07:59:01,270 DEBUG   SenderThread:16652 [sender.py:send():185] send: run\n2021-10-26 07:59:01,362 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: run_start\n2021-10-26 07:59:01,364 INFO    SenderThread:16652 [dir_watcher.py:__init__():169] watching files in: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\n2021-10-26 07:59:01,364 INFO    SenderThread:16652 [sender.py:_start_run_threads():768] run started: 8x3u2qoi with start time 1635256739\n2021-10-26 07:59:01,364 DEBUG   SenderThread:16652 [sender.py:send():185] send: summary\n2021-10-26 07:59:01,364 INFO    SenderThread:16652 [sender.py:_save_file():901] saving file wandb-summary.json with policy end\n2021-10-26 07:59:02,365 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_created():217] file\/dir created: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\wandb-summary.json\n2021-10-26 07:59:03,592 DEBUG   HandlerThread:16652 [meta.py:__init__():40] meta init\n2021-10-26 07:59:03,593 DEBUG   HandlerThread:16652 [meta.py:__init__():54] meta init done\n2021-10-26 07:59:03,593 DEBUG   HandlerThread:16652 [meta.py:probe():212] probe\n2021-10-26 07:59:03,645 DEBUG   HandlerThread:16652 [meta.py:_setup_git():202] setup git\n2021-10-26 07:59:03,796 DEBUG   HandlerThread:16652 [meta.py:_setup_git():209] setup git done\n2021-10-26 07:59:03,796 DEBUG   HandlerThread:16652 [meta.py:_save_code():90] save code\n2021-10-26 07:59:03,873 DEBUG   HandlerThread:16652 [meta.py:_save_code():111] save code done\n2021-10-26 07:59:03,873 DEBUG   HandlerThread:16652 [meta.py:_save_patches():128] save patches\n2021-10-26 07:59:04,310 DEBUG   HandlerThread:16652 [meta.py:_save_patches():170] save patches done\n2021-10-26 07:59:04,310 DEBUG   HandlerThread:16652 [meta.py:_save_pip():58] save pip\n2021-10-26 07:59:04,311 DEBUG   HandlerThread:16652 [meta.py:_save_pip():72] save pip done\n2021-10-26 07:59:04,312 DEBUG   HandlerThread:16652 [meta.py:_save_conda():79] save conda\n2021-10-26 07:59:04,364 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_created():217] file\/dir created: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\conda-environment.yaml\n2021-10-26 07:59:04,365 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_created():217] file\/dir created: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\code\\ppo_torch.py\n2021-10-26 07:59:04,365 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_created():217] file\/dir created: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\requirements.txt\n2021-10-26 07:59:04,365 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_created():217] file\/dir created: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\diff.patch\n2021-10-26 07:59:04,365 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_created():217] file\/dir created: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\code\n2021-10-26 07:59:07,367 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_created():217] file\/dir created: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:11,371 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:15,374 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:18,406 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\conda-environment.yaml\n2021-10-26 07:59:18,407 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:18,630 DEBUG   HandlerThread:16652 [meta.py:_save_conda():87] save conda done\n2021-10-26 07:59:18,631 DEBUG   HandlerThread:16652 [meta.py:probe():250] probe done\n2021-10-26 07:59:18,636 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: stop_status\n2021-10-26 07:59:18,636 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: stop_status\n2021-10-26 07:59:18,638 INFO    HandlerThread:16652 [handler.py:handle_tbrecord():640] handling tbrecord: tbrecord {\n  log_dir: \"logs\"\n  save: true\n}\n\n2021-10-26 07:59:18,639 DEBUG   HandlerThread:16652 [config_util.py:dict_from_config_file():101] no default config file found in config-defaults.yaml\n2021-10-26 07:59:18,667 DEBUG   Thread-30 :16652 [tb_watcher.py:_process_events():269] Encountered tensorboard directory watcher error: [WinError 1314] A required privilege is not held by the client: 'C:\\\\Users\\\\tim\\\\GitHub\\\\rl_agents\\\\logs\\\\events.out.tfevents.1635256746.DESKTOP-7MG5U16.33572.0' -&gt; 'C:\\\\Users\\\\tim\\\\GitHub\\\\rl_agents\\\\wandb\\\\run-20211026_075859-8x3u2qoi\\\\files\\\\events.out.tfevents.1635256746.DESKTOP-7MG5U16.33572.0'\n2021-10-26 07:59:18,695 DEBUG   SenderThread:16652 [sender.py:send():185] send: tbrecord\n2021-10-26 07:59:18,696 DEBUG   SenderThread:16652 [sender.py:send():185] send: files\n2021-10-26 07:59:18,696 INFO    SenderThread:16652 [sender.py:_save_file():901] saving file wandb-metadata.json with policy now\n2021-10-26 07:59:18,697 INFO    SenderThread:16652 [sender.py:_save_file():901] saving file code\\ppo_torch.py with policy now\n2021-10-26 07:59:18,699 INFO    SenderThread:16652 [sender.py:_save_file():901] saving file diff.patch with policy now\n2021-10-26 07:59:18,987 INFO    Thread-31 :16652 [upload_job.py:push():137] Uploaded file C:\\Users\\tim\\AppData\\Local\\Temp\\tmp70meo4ibwandb\\1turuybf-wandb-metadata.json\n2021-10-26 07:59:18,999 INFO    Thread-33 :16652 [upload_job.py:push():137] Uploaded file C:\\Users\\tim\\AppData\\Local\\Temp\\tmp70meo4ibwandb\\2dtlm77i-diff.patch\n2021-10-26 07:59:19,045 INFO    Thread-32 :16652 [upload_job.py:push():137] Uploaded file C:\\Users\\tim\\AppData\\Local\\Temp\\tmp70meo4ibwandb\\1pyzaeuj-code\/ppo_torch.py\n2021-10-26 07:59:19,409 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_created():217] file\/dir created: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\wandb-metadata.json\n2021-10-26 07:59:22,410 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:24,411 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:26,373 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: stop_status\n2021-10-26 07:59:26,373 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: stop_status\n2021-10-26 07:59:28,416 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:32,419 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:32,654 DEBUG   SenderThread:16652 [sender.py:send():185] send: stats\n2021-10-26 07:59:36,420 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:38,423 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:41,439 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: stop_status\n2021-10-26 07:59:41,439 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: stop_status\n2021-10-26 07:59:42,425 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:44,427 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:48,431 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:52,435 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:54,441 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 07:59:56,493 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: stop_status\n2021-10-26 07:59:56,493 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: stop_status\n2021-10-26 07:59:58,444 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:02,453 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:03,625 DEBUG   SenderThread:16652 [sender.py:send():185] send: stats\n2021-10-26 08:00:06,457 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:10,458 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:11,551 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: stop_status\n2021-10-26 08:00:11,551 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: stop_status\n2021-10-26 08:00:12,460 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:16,463 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:18,464 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:22,468 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:24,470 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:26,622 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: stop_status\n2021-10-26 08:00:26,622 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: stop_status\n2021-10-26 08:00:28,472 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:30,475 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:34,478 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:34,490 DEBUG   SenderThread:16652 [sender.py:send():185] send: stats\n2021-10-26 08:00:38,481 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:40,481 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:41,679 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: stop_status\n2021-10-26 08:00:41,679 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: stop_status\n2021-10-26 08:00:44,485 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:46,489 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:50,492 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:52,493 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:56,496 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:00:56,755 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: stop_status\n2021-10-26 08:00:56,755 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: stop_status\n2021-10-26 08:01:00,500 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:01:02,500 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:01:02,569 WARNING MainThread:16652 [internal.py:wandb_internal():147] Internal process interrupt: 1\n2021-10-26 08:01:02,578 DEBUG   SenderThread:16652 [sender.py:send():185] send: history\n2021-10-26 08:01:02,579 DEBUG   SenderThread:16652 [sender.py:send():185] send: summary\n2021-10-26 08:01:02,582 INFO    SenderThread:16652 [sender.py:_save_file():901] saving file wandb-summary.json with policy end\n2021-10-26 08:01:02,828 DEBUG   SenderThread:16652 [sender.py:send():185] send: telemetry\n2021-10-26 08:01:02,829 DEBUG   SenderThread:16652 [sender.py:send():185] send: exit\n2021-10-26 08:01:02,829 INFO    SenderThread:16652 [sender.py:send_exit():317] handling exit code: 255\n2021-10-26 08:01:02,829 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:02,829 INFO    SenderThread:16652 [sender.py:send_exit():319] handling runtime: 121\n2021-10-26 08:01:02,831 INFO    SenderThread:16652 [sender.py:_save_file():901] saving file wandb-summary.json with policy end\n2021-10-26 08:01:02,832 INFO    SenderThread:16652 [sender.py:send_exit():329] send defer\n2021-10-26 08:01:02,832 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:02,832 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:02,833 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 0\n2021-10-26 08:01:02,833 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:02,833 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 0\n2021-10-26 08:01:02,833 INFO    SenderThread:16652 [sender.py:transition_state():342] send defer: 1\n2021-10-26 08:01:02,833 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:02,833 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 1\n2021-10-26 08:01:02,923 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:02,923 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 1\n2021-10-26 08:01:02,923 INFO    SenderThread:16652 [sender.py:transition_state():342] send defer: 2\n2021-10-26 08:01:02,924 DEBUG   SenderThread:16652 [sender.py:send():185] send: stats\n2021-10-26 08:01:02,924 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:02,925 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 2\n2021-10-26 08:01:03,502 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\wandb-summary.json\n2021-10-26 08:01:03,503 INFO    Thread-12 :16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:01:09,927 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:09,927 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:09,927 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 2\n2021-10-26 08:01:09,927 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:09,928 INFO    SenderThread:16652 [sender.py:transition_state():342] send defer: 3\n2021-10-26 08:01:09,928 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:09,928 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:09,928 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:09,929 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 3\n2021-10-26 08:01:09,930 DEBUG   SenderThread:16652 [sender.py:send():185] send: summary\n2021-10-26 08:01:09,934 INFO    SenderThread:16652 [sender.py:_save_file():901] saving file wandb-summary.json with policy end\n2021-10-26 08:01:09,934 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:09,934 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 3\n2021-10-26 08:01:09,934 INFO    SenderThread:16652 [sender.py:transition_state():342] send defer: 4\n2021-10-26 08:01:09,935 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:09,936 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 4\n2021-10-26 08:01:09,936 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:09,936 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 4\n2021-10-26 08:01:10,030 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:10,042 INFO    SenderThread:16652 [sender.py:transition_state():342] send defer: 5\n2021-10-26 08:01:10,043 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:10,043 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:10,043 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 5\n2021-10-26 08:01:10,044 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:10,044 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 5\n2021-10-26 08:01:10,044 INFO    SenderThread:16652 [dir_watcher.py:finish():283] shutting down directory watcher\n2021-10-26 08:01:10,145 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:10,509 INFO    SenderThread:16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\wandb-summary.json\n2021-10-26 08:01:10,510 INFO    SenderThread:16652 [dir_watcher.py:_on_file_modified():230] file\/dir modified: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\config.yaml\n2021-10-26 08:01:10,512 INFO    SenderThread:16652 [dir_watcher.py:finish():313] scan: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\n2021-10-26 08:01:10,513 INFO    SenderThread:16652 [dir_watcher.py:finish():327] scan save: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\conda-environment.yaml conda-environment.yaml\n2021-10-26 08:01:10,515 INFO    SenderThread:16652 [dir_watcher.py:finish():327] scan save: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\config.yaml config.yaml\n2021-10-26 08:01:10,517 INFO    SenderThread:16652 [dir_watcher.py:finish():327] scan save: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\diff.patch diff.patch\n2021-10-26 08:01:10,518 INFO    SenderThread:16652 [dir_watcher.py:finish():327] scan save: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log output.log\n2021-10-26 08:01:10,535 INFO    SenderThread:16652 [dir_watcher.py:finish():327] scan save: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\requirements.txt requirements.txt\n2021-10-26 08:01:10,537 INFO    SenderThread:16652 [dir_watcher.py:finish():327] scan save: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\wandb-metadata.json wandb-metadata.json\n2021-10-26 08:01:10,537 INFO    SenderThread:16652 [dir_watcher.py:finish():327] scan save: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\wandb-summary.json wandb-summary.json\n2021-10-26 08:01:10,553 INFO    SenderThread:16652 [dir_watcher.py:finish():327] scan save: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\code\\ppo_torch.py code\\ppo_torch.py\n2021-10-26 08:01:10,553 INFO    SenderThread:16652 [sender.py:transition_state():342] send defer: 6\n2021-10-26 08:01:10,554 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:10,556 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:10,557 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 6\n2021-10-26 08:01:10,559 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:10,559 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 6\n2021-10-26 08:01:10,559 INFO    SenderThread:16652 [file_pusher.py:finish():177] shutting down file pusher\n2021-10-26 08:01:10,666 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:10,666 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:10,768 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:10,768 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:10,804 INFO    Thread-37 :16652 [upload_job.py:push():137] Uploaded file C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\requirements.txt\n2021-10-26 08:01:10,842 INFO    Thread-34 :16652 [upload_job.py:push():137] Uploaded file C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\conda-environment.yaml\n2021-10-26 08:01:10,856 INFO    Thread-36 :16652 [upload_job.py:push():137] Uploaded file C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\output.log\n2021-10-26 08:01:10,856 INFO    Thread-35 :16652 [upload_job.py:push():137] Uploaded file C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\config.yaml\n2021-10-26 08:01:10,856 INFO    Thread-38 :16652 [upload_job.py:push():137] Uploaded file C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\files\\wandb-summary.json\n2021-10-26 08:01:10,870 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:10,870 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:10,971 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:10,971 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:11,057 INFO    Thread-11 :16652 [sender.py:transition_state():342] send defer: 7\n2021-10-26 08:01:11,057 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:11,057 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 7\n2021-10-26 08:01:11,057 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:11,058 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 7\n2021-10-26 08:01:11,073 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:11,103 INFO    SenderThread:16652 [sender.py:transition_state():342] send defer: 8\n2021-10-26 08:01:11,103 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:11,104 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:11,104 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 8\n2021-10-26 08:01:11,104 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:11,104 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 8\n2021-10-26 08:01:11,104 INFO    SenderThread:16652 [sender.py:transition_state():342] send defer: 9\n2021-10-26 08:01:11,105 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: defer\n2021-10-26 08:01:11,105 INFO    HandlerThread:16652 [handler.py:handle_request_defer():147] handle defer: 9\n2021-10-26 08:01:11,105 DEBUG   SenderThread:16652 [sender.py:send():185] send: final\n2021-10-26 08:01:11,106 DEBUG   SenderThread:16652 [sender.py:send():185] send: footer\n2021-10-26 08:01:11,106 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: defer\n2021-10-26 08:01:11,106 INFO    SenderThread:16652 [sender.py:send_request_defer():338] handle sender defer: 9\n2021-10-26 08:01:11,205 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: poll_exit\n2021-10-26 08:01:11,205 DEBUG   SenderThread:16652 [sender.py:send_request():199] send_request: poll_exit\n2021-10-26 08:01:11,205 INFO    SenderThread:16652 [file_pusher.py:join():182] waiting for file pusher\n2021-10-26 08:01:11,321 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: get_summary\n2021-10-26 08:01:11,324 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: sampled_history\n2021-10-26 08:01:11,324 DEBUG   HandlerThread:16652 [handler.py:handle_request():130] handle_request: shutdown\n2021-10-26 08:01:11,325 INFO    HandlerThread:16652 [handler.py:finish():731] shutting down handler\n2021-10-26 08:01:12,106 INFO    WriterThread:16652 [datastore.py:close():281] close: C:\\Users\\tim\\GitHub\\rl_agents\\wandb\\run-20211026_075859-8x3u2qoi\\run-8x3u2qoi.wandb\n2021-10-26 08:01:12,320 INFO    SenderThread:16652 [sender.py:finish():1029] shutting down sender\n2021-10-26 08:01:12,320 INFO    SenderThread:16652 [file_pusher.py:finish():177] shutting down file pusher\n2021-10-26 08:01:12,320 INFO    SenderThread:16652 [file_pusher.py:join():182] waiting for file pusher\n2021-10-26 08:01:12,322 INFO    MainThread:16652 [internal.py:handle_exit():78] Internal process exited\n\n<\/code><\/pre>",
                "Answer_score":28.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-07T18:22:11.404Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/tims457\">@tims457<\/a> I am terribly sorry for the delay. Removing the <code>sync_tensorboard=True<\/code> argument from <code>wandb.init()<\/code>. If you want more information on using <code>wandb.tensorboard.patch()<\/code> check out this <a href=\"https:\/\/community.wandb.ai\/t\/pytorch-sync-tensorboard-help\/1017\/3\">FAQ<\/a><\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-30T11:59:40.291Z",
                "Answer_body":"<p>Not sure if you\u2019ve solved this by now. I\u2019ve expereinced something similar. It turned out that wandb wants to create a symbolic link which it can\u2019t simply do on Windows. When I run my script as admin everything worls fine.  You could try that as well.<br>\nIt\u2019s not very convenient but works for now.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pytorch sync_tensorboard help; Content: i\u2019m having some issues getting w&b to sync with tensorboard in pytorch. according to this issue and the docs, i should initializing summarywriter after w&b init possibly using .tensorboard.patch. so far i haven\u2019t been able to get this to work with either torch.utils.tensorboard or tensorboardx and with or without the patch. not sure if this is a bug or i\u2019m missing something. thanks. windows 11 python 3.8.8 0.12.4 torch 1.9.1 .tensorboard.patch(root_logdir=\"logs\") .init(config=hyperparameter_defaults, project=f\"ppo_{env_name}_torch\", sync_tensorboard=true, save_code=true, name=run_name) config = .config writer = summarywriter(f\"logs\")",
        "Question_original_content_gpt_summary":"The user is encountering challenges with syncing TensorBoard with Weights & Biases in PyTorch.",
        "Question_preprocessed_content":"Title: pytorch help; Content: im having some issues getting w&b to sync with tensorboard in pytorch. according to this issue and the docs, i should initializing after w&b possibly using . so far i havent been able to get this to work with either or and with or without the patch. not sure if this is a bug or im missing something. thanks. windows python torch",
        "Answer_original_content":"hey there, taking a look! hey there, are you getting an error? if so could you send the stack trace? could you share the debug logs for this run? they are in the run directory relative to your script. @tims457 i am terribly sorry for the delay. removing the sync_tensorboard=true argument from .init(). if you want more information on using .tensorboard.patch() check out this faq not sure if youve solved this by now. ive expereinced something similar. it turned out that wants to create a symbolic link which it cant simply do on windows. when i run my script as admin everything worls fine. you could try that as well. its not very convenient but works for now.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. Remove the `sync_tensorboard=true` argument from `.init()` to resolve the issue.\n2. Check the debug logs for the run directory relative to the script.\n3. If encountering an error, send the stack trace to get more information.\n4. If using Windows, try running the script as an admin to create a symbolic link.",
        "Answer_preprocessed_content":"hey there, taking a look! hey there, are you getting an error? if so could you send the stack trace? i dont get any errors. the code runs and logs to tensorboard, but nothing gets logged to w&b. heres an example run could you share the debug logs for this run? they are in the run directory relative to your script. i am terribly sorry for the delay. removing the argument from . if you want more information on using check out this faq not sure if youve solved this by now. ive expereinced something similar. it turned out that wants to create a symbolic link which it cant simply do on windows. when i run my script as admin everything worls fine. you could try that as well. its not very convenient but works for now."
    },
    {
        "Question_id":null,
        "Question_title":"How to deploy pipeline in Azure ML Studio?",
        "Question_body":"Doesn't seem to find the button for deploying pipeline...",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1653529001740,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/864494\/how-to-deploy-pipeline-in-azure-ml-studio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-31T09:14:09.033Z",
                "Answer_score":0,
                "Answer_body":"Hi\n\nI am a newbie and learning how to do it just now.\n\nYou should publish the pipeline then you are able to use it in Azure portal\n\nRead the document https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#use-published-pipelines-in-the-studio",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to deploy pipeline in studio?; Content: doesn't seem to find the button for deploying pipeline...",
        "Question_original_content_gpt_summary":"The user is having difficulty deploying a pipeline in Studio, as they cannot find the button to do so.",
        "Question_preprocessed_content":"Title: how to deploy pipeline in studio?; Content: doesn't seem to find the button for deploying",
        "Answer_original_content":"hi i am a newbie and learning how to do it just now. you should publish the pipeline then you are able to use it in azure portal read the document https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#use-published-pipelines-in-the-studio",
        "Answer_original_content_gpt_summary":"Solution: The user should publish the pipeline in Studio and then use it in Azure portal. They can refer to the documentation provided by Microsoft for more information on how to deploy pipelines.",
        "Answer_preprocessed_content":"hi i am a newbie and learning how to do it just now. you should publish the pipeline then you are able to use it in azure portal read the document"
    },
    {
        "Question_id":null,
        "Question_title":"az ml workspace share command (v1) alternatives",
        "Question_body":"I'm trying to share my azure ML workspace by SP (Service Principal)\n\nI've created a SP and share my workspace to the SP\n\nBut when I try to share a workspace using the cli, I get this error:\n\n'share' is misspelled or not recognized by the system.\n\n\n\n\nMaybe I guess this error was caused by version of azure cli, (share command only exists in v1 docs: https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml(v1)\/workspace?view=azure-cli-latest)\n\nbut I don't have any idea to alter the command above.\n\nThanks!",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1668184683380,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1085726\/az-ml-workspace-share-command-v1-alternatives.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-12T22:50:14.973Z",
                "Answer_score":0,
                "Answer_body":"Hello @92907795\n\nThanks for using Microsoft Q&A platform, it seems you are working on V2 and you want to do workspace sharing, but there are some changes between CLI V1 and V2.\n\nTo migrate from Azure Machine Learning V1 to V2, you need to upgrade az ml workspace share commands to equivalent az role assignment create commands.\n\nPlease refer to here - https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/role\/assignment?view=azure-cli-latest#az-role-assignment-create\nCreate a new role assignment for a user, group, or service principal.\n\n az role assignment create --role\n                           [--assignee]\n                           [--assignee-object-id]\n                           [--assignee-principal-type {ForeignGroup, Group, ServicePrincipal, User}]\n                           [--condition]\n                           [--condition-version]\n                           [--description]\n                           [--name]\n                           [--resource-group]\n                           [--scope]\n\n\n\nThere are some examples for how to use it here for your reference - https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/role\/assignment?view=azure-cli-latest#az-role-assignment-create-examples\n\nI hope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot!",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: az ml workspace share command (v1) alternatives; Content: i'm trying to share my workspace by sp (service principal) i've created a sp and share my workspace to the sp but when i try to share a workspace using the cli, i get this error: 'share' is misspelled or not recognized by the system. maybe i guess this error was caused by version of azure cli, (share command only exists in v1 docs: https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml(v1)\/workspace?view=azure-cli-latest) but i don't have any idea to alter the command above. thanks!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with sharing their workspace using the az ml workspace share command (v1) and is looking for alternatives.",
        "Question_preprocessed_content":"Title: az ml workspace share command alternatives; Content: i'm trying to share my workspace by sp i've created a sp and share my workspace to the sp but when i try to share a workspace using the cli, i get this error 'share' is misspelled or not recognized by the system. maybe i guess this error was caused by version of azure cli, but i don't have any idea to alter the command above. thanks!",
        "Answer_original_content":"hello @92907795 thanks for using microsoft q&a platform, it seems you are working on v2 and you want to do workspace sharing, but there are some changes between cli v1 and v2. to migrate from v1 to v2, you need to upgrade az ml workspace share commands to equivalent az role assignment create commands. please refer to here - https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/role\/assignment?view=azure-cli-latest#az-role-assignment-create create a new role assignment for a user, group, or service principal. az role assignment create --role [--assignee] [--assignee-object-id] [--assignee-principal-type {foreigngroup, group, serviceprincipal, user}] [--condition] [--condition-version] [--description] [--name] [--resource-group] [--scope] there are some examples for how to use it here for your reference - https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/role\/assignment?view=azure-cli-latest#az-role-assignment-create-examples i hope this helps! regards, yutong -please kindly accept the answer if you feel helpful to support the community, thanks a lot!",
        "Answer_original_content_gpt_summary":"The answer suggests upgrading to az role assignment create commands as an alternative to az ml workspace share command (v1) for workspace sharing. It provides a link to learn about creating a new role assignment for a user, group, or service principal and examples for how to use it.",
        "Answer_preprocessed_content":"hello thanks for using microsoft q&a platform, it seems you are working on v and you want to do workspace sharing, but there are some changes between cli v and v . to migrate from v to v , you need to upgrade az ml workspace share commands to equivalent az role assignment create commands. please refer to here create a new role assignment for a user, group, or service principal. az role assignment create there are some examples for how to use it here for your reference i hope this helps! regards, yutong please kindly accept the answer if you feel helpful to support the community, thanks a lot!"
    },
    {
        "Question_id":null,
        "Question_title":"Unstructured data in Vertex AI feature store",
        "Question_body":"Does Vertex AI feature store support ingestion, transformation and storage of unstructured data like images and audio?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1638290040000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":312.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Unstructured-data-in-Vertex-AI-feature-store\/td-p\/176796\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-01T13:50:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nAs per source data requirements [1] regarding Vertex AI Feature Store, it does not look like that you'll be able to ingest unstructured data.\n\n\"Vertex AI Feature Store can ingest data from tables in BigQuery or files in Cloud Storage. For files in Cloud Storage, they must be in the Avro or CSV format.\"\n\n[1]\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/source-data"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unstructured data in feature store; Content: does feature store support ingestion, transformation and storage of unstructured data like images and audio?",
        "Question_original_content_gpt_summary":"The user is facing the challenge of determining if feature store can support ingestion, transformation and storage of unstructured data such as images and audio.",
        "Question_preprocessed_content":"Title: unstructured data in feature store; Content: does feature store support ingestion, transformation and storage of unstructured data like images and audio?",
        "Answer_original_content":"hi, as per source data requirements [1] regarding feature store, it does not look like that you'll be able to ingest unstructured data. \" feature store can ingest data from tables in bigquery or files in cloud storage. for files in cloud storage, they must be in the avro or csv format.\" [1]https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/source-data",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Ingest unstructured data in a supported format such as AVRO or CSV.\n- Consider using a different tool or platform that supports ingestion, transformation, and storage of unstructured data. \n\nSummary: The feature store may not support ingestion, transformation, and storage of unstructured data such as images and audio. The user can consider using a different tool or platform or ingest unstructured data in a supported format.",
        "Answer_preprocessed_content":"hi, as per source data requirements regarding feature store, it does not look like that you'll be able to ingest unstructured data. feature store can ingest data from tables in bigquery or files in cloud storage. for files in cloud storage, they must be in the avro or csv"
    },
    {
        "Question_id":null,
        "Question_title":"Parameter importance for categorical variables",
        "Question_body":"<p>Hi!<\/p>\n<p>I have a conditional variable (True\/False) in my sweep for freezing a layer or not. It gets quite high importance and a high positive correlation. How do I know if the \u201cTrue\u201d or the \u201cFalse\u201d is the \u201chigher\u201d value? In other words, how do I know which to set it to? Is it based on the order specified in the sweep-config? Unfortunately I cannot see any clear pattern in the runs due to a lot of other parameters also being varied at the time in my Bayesian sweep.<\/p>\n<p>Thank you!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1661152408896,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":183.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/parameter-importance-for-categorical-variables\/2966",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-24T21:05:59.111Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/styrbjornkall\">@styrbjornkall<\/a> , I did a brief review of the sweep parallel coordinates chart in your <code>freezing_sweep<\/code> project, and based on the results, both <code>True<\/code> and <code>False<\/code> appear to have equitable influence on your results. It is difficult to interpret boolean results, yes, and our parameter importance panel won\u2019t tell you which value is the \u201chigher\u201d value, just that relative importance of the hyperparameter in respect to the chosen metric.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-30T00:29:22.842Z",
                "Answer_body":"<p>HI <a class=\"mention\" href=\"\/u\/styrbjornkall\">@styrbjornkall<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-29T00:29:38.517Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: parameter importance for categorical variables; Content: hi! i have a conditional variable (true\/false) in my sweep for freezing a layer or not. it gets quite high importance and a high positive correlation. how do i know if the \u201ctrue\u201d or the \u201cfalse\u201d is the \u201chigher\u201d value? in other words, how do i know which to set it to? is it based on the order specified in the sweep-config? unfortunately i cannot see any clear pattern in the runs due to a lot of other parameters also being varied at the time in my bayesian sweep. thank you!",
        "Question_original_content_gpt_summary":"The user is struggling to understand the importance of a categorical parameter in their Bayesian sweep and how to determine which value to set it to.",
        "Question_preprocessed_content":"Title: parameter importance for categorical variables; Content: hi! i have a conditional variable in my sweep for freezing a layer or not. it gets quite high importance and a high positive correlation. how do i know if the true or the false is the higher value? in other words, how do i know which to set it to? is it based on the order specified in the unfortunately i cannot see any clear pattern in the runs due to a lot of other parameters also being varied at the time in my bayesian sweep. thank you!",
        "Answer_original_content":"hi @styrbjornkall , i did a brief review of the sweep parallel coordinates chart in your freezing_sweep project, and based on the results, both true and false appear to have equitable influence on your results. it is difficult to interpret boolean results, yes, and our parameter importance panel wont tell you which value is the higher value, just that relative importance of the hyperparameter in respect to the chosen metric. hi @styrbjornkall , since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no clear solutions provided in the answer to the user's question. The answer suggests that both true and false values have equitable influence on the results, and that the parameter importance panel won't indicate which value is higher. The conversation was closed without any further input from the user.",
        "Answer_preprocessed_content":"hi , i did a brief review of the sweep parallel coordinates chart in your project, and based on the results, both and appear to have equitable influence on your results. it is difficult to interpret boolean results, yes, and our parameter importance panel wont tell you which value is the higher value, just that relative importance of the hyperparameter in respect to the chosen metric. hi , since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Is there a way to automate failure handling and retries when using Amazon SageMaker batch transform?",
        "Question_body":"How does Amazon SageMaker batch transform handle failures? Is there a way to automate failure handling and retries built into the service?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1593595381000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":120.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE10OtSwDRCiB-0pP6wflYQ\/is-there-a-way-to-automate-failure-handling-and-retries-when-using-amazon-sage-maker-batch-transform",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-01T15:34:51.000Z",
                "Answer_score":0,
                "Answer_body":"You can use the ModelClientConfig API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is there a way to automate failure handling and retries when using batch transform?; Content: how does batch transform handle failures? is there a way to automate failure handling and retries built into the service?",
        "Question_original_content_gpt_summary":"The user is looking for a way to automate failure handling and retries when using batch transform.",
        "Question_preprocessed_content":"Title: is there a way to automate failure handling and retries when using batch transform?; Content: how does batch transform handle failures? is there a way to automate failure handling and retries built into the service?",
        "Answer_original_content":"you can use the modelclientconfig api to configure the timeout and maximum number of retries for processing a transform job invocation. the maximum number of automated retries is three.",
        "Answer_original_content_gpt_summary":"To automate failure handling and retries when using batch transform, the user can use the modelclientconfig API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.",
        "Answer_preprocessed_content":"you can use the modelclientconfig api to configure the timeout and maximum number of retries for processing a transform job invocation. the maximum number of automated retries is three."
    },
    {
        "Question_id":null,
        "Question_title":"Trying Sagemaker example but getting error: AttributeError: module 'sagemaker' has no attribute 'create_transform_job'",
        "Question_body":"Hi, I keep getting this error: AttributeError: module 'sagemaker' has no attribute 'create_transform_job', when using a batch transform example that AWS graciously had in the notebook instances. Code: ***Also, I updated Sagemaker to the newest package and its still not working.\n\n%%time\nimport time\nfrom time import gmtime, strftime\n\nbatch_job_name = \"Batch-Transform-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\ninput_location = \"s3:\/\/{}\/{}\/batch\/{}\".format(\n    bucket, prefix, batch_file\n)  # use input data without ID column\noutput_location = \"s3:\/\/{}\/{}\/output\/{}\".format(bucket, prefix, batch_job_name)\n\nrequest = {\n    \"TransformJobName\": batch_job_name,\n    \"ModelName\": 'xgboost-parquet-example-training-2022-03-28-16-02-31-model',\n    \"TransformOutput\": {\n        \"S3OutputPath\": output_location,\n        \"Accept\": \"text\/csv\",\n        \"AssembleWith\": \"Line\",\n    },\n    \"TransformInput\": {\n        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": input_location}},\n        \"ContentType\": \"text\/csv\",\n        \"SplitType\": \"Line\",\n        \"CompressionType\": \"None\",\n    },\n    \"TransformResources\": {\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n}\n\nsagemaker.create_transform_job(**request)\nprint(\"Created Transform job with name: \", batch_job_name)\n\n# Wait until the job finishes\ntry:\n    sagemaker.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name)\nfinally:\n    response = sagemaker.describe_transform_job(TransformJobName=batch_job_name)\n    status = response[\"TransformJobStatus\"]\n    print(\"Transform job ended with status: \" + status)\n    if status == \"Failed\":\n        message = response[\"FailureReason\"]\n        print(\"Transform failed with the following error: {}\".format(message))\n        raise Exception(\"Transform job failed\")\n\n\nEverything else is working well. I've had no luck with this on anyother forum.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1648494191878,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":260.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUj8LepwTyQkq0ABgtX-nfew\/trying-sagemaker-example-but-getting-error-attribute-error-module-sagemaker-has-no-attribute-create-transform-job",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-28T21:45:21.581Z",
                "Answer_score":1,
                "Answer_body":"Please double check what type is sagemaker object. Check out this example\n\nsagemaker = boto3.client(service_name=\"sagemaker\") sagemaker.create_transform_job(...)",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: trying example but getting error: attributeerror: module '' has no attribute 'create_transform_job'; Content: hi, i keep getting this error: attributeerror: module '' has no attribute 'create_transform_job', when using a batch transform example that aws graciously had in the notebook instances. code: ***also, i updated to the newest package and its still not working. %%time import time from time import gmtime, strftime batch_job_name = \"batch-transform-\" + strftime(\"%y-%m-%d-%h-%m-%s\", gmtime()) input_location = \"s3:\/\/{}\/{}\/batch\/{}\".format( bucket, prefix, batch_file ) # use input data without id column output_location = \"s3:\/\/{}\/{}\/output\/{}\".format(bucket, prefix, batch_job_name) request = { \"transformjobname\": batch_job_name, \"modelname\": 'xgboost-parquet-example-training-2022-03-28-16-02-31-model', \"transformoutput\": { \"s3outputpath\": output_location, \"accept\": \"text\/csv\", \"assemblewith\": \"line\", }, \"transforminput\": { \"datasource\": {\"s3datasource\": {\"s3datatype\": \"s3prefix\", \"s3uri\": input_location}}, \"contenttype\": \"text\/csv\", \"splittype\": \"line\", \"compressiontype\": \"none\", }, \"transformresources\": {\"instancetype\": \"ml.m4.xlarge\", \"instancecount\": 1}, } .create_transform_job(**request) print(\"created transform job with name: \", batch_job_name) # wait until the job finishes try: .get_waiter(\"transform_job_completed_or_stopped\").wait(transformjobname=batch_job_name) finally: response = .describe_transform_job(transformjobname=batch_job_name) status = response[\"transformjobstatus\"] print(\"transform job ended with status: \" + status) if status == \"failed\": message = response[\"failurereason\"] print(\"transform failed with the following error: {}\".format(message)) raise exception(\"transform job failed\") everything else is working well. i've had no luck with this on anyother forum.",
        "Question_original_content_gpt_summary":"The user is encountering an error when trying to use a batch transform example provided by AWS, and has been unable to find a solution on other forums.",
        "Question_preprocessed_content":"Title: trying example but getting error attributeerror module '' has no attribute ; Content: hi, i keep getting this error attributeerror module '' has no attribute when using a batch transform example that aws graciously had in the notebook instances. code also, i updated to the newest package and its still not working. %%time import time from time import gmtime, strftime + gmtime bucket, prefix, use input data without id column prefix, request , transforminput , contenttype splittype line , compressiontype none , , transformresources , print wait until the job finishes try finally response status response print if status failed message response print raise exception everything else is working well. i've had no luck with this on anyother forum.",
        "Answer_original_content":"please double check what type is object. check out this example = boto3.client(service_name=\"\") .create_transform_job(...)",
        "Answer_original_content_gpt_summary":"The solution to the error encountered when using a batch transform example provided by AWS is to double check the type of the object and use the example provided by boto3.client.",
        "Answer_preprocessed_content":"please double check what type is object. check out this example"
    },
    {
        "Question_id":64097278.0,
        "Question_title":"Why is the field \"compute target\" for data drift monitoring in Azure ML studio still blank whereas I have a compute instance?",
        "Question_body":"<p>I have created a compute instance:<\/p>\n<p>Virtual machine size\nSTANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)<\/p>\n<p>Processing Unit\nCPU - General purpose<\/p>\n<p>But, I'm not able to access it when trying to set it for data drift monitoring.\nThe dropdown list is empty. I can't understand why. Can you help me please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1601276103867,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":88.0,
        "Owner_creation_time":1423640080283,
        "Owner_last_access_time":1663943557963,
        "Owner_reputation":457.0,
        "Owner_up_votes":19.0,
        "Owner_down_votes":2.0,
        "Owner_views":125.0,
        "Answer_body":"<p>I found the answer. You must give a <strong>cluster<\/strong> compute instance to do data drift in Azure Machine Learning Studio. As it is not clear, I'm planning to add something in the documentation of Azure.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1601278985528,
        "Answer_score":2.0,
        "Owner_location":"Lyon, France",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64097278",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why is the field \"compute target\" for data drift monitoring in studio still blank whereas i have a compute instance?; Content: i have created a compute instance: virtual machine size standard_ds3_v2 (4 cores, 14 gb ram, 28 gb disk) processing unit cpu - general purpose but, i'm not able to access it when trying to set it for data drift monitoring. the dropdown list is empty. i can't understand why. can you help me please?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where the \"compute target\" field for data drift monitoring in Studio is blank despite having created a compute instance.",
        "Question_preprocessed_content":"Title: why is the field compute target for data drift monitoring in studio still blank whereas i have a compute instance?; Content: i have created a compute instance virtual machine size processing unit cpu general purpose but, i'm not able to access it when trying to set it for data drift monitoring. the dropdown list is empty. i can't understand why. can you help me please?",
        "Answer_original_content":"i found the answer. you must give a cluster compute instance to do data drift in studio. as it is not clear, i'm planning to add something in the documentation of azure.",
        "Answer_original_content_gpt_summary":"Solution: To resolve the issue of the \"compute target\" field being blank for data drift monitoring in Studio, the user must provide a cluster compute instance. The answer suggests that this information is not clear in the Azure documentation and the user plans to add it.",
        "Answer_preprocessed_content":"i found the answer. you must give a cluster compute instance to do data drift in studio. as it is not clear, i'm planning to add something in the documentation of azure."
    },
    {
        "Question_id":null,
        "Question_title":"tensorboard with custom docker image without notebook",
        "Question_body":"Hi,\nIs it possible to use tensorboard with a custom docker image without using a notebook ? Is there any other method to monitor the training process ? I'm using the tensorflow object detection API and currently exposing metrics (only loss) from cloudwatch using a regex but I'd like a more detailed way like tensorboar.. is that possible ??\nThanks",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1588924703000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":61.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdIJGIVDEQSm-9eX3jo0ubA\/tensorboard-with-custom-docker-image-without-notebook",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-11T21:19:16.000Z",
                "Answer_score":0,
                "Answer_body":"Yes, it is possible to use tensorboard outside of the SageMaker notebooks.\nHere is an example https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb that uses TensorBoard to compare the training jobs.\nAll of it can be run locally.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-25T10:26:28.000Z",
                "Answer_score":0,
                "Answer_body":"For anyone interested I made a tutorial for this:\n\nhttps:\/\/github.com\/roccopietrini\/TFSagemakerDetection\n\nEdited by: rokk07 on Jan 25, 2021 2:28 AM",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tensorboard with custom docker image without notebook; Content: hi, is it possible to use tensorboard with a custom docker image without using a notebook ? is there any other method to monitor the training process ? i'm using the tensorflow object detection api and currently exposing metrics (only loss) from cloudwatch using a regex but i'd like a more detailed way like tensorboar.. is that possible ?? thanks",
        "Question_original_content_gpt_summary":"The user is looking for a way to use Tensorboard with a custom Docker image without using a notebook, and to monitor the training process in a more detailed way than just using Cloudwatch with a regex.",
        "Question_preprocessed_content":"Title: tensorboard with custom docker image without notebook; Content: hi, is it possible to use tensorboard with a custom docker image without using a notebook ? is there any other method to monitor the training process ? i'm using the tensorflow object detection api and currently exposing metrics from cloudwatch using a regex but i'd like a more detailed way like is that possible ?? thanks",
        "Answer_original_content":"yes, it is possible to use tensorboard outside of the notebooks. here is an example https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_cifar10.ipynb that uses tensorboard to compare the training jobs. all of it can be run locally. for anyone interested i made a tutorial for this: https:\/\/github.com\/roccopietrini\/tfdetection edited by: rokk07 on jan 25, 2021 2:28 am",
        "Answer_original_content_gpt_summary":"Possible solutions to using Tensorboard with a custom Docker image without using a notebook and monitoring the training process in a more detailed way than just using Cloudwatch with a regex are provided in the answer. The answer suggests that it is possible to use Tensorboard outside of notebooks and provides an example that uses Tensorboard to compare training jobs. The example can be run locally, and a tutorial is available for anyone interested.",
        "Answer_preprocessed_content":"yes, it is possible to use tensorboard outside of the notebooks. here is an example that uses tensorboard to compare the training jobs. all of it can be run locally. for anyone interested i made a tutorial for this edited by rokk on jan , am"
    },
    {
        "Question_id":55964972.0,
        "Question_title":"Can I pass arguments to the entrypoint of a SageMaker estimator?",
        "Question_body":"<p>I'm using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html#sagemaker.estimator.Framework\" rel=\"noreferrer\">SageMaker python sdk<\/a> and was hoping to pass in some arguments to be used by my entrypoint, I'm not seeing how to do this.<\/p>\n\n<pre><code>from sagemaker.sklearn.estimator import SKLearn  # sagemaker python sdk\n\nentrypoint = 'entrypoint_script.py'\n\nsklearn = SKLearn(entry_point=entrypoint,  # &lt;-- need to pass args to this\n                  train_instance_type=instance_class,\n                  role=role,\n                  sagemaker_session=sm)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1556867880610,
        "Question_favorite_count":1.0,
        "Question_score":5.0,
        "Question_view_count":2780.0,
        "Owner_creation_time":1343237570416,
        "Owner_last_access_time":1661366012027,
        "Owner_reputation":1325.0,
        "Owner_up_votes":303.0,
        "Owner_down_votes":10.0,
        "Owner_views":131.0,
        "Answer_body":"<p>The answer is no as there is no parameter on the Estimator base class, or the fit method, that accepts arguments to pass to the entrypoint.<\/p>\n\n<p>I resolved this by passing the parameter as part of the hyperparameter dictionary. This gets passed to the entrypoint as arguments.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1556949846448,
        "Answer_score":6.0,
        "Owner_location":"California",
        "Question_last_edit_time":1556899526172,
        "Answer_last_edit_time":1578346552883,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55964972",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can i pass arguments to the entrypoint of a estimator?; Content: i'm using the python sdk and was hoping to pass in some arguments to be used by my entrypoint, i'm not seeing how to do this. from .sklearn.estimator import sklearn # python sdk entrypoint = 'entrypoint_script.py' sklearn = sklearn(entry_point=entrypoint, # <-- need to pass args to this train_instance_type=instance_class, role=role, _session=sm)",
        "Question_original_content_gpt_summary":"The user is trying to pass arguments to the entrypoint of a Python SDK estimator.",
        "Question_preprocessed_content":"Title: can i pass arguments to the entrypoint of a estimator?; Content: i'm using the python sdk and was hoping to pass in some arguments to be used by my entrypoint, i'm not seeing how to do this.",
        "Answer_original_content":"the answer is no as there is no parameter on the estimator base class, or the fit method, that accepts arguments to pass to the entrypoint. i resolved this by passing the parameter as part of the hyperparameter dictionary. this gets passed to the entrypoint as arguments.",
        "Answer_original_content_gpt_summary":"Possible solution: Pass the parameter as part of the hyperparameter dictionary to the Python SDK estimator. This will get passed to the entrypoint as arguments.",
        "Answer_preprocessed_content":"the answer is no as there is no parameter on the estimator base class, or the fit method, that accepts arguments to pass to the entrypoint. i resolved this by passing the parameter as part of the hyperparameter dictionary. this gets passed to the entrypoint as arguments."
    },
    {
        "Question_id":67123040.0,
        "Question_title":"How to tell programmatically that an AWS Step Function execution has been completed?",
        "Question_body":"<p>I am triggering a Step Function execution via a Python cell in a SageMaker Notebook, like this:<\/p>\n<pre><code>state_machine_arn = 'arn:aws:states:us-west-1:1234567891:stateMachine:alexanderMyPackageStateMachineE3411O13-A1vQWERTP9q9'\nsfn = boto3.client('stepfunctions')\n..\nsfn.start_execution(**kwargs)  # Non Blocking Call\nrun_arn = response['executionArn']\nprint(f&quot;Started run {run_name}. ARN is {run_arn}.&quot;)\n<\/code><\/pre>\n<p>and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before I start doing some custom post-analysis on the results, I manually execute a cell with:<\/p>\n<pre><code>response = sfn.list_executions(\n    stateMachineArn=state_machine_arn,\n    maxResults=1\n)\nprint(response)\n<\/code><\/pre>\n<p>where I can see from the output the status of the execution, e.g. <code>'status': 'RUNNING'<\/code>.<\/p>\n<p>How can I automate this, i.e. trigger the Step Function and continue the execution on my post-analysis custom logic only after the execution has finished? Is there for example a blocking call to start the execution, or a callback method I could use?<\/p>\n<p>I can think of putting a sleep method, so that the Python Notebook cell would periodically call <code>list_executions()<\/code> and check the status, and only when the execution is completed, continue to rest of the code. I can statistically determine the sleep period, but I was wondering if there is a simpler\/more accurate way.<\/p>\n<hr \/>\n<p>PS: Related: <a href=\"https:\/\/stackoverflow.com\/questions\/46878423\/how-to-avoid-simultaneous-execution-in-aws-step-function\">How to avoid simultaneous execution in aws step function<\/a>, however I would like to avoid creating any new AWS resource, just for this, I would like to do everything from within the Notebook.<\/p>\n<p>PPS: I cannot make any change to <code>MyPackage<\/code> and the Step Function definition.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1618566783027,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":1216.0,
        "Owner_creation_time":1369257942212,
        "Owner_last_access_time":1663776093950,
        "Owner_reputation":70285.0,
        "Owner_up_votes":7595.0,
        "Owner_down_votes":12100.0,
        "Owner_views":13121.0,
        "Answer_body":"<p>Based on the comments.<\/p>\n<p>If no new resources are to be created (no CloudWatch Event rules, lambda functions) nor any changes to existing Step Function are allowed, then <strong>pooling iteratively<\/strong> <code>list_executions<\/code> would be the best solution.<\/p>\n<p>AWS CLI and boto3 have implemented similar solutions (not for Step Functions), but for some other services. They are called <code>waiters<\/code> (e.g. <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/ec2.html#waiters\" rel=\"nofollow noreferrer\">ec2 waiters<\/a>). So basically you would have to create your own <strong>waiter for Step Function<\/strong>, as AWS does not provide one for that. AWS uses <strong>15 seconds<\/strong> sleep time from what I recall for its waiters.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618568305232,
        "Answer_score":2.0,
        "Owner_location":"London, UK",
        "Question_last_edit_time":1618585069787,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67123040",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to tell programmatically that an aws step function execution has been completed?; Content: i am triggering a step function execution via a python cell in a notebook, like this: state_machine_arn = 'arn:aws:states:us-west-1:1234567891:statemachine:alexandermypackagestatemachinee3411o13-a1vqwertp9q9' sfn = boto3.client('stepfunctions') .. sfn.start_execution(**kwargs) # non blocking call run_arn = response['executionarn'] print(f\"started run {run_name}. arn is {run_arn}.\") and then in order to check that the execution (which might take hours to complete depending on the input) has been completed, before i start doing some custom post-analysis on the results, i manually execute a cell with: response = sfn.list_executions( statemachinearn=state_machine_arn, maxresults=1 ) print(response) where i can see from the output the status of the execution, e.g. 'status': 'running'. how can i automate this, i.e. trigger the step function and continue the execution on my post-analysis custom logic only after the execution has finished? is there for example a blocking call to start the execution, or a callback method i could use? i can think of putting a sleep method, so that the python notebook cell would periodically call list_executions() and check the status, and only when the execution is completed, continue to rest of the code. i can statistically determine the sleep period, but i was wondering if there is a simpler\/more accurate way. ps: related: how to avoid simultaneous execution in aws step function, however i would like to avoid creating any new aws resource, just for this, i would like to do everything from within the notebook. pps: i cannot make any change to mypackage and the step function definition.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge of programmatically determining when an AWS Step Function execution has been completed, and is looking for a simpler and more accurate way than using a sleep method to periodically check the status.",
        "Question_preprocessed_content":"Title: how to tell programmatically that an aws step function execution has been completed?; Content: i am triggering a step function execution via a python cell in a notebook, like this and then in order to check that the execution has been completed, before i start doing some custom on the results, i manually execute a cell with where i can see from the output the status of the execution, . how can i automate this, trigger the step function and continue the execution on my custom logic only after the execution has finished? is there for example a blocking call to start the execution, or a callback method i could use? i can think of putting a sleep method, so that the python notebook cell would periodically call and check the status, and only when the execution is completed, continue to rest of the code. i can statistically determine the sleep period, but i was wondering if there is a accurate way. ps related how to avoid simultaneous execution in aws step function, however i would like to avoid creating any new aws resource, just for this, i would like to do everything from within the notebook. pps i cannot make any change to and the step function definition.",
        "Answer_original_content":"based on the comments. if no new resources are to be created (no cloudwatch event rules, lambda functions) nor any changes to existing step function are allowed, then pooling iteratively list_executions would be the best solution. aws cli and boto3 have implemented similar solutions (not for step functions), but for some other services. they are called waiters (e.g. ec2 waiters). so basically you would have to create your own waiter for step function, as aws does not provide one for that. aws uses 15 seconds sleep time from what i recall for its waiters.",
        "Answer_original_content_gpt_summary":"Possible solutions for programmatically determining when an AWS Step Function execution has been completed include iteratively pooling list_executions, creating a custom waiter for Step Function, and using AWS CLI or boto3 waiters (although they are not currently available for Step Functions). The suggested sleep time for AWS waiters is 15 seconds.",
        "Answer_preprocessed_content":"based on the comments. if no new resources are to be created nor any changes to existing step function are allowed, then pooling iteratively would be the best solution. aws cli and boto have implemented similar solutions , but for some other services. they are called . so basically you would have to create your own waiter for step function, as aws does not provide one for that. aws uses seconds sleep time from what i recall for its waiters."
    },
    {
        "Question_id":null,
        "Question_title":"Getting Error Deadline Exceeded when deploying model from Cloud Firestoer functions",
        "Question_body":"Hello,I currently have a firebase function that is set to deploy my AutoML tables model everyday at 5am. This has been working fine for the past month, up until the last week. I have been getting the following error below when the function attempts to deploy the model.I watched a google tutorial and it recommend to return a promise from my cloud function. That seemed to work for 1 day, but I received the error again this morning.I am going to try to implement a retry function, but I figured I would ask on here as well. Also, I am thinking that moving from autoML to VertexAI might help alleviate my issues. Any guidance here is helpful.See below for my deploy model code:  ",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1638530220000,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":860.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-Error-Deadline-Exceeded-when-deploying-model-from-Cloud\/td-p\/177128\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-03T12:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Is deploying the model fails via Cloud Functions only, or does it also fail when the model is deployed manually, or by any other means?"
            },
            {
                "Answer_creation_time":"2021-12-03T15:00:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Manually (through the cloud console) it usually works. I have been testing using an http call in Firebase functions. Sometimes (not consistantly) that fails if I have recently undeployed the model. Although, I do not get any type of error notification, I just know it fails by checking the cloud console.\n\nThe deploy model function only runs once a day though, and the model has typically been undeployed for at least 20 hours before that, so I don't think I am getting that error because I am calling it too often."
            },
            {
                "Answer_creation_time":"2021-12-09T10:39:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"So after further investigation it looks like the model is failing when I am requesting predictions through firebase functions. I get the same error \"DEADLINE_EXCEEDED\". This is not consistent though, it worked for the previous 2 days before this and today failed again. I haven't changed anything.\n\nI have 2 questions:\n\n1) is it possible that congestion on the network is causing these to fail? Would it help if I moved the prediction to a different time? Currently I have it set at 6am PST.\n\n2) Since autoML is beta, would it help if I moved the model to VertexAI? I can make that move if it helps"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: getting error deadline exceeded when deploying model from cloud firestoer functions; Content: hello,i currently have a firebase function that is set to deploy my automl tables model everyday at 5am. this has been working fine for the past month, up until the last week. i have been getting the following error below when the function attempts to deploy the model.i watched a google tutorial and it recommend to return a promise from my cloud function. that seemed to work for 1 day, but i received the error again this morning.i am going to try to implement a retry function, but i figured i would ask on here as well. also, i am thinking that moving from automl to vertexai might help alleviate my issues. any guidance here is helpful.see below for my deploy model code:",
        "Question_original_content_gpt_summary":"The user is encountering an error of \"deadline exceeded\" when attempting to deploy their AutoML Tables model from Cloud Firestore Functions, and is considering moving to VertexAI to alleviate the issue.",
        "Question_preprocessed_content":"Title: getting error deadline exceeded when deploying model from cloud firestoer functions; Content: hello,i currently have a firebase function that is set to deploy my automl tables model everyday at am. this has been working fine for the past month, up until the last week. i have been getting the following error below when the function attempts to deploy the watched a google tutorial and it recommend to return a promise from my cloud function. that seemed to work for day, but i received the error again this am going to try to implement a retry function, but i figured i would ask on here as well. also, i am thinking that moving from automl to vertexai might help alleviate my issues. any guidance here is below for my deploy model code",
        "Answer_original_content":"is deploying the model fails via cloud functions only, or does it also fail when the model is deployed manually, or by any other means? manually (through the cloud console) it usually works. i have been testing using an http call in firebase functions. sometimes (not consistantly) that fails if i have recently undeployed the model. although, i do not get any type of error notification, i just know it fails by checking the cloud console. the deploy model function only runs once a day though, and the model has typically been undeployed for at least 20 hours before that, so i don't think i am getting that error because i am calling it too often. so after further investigation it looks like the model is failing when i am requesting predictions through firebase functions. i get the same error \"deadline_exceeded\". this is not consistent though, it worked for the previous 2 days before this and today failed again. i haven't changed anything. i have 2 questions: 1) is it possible that congestion on the network is causing these to fail? would it help if i moved the prediction to a different time? currently i have it set at 6am pst. 2) since automl is beta, would it help if i moved the model to vertexai? i can make that move if it helps",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are: \n1. The user can try deploying the model manually through the cloud console instead of using an HTTP call in Firebase functions. \n2. The user can check if network congestion is causing the issue and try moving the prediction to a different time. \n3. The user can consider moving the model to VertexAI as it may help since AutoML is still in beta.",
        "Answer_preprocessed_content":"is deploying the model fails via cloud functions only, or does it also fail when the model is deployed manually, or by any other means? manually it usually works. i have been testing using an http call in firebase functions. sometimes that fails if i have recently undeployed the model. although, i do not get any type of error notification, i just know it fails by checking the cloud console. the deploy model function only runs once a day though, and the model has typically been undeployed for at least hours before that, so i don't think i am getting that error because i am calling it too often. so after further investigation it looks like the model is failing when i am requesting predictions through firebase functions. i get the same error this is not consistent though, it worked for the previous days before this and today failed again. i haven't changed anything. i have questions is it possible that congestion on the network is causing these to fail? would it help if i moved the prediction to a different time? currently i have it set at am pst. since automl is beta, would it help if i moved the model to vertexai? i can make that move if it helps"
    },
    {
        "Question_id":null,
        "Question_title":"Submitting a job to Azure ML from Synapse workspace",
        "Question_body":"Assume a data scientist who is coding inside a Synapse notebook, aims to submit his AutoML job to Azure ML. Also assume that we already created the Azure ML workspace, and linked it to Synapse, and also gave Synapse workspace the contributor access to Azure ML workspace. Also the data scientist has the Azure reader role at the synapse workspace level. Data scientist run the following code according to this link (https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/spark\/apache-spark-azure-machine-learning-tutorial)\n\nfrom azureml.core import Workspace\n\nsubscription_id = \"xxxxxx\" #you should be owner or contributor\nresource_group = \"xxxxx\" #you should be owner or contributor\nworkspace_name = \"xxxxx\" #your workspace name\nworkspace_region = \"xxxxx\" #your region\n\nws = Workspace(workspace_name = workspace_name,\nsubscription_id = subscription_id,\nresource_group = resource_group)\n\nHowever, he receives an error that says he does not have the required contributor\/owner roles at the subscription and resource group level. But we (as the synapse administrators) we don't want to give him the contributor\/owner role at the subscription and resource group name\n\nQuestion: How the data scientist can submit his job without letting him to have the required contributor\/owner role. Can he use the managed identity of the Synapse workspace to connect to the Azure ML workspace?\n\nThank you",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1648588740927,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/792681\/submitting-a-job-to-azure-ml-from-synapse-workspac.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-30T06:04:50.573Z",
                "Answer_score":1,
                "Answer_body":"Hello anonymous user,\n\nThanks for the question and using MS Q&A platform.\n\nMake sure your Service principal or Managed Service Identity (MSI) must have \"Contributor\" access to the AML workspace.\n\nIf the model is registered in Azure Machine Learning, then you can choose either of the following two supported ways of authentication.\n\nThrough service principal: You can use service principal client ID and secret directly to authenticate to AML workspace. Service principal must have \"Contributor\" access to the AML workspace.\n\n\nThrough linked service: You can use linked service to authenticate to AML workspace. Linked service can use \"service principal\" or Synapse workspace's \"Managed Service Identity (MSI)\" for authentication. \"Service principal\" or \"Managed Service Identity (MSI)\" must have \"Contributor\" access to the AML workspace.\n\nHere is the complete walkthrough of authenticating AML workspace with Azure Synapse Analytics:\n\nFor more details, refer to Tutorial: Score machine learning models with PREDICT in serverless Apache Spark pools.\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":17.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: submitting a job to from synapse workspace; Content: assume a data scientist who is coding inside a synapse notebook, aims to submit his automl job to . also assume that we already created the workspace, and linked it to synapse, and also gave synapse workspace the contributor access to workspace. also the data scientist has the azure reader role at the synapse workspace level. data scientist run the following code according to this link (https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/spark\/apache-spark-azure-machine-learning-tutorial) from .core import workspace subscription_id = \"xxxxxx\" #you should be owner or contributor resource_group = \"xxxxx\" #you should be owner or contributor workspace_name = \"xxxxx\" #your workspace name workspace_region = \"xxxxx\" #your region ws = workspace(workspace_name = workspace_name, subscription_id = subscription_id, resource_group = resource_group) however, he receives an error that says he does not have the required contributor\/owner roles at the subscription and resource group level. but we (as the synapse administrators) we don't want to give him the contributor\/owner role at the subscription and resource group name question: how the data scientist can submit his job without letting him to have the required contributor\/owner role. can he use the managed identity of the synapse workspace to connect to the workspace? thank you",
        "Question_original_content_gpt_summary":"The user encountered challenges submitting a job to a Synapse workspace due to not having the required contributor\/owner roles at the subscription and resource group level.",
        "Question_preprocessed_content":"Title: submitting a job to from synapse workspace; Content: assume a data scientist who is coding inside a synapse notebook, aims to submit his automl job to . also assume that we already created the workspace, and linked it to synapse, and also gave synapse workspace the contributor access to workspace. also the data scientist has the azure reader role at the synapse workspace level. data scientist run the following code according to this link from import workspace xxxxxx you should be owner or contributor xxxxx you should be owner or contributor xxxxx your workspace name xxxxx your region ws however, he receives an error that says he does not have the required roles at the subscription and resource group level. but we we don't want to give him the role at the subscription and resource group name question how the data scientist can submit his job without letting him to have the required role. can he use the managed identity of the synapse workspace to connect to the workspace? thank you",
        "Answer_original_content":"hello anonymous user, thanks for the question and using ms q&a platform. make sure your service principal or managed service identity (msi) must have \"contributor\" access to the aml workspace. if the model is registered in , then you can choose either of the following two supported ways of authentication. through service principal: you can use service principal client id and secret directly to authenticate to aml workspace. service principal must have \"contributor\" access to the aml workspace. through linked service: you can use linked service to authenticate to aml workspace. linked service can use \"service principal\" or synapse workspace's \"managed service identity (msi)\" for authentication. \"service principal\" or \"managed service identity (msi)\" must have \"contributor\" access to the aml workspace. here is the complete walkthrough of authenticating aml workspace with azure synapse analytics: for more details, refer to tutorial: score machine learning models with predict in serverless apache spark pools. hope this will help. please let us know if any further queries. please don't forget to click on or upvote button whenever the information provided helps you. original posters help the community find answers faster by identifying the correct answer. here is how want a reminder to come back and check responses? here is how to subscribe to a notification if you are interested in joining the vm program and help shape the future of q&a: here is how you can be part of q&a volunteer moderators",
        "Answer_original_content_gpt_summary":"The answer suggests two possible ways to authenticate to the AML workspace: through service principal or through linked service. In both cases, the service principal or managed service identity (MSI) must have \"contributor\" access to the AML workspace. The answer also provides a tutorial for authenticating AML workspace with Azure Synapse Analytics.",
        "Answer_preprocessed_content":"hello anonymous user, thanks for the question and using ms q&a platform. make sure your service principal or managed service identity must have contributor access to the aml workspace. if the model is registered in , then you can choose either of the following two supported ways of authentication. through service principal you can use service principal client id and secret directly to authenticate to aml workspace. service principal must have contributor access to the aml workspace. through linked service you can use linked service to authenticate to aml workspace. linked service can use service principal or synapse workspace's managed service identity for authentication. service principal or managed service identity must have contributor access to the aml workspace. here is the complete walkthrough of authenticating aml workspace with azure synapse analytics for more details, refer to tutorial score machine learning models with predict in serverless apache spark pools. hope this will help. please let us know if any further queries. please don't forget to click on or upvote button whenever the information provided helps you. original posters help the community find answers faster by identifying the correct answer. here is how want a reminder to come back and check responses? here is how to subscribe to a notification if you are interested in joining the vm program and help shape the future of q&a here is how you can be part of q&a volunteer moderators"
    },
    {
        "Question_id":73288631.0,
        "Question_title":"Allowing users to view GPU utilization in GCP Vertex AI training jobs",
        "Question_body":"<p>I am running custom training jobs using Google cloud Vertex AI. But when I enter a custom training job page, the GPU utilization display is not shown, instead, there is a message saying &quot;you don't have access to this data.&quot;\n<a href=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t0D2M.jpg\" alt=\"enter image description here\" \/><\/a>\nI would appreciate help finding the right IAM role which will allow me to view the GPU utilization.\nThanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1660033318073,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":40.0,
        "Owner_creation_time":1379514677176,
        "Owner_last_access_time":1663759914008,
        "Owner_reputation":597.0,
        "Owner_up_votes":285.0,
        "Owner_down_votes":0.0,
        "Owner_views":64.0,
        "Answer_body":"<p>You can use <a href=\"https:\/\/cloud.google.com\/monitoring\/access-control#mon_roles_desc\" rel=\"nofollow noreferrer\"><code>monitoring.viewer<\/code><\/a> IAM role to display both CPU and GPU utilization in GCP Vertex AI training jobs on top of <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/access-control#predefined-roles\" rel=\"nofollow noreferrer\"><code>aiplatform.viewer<\/code><\/a> IAM role.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jbqyl.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1660101916403,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73288631",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: allowing users to view gpu utilization in gcp training jobs; Content: i am running custom training jobs using google cloud . but when i enter a custom training job page, the gpu utilization display is not shown, instead, there is a message saying \"you don't have access to this data.\" i would appreciate help finding the right iam role which will allow me to view the gpu utilization. thanks!",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in allowing users to view GPU utilization in GCP training jobs, as they are unable to access the data due to a lack of the right IAM role.",
        "Question_preprocessed_content":"Title: allowing users to view gpu utilization in gcp training jobs; Content: i am running custom training jobs using google cloud . but when i enter a custom training job page, the gpu utilization display is not shown, instead, there is a message saying you don't have access to this i would appreciate help finding the right iam role which will allow me to view the gpu utilization. thanks!",
        "Answer_original_content":"you can use monitoring.viewer iam role to display both cpu and gpu utilization in gcp training jobs on top of aiplatform.viewer iam role.",
        "Answer_original_content_gpt_summary":"Solution: To allow users to view GPU utilization in GCP training jobs, the user can use the monitoring.viewer IAM role in addition to the aiplatform.viewer IAM role. This will display both CPU and GPU utilization.",
        "Answer_preprocessed_content":"you can use iam role to display both cpu and gpu utilization in gcp training jobs on top of iam role."
    },
    {
        "Question_id":null,
        "Question_title":"Can we run a python script in Sagemaker using boto3 from a local machine?",
        "Question_body":"Here's what I am trying to do: In my application that resides outside aws, I take some user inputs, and trigger scripts that reside inside Sagemaker notebook instance. I am able to start or create a new instance using boto3, and also use lifecycle configuration to run some starter script while the instance turns on. But I want to run multiple scripts in short intervals based on user inputs, so I don't want to restart my instance each time with a new lifecycle configuration script. I am trying to find if there is a way to execute shell commands in sagemaker using boto3 (or any other way).",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1646819726632,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":808.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjnuGv6KCRaS9BCxzVgCYyA\/can-we-run-a-python-script-in-sagemaker-using-boto-3-from-a-local-machine",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-09T10:41:19.588Z",
                "Answer_score":1,
                "Answer_body":"It should be possible, but it's probably not a great idea...\n\nThis is not really an intended pattern for SageMaker notebooks today, and it's more likely that you should be using SageMaker Processing Jobs to schedule your regular tasks - taking input and output data direct from S3 rather than relying on local notebook storage.\n\nWith that warning out of the way, a hacky solution:\n\nSageMaker notebooks (both Notebook Instances and Studio) are based on Jupyter and thus today more-or-less conform (with some customizations) to Jupyter's client\/server API model, which has both REST and WebSocket\/ZeroMQ aspects. This means as long as you're able to handle authentication, it's possible to interact with the notebooks from a script using the same interfaces your browser would.\n\nThis automation-style solution would proceed as (assuming Python):\n\nUse boto3 and the SageMaker CreatePresignedNotebookInstanceUrl API to create a presigned notebook instance URL (Granting this IAM permission is what allows a User\/Role\/principal to open the notebook)\nUse a stateful HTTP library like requests to request this URL in a session and and save the cookie data set by the response. Fetching the URL logs your client in to Jupyter, and \"your client\" is the session - need to keep it persistent.\nUse the JupyterServer REST APIs for things like opening terminal or notebook sessions, listing available kernels, listing open sessions, etc.\nWhen you have a session open (terminal or notebook), use a WebSocket client library like websocket-client to interact with it (sending commands, receiving results, etc). Remember you'll need to use your same session for authentication.\n\nI think I only have end-to-end examples of this for SMStudio: The deprecated auto-installer of the official SageMaker Studio Auto-Shutdown Extension used to use this method before SMStudio Lifecycle Configuration Scripts became available, and some rough draft PoCs on GitHub explore the notebook side too but always with ref to Studio. However it should be possible for NBIs too with almost the same process: Just need to use the above mentioned API in place of CreatePresignedDomainUrl, and may need to check whether the REST api_base_url needs to be adjusted.\n\nIt might even be possible to use a higher-level solution like the nbclient library if you can get the authentication to work with it - would be interested to hear if anyone does!",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can we run a python script in using boto3 from a local machine?; Content: here's what i am trying to do: in my application that resides outside aws, i take some user inputs, and trigger scripts that reside inside notebook instance. i am able to start or create a new instance using boto3, and also use lifecycle configuration to run some starter script while the instance turns on. but i want to run multiple scripts in short intervals based on user inputs, so i don't want to restart my instance each time with a new lifecycle configuration script. i am trying to find if there is a way to execute shell commands in using boto3 (or any other way).",
        "Question_original_content_gpt_summary":"The user is trying to find a way to execute shell commands in AWS using boto3 from a local machine in order to run multiple scripts in short intervals based on user inputs.",
        "Question_preprocessed_content":"Title: can we run a python script in using boto from a local machine?; Content: here's what i am trying to do in my application that resides outside aws, i take some user inputs, and trigger scripts that reside inside notebook instance. i am able to start or create a new instance using boto , and also use lifecycle configuration to run some starter script while the instance turns on. but i want to run multiple scripts in short intervals based on user inputs, so i don't want to restart my instance each time with a new lifecycle configuration script. i am trying to find if there is a way to execute shell commands in using boto .",
        "Answer_original_content":"it should be possible, but it's probably not a great idea... this is not really an intended pattern for notebooks today, and it's more likely that you should be using processing jobs to schedule your regular tasks - taking input and output data direct from s3 rather than relying on local notebook storage. with that warning out of the way, a hacky solution: notebooks (both notebook instances and studio) are based on jupyter and thus today more-or-less conform (with some customizations) to jupyter's client\/server api model, which has both rest and websocket\/zeromq aspects. this means as long as you're able to handle authentication, it's possible to interact with the notebooks from a script using the same interfaces your browser would. this automation-style solution would proceed as (assuming python): use boto3 and the createpresignednotebookinstanceurl api to create a presigned notebook instance url (granting this iam permission is what allows a user\/role\/principal to open the notebook) use a stateful http library like requests to request this url in a session and and save the cookie data set by the response. fetching the url logs your client in to jupyter, and \"your client\" is the session - need to keep it persistent. use the jupyterserver rest apis for things like opening terminal or notebook sessions, listing available kernels, listing open sessions, etc. when you have a session open (terminal or notebook), use a websocket client library like websocket-client to interact with it (sending commands, receiving results, etc). remember you'll need to use your same session for authentication. i think i only have end-to-end examples of this for smstudio: the deprecated auto-installer of the official studio auto-shutdown extension used to use this method before smstudio lifecycle configuration scripts became available, and some rough draft pocs on github explore the notebook side too but always with ref to studio. however it should be possible for nbis too with almost the same process: just need to use the above mentioned api in place of createpresigneddomainurl, and may need to check whether the rest api_base_url needs to be adjusted. it might even be possible to use a higher-level solution like the nbclient library if you can get the authentication to work with it - would be interested to hear if anyone does!",
        "Answer_original_content_gpt_summary":"The answer suggests that it is possible to execute shell commands in AWS using boto3 from a local machine, but it is not a recommended approach. Instead, the user should use processing jobs to schedule regular tasks and take input and output data directly from S3. However, if the user still wants to proceed with the automation-style solution, they can use the Jupyter server REST APIs and a websocket client library like websocket-client to interact with the notebook instance. The user needs to handle authentication and keep the session persistent. The answer also mentions that there are some rough draft POCs on GitHub that explore the notebook side, but they are always with reference to Studio.",
        "Answer_preprocessed_content":"it should be possible, but it's probably not a great this is not really an intended pattern for notebooks today, and it's more likely that you should be using processing jobs to schedule your regular tasks taking input and output data direct from s rather than relying on local notebook storage. with that warning out of the way, a hacky solution notebooks are based on jupyter and thus today conform to jupyter's api model, which has both rest and aspects. this means as long as you're able to handle authentication, it's possible to interact with the notebooks from a script using the same interfaces your browser would. this solution would proceed as use boto and the createpresignednotebookinstanceurl api to create a presigned notebook instance url use a stateful http library like requests to request this url in a session and and save the cookie data set by the response. fetching the url logs your client in to jupyter, and your client is the session need to keep it persistent. use the jupyterserver rest apis for things like opening terminal or notebook sessions, listing available kernels, listing open sessions, etc. when you have a session open , use a websocket client library like to interact with it . remember you'll need to use your same session for authentication. i think i only have examples of this for smstudio the deprecated of the official studio extension used to use this method before smstudio lifecycle configuration scripts became available, and some rough draft pocs on github explore the notebook side too but always with ref to studio. however it should be possible for nbis too with almost the same process just need to use the above mentioned api in place of createpresigneddomainurl, and may need to check whether the rest needs to be adjusted. it might even be possible to use a solution like the nbclient library if you can get the authentication to work with it would be interested to hear if anyone does!"
    },
    {
        "Question_id":null,
        "Question_title":"Deploy model on AKS ModuleNotFoundError: No module named 'main'",
        "Question_body":"When trying to deploy a model on AKS an error occurs.\nThe traceback is the following:\nException in worker process\nTraceback (most recent call last):\nFile \"\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py\", line 583, in spawn_worker\nworker.init_process()\nFile \"\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py\", line 129, in init_process\nself.load_wsgi()\nFile \"\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py\", line 138, in load_wsgi\nself.wsgi = self.app.wsgi()\nFile \"\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py\", line 67, in wsgi\nself.callable = self.load()\nFile \"\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py\", line 52, in load\nreturn self.load_wsgiapp()\nFile \"\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py\", line 41, in load_wsgiapp\nreturn util.import_app(self.app_uri)\nFile \"\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/util.py\", line 350, in import_app\nimport(module)\nFile \"\/var\/azureml-server\/entry.py\", line 1, in <module>\nimport create_app\nFile \"\/var\/azureml-server\/create_app.py\", line 4, in <module>\nfrom routes_common import main\nFile \"\/var\/azureml-server\/routes_common.py\", line 32, in <module>\nfrom aml_blueprint import AMLBlueprint\nFile \"\/var\/azureml-server\/aml_blueprint.py\", line 29, in <module>\nimport main\nModuleNotFoundError: No module named 'main'",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1624613802117,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/451760\/deploy-model-on-aks-modulenotfounderror-no-module.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-02T04:59:21.207Z",
                "Answer_score":0,
                "Answer_body":"Thanks for the details. The product team is checking on this and updates at the following link for the same.\n\nhttps:\/\/github.com\/Azure\/azure-sdk-for-python\/issues\/19493",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deploy model on aks modulenotfounderror: no module named 'main'; Content: when trying to deploy a model on aks an error occurs. the traceback is the following: exception in worker process traceback (most recent call last): file \"\/-envs\/_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py\", line 583, in spawn_worker worker.init_process() file \"\/-envs\/_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py\", line 129, in init_process self.load_wsgi() file \"\/-envs\/_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py\", line 138, in load_wsgi self.wsgi = self.app.wsgi() file \"\/-envs\/_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py\", line 67, in wsgi self.callable = self.load() file \"\/-envs\/_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py\", line 52, in load return self.load_wsgiapp() file \"\/-envs\/_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py\", line 41, in load_wsgiapp return util.import_app(self.app_uri) file \"\/-envs\/_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/util.py\", line 350, in import_app import(module) file \"\/var\/-server\/entry.py\", line 1, in import create_app file \"\/var\/-server\/create_app.py\", line 4, in from routes_common import main file \"\/var\/-server\/routes_common.py\", line 32, in from aml_blueprint import amlblueprint file \"\/var\/-server\/aml_blueprint.py\", line 29, in import main modulenotfounderror: no module named 'main'",
        "Question_original_content_gpt_summary":"The user encountered a ModuleNotFoundError when trying to deploy a model on AKS.",
        "Question_preprocessed_content":"Title: deploy model on aks modulenotfounderror no module named 'main'; Content: when trying to deploy a model on aks an error occurs. the traceback is the following exception in worker process traceback file line , in file line , in file line , in file line , in wsgi file line , in load return file line , in return file line , in import file line , in import file line , in from import main file line , in from import amlblueprint file line , in import main modulenotfounderror no module named 'main'",
        "Answer_original_content":"thanks for the details. the product team is checking on this and updates at the following link for the same. https:\/\/github.com\/azure\/azure-sdk-for-python\/issues\/19493",
        "Answer_original_content_gpt_summary":"There is a ModuleNotFoundError issue when deploying a model on AKS. The product team is currently investigating the issue and updates can be found on the provided link. No specific solutions were mentioned in the answer.",
        "Answer_preprocessed_content":"thanks for the details. the product team is checking on this and updates at the following link for the same."
    },
    {
        "Question_id":null,
        "Question_title":"The group you tried to contact (support) may not exist",
        "Question_body":"<blockquote>\n<p>We\u2019re writing to let you know that the group you tried to contact (support) may not exist, or you may not have permission to post messages to the group. A few more details on why you weren\u2019t able to post:<\/p>\n<ul>\n<li>You might have spelled or formatted the group name incorrectly.<\/li>\n<li>The owner of the group may have removed this group.<\/li>\n<li>You may need to join the group before receiving permission to post.<\/li>\n<li>This group may not be open to posting.<\/li>\n<\/ul>\n<\/blockquote>\n<p>The email I was using is: <a href=\"mailto:support@wandb.com\">support@wandb.com<\/a><br>\nIt is displayed when my page crashes and I just wanted to report it.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1663158247836,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/the-group-you-tried-to-contact-support-may-not-exist\/3114",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-15T20:33:44.082Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dc914337\">@dc914337<\/a>, can you please expand on what you mean by the page crashing. Are you on a specific page when this occurs?<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-15T20:43:57.318Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dc914337\">@dc914337<\/a> , we will continue the conversation in <a href=\"https:\/\/community.wandb.ai\/t\/report-page-often-crashes-when-i-collapse-a-section-of-the-report\/3115\">the other thread you posted<\/a> as it seems the issue is tied to your reports.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-16T05:36:01.768Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> I\u2019m facing the same issue emailing <a href=\"mailto:support@wandb.com\">support@wandb.com<\/a> following instructions from <a href=\"https:\/\/docs.wandb.ai\/company\/getting-help\" class=\"inline-onebox\">Support - Documentation<\/a><\/p>\n<blockquote>\n<p>We\u2019re writing to let you know that the group you tried to contact (support) may not exist, or you may not have permission to post messages to the group. A few more details on why you weren\u2019t able to post:<\/p>\n<ul>\n<li>You might have spelled or formatted the group name incorrectly.<\/li>\n<li>The owner of the group may have removed this group.<\/li>\n<li>You may need to join the group before receiving permission to post.<\/li>\n<li>This group may not be open to posting.<\/li>\n<\/ul>\n<\/blockquote>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-16T16:18:04.295Z",
                "Answer_body":"<p>it\u2019s probably not. One is mail, second is your frontend.<br>\nHere is the screenshot of the undelivered message.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7340186597f35272af96614c058f92d16b1fd286.png\" data-download-href=\"\/uploads\/short-url\/gryeqiLtJdcWfcbZeJ34ZHNnmNo.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7340186597f35272af96614c058f92d16b1fd286.png\" alt=\"image\" data-base62-sha1=\"gryeqiLtJdcWfcbZeJ34ZHNnmNo\" width=\"690\" height=\"443\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7340186597f35272af96614c058f92d16b1fd286_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">736\u00d7473 14.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-15T16:18:10.511Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: the group you tried to contact (support) may not exist; Content: we\u2019re writing to let you know that the group you tried to contact (support) may not exist, or you may not have permission to post messages to the group. a few more details on why you weren\u2019t able to post: you might have spelled or formatted the group name incorrectly. the owner of the group may have removed this group. you may need to join the group before receiving permission to post. this group may not be open to posting. the email i was using is: support@.com it is displayed when my page crashes and i just wanted to report it.",
        "Question_original_content_gpt_summary":"The user encountered challenges when attempting to contact a support group, as the group may not exist or the user may not have permission to post messages to the group.",
        "Question_preprocessed_content":"Title: the group you tried to contact may not exist; Content: were writing to let you know that the group you tried to contact may not exist, or you may not have permission to post messages to the group. a few more details on why you werent able to post you might have spelled or formatted the group name incorrectly. the owner of the group may have removed this group. you may need to join the group before receiving permission to post. this group may not be open to posting. the email i was using is it is displayed when my page crashes and i just wanted to report it.",
        "Answer_original_content":"hi @dc914337, can you please expand on what you mean by the page crashing. are you on a specific page when this occurs? hi @dc914337 , we will continue the conversation in the other thread you posted as it seems the issue is tied to your reports. @mohammadbakir im facing the same issue emailing support@.com following instructions from support - documentation were writing to let you know that the group you tried to contact (support) may not exist, or you may not have permission to post messages to the group. a few more details on why you werent able to post: you might have spelled or formatted the group name incorrectly. the owner of the group may have removed this group. you may need to join the group before receiving permission to post. this group may not be open to posting. its probably not. one is mail, second is your frontend. here is the screenshot of the undelivered message. image736473 14.7 kb this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The answer does not provide any solutions to the user's challenge of contacting a support group. Instead, it seems to be a conversation between multiple users discussing different issues related to support.",
        "Answer_preprocessed_content":"hi can you please expand on what you mean by the page crashing. are you on a specific page when this occurs? hi , we will continue the conversation in the other thread you posted as it seems the issue is tied to your reports. im facing the same issue emailing following instructions from support documentation were writing to let you know that the group you tried to contact may not exist, or you may not have permission to post messages to the group. a few more details on why you werent able to post you might have spelled or formatted the group name incorrectly. the owner of the group may have removed this group. you may need to join the group before receiving permission to post. this group may not be open to posting. its probably not. one is mail, second is your frontend. here is the screenshot of the undelivered message. image kb this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":71926953.0,
        "Question_title":"Wandb website for Huggingface Trainer shows plots and logs only for the first model",
        "Question_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code>for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> website shows plots and logs only for the first file i.e., <code>file1<\/code> in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code>wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code>def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1650379721377,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":178.0,
        "Owner_creation_time":1392296244356,
        "Owner_last_access_time":1664050955008,
        "Owner_reputation":1020.0,
        "Owner_up_votes":278.0,
        "Owner_down_votes":17.0,
        "Owner_views":206.0,
        "Answer_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code>\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" rel=\"nofollow noreferrer\">https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650469853183,
        "Answer_score":0.0,
        "Owner_location":"Chicago, IL, USA",
        "Question_last_edit_time":1650406117732,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71926953",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: website for huggingface trainer shows plots and logs only for the first model; Content: i am finetuning multiple models using for loop as follows. for file in os.listdir(args.data_dir): finetune(args, file) but website shows plots and logs only for the first file i.e., file1 in data_dir although it is training and saving models for other files. it feels very strange behavior. : synced bertweet-base-finetuned-file1: https:\/\/.ai\/***\/huggingface\/runs\/*** this is a small snippet of finetuning code with huggingface: def finetune(args, file): training_args = trainingarguments( output_dir=f'{model_name}-finetuned-{file}', overwrite_output_dir=true, evaluation_strategy='no', num_train_epochs=args.epochs, learning_rate=args.lr, weight_decay=args.decay, per_device_train_batch_size=args.batch_size, per_device_eval_batch_size=args.batch_size, fp16=true, # mixed-precision training to boost speed save_strategy='no', seed=args.seed, dataloader_num_workers=4, ) trainer = trainer( model=model, args=training_args, train_dataset=tokenized_dataset['train'], eval_dataset=none, data_collator=data_collator, ) trainer.train() trainer.save_model()",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where the website for the Huggingface Trainer only shows plots and logs for the first model in the for loop, despite the fact that it is training and saving models for other files.",
        "Question_preprocessed_content":"Title: website for huggingface trainer shows plots and logs only for the first model; Content: i am finetuning multiple models using for loop as follows. but website shows plots and logs only for the first file in although it is training and saving models for other files. it feels very strange behavior. this is a small snippet of finetuning code with huggingface",
        "Answer_original_content":".init(reinit=true) and run.finish() helped me to log the models separately on website. the working code looks like below: for file in os.listdir(args.data_dir): finetune(args, file) import def finetune(args, file): run = .init(reinit=true) ... run.finish() reference: https:\/\/docs..ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of the Huggingface Trainer website only showing plots and logs for the first model in the for loop are to use .init(reinit=true) and run.finish() to log the models separately on the website. The code should be structured to include these functions within the for loop.",
        "Answer_preprocessed_content":"and helped me to log the models separately on website. the working code looks like below reference"
    },
    {
        "Question_id":null,
        "Question_title":"Detect SimpleNamespace as flag",
        "Question_body":"<p>Hi, I\u2019m trying to tune hyperparameters using guild. These are defined as follows:<\/p>\n<pre><code>from types import SimpleNamespace\nparams = SimpleNamespace(\n    embedding_dim = 256,\n    window_size = 5,\n    batch_size = 2048,\n    epochs = 2,\n    preprocessed = f'{DATASET_ROOT}\/{DATASET_PREFIX}',\n    working = f'{WORKING_ROOT}\/{DATASET_PREFIX}',\n    modelname = f'{WORKING_ROOT}\/{DATASET_VERSION}.pt',\n    train = True\n)\n<\/code><\/pre>\n<p>Guild won\u2019t find them by default and I am having a hard time working around this. I am  new to guild so maybe there is an answer in the DOCS i haven\u2019t found.<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1616438128886,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":266.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/detect-simplenamespace-as-flag\/567",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-24T20:09:04.466Z",
                "Answer_body":"<p>Hello and welcome! This is not supported using <code>SimpleNamespace<\/code> for flags directly, but you can use a standard Python dict this way:<\/p>\n<pre><code class=\"lang-python\">from types import SimpleNamespace\n\nparam_vals = {\n    \"foo\": 123,\n    \"bar\": \"hello there\",\n}\n\nparams = SimpleNamespace(**param_vals)\n\nprint(params)\n<\/code><\/pre>\n<p>The Guild file (<code>guild.yml<\/code>) needs to look like this in this case:<\/p>\n<pre><code class=\"lang-yaml\">op:\n  flags-dest: global:param_vals\n  flags-import: all\n<\/code><\/pre>\n<p>Support for <code>SimpleNamespace<\/code> based globals is a great idea! In the meantime, this workaround will get you past the blocker.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-25T07:03:13.876Z",
                "Answer_body":"<p>Thanks for the response!<\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-25T20:40:59.567Z",
                "Answer_body":"<p>We have a branch started for this feature:<\/p>\n<aside class=\"onebox githubfolder\">\n  <header class=\"source\">\n      <img src=\"https:\/\/github.githubassets.com\/favicons\/favicon.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/tree\/namespace-flags-dest\" target=\"_blank\" rel=\"noopener\">github.com<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:180\/180;\"><img src=\"https:\/\/avatars.githubusercontent.com\/u\/19977227?s=400&amp;amp;v=4\" class=\"thumbnail\" width=\"180\" height=\"180\"><\/div>\n\n<h3><a href=\"https:\/\/github.com\/guildai\/guildai\/tree\/namespace-flags-dest\" target=\"_blank\" rel=\"noopener\">guildai\/guildai<\/a><\/h3>\n\n<p><a href=\"https:\/\/github.com\/guildai\/guildai\/tree\/namespace-flags-dest\" target=\"_blank\" rel=\"noopener\">namespace-flags-dest<\/a><\/p>\n\n  <p><span class=\"label1\">Experiment tracking, ML developer tools. Contribute to guildai\/guildai development by creating an account on GitHub.<\/span><\/p>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>From the look of it, it\u2019s completed but we won\u2019t merge to master until 0.7.3 is released (hopefully later today).<\/p>\n<p>To support this, you need to use the <code>namespace<\/code> dest type like this:<\/p>\n<aside class=\"onebox githubblob\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/namespace-flags-dest\/examples\/flags\/guild.yml#L24-L28\" target=\"_blank\" rel=\"noopener\">github.com<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/namespace-flags-dest\/examples\/flags\/guild.yml#L24-L28\" target=\"_blank\" rel=\"noopener\">guildai\/guildai\/blob\/namespace-flags-dest\/examples\/flags\/guild.yml#L24-L28<\/a><\/h4>\n<pre class=\"onebox\"><code class=\"lang-yml\"><ol class=\"start lines\" start=\"24\" style=\"counter-reset: li-counter 23 ;\">\n<li>namespace:<\/li>\n<li>  description: Use a SimpleNamespace for flag values<\/li>\n<li>  main: global_namespace<\/li>\n<li>  flags-dest: namespace:params<\/li>\n<li>  flags-import: all<\/li>\n<\/ol><\/code><\/pre>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>With that, this code works as expected:<\/p>\n<aside class=\"onebox githubblob\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/namespace-flags-dest\/examples\/flags\/global_namespace.py#L17-L25\" target=\"_blank\" rel=\"noopener\">github.com<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/namespace-flags-dest\/examples\/flags\/global_namespace.py#L17-L25\" target=\"_blank\" rel=\"noopener\">guildai\/guildai\/blob\/namespace-flags-dest\/examples\/flags\/global_namespace.py#L17-L25<\/a><\/h4>\n<pre class=\"onebox\"><code class=\"lang-py\"><ol class=\"start lines\" start=\"17\" style=\"counter-reset: li-counter 16 ;\">\n<li>params = SimpleNamespace(<\/li>\n<li>    i=123,<\/li>\n<li>    f=1.123,<\/li>\n<li>    s=\"hello\",<\/li>\n<li>    b=False,<\/li>\n<li>    l=[1, 2, \"foo\"],<\/li>\n<li>)<\/li>\n<li>\n<li>print(params)<\/li>\n<\/ol><\/code><\/pre>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Thanks for the question\u2014I think it\u2019s a solid feature. The namespace interface is much cleaner than using dicts.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: detect simplenamespace as flag; Content: hi, i\u2019m trying to tune hyperparameters using guild. these are defined as follows: from types import simplenamespace params = simplenamespace( embedding_dim = 256, window_size = 5, batch_size = 2048, epochs = 2, preprocessed = f'{dataset_root}\/{dataset_prefix}', working = f'{working_root}\/{dataset_prefix}', modelname = f'{working_root}\/{dataset_version}.pt', train = true ) guild won\u2019t find them by default and i am having a hard time working around this. i am new to guild so maybe there is an answer in the docs i haven\u2019t found. thanks in advance",
        "Question_original_content_gpt_summary":"The user is encountering difficulty tuning hyperparameters using Guild, as Guild does not detect the simplenamespace flag by default.",
        "Question_preprocessed_content":"Title: detect simplenamespace as flag; Content: hi, im trying to tune hyperparameters using guild. these are defined as follows guild wont find them by default and i am having a hard time working around this. i am new to guild so maybe there is an answer in the docs i havent found. thanks in advance",
        "Answer_original_content":"hello and welcome! this is not supported using simplenamespace for flags directly, but you can use a standard python dict this way: from types import simplenamespace param_vals = { \"foo\": 123, \"bar\": \"hello there\", } params = simplenamespace(**param_vals) print(params) the guild file (guild.yml) needs to look like this in this case: op: flags-dest: global:param_vals flags-import: all support for simplenamespace based globals is a great idea! in the meantime, this workaround will get you past the blocker. thanks for the response! we have a branch started for this feature: github.com guildai\/guildai namespace-flags-dest experiment tracking, ml developer tools. contribute to guildai\/guildai development by creating an account on github. from the look of it, its completed but we wont merge to master until 0.7.3 is released (hopefully later today). to support this, you need to use the namespace dest type like this: github.com guildai\/guildai\/blob\/namespace-flags-dest\/examples\/flags\/guild.yml#l24-l28 namespace: description: use a simplenamespace for flag values main: global_namespace flags-dest: namespace:params flags-import: all with that, this code works as expected: github.com guildai\/guildai\/blob\/namespace-flags-dest\/examples\/flags\/global_namespace.py#l17-l25 params = simplenamespace( i=123, f=1.123, s=\"hello\", b=false, l=[1, 2, \"foo\"], ) print(params) thanks for the questioni think its a solid feature. the namespace interface is much cleaner than using dicts.",
        "Answer_original_content_gpt_summary":"Possible solutions to the difficulty of tuning hyperparameters using Guild include using a standard Python dict instead of simplenamespace for flags directly, and using the namespace dest type. The namespace interface is much cleaner than using dicts. A branch has been started for this feature, but it won't merge to master until 0.7.3 is released.",
        "Answer_preprocessed_content":"hello and welcome! this is not supported using for flags directly, but you can use a standard python dict this way the guild file needs to look like this in this case support for based globals is a great idea! in the meantime, this workaround will get you past the blocker. thanks for the response! we have a branch started for this feature experiment tracking, ml developer tools. contribute to development by creating an account on github. from the look of it, its completed but we wont merge to master until is released . to support this, you need to use the dest type like this with that, this code works as expected thanks for the questioni think its a solid feature. the namespace interface is much cleaner than using dicts."
    },
    {
        "Question_id":67496760.0,
        "Question_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Question_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1620788331577,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":770.0,
        "Owner_creation_time":1362580980910,
        "Owner_last_access_time":1663351509150,
        "Owner_reputation":4013.0,
        "Owner_up_votes":94.0,
        "Owner_down_votes":0.0,
        "Owner_views":72.0,
        "Answer_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1621009841940,
        "Answer_score":0.0,
        "Owner_location":"Akron, OH, USA",
        "Question_last_edit_time":1621022250560,
        "Answer_last_edit_time":1621012052112,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67496760",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: mounting an s3 bucket in docker in a agent; Content: what is the best practice for mounting an s3 container inside a docker image that will be using as a agent? i can think of 3 solutions, but have been unable to get any to work currently: use prefabbed configuration in , specifically _agent_k8s_host_mount. for this to work, the s3 bucket would be mounted separately on the host using rclone and then remapped into docker. this appears to only apply to kubernetes and not docker - and therefore would not work. mount using s3fuse as specified here. the issue is will it work with the s3 bucket secret stored in browser sessions? this would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar. pass arguments to docker using \"docker_args and docker_bash_setup_script arguments to task.create()\" as specified in the 1.0 release notes. this would be similar to (1), but the arguments would be for bind-mounting the volume. i do not see much documentation or examples on how this new feature may be used for this end.",
        "Question_original_content_gpt_summary":"The user is facing challenges in mounting an s3 bucket in a docker image to be used as an agent.",
        "Question_preprocessed_content":"Title: mounting an s bucket in docker in a agent; Content: what is the best practice for mounting an s container inside a docker image that will be using as a agent? i can think of solutions, but have been unable to get any to work currently use prefabbed configuration in , specifically for this to work, the s bucket would be mounted separately on the host using rclone and then remapped into docker. this appears to only apply to kubernetes and not docker and therefore would not work. mount using s fuse as specified here. the issue is will it work with the s bucket secret stored in browser sessions? this would also appear to be complicated and require custom docker images, not to mention running the docker image as or similar. pass arguments to docker using and arguments to as specified in the release notes. this would be similar to , but the arguments would be for the volume. i do not see much documentation or examples on how this new feature may be used for this end.",
        "Answer_original_content":"i was able to get another option entirely to work, namely, mount a drive on in wsl and then pass it to docker. let's get to it: why not host in windows itself, why rclone in wsl? docker running on wsl cannot access drives mounted through winfsp (what rclone uses) steps to mount the drive in in windows: you can install rclone in wsl and the mount will be accessible to docker create the folder \/data\/my-mount (this needs to be in \/data - i don't know why and i can't find out with a google search, but i found out about it here) you can put the configuration file in windows (use the --config option). note: will not support spaces in mounted paths, even though docker will. therefore your path has to be \/data\/my-mount rather than \/data\/my mount. there is a bug that i opened about this. you can test mounting by calling docker and mounting the file. example: docker run -it -v \\\\wsl$\\ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount note: you will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: docker: error response from daemon: error while creating mount source path now, you can setup the .conf file in c:\\users\\myself\\.conf such that: default_docker: { # default docker image to use when running in docker mode image: \"my-docker-image:latest\" # optional arguments to pass to docker image arguments: [\"-v\",\"\\\\wsl$\\ubuntu\\data:\/data\", ] } note that you can also run -agent out of wsl and then would only need to specify [\"-v\",\"\/data:\/data\", ]. run agent in cmd: -agent daemon --docker",
        "Answer_original_content_gpt_summary":"The solution to the challenge of mounting an s3 bucket in a docker image to be used as an agent is to mount a drive on in WSL and then pass it to docker. The steps to mount the drive in Windows are to install rclone in WSL, create the folder \/data\/my-mount, and put the configuration file in Windows. It is important to note that the path cannot have spaces in it and that mounting \/data rather than \/data\/my-mount is necessary. Additionally, the .conf file can be set up in c:\\users\\myself\\.conf to specify the default docker image to use when running in docker mode.",
        "Answer_preprocessed_content":"i was able to get another option entirely to work, namely, mount a drive on in wsl and then pass it to docker. let's get to it why not host in windows itself, why rclone in wsl? docker running on wsl cannot access drives mounted through winfsp steps to mount the drive in in windows you can install rclone in wsl and the mount will be accessible to docker create the folder you can put the configuration file in windows . note will not support spaces in mounted paths, even though docker will. therefore your path has to be rather than . there is a bug that i opened about this. you can test mounting by calling docker and mounting the file. example note you will have to mount rather than otherwise you may get this error now, you can setup the file in such that note that you can also run out of wsl and then would only need to specify . run agent in cmd"
    },
    {
        "Question_id":64921833.0,
        "Question_title":"DataBricks + Kedro Vs GCP + Kubeflow Vs Server + Kedro + Airflow",
        "Question_body":"<p>We are deploying a data consortium between more than 10 companies. Wi will deploy several machine learning models (in general advanced analytics models) for all the companies and we will administrate all the models. We are looking for a solution that administrates several servers, clusters and data science pipelines. I love kedro, but not sure what is the best option to administrate all while using kedro.<\/p>\n<p>In summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly Spark clusters. Our current options are:<\/p>\n<ul>\n<li><p>AWS as our data warehouse and Databricks for administrating servers, clusters and tasks. I don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so I would like to connect kedro to databricks (is it good? is it easy to schedule the run of the kedro pipelines using databricks?)<\/p>\n<\/li>\n<li><p>Using GCP for data warehouse and use kubeflow (iin GCP) for deploying models and the administration and the schedule of the pipelines and the needed resources<\/p>\n<\/li>\n<li><p>Setting up servers from ASW or GCP, install kedro and schedule the pipelines with airflow (I see a big problem administrating 20 servers and 40 pipelines)<\/p>\n<\/li>\n<\/ul>\n<p>I would like to know if someone knows what is the best option between these alternatives, their  downsides and advantages, or if there are more possibilities.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1605830422657,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":967.0,
        "Owner_creation_time":1605828724552,
        "Owner_last_access_time":1618356797460,
        "Owner_reputation":59.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":5.0,
        "Answer_body":"<p>I'll try and summarise what I know, but be aware that I've not been part of a KubeFlow project.<\/p>\n<h2>Kedro on Databricks<\/h2>\n<p>Our approach was to build our project with CI and then execute the pipeline from a notebook. We <em>did not<\/em> use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/11_tools_integration\/03_databricks.html\" rel=\"nofollow noreferrer\">kedro recommended approach<\/a> of using databricks-connect due to the <a href=\"https:\/\/databricks.com\/product\/aws-pricing\" rel=\"nofollow noreferrer\">large price difference<\/a> between Jobs and Interactive Clusters (which are needed for DB-connect). If you're working on several TB's of data, this quickly becomes relevant.<\/p>\n<p>As a DS, this approach may feel natural, as a SWE though it does not. Running pipelines in notebooks feels hacky. It works but it feels non-industrialised. Databricks performs well in automatically spinning up and down clusters &amp; taking care of the runtime for you. So their value add is abstracting IaaS away from you (more on that later).<\/p>\n<h2>GCP &amp; &quot;Cloud Native&quot;<\/h2>\n<p><strong>Pro<\/strong>: GCP's main selling point is BigQuery. It is an incredibly powerful platform, simply because you can be productive from day 0. I've seen people build entire web API's on top of it. KubeFlow isn't tied to GCP so you could port this somewhere else later on. Kubernetes will also allow you to run anything else you wish on the cluster, API's, streaming, web services, websites, you name it.<\/p>\n<p><strong>Con<\/strong>: Kubernetes is complex. If you have 10+ engineers to run this project long-term, you should be OK. But don't underestimate the complexity of Kubernetes. It is to the cloud what Linux is to the OS world. Think log management, noisy neighbours (one cluster for web APIs + batch spark jobs), multi-cluster management (one cluster per department\/project), security, resource access etc.<\/p>\n<h2>IaaS server approach<\/h2>\n<p>Your last alternative, the manual installation of servers is one I would recommend only if you have a large team, extremely large data and are building a long-term product who's revenue can sustain the large maintenance costs.<\/p>\n<h2>The people behind it<\/h2>\n<p>How does the talent market look like in your region? If you can hire experienced engineers with GCP knowledge, I'd go for the 2nd solution. GCP is a mature, &quot;native&quot; platform in the sense that it abstracts a lot away for customers. If your market has mainly AWS engineers, that may be a better road to take. If you have a number of kedro engineers, that also has relevance. Note that kedro is agnostic enough to run anywhere. It's really just python code.<\/p>\n<p><strong>Subjective advise<\/strong>:<\/p>\n<p>Having worked mostly on AWS projects and a few GCP projects, I'd go for GCP. I'd use the platform's components (BigQuery, Cloud Run, PubSub, Functions, K8S) as a toolbox to choose from and build an organisation around that. Kedro can run in any of these contexts, as a triggered job by the Scheduler, as a container on Kubernetes or as a ETL pipeline bringing data into (or out of) BigQuery.<\/p>\n<p>While Databricks is &quot;less management&quot; than raw AWS, it's still servers to think about and VPC networking charges to worry over. BigQuery is simply GB queried. Functions are simply invocation count. These high level components will allow you to quickly show value to customers and you only need to go deeper (RaaS -&gt; PaaS -&gt; IaaS) as you scale.<\/p>\n<p>AWS also has these higher level abstractions over IaaS but in general, it appears (to me) that Google's offering is the most mature. Mainly because they have published tools they've been using internally for almost a decade whereas AWS has built new tools for the market. AWS is the king of IaaS though.<\/p>\n<p>Finally, a bit of content, <a href=\"https:\/\/youtu.be\/kjhXMTOLtac?t=618\" rel=\"nofollow noreferrer\">two former colleagues have discussed ML industrialisation frameworks earlier this fall<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1605891844630,
        "Answer_score":4.0,
        "Owner_location":null,
        "Question_last_edit_time":1605836750283,
        "Answer_last_edit_time":1605947117163,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64921833",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: databricks + vs gcp + kubeflow vs server + + airflow; Content: we are deploying a data consortium between more than 10 companies. wi will deploy several machine learning models (in general advanced analytics models) for all the companies and we will administrate all the models. we are looking for a solution that administrates several servers, clusters and data science pipelines. i love , but not sure what is the best option to administrate all while using . in summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly spark clusters. our current options are: aws as our data warehouse and databricks for administrating servers, clusters and tasks. i don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so i would like to connect to databricks (is it good? is it easy to schedule the run of the pipelines using databricks?) using gcp for data warehouse and use kubeflow (iin gcp) for deploying models and the administration and the schedule of the pipelines and the needed resources setting up servers from asw or gcp, install and schedule the pipelines with airflow (i see a big problem administrating 20 servers and 40 pipelines) i would like to know if someone knows what is the best option between these alternatives, their downsides and advantages, or if there are more possibilities.",
        "Question_original_content_gpt_summary":"The user is looking for the best solution to administer several models, tasks, and pipelines in different servers and possibly Spark clusters, considering options such as AWS and Databricks, GCP and Kubeflow, and setting up servers with Airflow.",
        "Question_preprocessed_content":"Title: databricks + vs gcp + kubeflow vs server + + airflow; Content: we are deploying a data consortium between more than companies. wi will deploy several machine learning models for all the companies and we will administrate all the models. we are looking for a solution that administrates several servers, clusters and data science pipelines. i love , but not sure what is the best option to administrate all while using . in summary, we are looking for the best solution to administrate several models, tasks and pipelines in different servers and possibly spark clusters. our current options are aws as our data warehouse and databricks for administrating servers, clusters and tasks. i don't feel that the notebooks of databricks are a good solution for building pipelines and to work collaboratively, so i would like to connect to databricks using gcp for data warehouse and use kubeflow for deploying models and the administration and the schedule of the pipelines and the needed resources setting up servers from asw or gcp, install and schedule the pipelines with airflow i would like to know if someone knows what is the best option between these alternatives, their downsides and advantages, or if there are more possibilities.",
        "Answer_original_content":"i'll try and summarise what i know, but be aware that i've not been part of a kubeflow project. on databricks our approach was to build our project with ci and then execute the pipeline from a notebook. we did not use the recommended approach of using databricks-connect due to the large price difference between jobs and interactive clusters (which are needed for db-connect). if you're working on several tb's of data, this quickly becomes relevant. as a ds, this approach may feel natural, as a swe though it does not. running pipelines in notebooks feels hacky. it works but it feels non-industrialised. databricks performs well in automatically spinning up and down clusters & taking care of the runtime for you. so their value add is abstracting iaas away from you (more on that later). gcp & \"cloud native\" pro: gcp's main selling point is bigquery. it is an incredibly powerful platform, simply because you can be productive from day 0. i've seen people build entire web api's on top of it. kubeflow isn't tied to gcp so you could port this somewhere else later on. kubernetes will also allow you to run anything else you wish on the cluster, api's, streaming, web services, websites, you name it. con: kubernetes is complex. if you have 10+ engineers to run this project long-term, you should be ok. but don't underestimate the complexity of kubernetes. it is to the cloud what linux is to the os world. think log management, noisy neighbours (one cluster for web apis + batch spark jobs), multi-cluster management (one cluster per department\/project), security, resource access etc. iaas server approach your last alternative, the manual installation of servers is one i would recommend only if you have a large team, extremely large data and are building a long-term product who's revenue can sustain the large maintenance costs. the people behind it how does the talent market look like in your region? if you can hire experienced engineers with gcp knowledge, i'd go for the 2nd solution. gcp is a mature, \"native\" platform in the sense that it abstracts a lot away for customers. if your market has mainly aws engineers, that may be a better road to take. if you have a number of engineers, that also has relevance. note that is agnostic enough to run anywhere. it's really just python code. subjective advise: having worked mostly on aws projects and a few gcp projects, i'd go for gcp. i'd use the platform's components (bigquery, cloud run, pubsub, functions, k8s) as a toolbox to choose from and build an organisation around that. can run in any of these contexts, as a triggered job by the scheduler, as a container on kubernetes or as a etl pipeline bringing data into (or out of) bigquery. while databricks is \"less management\" than raw aws, it's still servers to think about and vpc networking charges to worry over. bigquery is simply gb queried. functions are simply invocation count. these high level components will allow you to quickly show value to customers and you only need to go deeper (raas -> paas -> iaas) as you scale. aws also has these higher level abstractions over iaas but in general, it appears (to me) that google's offering is the most mature. mainly because they have published tools they've been using internally for almost a decade whereas aws has built new tools for the market. aws is the king of iaas though. finally, a bit of content, two former colleagues have discussed ml industrialisation frameworks earlier this fall",
        "Answer_original_content_gpt_summary":"Possible solutions mentioned in the answer include using Databricks with CI and executing pipelines from a notebook, using GCP with Kubeflow and Kubernetes to run pipelines and other services, and setting up servers with Airflow as a last resort for large teams with extensive data and resources. The author also suggests considering the talent market in the region and using GCP's components as a toolbox to build an organization around. They recommend using higher-level abstractions over IaaS for faster value delivery and scaling.",
        "Answer_preprocessed_content":"i'll try and summarise what i know, but be aware that i've not been part of a kubeflow project. on databricks our approach was to build our project with ci and then execute the pipeline from a notebook. we did not use the recommended approach of using due to the large price difference between jobs and interactive clusters . if you're working on several tb's of data, this quickly becomes relevant. as a ds, this approach may feel natural, as a swe though it does not. running pipelines in notebooks feels hacky. it works but it feels databricks performs well in automatically spinning up and down clusters & taking care of the runtime for you. so their value add is abstracting iaas away from you . gcp & cloud native pro gcp's main selling point is bigquery. it is an incredibly powerful platform, simply because you can be productive from day . i've seen people build entire web api's on top of it. kubeflow isn't tied to gcp so you could port this somewhere else later on. kubernetes will also allow you to run anything else you wish on the cluster, api's, streaming, web services, websites, you name it. con kubernetes is complex. if you have + engineers to run this project you should be ok. but don't underestimate the complexity of kubernetes. it is to the cloud what linux is to the os world. think log management, noisy neighbours , management , security, resource access etc. iaas server approach your last alternative, the manual installation of servers is one i would recommend only if you have a large team, extremely large data and are building a product who's revenue can sustain the large maintenance costs. the people behind it how does the talent market look like in your region? if you can hire experienced engineers with gcp knowledge, i'd go for the nd solution. gcp is a mature, native platform in the sense that it abstracts a lot away for customers. if your market has mainly aws engineers, that may be a better road to take. if you have a number of engineers, that also has relevance. note that is agnostic enough to run anywhere. it's really just python code. subjective advise having worked mostly on aws projects and a few gcp projects, i'd go for gcp. i'd use the platform's components as a toolbox to choose from and build an organisation around that. can run in any of these contexts, as a triggered job by the scheduler, as a container on kubernetes or as a etl pipeline bringing data into bigquery. while databricks is less management than raw aws, it's still servers to think about and vpc networking charges to worry over. bigquery is simply gb queried. functions are simply invocation count. these high level components will allow you to quickly show value to customers and you only need to go deeper as you scale. aws also has these higher level abstractions over iaas but in general, it appears that google's offering is the most mature. mainly because they have published tools they've been using internally for almost a decade whereas aws has built new tools for the market. aws is the king of iaas though. finally, a bit of content, two former colleagues have discussed ml industrialisation frameworks earlier this fall"
    },
    {
        "Question_id":55580232.0,
        "Question_title":"Update SageMaker Jupyterlab environment",
        "Question_body":"<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1554750635030,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":1720.0,
        "Owner_creation_time":1361339272692,
        "Owner_last_access_time":1663965928400,
        "Owner_reputation":6281.0,
        "Owner_up_votes":430.0,
        "Owner_down_votes":17.0,
        "Owner_views":958.0,
        "Answer_body":"<p>Hi and thank you for using SageMaker!<\/p>\n\n<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait<\/code>.<\/p>\n\n<p>Best,\nKevin<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1555007316016,
        "Answer_score":8.0,
        "Owner_location":"NYC",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55580232",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: update jupyterlab environment; Content: how can i update my notebook's jupyter environment to the latest alpha release and then restart the process?",
        "Question_original_content_gpt_summary":"The user is looking to update their JupyterLab environment to the latest alpha release and restart the process.",
        "Question_preprocessed_content":"Title: update jupyterlab environment; Content: how can i update my notebook's jupyter environment to the latest alpha release and then restart the process?",
        "Answer_original_content":"hi and thank you for using ! to restart jupyter from within a notebook instance, you can issue the following command: sudo initctl restart jupyter-server --no-wait. best, kevin",
        "Answer_original_content_gpt_summary":"Possible solution: To restart Jupyter from within a notebook instance, the user can issue the command \"sudo initctl restart jupyter-server --no-wait\".",
        "Answer_preprocessed_content":"hi and thank you for using ! to restart jupyter from within a notebook instance, you can issue the following command . best, kevin"
    },
    {
        "Question_id":null,
        "Question_title":"Control knobs for sending commands back to the running job \/ controlling live variables from the das",
        "Question_body":"<h2>\n<a name=\"feature-1\" class=\"anchor\" href=\"#feature-1\"><\/a><img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/rocket.png?v=12\" title=\":rocket:\" class=\"emoji\" alt=\":rocket:\" loading=\"lazy\" width=\"20\" height=\"20\"> Feature<\/h2>\n<p>There should be a way to attach variables to the logger, that you can modify live from from controls in the dashboard.<\/p>\n<h3>\n<a name=\"motivation-2\" class=\"anchor\" href=\"#motivation-2\"><\/a>Motivation<\/h3>\n<p>When you have a running job and are monitoring the progress, you sometimes want to adjust the learning rate or other hyperparameter (should we switch to fine-tuning mode, etc.).<\/p>\n<h3>\n<a name=\"pitch-3\" class=\"anchor\" href=\"#pitch-3\"><\/a>Pitch<\/h3>\n<p>This is a bit of a unspoken black-magic deep learning technique. However, if you read papers from Meta, etc. or talk to hardcore old-school practitioners, they have these super long-running difficult optimization problems, and say something like: \u201cWell we trained the generator for X thousand epochs, then we enabled the discriminator, then Y thousand epochs later we dropped the learning rate, etc.\u201d This is ideally done by monitoring a live, running job and modifying the variables in situ.<\/p>\n<h3>\n<a name=\"alternatives-4\" class=\"anchor\" href=\"#alternatives-4\"><\/a>Alternatives<\/h3>\n<ul>\n<li>The non-agile way to do this is let your run go for a while, decide afterwards that you should have changed something at some point in time, code that, run it again and cross your fingers. This is obviously pretty slow and requires luck.<\/li>\n<li>A hacky way to do this is to create a DSL with sentinel files that the running job reads and applies. However, the workflow is useful enough that there should be a common way to do this.<\/li>\n<\/ul>\n<h3>\n<a name=\"additional-context-5\" class=\"anchor\" href=\"#additional-context-5\"><\/a>Additional context<\/h3>\n<p>I\u2019m not aware of any logging library that does this. So it would make great blog posts to show off and attract more users.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1674046663876,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":79.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/control-knobs-for-sending-commands-back-to-the-running-job-controlling-live-variables-from-the-das\/3710",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-27T20:33:49.173Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/turian\">@turian<\/a>, thank you for the feature request as well as the use-case this would unlock! I will go ahead and submit this to engineering team and follow up once they have a chance to look into this.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":20.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-27T20:50:18.560Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/nathank\">@nathank<\/a> Thanks. Here is another simple example, which would make a nice demonstration for a blog post:<\/p>\n<p>I was recently doing a randomized grid search, running 8 jobs simultaneously. After looking at a few training runs, it was clear that any model that did not achieve loss of 0.1 by batch 1000 should be stopped and restarted with new hyperparameters. So this is something that would be useful to control from the dashboard.<\/p>\n<p>(Another similar example is that I would then go manually adjust the grid script by hand, to remove learning rates that were too high or too low. I would prefer to do that from the dashboard.)<\/p>",
                "Answer_score":45.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: control knobs for sending commands back to the running job \/ controlling live variables from the das; Content: feature there should be a way to attach variables to the logger, that you can modify live from from controls in the dashboard. motivation when you have a running job and are monitoring the progress, you sometimes want to adjust the learning rate or other hyperparameter (should we switch to fine-tuning mode, etc.). pitch this is a bit of a unspoken black-magic deep learning technique. however, if you read papers from meta, etc. or talk to hardcore old-school practitioners, they have these super long-running difficult optimization problems, and say something like: well we trained the generator for x thousand epochs, then we enabled the discriminator, then y thousand epochs later we dropped the learning rate, etc. this is ideally done by monitoring a live, running job and modifying the variables in situ. alternatives the non-agile way to do this is let your run go for a while, decide afterwards that you should have changed something at some point in time, code that, run it again and cross your fingers. this is obviously pretty slow and requires luck. a hacky way to do this is to create a dsl with sentinel files that the running job reads and applies. however, the workflow is useful enough that there should be a common way to do this. additional context im not aware of any logging library that does this. so it would make great blog posts to show off and attract more users.",
        "Question_original_content_gpt_summary":"The user is seeking a way to modify variables in a running job in order to adjust hyperparameters and other settings in order to optimize performance, as opposed to the slow and luck-based alternative of running the job and then making changes afterwards.",
        "Question_preprocessed_content":"Title: control knobs for sending commands back to the running job \/ controlling live variables from the das; Content: feature there should be a way to attach variables to the logger, that you can modify live from from controls in the dashboard. motivation when you have a running job and are monitoring the progress, you sometimes want to adjust the learning rate or other hyperparameter . pitch this is a bit of a unspoken deep learning technique. however, if you read papers from meta, etc. or talk to hardcore practitioners, they have these super difficult optimization problems, and say something like well we trained the generator for x thousand epochs, then we enabled the discriminator, then y thousand epochs later we dropped the learning rate, etc. this is ideally done by monitoring a live, running job and modifying the variables in situ. alternatives the way to do this is let your run go for a while, decide afterwards that you should have changed something at some point in time, code that, run it again and cross your fingers. this is obviously pretty slow and requires luck. a hacky way to do this is to create a dsl with sentinel files that the running job reads and applies. however, the workflow is useful enough that there should be a common way to do this. additional context im not aware of any logging library that does this. so it would make great blog posts to show off and attract more users.",
        "Answer_original_content":"hi @turian, thank you for the feature request as well as the use-case this would unlock! i will go ahead and submit this to engineering team and follow up once they have a chance to look into this. thank you, nate @nathank thanks. here is another simple example, which would make a nice demonstration for a blog post: i was recently doing a randomized grid search, running 8 jobs simultaneously. after looking at a few training runs, it was clear that any model that did not achieve loss of 0.1 by batch 1000 should be stopped and restarted with new hyperparameters. so this is something that would be useful to control from the dashboard. (another similar example is that i would then go manually adjust the grid script by hand, to remove learning rates that were too high or too low. i would prefer to do that from the dashboard.)",
        "Answer_original_content_gpt_summary":"The answer does not provide any solutions to the user's question. Instead, it acknowledges the feature request and the use-case and promises to submit it to the engineering team for further consideration. The answer also provides an example of a situation where the ability to modify variables in a running job would be useful.",
        "Answer_preprocessed_content":"hi thank you for the feature request as well as the this would unlock! i will go ahead and submit this to engineering team and follow up once they have a chance to look into this. thank you, nate thanks. here is another simple example, which would make a nice demonstration for a blog post i was recently doing a randomized grid search, running jobs simultaneously. after looking at a few training runs, it was clear that any model that did not achieve loss of by batch should be stopped and restarted with new hyperparameters. so this is something that would be useful to control from the dashboard. another similar example is that i would then go manually adjust the grid script by hand, to remove learning rates that were too high or too low. i would prefer to do that from the"
    },
    {
        "Question_id":null,
        "Question_title":"Test dataset contains invalid data. ( Error 0018 ) in Azure ML Studio Evaluate Recommender",
        "Question_body":"I am doing a crop recommender system using the Matchbox recommender system in Azure ml studio.\nwhile splitting the dataset using Recommender split, it won't be split. but I split while using split rows, it works.\n\nbut when evaluating recommender it shows error like 'Test dataset contains invalid data'\nhow to overcome this issue?\n\n][2]\n\n\n\n\n[2]: \/answers\/storage\/attachments\/131249-rate.png",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1631430278073,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/548501\/test-dataset-contains-invalid-data-error-0018-in-a.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-24T07:19:36.587Z",
                "Answer_score":0,
                "Answer_body":"@JEYAKEERTHANANK-1817 Thanks, Please publish your project to azure gallery and share the link to your project to check. Here is the link to the document to Split Data using Recommender Split. We recommend that you review the walkthrough provided with this sample experiment in the Azure AI Gallery: Movie Recommendation.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: test dataset contains invalid data. ( error 0018 ) in studio evaluate recommender; Content: i am doing a crop recommender system using the matchbox recommender system in studio. while splitting the dataset using recommender split, it won't be split. but i split while using split rows, it works. but when evaluating recommender it shows error like 'test dataset contains invalid data' how to overcome this issue? ][2] [2]: \/answers\/storage\/attachments\/131249-rate.png",
        "Question_original_content_gpt_summary":"The user is encountering an error while attempting to evaluate a recommender system in Studio, which states that the test dataset contains invalid data.",
        "Question_preprocessed_content":"Title: test dataset contains invalid data. in studio evaluate recommender; Content: i am doing a crop recommender system using the matchbox recommender system in studio. while splitting the dataset using recommender split, it won't be split. but i split while using split rows, it works. but when evaluating recommender it shows error like 'test dataset contains invalid data' how to overcome this issue?",
        "Answer_original_content":"@jeyakeerthanank-1817 thanks, please publish your project to azure gallery and share the link to your project to check. here is the link to the document to split data using recommender split. we recommend that you review the walkthrough provided with this sample experiment in the azure ai gallery: movie recommendation.",
        "Answer_original_content_gpt_summary":"There are two possible solutions mentioned in the answer. The first is to publish the project to Azure Gallery and share the link for further checking. The second solution is to review the walkthrough provided in the Azure AI Gallery for splitting data using recommender split, specifically the movie recommendation experiment.",
        "Answer_preprocessed_content":"thanks, please publish your project to azure gallery and share the link to your project to check. here is the link to the document to split data using recommender split. we recommend that you review the walkthrough provided with this sample experiment in the azure ai gallery movie recommendation."
    },
    {
        "Question_id":null,
        "Question_title":"Generating scoring.py for azure ml model deployment",
        "Question_body":"Hi,\nI have registered a pretrained model to azure ml and i wish to deploy the model.\nThere is a compulsory file attachment called scoring file that i need to attach in order to deploy my model.\nMay i know how do i generate this scoring.py file?\nThanks in advance",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1642499410857,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/699403\/generating-scoringpy-for-azure-ml-model-deployment.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-18T17:38:53.153Z",
                "Answer_score":0,
                "Answer_body":"@Yuzu-9670\n\nHello,\n\nThanks for reaching out to us. Please follow below documentation to define your entry script after you registered your model.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli#define-a-dummy-entry-script\n\nPlease start with define a dummy entry and do the steps after.\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-26T04:53:39.51Z",
                "Answer_score":0,
                "Answer_body":"See this link:\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-managed-online-endpoints?tabs=azure-cli#understand-the-scoring-script\n\nSee this other thread as well:\nhttps:\/\/learn.microsoft.com\/en-us\/answers\/questions\/713048\/azure-ml-how-should-my-score-script-look-like-to-d.html\n\nIn my case, using Automated ML, I was able to download the scoring script by clicking on the model (yolov5 in my case) then clicking download\n@IamBeginner-6521",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":17.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: generating scoring.py for model deployment; Content: hi, i have registered a pretrained model to and i wish to deploy the model. there is a compulsory file attachment called scoring file that i need to attach in order to deploy my model. may i know how do i generate this scoring.py file? thanks in advance",
        "Question_original_content_gpt_summary":"The user is seeking guidance on how to generate a scoring.py file in order to deploy a pretrained model.",
        "Question_preprocessed_content":"Title: generating for model deployment; Content: hi, i have registered a pretrained model to and i wish to deploy the model. there is a compulsory file attachment called scoring file that i need to attach in order to deploy my model. may i know how do i generate this file? thanks in advance",
        "Answer_original_content":"@yuzu-9670 hello, thanks for reaching out to us. please follow below documentation to define your entry script after you registered your model. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli#define-a-dummy-entry-script please start with define a dummy entry and do the steps after. hope this will help. please let us know if any further queries. please don't forget to click on or upvote button whenever the information provided helps you. original posters help the community find answers faster by identifying the correct answer. here is how want a reminder to come back and check responses? here is how to subscribe to a notification if you are interested in joining the vm program and help shape the future of q&a: here is how you can be part of q&a volunteer moderators see this link: https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-managed-online-endpoints?tabs=azure-cli#understand-the-scoring-script see this other thread as well: https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/713048\/azure-ml-how-should-my-score-script-look-like-to-d.html in my case, using automated ml, i was able to download the scoring script by clicking on the model (yolov5 in my case) then clicking download @iambeginner-6521",
        "Answer_original_content_gpt_summary":"The answer provides a link to documentation on how to define an entry script for a pretrained model in order to generate a scoring.py file. The user is advised to start with defining a dummy entry and follow the steps provided in the documentation. Additionally, the answer provides links to other resources that may be helpful.",
        "Answer_preprocessed_content":"hello, thanks for reaching out to us. please follow below documentation to define your entry script after you registered your model. please start with define a dummy entry and do the steps after. hope this will help. please let us know if any further queries. please don't forget to click on or upvote button whenever the information provided helps you. original posters help the community find answers faster by identifying the correct answer. here is how want a reminder to come back and check responses? here is how to subscribe to a notification if you are interested in joining the vm program and help shape the future of q&a here is how you can be part of q&a volunteer moderators see this link see this other thread as well in my case, using automated ml, i was able to download the scoring script by clicking on the model then clicking download"
    },
    {
        "Question_id":57164290.0,
        "Question_title":"Training with TensorFlow on Sagemaker No module named 'tf_container'",
        "Question_body":"<p>I'm trying to training TensorFlow model on AWS Sagemaker.\nI created container with external lib for that (Use Your Own Algorithms or Models with Amazon SageMaker).<\/p>\n\n<p>we run a training job with TensorFlow API<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\nestimator = TensorFlow(\n  entry_point=\"entry.py\",             # entry script\n  role=role,\n  framework_version=\"1.13.0\",   \n  py_version='py3',\n  hyperparameters=hyperparameters,\n  train_instance_count=1,                   # \"The number of GPUs instances to use\"\n  train_instance_type=train_instance_type,\n  image_name=my_image\n\n)\nestimator.fit({'train': train_s3, 'eval': eval_s3})\n<\/code><\/pre>\n\n<p>and got an error:<\/p>\n\n<pre><code>09:06:46\n2019-07-23 09:06:45,463 INFO - root - running container entrypoint \uf141\n09:06:46\n2019-07-23 09:06:45,463 INFO - root - starting train task \uf141\n09:06:46\n2019-07-23 09:06:45,476 INFO - container_support.training - Training starting \uf141\n09:06:46\n2019-07-23 09:06:45,479 ERROR - container_support.training - uncaught exception during training: No module named 'tf_container'\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 136, in load_framework return importlib.import_module('mxnet_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 994, in _gcd_i \uf141\n09:06:46\nModuleNotFoundError: No module named 'mxnet_container'\n\uf141\n09:06:46\nDuring handling of the above exception, another exception occurred:\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/training.py\", line 35, in start fw = TrainingEnvironment.load_framework() File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 138, in load_framework return importlib.import_module('tf_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, \uf141\n09:06:46\nModuleNotFoundError: No module named 'tf_container'\n<\/code><\/pre>\n\n<p>What can I do to solve this issue? how can I debug this case?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1563885123080,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1191.0,
        "Owner_creation_time":1471952551663,
        "Owner_last_access_time":1662451190670,
        "Owner_reputation":53.0,
        "Owner_up_votes":3.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":"<p>I'm guessing that you used your own TF container, not the SageMaker one at <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>\n\n<p>If that's the case, your container is missing the support code needed to use the TensorFlow estimator ('tf_container' package).<\/p>\n\n<p>The solution is to start from the SageMaker container, customize it, push it back to ECR and pass the image name to the SageMaker estimator with the 'image_name' parameter.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1563901268600,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57164290",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: training with tensorflow on no module named 'tf_container'; Content: i'm trying to training tensorflow model on . i created container with external lib for that (use your own algorithms or models with ). we run a training job with tensorflow api from .tensorflow import tensorflow estimator = tensorflow( entry_point=\"entry.py\", # entry script role=role, framework_version=\"1.13.0\", py_version='py3', hyperparameters=hyperparameters, train_instance_count=1, # \"the number of gpus instances to use\" train_instance_type=train_instance_type, image_name=my_image ) estimator.fit({'train': train_s3, 'eval': eval_s3}) and got an error: 09:06:46 2019-07-23 09:06:45,463 info - root - running container entrypoint \uf141 09:06:46 2019-07-23 09:06:45,463 info - root - starting train task \uf141 09:06:46 2019-07-23 09:06:45,476 info - container_support.training - training starting \uf141 09:06:46 2019-07-23 09:06:45,479 error - container_support.training - uncaught exception during training: no module named 'tf_container' \uf141 09:06:46 traceback (most recent call last): file \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 136, in load_framework return importlib.import_module('mxnet_container') file \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) file \"<frozen importlib._bootstrap>\", line 994, in _gcd_i \uf141 09:06:46 modulenotfounderror: no module named 'mxnet_container' \uf141 09:06:46 during handling of the above exception, another exception occurred: \uf141 09:06:46 traceback (most recent call last): file \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/training.py\", line 35, in start fw = trainingenvironment.load_framework() file \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 138, in load_framework return importlib.import_module('tf_container') file \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, \uf141 09:06:46 modulenotfounderror: no module named 'tf_container' what can i do to solve this issue? how can i debug this case?",
        "Question_original_content_gpt_summary":"The user encountered a challenge while trying to train a TensorFlow model on AWS, resulting in an error of \"no module named 'tf_container'\".",
        "Question_preprocessed_content":"Title: training with tensorflow on no module named ; Content: i'm trying to training tensorflow model on . i created container with external lib for that . we run a training job with tensorflow api and got an error what can i do to solve this issue? how can i debug this case?",
        "Answer_original_content":"i'm guessing that you used your own tf container, not the one at https:\/\/github.com\/aws\/-tensorflow-container if that's the case, your container is missing the support code needed to use the tensorflow estimator ('tf_container' package). the solution is to start from the container, customize it, push it back to ecr and pass the image name to the estimator with the 'image_name' parameter.",
        "Answer_original_content_gpt_summary":"The solution to the error \"no module named 'tf_container'\" while training a TensorFlow model on AWS is to use the TensorFlow container from https:\/\/github.com\/aws\/-tensorflow-container, customize it, push it back to ECR, and pass the image name to the estimator with the 'image_name' parameter.",
        "Answer_preprocessed_content":"i'm guessing that you used your own tf container, not the one at if that's the case, your container is missing the support code needed to use the tensorflow estimator . the solution is to start from the container, customize it, push it back to ecr and pass the image name to the estimator with the parameter."
    },
    {
        "Question_id":69046990.0,
        "Question_title":"How to pass dependency files to sagemaker SKLearnProcessor and use it in Pipeline?",
        "Question_body":"<p>I need to import function from different python scripts, which will used inside <code>preprocessing.py<\/code> file. I was not able to find a way to pass the dependent files to <code>SKLearnProcessor<\/code> Object, due to which I am getting <code>ModuleNotFoundError<\/code>.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     instance_type='ml.m5.xlarge',\n                                     instance_count=1)\n\n\nsklearn_processor.run(code='preprocessing.py',\n                      inputs=[ProcessingInput(\n                        source=input_data,\n                        destination='\/opt\/ml\/processing\/input')],\n                      outputs=[ProcessingOutput(output_name='train_data',\n                                                source='\/opt\/ml\/processing\/train'),\n                               ProcessingOutput(output_name='test_data',\n                                                source='\/opt\/ml\/processing\/test')],\n                      arguments=['--train-test-split-ratio', '0.2']\n                     )\n<\/code><\/pre>\n<p>I would like to pass,\n<code>dependent_files = ['file1.py', 'file2.py', 'requirements.txt']<\/code>. So, that <code>preprocessing.py<\/code> have access to all the dependent modules.<\/p>\n<p>And also need to install libraries from <code>requirements.txt<\/code> file.<\/p>\n<p>Can you share any work around or a right way to do this?<\/p>\n<p><strong>Update-25-11-2021:<\/strong><\/p>\n<p><strong>Q1.<\/strong>(Answered but looking to solve using <code>FrameworkProcessor<\/code>)<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1426\" rel=\"noreferrer\">Here<\/a>, the <code>get_run_args<\/code> function, is handling <code>dependencies<\/code>, <code>source_dir<\/code> and <code>code<\/code> parameters by using <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/99f023e76a5db060907a796d4d8fee550f005844\/src\/sagemaker\/processing.py#L1265\" rel=\"noreferrer\">FrameworkProcessor<\/a>. Is there any way that we can set this parameters from <code>ScriptProcessor<\/code> or <code>SKLearnProcessor<\/code> or any other <code>Processor<\/code> to set them?<\/p>\n<p><strong>Q2.<\/strong><\/p>\n<p>Can you also please show some reference to use our <code>Processor<\/code> as <code>sagemaker.workflow.steps.ProcessingStep<\/code> and then use in <code>sagemaker.workflow.pipeline.Pipeline<\/code>?<\/p>\n<p>For having <code>Pipeline<\/code>, do we need <code>sagemaker-project<\/code> as mandatory or can we create <code>Pipeline<\/code> directly without any <code>Sagemaker-Project<\/code>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5.0,
        "Question_creation_time":1630681185260,
        "Question_favorite_count":2.0,
        "Question_score":11.0,
        "Question_view_count":2139.0,
        "Owner_creation_time":1500824148408,
        "Owner_last_access_time":1664025062208,
        "Owner_reputation":4419.0,
        "Owner_up_votes":434.0,
        "Owner_down_votes":324.0,
        "Owner_views":962.0,
        "Answer_body":"<p>There are a couple of options for you to accomplish that.<\/p>\n<p>One that is really simple is adding all additional files to a folder, example:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 my_package\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocessing.py\n<\/code><\/pre>\n<p>Then send this entire folder as another input under the same <code>\/opt\/ml\/processing\/input\/code\/<\/code>, example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import ProcessingInput, ProcessingOutput\n\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;0.20.0&quot;,\n    role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    instance_count=1,\n)\n\nsklearn_processor.run(\n    code=&quot;preprocessing.py&quot;,  # &lt;- this gets uploaded as \/opt\/ml\/processing\/input\/code\/preprocessing.py\n    inputs=[\n        ProcessingInput(source=input_data, destination='\/opt\/ml\/processing\/input'),\n        # Send my_package as \/opt\/ml\/processing\/input\/code\/my_package\/\n        ProcessingInput(source='my_package\/', destination=&quot;\/opt\/ml\/processing\/input\/code\/my_package\/&quot;)\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;),\n    ],\n    arguments=[&quot;--train-test-split-ratio&quot;, &quot;0.2&quot;],\n)\n<\/code><\/pre>\n<p>What happens is that <code>sagemaker-python-sdk<\/code> is going to put your argument <code>code=&quot;preprocessing.py&quot;<\/code> under <code>\/opt\/ml\/processing\/input\/code\/<\/code> and you will have <code>my_package\/<\/code> under the same directory.<\/p>\n<p><strong>Edit:<\/strong><\/p>\n<p>For the <code>requirements.txt<\/code>, you can add to your <code>preprocessing.py<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport subprocess\n\nsubprocess.check_call([\n    sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;-r&quot;,\n    &quot;\/opt\/ml\/processing\/input\/code\/my_package\/requirements.txt&quot;,\n])\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1637782762627,
        "Answer_score":17.0,
        "Owner_location":"India",
        "Question_last_edit_time":1637936310430,
        "Answer_last_edit_time":1637783228436,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69046990",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to pass dependency files to sklearnprocessor and use it in pipeline?; Content: i need to import function from different python scripts, which will used inside preprocessing.py file. i was not able to find a way to pass the dependent files to sklearnprocessor object, due to which i am getting modulenotfounderror. code: from .sklearn.processing import sklearnprocessor from .processing import processinginput, processingoutput sklearn_processor = sklearnprocessor(framework_version='0.20.0', role=role, instance_type='ml.m5.xlarge', instance_count=1) sklearn_processor.run(code='preprocessing.py', inputs=[processinginput( source=input_data, destination='\/opt\/ml\/processing\/input')], outputs=[processingoutput(output_name='train_data', source='\/opt\/ml\/processing\/train'), processingoutput(output_name='test_data', source='\/opt\/ml\/processing\/test')], arguments=['--train-test-split-ratio', '0.2'] ) i would like to pass, dependent_files = ['file1.py', 'file2.py', 'requirements.txt']. so, that preprocessing.py have access to all the dependent modules. and also need to install libraries from requirements.txt file. can you share any work around or a right way to do this? update-25-11-2021: q1.(answered but looking to solve using frameworkprocessor) here, the get_run_args function, is handling dependencies, source_dir and code parameters by using frameworkprocessor. is there any way that we can set this parameters from scriptprocessor or sklearnprocessor or any other processor to set them? q2. can you also please show some reference to use our processor as .workflow.steps.processingstep and then use in .workflow.pipeline.pipeline? for having pipeline, do we need -project as mandatory or can we create pipeline directly without any -project?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in passing dependency files to sklearnprocessor and using it in a pipeline, and is looking for a workaround or the right way to do this.",
        "Question_preprocessed_content":"Title: how to pass dependency files to sklearnprocessor and use it in pipeline?; Content: i need to import function from different python scripts, which will used inside file. i was not able to find a way to pass the dependent files to object, due to which i am getting . code i would like to pass, . so, that have access to all the dependent modules. and also need to install libraries from file. can you share any work around or a right way to do this? but looking to solve using here, the function, is handling , and parameters by using frameworkprocessor. is there any way that we can set this parameters from or or any other to set them? q . can you also please show some reference to use our as and then use in ? for having , do we need as mandatory or can we create directly without any ?",
        "Answer_original_content":"there are a couple of options for you to accomplish that. one that is really simple is adding all additional files to a folder, example: . my_package file1.py file2.py requirements.txt preprocessing.py then send this entire folder as another input under the same \/opt\/ml\/processing\/input\/code\/, example: from .sklearn.processing import sklearnprocessor from .processing import processinginput, processingoutput sklearn_processor = sklearnprocessor( framework_version=\"0.20.0\", role=role, instance_type=\"ml.m5.xlarge\", instance_count=1, ) sklearn_processor.run( code=\"preprocessing.py\", # <- this gets uploaded as \/opt\/ml\/processing\/input\/code\/preprocessing.py inputs=[ processinginput(source=input_data, destination='\/opt\/ml\/processing\/input'), # send my_package as \/opt\/ml\/processing\/input\/code\/my_package\/ processinginput(source='my_package\/', destination=\"\/opt\/ml\/processing\/input\/code\/my_package\/\") ], outputs=[ processingoutput(output_name=\"train_data\", source=\"\/opt\/ml\/processing\/train\"), processingoutput(output_name=\"test_data\", source=\"\/opt\/ml\/processing\/test\"), ], arguments=[\"--train-test-split-ratio\", \"0.2\"], ) what happens is that -python-sdk is going to put your argument code=\"preprocessing.py\" under \/opt\/ml\/processing\/input\/code\/ and you will have my_package\/ under the same directory. edit: for the requirements.txt, you can add to your preprocessing.py: import sys import subprocess subprocess.check_call([ sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"\/opt\/ml\/processing\/input\/code\/my_package\/requirements.txt\", ])",
        "Answer_original_content_gpt_summary":"The answer suggests a couple of options to pass dependency files to sklearnprocessor and use it in a pipeline. One option is to add all additional files to a folder and send this entire folder as another input under the same \/opt\/ml\/processing\/input\/code\/. Another option is to add the requirements.txt to the preprocessing.py file using the subprocess module.",
        "Answer_preprocessed_content":"there are a couple of options for you to accomplish that. one that is really simple is adding all additional files to a folder, example then send this entire folder as another input under the same , example what happens is that is going to put your argument under and you will have under the same directory. edit for the , you can add to your"
    },
    {
        "Question_id":30016116.0,
        "Question_title":"SQL - How to join two tables using values from the other table for missing or null values in either table",
        "Question_body":"<p><strong>Alright StackOverflow, I have a problem:<\/strong><\/p>\n\n<p>I am doing some work with <a href=\"http:\/\/azureml.com\" rel=\"nofollow\" title=\"Azure Machine Learning\">Azure Machine Learning<\/a> and I have reached an impasse. I have two tables, and I need to join them. The tables look like this:<\/p>\n\n<pre><code>   TABLE A          TABLE B   \n+-----------+    +-----------+\n| a | b | c |    | a | b | c |\n+-----------+    +-----------+\n| 1 | 2 |   |    |   | 2 | 3 |\n+-----------+    +-----------+\n<\/code><\/pre>\n\n<p>(those are just examples.)<\/p>\n\n<p>I need to join these tables when columns they share (in this case only b, but could be multiple) are equivalent. I also, however, need to populate missing values. If TABLE A is missing a value for one of its columns, and TABLE B has it for a matching row, they should combine values in the result table. I know that there is a way to do this one way, but it also needs to work in reverse, so that if TABLE B is missing a value, and TABLE A has one, it is populated.<\/p>\n\n<p><strong>EDIT: Desired Result:<\/strong><\/p>\n\n<pre><code>   TABLE C\n+-----------+\n| a | b | c |\n+-----------+\n| 1 | 2 | 3 |\n+-----------+\n<\/code><\/pre>\n\n<p>Some background information:<\/p>\n\n<ul>\n<li>AzureML uses a form of SQLite for their SQL interpretation, so please try and keep your answers in as basic SQL as possible. Thanks! :)<\/li>\n<li>AzureML has a built in join module, for those familiar with AzureML, but I don't think it'll be able to accomplish what is necessary. I'll use the SQL interpretation module.<\/li>\n<\/ul>\n\n<p>Your assistance is appreciated! Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1430669511030,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":90.0,
        "Owner_creation_time":1414621306620,
        "Owner_last_access_time":1663883459472,
        "Owner_reputation":1000.0,
        "Owner_up_votes":327.0,
        "Owner_down_votes":8.0,
        "Owner_views":124.0,
        "Answer_body":"<p><strong>Answering my own question:<\/strong><\/p>\n\n<p>It turned out the join type I needed was a <em>Full Outer Join.<\/em><\/p>\n\n<p>Background information:<\/p>\n\n<ul>\n<li>For those pursuing AzureML related to this question in the future, I had to enable the functionality to save columns from the 'Right' table input.<\/li>\n<li>I then ran this through a 'Project Columns' module and a 'Metadata' module to rename them to the form I desired.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1430677708887,
        "Answer_score":1.0,
        "Owner_location":"Raleigh, NC, USA",
        "Question_last_edit_time":1430676924820,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30016116",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sql - how to join two tables using values from the other table for missing or null values in either table; Content: alright stackoverflow, i have a problem: i am doing some work with and i have reached an impasse. i have two tables, and i need to join them. the tables look like this: table a table b +-----------+ +-----------+ | a | b | c | | a | b | c | +-----------+ +-----------+ | 1 | 2 | | | | 2 | 3 | +-----------+ +-----------+ (those are just examples.) i need to join these tables when columns they share (in this case only b, but could be multiple) are equivalent. i also, however, need to populate missing values. if table a is missing a value for one of its columns, and table b has it for a matching row, they should combine values in the result table. i know that there is a way to do this one way, but it also needs to work in reverse, so that if table b is missing a value, and table a has one, it is populated. edit: desired result: table c +-----------+ | a | b | c | +-----------+ | 1 | 2 | 3 | +-----------+ some background information: uses a form of sqlite for their sql interpretation, so please try and keep your answers in as basic sql as possible. thanks! :) has a built in join module, for those familiar with , but i don't think it'll be able to accomplish what is necessary. i'll use the sql interpretation module. your assistance is appreciated! thanks!",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in joining two tables using values from the other table for missing or null values in either table.",
        "Question_preprocessed_content":"Title: sql how to join two tables using values from the other table for missing or null values in either table; Content: alright stackoverflow, i have a problem i am doing some work with and i have reached an impasse. i have two tables, and i need to join them. the tables look like this those are just i need to join these tables when columns they share are equivalent. i also, however, need to populate missing values. if table a is missing a value for one of its columns, and table b has it for a matching row, they should combine values in the result table. i know that there is a way to do this one way, but it also needs to work in reverse, so that if table b is missing a value, and table a has one, it is populated. edit desired result some background information uses a form of sqlite for their sql interpretation, so please try and keep your answers in as basic sql as possible. thanks! has a built in join module, for those familiar with , but i don't think it'll be able to accomplish what is necessary. i'll use the sql interpretation module. your assistance is appreciated! thanks!",
        "Answer_original_content":"answering my own question: it turned out the join type i needed was a full outer join. background information: for those pursuing related to this question in the future, i had to enable the functionality to save columns from the 'right' table input. i then ran this through a 'project columns' module and a 'metadata' module to rename them to the form i desired.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of joining two tables using values from the other table for missing or null values is to use a full outer join. The user had to enable the functionality to save columns from the 'right' table input and then run it through a 'project columns' module and a 'metadata' module to rename them to the desired form.",
        "Answer_preprocessed_content":"answering my own question it turned out the join type i needed was a full outer join. background information for those pursuing related to this question in the future, i had to enable the functionality to save columns from the 'right' table input. i then ran this through a 'project columns' module and a 'metadata' module to rename them to the form i desired."
    },
    {
        "Question_id":null,
        "Question_title":"Endpoint deployment stuck on \u201cTransitioning\u201d in Azure Machine Learning",
        "Question_body":"I am trying to deploy an endpoint from the machine learning studio, but all endpoints get stuck in the transitioning state.\n\n\n\n\nWhen looking at the container's activity log, I can see the following operations took place:\n\nAnd if I select the top level failed action, I can see this error message:\n\nHowever, I should have the usual permissions for the group I'm deploying in, as I've created other resources in this group before.\n\nAm I missing a different permission which would not be needed for other resources?\n\nResource group: intrglmpdev00002\n\nSubscription Id: 931c1c11-140f-4489-a457-6f4b22023b26\n\nWorkspace: LimburgsMooisteML1\n\nWhile searching for a solution earlier I found this thread; https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/39341\/azure-ml-endpoint-stuck-in-transitioning-state.html I have also sent an email to microsoft as specified there.\n\nEDIT: I also just noticed that the description that I entered for testdeployment3 does not show up in the endpoint specifications.",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1605092257033,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/159147\/endpoint-deployment-stuck-on-transitioning-in-azur.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-16T07:04:01.327Z",
                "Answer_score":0,
                "Answer_body":"Found below error in the logs:\n\nStatus Code: 400 BadRequest Reason Phrase: Python interpreter version must be specified in the Conda file.\n\nPlease try specifying python version in your Conda file, something like:\n\ndependencies:\n# The python interpreter version.\n# Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2",
                "Answer_comment_count":7,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: endpoint deployment stuck on \u201ctransitioning\u201d in ; Content: i am trying to deploy an endpoint from the machine learning studio, but all endpoints get stuck in the transitioning state. when looking at the container's activity log, i can see the following operations took place: and if i select the top level failed action, i can see this error message: however, i should have the usual permissions for the group i'm deploying in, as i've created other resources in this group before. am i missing a different permission which would not be needed for other resources? resource group: intrglmpdev00002 subscription id: 931c1c11-140f-4489-a457-6f4b22023b26 workspace: limburgsmooisteml1 while searching for a solution earlier i found this thread; https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/39341\/azure-ml-endpoint-stuck-in-transitioning-state.html i have also sent an email to microsoft as specified there. edit: i also just noticed that the description that i entered for testdeployment3 does not show up in the endpoint specifications.",
        "Question_original_content_gpt_summary":"The user is encountering an issue with an endpoint deployment getting stuck in the \"transitioning\" state, despite having the necessary permissions for the resource group.",
        "Question_preprocessed_content":"Title: endpoint deployment stuck on transitioning in ; Content: i am trying to deploy an endpoint from the machine learning studio, but all endpoints get stuck in the transitioning state. when looking at the container's activity log, i can see the following operations took place and if i select the top level failed action, i can see this error message however, i should have the usual permissions for the group i'm deploying in, as i've created other resources in this group before. am i missing a different permission which would not be needed for other resources? resource group intrglmpdev subscription id workspace limburgsmooisteml while searching for a solution earlier i found this thread; i have also sent an email to microsoft as specified there. edit i also just noticed that the description that i entered for testdeployment does not show up in the endpoint specifications.",
        "Answer_original_content":"found below error in the logs: status code: 400 badrequest reason phrase: python interpreter version must be specified in the conda file. please try specifying python version in your conda file, something like: dependencies: # the python interpreter version. # currently only supports 3.5.2 and later. - python=3.6.2",
        "Answer_original_content_gpt_summary":"The solution to the issue of an endpoint deployment getting stuck in the \"transitioning\" state is to specify the Python version in the conda file. The error message suggests adding a line in the dependencies section of the conda file to specify the Python version, which currently only supports 3.5.2 and later.",
        "Answer_preprocessed_content":"found below error in the logs status code badrequest reason phrase python interpreter version must be specified in the conda file. please try specifying python version in your conda file, something like dependencies the python interpreter version. currently only supports and later."
    },
    {
        "Question_id":null,
        "Question_title":"Access to Azure ML named datasets",
        "Question_body":"We have a computer vision pipeline that finetunes a vision model.\nThe data for training the vision model is a large collection of images that lands in our data lake.\nThe problem is that we are not able to mount this dataset to our training job.\n\nIn Azure ML portal we defined a named dataset for the image folder in our data lake (not the default workspace data source).\nBut if in our job control script we will try to reference the dataset by name and mount it to the training job:\n\n docker_config = DockerConfiguration(use_docker=True)\n    \n # Access the dataset\n dataset = Dataset.get_by_name(ws, 'the name of our named dataset')\n    \n # Run the experiment\n args = ['--data-folder', dataset.as_mount() ]\n print(\"Mounted dataset\")\n    \n src = ScriptRunConfig(source_directory='.\/src',\n                       script='balearms_cnn_training.py',\n                       arguments=args,\n                       compute_target=compute_target,\n                       environment=keras_env,\n                       docker_runtime_config=docker_config)\n\n\n\n\nThe job will fail with the error:\n\n {\"NonCompliant\":\"UserErrorException:\\n\\tMessage: Cannot mount Dataset(id='389fb088-59eb-4288-8225-aa9fb55f14c0', name='balearms', version=1). Error Message: DataAccessError(PermissionDenied(Some(This request is not authorized to perform this operation using this permission.)))\\n\\tInnerException None\\n\\tErrorResponse \\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Cannot mount Dataset(id='389fb088-59eb-4288-8225-aa9fb55f14c0', name='balearms', version=1). Error Message: DataAccessError(PermissionDenied(Some(This request is not authorized to perform this operation using this permission.)))\\\"\\n    }\\n}\"}\n\n\n\n\u2026\n\n\n\n\nTo overcome this problem we use the default workspace dataset.\n\n datastore = ws.get_default_datastore()\n  dataset = Dataset.File.from_files(path=(datastore, 'datasets\/balearms\/ExtractedImages\/'))\n     \n  # Run the experiment\n  args = ['--data-folder', dataset.as_mount() ]\n  print(\"Mounted dataset\")\n     \n  src = ScriptRunConfig(source_directory='.\/src',\n  script='balearms_cnn_training.py',\n  arguments=args,\n  compute_target=compute_target,\n  environment=keras_env,\n  docker_runtime_config=docker_config)\n\n\n\nIt works but it also means that we have to copy the files to a different storage account (the default workspace dataset) and that creates complexities.\nThere is no doubt that we should be able to mount a named dataset. I believe that we miss just a small detail\u2026\n\nWould you be able to help us to figure out how to use named datasets and mount them to our training jobs?\n\nThanks\n\nManu",
        "Question_answer_count":2,
        "Question_comment_count":2.0,
        "Question_creation_time":1668460319020,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1088577\/access-to-azure-ml-named-datasets.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-17T00:37:32.987Z",
                "Answer_score":0,
                "Answer_body":"It's possible you defined the datalake as a datastore with identity-based access and also defined your compute instance\/cluster with managed identity enabled. In this case, you'll need to grant the Compute Cluster Managed Identity access to the Data Lake through RBAC. Check this link for more details: https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-identity-based-service-authentication",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-17T21:05:10.023Z",
                "Answer_score":0,
                "Answer_body":"The dataset that I needed to mount is a folder in my data lake (gen2).\nAs you would imagine the Azure ML data store I use is of type the data lake, and it is configured with a service principal that has all the permissions to access the data lake, but still, I cannot access the data.\n\nBUT. It is possible to create a blob data store (that uses the access key) to reference the same data.\nYes, it's not RBAC but when creating a data set from that blob-based data store, I am able to access the dataset, mount it, etc.\n\nSomething is broken in my data lake gen2 data store. I do not know what it is, but the blob data store is a reasonable workaround.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: access to named datasets; Content: we have a computer vision pipeline that finetunes a vision model. the data for training the vision model is a large collection of images that lands in our data lake. the problem is that we are not able to mount this dataset to our training job. in portal we defined a named dataset for the image folder in our data lake (not the default workspace data source). but if in our job control script we will try to reference the dataset by name and mount it to the training job: docker_config = dockerconfiguration(use_docker=true) # access the dataset dataset = dataset.get_by_name(ws, 'the name of our named dataset') # run the experiment args = ['--data-folder', dataset.as_mount() ] print(\"mounted dataset\") src = scriptrunconfig(source_directory='.\/src', script='balearms_cnn_training.py', arguments=args, compute_target=compute_target, environment=keras_env, docker_runtime_config=docker_config) the job will fail with the error: {\"noncompliant\":\"usererrorexception:\\n\\tmessage: cannot mount dataset(id='389fb088-59eb-4288-8225-aa9fb55f14c0', name='balearms', version=1). error message: dataaccesserror(permissiondenied(some(this request is not authorized to perform this operation using this permission.)))\\n\\tinnerexception none\\n\\terrorresponse \\n{\\n \\\"error\\\": {\\n \\\"code\\\": \\\"usererror\\\",\\n \\\"message\\\": \\\"cannot mount dataset(id='389fb088-59eb-4288-8225-aa9fb55f14c0', name='balearms', version=1). error message: dataaccesserror(permissiondenied(some(this request is not authorized to perform this operation using this permission.)))\\\"\\n }\\n}\"} \u2026 to overcome this problem we use the default workspace dataset. datastore = ws.get_default_datastore() dataset = dataset.file.from_files(path=(datastore, 'datasets\/balearms\/extractedimages\/')) # run the experiment args = ['--data-folder', dataset.as_mount() ] print(\"mounted dataset\") src = scriptrunconfig(source_directory='.\/src', script='balearms_cnn_training.py', arguments=args, compute_target=compute_target, environment=keras_env, docker_runtime_config=docker_config) it works but it also means that we have to copy the files to a different storage account (the default workspace dataset) and that creates complexities. there is no doubt that we should be able to mount a named dataset. i believe that we miss just a small detail\u2026 would you be able to help us to figure out how to use named datasets and mount them to our training jobs? thanks manu",
        "Question_original_content_gpt_summary":"The user is encountering challenges with accessing a named dataset and mounting it to their training job, resulting in the need to copy the files to a different storage account and creating complexities.",
        "Question_preprocessed_content":"Title: access to named datasets; Content: we have a computer vision pipeline that finetunes a vision model. the data for training the vision model is a large collection of images that lands in our data lake. the problem is that we are not able to mount this dataset to our training job. in portal we defined a named dataset for the image folder in our data lake . but if in our job control script we will try to reference the dataset by name and mount it to the training job access the dataset dataset 'the name of our named dataset' run the experiment args print src arguments args, the job will fail with the error to overcome this problem we use the default workspace dataset. datastore dataset run the experiment args print src arguments args, it works but it also means that we have to copy the files to a different storage account and that creates complexities. there is no doubt that we should be able to mount a named dataset. i believe that we miss just a small detail would you be able to help us to figure out how to use named datasets and mount them to our training jobs? thanks manu",
        "Answer_original_content":"it's possible you defined the datalake as a datastore with identity-based access and also defined your compute instance\/cluster with managed identity enabled. in this case, you'll need to grant the compute cluster managed identity access to the data lake through rbac. check this link for more details: https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-identity-based-service-authentication the dataset that i needed to mount is a folder in my data lake (gen2). as you would imagine the data store i use is of type the data lake, and it is configured with a service principal that has all the permissions to access the data lake, but still, i cannot access the data. but. it is possible to create a blob data store (that uses the access key) to reference the same data. yes, it's not rbac but when creating a data set from that blob-based data store, i am able to access the dataset, mount it, etc. something is broken in my data lake gen2 data store. i do not know what it is, but the blob data store is a reasonable workaround.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of accessing a named dataset and mounting it to a training job include granting the compute cluster managed identity access to the data lake through RBAC if the datalake is defined as a datastore with identity-based access and the compute instance\/cluster is defined with managed identity enabled. Another solution is to create a blob data store that uses the access key to reference the same data and create a dataset from that blob-based data store to access and mount the dataset.",
        "Answer_preprocessed_content":"it's possible you defined the datalake as a datastore with access and also defined your compute with managed identity enabled. in this case, you'll need to grant the compute cluster managed identity access to the data lake through rbac. check this link for more details the dataset that i needed to mount is a folder in my data lake . as you would imagine the data store i use is of type the data lake, and it is configured with a service principal that has all the permissions to access the data lake, but still, i cannot access the data. but. it is possible to create a blob data store to reference the same data. yes, it's not rbac but when creating a data set from that data store, i am able to access the dataset, mount it, etc. something is broken in my data lake gen data store. i do not know what it is, but the blob data store is a reasonable workaround."
    },
    {
        "Question_id":null,
        "Question_title":"Unable to import prophet",
        "Question_body":"I am attempting to run Prophet (fbprophet) in an Azure Notebook. I have installed prophet using the terminal window and it is listed as an installed package (prophet (0.1.1.post1)).\n\nWhen I attempt to import the Prophet module using either of the following commands in a Notebook cell I receive the error message; \"ModuleNotFoundError: No module named 'prophet'\"\n\n from fbprophet import Prophet\n or\n from prophet import Prophet\n\nCould someone please assist...thank you.",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1650846070067,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/824199\/unable-to-import-prophet.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-25T10:22:14.557Z",
                "Answer_score":0,
                "Answer_body":"@GrahamBenson-6517 Please try installing the package using the notebook cells instead of terminal window since you are running different kernels for the notebook session.\nPlease try the following from the cell and try to import the package.\n\n %pip install Prophet\n\n\n\nWorked in my notebook as seen below.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unable to import prophet; Content: i am attempting to run prophet (fbprophet) in an azure notebook. i have installed prophet using the terminal window and it is listed as an installed package (prophet (0.1.1.post1)). when i attempt to import the prophet module using either of the following commands in a notebook cell i receive the error message; \"modulenotfounderror: no module named 'prophet'\" from fbprophet import prophet or from prophet import prophet could someone please assist...thank you.",
        "Question_original_content_gpt_summary":"The user is unable to import the Prophet module in an Azure notebook despite having installed it successfully.",
        "Question_preprocessed_content":"Title: unable to import prophet; Content: i am attempting to run prophet in an azure notebook. i have installed prophet using the terminal window and it is listed as an installed package . when i attempt to import the prophet module using either of the following commands in a notebook cell i receive the error message; modulenotfounderror no module named 'prophet' from fbprophet import prophet or from prophet import prophet could someone please you.",
        "Answer_original_content":"@grahambenson-6517 please try installing the package using the notebook cells instead of terminal window since you are running different kernels for the notebook session. please try the following from the cell and try to import the package. %pip install prophet worked in my notebook as seen below. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"The solution to the problem of being unable to import the Prophet module in an Azure notebook despite having installed it successfully is to try installing the package using the notebook cells instead of the terminal window. This is because different kernels are running for the notebook session. The user can try using the command \"%pip install prophet\" in a cell and then try importing the package.",
        "Answer_preprocessed_content":"please try installing the package using the notebook cells instead of terminal window since you are running different kernels for the notebook session. please try the following from the cell and try to import the package. %pip install prophet worked in my notebook as seen below. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":71751105.0,
        "Question_title":"Upload custom file to s3 from training script in training component of AWS SageMaker Pipeline",
        "Question_body":"<p>I am new to Sagmaker, and I have created a pipeline from the SageMaker notebook, consisting of training and deployment components.\nIn the training script, we can upload the model to s3 via <strong>SM_MODEL_DIR<\/strong>. But now, I want to upload the classification report to s3. I tried this code. But It shows this is not a proper s3 bucket.<\/p>\n<pre><code>df_classification_report = pd.DataFrame(class_report).transpose()\nclassification_report_file_name = os.path.join(args.output_data_dir,\n                                               f&quot;{args.eval_model_name}_classification_report.csv&quot;)\ndf_classification_report.to_csv(classification_report_file_name)\n# instantiate S3 client and upload to s3\n\n# save classification report to s3\ns3 = boto3.resource('s3')\nprint(f&quot;classification_report is being uploaded to s3- {args.model_dir}&quot;)\ns3.meta.client.upload_file(classification_report_file_name, args.model_dir,\n                            f&quot;{args.eval_model_name}_classification_report.csv&quot;)\n<\/code><\/pre>\n<p>And the error<\/p>\n<pre><code>Invalid bucket name &quot;\/opt\/ml\/output\/data&quot;: Bucket name must match the regex &quot;^[a-zA-Z0-9.\\-_]{1,255}$&quot; or be an ARN matching the regex &quot;^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-zA-Z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-zA-Z0-9\\-]{1,63}$&quot;\n<\/code><\/pre>\n<p>Can anybody help? I really appreciate any help you can provide.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1649158713513,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":458.0,
        "Owner_creation_time":1383025927423,
        "Owner_last_access_time":1663674890763,
        "Owner_reputation":647.0,
        "Owner_up_votes":62.0,
        "Owner_down_votes":3.0,
        "Owner_views":105.0,
        "Answer_body":"<p>SageMaker Training Jobs will compress any files located in <code>\/opt\/ml\/model<\/code> which is the value of <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\/blob\/master\/ENVIRONMENT_VARIABLES.md#sm_model_dir\" rel=\"nofollow noreferrer\"><code>SM_MODEL_DIR<\/code><\/a> and upload it to S3 automatically. You could look at saving your file to <code>SM_MODEL_DIR<\/code> (Your classification report will thus be uploaded to S3 in the model tar ball).<\/p>\n<p>The <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.upload_file\" rel=\"nofollow noreferrer\"><code>upload_file()<\/code><\/a> function requires you to pass an S3 bucket.\nYou could also look at manually specify an S3 bucket in your code to upload the file to.<\/p>\n<pre><code>s3.meta.client.upload_file(classification_report_file_name, &lt;YourS3Bucket&gt;,\n                            f&quot;{args.eval_model_name}_classification_report.csv&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1649187284408,
        "Answer_score":2.0,
        "Owner_location":"Dhaka, Bangladesh",
        "Question_last_edit_time":1649166816216,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71751105",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: upload custom file to s3 from training script in training component of pipeline; Content: i am new to sagmaker, and i have created a pipeline from the notebook, consisting of training and deployment components. in the training script, we can upload the model to s3 via sm_model_dir. but now, i want to upload the classification report to s3. i tried this code. but it shows this is not a proper s3 bucket. df_classification_report = pd.dataframe(class_report).transpose() classification_report_file_name = os.path.join(args.output_data_dir, f\"{args.eval_model_name}_classification_report.csv\") df_classification_report.to_csv(classification_report_file_name) # instantiate s3 client and upload to s3 # save classification report to s3 s3 = boto3.resource('s3') print(f\"classification_report is being uploaded to s3- {args.model_dir}\") s3.meta.client.upload_file(classification_report_file_name, args.model_dir, f\"{args.eval_model_name}_classification_report.csv\") and the error invalid bucket name \"\/opt\/ml\/output\/data\": bucket name must match the regex \"^[a-za-z0-9.\\-_]{1,255}$\" or be an arn matching the regex \"^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]+:[0-9]{12}:accesspoint[\/:][a-za-z0-9\\-]{1,63}$|^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[\/:][a-za-z0-9\\-]{1,63}[\/:]accesspoint[\/:][a-za-z0-9\\-]{1,63}$\" can anybody help? i really appreciate any help you can provide.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with uploading a custom file to an S3 bucket from the training script in the training component of their pipeline.",
        "Question_preprocessed_content":"Title: upload custom file to s from training script in training component of pipeline; Content: i am new to sagmaker, and i have created a pipeline from the notebook, consisting of training and deployment components. in the training script, we can upload the model to s via but now, i want to upload the classification report to s . i tried this code. but it shows this is not a proper s bucket. and the error can anybody help? i really appreciate any help you can provide.",
        "Answer_original_content":"training jobs will compress any files located in \/opt\/ml\/model which is the value of sm_model_dir and upload it to s3 automatically. you could look at saving your file to sm_model_dir (your classification report will thus be uploaded to s3 in the model tar ball). the upload_file() function requires you to pass an s3 bucket. you could also look at manually specify an s3 bucket in your code to upload the file to. s3.meta.client.upload_file(classification_report_file_name, <yours3bucket>, f\"{args.eval_model_name}_classification_report.csv\")",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of uploading a custom file to an S3 bucket from the training script in the training component of the pipeline are: saving the file to sm_model_dir, manually specifying an S3 bucket in the code to upload the file to, and using the upload_file() function while passing an S3 bucket. The training job will automatically compress any files located in \/opt\/ml\/model and upload them to S3.",
        "Answer_preprocessed_content":"training jobs will compress any files located in which is the value of and upload it to s automatically. you could look at saving your file to . the function requires you to pass an s bucket. you could also look at manually specify an s bucket in your code to upload the file to."
    },
    {
        "Question_id":null,
        "Question_title":"Speech to Text using Microphone",
        "Question_body":"Hi there, Was trying to use convert speech to text using a microphone, getting a pop-up security error saying \"failed to construct 'worker': script at ..........\"Any idea why?Also, how can I use the speech to text service with microphone input, I tested uploading a video file and worked perfectly, but no idea how to use the service with microphone input?Please let me know AJ",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1637533140000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":99.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-Text-using-Microphone\/td-p\/176201\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-22T15:56:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"- The error message [1] can be seen in Chrome as Chrome doesn't let you load web workers when running scripts from a local file. You can try using a different web browser like Firefox and verify if it works for you.\n\n- In order to read from microphone, you can install PyAudio in your machine.\n\nYou can refer to the documentation [2] [3] to use the speech to text service with microphone input.\n\n[1] \"failed to construct 'worker': script at\n\n[2] https:\/\/www.thepythoncode.com\/article\/using-speech-recognition-to-convert-speech-to-text-python\n\n[3] https:\/\/pypi.org\/project\/SpeechRecognition\/"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: speech to text using microphone; Content: hi there, was trying to use convert speech to text using a microphone, getting a pop-up security error saying \"failed to construct 'worker': script at ..........\"any idea why?also, how can i use the speech to text service with microphone input, i tested uploading a video file and worked perfectly, but no idea how to use the service with microphone input?please let me know aj",
        "Question_original_content_gpt_summary":"The user was trying to use a speech to text service with microphone input, but encountered a security error and is unsure how to proceed.",
        "Question_preprocessed_content":"Title: speech to text using microphone; Content: hi there, was trying to use convert speech to text using a microphone, getting a security error saying failed to construct 'worker' script at idea why?also, how can i use the speech to text service with microphone input, i tested uploading a video file and worked perfectly, but no idea how to use the service with microphone input?please let me know aj",
        "Answer_original_content":"- the error message [1] can be seen in chrome as chrome doesn't let you load web workers when running scripts from a local file. you can try using a different web browser like firefox and verify if it works for you. - in order to read from microphone, you can install pyaudio in your machine. you can refer to the documentation [2] [3] to use the speech to text service with microphone input. [1] \"failed to construct 'worker': script at [2] https:\/\/www.thepythoncode.com\/article\/using-speech-recognition-to-convert-speech-to-text-python [3] https:\/\/pypi.org\/project\/speechrecognition\/",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are: \n- Try using a different web browser like Firefox to see if the speech to text service works.\n- Install Pyaudio in your machine and refer to the documentation to use the speech to text service with microphone input.",
        "Answer_preprocessed_content":"the error message can be seen in chrome as chrome doesn't let you load web workers when running scripts from a local file. you can try using a different web browser like firefox and verify if it works for you. in order to read from microphone, you can install pyaudio in your machine. you can refer to the documentation to use the speech to text service with microphone input. failed to construct 'worker' script at"
    },
    {
        "Question_id":72392070.0,
        "Question_title":"Sagemaker doesn't inference in an async manner",
        "Question_body":"<p>I've deployed a custom model with an async endpoint. I want to process video files with it because videos can have ~5-10 minutes I can't load all frames to memory. Of course, I want to make an inference on each frame.\nI've written<br \/>\n<code>input_fn<\/code> - download video file from s3 using boto and creates generator which loads video frames with a given batch size - return a generator - written with OpenCV<br \/>\n<code>predict_fn<\/code> - iterate over generator batched frames and generate prediction using model - save prediction in list<br \/>\n<code>output_fn<\/code> - transform prediction into json format, gzip all to reduce the size<\/p>\n<p>Endpoint works well, but the problem is concurrency. The sagemaker endpoint processes request after request (from cloudwatch and s3 save file time). I don't know why this happens.\nmax_concurrent_invocations_per_instance is set to 1000. Other settings from PyTorch serving are as follows:<\/p>\n<pre><code>SAGEMAKER_MODEL_SERVER_TIMEOUT: 100000\nSAGEMAKER_TS_MAX_BATCH_DELAY: 10000\nSAGEMAKER_TS_BATCH_SIZE: 1000\nSAGEMAKER_TS_MAX_WORKERS: 4\nSAGEMAKER_TS_RESPONSE_TIMEOUT: 100000\n<\/code><\/pre>\n<p>And still, it doesn't work. So how can I create an async inference endpoint with PyTorch to get concurrency?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1653569251380,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":109.0,
        "Owner_creation_time":1495477835623,
        "Owner_last_access_time":1662212097747,
        "Owner_reputation":148.0,
        "Owner_up_votes":36.0,
        "Owner_down_votes":0.0,
        "Owner_views":68.0,
        "Answer_body":"<p>The concurrency settings for TorchServe DLC are controlled by such mechanisms as # of workers, which can be set by defining the appropriate variables, such as <code>SAGEMAKER_TS_*<\/code>, and <code>SAGEMAKER_MODEL_*<\/code> (see, e.g., <a href=\"https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\" rel=\"nofollow noreferrer\">this page<\/a> for details on their meaning and implications).<\/p>\n<p>While the latter are agnostic to any particular serving stack and are defined in the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a>, the former are TorchServe-specific and are defined in <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\" rel=\"nofollow noreferrer\">TorchServe Inference Toolkit<\/a>. Moreover, since the TorchServe Inference Toolkit is built on top of the SageMaker Inference Toolkit, there is a non-trivial interplay between these two sets of params.<\/p>\n<p>Thus you may also want to experiment with such params as, e.g., <code>SAGEMAKER_MODEL_SERVER_WORKERS<\/code> to properly set up the concurrency setting of the SageMaker Async Endpoint.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1653766025390,
        "Answer_score":1.0,
        "Owner_location":"Rzeszow, Poland",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72392070",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: doesn't inference in an async manner; Content: i've deployed a custom model with an async endpoint. i want to process video files with it because videos can have ~5-10 minutes i can't load all frames to memory. of course, i want to make an inference on each frame. i've written input_fn - download video file from s3 using boto and creates generator which loads video frames with a given batch size - return a generator - written with opencv predict_fn - iterate over generator batched frames and generate prediction using model - save prediction in list output_fn - transform prediction into json format, gzip all to reduce the size endpoint works well, but the problem is concurrency. the endpoint processes request after request (from cloudwatch and s3 save file time). i don't know why this happens. max_concurrent_invocations_per_instance is set to 1000. other settings from pytorch serving are as follows: _model_server_timeout: 100000 _ts_max_batch_delay: 10000 _ts_batch_size: 1000 _ts_max_workers: 4 _ts_response_timeout: 100000 and still, it doesn't work. so how can i create an async inference endpoint with pytorch to get concurrency?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with creating an asynchronous inference endpoint with PyTorch to achieve concurrency, despite having set the max_concurrent_invocations_per_instance to 1000 and other settings.",
        "Question_preprocessed_content":"Title: doesn't inference in an async manner; Content: i've deployed a custom model with an async endpoint. i want to process video files with it because videos can have minutes i can't load all frames to memory. of course, i want to make an inference on each frame. i've written download video file from s using boto and creates generator which loads video frames with a given batch size return a generator written with opencv iterate over generator batched frames and generate prediction using model save prediction in list transform prediction into json format, gzip all to reduce the size endpoint works well, but the problem is concurrency. the endpoint processes request after request . i don't know why this happens. is set to . other settings from pytorch serving are as follows and still, it doesn't work. so how can i create an async inference endpoint with pytorch to get concurrency?",
        "Answer_original_content":"the concurrency settings for torchserve dlc are controlled by such mechanisms as # of workers, which can be set by defining the appropriate variables, such as _ts_*, and _model_* (see, e.g., this page for details on their meaning and implications). while the latter are agnostic to any particular serving stack and are defined in the inference toolkit, the former are torchserve-specific and are defined in torchserve inference toolkit. moreover, since the torchserve inference toolkit is built on top of the inference toolkit, there is a non-trivial interplay between these two sets of params. thus you may also want to experiment with such params as, e.g., _model_server_workers to properly set up the concurrency setting of the async endpoint.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of creating an asynchronous inference endpoint with PyTorch to achieve concurrency include adjusting the number of workers using variables such as _ts_* and _model_*, which are defined in the torchserve inference toolkit. Additionally, experimenting with parameters such as _model_server_workers_ may be necessary to properly set up the concurrency setting of the async endpoint.",
        "Answer_preprocessed_content":"the concurrency settings for torchserve dlc are controlled by such mechanisms as of workers, which can be set by defining the appropriate variables, such as , and . while the latter are agnostic to any particular serving stack and are defined in the inference toolkit, the former are and are defined in torchserve inference toolkit. moreover, since the torchserve inference toolkit is built on top of the inference toolkit, there is a interplay between these two sets of params. thus you may also want to experiment with such params as, to properly set up the concurrency setting of the async endpoint."
    },
    {
        "Question_id":62422682.0,
        "Question_title":"sagemaker notebook instance Elastic Inference tensorflow model local deployment",
        "Question_body":"<p>I am trying to replicate <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb<\/a><\/p>\n\n<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:<\/p>\n\n<pre><code>%%time\nimport boto3\n\nregion = boto3.Session().region_name\nsaved_model = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/model\/resnet\/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)\n\nimport sagemaker\nfrom sagemaker.tensorflow.serving import Model\n\nrole = sagemaker.get_execution_role()\n\ntensorflow_model = Model(model_data=saved_model,\nrole=role,\nframework_version='1.14')\ntf_predictor = tensorflow_model.deploy(initial_instance_count=1,\ninstance_type='local',\naccelerator_type='local_sagemaker_notebook')\n<\/code><\/pre>\n\n<p>I am getting following log in the notebook:<\/p>\n\n<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1\nalgo-1-7ynb1_1 | INFO:main:starting services\nalgo-1-7ynb1_1 | INFO:main:using default model name: Servo\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving model config:\nalgo-1-7ynb1_1 | model_config_list: {\nalgo-1-7ynb1_1 | config: {\nalgo-1-7ynb1_1 | name: \"Servo\",\nalgo-1-7ynb1_1 | base_path: \"\/opt\/ml\/model\/export\/Servo\",\nalgo-1-7ynb1_1 | model_platform: \"tensorflow\"\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:nginx config:\nalgo-1-7ynb1_1 | load_module modules\/ngx_http_js_module.so;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_processes auto;\nalgo-1-7ynb1_1 | daemon off;\nalgo-1-7ynb1_1 | pid \/tmp\/nginx.pid;\nalgo-1-7ynb1_1 | error_log \/dev\/stderr error;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_rlimit_nofile 4096;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | events {\nalgo-1-7ynb1_1 | worker_connections 2048;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | http {\nalgo-1-7ynb1_1 | include \/etc\/nginx\/mime.types;\nalgo-1-7ynb1_1 | default_type application\/json;\nalgo-1-7ynb1_1 | access_log \/dev\/stdout combined;\nalgo-1-7ynb1_1 | js_include tensorflow-serving.js;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream tfs_upstream {\nalgo-1-7ynb1_1 | server localhost:8501;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream gunicorn_upstream {\nalgo-1-7ynb1_1 | server unix:\/tmp\/gunicorn.sock fail_timeout=1;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | server {\nalgo-1-7ynb1_1 | listen 8080 deferred;\nalgo-1-7ynb1_1 | client_max_body_size 0;\nalgo-1-7ynb1_1 | client_body_buffer_size 100m;\nalgo-1-7ynb1_1 | subrequest_output_buffer_size 100m;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | set $tfs_version 1.14;\nalgo-1-7ynb1_1 | set $default_tfs_model Servo;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/tfs {\nalgo-1-7ynb1_1 | rewrite ^\/tfs\/(.) \/$1 break;\nalgo-1-7ynb1_1 | proxy_redirect off;\nalgo-1-7ynb1_1 | proxy_pass_request_headers off;\nalgo-1-7ynb1_1 | proxy_set_header Content-Type 'application\/json';\nalgo-1-7ynb1_1 | proxy_set_header Accept 'application\/json';\nalgo-1-7ynb1_1 | proxy_pass http:\/\/tfs_upstream;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ping {\nalgo-1-7ynb1_1 | js_content ping;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/invocations {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location ~ ^\/models\/(.)\/invoke {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/models {\nalgo-1-7ynb1_1 | proxy_pass http:\/\/gunicorn_upstream\/models;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ {\nalgo-1-7ynb1_1 | return 404 '{\"error\": \"Not Found\"}';\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | keepalive_timeout 3;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)\nalgo-1-7ynb1_1 | INFO:main:nginx version info:\nalgo-1-7ynb1_1 | nginx version: nginx\/1.16.1\nalgo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)\nalgo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016\nalgo-1-7ynb1_1 | TLS SNI support enabled\nalgo-1-7ynb1_1 | configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\nalgo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\nalgo-1-7ynb1_1 |\n!algo-1-7ynb1_1 | 172.18.0.1 - - [17\/Jun\/2020:05:02:10 +0000] \"GET \/ping HTTP\/1.1\" 200 3 \"-\" \"-\"\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-7ynb1_1 | EI Error Code: [3, 16, 8]\nalgo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | EI Client Version: 1.5.3\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\n`\n<\/code><\/pre>\n\n<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.<\/p>\n\n<p>I can see the docker of the model running\n<a href=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I try to infer\/predict from that model, I get error:<\/p>\n\n<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]\n\nalgo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-iikpj_1 | EI Error Code: [3, 16, 8]\nalgo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-iikpj_1 | EI Client Version: 1.5.3\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-iikpj_1 | INFO:main:tensorflow version info:\nalgo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-iikpj_1 | TensorFlow Library: 1.14.0\nalgo-1-iikpj_1 | EI Version: EI-1.4\nalgo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 172.18.0.1 - - [17\/Jun\/2020:05:29:47 +0000] \"POST \/invocations HTTP\/1.1\" 502 157 \"-\" \"-\"\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nJSONDecodeError Traceback (most recent call last)\nin ()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/serving.py in predict(self, data, initial_args)\n116 args[\"CustomAttributes\"] = self._model_attributes\n117\n--&gt; 118 return super(Predictor, self).predict(data, args)\n119\n120\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model)\n109 request_args = self._create_request_args(data, initial_args, target_model)\n110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n--&gt; 111 return self._handle_response(response)\n112\n113 def _handle_response(self, response):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in _handle_response(self, response)\n119 if self.deserializer is not None:\n120 # It's the deserializer's responsibility to close the stream\n--&gt; 121 return self.deserializer(response_body, response[\"ContentType\"])\n122 data = response_body.read()\n123 response_body.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in call(self, stream, content_type)\n578 \"\"\"\n579 try:\n--&gt; 580 return json.load(codecs.getreader(\"utf-8\")(stream))\n581 finally:\n582 stream.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n297 cls=cls, object_hook=object_hook,\n298 parse_float=parse_float, parse_int=parse_int,\n--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n300\n301\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 \"\"\"\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\n<\/code><\/pre>\n\n<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.\nI also tried Rest API post to docker deployed model:<\/p>\n\n<pre><code>curl -v -X POST \\ -H 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/Servo:predict\nbut still getting error:\n[![enter image description here][1]][1]\n<\/code><\/pre>\n\n<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.<\/p>\n\n<p>Please help me out. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1592375129670,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":350.0,
        "Owner_creation_time":1310699693432,
        "Owner_last_access_time":1663828283927,
        "Owner_reputation":2889.0,
        "Owner_up_votes":79.0,
        "Owner_down_votes":0.0,
        "Owner_views":62.0,
        "Answer_body":"<p>Solved it. The error I was getting is due to roles\/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1592876274340,
        "Answer_score":-1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62422682",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: notebook instance elastic inference tensorflow model local deployment; Content: i am trying to replicate https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb my elastic inference accelerator is attached to notebook instance. i am using conda_amazonei_tensorflow_p36 kernel. according to documentation i made the changes for local ei: %%time import boto3 region = boto3.session().region_name saved_model = 's3:\/\/-sample-data-{}\/tensorflow\/model\/resnet\/resnet_50_v2_fp32_nchw.tar.gz'.format(region) import from .tensorflow.serving import model role = .get_execution_role() tensorflow_model = model(model_data=saved_model, role=role, framework_version='1.14') tf_predictor = tensorflow_model.deploy(initial_instance_count=1, instance_type='local', accelerator_type='local__notebook') i am getting following log in the notebook: attaching to tmp6uqys1el_algo-1-7ynb1_1 algo-1-7ynb1_1 | info:main:starting services algo-1-7ynb1_1 | info:main:using default model name: servo algo-1-7ynb1_1 | info:main:tensorflow serving model config: algo-1-7ynb1_1 | model_config_list: { algo-1-7ynb1_1 | config: { algo-1-7ynb1_1 | name: \"servo\", algo-1-7ynb1_1 | base_path: \"\/opt\/ml\/model\/export\/servo\", algo-1-7ynb1_1 | model_platform: \"tensorflow\" algo-1-7ynb1_1 | } algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | algo-1-7ynb1_1 | info:main:nginx config: algo-1-7ynb1_1 | load_module modules\/ngx_http_js_module.so; algo-1-7ynb1_1 | algo-1-7ynb1_1 | worker_processes auto; algo-1-7ynb1_1 | daemon off; algo-1-7ynb1_1 | pid \/tmp\/nginx.pid; algo-1-7ynb1_1 | error_log \/dev\/stderr error; algo-1-7ynb1_1 | algo-1-7ynb1_1 | worker_rlimit_nofile 4096; algo-1-7ynb1_1 | algo-1-7ynb1_1 | events { algo-1-7ynb1_1 | worker_connections 2048; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | http { algo-1-7ynb1_1 | include \/etc\/nginx\/mime.types; algo-1-7ynb1_1 | default_type application\/json; algo-1-7ynb1_1 | access_log \/dev\/stdout combined; algo-1-7ynb1_1 | js_include tensorflow-serving.js; algo-1-7ynb1_1 | algo-1-7ynb1_1 | upstream tfs_upstream { algo-1-7ynb1_1 | server localhost:8501; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | upstream gunicorn_upstream { algo-1-7ynb1_1 | server unix:\/tmp\/gunicorn.sock fail_timeout=1; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | server { algo-1-7ynb1_1 | listen 8080 deferred; algo-1-7ynb1_1 | client_max_body_size 0; algo-1-7ynb1_1 | client_body_buffer_size 100m; algo-1-7ynb1_1 | subrequest_output_buffer_size 100m; algo-1-7ynb1_1 | algo-1-7ynb1_1 | set $tfs_version 1.14; algo-1-7ynb1_1 | set $default_tfs_model servo; algo-1-7ynb1_1 | algo-1-7ynb1_1 | location \/tfs { algo-1-7ynb1_1 | rewrite ^\/tfs\/(.) \/$1 break; algo-1-7ynb1_1 | proxy_redirect off; algo-1-7ynb1_1 | proxy_pass_request_headers off; algo-1-7ynb1_1 | proxy_set_header content-type 'application\/json'; algo-1-7ynb1_1 | proxy_set_header accept 'application\/json'; algo-1-7ynb1_1 | proxy_pass http:\/\/tfs_upstream; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | location \/ping { algo-1-7ynb1_1 | js_content ping; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | location \/invocations { algo-1-7ynb1_1 | js_content invocations; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | location ~ ^\/models\/(.)\/invoke { algo-1-7ynb1_1 | js_content invocations; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | location \/models { algo-1-7ynb1_1 | proxy_pass http:\/\/gunicorn_upstream\/models; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | location \/ { algo-1-7ynb1_1 | return 404 '{\"error\": \"not found\"}'; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | keepalive_timeout 3; algo-1-7ynb1_1 | } algo-1-7ynb1_1 | } algo-1-7ynb1_1 | algo-1-7ynb1_1 | algo-1-7ynb1_1 | info:main:tensorflow version info: algo-1-7ynb1_1 | tensorflow modelserver: 1.14.0-rc0+dev.sha.34d9e85 algo-1-7ynb1_1 | tensorflow library: 1.14.0 algo-1-7ynb1_1 | ei version: ei-1.4 algo-1-7ynb1_1 | info:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/\/model-config.cfg algo-1-7ynb1_1 | info:main:started tensorflow serving (pid: 8) algo-1-7ynb1_1 | info:main:nginx version info: algo-1-7ynb1_1 | nginx version: nginx\/1.16.1 algo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (ubuntu 5.4.0-6ubuntu1~16.04.11) algo-1-7ynb1_1 | built with openssl 1.0.2g 1 mar 2016 algo-1-7ynb1_1 | tls sni support enabled algo-1-7ynb1_1 | configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -o2 -fpie -fstack-protector-strong -wformat -werror=format-security -wp,-d_fortify_source=2 -fpic' --with-ld-opt='-wl,-bsymbolic-functions -fpie -pie -wl,-z,relro -wl,-z,now -wl,--as-needed -pie' algo-1-7ynb1_1 | info:main:started nginx (pid: 10) algo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: i tensorflow_serving\/model_servers\/server_core.cc:462] adding\/updating models. algo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: i tensorflow_serving\/model_servers\/server_core.cc:561] (re-)adding model: servo algo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: i tensorflow_serving\/core\/basic_manager.cc:739] successfully reserved resources to load servable {name: servo version: 1527887769} algo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: i tensorflow_serving\/core\/loader_harness.cc:66] approving load for servable version {name: servo version: 1527887769} algo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: i tensorflow_serving\/core\/loader_harness.cc:74] loading servable version {name: servo version: 1527887769} algo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: i external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] attempting to load native savedmodelbundle in bundle-shim from: \/opt\/ml\/model\/export\/servo\/1527887769 algo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] reading savedmodel from: \/opt\/ml\/model\/export\/servo\/1527887769 algo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] reading meta graph with tags { serve } algo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] restoring savedmodel bundle. algo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] running initialization op on savedmodel bundle at path: \/opt\/ml\/model\/export\/servo\/1527887769 algo-1-7ynb1_1 | using amazon elastic inference client library version: 1.5.3 algo-1-7ynb1_1 | number of elastic inference accelerators available: 1 algo-1-7ynb1_1 | elastic inference accelerator id: eia-813285f77ceb448c849e2331116f251b algo-1-7ynb1_1 | elastic inference accelerator type: eia2.medium algo-1-7ynb1_1 | elastic inference accelerator ordinal: 0 algo-1-7ynb1_1 | !algo-1-7ynb1_1 | 172.18.0.1 - - [17\/jun\/2020:05:02:10 +0000] \"get \/ping http\/1.1\" 200 3 \"-\" \"-\" algo-1-7ynb1_1 | [wed jun 17 05:02:11 2020, 662569us] [execution engine] error getting application context for [tensorflow][2] algo-1-7ynb1_1 | [wed jun 17 05:02:11 2020, 662722us] [execution engine][tensorflow][2] failed - last error: algo-1-7ynb1_1 | ei error code: [3, 16, 8] algo-1-7ynb1_1 | ei error description: unable to authenticate with accelerator algo-1-7ynb1_1 | ei request id: tf-d66b9810-d81a-448f-ace2-703fffa0f194 -- ei accelerator id: eia-813285f77ceb448c849e2331116f251b algo-1-7ynb1_1 | ei client version: 1.5.3 algo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: f external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] non-ok-status: swapexstatewithei(tmp_inputs, tmp_outputs, tmp_freeze) status: internal: failed to get the initial operator whitelist from server. algo-1-7ynb1_1 | warning:main:unexpected tensorflow serving exit (status: 6). restarting. algo-1-7ynb1_1 | info:main:tensorflow version info: algo-1-7ynb1_1 | tensorflow modelserver: 1.14.0-rc0+dev.sha.34d9e85 algo-1-7ynb1_1 | tensorflow library: 1.14.0 algo-1-7ynb1_1 | ei version: ei-1.4 algo-1-7ynb1_1 | info:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/\/model-config.cfg algo-1-7ynb1_1 | info:main:started tensorflow serving (pid: 38)`enter code here` algo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: i tensorflow_serving\/model_servers\/server_core.cc:462] adding\/updating models. algo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: i tensorflow_serving\/model_servers\/server_core.cc:561] (re-)adding model: servo algo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: i tensorflow_serving\/core\/basic_manager.cc:739] successfully reserved resources to load servable {name: servo version: 1527887769} algo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: i tensorflow_serving\/core\/loader_harness.cc:66] approving load for servable version {name: servo version: 1527887769} algo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: i tensorflow_serving\/core\/loader_harness.cc:74] loading servable version {name: servo version: 1527887769} algo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: i external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] attempting to load native savedmodelbundle in bundle-shim from: \/opt\/ml\/model\/export\/servo\/1527887769 algo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] reading savedmodel from: \/opt\/ml\/model\/export\/servo\/1527887769 algo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] reading meta graph with tags { serve } algo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] restoring savedmodel bundle. algo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] running initialization op on savedmodel bundle at path: \/opt\/ml\/model\/export\/servo\/1527887769 algo-1-7ynb1_1 | using amazon elastic inference client library version: 1.5.3 algo-1-7ynb1_1 | number of elastic inference accelerators available: 1 algo-1-7ynb1_1 | elastic inference accelerator id: eia-813285f77ceb448c849e2331116f251b algo-1-7ynb1_1 | elastic inference accelerator type: eia2.medium algo-1-7ynb1_1 | elastic inference accelerator ordinal: 0 ` the log never stops in notebook. it keeps throwing in notebook cells. i am not sure whether the model is deployed correctly. i can see the docker of the model running when i try to infer\/predict from that model, i get error: algo-1-iikpj_1 | [wed jun 17 05:29:47 2020, 761607us] [execution engine] error getting application context for [tensorflow][2] algo-1-iikpj_1 | [wed jun 17 05:29:47 2020, 761691us] [execution engine][tensorflow][2] failed - last error: algo-1-iikpj_1 | ei error code: [3, 16, 8] algo-1-iikpj_1 | ei error description: unable to authenticate with accelerator algo-1-iikpj_1 | ei request id: tf-adecd8ef-7138-4b5f-9c37-adfdc8122df1 -- ei accelerator id: eia-813285f77ceb448c849e2331116f251b algo-1-iikpj_1 | ei client version: 1.5.3 algo-1-iikpj_1 | 2020-06-17 05:29:47.768249: f external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] non-ok-status: swapexstatewithei(tmp_inputs, tmp_outputs, tmp_freeze) status: internal: failed to get the initial operator whitelist from server. algo-1-iikpj_1 | warning:main:unexpected tensorflow serving exit (status: 6). restarting. algo-1-iikpj_1 | info:main:tensorflow version info: algo-1-iikpj_1 | tensorflow modelserver: 1.14.0-rc0+dev.sha.34d9e85 algo-1-iikpj_1 | tensorflow library: 1.14.0 algo-1-iikpj_1 | ei version: ei-1.4 algo-1-iikpj_1 | info:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/\/model-config.cfg algo-1-iikpj_1 | info:main:started tensorflow serving (pid: 1052) algo-1-iikpj_1 | 2020-06-17 05:29:47.854331: i tensorflow_serving\/model_servers\/server_core.cc:462] adding\/updating models. algo-1-iikpj_1 | 2020-06-17 05:29:47.854405: i tensorflow_serving\/model_servers\/server_core.cc:561] (re-)adding model: servo algo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"post \/invocations http\/1.1\", subrequest: \"\/v1\/models\/servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/servo:predict\", host: \"localhost:8080\" algo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"post \/invocations http\/1.1\", subrequest: \"\/v1\/models\/servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/servo:predict\", host: \"localhost:8080\" algo-1-iikpj_1 | 172.18.0.1 - - [17\/jun\/2020:05:29:47 +0000] \"post \/invocations http\/1.1\" 502 157 \"-\" \"-\" algo-1-iikpj_1 | 2020-06-17 05:29:47.954825: i tensorflow_serving\/core\/basic_manager.cc:739] successfully reserved resources to load servable {name: servo version: 1527887769} algo-1-iikpj_1 | 2020-06-17 05:29:47.954887: i tensorflow_serving\/core\/loader_harness.cc:66] approving load for servable version {name: servo version: 1527887769} algo-1-iikpj_1 | 2020-06-17 05:29:47.955448: i tensorflow_serving\/core\/loader_harness.cc:74] loading servable version {name: servo version: 1527887769} algo-1-iikpj_1 | 2020-06-17 05:29:47.955494: i external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] attempting to load native savedmodelbundle in bundle-shim from: \/opt\/ml\/model\/export\/servo\/1527887769 algo-1-iikpj_1 | 2020-06-17 05:29:47.955859: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] reading savedmodel from: \/opt\/ml\/model\/export\/servo\/1527887769 algo-1-iikpj_1 | 2020-06-17 05:29:47.969511: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] reading meta graph with tags { serve } jsondecodeerror traceback (most recent call last) in () ~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/\/tensorflow\/serving.py in predict(self, data, initial_args) 116 args[\"customattributes\"] = self._model_attributes 117 --> 118 return super(predictor, self).predict(data, args) 119 120 ~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/\/predictor.py in predict(self, data, initial_args, target_model) 109 request_args = self._create_request_args(data, initial_args, target_model) 110 response = self._session._runtime_client.invoke_endpoint(**request_args) --> 111 return self._handle_response(response) 112 113 def _handle_response(self, response): ~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/\/predictor.py in _handle_response(self, response) 119 if self.deserializer is not none: 120 # it's the deserializer's responsibility to close the stream --> 121 return self.deserializer(response_body, response[\"contenttype\"]) 122 data = response_body.read() 123 response_body.close() ~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/\/predictor.py in call(self, stream, content_type) 578 \"\"\" 579 try: --> 580 return json.load(codecs.getreader(\"utf-8\")(stream)) 581 finally: 582 stream.close() ~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw) 297 cls=cls, object_hook=object_hook, 298 parse_float=parse_float, parse_int=parse_int, --> 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw) 300 301 ~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw) 352 parse_int is none and parse_float is none and 353 parse_constant is none and object_pairs_hook is none and not kw): --> 354 return _default_decoder.decode(s) 355 if cls is none: 356 cls = jsondecoder ~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w) 337 338 \"\"\" --> 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end()) 340 end = _w(s, end).end() 341 if end != len(s): ~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx) 355 obj, end = self.scan_once(s, idx) 356 except stopiteration as err: --> 357 raise jsondecodeerror(\"expecting value\", s, err.value) from none 358 return obj, end jsondecodeerror: expecting value: line 1 column 1 (char 0) algo-1-iikpj_1 | 2020-06-17 05:29:48.047106: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] restoring savedmodel bundle. algo-1-iikpj_1 | 2020-06-17 05:29:48.564452: i external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] running initialization op on savedmodel bundle at path: \/opt\/ml\/model\/export\/servo\/1527887769 algo-1-iikpj_1 | using amazon elastic inference client library version: 1.5.3 i tried several ways to solve jsondecodeerror: expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps. i also tried rest api post to docker deployed model: curl -v -x post \\ -h 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/servo:predict but still getting error: [![enter image description here][1]][1] please help me to resolve the issue. initially, i was trying to use my tensorflow serving model and getting the same errors. then i thought of following with the same model which was used in aws example notebook (resnet_50_v2_fp32_nchw.tar.gz'). so, the above experiment is using aws example notebook with model provided by -sample-data. please help me out. thanks",
        "Question_original_content_gpt_summary":"The user is encountering challenges with deploying a TensorFlow model with Elastic Inference on a notebook instance, resulting in errors such as \"jsondecodeerror: expecting value: line 1 column 1 (char 0)\" and \"curl -v -x post \\ -h 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/servo",
        "Question_preprocessed_content":"Title: notebook instance elastic inference tensorflow model local deployment; Content: i am trying to replicate my elastic inference accelerator is attached to notebook instance. i am using kernel. according to documentation i made the changes for local ei i am getting following log in the notebook the log never stops in notebook. it keeps throwing in notebook cells. i am not sure whether the model is deployed correctly. i can see the docker of the model running when i try to from that model, i get error i tried several ways to solve jsondecodeerror expecting value line column using etc but nothing helps. i also tried rest api post to docker deployed model please help me to resolve the issue. initially, i was trying to use my tensorflow serving model and getting the same errors. then i thought of following with the same model which was used in aws example notebook . so, the above experiment is using aws example notebook with model provided by please help me out. thanks",
        "Answer_original_content":"solved it. the error i was getting is due to roles\/permission of elastic inference attached to notebook. once fixed these permissions by our devops team. it worked as expected. see https:\/\/github.com\/aws\/-tensorflow-serving-container\/issues\/142",
        "Answer_original_content_gpt_summary":"The solution to the error encountered while deploying a TensorFlow model with Elastic Inference on a notebook instance is to fix the roles\/permissions of Elastic Inference attached to the notebook. The user's devops team fixed these permissions, and the deployment worked as expected.",
        "Answer_preprocessed_content":"solved it. the error i was getting is due to of elastic inference attached to notebook. once fixed these permissions by our devops team. it worked as expected. see"
    },
    {
        "Question_id":71172207.0,
        "Question_title":"AWS Sagemaker Life Cycle Configuration - AutoStop",
        "Question_body":"<p>I have created 4 instances in my AWS sagemaker NOTEBOOKS tab.<\/p>\n<p>I want to create a life cycle configuration where the instance should stop every day at 9:00 PM.<\/p>\n<p>I have seen some examples but it is with IDLE TIME but not with the specific time<\/p>\n<pre><code>#!\/bin\/bash\nset -e\n\n# PARAMETERS\nIDLE_TIME=3600\n\necho &quot;Fetching the autostop script&quot;\nwget -O autostop.py https:\/\/raw.githubusercontent.com\/mariokostelac\/sagemaker-setup\/master\/scripts\/auto-stop-idle\/autostop.py\n\necho &quot;Starting the SageMaker autostop script in cron&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/5 * * * * \/bin\/bash -c '\/usr\/bin\/python3 $DIR\/autostop.py --time ${IDLE_TIME} | tee -a \/home\/ec2-user\/SageMaker\/auto-stop-idle.log'&quot;) | crontab -\n\necho &quot;Changing cloudwatch configuration&quot;\ncurl https:\/\/raw.githubusercontent.com\/mariokostelac\/sagemaker-setup\/master\/scripts\/publish-logs-to-cloudwatch\/on-start.sh | sudo bash -s auto-stop-idle \/home\/ec2-user\/SageMaker\/auto-stop-idle.log\n<\/code><\/pre>\n<p>Can anyone help me out on this one?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1645181373237,
        "Question_favorite_count":0.0,
        "Question_score":1.0,
        "Question_view_count":357.0,
        "Owner_creation_time":1608154572552,
        "Owner_last_access_time":1663655775236,
        "Owner_reputation":75.0,
        "Owner_up_votes":34.0,
        "Owner_down_votes":0.0,
        "Owner_views":33.0,
        "Answer_body":"<p>Change the crontab syntax to <code>0 21 * * * shutdown.py<\/code><\/p>\n<p>Then create a shutdown.py which is reduced version of the <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py\" rel=\"nofollow noreferrer\">autostop.py<\/a> and contains mainly:<\/p>\n<pre><code>...\nprint('Closing notebook')\nclient = boto3.client('sagemaker')\nclient.stop_notebook_instance(NotebookInstanceName=get_notebook_name())\n<\/code><\/pre>\n<p>BTW: triggering <code>shutdown now<\/code> directly from the crontab command didn't work for me, therefore calling the SageMaker API instead.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1645343727107,
        "Answer_score":1.0,
        "Owner_location":"Islamabad, Pakistan",
        "Question_last_edit_time":1645181463240,
        "Answer_last_edit_time":1645350630310,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71172207",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: life cycle configuration - autostop; Content: i have created 4 instances in my notebooks tab. i want to create a life cycle configuration where the instance should stop every day at 9:00 pm. i have seen some examples but it is with idle time but not with the specific time #!\/bin\/bash set -e # parameters idle_time=3600 echo \"fetching the autostop script\" wget -o autostop.py https:\/\/raw.githubusercontent.com\/mariokostelac\/-setup\/master\/scripts\/auto-stop-idle\/autostop.py echo \"starting the autostop script in cron\" (crontab -l 2>\/dev\/null; echo \"*\/5 * * * * \/bin\/bash -c '\/usr\/bin\/python3 $dir\/autostop.py --time ${idle_time} | tee -a \/home\/ec2-user\/\/auto-stop-idle.log'\") | crontab - echo \"changing cloudwatch configuration\" curl https:\/\/raw.githubusercontent.com\/mariokostelac\/-setup\/master\/scripts\/publish-logs-to-cloudwatch\/on-start.sh | sudo bash -s auto-stop-idle \/home\/ec2-user\/\/auto-stop-idle.log can anyone help me out on this one?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in creating a life cycle configuration where the instance should stop every day at 9:00 pm.",
        "Question_preprocessed_content":"Title: life cycle configuration autostop; Content: i have created instances in my notebooks tab. i want to create a life cycle configuration where the instance should stop every day at pm. i have seen some examples but it is with idle time but not with the specific time can anyone help me out on this one?",
        "Answer_original_content":"change the crontab syntax to 0 21 * * * shutdown.py then create a shutdown.py which is reduced version of the autostop.py and contains mainly: ... print('closing notebook') client = boto3.client('') client.stop_notebook_instance(notebookinstancename=get_notebook_name()) btw: triggering shutdown now directly from the crontab command didn't work for me, therefore calling the api instead.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of creating a life cycle configuration where the instance should stop every day at 9:00 pm is to change the crontab syntax to 0 21 * * * shutdown.py. Then, create a shutdown.py file that contains a reduced version of the autostop.py file and mainly prints 'closing notebook' and calls the API to stop the notebook instance.",
        "Answer_preprocessed_content":"change the crontab syntax to then create a which is reduced version of the and contains mainly btw triggering directly from the crontab command didn't work for me, therefore calling the api instead."
    },
    {
        "Question_id":null,
        "Question_title":"Fill-back metrics",
        "Question_body":"<p>Say I am maintaining some data using dvc, and at some point decide I want to have a metric showing some data statistics (i.e. how many positive samples I have). Can I back-fill this metric to previous commits?  (the goal is to track the number of positives I had after each data-commit).<\/p>\n<p>If the commit history is A-&gt;B-&gt;C, I know I can go back to any commit and run a pipeline, but the metric output of this pipeline will need to be saved in a different commit, right?  So I will have to create new git commits: A-&gt;A\u2019  B-&gt;B\u2019, C-&gt;C\u2019 that will store the metric results, or is there a different way?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1594737345910,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":333.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/fill-back-metrics\/441",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-15T16:43:40.698Z",
                "Answer_body":"<p>Very interesting question <a class=\"mention\" href=\"\/u\/jonilaserson\">@jonilaserson<\/a> !<\/p>\n<p>I think it\u2019s fundamentally a Git question, as versioning is done entirely with Git. (DVC does the tracking of data via placeholders in small dvc.lock and .dvc files.)<\/p>\n<blockquote>\n<p>So I will have to create new git commits: A-&gt;A\u2019 B-&gt;B\u2019, C-&gt;C\u2019 that will store the metric results, or is there a different way?<\/p>\n<\/blockquote>\n<p>Correct. This can be achieved via <a href=\"https:\/\/git-scm.com\/book\/en\/v2\/Git-Branching-Rebasing\">rebase<\/a>, I believe. But it may imply rewriting the commit history \u2014 sometimes frowned upon.<\/p>\n<p>Please feel free to open a feature request for DVC to support this use case directly though! <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/new\/choose\">https:\/\/github.com\/iterative\/dvc\/issues\/new\/choose<\/a><\/p>",
                "Answer_score":96.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-15T18:49:59.073Z",
                "Answer_body":"<p>I actually meant to add the three branches like this:<br>\nA-&gt;A\u2019<br>\n|<br>\nv<br>\nB-&gt;B\u2019<br>\n|<br>\nv<br>\nC-&gt;C\u2019<br>\nAnd then I won\u2019t need rebase, but I will still need a way to know I should collect the metrics in these \u201cdetached\u201d commits.<\/p>",
                "Answer_score":41.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-15T19:56:21.409Z",
                "Answer_body":"<p>So C\u2019 is where you add the metrics to C, and then you cherry pick that commit separately onto B and onto A, creating your 2 extra branches.<\/p>\n<blockquote>\n<p>still need a way to know I should collect the metrics in these \u201cdetached\u201d commits<\/p>\n<\/blockquote>\n<p>If the metrics are data series (plots), you can use <code>dvc plots diff A' B' C'<\/code>. See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/plots\/diff\">https:\/\/dvc.org\/doc\/command-reference\/plots\/diff<\/a>. But this feature isn\u2019t available for plain metrics yet <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_frowning_face.png?v=9\" title=\":slightly_frowning_face:\" class=\"emoji\" alt=\":slightly_frowning_face:\"> \u2014 I just opened <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/4211\">an issue<\/a> for that.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: fill-back metrics; Content: say i am maintaining some data using , and at some point decide i want to have a metric showing some data statistics (i.e. how many positive samples i have). can i back-fill this metric to previous commits? (the goal is to track the number of positives i had after each data-commit). if the commit history is a->b->c, i know i can go back to any commit and run a pipeline, but the metric output of this pipeline will need to be saved in a different commit, right? so i will have to create new git commits: a->a\u2019 b->b\u2019, c->c\u2019 that will store the metric results, or is there a different way?",
        "Question_original_content_gpt_summary":"The user is looking for a way to back-fill metrics to previous commits in order to track the number of positive samples after each data-commit.",
        "Question_preprocessed_content":"Title: metrics; Content: say i am maintaining some data using , and at some point decide i want to have a metric showing some data statistics . can i this metric to previous commits? . if the commit history is i know i can go back to any commit and run a pipeline, but the metric output of this pipeline will need to be saved in a different commit, right? so i will have to create new git commits that will store the metric results, or is there a different way?",
        "Answer_original_content":"very interesting question @jonilaserson ! i think its fundamentally a git question, as versioning is done entirely with git. ( does the tracking of data via placeholders in small .lock and . files.) so i will have to create new git commits: a->a b->b, c->c that will store the metric results, or is there a different way? correct. this can be achieved via rebase, i believe. but it may imply rewriting the commit history sometimes frowned upon. please feel free to open a feature request for to support this use case directly though! https:\/\/github.com\/iterative\/\/issues\/new\/choose i actually meant to add the three branches like this: a->a | v b->b | v c->c and then i wont need rebase, but i will still need a way to know i should collect the metrics in these detached commits. so c is where you add the metrics to c, and then you cherry pick that commit separately onto b and onto a, creating your 2 extra branches. still need a way to know i should collect the metrics in these detached commits if the metrics are data series (plots), you can use plots diff a' b' c'. see https:\/\/.org\/doc\/command-reference\/plots\/diff. but this feature isnt available for plain metrics yet i just opened an issue for that.",
        "Answer_original_content_gpt_summary":"The answer suggests creating new git commits to store metric results, or adding three branches to the commit history. The user may need to collect metrics in these detached commits and can use plots diff if the metrics are data series. However, this feature is not available for plain metrics yet. The user is encouraged to open a feature request for direct support of this use case.",
        "Answer_preprocessed_content":"very interesting question ! i think its fundamentally a git question, as versioning is done entirely with git. so i will have to create new git commits that will store the metric results, or is there a different way? correct. this can be achieved via rebase, i believe. but it may imply rewriting the commit history sometimes frowned upon. please feel free to open a feature request for to support this use case directly though! i actually meant to add the three branches like this v v and then i wont need rebase, but i will still need a way to know i should collect the metrics in these detached commits. so c is where you add the metrics to c, and then you cherry pick that commit separately onto b and onto a, creating your extra branches. still need a way to know i should collect the metrics in these detached commits if the metrics are data series , you can use . see but this feature isnt available for plain metrics yet i just opened an issue for that."
    },
    {
        "Question_id":60045326.0,
        "Question_title":"AWS SageMaker Access Denied",
        "Question_body":"<p>I started to work with AWS SageMaker. I have an AWS Starter Account. I have been trying to deploy a built-in algorithm for 2 days but I always get <code>AccessDeniedException<\/code> despite the fact that I created IAM role according to <a href=\"https:\/\/aws.amazon.com\/tr\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/tr\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<blockquote>\n  <p>ClientError: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::161745376217:assumed-role\/AmazonSageMaker-ExecutionRole-20200203T194557\/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:161745376217:training-job\/blazingtext-2020-02-03-18-12-14-017 with an explicit deny<\/p>\n<\/blockquote>\n\n<p>Could you help me to solve this problem ?\nThank you so much<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1580755584317,
        "Question_favorite_count":0.0,
        "Question_score":1.0,
        "Question_view_count":3755.0,
        "Owner_creation_time":1481983208856,
        "Owner_last_access_time":1655816747470,
        "Owner_reputation":41.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":8.0,
        "Answer_body":"<p>You have created a role for SageMaker to access S3 bucket, but it seems your IAM user doesn't have access to SageMaker service. Please make sure your IAM user has permission to SageMaker.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1580764036812,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":1580790326547,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60045326",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: access denied; Content: i started to work with . i have an aws starter account. i have been trying to deploy a built-in algorithm for 2 days but i always get accessdeniedexception despite the fact that i created iam role according to https:\/\/aws.amazon.com\/tr\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-\/ clienterror: an error occurred (accessdeniedexception) when calling the createtrainingjob operation: user: arn:aws:sts::161745376217:assumed-role\/amazon-executionrole-20200203t194557\/ is not authorized to perform: :createtrainingjob on resource: arn:aws::us-east-1:161745376217:training-job\/blazingtext-2020-02-03-18-12-14-017 with an explicit deny could you help me to solve this problem ? thank you so much",
        "Question_original_content_gpt_summary":"The user is encountering an \"accessdeniedexception\" when attempting to deploy a built-in algorithm on AWS despite following the instructions provided.",
        "Question_preprocessed_content":"Title: access denied; Content: i started to work with . i have an aws starter account. i have been trying to deploy a algorithm for days but i always get despite the fact that i created iam role according to clienterror an error occurred when calling the createtrainingjob operation user is not authorized to perform createtrainingjob on resource with an explicit deny could you help me to solve this problem ? thank you so much",
        "Answer_original_content":"you have created a role for to access s3 bucket, but it seems your iam user doesn't have access to service. please make sure your iam user has permission to .",
        "Answer_original_content_gpt_summary":"The possible solution to the \"accessdeniedexception\" error when deploying a built-in algorithm on AWS is to ensure that the IAM user has permission to access the service. It is recommended to check if the IAM user has the necessary permissions to access the S3 bucket.",
        "Answer_preprocessed_content":"you have created a role for to access s bucket, but it seems your iam user doesn't have access to service. please make sure your iam user has permission to ."
    },
    {
        "Question_id":48990264.0,
        "Question_title":"Operationalize custom R model in Azure ML without loading it on every call of web service",
        "Question_body":"<p>I am trying to serve an R model as a web service in Azure ML.<\/p>\n\n<p>The model is trained locally and uses Xgboost and other packages. I have had issues submitting it directly from AzureML package due to size exceeding 130 MB. The workaround was to upload all the packages and the model as zip to Azure and source it from there.<\/p>\n\n<p>The current issue is that the model is loaded from a zip file by Azure ML EVERY time the service is called making the response time very slow (4.5 seconds).\nHow do I restructure the code so that the model is loaded only once from the file. Thank you for your help.<\/p>\n\n<p>Here is how it looks in AzureML <a href=\"https:\/\/i.stack.imgur.com\/I7QRT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I7QRT.png\" alt=\"enter image description here\"><\/a>\nAnd here is what is inside Execute R script\n<a href=\"https:\/\/i.stack.imgur.com\/qM1OF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qM1OF.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1519653576907,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":230.0,
        "Owner_creation_time":1363618231430,
        "Owner_last_access_time":1663601394030,
        "Owner_reputation":25.0,
        "Owner_up_votes":11.0,
        "Owner_down_votes":0.0,
        "Owner_views":10.0,
        "Answer_body":"<p>Here is a <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2016\/10\/26\/speeding-up-azure-ml-web-services-containing-r-or-python-modules\/?\" rel=\"nofollow noreferrer\">clever trick<\/a> for running initialization steps only the first time and not on every subsequent call.<\/p>\n<p>My understanding is that you would wrap the first 3 statements (that is through line 11) of your script in the following <code>if<\/code> statement:<\/p>\n<pre><code>if (!is.element(&quot;my_env&quot;, search()))\n<\/code><\/pre>\n<p>The <code>if<\/code> statement would also contain the initialization of the <code>my_env<\/code> variable as shown in the example used in that blog:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/S445n.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/S445n.png\" alt=\"R optimization example\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1519712850203,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1645949683923,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48990264",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: operationalize custom r model in without loading it on every call of web service; Content: i am trying to serve an r model as a web service in . the model is trained locally and uses xgboost and other packages. i have had issues submitting it directly from package due to size exceeding 130 mb. the workaround was to upload all the packages and the model as zip to azure and source it from there. the current issue is that the model is loaded from a zip file by every time the service is called making the response time very slow (4.5 seconds). how do i restructure the code so that the model is loaded only once from the file. thank you for your help. here is how it looks in and here is what is inside execute r script",
        "Question_original_content_gpt_summary":"The user is encountering challenges in operationalizing a custom R model in Azure without loading it on every call of the web service.",
        "Question_preprocessed_content":"Title: operationalize custom r model in without loading it on every call of web service; Content: i am trying to serve an r model as a web service in . the model is trained locally and uses xgboost and other packages. i have had issues submitting it directly from package due to size exceeding mb. the workaround was to upload all the packages and the model as zip to azure and source it from there. the current issue is that the model is loaded from a zip file by every time the service is called making the response time very slow . how do i restructure the code so that the model is loaded only once from the file. thank you for your help. here is how it looks in and here is what is inside execute r script",
        "Answer_original_content":"here is a clever trick for running initialization steps only the first time and not on every subsequent call. my understanding is that you would wrap the first 3 statements (that is through line 11) of your script in the following if statement: if (!is.element(\"my_env\", search())) the if statement would also contain the initialization of the my_env variable as shown in the example used in that blog:",
        "Answer_original_content_gpt_summary":"The answer suggests a solution to the challenge of operationalizing a custom R model in Azure without loading it on every call of the web service. The solution involves wrapping the initialization steps in an if statement that checks if the variable has already been initialized, and only runs the initialization steps if it hasn't.",
        "Answer_preprocessed_content":"here is a clever trick for running initialization steps only the first time and not on every subsequent call. my understanding is that you would wrap the first statements of your script in the following statement the statement would also contain the initialization of the variable as shown in the example used in that blog"
    },
    {
        "Question_id":null,
        "Question_title":"xAxis settings for line plot over different runs",
        "Question_body":"<p>I have developed a script to estimate some hardware evaluations for running my neural network. As parameter for this script the number of layers can be defined. This parameter is also passed to Wand.config.<br>\nThen the network trains over several epochs and evaluates the hardware. The results are all logged. At the end I would like to have a line chart with the number of layers as xaxis and for example the energy consumption on the yaxis. However, it is not possible to select the num_layers parameter in the menu.<br>\nMy guess is that line charts can only be plotted across a run and not as in my case where per value of the xaxis (num_layers) a different run specifies the value.<\/p>\n<p>Is there any way to implement my plan in Wandb in an automated way? Otherwise I would have to export the data and plot it with matplotlib etc.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1660160908832,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/xaxis-settings-for-line-plot-over-different-runs\/2894",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-15T21:06:21.541Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chwolters\">@chwolters<\/a> , can you please provide a link to your workspace for us to review. Thanks<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-16T19:31:56.771Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a>! You can have a look here: <a href=\"https:\/\/wandb.ai\/duke-tum\/example?workspace=user-chwolters\" class=\"inline-onebox\">Weights &amp; Biases<\/a><br>\nIn this workspace there are 6 runs; 3 for each learning rule which I want to compare. These two sets of runs have an increasing network depth (2, 3, 4). Now I\u2019d like to plot different metrics such as accuracy over the different network depts. So, on the x-axis there would be the depth with values 2, 3, 4; while measuring accuracy on the y-axis.<br>\nAll this should be plotted as a line chart with one line for each learning rule, i.e. comparing \u201cBP\u201d with \u201cDFA\u201d.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-24T07:32:00.510Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chwolters\">@chwolters<\/a> , this is definitely doable with <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. Specifically a chart with a custom x axis of 2,3,4, and three y fields values to referencing the runs accuracy. Alternatively yes, using Matplotlib is an option, however, with custom charts, you can save the preset for future use.<\/p>\n<p>If you decide for a custom chart, some of your Vega code may  appear as follows<\/p>\n<p>Define Multiple Y fields<\/p>\n<pre><code class=\"lang-auto\">\"transform\": [\n  {\"filter\": {\"field\": \"${field:yfield1}\", \"valid\": true}},\n  {\"filter\": {\"field\": \"${field:yfield2}\", \"valid\": true}},\n  {\"filter\": {\"field\": \"${field:yfield3}\", \"valid\": true}}\n],\n<\/code><\/pre>\n<p>Define custom x axis<\/p>\n<pre><code class=\"lang-auto\">\"x\":{\n  \"type\": \"quantitative\",\n  \"axis\": {\n    \"values\": [2, 3, 4]\n  }\n},\n<\/code><\/pre>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-29T19:55:08.679Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chwolters\">@chwolters<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-28T19:55:26.820Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: xaxis settings for line plot over different runs; Content: i have developed a script to estimate some hardware evaluations for running my neural network. as parameter for this script the number of layers can be defined. this parameter is also passed to wand.config. then the network trains over several epochs and evaluates the hardware. the results are all logged. at the end i would like to have a line chart with the number of layers as xaxis and for example the energy consumption on the yaxis. however, it is not possible to select the num_layers parameter in the menu. my guess is that line charts can only be plotted across a run and not as in my case where per value of the xaxis (num_layers) a different run specifies the value. is there any way to implement my plan in in an automated way? otherwise i would have to export the data and plot it with matplotlib etc.",
        "Question_original_content_gpt_summary":"The user is facing a challenge of plotting a line chart with the number of layers as the x-axis and the energy consumption on the y-axis, but is unable to select the num_layers parameter in the menu.",
        "Question_preprocessed_content":"Title: xaxis settings for line plot over different runs; Content: i have developed a script to estimate some hardware evaluations for running my neural network. as parameter for this script the number of layers can be defined. this parameter is also passed to then the network trains over several epochs and evaluates the hardware. the results are all logged. at the end i would like to have a line chart with the number of layers as xaxis and for example the energy consumption on the yaxis. however, it is not possible to select the parameter in the menu. my guess is that line charts can only be plotted across a run and not as in my case where per value of the xaxis a different run specifies the value. is there any way to implement my plan in in an automated way? otherwise i would have to export the data and plot it with matplotlib etc.",
        "Answer_original_content":"hi @chwolters , can you please provide a link to your workspace for us to review. thanks hi @mohammadbakir! you can have a look here: in this workspace there are 6 runs; 3 for each learning rule which i want to compare. these two sets of runs have an increasing network depth (2, 3, 4). now id like to plot different metrics such as accuracy over the different network depts. so, on the x-axis there would be the depth with values 2, 3, 4; while measuring accuracy on the y-axis. all this should be plotted as a line chart with one line for each learning rule, i.e. comparing bp with dfa. hi @chwolters , this is definitely doable with custom charts. specifically a chart with a custom x axis of 2,3,4, and three y fields values to referencing the runs accuracy. alternatively yes, using matplotlib is an option, however, with custom charts, you can save the preset for future use. if you decide for a custom chart, some of your vega code may appear as follows define multiple y fields \"transform\": [ {\"filter\": {\"field\": \"${field:yfield1}\", \"valid\": true}}, {\"filter\": {\"field\": \"${field:yfield2}\", \"valid\": true}}, {\"filter\": {\"field\": \"${field:yfield3}\", \"valid\": true}} ], define custom x axis \"x\":{ \"type\": \"quantitative\", \"axis\": { \"values\": [2, 3, 4] } }, hi @chwolters , since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's challenge of plotting a line chart with the number of layers as the x-axis and the energy consumption on the y-axis are: using custom charts with a custom x-axis of 2, 3, 4 and three y fields values to reference the runs' accuracy, or using matplotlib. Custom charts can save the preset for future use.",
        "Answer_preprocessed_content":"hi , can you please provide a link to your workspace for us to review. thanks hi you can have a look here in this workspace there are runs; for each learning rule which i want to compare. these two sets of runs have an increasing network depth . now id like to plot different metrics such as accuracy over the different network depts. so, on the there would be the depth with values , , ; while measuring accuracy on the all this should be plotted as a line chart with one line for each learning rule, comparing bp with dfa. hi , this is definitely doable with custom charts. specifically a chart with a custom x axis of , , , and three y fields values to referencing the runs accuracy. alternatively yes, using matplotlib is an option, however, with custom charts, you can save the preset for future use. if you decide for a custom chart, some of your vega code may appear as follows define multiple y fields define custom x axis hi , since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":37183494.0,
        "Question_title":"Azure machine learning predict order for customer",
        "Question_body":"<p>I have created a new experiment in Azure Machine Learning and added two datasets by manually uploading csv's.<\/p>\n\n<ul>\n<li>One is from a customer of which I'd like to predict which products he will order next. <\/li>\n<li>The second dataset has the same type of data, only then from all other customers as reference for learning.<\/li>\n<\/ul>\n\n<p>I have <code>productid<\/code>, <code>amount<\/code>, and <code>orderdate<\/code> and <code>orderid<\/code> for grouping and putting it on a timeframe.\nThe customer (dataset one) is always several months behind with ordering the latest products. therefor I added the dataset two with all other customers as reference.<\/p>\n\n<p>Also because the reference can tell which products are more popular (ordered more and by several customers) so perhaps I should add a customerid column to the dataset.<\/p>\n\n<p>I know how to start and get the data in, and I do know that it is common to split the data for training, feed it to the train model with a <code>Ilearnerdotnet<\/code> type and give the output to the score model and evaluate the model.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qH7lb.png\" alt=\"current experiment\"><\/a>\nI do not know how to choose a classification type and how this can give an output for the next three months of order. I have read some tutorials, but I just need someone who can give me some pointers.<\/p>\n\n<p><strong>edit<\/strong> I have added the customerid to the dataset so that I have just one set now which I should split to focus on a specific customer.\n<strong>edit2<\/strong> found these templates. will look into it <a href=\"https:\/\/stackoverflow.com\/a\/36552849\/169714\">https:\/\/stackoverflow.com\/a\/36552849\/169714<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1463047086927,
        "Question_favorite_count":1.0,
        "Question_score":2.0,
        "Question_view_count":845.0,
        "Owner_creation_time":1252330528847,
        "Owner_last_access_time":1643632792116,
        "Owner_reputation":5867.0,
        "Owner_up_votes":2127.0,
        "Owner_down_votes":9.0,
        "Owner_views":1692.0,
        "Answer_body":"<p>Go over this <a href=\"http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf\" rel=\"nofollow\">http:\/\/download.microsoft.com\/download\/0\/5\/A\/05AE6B94-E688-403E-90A5-6035DBE9EEC5\/machine-learning-basics-infographic-with-algorithm-examples.pdf<\/a><\/p>\n\n<p>If above infographic doesn't help, then you can try all of the learners by going over this experiment and use the one with best results - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Algo-Evaluater-Compare-Performance-of-Multiple-Algos-against-Your-Data-1<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1463090186116,
        "Answer_score":3.0,
        "Owner_location":"Netherlands",
        "Question_last_edit_time":1495541203567,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37183494",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: predict order for customer; Content: i have created a new experiment in and added two datasets by manually uploading csv's. one is from a customer of which i'd like to predict which products he will order next. the second dataset has the same type of data, only then from all other customers as reference for learning. i have productid, amount, and orderdate and orderid for grouping and putting it on a timeframe. the customer (dataset one) is always several months behind with ordering the latest products. therefor i added the dataset two with all other customers as reference. also because the reference can tell which products are more popular (ordered more and by several customers) so perhaps i should add a customerid column to the dataset. i know how to start and get the data in, and i do know that it is common to split the data for training, feed it to the train model with a ilearnerdotnet type and give the output to the score model and evaluate the model. i do not know how to choose a classification type and how this can give an output for the next three months of order. i have read some tutorials, but i just need someone who can give me some pointers. edit i have added the customerid to the dataset so that i have just one set now which i should split to focus on a specific customer. edit2 found these templates. will look into it https:\/\/stackoverflow.com\/a\/36552849\/169714",
        "Question_original_content_gpt_summary":"The user is facing challenges in predicting which products a customer will order next, using two datasets with productid, amount, orderdate, and orderid, and is unsure of how to choose a classification type and how to get an output for the next three months of orders.",
        "Question_preprocessed_content":"Title: predict order for customer; Content: i have created a new experiment in and added two datasets by manually uploading csv's. one is from a customer of which i'd like to predict which products he will order next. the second dataset has the same type of data, only then from all other customers as reference for learning. i have , , and and for grouping and putting it on a timeframe. the customer is always several months behind with ordering the latest products. therefor i added the dataset two with all other customers as reference. also because the reference can tell which products are more popular so perhaps i should add a customerid column to the dataset. i know how to start and get the data in, and i do know that it is common to split the data for training, feed it to the train model with a type and give the output to the score model and evaluate the model. i do not know how to choose a classification type and how this can give an output for the next three months of order. i have read some tutorials, but i just need someone who can give me some pointers. edit i have added the customerid to the dataset so that i have just one set now which i should split to focus on a specific customer. edit found these templates. will look into it",
        "Answer_original_content":"go over this http:\/\/download.microsoft.com\/download\/0\/5\/a\/05ae6b94-e688-403e-90a5-6035dbe9eec5\/machine-learning-basics-infographic-with-algorithm-examples.pdf if above infographic doesn't help, then you can try all of the learners by going over this experiment and use the one with best results - https:\/\/gallery.cortanaintelligence.com\/experiment\/algo-evaluater-compare-performance-of-multiple-algos-against-your-data-1",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. Check out the Microsoft machine learning basics infographic with algorithm examples.\n2. Try all of the learners by going over the experiment provided in the answer and use the one with the best results.",
        "Answer_preprocessed_content":"go over this if above infographic doesn't help, then you can try all of the learners by going over this experiment and use the one with best results"
    },
    {
        "Question_id":null,
        "Question_title":"Deleting data from self-hosted server",
        "Question_body":"<p>I saw <a href=\"https:\/\/community.wandb.ai\/t\/delete-files-from-a-run\/1031\">this<\/a> post, but it doesn\u2019t answer my question. We are running a self-hosted wandb instance. We have somewhat limited space, though we\u2019ve been good about deleting old runs through the interface.<\/p>\n<p>We thought deleting from the interface would also delete the physical files from the hard disk, but that doesn\u2019t appear to be the case. For example, going into the minio folder on the server shows a particular project taking up 130 GB of space, while wandb reports 30 GB. That\u2019s a big difference!<\/p>\n<p>How do we really really delete files from minio that wandb no longer knows anything about?<\/p>\n<p>Suggestion: please have a central <code>\/usage\/<\/code> URL for admins to look through usage from all teams and users rather than having to go through each one individually!<\/p>",
        "Question_answer_count":13,
        "Question_comment_count":null,
        "Question_creation_time":1655303395067,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":1072.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/deleting-data-from-self-hosted-server\/2613",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-17T18:27:10.321Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/tkott\">@tkott<\/a>, I\u2019m looking into this but I believe there isn\u2019t a way through the UI to physically delete the files from your hard drive. I\u2019ll look into this more to confirm this and if this is indeed the case, I can put in a feature request for you to make this possible through the UI.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":22.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-17T18:43:33.000Z",
                "Answer_body":"<p>Hi Nate \u2013 so how do you suggest that we free up space generally then? Do we have to note the hash \/ digest of files and look for them on the minio server by hand?<\/p>\n<p>I would expect that if you \u201cdelete\u201d from the server with a big scary \u201cthis is a permanent operation\u201d type warning, that the server will, in fact, remove the files. If it doesn\u2019t remove the files, it isn\u2019t actually a permanent operation since the files can be salvaged (albeit with some work).<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c068a531a4cb060559186c287c0508c8c7321469.jpeg\" alt=\"image001.jpg\" data-base62-sha1=\"rs7SiFek7tnQtx4wt2yshERms0p\" width=\"79\" height=\"79\"><\/p>",
                "Answer_score":27.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-22T15:10:19.015Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/nathank\">@nathank<\/a> any suggestions for how do it programmatically through wandb in the meantime?<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-12T20:09:02.108Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tkott\">@tkott<\/a>,<br>\nSorry for the delay here.  A member of our team put together a script to do this for you <a href=\"https:\/\/gist.github.com\/venky-wandb\/b95542f107b377fb9e1ad2c811201bbe\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>\n<p>You can use this by running <code>python file_cleanup.py -d 10<\/code> where the <code>-d<\/code> tag can specify how many days ago a run has to have been deleted from the UI in order for the script to delete it from Minio storage.<\/p>\n<p>Please note that this will only work if you are using our bare-metal Docker container setup. If you have connected an external database this will not work.<\/p>\n<p>Also, we are working towards making this a default part of our local deployment where you can set a retention policy for deleted runs and the server will automatically clean up deleted runs after a certain amount of time .<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":16.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-12T20:57:53.000Z",
                "Answer_body":"<p>Thanks! If I understand that gist right, I first need to use the UI to delete the run. At that point, the run files are still on the server. At that point, I can run this script (or maybe set to run weekly). When the script finds run that were deleted more than 10 days ago (via <code>-d 10<\/code> option), it will look for their references and delete files that live in the parent folder found ( <code> \"\/vol\/minio\/local-files\/{}\/{}\/{}\".format(entity_name, project_name, run_id)<\/code>).<\/p>\n<p>So a couple of questions:<\/p>\n<ol>\n<li>\n<p>What happens to the artifacts associated with runs?<\/p>\n<\/li>\n<li>\n<p>What happens with child folders? (And why isn\u2019t it all files and folders within the parent folder?)<\/p>\n<\/li>\n<li>\n<p>If child directories are present, are they now orphaned with no pointer from the database?<\/p>\n<\/li>\n<\/ol>\n<p>Thanks!<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c068a531a4cb060559186c287c0508c8c7321469.jpeg\" alt=\"image001.jpg\" data-base62-sha1=\"rs7SiFek7tnQtx4wt2yshERms0p\" width=\"79\" height=\"79\"><\/p>",
                "Answer_score":21.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-21T18:12:48.826Z",
                "Answer_body":"<ol>\n<li>The artifacts are not deleted, if there is an artifacts directory in the object storage we still keep it and delete all the rest of the files.<\/li>\n<li>All child folders are also deleted in the object storage. We don\u2019t delete Artifacts folder because artifacts are not just used by runs but also by other parts of the product so we keep them to avoid breaking things in other parts of the app.<\/li>\n<li>There is no connection with the runs table in the database once the deletion happens but there might be other tables that are still referring to these artifacts.<\/li>\n<\/ol>\n<p>This can be run from outside the container or inside the container. But in either case it expects to have the <code>mysql-connector-python<\/code> python package installed on the container. You can login to the container using <code>docker exec -it wandb-local bash<\/code> and run pip install <a href=\"https:\/\/gist.github.com\/venky-wandb\/b95542f107b377fb9e1ad2c811201bbe#file-file_cleanup-py-L2\">like this<\/a> to install the dependency.<\/p>",
                "Answer_score":26.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-21T18:37:24.000Z",
                "Answer_body":"<p>Thanks for confirming those questions!<\/p>\n<p>Can you also then recommend a way of deleting older versions of artifacts? (which you can do from the UI as well, but presumably this doesn\u2019t affect the underlying store in the same way that deleting runs doesn\u2019t remove them from the store.)<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c068a531a4cb060559186c287c0508c8c7321469.jpeg\" alt=\"image001.jpg\" data-base62-sha1=\"rs7SiFek7tnQtx4wt2yshERms0p\" width=\"79\" height=\"79\"><\/p>",
                "Answer_score":31.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-04T14:36:12.617Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/a-sh0ts\">@a-sh0ts<\/a>  or <a class=\"mention\" href=\"\/u\/nathank\">@nathank<\/a> \u2013 any suggestions on dealing with artifacts in a similar way?<\/p>",
                "Answer_score":31.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-12T18:00:13.295Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/tkott\">@tkott<\/a> , apologies for the delay here.<br>\nYou can delete artifact versions programmatically via our <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/artifact#delete\">public API<\/a>. Here\u2019s a sample script doing the same:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nproject = api.project('project_name')\n\nfor artifact_type in project.artifacts_types():\n    for artifact_collection in artifact_type.collections():        \n        for version in artifact_collection.versions():\n            if artifact_type.type == 'dataset':\n                if len(version.aliases) &gt; 0:\n                    # print out the name of the one we are keeping\n                    print(f'KEEPING {version.name}')\n                else:\n                    print(f'DELETING {version.name}')\n                    if not dry_run:\n                        print('')\n                        version.delete()\n<\/code><\/pre>\n<p>Please let me if this helps.<\/p>",
                "Answer_score":51.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-12T18:18:15.511Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/anmolmann\">@anmolmann<\/a> thanks and I think that\u2019s fine for deleting it from the UI, but my understanding is that this would not delete it from the actual physical drive through the minio interface. How do I do <em>that<\/em>?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-06T17:33:10.485Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/tkott\">@tkott<\/a> , we identified this as a bug where deleting the artifact versions via API or UI wouldn\u2019t actually delete them from the object storage as well. Our team is working on a fox for this and I\u2019ll keep you posted as soon as i\u2019ve an update on this issue.<br>\nMeanwhile, you can delete these artifact versions via a script similar <a href=\"https:\/\/gist.github.com\/venky-wandb\/b95542f107b377fb9e1ad2c811201bbe\" rel=\"noopener nofollow ugc\">to this one<\/a>. You should remove lines 44 and 45, and update the <code>elif<\/code> in line 41 to <code>elif os.path.isdir(f)<\/code> so that if the folder contains any artifacts then that folder is deleted as well. Also, you should add one more dir path, maybe have a list of dir_paths in line 31 as artifacts are also stored in <code>wandb_artifacts<\/code> in <code>local-files<\/code>. So, your additional dir_path would be <code>dir_path_2 = \"\/vol\/minio\/local-files\/wandb_artifacts\/{}\/{}\".format( idx_1, idx_2)<\/code>, where idx_1 is the artifact index and idx_2 is the artifact version index.<br>\nApologies for the inconvenience caused here as I do acknowledge this workaround would be a hacky way to get around this issue.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e.png\" data-download-href=\"\/uploads\/short-url\/2cdzanw1auqQjNmN2MsHLYoMSUm.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e_2_690x365.png\" alt=\"image\" data-base62-sha1=\"2cdzanw1auqQjNmN2MsHLYoMSUm\" width=\"690\" height=\"365\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e_2_690x365.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e_2_1035x547.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e.png 2x\" data-dominant-color=\"F3F3F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1174\u00d7622 35.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-06T18:12:51.000Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/anmolmann\">@anmolmann<\/a> Thanks for the suggestion. I\u2019m a little confused because while we\u2019ve used the gist you point out to remove old runs (yay!), if I understand it the artifacts are a separate thing. I don\u2019t know which run to delete that would also (when the changes to the gist are made as you suggest) delete a specific artifact version, and only that specific artifact version. I\u2019m also not sure that we necessarily want to delete the run at the same time as the artifact. Can you elaborate a bit more about the relationship between artifacts and runs and how the gist would handle that?<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/5\/52d5f1c08649212fe6768d2cacdcf8df711810d9.jpeg\" alt=\"~WRD000.jpg\" data-base62-sha1=\"bONwCMToQaefS5MCaw6ckHjvG5z\" width=\"100\" height=\"100\"><\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c068a531a4cb060559186c287c0508c8c7321469.jpeg\" alt=\"image001.jpg\" data-base62-sha1=\"rs7SiFek7tnQtx4wt2yshERms0p\" width=\"79\" height=\"79\"><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-05T18:13:10.590Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deleting data from self-hosted server; Content: i saw this post, but it doesn\u2019t answer my question. we are running a self-hosted instance. we have somewhat limited space, though we\u2019ve been good about deleting old runs through the interface. we thought deleting from the interface would also delete the physical files from the hard disk, but that doesn\u2019t appear to be the case. for example, going into the minio folder on the server shows a particular project taking up 130 gb of space, while reports 30 gb. that\u2019s a big difference! how do we really really delete files from minio that no longer knows anything about? suggestion: please have a central \/usage\/ url for admins to look through usage from all teams and users rather than having to go through each one individually!",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with deleting data from their self-hosted server, as the physical files are not being deleted when they are removed from the interface.",
        "Question_preprocessed_content":"Title: deleting data from server; Content: i saw this post, but it doesnt answer my question. we are running a instance. we have somewhat limited space, though weve been good about deleting old runs through the interface. we thought deleting from the interface would also delete the physical files from the hard disk, but that doesnt appear to be the case. for example, going into the minio folder on the server shows a particular project taking up gb of space, while reports gb. thats a big difference! how do we really really delete files from minio that no longer knows anything about? suggestion please have a central url for admins to look through usage from all teams and users rather than having to go through each one individually!",
        "Answer_original_content":"@tkott, im looking into this but i believe there isnt a way through the ui to physically delete the files from your hard drive. ill look into this more to confirm this and if this is indeed the case, i can put in a feature request for you to make this possible through the ui. thank you, nate hi nate so how do you suggest that we free up space generally then? do we have to note the hash \/ digest of files and look for them on the minio server by hand? i would expect that if you delete from the server with a big scary this is a permanent operation type warning, that the server will, in fact, remove the files. if it doesnt remove the files, it isnt actually a permanent operation since the files can be salvaged (albeit with some work). @nathank any suggestions for how do it programmatically through in the meantime? hi @tkott, sorry for the delay here. a member of our team put together a script to do this for you here. you can use this by running python file_cleanup.py -d 10 where the -d tag can specify how many days ago a run has to have been deleted from the ui in order for the script to delete it from minio storage. please note that this will only work if you are using our bare-metal docker container setup. if you have connected an external database this will not work. also, we are working towards making this a default part of our local deployment where you can set a retention policy for deleted runs and the server will automatically clean up deleted runs after a certain amount of time . thank you, nate thanks! if i understand that gist right, i first need to use the ui to delete the run. at that point, the run files are still on the server. at that point, i can run this script (or maybe set to run weekly). when the script finds run that were deleted more than 10 days ago (via -d 10 option), it will look for their references and delete files that live in the parent folder found ( \"\/vol\/minio\/local-files\/{}\/{}\/{}\".format(entity_name, project_name, run_id)). so a couple of questions: what happens to the artifacts associated with runs? what happens with child folders? (and why isnt it all files and folders within the parent folder?) if child directories are present, are they now orphaned with no pointer from the database? thanks! the artifacts are not deleted, if there is an artifacts directory in the object storage we still keep it and delete all the rest of the files. all child folders are also deleted in the object storage. we dont delete artifacts folder because artifacts are not just used by runs but also by other parts of the product so we keep them to avoid breaking things in other parts of the app. there is no connection with the runs table in the database once the deletion happens but there might be other tables that are still referring to these artifacts. this can be run from outside the container or inside the container. but in either case it expects to have the mysql-connector-python python package installed on the container. you can login to the container using docker exec -it -local bash and run pip install like this to install the dependency. thanks for confirming those questions! can you also then recommend a way of deleting older versions of artifacts? (which you can do from the ui as well, but presumably this doesnt affect the underlying store in the same way that deleting runs doesnt remove them from the store.) @a-sh0ts or @nathank any suggestions on dealing with artifacts in a similar way? hey @tkott , apologies for the delay here. you can delete artifact versions programmatically via our public api. heres a sample script doing the same: api = .api() project = api.project('project_name') for artifact_type in project.artifacts_types(): for artifact_collection in artifact_type.collections(): for version in artifact_collection.versions(): if artifact_type.type == 'dataset': if len(version.aliases) > 0: # print out the name of the one we are keeping print(f'keeping {version.name}') else: print(f'deleting {version.name}') if not dry_run: print('') version.delete() please let me if this helps. @anmolmann thanks and i think thats fine for deleting it from the ui, but my understanding is that this would not delete it from the actual physical drive through the minio interface. how do i do that? @tkott , we identified this as a bug where deleting the artifact versions via api or ui wouldnt actually delete them from the object storage as well. our team is working on a fox for this and ill keep you posted as soon as ive an update on this issue. meanwhile, you can delete these artifact versions via a script similar to this one. you should remove lines 44 and 45, and update the elif in line 41 to elif os.path.isdir(f) so that if the folder contains any artifacts then that folder is deleted as well. also, you should add one more dir path, maybe have a list of dir_paths in line 31 as artifacts are also stored in _artifacts in local-files. so, your additional dir_path would be dir_path_2 = \"\/vol\/minio\/local-files\/_artifacts\/{}\/{}\".format( idx_1, idx_2), where idx_1 is the artifact index and idx_2 is the artifact version index. apologies for the inconvenience caused here as i do acknowledge this workaround would be a hacky way to get around this issue. image1174622 35.5 kb @anmolmann thanks for the suggestion. im a little confused because while weve used the gist you point out to remove old runs (yay!), if i understand it the artifacts are a separate thing. i dont know which run to delete that would also (when the changes to the gist are made as you suggest) delete a specific artifact version, and only that specific artifact version. im also not sure that we necessarily want to delete the run at the same time as the artifact. can you elaborate a bit more about the relationship between artifacts and runs and how the gist would handle that? this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The user is having trouble deleting data from their self-hosted server. The answer suggests that there is no way to physically delete files from the hard drive through the UI, but a feature request can be made for this. In the meantime, a script can be used to delete files programmatically. The script can be run from outside or inside the container and requires the mysql-connector-python package to be installed. Artifacts associated with runs are not deleted, but child folders are deleted. To delete older versions of artifacts, a script can be used via the public API. However, deleting artifact versions via API or UI does not delete them from the object storage, and a workaround is suggested to delete them via a script.",
        "Answer_preprocessed_content":"im looking into this but i believe there isnt a way through the ui to physically delete the files from your hard drive. ill look into this more to confirm this and if this is indeed the case, i can put in a feature request for you to make this possible through the ui. thank you, nate hi nate so how do you suggest that we free up space generally then? do we have to note the hash \/ digest of files and look for them on the minio server by hand? i would expect that if you delete from the server with a big scary this is a permanent operation type warning, that the server will, in fact, remove the files. if it doesnt remove the files, it isnt actually a permanent operation since the files can be salvaged . any suggestions for how do it programmatically through in the meantime? hi sorry for the delay here. a member of our team put together a script to do this for you here. you can use this by running where the tag can specify how many days ago a run has to have been deleted from the ui in order for the script to delete it from minio storage. please note that this will only work if you are using our docker container setup. if you have connected an external database this will not work. also, we are working towards making this a default part of our local deployment where you can set a retention policy for deleted runs and the server will automatically clean up deleted runs after a certain amount of time . thank you, nate thanks! if i understand that gist right, i first need to use the ui to delete the run. at that point, the run files are still on the server. at that point, i can run this script . when the script finds run that were deleted more than days ago , it will look for their references and delete files that live in the parent folder found . so a couple of questions what happens to the artifacts associated with runs? what happens with child folders? if child directories are present, are they now orphaned with no pointer from the database? thanks! the artifacts are not deleted, if there is an artifacts directory in the object storage we still keep it and delete all the rest of the files. all child folders are also deleted in the object storage. we dont delete artifacts folder because artifacts are not just used by runs but also by other parts of the product so we keep them to avoid breaking things in other parts of the app. there is no connection with the runs table in the database once the deletion happens but there might be other tables that are still referring to these artifacts. this can be run from outside the container or inside the container. but in either case it expects to have the python package installed on the container. you can login to the container using and run pip install like this to install the dependency. thanks for confirming those questions! can you also then recommend a way of deleting older versions of artifacts? or any suggestions on dealing with artifacts in a similar way? hey , apologies for the delay here. you can delete artifact versions programmatically via our public api. heres a sample script doing the same please let me if this helps. thanks and i think thats fine for deleting it from the ui, but my understanding is that this would not delete it from the actual physical drive through the minio interface. how do i do that? , we identified this as a bug where deleting the artifact versions via api or ui wouldnt actually delete them from the object storage as well. our team is working on a fox for this and ill keep you posted as soon as ive an update on this issue. meanwhile, you can delete these artifact versions via a script similar to this one. you should remove lines and , and update the in line to so that if the folder contains any artifacts then that folder is deleted as well. also, you should add one more dir path, maybe have a list of in line as artifacts are also stored in in . so, your additional would be , where is the artifact index and is the artifact version index. apologies for the inconvenience caused here as i do acknowledge this workaround would be a hacky way to get around this issue. image kb thanks for the suggestion. im a little confused because while weve used the gist you point out to remove old runs , if i understand it the artifacts are a separate thing. i dont know which run to delete that would also delete a specific artifact version, and only that specific artifact version. im also not sure that we necessarily want to delete the run at the same time as the artifact. can you elaborate a bit more about the relationship between artifacts and runs and how the gist would handle that? this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"How to keep only last checkpoint artifact?",
        "Question_body":"<p>How do I keep only the last checkpoint artifact in wandb?<\/p>\n<p>I am using lightning\u2019s ModelCheckpoint to periodically save my checkpoint artifact to wandb. However, these artifacts are really large. If I keep multiple checkpoint artifact versions on wandb, they get big really quickly.<\/p>\n<p>However, I can\u2019t just checkpoint at the end of training. My GPUs occasionally terminate, so I need to checkpoint periodically.<\/p>\n<p>How do I make sure that only the last checkpoint artifact is kept on wandb?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_creation_time":1661589342720,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":472.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-keep-only-last-checkpoint-artifact\/3014",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-28T12:52:44.241Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/turian\">@turian<\/a>,<br>\nYou need to define a custom checkpoint callback which is straightforward:<\/p>\n<pre><code class=\"lang-auto\">from pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# define WANDB logger\nwandb_logger = WandbLogger(log_model=\"all\")\n\n# define pytorch lightning checkpoint callback\ncheckpoint_callback = ModelCheckpoint(every_n_epochs=1)\n\n# define trainer\ntrainer = Trainer(logger=wandb_logger, callbacks=[checkpoint_callback])\n<\/code><\/pre>\n<p>In this example, the checkpoint will be saved at the end of each epoch, but you can set whatever value you want. And if you want to save the checkpoints based on steps or time, you just need to set <code>every_n_train_steps<\/code> or <code>train_time_interval<\/code>, respectively.<\/p>\n<p>If you\u2019re looking for more specific information, I highly recommend you to check out the official docs:<\/p>\n<ul>\n<li><a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/extensions\/generated\/pytorch_lightning.loggers.WandbLogger.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">WandbLogger \u2014 PyTorch Lightning 1.7.3 documentation<\/a><\/li>\n<li><a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/api\/pytorch_lightning.callbacks.ModelCheckpoint.html#pytorch_lightning.callbacks.ModelCheckpoint\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">ModelCheckpoint \u2014 PyTorch Lightning 1.7.3 documentation<\/a><\/li>\n<\/ul>\n<p>Hope it does help you.<\/p>",
                "Answer_score":61.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-29T09:17:06.588Z",
                "Answer_body":"<p>Also, if you want to delete artifacts after training, you can use the <code>wandb.Api<\/code>.<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\n\"\"\"\ndeletes all models that do not have a tag attached\n\nby default this means wandb will delete all but the \"latest\" or \"best\" models\n\nset dry_run == False to delete...\n\"\"\"\nproject_name='demo-project'\nentity='_scott'\ndry_run = True\napi = wandb.Api(overrides={\"project\": project_name, \"entity\": entity})\nproject = api.project(project_name)\nfor artifact_type in project.artifacts_types():\n    for artifact_collection in artifact_type.collections():\n        for version in api.artifact_versions(artifact_type.type, artifact_collection.name):\n            if artifact_type.type == 'model':\n                if len(version.aliases) &gt; 0:\n                    # print out the name of the one we are keeping\n                    print(f'KEEPING {version.name}')\n                else:\n                    print(f'DELETING {version.name}')\n                    if not dry_run:\n                        version.delete()\n<\/code><\/pre>\n<p>Source for this snippet:<\/p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"1498\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"><\/div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/d\/d2c977\/40.png\" class=\"avatar\">\n    <a href=\"https:\/\/community.wandb.ai\/t\/using-the-python-api-to-delete-models-with-no-tag-minimal\/1498\">Using the python API to delete models with no tag (minimal)<\/a> <a class=\"badge-wrapper  bullet\" href=\"\/c\/show-the-community\/43\"><span class=\"badge-category-bg\" style=\"background-color: #92278F;\"><\/span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"Share what you\u2019ve built on top of W&amp;B! This is the place to share your latest open source project, blog post\/public W&amp;B Report, research paper, wandb code snippet, tutorial or any other project that uses W&amp;B.\">Show the Community!<\/span><\/a>\n  <\/div>\n  <blockquote>\n    Hey all, \nHere\u2019s a minimal example of how to delete models that have no tag. \nThis is useful when you blow your data limit by saving too many intermediate checkpoints during training. \nIf you improve this script, post your improvements to this thread for the benefit of all. \nPeace \nDuane \nimport wandb\n\n\"\"\"\ndeletes all models that do not have a tag attached\n\nby default this means wandb will delete all but the \"latest\" or \"best\" models\n\nset dry_run == False to delete...\n\"\"\"\n\ndry_run = True\napi = w\u2026\n  <\/blockquote>\n<\/aside>\n",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-31T09:15:10.969Z",
                "Answer_body":"<p>Hi Joseph, thanks for your question! Would the solutions proposed by Matteo and Scott work for you?<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-06T10:27:00.716Z",
                "Answer_body":"<p>Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-10T09:12:07.264Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/_scott\">@_scott<\/a> thanks for the code. As I mentioned <a href=\"https:\/\/community.wandb.ai\/t\/using-the-python-api-to-delete-models-with-no-tag-minimal\/1498\">here<\/a>, this doesn\u2019t appear to delete the artifacts any more, even with dry run disabled.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-09T09:12:59.790Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to keep only last checkpoint artifact?; Content: how do i keep only the last checkpoint artifact in ? i am using lightning\u2019s modelcheckpoint to periodically save my checkpoint artifact to . however, these artifacts are really large. if i keep multiple checkpoint artifact versions on , they get big really quickly. however, i can\u2019t just checkpoint at the end of training. my gpus occasionally terminate, so i need to checkpoint periodically. how do i make sure that only the last checkpoint artifact is kept on ?",
        "Question_original_content_gpt_summary":"The user is looking for a way to keep only the last checkpoint artifact on their system while using Lightning's ModelCheckpoint to periodically save their checkpoint artifact, as their GPUs occasionally terminate and they need to checkpoint periodically.",
        "Question_preprocessed_content":"Title: how to keep only last checkpoint artifact?; Content: how do i keep only the last checkpoint artifact in ? i am using lightnings modelcheckpoint to periodically save my checkpoint artifact to . however, these artifacts are really large. if i keep multiple checkpoint artifact versions on , they get big really quickly. however, i cant just checkpoint at the end of training. my gpus occasionally terminate, so i need to checkpoint periodically. how do i make sure that only the last checkpoint artifact is kept on ?",
        "Answer_original_content":"hey @turian, you need to define a custom checkpoint callback which is straightforward: from pytorch_lightning.loggers import logger from pytorch_lightning.callbacks import modelcheckpoint # define logger _logger = logger(log_model=\"all\") # define pytorch lightning checkpoint callback checkpoint_callback = modelcheckpoint(every_n_epochs=1) # define trainer trainer = trainer(logger=_logger, callbacks=[checkpoint_callback]) in this example, the checkpoint will be saved at the end of each epoch, but you can set whatever value you want. and if you want to save the checkpoints based on steps or time, you just need to set every_n_train_steps or train_time_interval, respectively. if youre looking for more specific information, i highly recommend you to check out the official docs: logger pytorch lightning 1.7.3 documentation modelcheckpoint pytorch lightning 1.7.3 documentation hope it does help you. also, if you want to delete artifacts after training, you can use the .api. import \"\"\" deletes all models that do not have a tag attached by default this means will delete all but the \"latest\" or \"best\" models set dry_run == false to delete... \"\"\" project_name='demo-project' entity='_scott' dry_run = true api = .api(overrides={\"project\": project_name, \"entity\": entity}) project = api.project(project_name) for artifact_type in project.artifacts_types(): for artifact_collection in artifact_type.collections(): for version in api.artifact_versions(artifact_type.type, artifact_collection.name): if artifact_type.type == 'model': if len(version.aliases) > 0: # print out the name of the one we are keeping print(f'keeping {version.name}') else: print(f'deleting {version.name}') if not dry_run: version.delete() source for this snippet: using the python api to delete models with no tag (minimal) show the community! hey all, heres a minimal example of how to delete models that have no tag. this is useful when you blow your data limit by saving too many intermediate checkpoints during training. if you improve this script, post your improvements to this thread for the benefit of all. peace duane import \"\"\" deletes all models that do not have a tag attached by default this means will delete all but the \"latest\" or \"best\" models set dry_run == false to delete... \"\"\" dry_run = true api = w hi joseph, thanks for your question! would the solutions proposed by matteo and scott work for you? hi there, i wanted to follow up on this request. please let us know if we can be of further assistance or if your issue has been resolved. hi @_scott thanks for the code. as i mentioned here, this doesnt appear to delete the artifacts any more, even with dry run disabled. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The answer suggests defining a custom checkpoint callback using PyTorch Lightning's ModelCheckpoint to periodically save checkpoint artifacts. The checkpoint can be saved at the end of each epoch, or based on steps or time. To delete artifacts after training, the answer provides a code snippet that uses the Python API to delete models with no tag. However, the user mentions that this code snippet may not work anymore.",
        "Answer_preprocessed_content":"hey you need to define a custom checkpoint callback which is straightforward in this example, the checkpoint will be saved at the end of each epoch, but you can set whatever value you want. and if you want to save the checkpoints based on steps or time, you just need to set or , respectively. if youre looking for more specific information, i highly recommend you to check out the official docs logger pytorch lightning documentation modelcheckpoint pytorch lightning documentation hope it does help you. also, if you want to delete artifacts after training, you can use the . source for this snippet using the python api to delete models with no tag show the community! hey all, heres a minimal example of how to delete models that have no tag. this is useful when you blow your data limit by saving too many intermediate checkpoints during training. if you improve this script, post your improvements to this thread for the benefit of all. peace duane import deletes all models that do not have a tag attached by default this means will delete all but the latest or best models set false to true api w hi joseph, thanks for your question! would the solutions proposed by matteo and scott work for you? hi there, i wanted to follow up on this request. please let us know if we can be of further assistance or if your issue has been resolved. hi thanks for the code. as i mentioned here, this doesnt appear to delete the artifacts any more, even with dry run disabled. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":60434642.0,
        "Question_title":"Version control of azure machine learning workspace notebooks",
        "Question_body":"<p>I'm trying to work with the capacities of the new Azure ML Workspace and I can't find any option to track my notebooks on git. <\/p>\n\n<p>It's this possible as well as you can do with Azure notebooks? If not is possible... how it's suposed to work with this notebooks? Only inside this workspace? <\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3.0,
        "Question_creation_time":1582811011920,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":344.0,
        "Owner_creation_time":1348126905516,
        "Owner_last_access_time":1663918528572,
        "Owner_reputation":327.0,
        "Owner_up_votes":50.0,
        "Owner_down_votes":0.0,
        "Owner_views":27.0,
        "Answer_body":"<p>AFAIK, Git isn't currently supported by Azure Machine Learning Notebooks. If you're looking for a more fully-featured development environment, I suggest setting one up locally. There's more work up front, but it will give you the ability to version control. Check out this development environment set-up guide. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment<\/a><\/p>\n\n<pre><code>| Environment                                                   | Pros                                                                                                                                                                                                                                    | Cons                                                                                                                                                                                 |\n|---------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud-based Azure Machine Learning compute instance (preview) | Easiest way to get started. The entire SDK is already installed in your workspace VM, and notebook tutorials are pre-cloned and ready to run.                                                                                           | Lack of control over your development environment and dependencies. Additional cost incurred for Linux VM (VM can be stopped when not in use to avoid charges). See pricing details. |\n| Local environment                                             | Full control of your development environment and dependencies. Run with any build tool, environment, or IDE of your choice.                                                                                                             | Takes longer to get started. Necessary SDK packages must be installed, and an environment must also be installed if you don't already have one.                                      |\n| Azure Databricks                                              | Ideal for running large-scale intensive machine learning workflows on the scalable Apache Spark platform.                                                                                                                               | Overkill for experimental machine learning, or smaller-scale experiments and workflows. Additional cost incurred for Azure Databricks. See pricing details.                          |\n| The Data Science Virtual Machine (DSVM)                       | Similar to the cloud-based compute instance (Python and the SDK are pre-installed), but with additional popular data science and machine learning tools pre-installed. Easy to scale and combine with other custom tools and workflows. | A slower getting started experience compared to the cloud-based compute instance.                                                                                                    |\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1582827641316,
        "Answer_score":1.0,
        "Owner_location":"Barcelona",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60434642",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: version control of workspace notebooks; Content: i'm trying to work with the capacities of the new workspace and i can't find any option to track my notebooks on git. it's this possible as well as you can do with azure notebooks? if not is possible... how it's suposed to work with this notebooks? only inside this workspace? thanks!",
        "Question_original_content_gpt_summary":"The user is trying to use version control with workspace notebooks and is unsure if it is possible or how to do it.",
        "Question_preprocessed_content":"Title: version control of workspace notebooks; Content: i'm trying to work with the capacities of the new workspace and i can't find any option to track my notebooks on git. it's this possible as well as you can do with azure notebooks? if not is how it's suposed to work with this notebooks? only inside this workspace? thanks!",
        "Answer_original_content":"afaik, git isn't currently supported by notebooks. if you're looking for a more fully-featured development environment, i suggest setting one up locally. there's more work up front, but it will give you the ability to version control. check out this development environment set-up guide. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment | environment | pros | cons | |---------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | cloud-based compute instance (preview) | easiest way to get started. the entire sdk is already installed in your workspace vm, and notebook tutorials are pre-cloned and ready to run. | lack of control over your development environment and dependencies. additional cost incurred for linux vm (vm can be stopped when not in use to avoid charges). see pricing details. | | local environment | full control of your development environment and dependencies. run with any build tool, environment, or ide of your choice. | takes longer to get started. necessary sdk packages must be installed, and an environment must also be installed if you don't already have one. | | azure databricks | ideal for running large-scale intensive machine learning workflows on the scalable apache spark platform. | overkill for experimental machine learning, or smaller-scale experiments and workflows. additional cost incurred for azure databricks. see pricing details. | | the data science virtual machine (dsvm) | similar to the cloud-based compute instance (python and the sdk are pre-installed), but with additional popular data science and machine learning tools pre-installed. easy to scale and combine with other custom tools and workflows. | a slower getting started experience compared to the cloud-based compute instance. |",
        "Answer_original_content_gpt_summary":"The user is trying to use version control with workspace notebooks and is unsure if it is possible or how to do it. Unfortunately, git is not currently supported by notebooks. However, if the user is looking for a more fully-featured development environment, they can set one up locally. This will give them the ability to version control. They can check out the development environment set-up guide for more information. There are several options for development environments, including cloud-based compute instances, local environments, Azure Databricks, and the Data Science Virtual Machine (DSVM). Each option has its pros and cons, such as ease of use, control over the environment, and cost.",
        "Answer_preprocessed_content":"afaik, git isn't currently supported by notebooks. if you're looking for a more development environment, i suggest setting one up locally. there's more work up front, but it will give you the ability to version control. check out this development environment guide."
    },
    {
        "Question_id":71579883.0,
        "Question_title":"Missing -symbol.json error when trying to compile a SageMaker semantic segmentation model (built-in algorithm) with SageMaker Neo",
        "Question_body":"<p>I have trained a SageMaker semantic segmentation model, using the built-in sagemaker semantic segmentation algorithm. This deploys ok to a SageMaker endpoint and I can run inference in the cloud  successfully from it.\nI would like to use the model on a edge device (AWS Panorama Appliance) which should just mean compiling the model with SageMaker Neo to the specifications of the target device.<\/p>\n<p>However, regardless of what my target device is (the Neo settings), I cant seem to compile the model with Neo as I get the following error:<\/p>\n<pre><code>ClientError: InputConfiguration: No valid Mxnet model file -symbol.json found\n<\/code><\/pre>\n<p>The model.tar.gz for semantic segmentation models contains hyperparams.json, model_algo-1, model_best.params. According to the docs, model_algo-1 is the serialized mxnet model. Aren't gluon models supported by Neo?<\/p>\n<p>Incidentally I encountered the exact same problem with another SageMaker built-in algorithm, the k-Nearest Neighbour (k-NN). It too seems to be compiled without a -symbol.json.<\/p>\n<p>Is there some scripts I can run to recreated a -symbol.json file or convert the compiled sagemaker model?<\/p>\n<p>After building my model with an Estimator, I got to compile it in SageMaker Neo with code:<\/p>\n<pre><code>optimized_ic = my_estimator.compile_model(\n target_instance_family=&quot;ml_c5&quot;,\n target_platform_os=&quot;LINUX&quot;,\n target_platform_arch=&quot;ARM64&quot;,\n input_shape={&quot;data&quot;: [1,3,512,512]},  \n output_path=s3_optimized_output_location,\n framework=&quot;mxnet&quot;,\n framework_version=&quot;1.8&quot;, \n)\n<\/code><\/pre>\n<p>I would expect this to compile ok, but that is where I get the error saying the model is missing the *-symbol.json file.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1647989575803,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":52.0,
        "Owner_creation_time":1586928819952,
        "Owner_last_access_time":1663214624716,
        "Owner_reputation":25.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":"<p>For some reason, AWS has decided to not make its built-in algorithms directly compatible with Neo... However, you can re-engineer the network parameters using the model.tar.gz output file and then compile.<\/p>\n<p>Step 1: Extract model from tar file<\/p>\n<pre><code>import tarfile\n#path to local tar file\nmodel = 'ss_model.tar.gz'\n\n#extract tar file \nt = tarfile.open(model, 'r:gz')\nt.extractall()\n<\/code><\/pre>\n<p>This should output two files:\nmodel_algo-1, model_best.params<\/p>\n<ol start=\"2\">\n<li>Load weights into network from gluon model zoo for the architecture that you chose<\/li>\n<\/ol>\n<p>In this case I used DeepLabv3 with resnet50<\/p>\n<pre><code>import gluoncv\nimport mxnet as mx\nfrom gluoncv import model_zoo\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\n\nmodel = model_zoo.DeepLabV3(nclass=2, backbone='resnet50', pretrained_base=False, height=800, width=1280, crop_size=240)\nmodel.load_parameters(&quot;model_algo-1&quot;)\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Check the parameters have loaded correctly by making a prediction with new model<\/li>\n<\/ol>\n<p>Use an image that was used for training.<\/p>\n<pre><code>#use cpu\nctx = mx.cpu(0)\n#decode image bytes of loaded file\nimg = image.imdecode(imbytes)\n\n#transform image\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\nprint('tranformed image shape: ', img.shape)\n\n#get prediction\noutput = model.predict(img)\n<\/code><\/pre>\n<ol start=\"4\">\n<li>Hybridise model into output required by Sagemaker Neo<\/li>\n<\/ol>\n<p>Additional check for image shape compatibility<\/p>\n<pre><code>model.hybridize()\nmodel(mx.nd.ones((1,3,800,1280)))\nexport_block('deeplabv3-res50', model, data_shape=(3,800,1280), preprocess=None, layout='CHW')\n<\/code><\/pre>\n<ol start=\"5\">\n<li>Recompile model into tar.gz format<\/li>\n<\/ol>\n<p>This contains the params and json file which Neo looks for.<\/p>\n<pre><code>tar = tarfile.open(&quot;comp_model.tar.gz&quot;, &quot;w:gz&quot;)\nfor name in [&quot;deeplabv3-res50-0000.params&quot;, &quot;deeplabv3-res50-symbol.json&quot;]:\n    tar.add(name)\ntar.close()\n<\/code><\/pre>\n<ol start=\"6\">\n<li>Save tar.gz file to s3 and then compile using Neo GUI<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1648038217500,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1648020944223,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71579883",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: missing -symbol.json error when trying to compile a semantic segmentation model (built-in algorithm) with neo; Content: i have trained a semantic segmentation model, using the built-in semantic segmentation algorithm. this deploys ok to a endpoint and i can run inference in the cloud successfully from it. i would like to use the model on a edge device (aws panorama appliance) which should just mean compiling the model with neo to the specifications of the target device. however, regardless of what my target device is (the neo settings), i cant seem to compile the model with neo as i get the following error: clienterror: inputconfiguration: no valid mxnet model file -symbol.json found the model.tar.gz for semantic segmentation models contains hyperparams.json, model_algo-1, model_best.params. according to the docs, model_algo-1 is the serialized mxnet model. aren't gluon models supported by neo? incidentally i encountered the exact same problem with another built-in algorithm, the k-nearest neighbour (k-nn). it too seems to be compiled without a -symbol.json. is there some scripts i can run to recreated a -symbol.json file or convert the compiled model? after building my model with an estimator, i got to compile it in neo with code: optimized_ic = my_estimator.compile_model( target_instance_family=\"ml_c5\", target_platform_os=\"linux\", target_platform_arch=\"arm64\", input_shape={\"data\": [1,3,512,512]}, output_path=s3_optimized_output_location, framework=\"mxnet\", framework_version=\"1.8\", ) i would expect this to compile ok, but that is where i get the error saying the model is missing the *-symbol.json file.",
        "Question_original_content_gpt_summary":"The user is encountering an error when trying to compile a semantic segmentation model with neo, due to the lack of a -symbol.json file.",
        "Question_preprocessed_content":"Title: missing error when trying to compile a semantic segmentation model with neo; Content: i have trained a semantic segmentation model, using the semantic segmentation algorithm. this deploys ok to a endpoint and i can run inference in the cloud successfully from it. i would like to use the model on a edge device which should just mean compiling the model with neo to the specifications of the target device. however, regardless of what my target device is , i cant seem to compile the model with neo as i get the following error the for semantic segmentation models contains according to the docs, is the serialized mxnet model. aren't gluon models supported by neo? incidentally i encountered the exact same problem with another algorithm, the neighbour . it too seems to be compiled without a is there some scripts i can run to recreated a file or convert the compiled model? after building my model with an estimator, i got to compile it in neo with code i would expect this to compile ok, but that is where i get the error saying the model is missing the file.",
        "Answer_original_content":"for some reason, aws has decided to not make its built-in algorithms directly compatible with neo... however, you can re-engineer the network parameters using the model.tar.gz output file and then compile. step 1: extract model from tar file import tarfile #path to local tar file model = 'ss_model.tar.gz' #extract tar file t = tarfile.open(model, 'r:gz') t.extractall() this should output two files: model_algo-1, model_best.params load weights into network from gluon model zoo for the architecture that you chose in this case i used deeplabv3 with resnet50 import gluoncv import mxnet as mx from gluoncv import model_zoo from gluoncv.data.transforms.presets.segmentation import test_transform model = model_zoo.deeplabv3(nclass=2, backbone='resnet50', pretrained_base=false, height=800, width=1280, crop_size=240) model.load_parameters(\"model_algo-1\") check the parameters have loaded correctly by making a prediction with new model use an image that was used for training. #use cpu ctx = mx.cpu(0) #decode image bytes of loaded file img = image.imdecode(imbytes) #transform image img = test_transform(img, ctx) img = img.astype('float32') print('tranformed image shape: ', img.shape) #get prediction output = model.predict(img) hybridise model into output required by neo additional check for image shape compatibility model.hybridize() model(mx.nd.ones((1,3,800,1280))) export_block('deeplabv3-res50', model, data_shape=(3,800,1280), preprocess=none, layout='chw') recompile model into tar.gz format this contains the params and json file which neo looks for. tar = tarfile.open(\"comp_model.tar.gz\", \"w:gz\") for name in [\"deeplabv3-res50-0000.params\", \"deeplabv3-res50-symbol.json\"]: tar.add(name) tar.close() save tar.gz file to s3 and then compile using neo gui",
        "Answer_original_content_gpt_summary":"Possible solutions to the error encountered when trying to compile a semantic segmentation model with neo due to the lack of a -symbol.json file are: re-engineer the network parameters using the model.tar.gz output file and then compile, extract the model from the tar file, load weights into the network from gluon model zoo, check the parameters have loaded correctly by making a prediction with the new model, hybridize the model into output required by neo, recompile the model into tar.gz format, and save the tar.gz file to S3 and then compile using neo GUI.",
        "Answer_preprocessed_content":"for some reason, aws has decided to not make its algorithms directly compatible with however, you can the network parameters using the output file and then compile. step extract model from tar file this should output two files load weights into network from gluon model zoo for the architecture that you chose in this case i used deeplabv with resnet check the parameters have loaded correctly by making a prediction with new model use an image that was used for training. hybridise model into output required by neo additional check for image shape compatibility recompile model into format this contains the params and json file which neo looks for. save file to s and then compile using neo gui"
    },
    {
        "Question_id":null,
        "Question_title":"How can make multi model endpoint with SageMaker?",
        "Question_body":"This is my code.\n\nfrom datetime import datetime\nfrom sagemaker.multidatamodel import MultiDataModel\nmme = MultiDataModel(\n    name=\"LV-multi-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n    model_data_prefix=model_dir, # 2\uc5d0\uc11c \uad6c\ud55c \ubaa8\ub378\uc774 \ubaa8\uc5ec\uc788\ub294 \ud3f4\ub354(\uacbd\ub85c)!!,\n    model=sagemaker_model,  # \ubaa8\ub378 \uac1d\uccb4 1\uac1c \uc6b0\uc120 \ub123\uae30\n    sagemaker_session=sess\n)\n\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=\"ml.g4dn.xlarge\"\n)\n\nAnd error message. How can I find Ecr Image(within multi-models=true)?\n\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Your Ecr Image 763104351884.dkr.ecr.ap-northeast-2.amazonaws.com\/pytorch-inference:1.8.1-gpu-py3 does not contain required com.amazonaws.sagemaker.capabilities.multi-models=true Docker label(s).",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1660122368052,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJQBp6A_dSQm1RJ3f8AYMmg\/how-can-make-multi-model-endpoint-with-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-10T13:57:13.476Z",
                "Answer_score":1,
                "Answer_body":"Hi there - thanks for opening this thread. Multi-model endpoints are not supported on GPU instance types, see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-endpoint-instance\n\nIn order to host a multi-model endpoint, choose a CPU instance type instead. The ECR image for CPUs will contain the required com.amazonaws.sagemaker.capabilities.multi-models=true label, see here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can make multi model endpoint with ?; Content: this is my code. from datetime import datetime from .multidatamodel import multidatamodel mme = multidatamodel( name=\"lv-multi-\" + datetime.now().strftime(\"%y-%m-%d-%h-%m-%s\"), model_data_prefix=model_dir, # 2\uc5d0\uc11c \uad6c\ud55c \ubaa8\ub378\uc774 \ubaa8\uc5ec\uc788\ub294 \ud3f4\ub354(\uacbd\ub85c)!!, model=_model, # \ubaa8\ub378 \uac1d\uccb4 1\uac1c \uc6b0\uc120 \ub123\uae30 _session=sess ) predictor = mme.deploy( initial_instance_count=1, instance_type=\"ml.g4dn.xlarge\" ) and error message. how can i find ecr image(within multi-models=true)? clienterror: an error occurred (validationexception) when calling the createmodel operation: your ecr image 763104351884.dkr.ecr.ap-northeast-2.amazonaws.com\/pytorch-inference:1.8.1-gpu-py3 does not contain required com.amazonaws..capabilities.multi-models=true docker label(s).",
        "Question_original_content_gpt_summary":"The user encountered a challenge when attempting to create a multi-model endpoint, resulting in a validation exception due to the lack of a required docker label.",
        "Question_preprocessed_content":"Title: how can make multi model endpoint with ?; Content: this is my code. from datetime import datetime from import multidatamodel mme multidatamodel !!, predictor and error message. how can i find ecr image ? clienterror an error occurred when calling the createmodel operation your ecr image does not contain required docker label .",
        "Answer_original_content":"hi there - thanks for opening this thread. multi-model endpoints are not supported on gpu instance types, see here: https:\/\/docs.aws.amazon.com\/\/latest\/dg\/multi-model-endpoints.html#multi-model-endpoint-instance in order to host a multi-model endpoint, choose a cpu instance type instead. the ecr image for cpus will contain the required com.amazonaws..capabilities.multi-models=true label, see here: https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/dockerfile.cpu",
        "Answer_original_content_gpt_summary":"Summary: The solution to the validation exception encountered when attempting to create a multi-model endpoint is to choose a CPU instance type instead of a GPU instance type. The ECR image for CPUs will contain the required label for multi-models.",
        "Answer_preprocessed_content":"hi there thanks for opening this thread. endpoints are not supported on gpu instance types, see here in order to host a endpoint, choose a cpu instance type instead. the ecr image for cpus will contain the required label, see here"
    },
    {
        "Question_id":null,
        "Question_title":"Vertex AI image classification models lose accuracy when being placed in a python dictionary",
        "Question_body":"I have made a  model using vertex AI's image classification. Exported as EdgeTPU tflite model to my Raspberry pi 4 with Coral USB accelerator. When I used the Pycoral's example code https:\/\/github.com\/google-coral\/pycoral\/blob\/master\/examples\/classify_image.py  to run my model, I get a perfect prediction result. But when I passed them to a python dictionary in my script, the prediction accuracy is way off. https:\/\/github.com\/hillyuyichu\/Pycoral-python-API\/blob\/main\/pycoral_classification.py   Here is a screenshot of the prediction results on my python classification.py:The label in row 1 is always the most active. The one in the last rows are the least active and most inaccurate.ex: In picture 2, the label empty_pan barely ever cross 0.10 mark when it should have been more than 0.50",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1666779180000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":106.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-image-classification-models-lose-accuracy-when-being\/td-p\/482475\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-31T12:54:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"It seems Vertex AI is not supposed to be placed in a Python directory.\n\nIf this is impacting your application or your business, you can file a feature request using the following link. File the feature request, and they could assist you with the feature you are trying to implement."
            },
            {
                "Answer_creation_time":"2022-11-02T09:30:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi Comaro!\n\nThanks for the reply! I was able to find a way to work around it."
            },
            {
                "Answer_creation_time":"2022-11-02T11:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Could you share the workaround, Hillyu?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: image classification models lose accuracy when being placed in a python dictionary; Content: i have made a model using 's image classification. exported as edgetpu tflite model to my raspberry pi 4 with coral usb accelerator. when i used the pycoral's example code https:\/\/github.com\/google-coral\/pycoral\/blob\/master\/examples\/classify_image.py to run my model, i get a perfect prediction result. but when i passed them to a python dictionary in my script, the prediction accuracy is way off. https:\/\/github.com\/hillyuyichu\/pycoral-python-api\/blob\/main\/pycoral_classification.py here is a screenshot of the prediction results on my python classification.py:the label in row 1 is always the most active. the one in the last rows are the least active and most inaccurate.ex: in picture 2, the label empty_pan barely ever cross 0.10 mark when it should have been more than 0.50",
        "Question_original_content_gpt_summary":"The user encountered a challenge where their image classification models lost accuracy when being placed in a Python dictionary.",
        "Question_preprocessed_content":"Title: image classification models lose accuracy when being placed in a python dictionary; Content: i have made a model using 's image classification. exported as edgetpu tflite model to my raspberry pi with coral usb accelerator. when i used the pycoral's example code to run my model, i get a perfect prediction result. but when i passed them to a python dictionary in my script, the prediction accuracy is way off. here is a screenshot of the prediction results on my python label in row is always the most active. the one in the last rows are the least active and most in picture , the label barely ever cross mark when it should have been more than",
        "Answer_original_content":"it seems is not supposed to be placed in a python directory. if this is impacting your application or your business, you can file a feature request using the following link. file the feature request, and they could assist you with the feature you are trying to implement. hi comaro! thanks for the reply! i was able to find a way to work around it. could you share the workaround, hillyu?",
        "Answer_original_content_gpt_summary":"Possible solution: The image classification models should not be placed in a Python dictionary. If this is impacting the application or business, a feature request can be filed to get assistance with the feature being implemented. The user was able to find a workaround, but it is not specified what it is.",
        "Answer_preprocessed_content":"it seems is not supposed to be placed in a python directory. if this is impacting your application or your business, you can file a feature request using the following link. file the feature request, and they could assist you with the feature you are trying to implement. hi comaro! thanks for the reply! i was able to find a way to work around it. could you share the workaround, hillyu?"
    },
    {
        "Question_id":null,
        "Question_title":"WandB and AWS Lambda",
        "Question_body":"<p>We\u2019re trying to run WandB (0.13.4) in an AWS Lambda with Python 3.9.  We have set an environment variable for our API Key.  But we are getting this error message from Lambda:<\/p>\n<pre><code class=\"lang-auto\">{\n  \"errorMessage\": \"Error communicating with wandb process\",\n  \"errorType\": \"UsageError\",\n  \"requestId\": \"0b6c4576-adbe-4182-826a-eca4dd06bc6f\",\n  \"stackTrace\": [\n    \"  File \\\"\/var\/task\/lambda_monitor.py\\\", line 73, in test_harness\\n    run  = wandb.init(project= WB_PROJECT,\\n\",\n    \"  File \\\"\/var\/task\/wandb\/sdk\/wandb_init.py\\\", line 1078, in init\\n    run = wi.init()\\n\",\n    \"  File \\\"\/var\/task\/wandb\/sdk\/wandb_init.py\\\", line 719, in init\\n    raise UsageError(error_message)\\n\"\n  ]\n}\n<\/code><\/pre>\n<p>The calling code is this:<\/p>\n<pre><code class=\"lang-auto\">    run  = wandb.init(project= WB_PROJECT,\n                      id     = WB_RUN_ID,  # We force the run to continue with the specific monitoring Run ID\n                      resume = True,\n                      settings=wandb.Settings(start_method=\"fork\"))\n<\/code><\/pre>\n<p>And this error message from CloudWatch:<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Path \/var\/task\/wandb\/ wasn't writable, using system temp directory.\nOpenBLAS WARNING - could not determine the L2 cache size on this system, assuming 256k\nwandb: WARNING Path \/var\/task\/wandb\/ wasn't writable, using system temp directory\nwandb: W&amp;B API key is configured. Use `wandb login --relogin` to force relogin\nTraceback (most recent call last):\nFile \"\/var\/lang\/lib\/python3.9\/runpy.py\", line 197, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\nFile \"\/var\/lang\/lib\/python3.9\/runpy.py\", line 87, in _run_code\nexec(code, run_globals)\nFile \"\/var\/task\/wandb\/__main__.py\", line 3, in &lt;module&gt;\ncli.cli(prog_name=\"python -m wandb\")\nFile \"\/var\/task\/click\/core.py\", line 1130, in __call__\nreturn self.main(*args, **kwargs)\nFile \"\/var\/task\/click\/core.py\", line 1055, in main\nrv = self.invoke(ctx)\nFile \"\/var\/task\/click\/core.py\", line 1657, in invoke\nreturn _process_result(sub_ctx.command.invoke(sub_ctx))\nFile \"\/var\/task\/click\/core.py\", line 1404, in invoke\nreturn ctx.invoke(self.callback, **ctx.params)\nFile \"\/var\/task\/click\/core.py\", line 760, in invoke\nreturn __callback(*args, **kwargs)\nFile \"\/var\/task\/wandb\/cli\/cli.py\", line 97, in wrapper\nreturn func(*args, **kwargs)\nFile \"\/var\/task\/wandb\/cli\/cli.py\", line 282, in service\nserver.serve()\nFile \"\/var\/task\/wandb\/sdk\/service\/server.py\", line 142, in serve\nmux.loop()\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 394, in loop\nraise e\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 392, in loop\nself._loop()\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 385, in _loop\nself._process_action(action)\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 350, in _process_action\nself._process_add(action)\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 203, in _process_add\nstream = StreamRecord(action._data, mailbox=self._mailbox)\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 61, in __init__\nself._record_q = multiprocessing.Queue()\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/context.py\", line 103, in Queue\nreturn Queue(maxsize, ctx=self.get_context())\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/queues.py\", line 43, in __init__\nself._rlock = ctx.Lock()\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/context.py\", line 68, in Lock\nreturn Lock(ctx=self.get_context())\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 162, in __init__\nSemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 57, in __init__\nsl = self._semlock = _multiprocessing.SemLock(\nOSError: [Errno 38] Function not implemented\nwandb: - Waiting for wandb.init()...\nwandb: \\ Waiting for wandb.init()...\nwandb: | Waiting for wandb.init()...\nwandb: \/ Waiting for wandb.init()...\nwandb: - Waiting for wandb.init()...\nwandb: \\ Waiting for wandb.init()...\nwandb: | Waiting for wandb.init()...\nwandb: \/ Waiting for wandb.init()...\nwandb: - Waiting for wandb.init()...\nwandb: \\ Waiting for wandb.init()...\nwandb: | Waiting for wandb.init()...\nwandb: \/ Waiting for wandb.init()...\nwandb: - Waiting for wandb.init()...\nwandb: \\ Waiting for wandb.init()...\nwandb: | Waiting for wandb.init()...\nwandb: \/ Waiting for wandb.init()...\nwandb: - Waiting for wandb.init()...\nwandb: \\ Waiting for wandb.init()...\nwandb: | Waiting for wandb.init()...\nwandb: \/ Waiting for wandb.init()...\nwandb: - Waiting for wandb.init()...\nwandb: \\ Waiting for wandb.init()...\nwandb: | Waiting for wandb.init()...\nwandb: \/ Waiting for wandb.init()...\nwandb: - Waiting for wandb.init()...\nwandb: \\ Waiting for wandb.init()...\nwandb: | Waiting for wandb.init()...\nwandb: \/ Waiting for wandb.init()...\nwandb: - Waiting for wandb.init()...\nwandb: \\ Waiting for wandb.init()...\nwandb: | Waiting for wandb.init()...\nwandb: \/ Waiting for wandb.init()...\nwandb: - Waiting for wandb.init()...\nwandb: \\ Waiting for wandb.init()...\nwandb: | Waiting for wandb.init()...<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1667288878824,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":412.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-and-aws-lambda\/3354",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-01T07:53:50.760Z",
                "Answer_body":"<p>I added the additional Environment Variables as mentioned by <code>Akshey<\/code> (<a href=\"https:\/\/community.wandb.ai\/t\/integration-of-wandb-with-aws-lambda\/2280\">here<\/a>):<\/p>\n<pre><code class=\"lang-auto\">WANDB_CACHE_DIR= \/tmp\/\nWANDB_CONFIG_DIR=\/tmp\/\nWANDB_DIR=\/tmp\/\nWANDB_SILENT=true\n<\/code><\/pre>\n<p>And the Cloudwatch log becomes:<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\nFile \"\/var\/lang\/lib\/python3.9\/runpy.py\", line 197, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\nFile \"\/var\/lang\/lib\/python3.9\/runpy.py\", line 87, in _run_code\nexec(code, run_globals)\nFile \"\/var\/task\/wandb\/__main__.py\", line 3, in &lt;module&gt;\ncli.cli(prog_name=\"python -m wandb\")\nFile \"\/var\/task\/click\/core.py\", line 1130, in __call__\nreturn self.main(*args, **kwargs)\nFile \"\/var\/task\/click\/core.py\", line 1055, in main\nrv = self.invoke(ctx)\nFile \"\/var\/task\/click\/core.py\", line 1657, in invoke\nreturn _process_result(sub_ctx.command.invoke(sub_ctx))\nFile \"\/var\/task\/click\/core.py\", line 1404, in invoke\nreturn ctx.invoke(self.callback, **ctx.params)\nFile \"\/var\/task\/click\/core.py\", line 760, in invoke\nreturn __callback(*args, **kwargs)\nFile \"\/var\/task\/wandb\/cli\/cli.py\", line 97, in wrapper\nreturn func(*args, **kwargs)\nFile \"\/var\/task\/wandb\/cli\/cli.py\", line 282, in service\nserver.serve()\nFile \"\/var\/task\/wandb\/sdk\/service\/server.py\", line 142, in serve\nmux.loop()\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 394, in loop\nraise e\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 392, in loop\nself._loop()\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 385, in _loop\nself._process_action(action)\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 350, in _process_action\nself._process_add(action)\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 203, in _process_add\nstream = StreamRecord(action._data, mailbox=self._mailbox)\nFile \"\/var\/task\/wandb\/sdk\/service\/streams.py\", line 61, in __init__\nself._record_q = multiprocessing.Queue()\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/context.py\", line 103, in Queue\nreturn Queue(maxsize, ctx=self.get_context())\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/queues.py\", line 43, in __init__\nself._rlock = ctx.Lock()\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/context.py\", line 68, in Lock\nreturn Lock(ctx=self.get_context())\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 162, in __init__\nSemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx)\nFile \"\/var\/lang\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 57, in __init__\nsl = self._semlock = _multiprocessing.SemLock(\nOSError: [Errno 38] Function not implemented\nProblem at: \/var\/task\/lambda_monitor.py 73 test_harness[DEBUG]\t2022-11-01T07:51:38.952Z\tb1a9ad4c-e05a-40d4-b2cb-14eaa17be2c0\tStarting new HTTPS connection (1): o151352.ingest.sentry.io:443\n[ERROR] UsageError: Error communicating with wandb process\nTraceback (most recent call last):\n  File \"\/var\/task\/lambda_monitor.py\", line 73, in test_harness\n    run  = wandb.init(project= WB_PROJECT,\n  File \"\/var\/task\/wandb\/sdk\/wandb_init.py\", line 1078, in init\n    run = wi.init()\n  File \"\/var\/task\/wandb\/sdk\/wandb_init.py\", line 719, in init\n    raise UsageError(error_message)<\/code><\/pre>",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-02T19:49:24.829Z",
                "Answer_body":"<p>Hi Leslie,<br>\nAny updates?<\/p>\n<p>Kevin A. Shaw, Ph.D.<br>\nCTO \/ CoFounder  |   Algorithmic Intuition Inc<br>\n<a href=\"http:\/\/www.algorithmicintuition.com\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">www.algorithmicintuition.com<\/a><br>\n<a href=\"mailto:kevin@algoint.com\">kevin@algoint.com<\/a>   |  Twitter: <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a><\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-03T18:23:55.391Z",
                "Answer_body":"<p>Hi Kevin, thank you for your patience! From your logs it looks like you are trying to use multiprocessing with AWS Lambda. AWS lambda doesn\u2019t have a shared memory folder hence why you\u2019re running into this issue. You can try to use <code>spawn<\/code> instead of <code>fork<\/code> in your wandb init function but since this is due to AWS Lambda, it might not be able to help here<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-31T07:54:12.561Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: and aws lambda; Content: we\u2019re trying to run (0.13.4) in an aws lambda with python 3.9. we have set an environment variable for our api key. but we are getting this error message from lambda: { \"errormessage\": \"error communicating with process\", \"errortype\": \"usageerror\", \"requestid\": \"0b6c4576-adbe-4182-826a-eca4dd06bc6f\", \"stacktrace\": [ \" file \\\"\/var\/task\/lambda_monitor.py\\\", line 73, in test_harness\\n run = .init(project= wb_project,\\n\", \" file \\\"\/var\/task\/\/sdk\/_init.py\\\", line 1078, in init\\n run = wi.init()\\n\", \" file \\\"\/var\/task\/\/sdk\/_init.py\\\", line 719, in init\\n raise usageerror(error_message)\\n\" ] } the calling code is this: run = .init(project= wb_project, id = wb_run_id, # we force the run to continue with the specific monitoring run id resume = true, settings=.settings(start_method=\"fork\")) and this error message from cloudwatch: : warning path \/var\/task\/\/ wasn't writable, using system temp directory. openblas warning - could not determine the l2 cache size on this system, assuming 256k : warning path \/var\/task\/\/ wasn't writable, using system temp directory : w&b api key is configured. use ` login --relogin` to force relogin traceback (most recent call last): file \"\/var\/lang\/lib\/python3.9\/runpy.py\", line 197, in _run_module_as_main return _run_code(code, main_globals, none, file \"\/var\/lang\/lib\/python3.9\/runpy.py\", line 87, in _run_code exec(code, run_globals) file \"\/var\/task\/\/__main__.py\", line 3, in <module> cli.cli(prog_name=\"python -m \") file \"\/var\/task\/click\/core.py\", line 1130, in __call__ return self.main(*args, **kwargs) file \"\/var\/task\/click\/core.py\", line 1055, in main rv = self.invoke(ctx) file \"\/var\/task\/click\/core.py\", line 1657, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) file \"\/var\/task\/click\/core.py\", line 1404, in invoke return ctx.invoke(self.callback, **ctx.params) file \"\/var\/task\/click\/core.py\", line 760, in invoke return __callback(*args, **kwargs) file \"\/var\/task\/\/cli\/cli.py\", line 97, in wrapper return func(*args, **kwargs) file \"\/var\/task\/\/cli\/cli.py\", line 282, in service server.serve() file \"\/var\/task\/\/sdk\/service\/server.py\", line 142, in serve mux.loop() file \"\/var\/task\/\/sdk\/service\/streams.py\", line 394, in loop raise e file \"\/var\/task\/\/sdk\/service\/streams.py\", line 392, in loop self._loop() file \"\/var\/task\/\/sdk\/service\/streams.py\", line 385, in _loop self._process_action(action) file \"\/var\/task\/\/sdk\/service\/streams.py\", line 350, in _process_action self._process_add(action) file \"\/var\/task\/\/sdk\/service\/streams.py\", line 203, in _process_add stream = streamrecord(action._data, mailbox=self._mailbox) file \"\/var\/task\/\/sdk\/service\/streams.py\", line 61, in __init__ self._record_q = multiprocessing.queue() file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/context.py\", line 103, in queue return queue(maxsize, ctx=self.get_context()) file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/queues.py\", line 43, in __init__ self._rlock = ctx.lock() file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/context.py\", line 68, in lock return lock(ctx=self.get_context()) file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 162, in __init__ semlock.__init__(self, semaphore, 1, 1, ctx=ctx) file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 57, in __init__ sl = self._semlock = _multiprocessing.semlock( oserror: [errno 38] function not implemented : - waiting for .init()... : \\ waiting for .init()... : | waiting for .init()... : \/ waiting for .init()... : - waiting for .init()... : \\ waiting for .init()... : | waiting for .init()... : \/ waiting for .init()... : - waiting for .init()... : \\ waiting for .init()... : | waiting for .init()... : \/ waiting for .init()... : - waiting for .init()... : \\ waiting for .init()... : | waiting for .init()... : \/ waiting for .init()... : - waiting for .init()... : \\ waiting for .init()... : | waiting for .init()... : \/ waiting for .init()... : - waiting for .init()... : \\ waiting for .init()... : | waiting for .init()... : \/ waiting for .init()... : - waiting for .init()... : \\ waiting for .init()... : | waiting for .init()... : \/ waiting for .init()... : - waiting for .init()... : \\ waiting for .init()... : | waiting for .init()... : \/ waiting for .init()... : - waiting for .init()... : \\ waiting for .init()... : | waiting for .init()...",
        "Question_original_content_gpt_summary":"The user is encountering challenges running 0.13.4 in an AWS Lambda with Python 3.9, including an error message from Lambda, a warning from Cloudwatch, and an OSError.",
        "Question_preprocessed_content":"Title: and aws lambda; Content: were trying to run in an aws lambda with python we have set an environment variable for our api key. but we are getting this error message from lambda the calling code is this and this error message from cloudwatch",
        "Answer_original_content":"i added the additional environment variables as mentioned by akshey (here): _cache_dir= \/tmp\/ _config_dir=\/tmp\/ _dir=\/tmp\/ _silent=true and the cloudwatch log becomes: traceback (most recent call last): file \"\/var\/lang\/lib\/python3.9\/runpy.py\", line 197, in _run_module_as_main return _run_code(code, main_globals, none, file \"\/var\/lang\/lib\/python3.9\/runpy.py\", line 87, in _run_code exec(code, run_globals) file \"\/var\/task\/\/__main__.py\", line 3, in <module> cli.cli(prog_name=\"python -m \") file \"\/var\/task\/click\/core.py\", line 1130, in __call__ return self.main(*args, **kwargs) file \"\/var\/task\/click\/core.py\", line 1055, in main rv = self.invoke(ctx) file \"\/var\/task\/click\/core.py\", line 1657, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) file \"\/var\/task\/click\/core.py\", line 1404, in invoke return ctx.invoke(self.callback, **ctx.params) file \"\/var\/task\/click\/core.py\", line 760, in invoke return __callback(*args, **kwargs) file \"\/var\/task\/\/cli\/cli.py\", line 97, in wrapper return func(*args, **kwargs) file \"\/var\/task\/\/cli\/cli.py\", line 282, in service server.serve() file \"\/var\/task\/\/sdk\/service\/server.py\", line 142, in serve mux.loop() file \"\/var\/task\/\/sdk\/service\/streams.py\", line 394, in loop raise e file \"\/var\/task\/\/sdk\/service\/streams.py\", line 392, in loop self._loop() file \"\/var\/task\/\/sdk\/service\/streams.py\", line 385, in _loop self._process_action(action) file \"\/var\/task\/\/sdk\/service\/streams.py\", line 350, in _process_action self._process_add(action) file \"\/var\/task\/\/sdk\/service\/streams.py\", line 203, in _process_add stream = streamrecord(action._data, mailbox=self._mailbox) file \"\/var\/task\/\/sdk\/service\/streams.py\", line 61, in __init__ self._record_q = multiprocessing.queue() file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/context.py\", line 103, in queue return queue(maxsize, ctx=self.get_context()) file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/queues.py\", line 43, in __init__ self._rlock = ctx.lock() file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/context.py\", line 68, in lock return lock(ctx=self.get_context()) file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 162, in __init__ semlock.__init__(self, semaphore, 1, 1, ctx=ctx) file \"\/var\/lang\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 57, in __init__ sl = self._semlock = _multiprocessing.semlock( oserror: [errno 38] function not implemented problem at: \/var\/task\/lambda_monitor.py 73 test_harness[debug] 2022-11-01t07:51:38.952z b1a9ad4c-e05a-40d4-b2cb-14eaa17be2c0 starting new https connection (1): o151352.ingest.sentry.io:443 [error] usageerror: error communicating with process traceback (most recent call last): file \"\/var\/task\/lambda_monitor.py\", line 73, in test_harness run = .init(project= wb_project, file \"\/var\/task\/\/sdk\/_init.py\", line 1078, in init run = wi.init() file \"\/var\/task\/\/sdk\/_init.py\", line 719, in init raise usageerror(error_message) hi leslie, any updates? kevin a. shaw, ph.d. cto \/ cofounder | algorithmic intuition inc www.algorithmicintuition.com kevin@algoint.com | twitter: @kevinashaw hi kevin, thank you for your patience! from your logs it looks like you are trying to use multiprocessing with aws lambda. aws lambda doesnt have a shared memory folder hence why youre running into this issue. you can try to use spawn instead of fork in your init function but since this is due to aws lambda, it might not be able to help here this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The answer suggests adding additional environment variables to resolve the error message and warning encountered while running 0.13.4 in an AWS Lambda with Python 3.9. The error message is related to multiprocessing with AWS Lambda, which does not have a shared memory folder. The solution suggested is to try using spawn instead of fork in the init function, but it may not be helpful due to the limitations of AWS Lambda.",
        "Answer_preprocessed_content":"i added the additional environment variables as mentioned by and the cloudwatch log becomes hi leslie, any updates? kevin a. shaw, cto \/ cofounder algorithmic intuition inc twitter hi kevin, thank you for your patience! from your logs it looks like you are trying to use multiprocessing with aws lambda. aws lambda doesnt have a shared memory folder hence why youre running into this issue. you can try to use instead of in your init function but since this is due to aws lambda, it might not be able to help here this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Line magic error in SageMaker Studio",
        "Question_body":"Hi AWS, I am trying to create virtual environment in SageMaker Studio but while doing so I am experiencing a line magic function error. Also I am not able to import the libraries. I am attaching the error screenshot for the same.\n\nThanks",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1658670880505,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":60.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZPg9WRBtSwGvYhI9-3tscA\/line-magic-error-in-sage-maker-studio",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-25T08:28:37.568Z",
                "Answer_score":0,
                "Answer_body":"As far as I can tell, %virtualenv and %import are not standard line magics in IPython - and from a quick search around, I couldn't see what package implements them?\n\nBut I think you might want to consider more fundamental changes to your workflow and will try to make a case for it...\n\nIn SageMaker Studio, as the other answer noted already, kernel environments are containerized - rather than conda-based. This is nicely consistent with how SageMaker job environments (training, deployment, processing, etc) are also container-based.\n\nAs a result, my usual environment management suggestions would be:\n\nFor quickly and interactively experimenting with different library installations in the notebook, simply use !pip shell commands to install\/edit packages directly in the running container. (e.g. !pip install abc==x.y.z, !pip show abc, and so on). You could even save these dependencies in a requirements.txt and !pip install from that file (which would be nice because SageMaker script-mode jobs can accept a requirements.txt).\nFor formalizing an environment to be standard, repeatable, and shareable between users without them having to re-run the install commands: set up a custom kernel container image.\n\nAs outlined here, all your open notebooks with the same kernel and instance type selected will share a running container. When you delete\/restart the \"app\" (container), these changes will be lost. Keeping running\/customized kernel environments as disposable as possible, and trying to use SageMaker jobs (training, processing, etc) early in the workflow instead of sticking everything in notebook, helps to prevent reproducibility problems.\n\nBecause they're containerized, I find I don't really need to worry about environment management\/virtualization technologies: In notebook experimentation, my different kernels are fully separated anyway and I can just restart the container (\"app\") to reset. In SageMaker jobs there's only one task being run in the container, so it doesn't need to play nice with other workloads. It's different, and actually quite nice, compared to working on my local laptop where I have to carefully manage separate project environments and fix things if they break!\n\nEdit to add: The following works for me... Perhaps your problem was that the code folder didn't exist yet? os.makedirs() or !mkdir or UI actions can fix that:\n\n%%writefile src\/main.py\n\nimport os",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-28T04:44:00.831Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nWhen running a notebook on SageMaker Studio you can select the default environment you want by clicking on the top right corner on the kernel you currently have enabled (Default should be Data Science) and select the one you want from the dropdown.\n\nIn the case you want to install more\/custom packages, then you can install those straight from your notebook using one of the valid magic cell commands. These are %conda install and %pip install (see doc: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-notebooks-add-external.html)\n\nAlternatively you can open a terminal and use your tool of choice to manage the python packages installed.\n\nIn terms of importing packages, the correct syntax for that is without the %. So please instead of %import <package_name> try import <package_name>.\n\nLet us know if this solved your issue,\n\nhave a nice day",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: line magic error in studio; Content: hi aws, i am trying to create virtual environment in studio but while doing so i am experiencing a line magic function error. also i am not able to import the libraries. i am attaching the error screenshot for the same. thanks",
        "Question_original_content_gpt_summary":"The user is experiencing a line magic function error while trying to create a virtual environment in Studio and is unable to import libraries.",
        "Question_preprocessed_content":"Title: line magic error in studio; Content: hi aws, i am trying to create virtual environment in studio but while doing so i am experiencing a line magic function error. also i am not able to import the libraries. i am attaching the error screenshot for the same. thanks",
        "Answer_original_content":"as far as i can tell, %virtualenv and %import are not standard line magics in ipython - and from a quick search around, i couldn't see what package implements them? but i think you might want to consider more fundamental changes to your workflow and will try to make a case for it... in studio, as the other answer noted already, kernel environments are containerized - rather than conda-based. this is nicely consistent with how job environments (training, deployment, processing, etc) are also container-based. as a result, my usual environment management suggestions would be: for quickly and interactively experimenting with different library installations in the notebook, simply use !pip shell commands to install\/edit packages directly in the running container. (e.g. !pip install abc==x.y.z, !pip show abc, and so on). you could even save these dependencies in a requirements.txt and !pip install from that file (which would be nice because script-mode jobs can accept a requirements.txt). for formalizing an environment to be standard, repeatable, and shareable between users without them having to re-run the install commands: set up a custom kernel container image. as outlined here, all your open notebooks with the same kernel and instance type selected will share a running container. when you delete\/restart the \"app\" (container), these changes will be lost. keeping running\/customized kernel environments as disposable as possible, and trying to use jobs (training, processing, etc) early in the workflow instead of sticking everything in notebook, helps to prevent reproducibility problems. because they're containerized, i find i don't really need to worry about environment management\/virtualization technologies: in notebook experimentation, my different kernels are fully separated anyway and i can just restart the container (\"app\") to reset. in jobs there's only one task being run in the container, so it doesn't need to play nice with other workloads. it's different, and actually quite nice, compared to working on my local laptop where i have to carefully manage separate project environments and fix things if they break! edit to add: the following works for me... perhaps your problem was that the code folder didn't exist yet? os.makedirs() or !mkdir or ui actions can fix that: %%writefile src\/main.py import os hi, when running a notebook on studio you can select the default environment you want by clicking on the top right corner on the kernel you currently have enabled (default should be data science) and select the one you want from the dropdown. in the case you want to install more\/custom packages, then you can install those straight from your notebook using one of the valid magic cell commands. these are %conda install and %pip install (see doc: https:\/\/docs.aws.amazon.com\/\/latest\/dg\/studio-notebooks-add-external.html) alternatively you can open a terminal and use your tool of choice to manage the python packages installed. in terms of importing packages, the correct syntax for that is without the %. so please instead of %import try import . let us know if this solved your issue, have a nice day",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n- Use !pip shell commands to install\/edit packages directly in the running container for quickly and interactively experimenting with different library installations in the notebook.\n- Set up a custom kernel container image for formalizing an environment to be standard, repeatable, and shareable between users without them having to re-run the install commands.\n- Select the default environment by clicking on the top right corner on the kernel and select the one you want from the dropdown.\n- Install more\/custom packages straight from your notebook using one of the valid magic cell commands, %conda install and %pip install.\n- Open a terminal and use your tool of choice to manage the python packages installed.\n- Use import instead of %import for importing packages.",
        "Answer_preprocessed_content":"as far as i can tell, %virtualenv and %import are not standard line magics in ipython and from a quick search around, i couldn't see what package implements them? but i think you might want to consider more fundamental changes to your workflow and will try to make a case for in studio, as the other answer noted already, kernel environments are containerized rather than this is nicely consistent with how job environments are also as a result, my usual environment management suggestions would be for quickly and interactively experimenting with different library installations in the notebook, simply use !pip shell commands to packages directly in the running container. . you could even save these dependencies in a and !pip install from that file . for formalizing an environment to be standard, repeatable, and shareable between users without them having to the install commands set up a custom kernel container image. as outlined here, all your open notebooks with the same kernel and instance type selected will share a running container. when you the app , these changes will be lost. keeping kernel environments as disposable as possible, and trying to use jobs early in the workflow instead of sticking everything in notebook, helps to prevent reproducibility problems. because they're containerized, i find i don't really need to worry about environment technologies in notebook experimentation, my different kernels are fully separated anyway and i can just restart the container to reset. in jobs there's only one task being run in the container, so it doesn't need to play nice with other workloads. it's different, and actually quite nice, compared to working on my local laptop where i have to carefully manage separate project environments and fix things if they break! edit to add the following works for perhaps your problem was that the code folder didn't exist yet? or !mkdir or ui actions can fix that %%writefile import os hi, when running a notebook on studio you can select the default environment you want by clicking on the top right corner on the kernel you currently have enabled and select the one you want from the dropdown. in the case you want to install packages, then you can install those straight from your notebook using one of the valid magic cell commands. these are %conda install and %pip install alternatively you can open a terminal and use your tool of choice to manage the python packages installed. in terms of importing packages, the correct syntax for that is without the %. so please instead of %import try import . let us know if this solved your issue, have a nice day"
    },
    {
        "Question_id":61181955.0,
        "Question_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Question_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1586755348657,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":95.0,
        "Owner_creation_time":1586754432800,
        "Owner_last_access_time":1593512686190,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1586886613192,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61181955",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is there any limits of saving result on s3 from processing?; Content: \u203b i used google translation, if you have any question, let me know! i am trying to run python script with huge 4 data, using processing. and my current situation are as follows: can run this script with 3 data can't run the script with only 1 data (the biggest, the same structure with others) as for all of 4 data, the script has finished (so, i suspected this error in s3, ie. when copying result to s3) the error i got is this internalservererror. traceback (most recent call last): file \"_train_and_predict.py\", line 56, in <module> outputs=outputs file \"{xxx}\/_constructor.py\", line 39, in run outputs=outputs file \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/\/processing.py\", line 408, in run self.latest_job.wait(logs=logs) file \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/\/processing.py\", line 723, in wait self._session.logs_for_processing_job(self.job_name, wait=true) file \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/\/session.py\", line 3111, in logs_for_processing_job self._check_job_status(job_name, description, \"processingjobstatus\") file \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/\/session.py\", line 2615, in _check_job_status actual_status=status, .exceptions.unexpectedstatusexception: error for processing job -vm-train-and-predict-2020-04-12-04-15-40-655: failed. reason: internalservererror: we encountered an internal error. please try again.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with running a Python script with four data sets, where the script can run with three data sets but not with the largest one, and the error they are receiving is an \"InternalServerError\".",
        "Question_preprocessed_content":"Title: is there any limits of saving result on s from processing?; Content: i used google translation, if you have any question, let me know! i am trying to run python script with huge data, using processing. and my current situation are as follows can run this script with data can't run the script with only data as for all of data, the script has finished the error i got is this internalservererror.",
        "Answer_original_content":"there may be some issue transferring the output data to s3 if the output is generated at a high rate and size is too large. you can 1) try to slow down writing the output a bit or 2) call s3 from your algorithm container to upload the output directly using boto client (https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html).",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of running a Python script with four data sets, where the script can run with three data sets but not with the largest one, and the error being an \"InternalServerError\" are: 1) slowing down the writing of the output, or 2) calling s3 from the algorithm container to upload the output directly using boto client. The issue may be related to transferring the output data to s3 if the output is generated at a high rate and size is too large.",
        "Answer_preprocessed_content":"there may be some issue transferring the output data to s if the output is generated at a high rate and size is too large. you can try to slow down writing the output a bit or call s from your algorithm container to upload the output directly using boto client ."
    },
    {
        "Question_id":null,
        "Question_title":"Best compute cluster for training large image datasets !",
        "Question_body":"Good morning,\nI have a a dataset that consist of 99000 (256 x 256 pixels) images. I am trying to use this dataset to training a generative advesarial network (GAN) for at least a 1,000 epoch.\nCurrently, I am using a standard_NC24r (24 cores, 224 GB RAM, 1440 GB disk) GPU (4 x NVIDIA Tesla K80) cluster but the training is slow. It takes about 3000 seconds to train 1 epoch. This implies it would take at least a month to complete training.\nIs a cluster that I can used to speed up training?\n\nThanks for your help in advance\n\nMany thanks\n\nRoland",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1633703536287,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/583349\/best-compute-cluster-for-training-large-image-data.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-11T05:01:12.66Z",
                "Answer_score":0,
                "Answer_body":"@OkwenRolandT-6377 Thanks, Instead of bigger machines with more memory, there are techniques to be used with Aml Compute for larger datasets. The Parallel Run Step is an AzureML Pipeline Step which enables parallel processing or data partitions across multiple workers on multiple nodes. PRS (ParallelRunStep) is designed for embarrassingly parallel workload, e.g. train many models, batch inference, etc.\n\nAlso look into using some of the curated images provided for compute clusters.\nSpecifically look into the DASK image.\n\nCurated environments - Azure Machine Learning | Microsoft Docs",
                "Answer_comment_count":4,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: best compute cluster for training large image datasets !; Content: good morning, i have a a dataset that consist of 99000 (256 x 256 pixels) images. i am trying to use this dataset to training a generative advesarial network (gan) for at least a 1,000 epoch. currently, i am using a standard_nc24r (24 cores, 224 gb ram, 1440 gb disk) gpu (4 x nvidia tesla k80) cluster but the training is slow. it takes about 3000 seconds to train 1 epoch. this implies it would take at least a month to complete training. is a cluster that i can used to speed up training? thanks for your help in advance many thanks roland",
        "Question_original_content_gpt_summary":"The user is trying to use a large image dataset to train a generative adversarial network (GAN) and is looking for a compute cluster to speed up the training process.",
        "Question_preprocessed_content":"Title: best compute cluster for training large image datasets !; Content: good morning, i have a a dataset that consist of images. i am trying to use this dataset to training a generative advesarial network for at least a , epoch. currently, i am using a gpu cluster but the training is slow. it takes about seconds to train epoch. this implies it would take at least a month to complete training. is a cluster that i can used to speed up training? thanks for your help in advance many thanks roland",
        "Answer_original_content":"@okwenrolandt-6377 thanks, instead of bigger machines with more memory, there are techniques to be used with aml compute for larger datasets. the parallel run step is an pipeline step which enables parallel processing or data partitions across multiple workers on multiple nodes. prs (parallelrunstep) is designed for embarrassingly parallel workload, e.g. train many models, batch inference, etc. also look into using some of the curated images provided for compute clusters. specifically look into the dask image. curated environments - | microsoft docs",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include using AML compute for larger datasets, utilizing the parallel run step for parallel processing, and considering using curated images provided for compute clusters, such as the dask image.",
        "Answer_preprocessed_content":"thanks, instead of bigger machines with more memory, there are techniques to be used with aml compute for larger datasets. the parallel run step is an pipeline step which enables parallel processing or data partitions across multiple workers on multiple nodes. prs is designed for embarrassingly parallel workload, train many models, batch inference, etc. also look into using some of the curated images provided for compute clusters. specifically look into the dask image. curated environments microsoft docs"
    },
    {
        "Question_id":null,
        "Question_title":"Error while accessing the dataset from a datastore",
        "Question_body":"I have tried to read the dataset from datastore. Also tried to create the dataset also.\n\nThe code for reading the dataset is below\n\n from azureml.core import Workspace\n ws = Workspace.from_config()\n datastore = Datastore.get(ws, 'qdataset')\n\n\n\nIt works fine still now.\n\n from azureml.core.dataset import Dataset\n six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n\n\n\nAlso i have tried from azureml.core import Dataset\n\nIt shows the following error:\n\n2021-04-29 11:56:47.284077 | ActivityCompleted: Activity=_dataflow, HowEnded=Failure, Duration=0.0 [ms], Info = {'activity_id': 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'activity_name': '_dataflow', 'activity_type': 'InternalCall', 'app_name': 'dataset', 'source': 'azureml.dataset', 'version': '1.27.0', 'dataprepVersion': '2.14.2', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionStatus': 'Failure', 'durationMs': 962.01}, Exception=AttributeError; module 'azureml.dataprep' has no attribute 'api'\n\n\n\n\nAttributeError Traceback (most recent call last)\n<ipython-input-34-ac7a8d35da4d> in <module>\n1 from azureml.core.dataset import Dataset\n----> 2 six_dataset = Dataset.get_by_name(workspace=ws, name='combined_classifier')\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in get_by_name(workspace, name, version)\n87 :rtype: typing.Union[azureml.data.TabularDataset, azureml.data.FileDataset]\n88 \"\"\"\n---> 89 dataset = AbstractDataset._get_by_name(workspace, name, version)\n90 AbstractDataset._track_lineage([dataset])\n91 return dataset\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _get_by_name(workspace, name, version)\n652 if not success:\n653 raise result\n--> 654 dataset = _dto_to_dataset(workspace, result)\n655 warn_deprecated_blocks(dataset)\n656 return dataset\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_dataset_rest_helper.py in _dto_to_dataset(workspace, dto)\n93 registration=registration)\n94 if dto.dataset_type == _DATASET_TYPE_FILE:\n---> 95 return FileDataset._create(\n96 definition=dataflow_json,\n97 properties=dto.latest.properties,\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _create(cls, definition, properties, registration, telemetry_info)\n555 from azureml.data._partition_format import parse_partition_format\n556\n--> 557 steps = dataset._dataflow._get_steps()\n558 partition_keys = []\n559 for step in steps:\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data_loggerfactory.py in wrapper(args, *kwargs)\n127 with LoggerFactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al:\n128 try:\n--> 129 return func(args, *kwargs)\n130 except Exception as e:\n131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'):\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\azureml\\data\\abstract_dataset.py in _dataflow(self)\n215 raise UserErrorException('Dataset definition is missing. Please check how the dataset is created.')\n216 if self._registration and self._registration.workspace:\n--> 217 dataprep().api._datastore_helper._set_auth_type(self._registration.workspace)\n218 if not isinstance(self._definition, dataprep().Dataflow):\n219 try:\n\nAttributeError: module 'azureml.dataprep' has no attribute 'api'\n\n\n\n\n\nPlease give a solution to solve this",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1619698599813,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/377203\/error-while-accessing-the-dataset-from-a-datastore.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-29T13:22:51.567Z",
                "Answer_score":1,
                "Answer_body":"It now worked..\nWe need to install azure-ml-api-sdk using this command\n\npip install azure-ml-api-sdk",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: error while accessing the dataset from a datastore; Content: i have tried to read the dataset from datastore. also tried to create the dataset also. the code for reading the dataset is below from .core import workspace ws = workspace.from_config() datastore = datastore.get(ws, 'qdataset') it works fine still now. from .core.dataset import dataset six_dataset = dataset.get_by_name(workspace=ws, name='combined_classifier') also i have tried from .core import dataset it shows the following error: 2021-04-29 11:56:47.284077 | activitycompleted: activity=_dataflow, howended=failure, duration=0.0 [ms], info = {'activity_id': 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx', 'activity_name': '_dataflow', 'activity_type': 'internalcall', 'app_name': 'dataset', 'source': '.dataset', 'version': '1.27.0', 'dataprepversion': '2.14.2', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': '', 'completionstatus': 'failure', 'durationms': 962.01}, exception=attributeerror; module '.dataprep' has no attribute 'api' attributeerror traceback (most recent call last) in 1 from .core.dataset import dataset ----> 2 six_dataset = dataset.get_by_name(workspace=ws, name='combined_classifier') ~\\appdata\\roaming\\python\\python38\\site-packages\\\\data_loggerfactory.py in wrapper(args, *kwargs) 127 with loggerfactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al: 128 try: --> 129 return func(args, *kwargs) 130 except exception as e: 131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'): ~\\appdata\\roaming\\python\\python38\\site-packages\\\\data\\abstract_dataset.py in get_by_name(workspace, name, version) 87 :rtype: typing.union[.data.tabulardataset, .data.filedataset] 88 \"\"\" ---> 89 dataset = abstractdataset._get_by_name(workspace, name, version) 90 abstractdataset._track_lineage([dataset]) 91 return dataset ~\\appdata\\roaming\\python\\python38\\site-packages\\\\data\\abstract_dataset.py in _get_by_name(workspace, name, version) 652 if not success: 653 raise result --> 654 dataset = _dto_to_dataset(workspace, result) 655 warn_deprecated_blocks(dataset) 656 return dataset ~\\appdata\\roaming\\python\\python38\\site-packages\\\\data_dataset_rest_helper.py in _dto_to_dataset(workspace, dto) 93 registration=registration) 94 if dto.dataset_type == _dataset_type_file: ---> 95 return filedataset._create( 96 definition=dataflow_json, 97 properties=dto.latest.properties, ~\\appdata\\roaming\\python\\python38\\site-packages\\\\data_loggerfactory.py in wrapper(args, *kwargs) 127 with loggerfactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al: 128 try: --> 129 return func(args, *kwargs) 130 except exception as e: 131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'): ~\\appdata\\roaming\\python\\python38\\site-packages\\\\data\\abstract_dataset.py in _create(cls, definition, properties, registration, telemetry_info) 555 from .data._partition_format import parse_partition_format 556 --> 557 steps = dataset._dataflow._get_steps() 558 partition_keys = [] 559 for step in steps: ~\\appdata\\roaming\\python\\python38\\site-packages\\\\data_loggerfactory.py in wrapper(args, *kwargs) 127 with loggerfactory.track_activity(logger, func.name_, activity_type, custom_dimensions) as al: 128 try: --> 129 return func(args, *kwargs) 130 except exception as e: 131 if hasattr(al, 'activity_info') and hasattr(e, 'error_code'): ~\\appdata\\roaming\\python\\python38\\site-packages\\\\data\\abstract_dataset.py in _dataflow(self) 215 raise usererrorexception('dataset definition is missing. please check how the dataset is created.') 216 if self._registration and self._registration.workspace: --> 217 dataprep().api._datastore_helper._set_auth_type(self._registration.workspace) 218 if not isinstance(self._definition, dataprep().dataflow): 219 try: attributeerror: module '.dataprep' has no attribute 'api' please give a solution to solve this",
        "Question_original_content_gpt_summary":"The user encountered an error while attempting to access a dataset from a datastore, and is receiving an \"attributeerror\" with the message that the '.dataprep' module has no attribute 'api'.",
        "Question_preprocessed_content":"Title: error while accessing the dataset from a datastore; Content: i have tried to read the dataset from datastore. also tried to create the dataset also. the code for reading the dataset is below from import workspace ws datastore 'qdataset' it works fine still now. from import dataset also i have tried from import dataset it shows the following error activitycompleted howended failure, , info , exception attributeerror; module has no attribute 'api' attributeerror traceback in from import dataset in wrapper with as al try return func except exception as e if hasattr and hasattr in name, version rtype dataset name, version return dataset in name, version if not success raise result dataset result return dataset in dto registration registration if return in wrapper with as al try return func except exception as e if hasattr and hasattr in definition, properties, registration, from import steps for step in steps in wrapper with as al try return func except exception as e if hasattr and hasattr in raise usererrorexception if and if not try attributeerror module has no attribute 'api' please give a solution to solve this",
        "Answer_original_content":"it now worked.. we need to install azure-ml-api-sdk using this command pip install azure-ml-api-sdk",
        "Answer_original_content_gpt_summary":"Solution: The user needs to install the azure-ml-api-sdk using the command \"pip install azure-ml-api-sdk\" to resolve the \"attributeerror\" issue while accessing the dataset from a datastore.",
        "Answer_preprocessed_content":"it now we need to install using this command pip install"
    },
    {
        "Question_id":34047782.0,
        "Question_title":"Jupyter notebook kernel dies when creating dummy variables with pandas",
        "Question_body":"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the \"FinelineNumber\" column. For context, <code>df.shape<\/code> returns <code>(647054, 7)<\/code>. I am trying to make a dummy column for <code>df['FinelineNumber']<\/code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)<\/code>, which I then plan to <code>concat<\/code> to the original dataframe. <\/p>\n\n<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')<\/code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.<\/code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.<\/p>\n\n<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1449073471520,
        "Question_favorite_count":1.0,
        "Question_score":5.0,
        "Question_view_count":5063.0,
        "Owner_creation_time":1373475615300,
        "Owner_last_access_time":1663780318292,
        "Owner_reputation":2037.0,
        "Owner_up_votes":61.0,
        "Owner_down_votes":1.0,
        "Owner_views":193.0,
        "Answer_body":"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.<\/p>\n\n<p>You might try doing <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/sparse.html\" rel=\"noreferrer\">.to_sparse()<\/a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1449768300887,
        "Answer_score":8.0,
        "Owner_location":"New York, United States",
        "Question_last_edit_time":1454300528203,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34047782",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: jupyter notebook kernel dies when creating dummy variables with pandas; Content: i am working on the walmart kaggle competition and i'm trying to create a dummy column of of the \"finelinenumber\" column. for context, df.shape returns (647054, 7). i am trying to make a dummy column for df['finelinenumber'], which has 5,196 unique values. the results should be a dataframe of shape (647054, 5196), which i then plan to concat to the original dataframe. nearly every time i run fineline_dummies = pd.get_dummies(df['finelinenumber'], prefix='fl'), i get the following error message the kernel appears to have died. it will restart automatically. i am running python 2.7 in jupyter notebook on a macbookpro with 16gb ram. can someone explain why this is happening (and why it happens most of the time but not every time)? is it a jupyter notebook or pandas bug? also, i thought it might have to do with not enough ram but i get the same error on a notebook with >100 gb of ram. on , the kernel dies every time - almost immediately.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where their Jupyter Notebook kernel dies when creating dummy variables with Pandas for a Walmart Kaggle competition.",
        "Question_preprocessed_content":"Title: jupyter notebook kernel dies when creating dummy variables with pandas; Content: i am working on the walmart kaggle competition and i'm trying to create a dummy column of of the finelinenumber column. for context, returns . i am trying to make a dummy column for , which has , unique values. the results should be a dataframe of shape , which i then plan to to the original dataframe. nearly every time i run , i get the following error message i am running python in jupyter notebook on a macbookpro with gb ram. can someone explain why this is happening ? is it a jupyter notebook or pandas bug? also, i thought it might have to do with not enough ram but i get the same error on a notebook with gb of ram. on , the kernel dies every time almost immediately.",
        "Answer_original_content":"it very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24gb just for the pointers to the objects on a 64-bit system. on while the vm has a large amount of memory you're actually limited in how much memory you have available (currently 2gb, soon to be 4gb) - and when you hit the limit the kernel typically dies. so it seems very likely it is a memory usage issue. you might try doing .to_sparse() on the data frame first before doing any additional manipulations. that should allow pandas to keep most of the data frame out of memory.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of Jupyter Notebook kernel dying when creating dummy variables with Pandas for a Walmart Kaggle competition are to check for memory usage and try using .to_sparse() on the data frame before any additional manipulations to keep most of the data frame out of memory.",
        "Answer_preprocessed_content":"it very much could be memory usage a , data frame has , , , elements, which would be gb just for the pointers to the objects on a system. on while the vm has a large amount of memory you're actually limited in how much memory you have available and when you hit the limit the kernel typically dies. so it seems very likely it is a memory usage issue. you might try doing on the data frame first before doing any additional manipulations. that should allow pandas to keep most of the data frame out of memory."
    },
    {
        "Question_id":64597526.0,
        "Question_title":"Provision AKS with internal load balancer from AMLS on Azure",
        "Question_body":"<p>I would like to provision an AKS cluster that is connected to a vnet and has an internal load balancer on Azure. I am using code from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet?tabs=python\" rel=\"nofollow noreferrer\">here<\/a> that looks like this:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Verify that cluster does not exist already\ntry:\n    aks_target = AksCompute(workspace=ws, name=aks_cluster_name)\n    print(&quot;Found existing aks cluster&quot;)\n\nexcept:\n    print(&quot;Creating new aks cluster&quot;)\n\n    # Subnet to use for AKS\n    subnet_name = &quot;default&quot;\n    # Create AKS configuration\n    prov_config=AksCompute.provisioning_configuration(load_balancer_type=&quot;InternalLoadBalancer&quot;)\n    # Set info for existing virtual network to create the cluster in\n    prov_config.vnet_resourcegroup_name = &quot;myvnetresourcegroup&quot;\n    prov_config.vnet_name = &quot;myvnetname&quot;\n    prov_config.service_cidr = &quot;10.0.0.0\/16&quot;\n    prov_config.dns_service_ip = &quot;10.0.0.10&quot;\n    prov_config.subnet_name = subnet_name\n    prov_config.docker_bridge_cidr = &quot;172.17.0.1\/16&quot;\n\n    # Create compute target\n    aks_target = ComputeTarget.create(workspace = ws, name = &quot;myaks&quot;, provisioning_configuration = prov_config)\n    # Wait for the operation to complete\n    aks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>However, I get the following error<\/p>\n<pre><code>K8s failed to assign an IP for Load Balancer after waiting for an hour.\n<\/code><\/pre>\n<p>Is this because the AKS cluster does not yet have a 'network contributor' role for the vnet resource group? Is the only way to get this to work to first create AKS outside of AMLS, grant the network contributor role to the vnet resource group, then attach the AKS cluster to AMLS and configure the internal load balancer afterwards?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1603997735143,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":352.0,
        "Owner_creation_time":1589738451347,
        "Owner_last_access_time":1656358607687,
        "Owner_reputation":179.0,
        "Owner_up_votes":2.0,
        "Owner_down_votes":0.0,
        "Owner_views":53.0,
        "Answer_body":"<p>I was able to get this to work by first creating an AKS resource without an internal load balancer, then separately updating the load balancer following this code:<\/p>\n<pre><code>import azureml.core\nfrom azureml.core.compute.aks import AksUpdateConfiguration\nfrom azureml.core.compute import AksCompute\n\n# ws = workspace object. Creation not shown in this snippet\naks_target = AksCompute(ws,&quot;myaks&quot;)\n\n# Change to the name of the subnet that contains AKS\nsubnet_name = &quot;default&quot;\n# Update AKS configuration to use an internal load balancer\nupdate_config = AksUpdateConfiguration(None, &quot;InternalLoadBalancer&quot;, subnet_name)\naks_target.update(update_config)\n# Wait for the operation to complete\naks_target.wait_for_completion(show_output = True)\n<\/code><\/pre>\n<p>No network contributor role was required.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1604359053332,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1604002322987,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64597526",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: provision aks with internal load balancer from amls on azure; Content: i would like to provision an aks cluster that is connected to a vnet and has an internal load balancer on azure. i am using code from here that looks like this: import .core from .core.compute import akscompute, computetarget # verify that cluster does not exist already try: aks_target = akscompute(workspace=ws, name=aks_cluster_name) print(\"found existing aks cluster\") except: print(\"creating new aks cluster\") # subnet to use for aks subnet_name = \"default\" # create aks configuration prov_config=akscompute.provisioning_configuration(load_balancer_type=\"internalloadbalancer\") # set info for existing virtual network to create the cluster in prov_config.vnet_resourcegroup_name = \"myvnetresourcegroup\" prov_config.vnet_name = \"myvnetname\" prov_config.service_cidr = \"10.0.0.0\/16\" prov_config.dns_service_ip = \"10.0.0.10\" prov_config.subnet_name = subnet_name prov_config.docker_bridge_cidr = \"172.17.0.1\/16\" # create compute target aks_target = computetarget.create(workspace = ws, name = \"myaks\", provisioning_configuration = prov_config) # wait for the operation to complete aks_target.wait_for_completion(show_output = true) however, i get the following error k8s failed to assign an ip for load balancer after waiting for an hour. is this because the aks cluster does not yet have a 'network contributor' role for the vnet resource group? is the only way to get this to work to first create aks outside of amls, grant the network contributor role to the vnet resource group, then attach the aks cluster to amls and configure the internal load balancer afterwards?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in provisioning an AKS cluster connected to a VNET with an internal load balancer on Azure.",
        "Question_preprocessed_content":"Title: provision aks with internal load balancer from amls on azure; Content: i would like to provision an aks cluster that is connected to a vnet and has an internal load balancer on azure. i am using code from here that looks like this however, i get the following error is this because the aks cluster does not yet have a 'network contributor' role for the vnet resource group? is the only way to get this to work to first create aks outside of amls, grant the network contributor role to the vnet resource group, then attach the aks cluster to amls and configure the internal load balancer afterwards?",
        "Answer_original_content":"i was able to get this to work by first creating an aks resource without an internal load balancer, then separately updating the load balancer following this code: import .core from .core.compute.aks import aksupdateconfiguration from .core.compute import akscompute # ws = workspace object. creation not shown in this snippet aks_target = akscompute(ws,\"myaks\") # change to the name of the subnet that contains aks subnet_name = \"default\" # update aks configuration to use an internal load balancer update_config = aksupdateconfiguration(none, \"internalloadbalancer\", subnet_name) aks_target.update(update_config) # wait for the operation to complete aks_target.wait_for_completion(show_output = true) no network contributor role was required.",
        "Answer_original_content_gpt_summary":"The solution to the challenge of provisioning an AKS cluster connected to a VNET with an internal load balancer on Azure involves creating an AKS resource without an internal load balancer, and then separately updating the load balancer using the provided code. The code involves importing .core from .core.compute.aks and .core.compute.akscompute, and then updating the AKS configuration to use an internal load balancer. No network contributor role is required.",
        "Answer_preprocessed_content":"i was able to get this to work by first creating an aks resource without an internal load balancer, then separately updating the load balancer following this code no network contributor role was required."
    },
    {
        "Question_id":null,
        "Question_title":"build an AZURE ML enviroment from docker image in dockerhub",
        "Question_body":"I want to use https:\/\/hub.docker.com\/layers\/shi2yu3\/mainz\/v7_nccl2804\/images\/sha256-298ca59f25e56ff8d0f3faeb7493618ff7d69149e6a92283ed3176a215b8da8d?context=explore in Azure ML as an environment for model training and testing. Any idea how to do this?",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1635963924763,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/614519\/build-an-azure-ml-enviroment-from-docker-image-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-05T10:56:13.927Z",
                "Answer_score":0,
                "Answer_body":"@MiaHu-2196 I think your scenario needs to use a custom docker image for training. In that case, you can refer to the steps in this documentation to use this image in your build.\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: build an enviroment from docker image in dockerhub; Content: i want to use https:\/\/hub.docker.com\/layers\/shi2yu3\/mainz\/v7_nccl2804\/images\/sha256-298ca59f25e56ff8d0f3faeb7493618ff7d69149e6a92283ed3176a215b8da8d?context=explore in as an environment for model training and testing. any idea how to do this?",
        "Question_original_content_gpt_summary":"The user is looking to use a Docker image from DockerHub to build an environment for model training and testing.",
        "Question_preprocessed_content":"Title: build an enviroment from docker image in dockerhub; Content: i want to use in as an environment for model training and testing. any idea how to do this?",
        "Answer_original_content":"@miahu-2196 i think your scenario needs to use a custom docker image for training. in that case, you can refer to the steps in this documentation to use this image in your build. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Use a custom Docker image for training.\n- Refer to the steps in the provided documentation to use the custom image in the build.",
        "Answer_preprocessed_content":"i think your scenario needs to use a custom docker image for training. in that case, you can refer to the steps in this documentation to use this image in your build. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":null,
        "Question_title":"SSML to read Date in German Language not working",
        "Question_body":"Hi - I am working on converting Text to Speech using SSML via Google Speech API. Below is the request to the API. This perfectly works when language Code is En-US , however for code de-DE and to hear in German voice, the output is totally random. Please help me in checking this issue,  TTS Request JSON :: {\"voice\":{\"ssmlGender\":\"MALE\",\"name\":\"de-DE-Wavenet-E\",\"languageCode\":\"de-DE\"},\"input\":{\"ssml\":\"<speak><say-as interpret-as=\\\"date\\\" format=\\\"yyyymmdd\\\"> 20220506<\\\/say-as><\\\/speak>\"},\"audioConfig\":{\"sampleRateHertz\":8000,\"volumeGainDb\":0,\"speakingRate\":1,\"audioEncoding\":\"LINEAR16\",\"pitch\":0,\"effectsProfileId\":[\"telephony-class-application\"]}} The same request when changeing the name and Language code works perfectly.",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1650551940000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":77.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/SSML-to-read-Date-in-German-Language-not-working\/td-p\/415908\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-06T07:56:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"You will want to see this documentation where it\u2019s explained how to use different languages instead of english in SSML they also provide some code examples as the one i\u2019m sharing where it explains how to use the Voice tag in SSML.\n\n\u00a0\n\n<speak>The dog is friendly<voice name=\"fr-CA-Wavenet-B\">mais la chat est\u00a0mignon<\/voice><break time=\"250ms\"\/> said a pet shop\u00a0owner<\/speak>"
            },
            {
                "Answer_creation_time":"2022-10-22T09:24:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi Eduardo,\n\nyou may have a look at this as well. The google asr has still problems with german date transcription:\n\nhttps:\/\/stackoverflow.com\/questions\/66799469\/google-speech-recognition-weak-date-transcription\n\nBest Regards\n\nAndre"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: ssml to read date in german language not working; Content: hi - i am working on converting text to speech using ssml via google speech api. below is the request to the api. this perfectly works when language code is en-us , however for code de-de and to hear in german voice, the output is totally random. please help me in checking this issue, tts request json :: {\"voice\":{\"ssmlgender\":\"male\",\"name\":\"de-de-wavenet-e\",\"languagecode\":\"de-de\"},\"input\":{\"ssml\":\" 20220506<\\\/say-as><\\\/speak>\"},\"audioconfig\":{\"sampleratehertz\":8000,\"volumegaindb\":0,\"speakingrate\":1,\"audioencoding\":\"linear16\",\"pitch\":0,\"effectsprofileid\":[\"telephony-class-application\"]}} the same request when changeing the name and language code works perfectly.",
        "Question_original_content_gpt_summary":"The user is encountering an issue with the Google Speech API where the SSML to read a date in German language is not working correctly, resulting in random output.",
        "Question_preprocessed_content":"Title: ssml to read date in german language not working; Content: hi i am working on converting text to speech using ssml via google speech api. below is the request to the api. this perfectly works when language code is , however for code and to hear in german voice, the output is totally random. please help me in checking this issue, tts request json , input ssml the same request when changeing the name and language code works perfectly.",
        "Answer_original_content":"you will want to see this documentation where its explained how to use different languages instead of english in ssml they also provide some code examples as the one im sharing where it explains how to use the voice tag in ssml. the dog is friendlymais la chat estmignon said a pet shopowner hi eduardo, you may have a look at this as well. the google asr has still problems with german date transcription: https:\/\/stackoverflow.com\/questions\/66799469\/google-speech-recognition-weak-date-transcription best regards andre",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. Refer to the documentation provided by Google Speech API to use different languages in SSML and use the voice tag in SSML.\n2. Check out the link provided by the answerer to see if there are any updates on the issue with German date transcription in Google Speech Recognition.",
        "Answer_preprocessed_content":"you will want to see this documentation where its explained how to use different languages instead of english in ssml they also provide some code examples as the one im sharing where it explains how to use the voice tag in ssml. the dog is friendlymais la chat estmignon said a pet shopowner hi eduardo, you may have a look at this as well. the google asr has still problems with german date transcription best regards andre"
    },
    {
        "Question_id":51533650.0,
        "Question_title":"No space left on device in Sagemaker model training",
        "Question_body":"<p>I'm using custom algorithm running shipped with Docker image on p2 instance with AWS Sagemaker (a bit similar to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>)<\/p>\n\n<p>At the end of training process, I try to write down my model to output directory, that is mounted via Sagemaker (like in tutorial), like this:<\/p>\n\n<pre><code>model_path = \"\/opt\/ml\/model\"\nmodel.save(os.path.join(model_path, 'model.h5'))\n<\/code><\/pre>\n\n<p>Unluckily, apparently the model gets too big with time and I get the\nfollowing error:<\/p>\n\n<blockquote>\n  <p>RuntimeError: Problems closing file (file write failed: time = Thu Jul\n  26 00:24:48 2018<\/p>\n  \n  <p>00:24:49 , filename = 'model.h5', file descriptor = 22, errno = 28,\n  error message = 'No space left on device', buf = 0x1a41d7d0, total\n  write[...]<\/p>\n<\/blockquote>\n\n<p>So all my hours of GPU time are wasted. How can I prevent this from happening again? Does anyone know what is the size limit for model that I store on Sagemaker\/mounted directories?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1532591463817,
        "Question_favorite_count":1.0,
        "Question_score":2.0,
        "Question_view_count":2721.0,
        "Owner_creation_time":1366408339740,
        "Owner_last_access_time":1663750023060,
        "Owner_reputation":8057.0,
        "Owner_up_votes":1123.0,
        "Owner_down_votes":72.0,
        "Owner_views":491.0,
        "Answer_body":"<p>When you train a model with <code>Estimators<\/code>, it <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/estimators.html\" rel=\"nofollow noreferrer\">defaults to 30 GB of storage<\/a>, which may not be enough. You can use the <code>train_volume_size<\/code> param on the constructor to increase this value. Try with a large-ish number (like 100GB) and see how big your model is. In subsequent jobs, you can tune down the value to something closer to what you actually need.<\/p>\n\n<p>Storage costs <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/\" rel=\"nofollow noreferrer\">$0.14 per GB-month of provisioned storage<\/a>. Partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1539631654852,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51533650",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: no space left on device in model training; Content: i'm using custom algorithm running shipped with docker image on p2 instance with (a bit similar to https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb) at the end of training process, i try to write down my model to output directory, that is mounted via (like in tutorial), like this: model_path = \"\/opt\/ml\/model\" model.save(os.path.join(model_path, 'model.h5')) unluckily, apparently the model gets too big with time and i get the following error: runtimeerror: problems closing file (file write failed: time = thu jul 26 00:24:48 2018 00:24:49 , filename = 'model.h5', file descriptor = 22, errno = 28, error message = 'no space left on device', buf = 0x1a41d7d0, total write[...] so all my hours of gpu time are wasted. how can i prevent this from happening again? does anyone know what is the size limit for model that i store on \/mounted directories?",
        "Question_original_content_gpt_summary":"The user encountered a challenge of having no space left on the device while attempting to save a model to an output directory mounted via a tutorial, resulting in wasted GPU time.",
        "Question_preprocessed_content":"Title: no space left on device in model training; Content: i'm using custom algorithm running shipped with docker image on p instance with at the end of training process, i try to write down my model to output directory, that is mounted via , like this unluckily, apparently the model gets too big with time and i get the following error runtimeerror problems closing file file write failed time thu jul , filename file descriptor , errno , error message 'no space left on device', buf x a d d , total so all my hours of gpu time are wasted. how can i prevent this from happening again? does anyone know what is the size limit for model that i store on directories?",
        "Answer_original_content":"when you train a model with estimators, it defaults to 30 gb of storage, which may not be enough. you can use the train_volume_size param on the constructor to increase this value. try with a large-ish number (like 100gb) and see how big your model is. in subsequent jobs, you can tune down the value to something closer to what you actually need. storage costs $0.14 per gb-month of provisioned storage. partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n- Increase the train_volume_size parameter on the constructor to allocate more storage for the model training process.\n- Try with a large-ish number (like 100gb) and see how big the model is.\n- In subsequent jobs, tune down the value to something closer to what is actually needed.\n- Giving some extra room is a cheap insurance policy against running out of storage, as storage costs $0.14 per gb-month of provisioned storage and partial usage is prorated.",
        "Answer_preprocessed_content":"when you train a model with , it defaults to gb of storage, which may not be enough. you can use the param on the constructor to increase this value. try with a number and see how big your model is. in subsequent jobs, you can tune down the value to something closer to what you actually need. storage costs per of provisioned storage. partial usage is prorated, so giving yourself some extra room is a cheap insurance policy against running out of storage."
    },
    {
        "Question_id":null,
        "Question_title":"Listing files in the remote storage",
        "Question_body":"<p>Hi everyone,<\/p>\n<p>We have pushed artifacts to remote storage (S3 bucket in our case). They got renamed and appear in S3 the way which makes it impossible to tell artifacts apart.<\/p>\n<p>We would like to delete some artifacts that are not used anymore. But the bucket is used by multiple projects, so we cannot figure out what is what.<br>\nHow to list the artifacts on the remote server?<\/p>\n<p><code>dvs remote list<\/code> only lists storages, not the contents of the storage.<\/p>\n<hr>\n<p>This is also connected to a question of the best practice on organizing remote storages: is it better to have separate S3 buckets per project? Or one bucket, but separate folder for each artifact?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1612427783386,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":667.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/listing-files-in-the-remote-storage\/654",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-04T09:20:40.575Z",
                "Answer_body":"<p>Another related question: how not to make the storage a mess? After some time it looks like a bunch of md5 named files\/folders. Many of then can be orphans. How to approach this? Any tips\/best practices?<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-04T12:51:45.162Z",
                "Answer_body":"<p>Hi, <a class=\"mention\" href=\"\/u\/versus\">@Versus<\/a>. You could separate the remote storage and cache by namespacing them.<\/p>\n<p>You could add a DVC remote as a <code>s3:\/\/my-bucket\/dvc-remote<\/code> and then everything will have that prefix. You could create multiple DVC remotes this way.<\/p>\n<blockquote>\n<p><code>dvs remote list<\/code> only lists storages, not the contents of the storage.<\/p>\n<\/blockquote>\n<p>It is better to use remote specific tools to visualize this. <code>aws s3 ls<\/code> can help you here.<\/p>\n<blockquote>\n<p>is it better to have separate S3 buckets per project? Or one bucket, but separate folder for each artifact?<\/p>\n<\/blockquote>\n<p>It depends on your preference and requirements, as DVC does not have any particular requirements.<br>\nYou can use single or multiple buckets as you like. Or, use a namespacing with prefix and enable ACL.<br>\nYou can even share a single remote across all of the projects. In that case, DVC will deduplicate your artefacts and data, but you need to be careful when using <code>dvc gc<\/code>.<\/p>",
                "Answer_score":11.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-04T14:08:44.732Z",
                "Answer_body":"<p>Thank you for your reply.<br>\n<code>aws s3 ls<\/code> shows md5 named files and folders. In case we have some orphan files\/folders there, we cannot figure out what can be deleted. Do you have a tip on how to fight this problem?<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-04T14:39:55.802Z",
                "Answer_body":"<p>There\u2019s <code>dvc gc --cloud<\/code> that does that, but as the files are mixed with the cache, I\u2019d suggest you  push to a different remote  and then do something similar to the following on most of the md5 directories and remove manually:<\/p>\n<pre><code class=\"lang-auto\">aws s3 rm  --recursive s3:\/\/my-bucket\/d3\/\n<\/code><\/pre>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: listing files in the remote storage; Content: hi everyone, we have pushed artifacts to remote storage (s3 bucket in our case). they got renamed and appear in s3 the way which makes it impossible to tell artifacts apart. we would like to delete some artifacts that are not used anymore. but the bucket is used by multiple projects, so we cannot figure out what is what. how to list the artifacts on the remote server? dvs remote list only lists storages, not the contents of the storage. this is also connected to a question of the best practice on organizing remote storages: is it better to have separate s3 buckets per project? or one bucket, but separate folder for each artifact? thanks!",
        "Question_original_content_gpt_summary":"The user is facing challenges in listing files in their remote storage, as well as determining the best practice for organizing remote storages.",
        "Question_preprocessed_content":"Title: listing files in the remote storage; Content: hi everyone, we have pushed artifacts to remote storage . they got renamed and appear in s the way which makes it impossible to tell artifacts apart. we would like to delete some artifacts that are not used anymore. but the bucket is used by multiple projects, so we cannot figure out what is what. how to list the artifacts on the remote server? only lists storages, not the contents of the storage. this is also connected to a question of the best practice on organizing remote storages is it better to have separate s buckets per project? or one bucket, but separate folder for each artifact? thanks!",
        "Answer_original_content":"another related question: how not to make the storage a mess? after some time it looks like a bunch of md5 named files\/folders. many of then can be orphans. how to approach this? any tips\/best practices? hi, @versus. you could separate the remote storage and cache by namespacing them. you could add a remote as a s3:\/\/my-bucket\/-remote and then everything will have that prefix. you could create multiple remotes this way. dvs remote list only lists storages, not the contents of the storage. it is better to use remote specific tools to visualize this. aws s3 ls can help you here. is it better to have separate s3 buckets per project? or one bucket, but separate folder for each artifact? it depends on your preference and requirements, as does not have any particular requirements. you can use single or multiple buckets as you like. or, use a namespacing with prefix and enable acl. you can even share a single remote across all of the projects. in that case, will deduplicate your artefacts and data, but you need to be careful when using gc. thank you for your reply. aws s3 ls shows md5 named files and folders. in case we have some orphan files\/folders there, we cannot figure out what can be deleted. do you have a tip on how to fight this problem? theres gc --cloud that does that, but as the files are mixed with the cache, id suggest you push to a different remote and then do something similar to the following on most of the md5 directories and remove manually: aws s3 rm --recursive s3:\/\/my-bucket\/d3\/",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include: \n\n1. Namespacing remote storage and cache to organize them better.\n2. Using remote-specific tools like aws s3 ls to visualize the contents of the storage.\n3. Deciding whether to use separate s3 buckets per project or a single bucket with separate folders for each artifact based on preference and requirements.\n4. Enabling acl and sharing a single remote across all projects to deduplicate artifacts and data.\n5. Using gc --cloud to remove orphan files\/folders, but pushing them to a different remote first to avoid mixing them with the cache.",
        "Answer_preprocessed_content":"another related question how not to make the storage a mess? after some time it looks like a bunch of md named many of then can be orphans. how to approach this? any practices? hi, you could separate the remote storage and cache by namespacing them. you could add a remote as a and then everything will have that prefix. you could create multiple remotes this way. only lists storages, not the contents of the storage. it is better to use remote specific tools to visualize this. can help you here. is it better to have separate s buckets per project? or one bucket, but separate folder for each artifact? it depends on your preference and requirements, as does not have any particular requirements. you can use single or multiple buckets as you like. or, use a namespacing with prefix and enable acl. you can even share a single remote across all of the projects. in that case, will deduplicate your artefacts and data, but you need to be careful when using . thank you for your reply. shows md named files and folders. in case we have some orphan there, we cannot figure out what can be deleted. do you have a tip on how to fight this problem? theres that does that, but as the files are mixed with the cache, id suggest you push to a different remote and then do something similar to the following on most of the md directories and remove manually"
    },
    {
        "Question_id":65699980.0,
        "Question_title":"Change model file save location on AWS SageMaker Training Job",
        "Question_body":"<p>I am trying to run custom python\/sklearn sagemaker script on AWS, basically learning from these examples: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>All works fine, if define the arguments, train the model and output the file:<\/p>\n<pre><code>parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\nparser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\nparser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n# train the model...\n\njoblib.dump(model, os.path.join(args.model_dir, &quot;model.joblib&quot;))\n<\/code><\/pre>\n<p>And call the job with:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test'}, wait=False)\n<\/code><\/pre>\n<p>In this case model gets stored on different auto-generated bucket, which I do not want. I want to get the output (.joblib file) in the same s3 bucket I took data from. So I add the parameter <code>model-dir<\/code>:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test', `model-dir`: 's3:\/\/path\/to\/model'}, wait=False)\n<\/code><\/pre>\n<p>But it results in error:\n<code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/path\/to\/model\/model.joblib'<\/code><\/p>\n<p>Same happens if I hardcode the output path inside the training script.<\/p>\n<p>So the main question, how can I get the output file in the bucket of my choice?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1610533493827,
        "Question_favorite_count":0.0,
        "Question_score":2.0,
        "Question_view_count":1244.0,
        "Owner_creation_time":1572957474856,
        "Owner_last_access_time":1626866876556,
        "Owner_reputation":123.0,
        "Owner_up_votes":6.0,
        "Owner_down_votes":0.0,
        "Owner_views":18.0,
        "Answer_body":"<p>You can use parameter <code>output_path<\/code> when you define the estimator. If you use the\n<code>model_dir<\/code> I guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on S3). You can take a look at my <a href=\"https:\/\/github.com\/roccopietrini\/TFSagemakerDetection\" rel=\"nofollow noreferrer\">repo<\/a> for this specific case.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1610545645387,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65699980",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: change model file save location on training job; Content: i am trying to run custom python\/sklearn script on aws, basically learning from these examples: https:\/\/github.com\/aws\/amazon--examples\/blob\/master\/-python-sdk\/scikit_learn_randomforest\/sklearn_on__end2end.ipynb all works fine, if define the arguments, train the model and output the file: parser.add_argument('--model-dir', type=str, default=os.environ.get('sm_model_dir')) parser.add_argument('--train', type=str, default=os.environ.get('sm_channel_train')) parser.add_argument('--test', type=str, default=os.environ.get('sm_channel_test')) # train the model... joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\")) and call the job with: aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test'}, wait=false) in this case model gets stored on different auto-generated bucket, which i do not want. i want to get the output (.joblib file) in the same s3 bucket i took data from. so i add the parameter model-dir: aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test', `model-dir`: 's3:\/\/path\/to\/model'}, wait=false) but it results in error: filenotfounderror: [errno 2] no such file or directory: 's3:\/\/path\/to\/model\/model.joblib' same happens if i hardcode the output path inside the training script. so the main question, how can i get the output file in the bucket of my choice?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in trying to change the model file save location on a training job in AWS, resulting in a \"FileNotFoundError\" when attempting to save the model.",
        "Question_preprocessed_content":"Title: change model file save location on training job; Content: i am trying to run custom script on aws, basically learning from these examples all works fine, if define the arguments, train the model and output the file and call the job with in this case model gets stored on different bucket, which i do not want. i want to get the output in the same s bucket i took data from. so i add the parameter but it results in error same happens if i hardcode the output path inside the training script. so the main question, how can i get the output file in the bucket of my choice?",
        "Answer_original_content":"you can use parameter output_path when you define the estimator. if you use the model_dir i guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on s3). you can take a look at my repo for this specific case.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of changing the model file save location on a training job in AWS and encountering a \"FileNotFoundError\" include using the parameter output_path when defining the estimator or creating the model_dir bucket beforehand. Using model_dir has the advantage of saving artifacts in real-time during training if the instance has rights on S3.",
        "Answer_preprocessed_content":"you can use parameter when you define the estimator. if you use the i guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training . you can take a look at my repo for this specific case."
    },
    {
        "Question_id":null,
        "Question_title":"About the end of Machine Learning Studio (classic)#2",
        "Question_body":"hello.\n\nI am currently using Machine Learning Studio (classic).\n\n'From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) experiments and web services. Beginning 1 December 2021, new creation of Machine Learning Studio (classic) resources will not be available.\n\nIs the following interpretation correct?\n\nThings you can't do from 1 December 2021\n-Creating a workspace for Machine Learning Studio (classic)\n-Creating a web service plan for Machine Learning Studio (classic)\n\nWhat you can do until 1 December 2021\n-Creating new Machine Learning Studio (classic) experiments\n-Creating new Machine Learning Studio (classic) trained models\n-Creating a new Machine Learning Studio (classic) web service",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1636119906810,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/616952\/about-the-end-of-machine-learning-studio-classic2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-06T14:35:42.13Z",
                "Answer_score":0,
                "Answer_body":"Hi, customers will not be able to create new ML Studio(classic) workspaces after Dec 1, 2021. However, customers can create or update experiments\/web services in existing workspaces until Aug 31, 2024.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: about the end of machine learning studio (classic)#2; Content: hello. i am currently using machine learning studio (classic). 'from now through 31 august 2024, you can continue to use the existing machine learning studio (classic) experiments and web services. beginning 1 december 2021, new creation of machine learning studio (classic) resources will not be available. is the following interpretation correct? things you can't do from 1 december 2021 -creating a workspace for machine learning studio (classic) -creating a web service plan for machine learning studio (classic) what you can do until 1 december 2021 -creating new machine learning studio (classic) experiments -creating new machine learning studio (classic) trained models -creating a new machine learning studio (classic) web service",
        "Question_original_content_gpt_summary":"The user is encountering challenges understanding the timeline for the end of Machine Learning Studio (Classic) and what they can and cannot do until 1 December 2021.",
        "Question_preprocessed_content":"Title: about the end of machine learning studio ; Content: hello. i am currently using machine learning studio . 'from now through august , you can continue to use the existing machine learning studio experiments and web services. beginning december , new creation of machine learning studio resources will not be available. is the following interpretation correct? things you can't do from december creating a workspace for machine learning studio creating a web service plan for machine learning studio what you can do until december creating new machine learning studio experiments creating new machine learning studio trained models creating a new machine learning studio web service",
        "Answer_original_content":"hi, customers will not be able to create new ml studio(classic) workspaces after dec 1, 2021. however, customers can create or update experiments\/web services in existing workspaces until aug 31, 2024. --- kindly accept answer if the information helps. thanks.",
        "Answer_original_content_gpt_summary":"Summary: Customers can no longer create new Machine Learning Studio (Classic) workspaces after December 1, 2021. However, they can still create or update experiments and web services in existing workspaces until August 31, 2024.",
        "Answer_preprocessed_content":"hi, customers will not be able to create new ml studio workspaces after dec , . however, customers can create or update services in existing workspaces until aug , . kindly accept answer if the information helps. thanks."
    },
    {
        "Question_id":null,
        "Question_title":"How is AML's average GpuUtilization metric computed?",
        "Question_body":"How is the \"GpuUtilization\" metric computed for an AML workspace? What are the inputs and what is the equation used to compute GpuUtilization?\n\nThe \"metrics\" tab in the AML web portal shows a chart of the GpuUtilization over a specified time period, along with the average GpuUtilization for that time period. However, I have found that average GpuUtilization does not appear to accurately reflect the data shown in the chart for some of my organization's AML workspaces.\n\nFor example, the following screenshot shows the GpuUtilization for July 1-31, with the average GpuUtilization reported as 54.06. This is clearly much higher than what is shown in the chart. When I download the data from the chart (Share -> Download to Excel), I compute the average GpuUtilization to be ~11% in Excel. Why is there such a discrepancy?\n\nI have found similar discrepancies for other AML workspaces as well. However, the average GpuUtilization appears to be more accurate for the August 1-25 time period than it is for July 1-31. I wish to better understand how AML computes the average GpuUtilization over a time period so we can accurately account for my organization's AML GPU usage on a per-workspace basis.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1598450649953,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/77678\/how-is-aml39s-gpuutilization-metric-computed.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-27T14:49:02.87Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. GpuUtilization shows how much percentage of GPU was utilized for a given node during a run\/job. One node can have one or more GPUs. This metric is published per GPU per node. You can apply filters based on node to understand the computation better. Let me know if that helps or if you need further assistance. Thanks.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how is aml's average gpuutilization metric computed?; Content: how is the \"gpuutilization\" metric computed for an aml workspace? what are the inputs and what is the equation used to compute gpuutilization? the \"metrics\" tab in the aml web portal shows a chart of the gpuutilization over a specified time period, along with the average gpuutilization for that time period. however, i have found that average gpuutilization does not appear to accurately reflect the data shown in the chart for some of my organization's aml workspaces. for example, the following screenshot shows the gpuutilization for july 1-31, with the average gpuutilization reported as 54.06. this is clearly much higher than what is shown in the chart. when i download the data from the chart (share -> download to excel), i compute the average gpuutilization to be ~11% in excel. why is there such a discrepancy? i have found similar discrepancies for other aml workspaces as well. however, the average gpuutilization appears to be more accurate for the august 1-25 time period than it is for july 1-31. i wish to better understand how aml computes the average gpuutilization over a time period so we can accurately account for my organization's aml gpu usage on a per-workspace basis.",
        "Question_original_content_gpt_summary":"The user is encountering discrepancies between the average GPU utilization metric reported in the AML web portal and the average GPU utilization metric computed from the data downloaded from the chart, and is seeking to better understand how AML computes the average GPU utilization metric over a time period.",
        "Question_preprocessed_content":"Title: how is aml's average gpuutilization metric computed?; Content: how is the gpuutilization metric computed for an aml workspace? what are the inputs and what is the equation used to compute gpuutilization? the metrics tab in the aml web portal shows a chart of the gpuutilization over a specified time period, along with the average gpuutilization for that time period. however, i have found that average gpuutilization does not appear to accurately reflect the data shown in the chart for some of my organization's aml workspaces. for example, the following screenshot shows the gpuutilization for july with the average gpuutilization reported as this is clearly much higher than what is shown in the chart. when i download the data from the chart , i compute the average gpuutilization to be % in excel. why is there such a discrepancy? i have found similar discrepancies for other aml workspaces as well. however, the average gpuutilization appears to be more accurate for the august time period than it is for july i wish to better understand how aml computes the average gpuutilization over a time period so we can accurately account for my organization's aml gpu usage on a basis.",
        "Answer_original_content":"hi, thanks for reaching out. gpuutilization shows how much percentage of gpu was utilized for a given node during a run\/job. one node can have one or more gpus. this metric is published per gpu per node. you can apply filters based on node to understand the computation better. let me know if that helps or if you need further assistance. thanks.",
        "Answer_original_content_gpt_summary":"The answer provides a possible solution to the user's problem by suggesting that they apply filters based on node to better understand how AML computes the average GPU utilization metric over a time period.",
        "Answer_preprocessed_content":"hi, thanks for reaching out. gpuutilization shows how much percentage of gpu was utilized for a given node during a one node can have one or more gpus. this metric is published per gpu per node. you can apply filters based on node to understand the computation better. let me know if that helps or if you need further assistance. thanks."
    },
    {
        "Question_id":null,
        "Question_title":"Which azure service should I use.",
        "Question_body":"So I have a pretty big on-premise ssms database and I want to use some data from it in azure Machine Learning.\nI need to use just a small amount of data from my db, from certain tables.\nAlso this data updates from time to time.\nI want you to help me with the choice of correct azure service for my purpose.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1633510606170,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/579427\/which-azure-service-should-i-use.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-06T10:17:26.467Z",
                "Answer_score":0,
                "Answer_body":"@CherkasovDmitriy-9587 Welcome to Microsoft Q&A forums.\n\nFrom the description of your use case, I understand that the database won't be used round the clock but in short bursts of time, when you want to your ML workloads or while updating the tables from your on-premises database.\n\nYou might want to consider Azure SQL Serverless database.\nYou will be billed only for the time the database is in use and it automatically pauses itself during inactive periods.\n\nPlease let us know if you have any further questions.\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: which azure service should i use.; Content: so i have a pretty big on-premise ssms database and i want to use some data from it in . i need to use just a small amount of data from my db, from certain tables. also this data updates from time to time. i want you to help me with the choice of correct azure service for my purpose.",
        "Question_original_content_gpt_summary":"The user needs help choosing the correct Azure service to use for accessing a small amount of data from an on-premise SSMS database that updates from time to time.",
        "Question_preprocessed_content":"Title: which azure service should i use.; Content: so i have a pretty big ssms database and i want to use some data from it in . i need to use just a small amount of data from my db, from certain tables. also this data updates from time to time. i want you to help me with the choice of correct azure service for my purpose.",
        "Answer_original_content":"@cherkasovdmitriy-9587 welcome to microsoft q&a forums. from the description of your use case, i understand that the database won't be used round the clock but in short bursts of time, when you want to your ml workloads or while updating the tables from your on-premises database. you might want to consider azure sql serverless database. you will be billed only for the time the database is in use and it automatically pauses itself during inactive periods. please let us know if you have any further questions. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"Possible solutions for accessing a small amount of data from an on-premise SSMS database that updates from time to time are to consider using Azure SQL Serverless Database. This service bills only for the time the database is in use and automatically pauses itself during inactive periods.",
        "Answer_preprocessed_content":"welcome to microsoft q&a forums. from the description of your use case, i understand that the database won't be used round the clock but in short bursts of time, when you want to your ml workloads or while updating the tables from your database. you might want to consider azure sql serverless database. you will be billed only for the time the database is in use and it automatically pauses itself during inactive periods. please let us know if you have any further questions. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":null,
        "Question_title":"How to log custom criterion function?",
        "Question_body":"<p>We can use <code>wandb.watch(model, criterion, ...)<\/code> in order to log a model + a loss function.<br>\nBut my loss function is not something simple like: <code>criterion = nn.CrossEntropyLoss()<\/code>.<\/p>\n<p>Rather, here\u2019s how I calculate my loss:<\/p>\n<pre><code class=\"lang-auto\">            # `set_to_none=True` boosts performance\n            optimizer.zero_grad(set_to_none=True)\n            masks_pred = model(imgs)\n\n            probs = F.softmax(masks_pred, dim=1).float()\n            ground_truth = F.one_hot(masks, model.n_classes).permute(0, 3, 1, 2).float()\n\n            loss = criterion(masks_pred, masks) + dice_loss(probs, ground_truth)\n            loss.backward()\n            optimizer.step()\n<\/code><\/pre>\n<p>As you can see, the loss is a composition of 2 functions: the criterion and the <code>dice_loss<\/code> function.<br>\nWhat should I pass to <code>wandb.watch<\/code> for the <code>criterion<\/code> argument?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1657061987579,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":73.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-log-custom-criterion-function\/2703",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-08T18:53:54.861Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vroomerify\">@vroomerify<\/a>,<\/p>\n<p>Thanks for reaching out. <code>wandb.watch<\/code> expects a torch function as a criterion parameter. You can set up a custom criterion function by subclassing <code>torch.nn.Module<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-13T23:00:09.811Z",
                "Answer_body":"<p>Hi Vedant,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-19T18:13:41.309Z",
                "Answer_body":"<p>Hi Vedant, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-06T18:54:20.439Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to log custom criterion function?; Content: we can use .watch(model, criterion, ...) in order to log a model + a loss function. but my loss function is not something simple like: criterion = nn.crossentropyloss(). rather, here\u2019s how i calculate my loss: # `set_to_none=true` boosts performance optimizer.zero_grad(set_to_none=true) masks_pred = model(imgs) probs = f.softmax(masks_pred, dim=1).float() ground_truth = f.one_hot(masks, model.n_classes).permute(0, 3, 1, 2).float() loss = criterion(masks_pred, masks) + dice_loss(probs, ground_truth) loss.backward() optimizer.step() as you can see, the loss is a composition of 2 functions: the criterion and the dice_loss function. what should i pass to .watch for the criterion argument?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in logging a custom criterion function composed of two functions, a criterion and a dice_loss function, when using the .watch(model, criterion, ...) method.",
        "Question_preprocessed_content":"Title: how to log custom criterion function?; Content: we can use in order to log a model + a loss function. but my loss function is not something simple like . rather, heres how i calculate my loss as you can see, the loss is a composition of functions the criterion and the function. what should i pass to for the argument?",
        "Answer_original_content":"hi @vroomerify, thanks for reaching out. .watch expects a torch function as a criterion parameter. you can set up a custom criterion function by subclassing torch.nn.module. thanks, ramit",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of logging a custom criterion function composed of two functions, a criterion and a dice_loss function, when using the .watch(model, criterion, ...) method are: \n- Subclassing torch.nn.module to set up a custom criterion function\n- Using a torch function as a criterion parameter for the .watch method.",
        "Answer_preprocessed_content":"hi thanks for reaching out. expects a torch function as a criterion parameter. you can set up a custom criterion function by subclassing . thanks, ramit"
    },
    {
        "Question_id":null,
        "Question_title":"Accidentally deleted user account in local - is there any way to recover user or projects?",
        "Question_body":"<p>While investigating an unrelated issue I incorrectly thought there was something wrong with the user as the local instance was having issues in the GUI. I tried a lot of troubleshooting steps, one of which I think included an attempt to recreate the account.<\/p>\n<p>Is there a way to recover the data associated with the old account or to recover the account itself? I tried to dig around myself and found entries relating to the old user in the mysql database and was hopeful there was a way I could recreate the account or recover the data to a new user in our local instance.  Is this at all possible?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1648474628027,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":105.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/accidentally-deleted-user-account-in-local-is-there-any-way-to-recover-user-or-projects\/2163",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-28T20:02:46.441Z",
                "Answer_body":"<p>A shot in the dark but If I deleted the old user_id and entity_id from the users and entities table and updated the new account to have the old user_id \/ entity_id would that potentially work?<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-29T11:36:37.444Z",
                "Answer_body":"<p>Hey Andrew,<\/p>\n<p>I\u2019ll double-check with the team to see if there is a way to recover the account and get back to you.<\/p>\n<p>Best,<br>\nArman<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-27T20:03:39.316Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: accidentally deleted user account in local - is there any way to recover user or projects?; Content: while investigating an unrelated issue i incorrectly thought there was something wrong with the user as the local instance was having issues in the gui. i tried a lot of troubleshooting steps, one of which i think included an attempt to recreate the account. is there a way to recover the data associated with the old account or to recover the account itself? i tried to dig around myself and found entries relating to the old user in the mysql database and was hopeful there was a way i could recreate the account or recover the data to a new user in our local instance. is this at all possible?",
        "Question_original_content_gpt_summary":"The user accidentally deleted their user account in their local instance while attempting to troubleshoot an unrelated issue, and is now looking for a way to recover the data associated with the old account or to recreate the account itself.",
        "Question_preprocessed_content":"Title: accidentally deleted user account in local is there any way to recover user or projects?; Content: while investigating an unrelated issue i incorrectly thought there was something wrong with the user as the local instance was having issues in the gui. i tried a lot of troubleshooting steps, one of which i think included an attempt to recreate the account. is there a way to recover the data associated with the old account or to recover the account itself? i tried to dig around myself and found entries relating to the old user in the mysql database and was hopeful there was a way i could recreate the account or recover the data to a new user in our local instance. is this at all possible?",
        "Answer_original_content":"a shot in the dark but if i deleted the old user_id and entity_id from the users and entities table and updated the new account to have the old user_id \/ entity_id would that potentially work? hey andrew, ill double-check with the team to see if there is a way to recover the account and get back to you. best, arman this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The answer suggests a potential solution of deleting the old user_id and entity_id from the users and entities table and updating the new account to have the old user_id\/entity_id. The responder also mentions checking with the team to see if there is a way to recover the account.",
        "Answer_preprocessed_content":"a shot in the dark but if i deleted the old and from the users and entities table and updated the new account to have the old \/ would that potentially work? hey andrew, ill with the team to see if there is a way to recover the account and get back to you. best, arman this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"OSError: Cannot save file into a non-existent directory",
        "Question_body":"I am using Azure ML Studio to read data from a csv file by creating a data asset test5 and write data into a csv file for my current working directory (which is failing). I am submitting a Job using a Compute Cluster and a Custom Environment and I am following the instructions from the tutorial: https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-azure-ml-in-a-day\n\nI have written the code in a notebook cell as:\n\n # Handle to the workspace\n from azure.ai.ml import MLClient\n    \n # Authentication package\n from azure.identity import DefaultAzureCredential\n credential = DefaultAzureCredential()\n    \n # Get a handle to the workspace\n ml_client = MLClient(\n     credential=credential,\n     subscription_id=\"abc\",\n     resource_group_name=\"xyz\",\n     workspace_name=\"pqr\",\n )\n from azure.ai.ml import command\n from azure.ai.ml import Input\n    \n registered_model_name = \"read_data\"\n env_name = \"docker-context\"\n job = command(inputs=dict(\n         data=Input(\n             type=\"uri_file\",\n             path=\"azureml:test5:1\",\n         ),\n         registered_model_name=registered_model_name\n     ),   \n     code=\".\/src\/\",  # location of source code\n     command=\"python main.py --data ${<!-- -->{inputs.data}} --registered_model_name ${<!-- -->{inputs.registered_model_name}}\",\n     environment=\"docker-context:10\",\n     compute=\"amlcluster01\",\n     experiment_name=\"read_data1\",\n     display_name=\"read_data2\",\n     )\n ml_client.create_or_update(job)\n\n\n\nThis works fine. The content of the main.py is:\n\n import os\n import argparse\n import pandas as pd\n    \n def main():\n     print(\"Hello\")\n      # input and output arguments\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n     parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n     args = parser.parse_args()\n     print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n     print(\"input data:\", args.data)\n     read_data=pd.read_csv(args.data)\n     #read_data=pd.read_parquet(args.data, engine='pyarrow')\n     #credit_df = pd.read_excel(args.data, header=1, index_col=0)\n     print(read_data)\n     read_data.to_csv(r'\/home\/azureuser\/cloudfiles\/code\/Users\/Ankit19.Gupta\/azureml-in-a-day\/src\/file3.csv')\n    \n     print(\"Hello World !\")\n    \n if __name__ == \"__main__\":\n     main()\n\nHere, all lines of code work fine except read_data.to_csv(r'\/home\/azureuser\/cloudfiles\/code\/Users\/Ankit19.Gupta\/azureml-in-a-day\/src\/file3.csv').\n\nIt shows the error message as: OSError: Cannot save file into a non-existent directory:\/home\/azureuser\/cloudfiles\/code\/Users\/Ankit19.Gupta\/azureml-in-a-day\/src\n\nCan anyone please help me how to save dataframe into a csv file into my current working directory through a Job. Any help would be appreciated.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1666367036920,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1058457\/oserror-cannot-save-file-into-a-non-existent-direc.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-23T19:18:49.723Z",
                "Answer_score":0,
                "Answer_body":"Helllo @Ankit19Gupta-9721\n\nThanks for using Microsoft Q&A platform, for \"Reading and Writing data in a job\" in official guidance, please refer to below sample for ML SDK V2 - https:\/\/github.com\/Azure\/azureml-examples\/blob\/sdk-preview\/sdk\/assets\/data\/data.ipynb\n\nIf that's not want you want, I have done some researches around it and found a thread about the same issue in Stack - https:\/\/stackoverflow.com\/questions\/47143836\/pandas-dataframe-to-csv-raising-ioerror-no-such-file-or-directory\n\nIt seems this error was caused by to_csv does create the file if it doesn't exist as you said, but it does not create directories that don't exist. Ensure that the subdirectory you are trying to save your file within has been created first as below -\n\n import os\n    \n outname = 'name.csv'\n    \n outdir = '.\/dir'\n if not os.path.exists(outdir):\n     os.mkdir(outdir)\n    \n fullname = os.path.join(outdir, outname)    \n    \n df.to_csv(fullname)\n\n\n\nPlease have a try and I hope above helps, let me know how is going and we are happy to help.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: oserror: cannot save file into a non-existent directory; Content: i am using studio to read data from a csv file by creating a data asset test5 and write data into a csv file for my current working directory (which is failing). i am submitting a job using a compute cluster and a custom environment and i am following the instructions from the tutorial: https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-azure-ml-in-a-day i have written the code in a notebook cell as: # handle to the workspace from azure.ai.ml import mlclient # authentication package from azure.identity import defaultazurecredential credential = defaultazurecredential() # get a handle to the workspace ml_client = mlclient( credential=credential, subscription_id=\"abc\", resource_group_name=\"xyz\", workspace_name=\"pqr\", ) from azure.ai.ml import command from azure.ai.ml import input registered_model_name = \"read_data\" env_name = \"docker-context\" job = command(inputs=dict( data=input( type=\"uri_file\", path=\":test5:1\", ), registered_model_name=registered_model_name ), code=\".\/src\/\", # location of source code command=\"python main.py --data ${{inputs.data}} --registered_model_name ${{inputs.registered_model_name}}\", environment=\"docker-context:10\", compute=\"amlcluster01\", experiment_name=\"read_data1\", display_name=\"read_data2\", ) ml_client.create_or_update(job) this works fine. the content of the main.py is: import os import argparse import pandas as pd def main(): print(\"hello\") # input and output arguments parser = argparse.argumentparser() parser.add_argument(\"--data\", type=str, help=\"path to input data\") parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\") args = parser.parse_args() print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items())) print(\"input data:\", args.data) read_data=pd.read_csv(args.data) #read_data=pd.read_parquet(args.data, engine='pyarrow') #credit_df = pd.read_excel(args.data, header=1, index_col=0) print(read_data) read_data.to_csv(r'\/home\/azureuser\/cloudfiles\/code\/users\/ankit19.gupta\/-in-a-day\/src\/file3.csv') print(\"hello world !\") if __name__ == \"__main__\": main() here, all lines of code work fine except read_data.to_csv(r'\/home\/azureuser\/cloudfiles\/code\/users\/ankit19.gupta\/-in-a-day\/src\/file3.csv'). it shows the error message as: oserror: cannot save file into a non-existent directory:\/home\/azureuser\/cloudfiles\/code\/users\/ankit19.gupta\/-in-a-day\/src can anyone please help me how to save dataframe into a csv file into my current working directory through a job. any help would be appreciated.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with saving a dataframe into a CSV file in their current working directory through a job, resulting in an \"OSError: cannot save file into a non-existent directory\" error message.",
        "Question_preprocessed_content":"Title: oserror cannot save file into a directory; Content: i am using studio to read data from a csv file by creating a data asset test and write data into a csv file for my current working directory . i am submitting a job using a compute cluster and a custom environment and i am following the instructions from the tutorial i have written the code in a notebook cell as handle to the workspace from import mlclient authentication package from import defaultazurecredential credential defaultazurecredential get a handle to the workspace mlclient from import command from import input job command , , location of source code command python compute amlcluster , this works fine. the content of the is import os import argparse import pandas as pd def main print input and output arguments parser type str, help path to input data type str, help model name args print .items print engine 'pyarrow' header , print if main here, all lines of code work fine except it shows the error message as oserror cannot save file into a can anyone please help me how to save dataframe into a csv file into my current working directory through a job. any help would be appreciated.",
        "Answer_original_content":"helllo @ankit19gupta-9721 thanks for using microsoft q&a platform, for \"reading and writing data in a job\" in official guidance, please refer to below sample for ml sdk v2 - https:\/\/github.com\/azure\/-examples\/blob\/sdk-preview\/sdk\/assets\/data\/data.ipynb if that's not want you want, i have done some researches around it and found a thread about the same issue in stack - https:\/\/stackoverflow.com\/questions\/47143836\/pandas-dataframe-to-csv-raising-ioerror-no-such-file-or-directory it seems this error was caused by to_csv does create the file if it doesn't exist as you said, but it does not create directories that don't exist. ensure that the subdirectory you are trying to save your file within has been created first as below - import os outname = 'name.csv' outdir = '.\/dir' if not os.path.exists(outdir): os.mkdir(outdir) fullname = os.path.join(outdir, outname) df.to_csv(fullname) please have a try and i hope above helps, let me know how is going and we are happy to help. regards, yutong -please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_original_content_gpt_summary":"The solution to the \"OSError: cannot save file into a non-existent directory\" error message when saving a dataframe into a CSV file through a job is to ensure that the subdirectory where the file is being saved has been created first. This can be done using the code snippet provided in the answer.",
        "Answer_preprocessed_content":"helllo thanks for using microsoft q&a platform, for reading and writing data in a job in official guidance, please refer to below sample for ml sdk v if that's not want you want, i have done some researches around it and found a thread about the same issue in stack it seems this error was caused by does create the file if it doesn't exist as you said, but it does not create directories that don't exist. ensure that the subdirectory you are trying to save your file within has been created first as below import os outname outdir if not fullname outname please have a try and i hope above helps, let me know how is going and we are happy to help. regards, yutong please kindly accept the answer if you feel helpful to support the community, thanks a lot."
    },
    {
        "Question_id":null,
        "Question_title":"Forcing Pre-emption in a sweep",
        "Question_body":"<p>This is a bit of a weird one\u2026<br>\nMy lab has a cluster that usually runs using slurm, but slurm is down (and may be not up for a while). We would like to still use wandb and still maintain the priority levels that slurm gives (i.e. we want to make sure if there is some critical jobs that need to get run, we can easily pre-empt existing jobs that are running in sweeps and still get them to requeue later when the critical jobs are over (essentially we are trying to do manual pre-emption)<\/p>\n<p>I have been trying to set this up with a dummy sweep that just sends a single \u201cmagic number\u201d to each run, however no matter what I do I cannot seem to get the run to be pre-empted. If I try killing via <code>ctrl+C<\/code>, the wandb process shuts down normally and it marks the run as finished. Is there any way I can get around this to force the pre-emption? Thank you so much for your help!<\/p>\n<pre><code class=\"lang-auto\">import wandb\nimport time\nfrom random import randint\nimport os\nimport sys\nfrom tqdm.auto import tqdm\nwandb.init()\nmagic_number = wandb.config.magic_number\ntry:\n    print(f'Hello, world! Magic number is {magic_number}')\n    print('My PID is', os.getpid())\n    size = 1_000_000_000\n    for count in tqdm(range(size)):\n        if count % (size \/\/ 10) == 0:\n            print(f'On count {count}')\nexcept (Exception, KeyboardInterrupt, SystemExit) as e:\n    print('Keyboard interrupt!')\n    # I cannot reach this piece of code no matter when I do\n    # I have tried ctrl+c, killing the process corresponding to this python script, killing the wandb agent process\n    wandb.mark_preempting()\n    print('Preempted!')\n    sys.exit(999)\nprint('Done!')\n<\/code><\/pre>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1667929194104,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":272.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/forcing-pre-emption-in-a-sweep\/3391",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-11T01:16:46.971Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/evanv\">@evanv<\/a> , happy to help. As a point of clarification, are you wanting t to pre-empt a run called by a sweep?<\/p>\n<p>I\u2019m not quite sure what could be happening on your end. I ran your exact same code, made a minor tweak by passing  a config that includes a \u2018magic_number\u2019 to <code>wandb.init(). I successfully ran the code example, manually killed the run via <\/code>ctrl+c` and was able to place the run in a preempted state.<\/p>\n<pre><code class=\"lang-auto\">  config = {\"magic_number\":10}\n  wandb.init(config=config)\n  magic_number = wandb.config.magic_number\n  try:\n      print(f'Hello, world! Magic number is {magic_number}')\n      print('My PID is', os.getpid())\n      size = 1_000_000_000\n      for count in tqdm(range(size)):\n          if count % (size \/\/ 10) == 0:\n              print(f'On count {count}')\n  except (Exception, KeyboardInterrupt, SystemExit) as e:\n      #I was successful in triggering this except block with ctrl+c\n      print('Keyboard interrupt!')\n      wandb.mark_preempting()\n      print('Preempted!')\n      sys.exit(999)\n  print('Done!')\n<\/code><\/pre>\n<p>The produced ran can be viewed <a href=\"https:\/\/wandb.ai\/mohammadbakir\/uncategorized\/runs\/1nvrqaqq\/overview?workspace=user-mohammadbakir\">here<\/a>. Could you provide me a link to your workspace where the runs are showing up as finished.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-12T02:22:57.742Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> ,<\/p>\n<p>Yes, I am trying to pre-empt a run which is called by a sweep. I may indeed have had some kind of strange issue on my end because your code also works for me. However one thing I noticed is that the runs which are pre-empted seem not to be immediately re-qued by a sweep. Interestingly it looks like you were able to use the <code>wand.mark_preempting<\/code> without a sweep in progress. Are you able to give a bit more insight into how this command works and what its implications are for the run queue? The <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming#preemptible-sweeps\">documentation<\/a> seems to say they should be immediately re-queued, but I have not observed that to be the case. Is there any way I can check the queue to see what is happening?<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-18T00:17:37.539Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/evanv\">@evanv<\/a> <code>wandb.mark_preempting()<\/code> will mark a run <code>preempting<\/code>, but the run is not requeued until the status is <code>preempted<\/code>. The status change <code>preempting<\/code> \u2192 <code>preempted<\/code> happens when the run exits with non zero status (maybe your signal handler is preventing this) or after the run spends 5 minutes in the preempting state and our backend receives no heartbeats from the run. If the run exits successfully (with zero status) after being put into the preempting state, we assume the run finished successfully before being preempted by the server and the run state is set to finished. In this case the run <strong>will not be requeued<\/strong>.<\/p>\n<p>There are likely two ways to have control over your exist status and force this <code>preempting<\/code> \u2192 <code>preempted<\/code><\/p>\n<ol>\n<li>call <code>wandb.finish(exit_code=1)<\/code> after you mark the run as prempting<\/li>\n<li>Make your process exit with a non-zero status, <code>exit(1)<\/code>\n<\/li>\n<\/ol>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-23T19:06:51.423Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/evanv\">@evanv<\/a>  since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-22T19:07:30.766Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: forcing pre-emption in a sweep; Content: this is a bit of a weird one\u2026 my lab has a cluster that usually runs using slurm, but slurm is down (and may be not up for a while). we would like to still use and still maintain the priority levels that slurm gives (i.e. we want to make sure if there is some critical jobs that need to get run, we can easily pre-empt existing jobs that are running in sweeps and still get them to requeue later when the critical jobs are over (essentially we are trying to do manual pre-emption) i have been trying to set this up with a dummy sweep that just sends a single \u201cmagic number\u201d to each run, however no matter what i do i cannot seem to get the run to be pre-empted. if i try killing via ctrl+c, the process shuts down normally and it marks the run as finished. is there any way i can get around this to force the pre-emption? thank you so much for your help! import import time from random import randint import os import sys from tqdm.auto import tqdm .init() magic_number = .config.magic_number try: print(f'hello, world! magic number is {magic_number}') print('my pid is', os.getpid()) size = 1_000_000_000 for count in tqdm(range(size)): if count % (size \/\/ 10) == 0: print(f'on count {count}') except (exception, keyboardinterrupt, systemexit) as e: print('keyboard interrupt!') # i cannot reach this piece of code no matter when i do # i have tried ctrl+c, killing the process corresponding to this python script, killing the agent process .mark_preempting() print('preempted!') sys.exit(999) print('done!')",
        "Question_original_content_gpt_summary":"The user is encountering challenges in trying to force pre-emption in a sweep and maintain priority levels in a cluster running on Slurm, despite Slurm being down.",
        "Question_preprocessed_content":"Title: forcing in a sweep; Content: this is a bit of a weird one my lab has a cluster that usually runs using slurm, but slurm is down . we would like to still use and still maintain the priority levels that slurm gives i have been trying to set this up with a dummy sweep that just sends a single magic number to each run, however no matter what i do i cannot seem to get the run to be if i try killing via , the process shuts down normally and it marks the run as finished. is there any way i can get around this to force the thank you so much for your help!",
        "Answer_original_content":"hi @evanv , happy to help. as a point of clarification, are you wanting t to pre-empt a run called by a sweep? im not quite sure what could be happening on your end. i ran your exact same code, made a minor tweak by passing a config that includes a magic_number to .init(). i successfully ran the code example, manually killed the run via ctrl+c` and was able to place the run in a preempted state. config = {\"magic_number\":10} .init(config=config) magic_number = .config.magic_number try: print(f'hello, world! magic number is {magic_number}') print('my pid is', os.getpid()) size = 1_000_000_000 for count in tqdm(range(size)): if count % (size \/\/ 10) == 0: print(f'on count {count}') except (exception, keyboardinterrupt, systemexit) as e: #i was successful in triggering this except block with ctrl+c print('keyboard interrupt!') .mark_preempting() print('preempted!') sys.exit(999) print('done!') the produced ran can be viewed here. could you provide me a link to your workspace where the runs are showing up as finished. hey @mohammadbakir , yes, i am trying to pre-empt a run which is called by a sweep. i may indeed have had some kind of strange issue on my end because your code also works for me. however one thing i noticed is that the runs which are pre-empted seem not to be immediately re-qued by a sweep. interestingly it looks like you were able to use the wand.mark_preempting without a sweep in progress. are you able to give a bit more insight into how this command works and what its implications are for the run queue? the documentation seems to say they should be immediately re-queued, but i have not observed that to be the case. is there any way i can check the queue to see what is happening? hi @evanv .mark_preempting() will mark a run preempting, but the run is not requeued until the status is preempted. the status change preempting preempted happens when the run exits with non zero status (maybe your signal handler is preventing this) or after the run spends 5 minutes in the preempting state and our backend receives no heartbeats from the run. if the run exits successfully (with zero status) after being put into the preempting state, we assume the run finished successfully before being preempted by the server and the run state is set to finished. in this case the run will not be requeued. there are likely two ways to have control over your exist status and force this preempting preempted call .finish(exit_code=1) after you mark the run as prempting make your process exit with a non-zero status, exit(1) hi @evanv since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The user is encountering challenges in trying to force pre-emption in a sweep and maintain priority levels in a cluster running on Slurm, despite Slurm being down. The answer suggests using the \".mark_preempting()\" command to mark a run as preempting, but the run is not requeued until the status is preempted. The status change preempting preempted happens when the run exits with non-zero status or after the run spends 5 minutes in the preempting state and the backend receives no heartbeats from the run. The answer also suggests using \".finish(exit_code=1)\" after marking the run as preempting and making the process exit with a non-zero status, \"exit(1)\".",
        "Answer_preprocessed_content":"hi , happy to help. as a point of clarification, are you wanting t to a run called by a sweep? im not quite sure what could be happening on your end. i ran your exact same code, made a minor tweak by passing a config that includes a to ctrl+c` and was able to place the run in a preempted state. the produced ran can be viewed here. could you provide me a link to your workspace where the runs are showing up as finished. hey , yes, i am trying to a run which is called by a sweep. i may indeed have had some kind of strange issue on my end because your code also works for me. however one thing i noticed is that the runs which are seem not to be immediately by a sweep. interestingly it looks like you were able to use the without a sweep in progress. are you able to give a bit more insight into how this command works and what its implications are for the run queue? the documentation seems to say they should be immediately but i have not observed that to be the case. is there any way i can check the queue to see what is happening? hi will mark a run , but the run is not requeued until the status is . the status change happens when the run exits with non zero status or after the run spends minutes in the preempting state and our backend receives no heartbeats from the run. if the run exits successfully after being put into the preempting state, we assume the run finished successfully before being preempted by the server and the run state is set to finished. in this case the run will not be requeued. there are likely two ways to have control over your exist status and force this call after you mark the run as prempting make your process exit with a status, hi since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Using R model in SageMaker ML pipelines",
        "Question_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples here. And it doesn't look that R is fully supported currently by ML Pipelines. Any examples and success stories are very welcome.\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1643230196748,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":97.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sage-maker-ml-pipelines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-28T21:07:43.709Z",
                "Answer_score":1,
                "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: using r model in ml pipelines; Content: hi there, is it possible to use r model training and serving in ml pipelines? looked in examples here. and it doesn't look that r is fully supported currently by ml pipelines. any examples and success stories are very welcome. thanks.",
        "Question_original_content_gpt_summary":"The user is looking to use R models in ML pipelines and is seeking examples and success stories.",
        "Question_preprocessed_content":"Title: using r model in ml pipelines; Content: hi there, is it possible to use r model training and serving in ml pipelines? looked in examples here. and it doesn't look that r is fully supported currently by ml pipelines. any examples and success stories are very welcome. thanks.",
        "Answer_original_content":"in general it is possible to use the python sdk and boto3 using the reticulate package in r, however do not have direct examples of pipelines using r. it is possible to orchestrate the production pipeline using the r containers for training and serving and setting up the dag can be done with reticulate and python sdk and can be achieved using the aws step functions. please refer to the following example for reference. https:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon- https:\/\/www.youtube.com\/watch?v=zpp0nfvqdca",
        "Answer_original_content_gpt_summary":"Possible solutions for using R models in ML pipelines include using the Python SDK and boto3 with the reticulate package in R, orchestrating the production pipeline using R containers for training and serving, and setting up the DAG using reticulate and the Python SDK. AWS Step Functions can also be used to achieve this. The provided GitHub example and YouTube video can be used as references.",
        "Answer_preprocessed_content":"in general it is possible to use the python sdk and boto using the reticulate package in r, however do not have direct examples of pipelines using r. it is possible to orchestrate the production pipeline using the r containers for training and serving and setting up the dag can be done with reticulate and python sdk and can be achieved using the aws step functions. please refer to the following example for reference."
    },
    {
        "Question_id":63642175.0,
        "Question_title":"Error when invoking AWS SageMaker endpoint using boto3 : \"Unable to parse data as JSON. Make sure the Content-Type header is set to \"application\/json\"",
        "Question_body":"<p>I am trying to invoke an AWS SageMaker endpoint through the following simple code using boto3<\/p>\n<pre><code>import boto3\n\nsession = boto3.Session(profile_name='mlacc',\n                        region_name='us-west-2')\n\nsagemaker_client = session.client('sagemaker-runtime')\n\nrequest_body = &quot;{\\n    \\&quot;requestSource\\&quot;: \\&quot;unittest\\&quot;,\\n    \\&quot;clusters\\&quot;: [{\\n        \\&quot;clusterMetadata\\&quot;: {\\n &quot;\n&quot;\\&quot;clusterId\\&quot;: \\&quot;id1\\&quot;,\\n            \\&quot;topic\\&quot;: [\\&quot;corona virus\\&quot;, \\&quot;Donald Trump\\&quot;],\\n            &quot;\n&quot;\\&quot;clusterSize\\&quot;: 2\\n        },\\n        \\&quot;documents\\&quot;: [{\\n            \\&quot;uid\\&quot;: \\&quot;1\\&quot;,\\n            &quot;\n&quot;\\&quot;content\\&quot;: \\&quot;content2\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: \\&quot;This is a &quot;\n&quot;title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,\\n            &quot;\n&quot;\\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }, {\\n            \\&quot;uid\\&quot;: \\&quot;2\\&quot;,&quot;\n&quot;\\n            \\&quot;content\\&quot;: \\&quot;content2\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: &quot;\n&quot;\\&quot;This is a title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,&quot;\n&quot;\\n            \\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }, {\\n            \\&quot;uid\\&quot;: &quot;\n&quot;\\&quot;2\\&quot;,\\n            \\&quot;content\\&quot;: \\&quot;content3\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: &quot;\n&quot;\\&quot;This is a title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,&quot;\n&quot;\\n            \\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }]\\n    }]\\n}&quot;\n\n\nresponse = sagemaker_client.invoke_endpoint(\n    EndpointName='myEndpoint22',\n    Body=request_body,\n    ContentType='application\/json',\n)\n\nresponse_json = response['Body'].read().decode('utf-8')\n\nprint(response_json)\n<\/code><\/pre>\n<p>I get the following error when I run this code<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/Users\/rppatwa\/Desktop\/WorkDocs\/CodePlayground\/SimplePythonProject\/src\/PrototypeTesting\/SummarizationLocal.py&quot;, line 205, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/rppatwa\/Desktop\/WorkDocs\/CodePlayground\/SimplePythonProject\/src\/PrototypeTesting\/SummarizationLocal.py&quot;, line 186, in main\n    ContentType='application\/json',\n  File &quot;\/Users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/Users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;Unable to parse data as JSON. Make sure the Content-Type header is set to &quot;application\/json&quot;&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/KeyurshaASMLModel in account 753843489946 for more information.\n<\/code><\/pre>\n<p>If I inline the Body (not using the request_json) this call succeeds. Please let me know what I am missing.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1598659048840,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":1170.0,
        "Owner_creation_time":1470101805440,
        "Owner_last_access_time":1645554133316,
        "Owner_reputation":107.0,
        "Owner_up_votes":6.0,
        "Owner_down_votes":0.0,
        "Owner_views":21.0,
        "Answer_body":"<p>You need to remove the trailing comma after  <code>ContentType='application\/json',<\/code> and try below snippet for passing JSON to body field.<\/p>\n<pre><code>import json \njson.dumps(request_body) \ntest=json.dumps(request_body).encode()\n<\/code><\/pre>\n<p>This will also validate the JSON that you are passing.Now pass test to body for invoking the endopoint.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1598927909716,
        "Answer_score":1.0,
        "Owner_location":"Santa Monica, CA, USA",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63642175",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: error when invoking endpoint using boto3 : \"unable to parse data as json. make sure the content-type header is set to \"application\/json\"; Content: i am trying to invoke an endpoint through the following simple code using boto3 import boto3 session = boto3.session(profile_name='mlacc', region_name='us-west-2') _client = session.client('-runtime') request_body = \"{\\n \\\"requestsource\\\": \\\"unittest\\\",\\n \\\"clusters\\\": [{\\n \\\"clustermetadata\\\": {\\n \" \"\\\"clusterid\\\": \\\"id1\\\",\\n \\\"topic\\\": [\\\"corona virus\\\", \\\"donald trump\\\"],\\n \" \"\\\"clustersize\\\": 2\\n },\\n \\\"documents\\\": [{\\n \\\"uid\\\": \\\"1\\\",\\n \" \"\\\"content\\\": \\\"content2\\\",\\n \\\"domain\\\": \\\"cnn.com\\\",\\n \\\"title\\\": \\\"this is a \" \"title\\\",\\n \\\"similarityscore\\\": 1.3,\\n \\\"published_at\\\": 1566264017,\\n \" \"\\\"domain_rank\\\": 1,\\n \\\"trust_domain_score\\\": 1\\n }, {\\n \\\"uid\\\": \\\"2\\\",\" \"\\n \\\"content\\\": \\\"content2\\\",\\n \\\"domain\\\": \\\"cnn.com\\\",\\n \\\"title\\\": \" \"\\\"this is a title\\\",\\n \\\"similarityscore\\\": 1.3,\\n \\\"published_at\\\": 1566264017,\" \"\\n \\\"domain_rank\\\": 1,\\n \\\"trust_domain_score\\\": 1\\n }, {\\n \\\"uid\\\": \" \"\\\"2\\\",\\n \\\"content\\\": \\\"content3\\\",\\n \\\"domain\\\": \\\"cnn.com\\\",\\n \\\"title\\\": \" \"\\\"this is a title\\\",\\n \\\"similarityscore\\\": 1.3,\\n \\\"published_at\\\": 1566264017,\" \"\\n \\\"domain_rank\\\": 1,\\n \\\"trust_domain_score\\\": 1\\n }]\\n }]\\n}\" response = _client.invoke_endpoint( endpointname='myendpoint22', body=request_body, contenttype='application\/json', ) response_json = response['body'].read().decode('utf-8') print(response_json) i get the following error when i run this code traceback (most recent call last): file \"\/users\/rppatwa\/desktop\/workdocs\/codeplayground\/simplepythonproject\/src\/prototypetesting\/summarizationlocal.py\", line 205, in <module> main() file \"\/users\/rppatwa\/desktop\/workdocs\/codeplayground\/simplepythonproject\/src\/prototypetesting\/summarizationlocal.py\", line 186, in main contenttype='application\/json', file \"\/users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 316, in _api_call return self._make_api_call(operation_name, kwargs) file \"\/users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 635, in _make_api_call raise error_class(parsed_response, operation_name) botocore.errorfactory.modelerror: an error occurred (modelerror) when calling the invokeendpoint operation: received client error (400) from model with message \"unable to parse data as json. make sure the content-type header is set to \"application\/json\"\". see https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logeventviewer:group=\/aws\/\/endpoints\/keyurshaasmlmodel in account 753843489946 for more information. if i inline the body (not using the request_json) this call succeeds. please let me know what i am missing. thanks",
        "Question_original_content_gpt_summary":"The user is encountering an error when invoking an endpoint using boto3, receiving a \"unable to parse data as json. make sure the content-type header is set to \"application\/json\" error.",
        "Question_preprocessed_content":"Title: error when invoking endpoint using boto unable to parse data as json. make sure the header is set to ; Content: i am trying to invoke an endpoint through the following simple code using boto i get the following error when i run this code if i inline the body this call succeeds. please let me know what i am missing. thanks",
        "Answer_original_content":"you need to remove the trailing comma after contenttype='application\/json', and try below snippet for passing json to body field. import json json.dumps(request_body) test=json.dumps(request_body).encode() this will also validate the json that you are passing.now pass test to body for invoking the endopoint.",
        "Answer_original_content_gpt_summary":"The solution to the error encountered when invoking an endpoint using boto3 is to remove the trailing comma after contenttype='application\/json'. Additionally, the user should try passing the JSON to the body field using the provided code snippet, which includes importing and encoding the JSON to validate it before passing it to the endpoint.",
        "Answer_preprocessed_content":"you need to remove the trailing comma after and try below snippet for passing json to body field. this will also validate the json that you are pass test to body for invoking the endopoint."
    },
    {
        "Question_id":null,
        "Question_title":"Artifacts logged with run_id",
        "Question_body":"<p>Hello everyone,<br>\nI am new to w&amp;b, so this might be a beginner question, but I was wondering why when I run<br>\n<code> wandb.log_artifact(file_path, name='dataset', type='dataset')<\/code><br>\nI am able to log artifacts correctly without many issues,  whereas if I use the example provided <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/wandb-artifacts\/Pipeline_Versioning_with_W%26B_Artifacts.ipynb#scrollTo=Mb8GiolzPgUU\" rel=\"noopener nofollow ugc\">here<\/a><\/p>\n<pre><code class=\"lang-auto\">def load_and_log():\n\n    # \ud83d\ude80 start a run, with a type to label it and a project it can call home\n    with wandb.init(project=\"artifacts-example\", job_type=\"load-data\") as run:\n        \n        datasets = load()  # separate code for loading the datasets\n        names = [\"training\", \"validation\", \"test\"]\n\n        # \ud83c\udffa create our Artifact\n        raw_data = wandb.Artifact(\n            \"mnist-raw\", type=\"dataset\",\n            description=\"Raw MNIST dataset, split into train\/val\/test\",\n            metadata={\"source\": \"torchvision.datasets.MNIST\",\n                      \"sizes\": [len(dataset) for dataset in datasets]})\n\n        for name, data in zip(names, datasets):\n            # \ud83d\udc23 Store a new file in the artifact, and write something into its contents.\n            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n                x, y = data.tensors\n                torch.save((x, y), file)\n\n        # \u270d\ufe0f Save the artifact to W&amp;B.\n        run.log_artifact(raw_data)\n\nload_and_log()\n<\/code><\/pre>\n<p>I get the artifacts stored in a run_table, and it makes versioning impossible.<br>\nAm I doing something wrong? Below you can find the same function as I modified it for my project, in case I might have missed something<\/p>\n<pre><code class=\"lang-auto\">from wandb.sdk import wandb_init\ndef load_and_log():\n\n    # \ud83d\ude80 start a run, with a type to label it and a project it can call home\n    with wandb.init(project=\"project\", job_type=\"load-data\", resume=\"allow\") as run:\n        \n        dataset = my_function(dir_path + '\/datas', MAX_SAMPLES, MAX_LENGTH) #returns a tuple of lists\n        datasets = dataset.load()  # separate code for loading the datasets\n        names = [\"questions\", \"answers\"]\n\n        # \ud83c\udffa create our Artifact\n        raw_data = wandb.Artifact(\n            \"dataset\", type=\"dataset\",\n            description=\"json of the preprocessed dataset - not split\",\n            metadata={\"source\": \"https:\/\/source.php\",\n                      \"sizes\": [len(dataset) for dataset in datasets]})\n\n        # transfer lists into table\n        table = wandb.Table(columns=[], data=[])\n        for name, dataset in zip(names, datasets):\n          table.add_column(name=f\"{name}\", data=dataset)\n\n        # \u270d\ufe0f Save the artifact to W&amp;B.\n        wandb.log({f\"dataset_{MAX_SAMPLES}_{MAX_LENGTH}\": table})\n\nload_and_log()\n<\/code><\/pre>\n<p>Thank you in advance if you have an answer!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1657984287757,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":427.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/artifacts-logged-with-run-id\/2759",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-20T17:40:54.801Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/guen\">@guen<\/a> ,<\/p>\n<p>In our example we are creating an artifact of the <code>Raw MNIST dataset, split into train\/val\/test<\/code>and logging it using <code>run.log_artifact(raw_data)<\/code>. In your code you are generating a table of your dataset and logging the Table to wandb, <code>wandb.log({f\"dataset_{MAX_SAMPLES}_{MAX_LENGTH}\": table})<\/code>. Is this your intended approach?<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-25T23:36:59.336Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/guen\">@guen<\/a> ,<\/p>\n<p>Following up on this request. Please let us know if you have any questions on the above.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-29T17:58:15.729Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/guen\">@guen<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-27T17:58:24.360Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: artifacts logged with run_id; Content: hello everyone, i am new to w&b, so this might be a beginner question, but i was wondering why when i run .log_artifact(file_path, name='dataset', type='dataset') i am able to log artifacts correctly without many issues, whereas if i use the example provided here def load_and_log(): # \ud83d\ude80 start a run, with a type to label it and a project it can call home with .init(project=\"artifacts-example\", job_type=\"load-data\") as run: datasets = load() # separate code for loading the datasets names = [\"training\", \"validation\", \"test\"] # \ud83c\udffa create our artifact raw_data = .artifact( \"mnist-raw\", type=\"dataset\", description=\"raw mnist dataset, split into train\/val\/test\", metadata={\"source\": \"torchvision.datasets.mnist\", \"sizes\": [len(dataset) for dataset in datasets]}) for name, data in zip(names, datasets): # \ud83d\udc23 store a new file in the artifact, and write something into its contents. with raw_data.new_file(name + \".pt\", mode=\"wb\") as file: x, y = data.tensors torch.save((x, y), file) # \u270d\ufe0f save the artifact to w&b. run.log_artifact(raw_data) load_and_log() i get the artifacts stored in a run_table, and it makes versioning impossible. am i doing something wrong? below you can find the same function as i modified it for my project, in case i might have missed something from .sdk import _init def load_and_log(): # \ud83d\ude80 start a run, with a type to label it and a project it can call home with .init(project=\"project\", job_type=\"load-data\", resume=\"allow\") as run: dataset = my_function(dir_path + '\/datas', max_samples, max_length) #returns a tuple of lists datasets = dataset.load() # separate code for loading the datasets names = [\"questions\", \"answers\"] # \ud83c\udffa create our artifact raw_data = .artifact( \"dataset\", type=\"dataset\", description=\"json of the preprocessed dataset - not split\", metadata={\"source\": \"https:\/\/source.php\", \"sizes\": [len(dataset) for dataset in datasets]}) # transfer lists into table table = .table(columns=[], data=[]) for name, dataset in zip(names, datasets): table.add_column(name=f\"{name}\", data=dataset) # \u270d\ufe0f save the artifact to w&b. .log({f\"dataset_{max_samples}_{max_length}\": table}) load_and_log() thank you in advance if you have an answer!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with logging artifacts correctly when using the example provided, resulting in artifacts being stored in a run_table and making versioning impossible.",
        "Question_preprocessed_content":"Title: artifacts logged with ; Content: hello everyone, i am new to w&b, so this might be a beginner question, but i was wondering why when i run i am able to log artifacts correctly without many issues, whereas if i use the example provided here i get the artifacts stored in a and it makes versioning impossible. am i doing something wrong? below you can find the same function as i modified it for my project, in case i might have missed something thank you in advance if you have an answer!",
        "Answer_original_content":"hi @guen , in our example we are creating an artifact of the raw mnist dataset, split into train\/val\/testand logging it using run.log_artifact(raw_data). in your code you are generating a table of your dataset and logging the table to , .log({f\"dataset_{max_samples}_{max_length}\": table}). is this your intended approach? hi @guen , following up on this request. please let us know if you have any questions on the above. hi @guen , since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer. The answer is a series of messages from a support team member trying to clarify the user's intended approach and following up on their request. The conversation ends with the support team member closing the request due to lack of response from the user.",
        "Answer_preprocessed_content":"hi , in our example we are creating an artifact of the and logging it using . in your code you are generating a table of your dataset and logging the table to , . is this your intended approach? hi , following up on this request. please let us know if you have any questions on the above. hi , since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":38119062.0,
        "Question_title":"Pulling data from Stream Analytics to Azure Machine Learning",
        "Question_body":"<p>Working on a IoT telemetry project that receives humidity and weather pollution data from different sites on the field. I will then apply Machine Learning on the collected data. I'm using Event Hubs and Stream Analytics. Is there a way of pulling the data to Azure Machine Learning without the hassle of writing an application to get it from Stream Analytics and push to AML web service?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1467278958850,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":627.0,
        "Owner_creation_time":1311017514580,
        "Owner_last_access_time":1664027428383,
        "Owner_reputation":408.0,
        "Owner_up_votes":88.0,
        "Owner_down_votes":6.0,
        "Owner_views":32.0,
        "Answer_body":"<p>Stream Analytics has a functionality called the \u201c<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">Functions<\/a>\u201d. You can call any web service you\u2019ve published using AML from within Stream Analytics and apply it within your Stream Analytics query. Check this <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link for a tutorial<\/a>.\nExample workflow in your case would be like the following;<\/p>\n\n<ul>\n<li>Telemetry arrives and reaches Stream Analytics<\/li>\n<li>Streaming Analytics (SA) calls the Machine Learning function to apply it on the data<\/li>\n<li>SA redirects it to the output accordingly, here you can use the PowerBI to create a predictions dashboards.<\/li>\n<\/ul>\n\n<p>Another way would be using R, and here\u2019s a good tutorial showing that <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/<\/a> . \nIt is more work of course but can give you more control as you control the code.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1467284456456,
        "Answer_score":3.0,
        "Owner_location":"Beirut, Lebanon",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38119062",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pulling data from stream analytics to ; Content: working on a iot telemetry project that receives humidity and weather pollution data from different sites on the field. i will then apply machine learning on the collected data. i'm using event hubs and stream analytics. is there a way of pulling the data to without the hassle of writing an application to get it from stream analytics and push to aml web service?",
        "Question_original_content_gpt_summary":"The user is working on an IoT telemetry project that requires them to pull data from Stream Analytics to an Azure Machine Learning web service without having to write an application.",
        "Question_preprocessed_content":"Title: pulling data from stream analytics to ; Content: working on a iot telemetry project that receives humidity and weather pollution data from different sites on the field. i will then apply machine learning on the collected data. i'm using event hubs and stream analytics. is there a way of pulling the data to without the hassle of writing an application to get it from stream analytics and push to aml web service?",
        "Answer_original_content":"stream analytics has a functionality called the functions. you can call any web service youve published using aml from within stream analytics and apply it within your stream analytics query. check this link for a tutorial. example workflow in your case would be like the following; telemetry arrives and reaches stream analytics streaming analytics (sa) calls the machine learning function to apply it on the data sa redirects it to the output accordingly, here you can use the powerbi to create a predictions dashboards. another way would be using r, and heres a good tutorial showing that https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/ . it is more work of course but can give you more control as you control the code.",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's problem of pulling data from Stream Analytics to an Azure Machine Learning web service without writing an application are: using Stream Analytics' Functions to call the web service and apply it within the query, and using R to have more control over the code. The first solution is easier and has a tutorial available, while the second solution requires more work but can provide more control. The workflow for the first solution involves telemetry arriving, reaching Stream Analytics, calling the Machine Learning function, redirecting it to the output, and using PowerBI to create predictions dashboards.",
        "Answer_preprocessed_content":"stream analytics has a functionality called the functions. you can call any web service youve published using aml from within stream analytics and apply it within your stream analytics query. check this link for a tutorial. example workflow in your case would be like the following; telemetry arrives and reaches stream analytics streaming analytics calls the machine learning function to apply it on the data sa redirects it to the output accordingly, here you can use the powerbi to create a predictions dashboards. another way would be using r, and heres a good tutorial showing that . it is more work of course but can give you more control as you control the code."
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker training with FSx: what is \"directory_path\"",
        "Question_body":"SageMaker can train on FSx data. One SageMaker SDK parameter for FSx training is directory_path. Where do we find that?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1604678225000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":105.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCaemzfoDRIy9AgLRW8suqw\/sage-maker-training-with-f-sx-what-is-directory-path",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-06T16:13:42.000Z",
                "Answer_score":0,
                "Answer_body":"FSx for Lustre is a file system that you can use to provide high performance for ML training workloads. The directory_path should point to the location on your file system where your dataset is stored.\n\nIn the example in the docs: directory_path='\/fsx\/tensorflow',\n\n\/fsx is the directory you define on your compute instances where you are mounting the file system \/tensorflow would represent a folder within the fsx directory\n\nIf you are using an S3-linked FSx for Lustre file system \/tensorflow would be a prefix within your S3-linked bucket.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: training with fsx: what is \"directory_path\"; Content: can train on fsx data. one sdk parameter for fsx training is directory_path. where do we find that?",
        "Question_original_content_gpt_summary":"The user is trying to understand the parameter \"directory_path\" for training with FSX and is looking for guidance on where to find it.",
        "Question_preprocessed_content":"Title: training with fsx what is ; Content: can train on fsx data. one sdk parameter for fsx training is where do we find that?",
        "Answer_original_content":"fsx for lustre is a file system that you can use to provide high performance for ml training workloads. the directory_path should point to the location on your file system where your dataset is stored. in the example in the docs: directory_path='\/fsx\/tensorflow', \/fsx is the directory you define on your compute instances where you are mounting the file system \/tensorflow would represent a folder within the fsx directory if you are using an s3-linked fsx for lustre file system \/tensorflow would be a prefix within your s3-linked bucket.",
        "Answer_original_content_gpt_summary":"The solution to understanding the \"directory_path\" parameter for training with FSX is to point it to the location on your file system where your dataset is stored. In the example given in the documentation, the directory_path is set to '\/fsx\/tensorflow', where '\/fsx' is the directory defined on the compute instances where the file system is mounted, and '\/tensorflow' represents a folder within the FSx directory. If using an S3-linked FSx for Lustre file system, '\/tensorflow' would be a prefix within the S3-linked bucket.",
        "Answer_preprocessed_content":"fsx for lustre is a file system that you can use to provide high performance for ml training workloads. the should point to the location on your file system where your dataset is stored. in the example in the docs is the directory you define on your compute instances where you are mounting the file system would represent a folder within the fsx directory if you are using an fsx for lustre file system would be a prefix within your bucket."
    },
    {
        "Question_id":null,
        "Question_title":"What difference environments\/domains does vertex ai is being in use ?",
        "Question_body":"I wanna learn more about how to use vertex ai in more domains than in recognition and things , so i can learn how to use Vertex ai in other fields and domain , I want to learn like projects using live vertex ai auto ml or realated to those .",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1662084180000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":31.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-difference-environments-domains-does-vertex-ai-is-being-in\/td-p\/462390\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-02T02:03:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I wanna learn more about how to use vertex ai in more domains than in recognition and things , so i can learn how to use Vertex ai in other fields and domain ,\u00a0\n\nI want to learn like projects using live vertex ai auto ml or realated to those ."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what difference environments\/domains does is being in use ?; Content: i wanna learn more about how to use in more domains than in recognition and things , so i can learn how to use in other fields and domain , i want to learn like projects using live auto ml or realated to those .",
        "Question_original_content_gpt_summary":"The user wants to learn how to use Auto ML in more domains than just recognition and wants to learn how to use it in other fields and domains.",
        "Question_preprocessed_content":"Title: what difference does is being in use ?; Content: i wanna learn more about how to use in more domains than in recognition and things , so i can learn how to use in other fields and domain , i want to learn like projects using live auto ml or realated to those .",
        "Answer_original_content":"i wanna learn more about how to use in more domains than in recognition and things , so i can learn how to use in other fields and domain , i want to learn like projects using live auto ml or realated to those .",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include: \n1. Learning about using Auto ML in other fields and domains beyond recognition. \n2. Exploring projects that use live Auto ML or are related to it. \n\nIn summary, the user can expand their knowledge of Auto ML by exploring its applications in different fields and domains and by studying related projects.",
        "Answer_preprocessed_content":"i wanna learn more about how to use in more domains than in recognition and things , so i can learn how to use in other fields and domain , i want to learn like projects using live auto ml or realated to those ."
    },
    {
        "Question_id":51450610.0,
        "Question_title":"Athena query works in console but not with boto3 client in sagemaker (convert csv into table)",
        "Question_body":"<p>I am trying to convert a csv file from s3 into a table in Athena. When I run the query on Athena console it works but when I run it on Sagemaker Jupyter notebook with boto3 client it returns: <\/p>\n\n<pre><code>\"**InvalidRequestException**: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: line 1:8: no viable alternative at input 'CREATE EXTERNAL'\"\n<\/code><\/pre>\n\n<p>Here is my code <\/p>\n\n<pre><code>def run_query(query):\n    client = boto3.client('athena')\n    response = client.start_query_execution(\n        QueryString=query,\n        ResultConfiguration={\n            'OutputLocation': 's3:\/\/path\/to\/s3output',\n            }\n        )\n    print('Execution ID: ' + response['QueryExecutionId'])\n    return response\n\ncreateTable = \\\n\"\"\"CREATE EXTERNAL TABLE TestTable (\n    ID string,\n    CustomerId string, \n    Ip string,\n    MessageFilename string\n\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n   'separatorChar' = ',',\n   'quoteChar' = '\\\"',\n   'escapeChar' = '\\\\'\n )\nSTORED AS TEXTFILE\nLOCATION 's3:\/\/bucket_name\/results\/csv\/'\nTBLPROPERTIES (\"skip.header.line.count\"=\"1\")\"\"\"\n\nresponse = run_query(createTable, s3_output)\nprint(response)\n<\/code><\/pre>\n\n<p>I have run queries through boto3 client in json format (so, using  ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe') which have worked well but somehow this doesn't. I have tried changing names, syntax, quotes but that doesn't seem to work. <\/p>\n\n<p>Any suggestion would be very appreciated, \nThank you! <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1532122235887,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":1635.0,
        "Owner_creation_time":1395796123356,
        "Owner_last_access_time":1533219077412,
        "Owner_reputation":23.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":"<p>Thanks for sharing complete example. The issue is with the escaping in <code>SERDEPROPERTIES<\/code>. After modifying <code>createTable<\/code> as below it works<\/p>\n\n<pre><code>createTable = \\\n\"\"\"CREATE EXTERNAL TABLE testtable (\n    `id` string,\n    `customerid` string, \n    `ip` string,\n    `messagefilename` string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n  'separatorChar' = ',', \n  'quoteChar' = '\\\\\\\"', \n  'escapeChar' = '\\\\\\\\' )\nSTORED AS TEXTFILE\nLOCATION 's3:\/\/bucket_name\/results\/csv\/'\nTBLPROPERTIES (\"skip.header.line.count\"=\"1\");\"\"\"\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1532127104847,
        "Answer_score":2.0,
        "Owner_location":"Atlanta, GA, USA",
        "Question_last_edit_time":1532122372580,
        "Answer_last_edit_time":1532127518448,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51450610",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: athena query works in console but not with boto3 client in (convert csv into table); Content: i am trying to convert a csv file from s3 into a table in athena. when i run the query on athena console it works but when i run it on jupyter notebook with boto3 client it returns: \"**invalidrequestexception**: an error occurred (invalidrequestexception) when calling the startqueryexecution operation: line 1:8: no viable alternative at input 'create external'\" here is my code def run_query(query): client = boto3.client('athena') response = client.start_query_execution( querystring=query, resultconfiguration={ 'outputlocation': 's3:\/\/path\/to\/s3output', } ) print('execution id: ' + response['queryexecutionid']) return response createtable = \\ \"\"\"create external table testtable ( id string, customerid string, ip string, messagefilename string ) row format serde 'org.apache.hadoop.hive.serde2.opencsvserde' with serdeproperties ( 'separatorchar' = ',', 'quotechar' = '\\\"', 'escapechar' = '\\\\' ) stored as textfile location 's3:\/\/bucket_name\/results\/csv\/' tblproperties (\"skip.header.line.count\"=\"1\")\"\"\" response = run_query(createtable, s3_output) print(response) i have run queries through boto3 client in json format (so, using row format serde 'org.openx.data.jsonserde.jsonserde') which have worked well but somehow this doesn't. i have tried changing names, syntax, quotes but that doesn't seem to work. any suggestion would be very appreciated, thank you!",
        "Question_original_content_gpt_summary":"The user is encountering an \"invalidrequestexception\" when attempting to convert a CSV file from S3 into a table in Athena using boto3 client, despite the query working in the Athena console.",
        "Question_preprocessed_content":"Title: athena query works in console but not with boto client in ; Content: i am trying to convert a csv file from s into a table in athena. when i run the query on athena console it works but when i run it on jupyter notebook with boto client it returns here is my code i have run queries through boto client in json format which have worked well but somehow this doesn't. i have tried changing names, syntax, quotes but that doesn't seem to work. any suggestion would be very appreciated, thank you!",
        "Answer_original_content":"thanks for sharing complete example. the issue is with the escaping in serdeproperties. after modifying createtable as below it works createtable = \\ \"\"\"create external table testtable ( `id` string, `customerid` string, `ip` string, `messagefilename` string ) row format serde 'org.apache.hadoop.hive.serde2.opencsvserde' with serdeproperties ( 'separatorchar' = ',', 'quotechar' = '\\\\\\\"', 'escapechar' = '\\\\\\\\' ) stored as textfile location 's3:\/\/bucket_name\/results\/csv\/' tblproperties (\"skip.header.line.count\"=\"1\");\"\"\"",
        "Answer_original_content_gpt_summary":"The solution to the \"invalidrequestexception\" error when converting a CSV file from S3 into a table in Athena using boto3 client is to modify the createtable code by adjusting the escaping in serdeproperties. Specifically, the solution involves changing the quotechar and escapechar values in the code.",
        "Answer_preprocessed_content":"thanks for sharing complete example. the issue is with the escaping in . after modifying as below it works"
    },
    {
        "Question_id":51725489.0,
        "Question_title":"QuickSight using ML",
        "Question_body":"<p>I would like to use the ML model I created in AWS in my QuickSight reports.<\/p>\n\n<ul>\n<li>Is there a way to consume the ML endpoint in order to run batch predictions in QuickSight?<\/li>\n<li>Can I define a 'calculated field' in order to do that?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1533640683263,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":490.0,
        "Owner_creation_time":1501420854056,
        "Owner_last_access_time":1663846017710,
        "Owner_reputation":507.0,
        "Owner_up_votes":8.0,
        "Owner_down_votes":0.0,
        "Owner_views":78.0,
        "Answer_body":"<p>At this time there is no direct integration with AWS SageMaker and QuickSight, however you can use utilize SageMaker's batch transform jobs to convert data outside of QuickSight and then import this information into QuickSight for visualization. The output format for SageMaker's batch transform jobs is S3, which is a supported input data source for QuickSight.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2018\/07\/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/about-aws\/whats-new\/2018\/07\/amazon-sagemaker-supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing\/<\/a><\/li>\n<\/ul>\n\n<p>Depending on how fancy you want to be, you can also integrate calls to AWS services such as AWS Lambda or AWS SageMaker as a user-defined function (UDF) within your datastore. Here are a few resources that may help:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/user-defined-functions.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/user-defined-functions.html<\/a><\/li>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/big-data\/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases\/<\/a><\/li>\n<\/ul>\n\n<p>Calculated fields will probably not help you in this regard - calculated fields are restricted to a relatively small set of operations, and none of these operations support calls to external sources.<\/p>\n\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/quicksight\/latest\/user\/calculated-field-reference.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/quicksight\/latest\/user\/calculated-field-reference.html<\/a> <\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1534265395896,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51725489",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: quicksight using ml; Content: i would like to use the ml model i created in aws in my quicksight reports. is there a way to consume the ml endpoint in order to run batch predictions in quicksight? can i define a 'calculated field' in order to do that?",
        "Question_original_content_gpt_summary":"The user is looking for a way to use an AWS ML model in Quicksight reports, and to define a 'calculated field' in order to run batch predictions.",
        "Question_preprocessed_content":"Title: quicksight using ml; Content: i would like to use the ml model i created in aws in my quicksight reports. is there a way to consume the ml endpoint in order to run batch predictions in quicksight? can i define a 'calculated field' in order to do that?",
        "Answer_original_content":"at this time there is no direct integration with and quicksight, however you can use utilize 's batch transform jobs to convert data outside of quicksight and then import this information into quicksight for visualization. the output format for 's batch transform jobs is s3, which is a supported input data source for quicksight. https:\/\/aws.amazon.com\/about-aws\/whats-new\/2018\/07\/amazon--supports-high-throughput-batch-transform-jobs-for-non-real-time-inferencing\/ depending on how fancy you want to be, you can also integrate calls to aws services such as aws lambda or as a user-defined function (udf) within your datastore. here are a few resources that may help: https:\/\/docs.aws.amazon.com\/redshift\/latest\/dg\/user-defined-functions.html https:\/\/aws.amazon.com\/blogs\/big-data\/from-sql-to-microservices-integrating-aws-lambda-with-relational-databases\/ calculated fields will probably not help you in this regard - calculated fields are restricted to a relatively small set of operations, and none of these operations support calls to external sources. https:\/\/docs.aws.amazon.com\/quicksight\/latest\/user\/calculated-field-reference.html",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include using AWS batch transform jobs to convert data outside of Quicksight and then importing it into Quicksight for visualization. Another solution is to integrate calls to AWS services such as AWS Lambda or as a user-defined function (UDF) within the datastore. However, calculated fields are restricted to a relatively small set of operations and do not support calls to external sources.",
        "Answer_preprocessed_content":"at this time there is no direct integration with and quicksight, however you can use utilize 's batch transform jobs to convert data outside of quicksight and then import this information into quicksight for visualization. the output format for 's batch transform jobs is s , which is a supported input data source for quicksight. depending on how fancy you want to be, you can also integrate calls to aws services such as aws lambda or as a function within your datastore. here are a few resources that may help calculated fields will probably not help you in this regard calculated fields are restricted to a relatively small set of operations, and none of these operations support calls to external sources."
    },
    {
        "Question_id":null,
        "Question_title":"AI\/ML Research Paper Publish",
        "Question_body":"How should get into research about AI\/ML to get some international research intern offer?",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1667205120000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":21.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AI-ML-Research-Paper-Publish\/td-p\/484051\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-31T08:32:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"How should get into research about AI\/ML to get some international research intern offer?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: ai\/ml research paper publish; Content: how should get into research about ai\/ml to get some international research intern offer?",
        "Question_original_content_gpt_summary":"The user is looking for advice on how to get into AI\/ML research in order to gain international research intern offers.",
        "Question_preprocessed_content":"Title: research paper publish; Content: how should get into research about to get some international research intern offer?",
        "Answer_original_content":"how should get into research about ai\/ml to get some international research intern offer?",
        "Answer_original_content_gpt_summary":"The answer does not provide any specific solutions or advice on how to get into AI\/ML research to gain international research intern offers.",
        "Answer_preprocessed_content":"how should get into research about to get some international research intern offer?"
    },
    {
        "Question_id":null,
        "Question_title":"Suboptimal subsampling behavior",
        "Question_body":"<p>I am trying to build a plot of \u201closs\/eval\u201d vs \u201ccompleted_steps\u201d. \u201closs\/eval\u201d is the validation loss and it is logged once in a while. \u201ccompleted_steps\u201d is logged at every step. I would like to see all datapoints where \u201closs\/eval\u201d is logged to be displayed in the plot, because there is not many of them (about 30 for each run). Instead I only see random ones because apparently the downsampling procedure is based on frequency of \u201ccompleted_steps\u201d logging. As a result the plots are not very informative;<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/bf980db958e2b3207bbe7973f8a99a123bb9cb84.png\" alt=\"image\" data-base62-sha1=\"rkUY9uzfHFfCzy1pCZMz1zVMVSs\" width=\"649\" height=\"305\"><\/p>\n<p>I think a better behavior when plotting a metric X against metric Y would be to fetch all (X, Y) pairs first and then downsample if there is too many such pairs.<\/p>\n<p>Can I hope to see improvements in downsampling logic at some point?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1650977857791,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":147.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/suboptimal-subsampling-behavior\/2320",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-28T15:21:47.615Z",
                "Answer_body":"<p>This discussion was followed up via email, but I\u2019ll follow up here once it\u2019s resolved<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-25T12:58:16.765Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: suboptimal subsampling behavior; Content: i am trying to build a plot of \u201closs\/eval\u201d vs \u201ccompleted_steps\u201d. \u201closs\/eval\u201d is the validation loss and it is logged once in a while. \u201ccompleted_steps\u201d is logged at every step. i would like to see all datapoints where \u201closs\/eval\u201d is logged to be displayed in the plot, because there is not many of them (about 30 for each run). instead i only see random ones because apparently the downsampling procedure is based on frequency of \u201ccompleted_steps\u201d logging. as a result the plots are not very informative; i think a better behavior when plotting a metric x against metric y would be to fetch all (x, y) pairs first and then downsample if there is too many such pairs. can i hope to see improvements in downsampling logic at some point?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with suboptimal subsampling behavior when trying to build a plot of \u201closs\/eval\u201d vs \u201ccompleted_steps\u201d, resulting in plots that are not very informative.",
        "Question_preprocessed_content":"Title: suboptimal subsampling behavior; Content: i am trying to build a plot of vs is the validation loss and it is logged once in a while. is logged at every step. i would like to see all datapoints where is logged to be displayed in the plot, because there is not many of them . instead i only see random ones because apparently the downsampling procedure is based on frequency of logging. as a result the plots are not very informative; i think a better behavior when plotting a metric x against metric y would be to fetch all pairs first and then downsample if there is too many such pairs. can i hope to see improvements in downsampling logic at some point?",
        "Answer_original_content":"this discussion was followed up via email, but ill follow up here once its resolved this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer as the discussion was followed up via email. The topic has been automatically closed and new replies are no longer allowed.",
        "Answer_preprocessed_content":"this discussion was followed up via email, but ill follow up here once its resolved this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Team members can not access moved runs",
        "Question_body":"<p>I moved several runs to an existing team following the description here: <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/teams#move-runs-to-a-team\" class=\"inline-onebox\">Teams - Documentation<\/a>. I can see the runs in the team workspace, however the other members of the team still can not access these runs, despite the privacy of these runs showing as \u2018team\u2019. What can I do to share the runs with other members of the team?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1676621249270,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/team-members-can-not-access-moved-runs\/3903",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-17T10:46:13.311Z",
                "Answer_body":"<p>Hi Tom, thank you for reporting this issue. I will close this ticket here in the forum, as I have followed up with you by email, as more information regarding your account is needed - and we can continue the resolution from there.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: team members can not access moved runs; Content: i moved several runs to an existing team following the description here: teams - documentation. i can see the runs in the team workspace, however the other members of the team still can not access these runs, despite the privacy of these runs showing as team. what can i do to share the runs with other members of the team?",
        "Question_original_content_gpt_summary":"The user is unable to share runs with other members of their team after moving them to an existing team workspace.",
        "Question_preprocessed_content":"Title: team members can not access moved runs; Content: i moved several runs to an existing team following the description here teams documentation. i can see the runs in the team workspace, however the other members of the team still can not access these runs, despite the privacy of these runs showing as team. what can i do to share the runs with other members of the team?",
        "Answer_original_content":"hi tom, thank you for reporting this issue. i will close this ticket here in the forum, as i have followed up with you by email, as more information regarding your account is needed - and we can continue the resolution from there.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer. The support team has requested more information from the user via email to continue resolving the issue of not being able to share runs with team members after moving them to an existing team workspace.",
        "Answer_preprocessed_content":"hi tom, thank you for reporting this issue. i will close this ticket here in the forum, as i have followed up with you by email, as more information regarding your account is needed and we can continue the resolution from there."
    },
    {
        "Question_id":64875623.0,
        "Question_title":"Invoking an endpoint in AWS with a multidimensional array",
        "Question_body":"<p>I have deployed a Tensorflow-Model in SageMaker Studio following this tutorial:\n<a href=\"https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a>\nThe Model needs a Multidimensional Array as input. Invoking it from the Notebook itself is working:<\/p>\n<pre><code>import numpy as np\nimport json\ndata = np.load(&quot;testValues.npy&quot;)\npred=predictor.predict(data)\n<\/code><\/pre>\n<p>But I wasnt able to invoke it from a boto 3 client using this code:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n \nclient = boto3.client('runtime.sagemaker')\ndatain = np.load(&quot;testValues.npy&quot;)\ndata=datain.tolist();\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>This throws the Error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: Unknown&quot;}&quot;.\n<\/code><\/pre>\n<p>I guess the reason is the json Media Type but i have no clue how to get it back in shape.\nI tried this:<a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644<\/a> but it doesnt seem to change anything<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1605617435417,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":550.0,
        "Owner_creation_time":1542628542703,
        "Owner_last_access_time":1643066729968,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":0.0,
        "Answer_body":"<p>This fixed it for me:\nThe Content Type was missing.<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n\nclient = boto3.client('runtime.sagemaker',aws_access_key_id=..., aws_secret_access_key=...,region_name=...)\nendpoint_name = '...'\n\ndata = np.load(&quot;testValues.npy&quot;)\n\n\npayload = json.dumps(data.tolist())\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                                  ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nres = result['predictions']\nprint(&quot;test&quot;)\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1605773068536,
        "Answer_score":0.0,
        "Owner_location":"Germany",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64875623",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: invoking an endpoint in aws with a multidimensional array; Content: i have deployed a tensorflow-model in studio following this tutorial: https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-\/ the model needs a multidimensional array as input. invoking it from the notebook itself is working: import numpy as np import json data = np.load(\"testvalues.npy\") pred=predictor.predict(data) but i wasnt able to invoke it from a boto 3 client using this code: import json import boto3 import numpy as np import io client = boto3.client('runtime.') datain = np.load(\"testvalues.npy\") data=datain.tolist(); response = client.invoke_endpoint(endpointname=endpoint_name, body=json.dumps(data)) response_body = response['body'] print(response_body.read()) this throws the error: an error occurred (modelerror) when calling the invokeendpoint operation: received client error (415) from model with message \"{\"error\": \"unsupported media type: unknown\"}\". i guess the reason is the json media type but i have no clue how to get it back in shape. i tried this:https:\/\/github.com\/aws\/amazon--examples\/issues\/644 but it doesnt seem to change anything",
        "Question_original_content_gpt_summary":"The user is encountering a challenge when attempting to invoke an endpoint in AWS with a multidimensional array, receiving an error message of \"unsupported media type: unknown\".",
        "Question_preprocessed_content":"Title: invoking an endpoint in aws with a multidimensional array; Content: i have deployed a in studio following this tutorial the model needs a multidimensional array as input. invoking it from the notebook itself is working but i wasnt able to invoke it from a boto client using this code this throws the error i guess the reason is the json media type but i have no clue how to get it back in shape. i tried but it doesnt seem to change anything",
        "Answer_original_content":"this fixed it for me: the content type was missing. import json import boto3 import numpy as np import io client = boto3.client('runtime.',aws_access_key_id=..., aws_secret_access_key=...,region_name=...) endpoint_name = '...' data = np.load(\"testvalues.npy\") payload = json.dumps(data.tolist()) response = client.invoke_endpoint(endpointname=endpoint_name, contenttype='application\/json', body=payload) result = json.loads(response['body'].read().decode()) res = result['predictions'] print(\"test\")",
        "Answer_original_content_gpt_summary":"The solution to the error message \"unsupported media type: unknown\" when invoking an endpoint in AWS with a multidimensional array is to include the missing content type. This can be done by importing the necessary libraries, defining the endpoint name and data, and specifying the content type as 'application\/json' in the client.invoke_endpoint() function.",
        "Answer_preprocessed_content":"this fixed it for me the content type was missing."
    },
    {
        "Question_id":null,
        "Question_title":"Vertex AI Batch Predictions: Bigquery format must be used as input and output simultaneously",
        "Question_body":"I'm encountering an error when I try to create a batch prediction job with a bigquery table as my input, and a JSONL output in a GCS bucket. The documentation for batch predictions seems to indicate that I can do so, but I still see an error.I'm trying to create a batch prediction job on the Vertex AI console, and I see this error. ",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1667318280000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":70.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Batch-Predictions-Bigquery-format-must-be-used-as\/td-p\/484734\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-02T16:33:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"The documentation does seem unclear about this error, do you see the same issue if you test selecting a different output format, for instance CSV? This so I can investigate further and contact the appropriate teams with this inquiry."
            },
            {
                "Answer_creation_time":"2022-11-10T08:29:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"If you use BigQuery as your batch prediction input you only can store the batch prediction results in BigQuery (input and output has to be the same)"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: batch predictions: bigquery format must be used as input and output simultaneously; Content: i'm encountering an error when i try to create a batch prediction job with a bigquery table as my input, and a jsonl output in a gcs bucket. the documentation for batch predictions seems to indicate that i can do so, but i still see an error.i'm trying to create a batch prediction job on the console, and i see this error.",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to create a batch prediction job with a BigQuery table as input and a JSONL output in a GCS bucket.",
        "Question_preprocessed_content":"Title: batch predictions bigquery format must be used as input and output simultaneously; Content: i'm encountering an error when i try to create a batch prediction job with a bigquery table as my input, and a jsonl output in a gcs bucket. the documentation for batch predictions seems to indicate that i can do so, but i still see an trying to create a batch prediction job on the console, and i see this error.",
        "Answer_original_content":"the documentation does seem unclear about this error, do you see the same issue if you test selecting a different output format, for instance csv? this so i can investigate further and contact the appropriate teams with this inquiry. if you use bigquery as your batch prediction input you only can store the batch prediction results in bigquery (input and output has to be the same)",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Test selecting a different output format, such as CSV, to see if the error persists.\n- If using BigQuery as the batch prediction input, the output must also be stored in BigQuery.",
        "Answer_preprocessed_content":"the documentation does seem unclear about this error, do you see the same issue if you test selecting a different output format, for instance csv? this so i can investigate further and contact the appropriate teams with this inquiry. if you use bigquery as your batch prediction input you only can store the batch prediction results in bigquery"
    },
    {
        "Question_id":67210677.0,
        "Question_title":"If I close my JupyterLab from notebook instance, would my code be gone?",
        "Question_body":"<p>I'm new to AWS and I'm trying out AWS Sagemaker. I'm currently doing my project which involves quite a long time to finish and I don't think I can finish it in a day. I'm worried if I close my JupyterLab of my notebook instance in SageMaker, my code will be gone. How do I save my code and cell run progress when using Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1619084183027,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":305.0,
        "Owner_creation_time":1605938672327,
        "Owner_last_access_time":1622704795190,
        "Owner_reputation":97.0,
        "Owner_up_votes":18.0,
        "Owner_down_votes":0.0,
        "Owner_views":34.0,
        "Answer_body":"<p>If you are training directly in the notebook the answer is yes.\nHowever the best practice is not to train directly with the notebook.\nUse instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. In that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. You can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful!\nAs a quick example.. in my notebook I have this cell:<\/p>\n<pre><code>import sagemaker from sagemaker.tensorflow import TensorFlow from sagemaker import get_execution_role\n\nbucket = 'mybucket'\n\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\n\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\nhyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' :\n0.01}\n\nmetrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n\ninputs = {'train': train_data, 'test': validation_data}\n\nmyJobName = 'myname'\n\ntf_estimator.fit(inputs=inputs, job_name=myJobName)\n<\/code><\/pre>\n<p>My training script training.py is something like this:<\/p>\n<pre><code>if __name__ =='__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # input data and model directories\n    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n    parser.add_argument('--learning-rate', type=float, default=0.0001)\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--epochs', type=int, default=1)\n....\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1619085494700,
        "Answer_score":2.0,
        "Owner_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67210677",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: if i close my jupyterlab from notebook instance, would my code be gone?; Content: i'm new to aws and i'm trying out . i'm currently doing my project which involves quite a long time to finish and i don't think i can finish it in a day. i'm worried if i close my jupyterlab of my notebook instance in , my code will be gone. how do i save my code and cell run progress when using ?",
        "Question_original_content_gpt_summary":"The user is worried about losing their code and progress if they close their JupyterLab from their notebook instance in AWS.",
        "Question_preprocessed_content":"Title: if i close my jupyterlab from notebook instance, would my code be gone?; Content: i'm new to aws and i'm trying out . i'm currently doing my project which involves quite a long time to finish and i don't think i can finish it in a day. i'm worried if i close my jupyterlab of my notebook instance in , my code will be gone. how do i save my code and cell run progress when using ?",
        "Answer_original_content":"if you are training directly in the notebook the answer is yes. however the best practice is not to train directly with the notebook. use instead the notebook (you can choose a very cheap instance for the notebook) to launch your training job (in the instance type you desire) adapting you code to be the entrypoint of the estimator. in that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. you can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful! as a quick example.. in my notebook i have this cell: import from .tensorflow import tensorflow from import get_execution_role bucket = 'mybucket' train_data = 's3:\/\/{}\/{}'.format(bucket,'train') validation_data = 's3:\/\/{}\/{}'.format(bucket,'test') s3_output_location = 's3:\/\/{}'.format(bucket) hyperparameters = {'epochs': 70, 'batch-size' : 32, 'learning-rate' : 0.01} metrics = [{'name': 'loss', 'regex': 'loss: ([0-9\\.]+)'}, {'name': 'accuracy', 'regex': 'acc: ([0-9\\.]+)'}, {'name': 'epoch', 'regex': 'epoch ([0-9\\.]+)'}, {'name': 'validation_acc', 'regex': 'val_acc: ([0-9\\.]+)'}, {'name': 'validation_loss', 'regex': 'val_loss: ([0-9\\.]+)'}] tf_estimator = tensorflow(entry_point='training.py', role=get_execution_role(), train_instance_count=1, train_instance_type='ml.p2.xlarge', train_max_run=172800, output_path=s3_output_location, framework_version='1.12', py_version='py3', metric_definitions = metrics, hyperparameters = hyperparameters) inputs = {'train': train_data, 'test': validation_data} myjobname = 'myname' tf_estimator.fit(inputs=inputs, job_name=myjobname) my training script training.py is something like this: if __name__ =='__main__': parser = argparse.argumentparser() # input data and model directories parser.add_argument('--gpu-count', type=int, default=os.environ['sm_num_gpus']) parser.add_argument('--model-dir', type=str, default=os.environ.get('sm_model_dir')) parser.add_argument('--train', type=str, default=os.environ.get('sm_channel_train')) parser.add_argument('--test', type=str, default=os.environ.get('sm_channel_test')) parser.add_argument('--learning-rate', type=float, default=0.0001) parser.add_argument('--batch-size', type=int, default=32) parser.add_argument('--epochs', type=int, default=1) ....",
        "Answer_original_content_gpt_summary":"The answer suggests that the best practice is not to train directly with the notebook. Instead, use the notebook to launch the training job in the instance type you desire, adapting your code to be the entry point of the estimator. In this way, you can close the notebook after launching the training job and monitor the training job using CloudWatch. You can also define some regex to capture metrics from the stout, and CloudWatch will automatically plot for you.",
        "Answer_preprocessed_content":"if you are training directly in the notebook the answer is yes. however the best practice is not to train directly with the notebook. use instead the notebook to launch your training job adapting you code to be the entrypoint of the estimator. in that way, you can close the notebook after launching the training job and monitor the training job using cloudwatch. you can also define some regex to capture metrics from the stout and cloudwatch will automatically plot for you, which is very useful! as a quick in my notebook i have this cell my training script is something like this"
    },
    {
        "Question_id":63182406.0,
        "Question_title":"Does kedro support tfrecord?",
        "Question_body":"<p>To train tensorflow keras models on AI Platform using Docker containers, we convert our raw images stored on GCS to a tfrecord dataset using <code>tf.data.Dataset<\/code>. Thereby the data is never stored locally. Instead the raw images are transformed directly to tfrecords to another bucket. Is it possible to make use of <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\" rel=\"nofollow noreferrer\">kedro<\/a> with a tfrecord dataset and the streaming capability of <code>tf.data.Dataset<\/code>? According to the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\">docs<\/a> kedro doesn't seem to support tfrecord datasets.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1596148880400,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":107.0,
        "Owner_creation_time":1362514672823,
        "Owner_last_access_time":1663952544520,
        "Owner_reputation":1570.0,
        "Owner_up_votes":595.0,
        "Owner_down_votes":1.0,
        "Owner_views":159.0,
        "Answer_body":"<p>Only TF related dataset we have at the moment is <code>TensorFlowModelDataset<\/code> (<a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/_modules\/kedro\/extras\/datasets\/tensorflow\/tensorflow_model_dataset.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/latest\/_modules\/kedro\/extras\/datasets\/tensorflow\/tensorflow_model_dataset.html<\/a>), but you can easily <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/07_extend_kedro\/01_custom_datasets.html#custom-datasets\" rel=\"nofollow noreferrer\">add your own custom dataset<\/a>, or please add a feature request\/your contribution in <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\" rel=\"nofollow noreferrer\">the repo<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1596184916008,
        "Answer_score":3.0,
        "Owner_location":"Vorarlberg, Austria",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1596272833648,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63182406",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: does support tfrecord?; Content: to train tensorflow keras models on ai platform using docker containers, we convert our raw images stored on gcs to a tfrecord dataset using tf.data.dataset. thereby the data is never stored locally. instead the raw images are transformed directly to tfrecords to another bucket. is it possible to make use of with a tfrecord dataset and the streaming capability of tf.data.dataset? according to the docs doesn't seem to support tfrecord datasets.",
        "Question_original_content_gpt_summary":"The user is questioning whether they can use AI Platform with a TFRecord dataset and the streaming capability of tf.data.dataset, as the documentation suggests that AI Platform does not support TFRecord datasets.",
        "Question_preprocessed_content":"Title: does support tfrecord?; Content: to train tensorflow keras models on ai platform using docker containers, we convert our raw images stored on gcs to a tfrecord dataset using . thereby the data is never stored locally. instead the raw images are transformed directly to tfrecords to another bucket. is it possible to make use of with a tfrecord dataset and the streaming capability of ? according to the docs doesn't seem to support tfrecord datasets.",
        "Answer_original_content":"only tf related dataset we have at the moment is tensorflowmodeldataset (https:\/\/.readthedocs.io\/en\/latest\/_modules\/\/extras\/datasets\/tensorflow\/tensorflow_model_dataset.html), but you can easily add your own custom dataset, or please add a feature request\/your contribution in the repo",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Use the tensorflowmodeldataset provided by AI Platform, which is the only tf related dataset available at the moment.\n- Add your own custom dataset to AI Platform.\n- Submit a feature request or contribute to the repository.",
        "Answer_preprocessed_content":"only tf related dataset we have at the moment is , but you can easily add your own custom dataset, or please add a feature contribution in the repo"
    },
    {
        "Question_id":null,
        "Question_title":"Recording videos of custom gym environments",
        "Question_body":"<p>Hi, I am try to use VecVideoRecorder to log videos of my custom environment. The observation come from a camera, though I don\u2019t think that is the issue.  The error is:<\/p>\n<p>AttributeError(\u201c\u2018VideoRecorder\u2019 object has no attribute \u2018path\u2019\u201d)<\/p>\n<p>I\u2019m not directly setting nor accessing an attribute \u2018path\u2019 so I\u2019m having identifying where this is coming from. My code looks like this:<\/p>\n<pre><code>def make_env():\n    env = DummyVecEnv([lambda:    Monitor(ReacherFiveJointsImageSpace(random_start=wandb.config.random_start,\n                                                        log_state_actions=True,\n                                                        shape_reward=wandb.config.shape_reward,\n                                                        file_name_prefix=wandb.config.rl_name,\n                                                        env_type=wandb.config.env_type,\n                                                        seed=(wandb.config.seed+wandb.config.run)),\n                                       log_dir)])\n    env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=5.)\n    env = VecVideoRecorder(env, video_folder=log_dir,\n                           record_video_trigger=lambda x: x % 100 == 0, video_length=10)  # record videos\n    stats_path = os.path.join(log_dir,\n                              \"run\" + str(wandb.config.run) + \"_vec_normalize_\" + run.id + \".pkl\")\n    env.save(stats_path)\n    return env\n<\/code><\/pre>\n<p>Can anyone point me in the right direction?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1663105781075,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":683.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/recording-videos-of-custom-gym-environments\/3110",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-15T21:51:44.445Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cbellinger\">@cbellinger<\/a> , this doesn\u2019t appear to be directly tied to W&amp;B.  Can you please revisit source code for the <a href=\"https:\/\/stable-baselines.readthedocs.io\/en\/master\/_modules\/stable_baselines\/common\/vec_env\/vec_video_recorder.html\" rel=\"noopener nofollow ugc\">videorecorder<\/a> to identify if your setup is correct. Thanks<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-21T06:14:12.613Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cbellinger\">@cbellinger<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-20T06:14:50.416Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: recording videos of custom gym environments; Content: hi, i am try to use vecvideorecorder to log videos of my custom environment. the observation come from a camera, though i don\u2019t think that is the issue. the error is: attributeerror(\u201c\u2018videorecorder\u2019 object has no attribute \u2018path\u2019\u201d) i\u2019m not directly setting nor accessing an attribute \u2018path\u2019 so i\u2019m having identifying where this is coming from. my code looks like this: def make_env(): env = dummyvecenv([lambda: monitor(reacherfivejointsimagespace(random_start=.config.random_start, log_state_actions=true, shape_reward=.config.shape_reward, file_name_prefix=.config.rl_name, env_type=.config.env_type, seed=(.config.seed+.config.run)), log_dir)]) env = vecnormalize(env, norm_obs=true, norm_reward=true, clip_obs=5.) env = vecvideorecorder(env, video_folder=log_dir, record_video_trigger=lambda x: x % 100 == 0, video_length=10) # record videos stats_path = os.path.join(log_dir, \"run\" + str(.config.run) + \"_vec_normalize_\" + run.id + \".pkl\") env.save(stats_path) return env can anyone point me in the right direction? thanks!",
        "Question_original_content_gpt_summary":"The user is encountering an AttributeError while attempting to record videos of a custom gym environment using vecvideorecorder.",
        "Question_preprocessed_content":"Title: recording videos of custom gym environments; Content: hi, i am try to use vecvideorecorder to log videos of my custom environment. the observation come from a camera, though i dont think that is the issue. the error is attributeerror im not directly setting nor accessing an attribute path so im having identifying where this is coming from. my code looks like this can anyone point me in the right direction? thanks!",
        "Answer_original_content":"hi @cbellinger , this doesnt appear to be directly tied to w&b. can you please revisit source code for the videorecorder to identify if your setup is correct. thanks hi @cbellinger , since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no specific solutions provided in the answer. The responder suggests revisiting the source code for the videorecorder to identify if the setup is correct. If the user wants to re-open the conversation, they can let the responder know.",
        "Answer_preprocessed_content":"hi , this doesnt appear to be directly tied to w&b. can you please revisit source code for the videorecorder to identify if your setup is correct. thanks hi , since we have not heard back from you we are going to close this request. if you would like to the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Due to an error, Vertex AI was unable to train model \"some_model",
        "Question_body":"Hi Team\nWe are trying to train the model, but we are getting the below error after running 2 hrs.Region       : us-centerl1(IOWA) Algorithm : AutoML\nObjective  : Image classification (Single-label)\nData split:   Randomly assigned (80\/10\/10)Due to an error, Vertex AI was unable to train model \"some_model\".\nAdditional Details:\nOperation State: Failed with errors\nResource Name: \nprojects\/1096088445304\/locations\/us-central1\/trainingPipelines\/8154185764406558720\nError Messages: INTERNALKindly help us to resolve the issue. \nThanks & Regards\nJambu ",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1665301980000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":61.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Due-to-an-error-Vertex-AI-was-unable-to-train-model-quot-some\/td-p\/476128\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-11T08:14:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Could you please share more information on the error message that you are receiving, since the information you are sharing isn\u2019t enough to properly help with the issue you are facing.\n\nThe internal errors occur when there\u2019s an issue with your system. The error could be transient, try to resubmit the CustomJob, HyperparameterTuningJob or TrainingPipeline, if the error persists what is recommended that you do is to contact support."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: due to an error, was unable to train model \"some_model; Content: hi team we are trying to train the model, but we are getting the below error after running 2 hrs.region : us-centerl1(iowa) algorithm : automl objective : image classification (single-label) data split: randomly assigned (80\/10\/10)due to an error, was unable to train model \"some_model\". additional details: operation state: failed with errors resource name: projects\/1096088445304\/locations\/us-central1\/trainingpipelines\/8154185764406558720 error messages: internalkindly help us to resolve the issue. thanks & regards jambu",
        "Question_original_content_gpt_summary":"The user was unable to train the model \"some_model\" due to an error encountered while running the algorithm \"automl\" for image classification.",
        "Question_preprocessed_content":"Title: due to an error, was unable to train model ; Content: hi team we are trying to train the model, but we are getting the below error after running algorithm automl objective image classification data split randomly assigned due to an error, was unable to train model additional details operation state failed with errors resource name error messages internalkindly help us to resolve the issue. thanks & regards jambu",
        "Answer_original_content":"could you please share more information on the error message that you are receiving, since the information you are sharing isnt enough to properly help with the issue you are facing. the internal errors occur when theres an issue with your system. the error could be transient, try to resubmit the customjob, hyperparametertuningjob or trainingpipeline, if the error persists what is recommended that you do is to contact support.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Share more information on the error message received.\n- Resubmit the custom job, hyperparameter tuning job, or training pipeline as the error could be transient.\n- Contact support if the error persists.",
        "Answer_preprocessed_content":"could you please share more information on the error message that you are receiving, since the information you are sharing isnt enough to properly help with the issue you are facing. the internal errors occur when theres an issue with your system. the error could be transient, try to resubmit the customjob, hyperparametertuningjob or trainingpipeline, if the error persists what is recommended that you do is to contact support."
    },
    {
        "Question_id":68402406.0,
        "Question_title":"MLflow webserver returns 400 status, \"Incompatible input types for column X. Can not safely convert float64 to <U0.\"",
        "Question_body":"<p>I am implementing an anomaly detection web service using <code>MLflow<\/code> and <code>sklearn.pipeline.Pipeline()<\/code>. The aim of the model is to detect web crawlers using server log and <code>response_length<\/code> column is one of my features. After serving model, for testing the web service I send below request that contains the 20 first columns of the train data.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>$ curl  --location --request POST '127.0.0.1:8000\/invocations'\n        --header 'Content-Type: text\/csv' \\\n        --data-binary 'datasets\/test.csv'\n<\/code><\/pre>\n<p>But response of the web server has status code 400 (BAD REQUEST) and this JSON body:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;error_code&quot;: &quot;BAD_REQUEST&quot;,\n    &quot;message&quot;: &quot;Incompatible input types for column response_length. Can not safely convert float64 to &lt;U0.&quot;\n}\n<\/code><\/pre>\n<p>Here is the model compilation MLflow Tracking component log:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[Pipeline] ......... (step 1 of 3) Processing transform, total=11.8min\n[Pipeline] ............... (step 2 of 3) Processing pca, total=   4.8s\n[Pipeline] ........ (step 3 of 3) Processing rule_based, total=   0.0s\n2021\/07\/16 04:55:12 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n2021\/07\/16 04:55:12 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: &quot;\/home\/matin\/workspace\/Rahnema College\/venv\/lib\/python3.8\/site-packages\/mlflow\/models\/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values &lt;https:\/\/www.mlflow.org\/docs\/latest\/models.html#handling-integers-with-missing-values&gt;`_ for more details.&quot;\nLogged data and model in run: 8843336f5c31482c9e246669944b1370\n\n---------- logged params ----------\n{'memory': 'None',\n 'pca': 'PCAEstimator()',\n 'rule_based': 'RuleBasedEstimator()',\n 'steps': &quot;[('transform', &lt;log_transformer.LogTransformer object at &quot;\n          &quot;0x7f05a8b95760&gt;), ('pca', PCAEstimator()), ('rule_based', &quot;\n          'RuleBasedEstimator())]',\n 'transform': '&lt;log_transformer.LogTransformer object at 0x7f05a8b95760&gt;',\n 'verbose': 'True'}\n\n---------- logged metrics ----------\n{}\n\n---------- logged tags ----------\n{'estimator_class': 'sklearn.pipeline.Pipeline', 'estimator_name': 'Pipeline'}\n\n---------- logged artifacts ----------\n['model\/MLmodel',\n 'model\/conda.yaml',\n 'model\/model.pkl',\n 'model\/requirements.txt']\n<\/code><\/pre>\n<p>Could anyone tell me exactly how I can fix this model serve problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1626398499507,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":787.0,
        "Owner_creation_time":1553088438367,
        "Owner_last_access_time":1664054397820,
        "Owner_reputation":415.0,
        "Owner_up_votes":300.0,
        "Owner_down_votes":2.0,
        "Owner_views":37.0,
        "Answer_body":"<p>The problem caused by <code>mlflow.utils.autologging_utils<\/code> WARNING.<\/p>\n<p>When the model is created, data input signature is saved on the <code>MLmodel<\/code> file with some.\nYou should change <code>response_length<\/code> signature input type from <code>string<\/code> to <code>double<\/code> by replacing<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;double&quot;}\n<\/code><\/pre>\n<p>instead of<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;string&quot;}\n<\/code><\/pre>\n<p>so it doesn't need to be converted. After serving the model with edited <code>MLmodel<\/code> file, the web server worked as expected.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626398499507,
        "Answer_score":1.0,
        "Owner_location":"Tehran, Tehran Province, Iran",
        "Question_last_edit_time":1657456057947,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68402406",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: webserver returns 400 status, \"incompatible input types for column x. can not safely convert float64 to i am implementing an anomaly detection web service using and sklearn.pipeline.pipeline(). the aim of the model is to detect web crawlers using server log and response_length column is one of my features. after serving model, for testing the web service i send below request that contains the 20 first columns of the train data. $ curl --location --request post '127.0.0.1:8000\/invocations' --header 'content-type: text\/csv' \\ --data-binary 'datasets\/test.csv' but response of the web server has status code 400 (bad request) and this json body: { \"error_code\": \"bad_request\", \"message\": \"incompatible input types for column response_length. can not safely convert float64 to <u0.\" } here is the model compilation tracking component log: [pipeline] ......... (step 1 of 3) processing transform, total=11.8min [pipeline] ............... (step 2 of 3) processing pca, total= 4.8s [pipeline] ........ (step 3 of 3) processing rule_based, total= 0.0s 2021\/07\/16 04:55:12 warning .sklearn: training metrics will not be recorded because training labels were not specified. to automatically record training metrics, provide training labels as inputs to the model training function. 2021\/07\/16 04:55:12 warning .utils.autologging_utils: autologging encountered a warning: \"\/home\/matin\/workspace\/rahnema college\/venv\/lib\/python3.8\/site-packages\/\/models\/signature.py:129: userwarning: hint: inferred schema contains integer column(s). integer columns in python cannot represent missing values. if your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. the best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. see `handling integers with missing values <https:\/\/www..org\/docs\/latest\/models.html#handling-integers-with-missing-values>`_ for more details.\" logged data and model in run: 8843336f5c31482c9e246669944b1370 ---------- logged params ---------- {'memory': 'none', 'pca': 'pcaestimator()', 'rule_based': 'rulebasedestimator()', 'steps': \"[('transform', <log_transformer.logtransformer object at \" \"0x7f05a8b95760>), ('pca', pcaestimator()), ('rule_based', \" 'rulebasedestimator())]', 'transform': '<log_transformer.logtransformer object at 0x7f05a8b95760>', 'verbose': 'true'} ---------- logged metrics ---------- {} ---------- logged tags ---------- {'estimator_class': 'sklearn.pipeline.pipeline', 'estimator_name': 'pipeline'} ---------- logged artifacts ---------- ['model\/mlmodel', 'model\/conda.yaml', 'model\/model.pkl', 'model\/requirements.txt'] could anyone tell me exactly how i can fix this model serve problem?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with their webserver returning a 400 status code and an \"incompatible input types for column x. can not safely convert float64 to <u0.\" error when attempting to serve a model using sklearn.pipeline.pipeline().",
        "Question_preprocessed_content":"Title: webserver returns status, incompatible input types for column x. can not safely convert float to ; Content: i am implementing an anomaly detection web service using and . the aim of the model is to detect web crawlers using server log and column is one of my features. after serving model, for testing the web service i send below request that contains the first columns of the train data. but response of the web server has status code and this json body here is the model compilation tracking component log could anyone tell me exactly how i can fix this model serve problem?",
        "Answer_original_content":"the problem caused by .utils.autologging_utils warning. when the model is created, data input signature is saved on the mlmodel file with some. you should change response_length signature input type from string to double by replacing {\"name\": \"response_length\", \"type\": \"double\"} instead of {\"name\": \"response_length\", \"type\": \"string\"} so it doesn't need to be converted. after serving the model with edited mlmodel file, the web server worked as expected.",
        "Answer_original_content_gpt_summary":"The solution to the problem of encountering a 400 status code and an \"incompatible input types for column x. can not safely convert float64 to <u0.\" error when serving a model using sklearn.pipeline.pipeline() is to change the response_length signature input type from string to double by replacing {\"name\": \"response_length\", \"type\": \"string\"} with {\"name\": \"response_length\", \"type\": \"double\"} in the mlmodel file. This will prevent the need for conversion and allow the web server to work as expected.",
        "Answer_preprocessed_content":"the problem caused by warning. when the model is created, data input signature is saved on the file with some. you should change signature input type from to by replacing instead of so it doesn't need to be converted. after serving the model with edited file, the web server worked as expected."
    },
    {
        "Question_id":null,
        "Question_title":"how i can recover my compute instance ?",
        "Question_body":"error: The specified Azure ML Compute Instance cs-bi-cloud2 encountered an unusable node. Please try to restart the compute instance to recover. If it failed at creation time, please delete and try to recreate the compute instance. If the problem persists, please follow up with Azure Suppor",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1655132409567,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/887255\/how-i-can-recover-my-compute-instance.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-27T16:42:10.383Z",
                "Answer_score":0,
                "Answer_body":"Hello @csbicloudcsbicloud-1019\n\nHope you have solved this issue since you have not respponded to Ram's comment.\n\nAs the error message said, please try to restart the compute instance to recover. If it failed at creation time, please delete and try to recreate the compute instance. If the problem persists, please follow up with Azure Support.\n\nPlease let us know if you still see this error and we can help you to connect with support to check on the backend, we are glad to help more. Thanks a lot.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how i can recover my compute instance ?; Content: error: the specified compute instance cs-bi-cloud2 encountered an unusable node. please try to restart the compute instance to recover. if it failed at creation time, please delete and try to recreate the compute instance. if the problem persists, please follow up with azure suppor",
        "Question_original_content_gpt_summary":"The user encountered an unusable node while attempting to recover their compute instance, and was advised to restart, delete, and recreate the compute instance, or to follow up with Azure Support.",
        "Question_preprocessed_content":"Title: how i can recover my compute instance ?; Content: error the specified compute instance encountered an unusable node. please try to restart the compute instance to recover. if it failed at creation time, please delete and try to recreate the compute instance. if the problem persists, please follow up with azure suppor",
        "Answer_original_content":"hello @csbicloudcsbicloud-1019 hope you have solved this issue since you have not respponded to ram's comment. as the error message said, please try to restart the compute instance to recover. if it failed at creation time, please delete and try to recreate the compute instance. if the problem persists, please follow up with azure support. please let us know if you still see this error and we can help you to connect with support to check on the backend, we are glad to help more. thanks a lot. regards, yutong",
        "Answer_original_content_gpt_summary":"Possible solutions to the unusable node issue are to restart the compute instance, delete and recreate it if it failed at creation time, or follow up with Azure Support. If the problem persists, the user can connect with support to check on the backend.",
        "Answer_preprocessed_content":"hello hope you have solved this issue since you have not respponded to ram's comment. as the error message said, please try to restart the compute instance to recover. if it failed at creation time, please delete and try to recreate the compute instance. if the problem persists, please follow up with azure support. please let us know if you still see this error and we can help you to connect with support to check on the backend, we are glad to help more. thanks a lot. regards, yutong"
    },
    {
        "Question_id":68186468.0,
        "Question_title":"Is there a REST API available for SageMaker, or is it possible to interact with SageMaker over the Amazon API Gateway?",
        "Question_body":"<p>SageMaker provides a full machine learning development environment on AWS. It works with the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Amazon SageMaker Python SDK<\/a>, which allows Jupyter Notebooks to interact with the functionality. This also provides the path to using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_featurestore.html\" rel=\"nofollow noreferrer\">Amazon SageMaker Feature Store<\/a>.<\/p>\n<p>Is there any REST API available for SageMaker? Say one wanted to create their own custom UI, but still use SageMaker features, is this possible?<\/p>\n<p>Can it be done using the <a href=\"https:\/\/aws.amazon.com\/api-gateway\/\" rel=\"nofollow noreferrer\">Amazon API Gateway<\/a>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1625006659240,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1164.0,
        "Owner_creation_time":1346443720088,
        "Owner_last_access_time":1664079461296,
        "Owner_reputation":11650.0,
        "Owner_up_votes":6318.0,
        "Owner_down_votes":21.0,
        "Owner_views":977.0,
        "Answer_body":"<p>Amazon API Gateway currently does not provide first-class integration for SageMaker. But you can use these services via AWS SDK. If you wish, you can embed the AWS SDK calls into a service, host on AWS (e.g. running on EC2 or as lambda functions) and use API gateway to expose your REST API.<\/p>\n<p>Actually, SageMaker is not fundamentally different from any other AWS service from this aspect.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1626469400220,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68186468",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is there a rest api available for , or is it possible to interact with over the amazon api gateway?; Content: provides a full machine learning development environment on aws. it works with the python sdk, which allows jupyter notebooks to interact with the functionality. this also provides the path to using the feature store. is there any rest api available for ? say one wanted to create their own custom ui, but still use features, is this possible? can it be done using the amazon api gateway?",
        "Question_original_content_gpt_summary":"The user is inquiring about the possibility of using a REST API or Amazon API Gateway to interact with the features of a machine learning development environment on AWS.",
        "Question_preprocessed_content":"Title: is there a rest api available for , or is it possible to interact with over the amazon api gateway?; Content: provides a full machine learning development environment on aws. it works with the python sdk, which allows jupyter notebooks to interact with the functionality. this also provides the path to using the feature store. is there any rest api available for ? say one wanted to create their own custom ui, but still use features, is this possible? can it be done using the amazon api gateway?",
        "Answer_original_content":"amazon api gateway currently does not provide first-class integration for . but you can use these services via aws sdk. if you wish, you can embed the aws sdk calls into a service, host on aws (e.g. running on ec2 or as lambda functions) and use api gateway to expose your rest api. actually, is not fundamentally different from any other aws service from this aspect.",
        "Answer_original_content_gpt_summary":"Possible solutions: \n- Use AWS SDK to interact with the features of a machine learning development environment on AWS.\n- Embed the AWS SDK calls into a service, host on AWS (e.g. running on EC2 or as Lambda functions) and use API Gateway to expose your REST API.",
        "Answer_preprocessed_content":"amazon api gateway currently does not provide integration for . but you can use these services via aws sdk. if you wish, you can embed the aws sdk calls into a service, host on aws and use api gateway to expose your rest api. actually, is not fundamentally different from any other aws service from this aspect."
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML connectivity with hive metastore",
        "Question_body":"Dear MS team,\nwe're exploring the AML implementation in our org and i found AML can connect to ADLS2 through datastore; but I'm not sure if it can read the hive metastore and read the tables directly?\n\n( i choose some random tag below as it's not allowing to submit the question, pls ignore the tagging for this question)",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1649938200287,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/812660\/azure-ml-connectivity-with-hive-metastore.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-02T23:04:22.353Z",
                "Answer_score":0,
                "Answer_body":"Any update? Seems Azure only support Azure storage now, any roadmap?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: connectivity with hive metastore; Content: dear ms team, we're exploring the aml implementation in our org and i found aml can connect to adls2 through datastore; but i'm not sure if it can read the hive metastore and read the tables directly? ( i choose some random tag below as it's not allowing to submit the question, pls ignore the tagging for this question)",
        "Question_original_content_gpt_summary":"The user is exploring the implementation of Azure Machine Learning and is unsure if it can read the Hive Metastore and read the tables directly.",
        "Question_preprocessed_content":"Title: connectivity with hive metastore; Content: dear ms team, we're exploring the aml implementation in our org and i found aml can connect to adls through datastore; but i'm not sure if it can read the hive metastore and read the tables directly? i choose some random tag below as it's not allowing to submit the question, pls ignore the tagging for this question",
        "Answer_original_content":"any update? seems azure only support azure storage now, any roadmap?",
        "Answer_original_content_gpt_summary":"There is currently no support for reading the Hive Metastore directly in Azure Machine Learning. The only supported option is to use Azure Storage. There is no information available about any future roadmap for supporting Hive Metastore.",
        "Answer_preprocessed_content":"any update? seems azure only support azure storage now, any roadmap?"
    },
    {
        "Question_id":null,
        "Question_title":"GC overhead limit exceeded",
        "Question_body":"I have a modest size dataset, and I am running Jupyter Notebook in Sagemaker (instance type ml.c5.xlarge with 200G instance size). I receive the error message \" GC overhead limit exceeded\" Everything ran fine with small data size. BTW, I need to go through the dataframe one row at a time using df.collect(), which seems t be an expensive operation... Would you suggest another way of accomplishing this? I would appreciate your kind help.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1654390632353,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":87.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDc_WeDqcTjitN1bkIJRosg\/gc-overhead-limit-exceeded",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-08T07:40:20.941Z",
                "Answer_score":0,
                "Answer_body":"The GC overhead limit exceeded error indicates that the JVM spent a lot of time on garbage collection but recovered very little memory, so it throws this error to let you know that your program is not making much progress but wasting time on doing useless garbage collection task. Iterating through the dataframe might be the problem, because you might be creating a lot of temporary objects when you go through each line, and they couldn't be garbage collected. What is the framework that you are using? And what are you trying to do by going through the dataframe row-by-row? Maybe you can think about processing multiple lines in a batch? For example using some vectorization or matrix operation as georgios_s suggested in the comment.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: gc overhead limit exceeded; Content: i have a modest size dataset, and i am running jupyter notebook in (instance type ml.c5.xlarge with 200g instance size). i receive the error message \" gc overhead limit exceeded\" everything ran fine with small data size. btw, i need to go through the dataframe one row at a time using df.collect(), which seems t be an expensive operation... would you suggest another way of accomplishing this? i would appreciate your kind help.",
        "Question_original_content_gpt_summary":"The user is encountering a \"gc overhead limit exceeded\" error while running Jupyter Notebook on an ml.c5.xlarge instance with 200g instance size, and is looking for an alternative way to go through a dataframe one row at a time using df.collect().",
        "Question_preprocessed_content":"Title: gc overhead limit exceeded; Content: i have a modest size dataset, and i am running jupyter notebook in . i receive the error message gc overhead limit exceeded everything ran fine with small data size. btw, i need to go through the dataframe one row at a time using which seems t be an expensive would you suggest another way of accomplishing this? i would appreciate your kind help.",
        "Answer_original_content":"the gc overhead limit exceeded error indicates that the jvm spent a lot of time on garbage collection but recovered very little memory, so it throws this error to let you know that your program is not making much progress but wasting time on doing useless garbage collection task. iterating through the dataframe might be the problem, because you might be creating a lot of temporary objects when you go through each line, and they couldn't be garbage collected. what is the framework that you are using? and what are you trying to do by going through the dataframe row-by-row? maybe you can think about processing multiple lines in a batch? for example using some vectorization or matrix operation as georgios_s suggested in the comment.",
        "Answer_original_content_gpt_summary":"Possible solutions to the \"gc overhead limit exceeded\" error while running Jupyter Notebook on an ml.c5.xlarge instance with 200g instance size are to avoid iterating through the dataframe one row at a time using df.collect() and to consider processing multiple lines in a batch using vectorization or matrix operation. It is also suggested to provide more context about the framework being used and the purpose of going through the dataframe row-by-row.",
        "Answer_preprocessed_content":"the gc overhead limit exceeded error indicates that the jvm spent a lot of time on garbage collection but recovered very little memory, so it throws this error to let you know that your program is not making much progress but wasting time on doing useless garbage collection task. iterating through the dataframe might be the problem, because you might be creating a lot of temporary objects when you go through each line, and they couldn't be garbage collected. what is the framework that you are using? and what are you trying to do by going through the dataframe maybe you can think about processing multiple lines in a batch? for example using some vectorization or matrix operation as suggested in the comment."
    },
    {
        "Question_id":66950948.0,
        "Question_title":"How to make inference to a keras model hosted on AWS SageMaker via AWS Lambda function?",
        "Question_body":"<p>I have a pre-trained <code>keras<\/code> model which I have hosted on <code>AWS<\/code> using <code>AWS SageMaker<\/code>. I've got an <code>endpoint<\/code> and I can make successful <code>predictions<\/code> using the <code>Amazon SageMaker Notebook instance<\/code>.<\/p>\n<p>What I do there is that I serve a <code>.PNG image<\/code> like the following and the model gives me correct prediction.<\/p>\n<pre><code>file= s3.Bucket(bucketname).download_file(filename_1, 'normal.png')\nfile_name_1='normal.png'\n\n\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\n\nendpoint = 'tensorflow-inference-0000-11-22-33-44-55-666' #endpoint\n\npredictor=sagemaker.tensorflow.model.TensorFlowPredictor(endpoint, sagemaker_session)\ndata = np.array([resize(imread(file_name), (137, 310, 3))])\npredictor.predict(data)\n<\/code><\/pre>\n<p>Now I wanted to make predictions using a <code>mobile application<\/code>. For that I have to wrote a <code>Lambda function<\/code> in python and attached an <code>API gateway<\/code> to it. My <code>Lambda function<\/code> is the following.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nfrom scipy import signal\nfrom scipy.signal import butter, lfilter\nfrom scipy.io import wavfile\nimport scipy.signal as sps\nimport io\nfrom io import BytesIO\nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nfrom datetime import datetime\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom PIL import Image\n\nENDPOINT_NAME = 'tensorflow-inference-0000-11-22-33-44-55-666'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    image = Image.open(io.BytesIO(decoded_file_name))\n\n    data = np.array([resize(imread(image), (137, 310, 3))])\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='text\/csv', Body=data)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n<\/code><\/pre>\n<p>The third last line is giving me error <code>'PngImageFile' object has no attribute 'read'<\/code>.\nAny idea what I am missing here?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1617615515810,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":114.0,
        "Owner_creation_time":1528629350990,
        "Owner_last_access_time":1663824563776,
        "Owner_reputation":439.0,
        "Owner_up_votes":80.0,
        "Owner_down_votes":0.0,
        "Owner_views":51.0,
        "Answer_body":"<p>I was missing one thing which was causing this error. After receiving the image data I used python list and then <code>json.dump<\/code> that list (of lists). Below is the code for reference.<\/p>\n<pre><code>import os\nimport sys\n\nCWD = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(CWD, &quot;lib&quot;))\n\nimport json\nimport base64\nimport boto3\nimport numpy as np\nimport io\nfrom io import BytesIO\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\n# grab environment variable of Lambda Function\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    s3 = boto3.client(&quot;s3&quot;)\n    \n    # retrieving data from event.\n    get_file_content_from_postman = event[&quot;content&quot;]\n    \n    # decoding data.\n    decoded_file_name = base64.b64decode(get_file_content_from_postman)\n    \n    data = np.array([resize(imread(io.BytesIO(decoded_file_name)), (137, 310, 3))])\n    \n    payload = json.dumps(data.tolist())\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n        \n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n        \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618052704636,
        "Answer_score":0.0,
        "Owner_location":"Pakistan",
        "Question_last_edit_time":1618052912968,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66950948",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to make inference to a keras model hosted on via aws lambda function?; Content: i have a pre-trained keras model which i have hosted on aws using . i've got an endpoint and i can make successful predictions using the notebook instance. what i do there is that i serve a .png image like the following and the model gives me correct prediction. file= s3.bucket(bucketname).download_file(filename_1, 'normal.png') file_name_1='normal.png' import from .tensorflow.model import tensorflowmodel endpoint = 'tensorflow-inference-0000-11-22-33-44-55-666' #endpoint predictor=.tensorflow.model.tensorflowpredictor(endpoint, _session) data = np.array([resize(imread(file_name), (137, 310, 3))]) predictor.predict(data) now i wanted to make predictions using a mobile application. for that i have to wrote a lambda function in python and attached an api gateway to it. my lambda function is the following. import os import sys cwd = os.path.dirname(os.path.realpath(__file__)) sys.path.insert(0, os.path.join(cwd, \"lib\")) import json import base64 import boto3 import numpy as np from scipy import signal from scipy.signal import butter, lfilter from scipy.io import wavfile import scipy.signal as sps import io from io import bytesio import matplotlib.pylab as plt from matplotlib import pyplot as plt import matplotlib.image as mpimg from datetime import datetime from skimage.io import imread from skimage.transform import resize from pil import image endpoint_name = 'tensorflow-inference-0000-11-22-33-44-55-666' runtime= boto3.client('runtime.') def lambda_handler(event, context): s3 = boto3.client(\"s3\") # retrieving data from event. get_file_content_from_postman = event[\"content\"] # decoding data. decoded_file_name = base64.b64decode(get_file_content_from_postman) image = image.open(io.bytesio(decoded_file_name)) data = np.array([resize(imread(image), (137, 310, 3))]) response = runtime.invoke_endpoint(endpointname=endpoint_name, contenttype='text\/csv', body=data) result = json.loads(response['body'].read().decode()) return result the third last line is giving me error 'pngimagefile' object has no attribute 'read'. any idea what i am missing here?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in making inference to a Keras model hosted on AWS Lambda Function, receiving an error of 'pngimagefile' object has no attribute 'read'.",
        "Question_preprocessed_content":"Title: how to make inference to a keras model hosted on via aws lambda function?; Content: i have a model which i have hosted on using . i've got an and i can make successful using the . what i do there is that i serve a like the following and the model gives me correct prediction. now i wanted to make predictions using a . for that i have to wrote a in python and attached an to it. my is the following. the third last line is giving me error . any idea what i am missing here?",
        "Answer_original_content":"i was missing one thing which was causing this error. after receiving the image data i used python list and then json.dump that list (of lists). below is the code for reference. import os import sys cwd = os.path.dirname(os.path.realpath(__file__)) sys.path.insert(0, os.path.join(cwd, \"lib\")) import json import base64 import boto3 import numpy as np import io from io import bytesio from skimage.io import imread from skimage.transform import resize # grab environment variable of lambda function endpoint_name = os.environ['endpoint_name'] runtime= boto3.client('runtime.') def lambda_handler(event, context): s3 = boto3.client(\"s3\") # retrieving data from event. get_file_content_from_postman = event[\"content\"] # decoding data. decoded_file_name = base64.b64decode(get_file_content_from_postman) data = np.array([resize(imread(io.bytesio(decoded_file_name)), (137, 310, 3))]) payload = json.dumps(data.tolist()) response = runtime.invoke_endpoint(endpointname=endpoint_name, contenttype='application\/json', body=payload) result = json.loads(response['body'].read().decode()) return result",
        "Answer_original_content_gpt_summary":"The answer does not provide any solutions to the error mentioned in the question. Instead, it provides a code snippet that retrieves data from an event, decodes it, resizes it, and then sends it to an endpoint for inference.",
        "Answer_preprocessed_content":"i was missing one thing which was causing this error. after receiving the image data i used python list and then that list . below is the code for reference."
    },
    {
        "Question_id":null,
        "Question_title":"Early Stopping",
        "Question_body":"<p>Hello All,<\/p>\n<p>I\u2019m configuring a hyper parameter sweep. I have training, validation, and test set.<\/p>\n<p>I\u2019d like to use the test_loss as the final metric to optimize and val_loss for early stopping.<\/p>\n<p>I don\u2019t see a place to specify a metric for early stopping. Does it default to the same metric specified for overall optimization (of hyper parameters)? If so, how can I change this?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":null,
        "Question_creation_time":1630705196042,
        "Question_favorite_count":null,
        "Question_score":13.0,
        "Question_view_count":1487.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/early-stopping\/422",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-04T10:00:58.565Z",
                "Answer_body":"<p>It isn\u2019t possible to have a different metric for hyperband early stopping and search strategy. <a href=\"https:\/\/github.com\/wandb\/sweeps\/blob\/master\/hyperband_stopping.py#L176\">https:\/\/github.com\/wandb\/sweeps\/blob\/master\/hyperband_stopping.py#L176<\/a><\/p>\n<p>One workaround would be to use a search strategy that doesn\u2019t require a metric like <code>random<\/code> or <code>grid<\/code> and then use <code>val_loss<\/code> as your metric for early stopping. You can then easily reconfigure the resulting parameter importance and parallel coordinate plots to show <code>test_loss<\/code> in your dashboard.<\/p>\n<p>If you would would like this feature, you can file a feature request on our client repo issues.<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/github.com\/wandb\/client\">\n  <header class=\"source\">\n      <img src=\"https:\/\/github.githubassets.com\/favicons\/favicon.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/github.com\/wandb\/client\" target=\"_blank\" rel=\"noopener\">GitHub<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/345;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98_2_690x345.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98_2_1035x517.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/github.com\/wandb\/client\" target=\"_blank\" rel=\"noopener\">GitHub - wandb\/client: \ud83d\udd25 A tool for visualizing and tracking your machine...<\/a><\/h3>\n\n  <p>\ud83d\udd25 A tool for visualizing and tracking your machine learning experiments. This repo contains the CLI and Python API. - GitHub - wandb\/client: \ud83d\udd25 A tool for visualizing and tracking your machine learn...<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":103.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T01:21:46.052Z",
                "Answer_body":"<p>Welcome to the forum, <a class=\"mention\" href=\"\/u\/max_wasserman\">@max_wasserman<\/a>! Great first question.<\/p>\n<p>While I can see why it might be good for us to add the ability to separate the early-stopping metric from the Bayesian optimization metric, <strong>I would strongly caution against using the test loss in any step of the process<\/strong> \u2013 whether its the optimization of parameters (obviously a  no-no!) or the optimization of hyperparameters. The PyTorch Lightning docs even say that you should only call <code>.test<\/code> <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/common\/trainer.html#testing\">\u201c[o]nly right before publishing your paper or pushing to production\u201d<\/a>.<\/p>\n<p>The purpose of metrics measured on the test set is to reflect, as veridically as possible, the performance of the model on more data drawn from the same distribution, which we in turn hope reflects the performance of the model on data in production. Selecting hyperparameters based on the test set breaks the \u201cinformation wall\u201d (more technically, the conditional independence relation) between the test data and the model\u2019s parameters that make the test set useful for getting unbiased estimates of true generalization performance.<\/p>\n<p>There is at least some indication that the use of fixed validation and test sets has led the ML field as a whole to \u201coverfit\u201d, in the sense of over-estimation of true generalization performance:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/arxiv.org\/abs\/1806.00451\">\n  <header class=\"source\">\n      <img src=\"https:\/\/static.arxiv.org\/static\/browse\/0.3.2.8\/images\/icons\/favicon.ico\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/arxiv.org\/abs\/1806.00451\" target=\"_blank\" rel=\"noopener\">arXiv.org<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_500x500.png\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_500x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_750x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_1000x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_10x10.png\">\n\n<h3><a href=\"https:\/\/arxiv.org\/abs\/1806.00451\" target=\"_blank\" rel=\"noopener\">Do CIFAR-10 Classifiers Generalize to CIFAR-10?<\/a><\/h3>\n\n  <p>Machine learning is currently dominated by largely experimental work focused\non improvements in a few key tasks. However, the impressive accuracy numbers of\nthe best performing models are questionable because the same test sets have\nbeen used to...<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"http:\/\/proceedings.mlr.press\/v97\/recht19a.html\">\n  <header class=\"source\">\n      <img src=\"https:\/\/proceedings.mlr.press\/v97\/assets\/images\/favicon-pmlr.ico\" class=\"site-icon\" width=\"48\" height=\"48\">\n\n      <a href=\"http:\/\/proceedings.mlr.press\/v97\/recht19a.html\" target=\"_blank\" rel=\"noopener\" title=\"12:00AM - 24 May 2019\">PMLR \u2013 24 May 19<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"http:\/\/proceedings.mlr.press\/v97\/recht19a.html\" target=\"_blank\" rel=\"noopener\">Do ImageNet Classifiers Generalize to ImageNet?<\/a><\/h3>\n\n  <p>We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used ...<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":136.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T19:17:08.278Z",
                "Answer_body":"<aside class=\"quote group-team\" data-username=\"charlesfrye\" data-post=\"3\" data-topic=\"422\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sea2.discourse-cdn.com\/business7\/user_avatar\/community.wandb.ai\/charlesfrye\/40\/52_2.png\" class=\"avatar\"> charlesfrye:<\/div>\n<blockquote>\n<p>electing hyperparameters based on the test set breaks the \u201cinformation wall\u201d (more technically, the conditional independence relation) betw<\/p>\n<\/blockquote>\n<\/aside>\n<p>Thanks so much for the responses.<\/p>\n<p>In this case I am using synthetic data (I can generate a lot it cheaply). I used the names val\/test_loss instead of validation set 1 (for early stopping) and validation set 2 (for bayes optimization) for simplicity.  I will generate more data after this (my true test set) for final unbiased estimation of generalization.<\/p>\n<p>It seems the best solution at the moment is to simply do what <a class=\"mention\" href=\"\/u\/_scott\">@_scott<\/a> recommended: use a random search and log \u2018test_loss\u2019 (actually validation set 2) for viz later.<\/p>\n<p>PS is this the preferred location\/forum where I should post technical questions of this kind? The GitHub page refers to a slack group that appears to be closed.<\/p>",
                "Answer_score":119.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T19:34:38.217Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"max_wasserman\" data-post=\"4\" data-topic=\"422\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/7bcc69\/40.png\" class=\"avatar\"> max_wasserman:<\/div>\n<blockquote>\n<p>PS is this the preferred location\/forum where I should post technical questions of this kind? The GitHub page refers to a slack group that appears to be closed.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yes, this is the place, Thanks for checking!<\/p>\n<p>We really want to make sure our community enjoys the forums so we\u2019re silently moving from slack to discourse and we\u2019ll be making the announcement soon once we\u2019re confident the forums are all setup <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":44.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T19:54:39.788Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"max_wasserman\" data-post=\"4\" data-topic=\"422\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/7bcc69\/40.png\" class=\"avatar\"> max_wasserman:<\/div>\n<blockquote>\n<p>I will generate more data after this (my true test set) for final unbiased estimation of generalization.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Ah okay, if you\u2019ve got an actual unbiased test set, then you\u2019re golden. I\u2019d be interested to hear more about your project!<\/p>\n<p>And yes, as <a class=\"mention\" href=\"\/u\/_scott\">@_scott<\/a> points out, if you aren\u2019t using Bayesian optimization, the choice of <code>metric<\/code> won\u2019t impact the behavior of your search. <code>random<\/code> is actually a pretty good choice for HPO, competitive with <code>bayes<\/code> in my and others\u2019 experience \u2013 and less prone to error\/misconfiguration. Also BTW, <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#early_terminate\">the <code>early_terminate<\/code> feature uses HyperBand<\/a>, which is more aggressive than the usual early stopping folks learn about in an ML class, based on stopping training when you see increasing validation set error. That style of early stopping is best delegated to the ML framework you\u2019re using.<\/p>\n<p>Thanks for pointing out the issue with the Slack link. As <a class=\"mention\" href=\"\/u\/bhutanisanyam1\">@bhutanisanyam1<\/a> said, we are moving discussion to this forum, but that link should\u2019ve still been in operation anyway. Will fix it shortly.<\/p>",
                "Answer_score":68.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T23:48:41.384Z",
                "Answer_body":"<p>I\u2019m doing some graph learning work (inputs are graphs, labels are graphs). Submitting paper soon, so I\u2019ll post it to one of these forums after!<\/p>",
                "Answer_score":113.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:07.313Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: early stopping; Content: hello all, i\u2019m configuring a hyper parameter sweep. i have training, validation, and test set. i\u2019d like to use the test_loss as the final metric to optimize and val_loss for early stopping. i don\u2019t see a place to specify a metric for early stopping. does it default to the same metric specified for overall optimization (of hyper parameters)? if so, how can i change this? thanks!",
        "Question_original_content_gpt_summary":"The user is looking for a way to specify a metric for early stopping that is different from the metric used for overall optimization of hyperparameters.",
        "Question_preprocessed_content":"Title: early stopping; Content: hello all, im configuring a hyper parameter sweep. i have training, validation, and test set. id like to use the as the final metric to optimize and for early stopping. i dont see a place to specify a metric for early stopping. does it default to the same metric specified for overall optimization ? if so, how can i change this? thanks!",
        "Answer_original_content":"it isnt possible to have a different metric for hyperband early stopping and search strategy. https:\/\/github.com\/\/sweeps\/blob\/master\/hyperband_stopping.py#l176 one workaround would be to use a search strategy that doesnt require a metric like random or grid and then use val_loss as your metric for early stopping. you can then easily reconfigure the resulting parameter importance and parallel coordinate plots to show test_loss in your dashboard. if you would would like this feature, you can file a feature request on our client repo issues. github github - \/client: a tool for visualizing and tracking your machine... a tool for visualizing and tracking your machine learning experiments. this repo contains the cli and python api. - github - \/client: a tool for visualizing and tracking your machine learn... welcome to the forum, @max_wasserman! great first question. while i can see why it might be good for us to add the ability to separate the early-stopping metric from the bayesian optimization metric, i would strongly caution against using the test loss in any step of the process whether its the optimization of parameters (obviously a no-no!) or the optimization of hyperparameters. the pytorch lightning docs even say that you should only call .test [o]nly right before publishing your paper or pushing to production. the purpose of metrics measured on the test set is to reflect, as veridically as possible, the performance of the model on more data drawn from the same distribution, which we in turn hope reflects the performance of the model on data in production. selecting hyperparameters based on the test set breaks the information wall (more technically, the conditional independence relation) between the test data and the models parameters that make the test set useful for getting unbiased estimates of true generalization performance. there is at least some indication that the use of fixed validation and test sets has led the ml field as a whole to overfit, in the sense of over-estimation of true generalization performance: arxiv.org do cifar-10 classifiers generalize to cifar-10? machine learning is currently dominated by largely experimental work focused on improvements in a few key tasks. however, the impressive accuracy numbers of the best performing models are questionable because the same test sets have been used to... pmlr 24 may 19 do imagenet classifiers generalize to imagenet? we build new test sets for the cifar-10 and imagenet datasets. both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used ... charlesfrye: electing hyperparameters based on the test set breaks the information wall (more technically, the conditional independence relation) betw thanks so much for the responses. in this case i am using synthetic data (i can generate a lot it cheaply). i used the names val\/test_loss instead of validation set 1 (for early stopping) and validation set 2 (for bayes optimization) for simplicity. i will generate more data after this (my true test set) for final unbiased estimation of generalization. it seems the best solution at the moment is to simply do what @_scott recommended: use a random search and log test_loss (actually validation set 2) for viz later. ps is this the preferred location\/forum where i should post technical questions of this kind? the github page refers to a slack group that appears to be closed. max_wasserman: ps is this the preferred location\/forum where i should post technical questions of this kind? the github page refers to a slack group that appears to be closed. yes, this is the place, thanks for checking! we really want to make sure our community enjoys the forums so were silently moving from slack to discourse and well be making the announcement soon once were confident the forums are all setup max_wasserman: i will generate more data after this (my true test set) for final unbiased estimation of generalization. ah okay, if youve got an actual unbiased test set, then youre golden. id be interested to hear more about your project! and yes, as @_scott points out, if you arent using bayesian optimization, the choice of metric wont impact the behavior of your search. random is actually a pretty good choice for hpo, competitive with bayes in my and others experience and less prone to error\/misconfiguration. also btw, the early_terminate feature uses hyperband, which is more aggressive than the usual early stopping folks learn about in an ml class, based on stopping training when you see increasing validation set error. that style of early stopping is best delegated to the ml framework youre using. thanks for pointing out the issue with the slack link. as @bhutanisanyam1 said, we are moving discussion to this forum, but that link shouldve still been in operation anyway. will fix it shortly. im doing some graph learning work (inputs are graphs, labels are graphs). submitting paper soon, so ill post it to one of these forums after! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"The answer suggests that it is not possible to have a different metric for hyperband early stopping and search strategy. However, a possible workaround is to use a search strategy that does not require a metric like random or grid and then use val_loss as the metric for early stopping. The answer also cautions against using the test loss in any step of the process, including the optimization of hyperparameters. Instead, it recommends using an actual unbiased test set for final unbiased estimation of generalization. The answer also suggests that random is a good choice for hyperparameter optimization and that the early_terminate feature uses hyperband, which is more aggressive than the usual early stopping. Finally, the answer confirms that this is the preferred location\/forum to post technical questions of this kind.",
        "Answer_preprocessed_content":"it isnt possible to have a different metric for hyperband early stopping and search strategy. one workaround would be to use a search strategy that doesnt require a metric like or and then use as your metric for early stopping. you can then easily reconfigure the resulting parameter importance and parallel coordinate plots to show in your dashboard. if you would would like this feature, you can file a feature request on our client repo issues. github github a tool for visualizing and tracking your a tool for visualizing and tracking your machine learning experiments. this repo contains the cli and python api. github a tool for visualizing and tracking your machine welcome to the forum, great first question. while i can see why it might be good for us to add the ability to separate the metric from the bayesian optimization metric, i would strongly caution against using the test loss in any step of the process whether its the optimization of parameters or the optimization of hyperparameters. the pytorch lightning docs even say that you should only call nly right before publishing your paper or pushing to production. the purpose of metrics measured on the test set is to reflect, as veridically as possible, the performance of the model on more data drawn from the same distribution, which we in turn hope reflects the performance of the model on data in production. selecting hyperparameters based on the test set breaks the information wall between the test data and the models parameters that make the test set useful for getting unbiased estimates of true generalization performance. there is at least some indication that the use of fixed validation and test sets has led the ml field as a whole to overfit, in the sense of of true generalization performance do classifiers generalize to machine learning is currently dominated by largely experimental work focused on improvements in a few key tasks. however, the impressive accuracy numbers of the best performing models are questionable because the same test sets have been used pmlr may do imagenet classifiers generalize to imagenet? we build new test sets for the and imagenet datasets. both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively charlesfrye electing hyperparameters based on the test set breaks the information wall betw thanks so much for the responses. in this case i am using synthetic data . i used the names instead of validation set and validation set for simplicity. i will generate more data after this for final unbiased estimation of generalization. it seems the best solution at the moment is to simply do what recommended use a random search and log for viz later. ps is this the preferred where i should post technical questions of this kind? the github page refers to a slack group that appears to be closed. ps is this the preferred where i should post technical questions of this kind? the github page refers to a slack group that appears to be closed. yes, this is the place, thanks for checking! we really want to make sure our community enjoys the forums so were silently moving from slack to discourse and well be making the announcement soon once were confident the forums are all setup i will generate more data after this for final unbiased estimation of generalization. ah okay, if youve got an actual unbiased test set, then youre golden. id be interested to hear more about your project! and yes, as points out, if you arent using bayesian optimization, the choice of wont impact the behavior of your search. is actually a pretty good choice for hpo, competitive with in my and others experience and less prone to also btw, the feature uses hyperband, which is more aggressive than the usual early stopping folks learn about in an ml class, based on stopping training when you see increasing validation set error. that style of early stopping is best delegated to the ml framework youre using. thanks for pointing out the issue with the slack link. as said, we are moving discussion to this forum, but that link shouldve still been in operation anyway. will fix it shortly. im doing some graph learning work . submitting paper soon, so ill post it to one of these forums after! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"How to log multiple metrics to the same chart?",
        "Question_body":"<p>Under <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#common-workflows\" class=\"inline-onebox\">Log Data with wandb.log - Documentation<\/a> it says I can have multiple metrics show up on the same chart by logging them in the same dict. On the UI they still show up on different charts though <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/frowning.png?v=10\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\"><br>\nAny ideas on what I could be doing wrong? I can post code if necessary but I really don\u2019t think I\u2019m using the API incorrectly<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":null,
        "Question_creation_time":1635573946747,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":692.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-log-multiple-metrics-to-the-same-chart\/1160",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-01T13:04:41.401Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rowan-dempster\">@rowan-dempster<\/a>, welcome to our forums!<\/p>\n<p>By default, each chart has one metric but you can easily add other metrics in the UI by editing a chart using the pencil icon in the top right (appears on hover) and adding the other metric or metrics you care about to your Y-axis.<br>\nHope this helps!<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-01T16:36:54.491Z",
                "Answer_body":"<p>Hi Scott,<\/p>\n<p>Thanks for your reply and your suggestion. This is the approach I have been taking and would be satisfactory if I did not have to re-create the chart parameters for every run. Is there a way to have a specific chart config show up by default on new runs of the same project?<\/p>\n<p>Thanks,<br>\nRowan<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-01T16:45:04.327Z",
                "Answer_body":"<p>Glad you found that solution yourself.<\/p>\n<blockquote>\n<p>Is there a way to have a specific chart config show up by default on new runs of the same project?<\/p>\n<\/blockquote>\n<p>When you create or edit a chart (panel) in a <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/pages\/run-page\">Run Page<\/a>, it should appear on your other runs\u2019 Run Page.<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-01T17:37:36.694Z",
                "Answer_body":"<blockquote>\n<p>When you create or edit a chart (panel) in a <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/pages\/run-page\">Run Page<\/a>, it should appear on your other runs\u2019 Run Page.<\/p>\n<\/blockquote>\n<p>This is the behavior I was expecting, but I am not observing it. Specifically, whenever a new run is logged the charts I see are always the same (just one chart per metric). New custom charts, and sections for that matter, are not preserved.<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-01T21:24:11.432Z",
                "Answer_body":"<blockquote>\n<p>but I am not observing it<\/p>\n<\/blockquote>\n<p>I take this back, I just observed the charts synchronizing themselves across runs. But it does not happen consistently, I just started another runs and the custom charts are not present. Maybe they show after the run finishes?<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T10:10:28.296Z",
                "Answer_body":"<aside class=\"quote no-group quote-modified\" data-username=\"rowan-dempster\" data-post=\"5\" data-topic=\"1160\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/73ab20\/40.png\" class=\"avatar\"> rowan-dempster:<\/div>\n<blockquote>\n<blockquote>\n<p>When you create or edit a chart (panel) in a <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/pages\/run-page\">Run Page<\/a>, it should appear on your other runs\u2019 Run Page.<\/p>\n<\/blockquote>\n<\/blockquote>\n<\/aside>\n<p>The expected behaviour of this is that the chart will show up when possible in the run. For example, if you try plot \u201ctest_accuracy\u201d but you haven\u2019t logged that yet for a given run, that chart won\u2019t show up. As soon as you have logged a value for the metric you\u2019re trying to display, you chart displaying that should appear.<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-02T10:11:20.754Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to log multiple metrics to the same chart?; Content: under log data with .log - documentation it says i can have multiple metrics show up on the same chart by logging them in the same dict. on the ui they still show up on different charts though any ideas on what i could be doing wrong? i can post code if necessary but i really don\u2019t think i\u2019m using the api incorrectly",
        "Question_original_content_gpt_summary":"The user is having difficulty logging multiple metrics to the same chart, despite following the documentation instructions.",
        "Question_preprocessed_content":"Title: how to log multiple metrics to the same chart?; Content: under log data with documentation it says i can have multiple metrics show up on the same chart by logging them in the same dict. on the ui they still show up on different charts though any ideas on what i could be doing wrong? i can post code if necessary but i really dont think im using the api incorrectly",
        "Answer_original_content":"hi @rowan-dempster, welcome to our forums! by default, each chart has one metric but you can easily add other metrics in the ui by editing a chart using the pencil icon in the top right (appears on hover) and adding the other metric or metrics you care about to your y-axis. hope this helps! hi scott, thanks for your reply and your suggestion. this is the approach i have been taking and would be satisfactory if i did not have to re-create the chart parameters for every run. is there a way to have a specific chart config show up by default on new runs of the same project? thanks, rowan glad you found that solution yourself. is there a way to have a specific chart config show up by default on new runs of the same project? when you create or edit a chart (panel) in a run page, it should appear on your other runs run page. when you create or edit a chart (panel) in a run page, it should appear on your other runs run page. this is the behavior i was expecting, but i am not observing it. specifically, whenever a new run is logged the charts i see are always the same (just one chart per metric). new custom charts, and sections for that matter, are not preserved. but i am not observing it i take this back, i just observed the charts synchronizing themselves across runs. but it does not happen consistently, i just started another runs and the custom charts are not present. maybe they show after the run finishes? rowan-dempster: when you create or edit a chart (panel) in a run page, it should appear on your other runs run page. the expected behaviour of this is that the chart will show up when possible in the run. for example, if you try plot test_accuracy but you havent logged that yet for a given run, that chart wont show up. as soon as you have logged a value for the metric youre trying to display, you chart displaying that should appear. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Possible solutions to the user's difficulty in logging multiple metrics to the same chart are: editing the chart using the pencil icon in the top right and adding the other metric or metrics to the y-axis, and creating or editing a chart in a run page so that it appears on other runs' run page. The expected behavior is that the chart will show up when possible in the run, and it won't show up if the metric hasn't been logged yet.",
        "Answer_preprocessed_content":"hi welcome to our forums! by default, each chart has one metric but you can easily add other metrics in the ui by editing a chart using the pencil icon in the top right and adding the other metric or metrics you care about to your hope this helps! hi scott, thanks for your reply and your suggestion. this is the approach i have been taking and would be satisfactory if i did not have to the chart parameters for every run. is there a way to have a specific chart config show up by default on new runs of the same project? thanks, rowan glad you found that solution yourself. is there a way to have a specific chart config show up by default on new runs of the same project? when you create or edit a chart in a run page, it should appear on your other runs run page. when you create or edit a chart in a run page, it should appear on your other runs run page. this is the behavior i was expecting, but i am not observing it. specifically, whenever a new run is logged the charts i see are always the same . new custom charts, and sections for that matter, are not preserved. but i am not observing it i take this back, i just observed the charts synchronizing themselves across runs. but it does not happen consistently, i just started another runs and the custom charts are not present. maybe they show after the run finishes? when you create or edit a chart in a run page, it should appear on your other runs run page. the expected behaviour of this is that the chart will show up when possible in the run. for example, if you try plot but you havent logged that yet for a given run, that chart wont show up. as soon as you have logged a value for the metric youre trying to display, you chart displaying that should appear. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":72694707.0,
        "Question_title":"Multiple artifact paths when logging a model using mlflow and sklearn",
        "Question_body":"<p>I'm using mlflow to log parameters and artifacts of a Logistic Regression, but when I try to log the model so I can see all the files in the Mlflow UI, I see two folders: one named 'model' and the other one named 'logger' (the one I set).<\/p>\n<pre><code>model = LogisticRegression()\n\nmlflow.set_tracking_uri('file:\/\/\/artifacts')\nmlflow.set_experiment('test')\nmlflow.autolog()\n\nwith mlflow.start_run(run_name=run_name) as run:\n   model.train(X_train, y_train)\n   mlflow.sklearn.log_model(model, 'logreg')\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BtIHo.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Not sure if I'm missing something or if there's a configuration for that.<\/p>\n<p>I hope someone out there can help me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1655778299153,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":122.0,
        "Owner_creation_time":1544467691223,
        "Owner_last_access_time":1659381626412,
        "Owner_reputation":131.0,
        "Owner_up_votes":5.0,
        "Owner_down_votes":0.0,
        "Owner_views":42.0,
        "Answer_body":"<p>You have set <code>autolog<\/code> and you are also logging the model explicitly. Remove one and then try.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1655933087183,
        "Answer_score":2.0,
        "Owner_location":"Zacatecas, Mexico",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72694707",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: multiple artifact paths when logging a model using and sklearn; Content: i'm using to log parameters and artifacts of a logistic regression, but when i try to log the model so i can see all the files in the ui, i see two folders: one named 'model' and the other one named 'logger' (the one i set). model = logisticregression() .set_tracking_uri('file:\/\/\/artifacts') .set_experiment('test') .autolog() with .start_run(run_name=run_name) as run: model.train(x_train, y_train) .sklearn.log_model(model, 'logreg') not sure if i'm missing something or if there's a configuration for that. i hope someone out there can help me!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with logging a model using and sklearn, resulting in multiple artifact paths being created in the UI.",
        "Question_preprocessed_content":"Title: multiple artifact paths when logging a model using and sklearn; Content: i'm using to log parameters and artifacts of a logistic regression, but when i try to log the model so i can see all the files in the ui, i see two folders one named 'model' and the other one named 'logger' . not sure if i'm missing something or if there's a configuration for that. i hope someone out there can help me!",
        "Answer_original_content":"you have set autolog and you are also logging the model explicitly. remove one and then try.",
        "Answer_original_content_gpt_summary":"Possible solution: The user should remove either the autolog or the explicit model logging to avoid creating multiple artifact paths in the UI while logging a model using sklearn.",
        "Answer_preprocessed_content":"you have set and you are also logging the model explicitly. remove one and then try."
    },
    {
        "Question_id":41902672.0,
        "Question_title":"Azure Machine Learning Batch request returning mal-formatted umlauts with Unixes",
        "Question_body":"<p>I have deployed my code in Azure Machine Learning and run the batch request in R with different operating systems, such as Unix and W10. For some reason, the host outputs are properly formatted only in R of W10 but I am unable to get properly formatted output in Unix systems. Only way I can get properly formatted outputs in all systems is through the Azure GUI and manually download the file. In W10, I have the luxury to get the properly formatted file directly with my Rscript\/Rstudio thing. In R, I have used <code>system(&quot;defaults write org.R-project.R force.LANG en_US.UTF-8&quot;)<\/code> as hinted <a href=\"https:\/\/stackoverflow.com\/questions\/41873359\/r-encoding-failing-with-umlauts-such-as-%C3%A4-and-%C3%B6\">here<\/a> to explicitly specify the encoding but this does not have any effect on the batch request R script that is executed in Azure servers run by Microsoft.<\/p>\n<p>What is happening is that <code>UTF-8 characters bytes are returned as Latin-1 characters bytes<\/code>, for example<\/p>\n<blockquote>\n<ol>\n<li><p><code>\u00f6<\/code> as <code>\u00c3 \u00b6<\/code><\/p>\n<\/li>\n<li><p><code>\u00e4<\/code> as <code>\u00c3 \u00a4<\/code><\/p>\n<\/li>\n<li><p><code>\u00c4<\/code> as <code>\u00c3 \u00a5<\/code><\/p>\n<\/li>\n<\/ol>\n<\/blockquote>\n<p>as can be demonstrated and tested with this tool <a href=\"http:\/\/www.ltg.ed.ac.uk\/%7Erichard\/utf-8.cgi?input=%C3%84&amp;mode=char\" rel=\"nofollow noreferrer\">here<\/a> about Latin-1 characters. So what are best ways to deal with this encoding issue, can it be addressed somehow inside Azure ML? Where can you do bug reports? Does there exist some tool to convert Latin-1 to UTF-8 in R?<\/p>\n<p><strong>How can you get properly formatted UTF-8 files with umlauts with R batch requests in Azure ML (not in Latin-1 characters)?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1485549679360,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Owner_creation_time":1251372839052,
        "Owner_last_access_time":1653648989307,
        "Owner_reputation":48616.0,
        "Owner_up_votes":1240.0,
        "Owner_down_votes":41.0,
        "Owner_views":3348.0,
        "Answer_body":"<p>The Batch request R command has a <code>saveBlobToFile<\/code> function. The problem is in the <code>saveBlobToFile<\/code> function that uses wrong encoding with <code>getUrl<\/code>.  <code>getUrl<\/code> function needs to specify the encodings explicitly. Do the following changes<\/p>\n\n<pre><code>blobContent = getURL(blobUrl, .encoding=\"UTF-8\")\n<\/code><\/pre>\n\n<p>where without <code>.encoding<\/code>, the output is <code>ISO8859-1('latin1')<\/code> or something inherited from your system.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1486366682512,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1592644375060,
        "Answer_last_edit_time":1486367568768,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41902672",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: batch request returning mal-formatted umlauts with unixes; Content: i have deployed my code in and run the batch request in r with different operating systems, such as unix and w10. for some reason, the host outputs are properly formatted only in r of w10 but i am unable to get properly formatted output in unix systems. only way i can get properly formatted outputs in all systems is through the azure gui and manually download the file. in w10, i have the luxury to get the properly formatted file directly with my rscript\/rstudio thing. in r, i have used system(\"defaults write org.r-project.r force.lang en_us.utf-8\") as hinted here to explicitly specify the encoding but this does not have any effect on the batch request r script that is executed in azure servers run by microsoft. what is happening is that utf-8 characters bytes are returned as latin-1 characters bytes, for example \u00f6 as \u00e3 \u00b6 \u00e4 as \u00e3 \u00a4 \u00e4 as \u00e3 \u00a5 as can be demonstrated and tested with this tool here about latin-1 characters. so what are best ways to deal with this encoding issue, can it be addressed somehow inside ? where can you do bug reports? does there exist some tool to convert latin-1 to utf-8 in r? how can you get properly formatted utf-8 files with umlauts with r batch requests in (not in latin-1 characters)?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with batch requests in Azure returning mal-formatted umlauts with unixes, and is seeking ways to get properly formatted utf-8 files with umlauts with r batch requests in Azure.",
        "Question_preprocessed_content":"Title: batch request returning umlauts with unixes; Content: i have deployed my code in and run the batch request in r with different operating systems, such as unix and w . for some reason, the host outputs are properly formatted only in r of w but i am unable to get properly formatted output in unix systems. only way i can get properly formatted outputs in all systems is through the azure gui and manually download the file. in w , i have the luxury to get the properly formatted file directly with my thing. in r, i have used as hinted here to explicitly specify the encoding but this does not have any effect on the batch request r script that is executed in azure servers run by microsoft. what is happening is that , for example as as as as can be demonstrated and tested with this tool here about characters. so what are best ways to deal with this encoding issue, can it be addressed somehow inside ? where can you do bug reports? does there exist some tool to convert to in r? how can you get properly formatted files with umlauts with r batch requests in ?",
        "Answer_original_content":"the batch request r command has a saveblobtofile function. the problem is in the saveblobtofile function that uses wrong encoding with geturl. geturl function needs to specify the encodings explicitly. do the following changes blobcontent = geturl(bloburl, .encoding=\"utf-8\") where without .encoding, the output is iso8859-1('latin1') or something inherited from your system.",
        "Answer_original_content_gpt_summary":"The solution to the problem of mal-formatted umlauts with unixes in batch requests in Azure is to use the saveblobtofile function in the batch request r command. However, the saveblobtofile function uses the wrong encoding with geturl. To fix this, the geturl function needs to specify the encodings explicitly by making changes to the blobcontent. Without specifying the encoding, the output is iso8859-1('latin1') or something inherited from your system.",
        "Answer_preprocessed_content":"the batch request r command has a function. the problem is in the function that uses wrong encoding with . function needs to specify the encodings explicitly. do the following changes where without , the output is or something inherited from your system."
    },
    {
        "Question_id":null,
        "Question_title":"Reason why errors occur when starting SageMaker Studio",
        "Question_body":"Hello! I have a question about errors found when starting SageMaker Studio (below).\n\nAccessDeniedException User: X is not auth**orized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker:ap-northeast-1:XX because no identity-based policy allows the sagemaker:CreateDomain action\n\nValidationException Access denied in getting\/deleting the portfolio shared by SageMaker. Please call withservicecatalog:ListAcceptedPortfolioShares permission.\n\nAccessDeniedException User: X is not authorized to perform: sagemaker:CreateUserProfile on resource: arn:aws:sagemaker:ap-northeast-1:XX because no identity-based policy allows the sagemaker:CreateUserProfile action\n\nI resolved the errors by adding some inline policies, but I cannot understand the reason why the errors occur on my user with S3 Full Access and SageMaker Full Access policies.\n\nI'd happy to tell me any information about the errors. Thank you!",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1661920764293,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":152.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfTu0DsSxTf-B8n4srTzG9A\/reason-why-errors-occur-when-starting-sage-maker-studio",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-31T12:25:56.766Z",
                "Answer_score":1,
                "Answer_body":"As far as I know, according the aws docs the passRole action should be granted to the SageMake execution role for some cases such as creating images. So your s3:* and sagemaker:* is not enough, but still need to add iam:passRole to policies",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: reason why errors occur when starting studio; Content: hello! i have a question about errors found when starting studio (below). accessdeniedexception user: x is not auth**orized to perform: :createdomain on resource: arn:aws::ap-northeast-1:xx because no identity-based policy allows the :createdomain action validationexception access denied in getting\/deleting the portfolio shared by . please call withservicecatalog:listacceptedportfolioshares permission. accessdeniedexception user: x is not authorized to perform: :createuserprofile on resource: arn:aws::ap-northeast-1:xx because no identity-based policy allows the :createuserprofile action i resolved the errors by adding some inline policies, but i cannot understand the reason why the errors occur on my user with s3 full access and full access policies. i'd happy to tell me any information about the errors. thank you!",
        "Question_original_content_gpt_summary":"The user encountered AccessDeniedExceptions and ValidationExceptions when starting Studio, which they resolved by adding inline policies, but they are still unsure of the reason why the errors occurred despite having S3 Full Access and Full Access policies.",
        "Question_preprocessed_content":"Title: reason why errors occur when starting studio; Content: hello! i have a question about errors found when starting studio . accessdeniedexception user x is not auth orized to perform createdomain on resource because no policy allows the createdomain action validationexception access denied in the portfolio shared by . please call withservicecatalog listacceptedportfolioshares permission. accessdeniedexception user x is not authorized to perform createuserprofile on resource because no policy allows the createuserprofile action i resolved the errors by adding some inline policies, but i cannot understand the reason why the errors occur on my user with s full access and full access policies. i'd happy to tell me any information about the errors. thank you!",
        "Answer_original_content":"as far as i know, according the aws docs the passrole action should be granted to the sagemake execution role for some cases such as creating images. so your s3:* and :* is not enough, but still need to add iam:passrole to policies",
        "Answer_original_content_gpt_summary":"Possible solution: The user may need to add the \"iam:passrole\" action to their policies in addition to \"s3:*\" and \":*\" to resolve AccessDeniedExceptions and ValidationExceptions when starting Studio. This action is required for some cases such as creating images and is documented in AWS documentation.",
        "Answer_preprocessed_content":"as far as i know, according the aws docs the passrole action should be granted to the sagemake execution role for some cases such as creating images. so your s and is not enough, but still need to add iam passrole to policies"
    },
    {
        "Question_id":51430645.0,
        "Question_title":"split dataframe column header and values into multiple columns",
        "Question_body":"<p>I've uploaded my <code>csv<\/code> file on Azure, but for some reason it became like this<\/p>\n\n<pre><code> nominal;data;curs;cdx         Column 1\n0          1;21.06.2000;28  2300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n1          1;22.06.2000;28  2200;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n2          1;23.06.2000;28  1900;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n3          1;24.06.2000;28  1700;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n4          1;27.06.2000;28  1300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n5          1;28.06.2000;28  1100;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n<\/code><\/pre>\n\n<p>Basically instead of four columns <code>nominal<\/code>, <code>data<\/code>, <code>curs<\/code>, <code>cdx<\/code> I got two columns with one having all the values and the last one (it is empty or something because the last column has encoding issue) - no idea what.<\/p>\n\n<p>I have deleted the column <code>Column 1<\/code> like this<\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1.drop(['Column 1'], axis = 1, inplace = True)\n    print('Input pandas.DataFrame #1:\\r\\n\\r\\n{0}'.format(dataframe1))\n    return dataframe1,\n<\/code><\/pre>\n\n<p>How to split the first column into multiple now? To get 4 separate columns<\/p>\n\n<p>I am using pandas 0.18<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9.0,
        "Question_creation_time":1532029428013,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":1086.0,
        "Owner_creation_time":1338678668792,
        "Owner_last_access_time":1663513309420,
        "Owner_reputation":11479.0,
        "Owner_up_votes":528.0,
        "Owner_down_votes":1.0,
        "Owner_views":887.0,
        "Answer_body":"<p>You need to split the column with:<\/p>\n\n<pre><code>dataframe1['nominal;data;curs;cdx'].str.split(';',expand=True)\n<\/code><\/pre>\n\n<p>Then change the headers with:<\/p>\n\n<pre><code>dataframe1.columns = 'nominal;data;curs;cdx'.split(';')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1532031297043,
        "Answer_score":1.0,
        "Owner_location":"Eindhoven, Netherlands",
        "Question_last_edit_time":1532029712492,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51430645",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: split dataframe column header and values into multiple columns; Content: i've uploaded my csv file on azure, but for some reason it became like this nominal;data;curs;cdx column 1 0 1;21.06.2000;28 2300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd 1 1;22.06.2000;28 2200;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd 2 1;23.06.2000;28 1900;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd 3 1;24.06.2000;28 1700;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd 4 1;27.06.2000;28 1300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd 5 1;28.06.2000;28 1100;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd basically instead of four columns nominal, data, curs, cdx i got two columns with one having all the values and the last one (it is empty or something because the last column has encoding issue) - no idea what. i have deleted the column column 1 like this import pandas as pd def _main(dataframe1 = none, dataframe2 = none): dataframe1.drop(['column 1'], axis = 1, inplace = true) print('input pandas.dataframe #1:\\r\\n\\r\\n{0}'.format(dataframe1)) return dataframe1, how to split the first column into multiple now? to get 4 separate columns i am using pandas 0.18",
        "Question_original_content_gpt_summary":"The user is encountering a challenge of splitting a dataframe column header and values into multiple columns using Pandas 0.18.",
        "Question_preprocessed_content":"Title: split dataframe column header and values into multiple columns; Content: i've uploaded my file on azure, but for some reason it became like this basically instead of four columns , , , i got two columns with one having all the values and the last one no idea what. i have deleted the column like this how to split the first column into multiple now? to get separate columns i am using pandas",
        "Answer_original_content":"you need to split the column with: dataframe1['nominal;data;curs;cdx'].str.split(';',expand=true) then change the headers with: dataframe1.columns = 'nominal;data;curs;cdx'.split(';')",
        "Answer_original_content_gpt_summary":"To split a dataframe column header and values into multiple columns using Pandas 0.18, you can use the following solutions: \n1. Split the column using `dataframe1['nominal;data;curs;cdx'].str.split(';',expand=true)`\n2. Change the headers using `dataframe1.columns = 'nominal;data;curs;cdx'.split(';')`",
        "Answer_preprocessed_content":"you need to split the column with then change the headers with"
    },
    {
        "Question_id":52070950.0,
        "Question_title":"AWS Sagemaker | how to train text data | For ticket classification",
        "Question_body":"<p>I am new to Sagemaker and not sure how to classify the text input in AWS sagemaker, <\/p>\n\n<p>Suppose I have a Dataframe having two fields like 'Ticket' and 'Category', Both are text input, Now I want to split it test and training set and upload in Sagemaker training model. <\/p>\n\n<pre><code>X_train, X_test, y_train, y_test = model_selection.train_test_split(fewRecords['Ticket'],fewRecords['Category'])\n<\/code><\/pre>\n\n<p>Now as I want to perform TD-IDF feature extraction and then convert it to numeric value, so performing this operation<\/p>\n\n<pre><code>tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(fewRecords['Category'])\nxtrain_tfidf =  tfidf_vect.transform(X_train)\nxvalid_tfidf =  tfidf_vect.transform(X_test)\n<\/code><\/pre>\n\n<p>When I want to upload the model in Sagemaker so I can perform next operation like <\/p>\n\n<pre><code>buf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\nbuf.seek(0)\n<\/code><\/pre>\n\n<p>I am getting this error <\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-36-8055e6cdbf34&gt; in &lt;module&gt;()\n      1 buf = io.BytesIO()\n----&gt; 2 smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n      3 buf.seek(0)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n     98             raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n     99                              labels.shape, array.shape))\n--&gt; 100         resolved_label_type = _resolve_type(labels.dtype)\n    101     resolved_type = _resolve_type(array.dtype)\n    102 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in _resolve_type(dtype)\n    205     elif dtype == np.dtype('float32'):\n    206         return 'Float32'\n--&gt; 207     raise ValueError('Unsupported dtype {} on array'.format(dtype))\n\nValueError: Unsupported dtype object on array\n<\/code><\/pre>\n\n<p>Other than this exception, I am not clear if this is right way as TfidfVectorizer convert the series to Matrix.<\/p>\n\n<p>The code is predicting fine on my local machine but not sure how to do the same on Sagemaker, All the example mentioned there are too lengthy and not for the person who still reached to SciKit Learn <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1535524538620,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":1519.0,
        "Owner_creation_time":1501403168107,
        "Owner_last_access_time":1663685130870,
        "Owner_reputation":1370.0,
        "Owner_up_votes":94.0,
        "Owner_down_votes":1.0,
        "Owner_views":125.0,
        "Answer_body":"<p>The output of <code>TfidfVectorizer<\/code> is a scipy sparse matrix, not a simple numpy array.<\/p>\n<p>So either use a different function like:<\/p>\n<blockquote>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/amazon\/common.py#L113\" rel=\"nofollow noreferrer\">write_spmatrix_to_sparse_tensor<\/a><\/p>\n<p>&quot;&quot;&quot;Writes a scipy sparse matrix to a sparse tensor&quot;&quot;&quot;<\/p>\n<\/blockquote>\n<p>See <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/27\" rel=\"nofollow noreferrer\">this issue<\/a> for more details.<\/p>\n<p><strong>OR<\/strong> first convert the output of <code>TfidfVectorizer<\/code> to a dense numpy array and then use your above code<\/p>\n<pre><code>xtrain_tfidf =  tfidf_vect.transform(X_train).toarray()   \nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n...\n...\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1535540801190,
        "Answer_score":1.0,
        "Owner_location":"Delhi, India",
        "Question_last_edit_time":1535540817292,
        "Answer_last_edit_time":1592644375060,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52070950",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: | how to train text data | for ticket classification; Content: i am new to and not sure how to classify the text input in , suppose i have a dataframe having two fields like 'ticket' and 'category', both are text input, now i want to split it test and training set and upload in training model. x_train, x_test, y_train, y_test = model_selection.train_test_split(fewrecords['ticket'],fewrecords['category']) now as i want to perform td-idf feature extraction and then convert it to numeric value, so performing this operation tfidf_vect = tfidfvectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000) tfidf_vect.fit(fewrecords['category']) xtrain_tfidf = tfidf_vect.transform(x_train) xvalid_tfidf = tfidf_vect.transform(x_test) when i want to upload the model in so i can perform next operation like buf = io.bytesio() smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train) buf.seek(0) i am getting this error --------------------------------------------------------------------------- valueerror traceback (most recent call last) <ipython-input-36-8055e6cdbf34> in <module>() 1 buf = io.bytesio() ----> 2 smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train) 3 buf.seek(0) ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels) 98 raise valueerror(\"label shape {} not compatible with array shape {}\".format( 99 labels.shape, array.shape)) --> 100 resolved_label_type = _resolve_type(labels.dtype) 101 resolved_type = _resolve_type(array.dtype) 102 ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/amazon\/common.py in _resolve_type(dtype) 205 elif dtype == np.dtype('float32'): 206 return 'float32' --> 207 raise valueerror('unsupported dtype {} on array'.format(dtype)) valueerror: unsupported dtype object on array other than this exception, i am not clear if this is right way as tfidfvectorizer convert the series to matrix. the code is predicting fine on my local machine but not sure how to do the same on , all the example mentioned there are too lengthy and not for the person who still reached to scikit learn",
        "Question_original_content_gpt_summary":"The user is encountering challenges with training text data for ticket classification on Amazon SageMaker, including how to perform TD-IDF feature extraction and convert it to a numeric value, as well as an error when attempting to upload the model.",
        "Question_preprocessed_content":"Title: how to train text data for ticket classification; Content: i am new to and not sure how to classify the text input in , suppose i have a dataframe having two fields like 'ticket' and 'category', both are text input, now i want to split it test and training set and upload in training model. now as i want to perform feature extraction and then convert it to numeric value, so performing this operation when i want to upload the model in so i can perform next operation like i am getting this error other than this exception, i am not clear if this is right way as tfidfvectorizer convert the series to matrix. the code is predicting fine on my local machine but not sure how to do the same on , all the example mentioned there are too lengthy and not for the person who still reached to scikit learn",
        "Answer_original_content":"the output of tfidfvectorizer is a scipy sparse matrix, not a simple numpy array. so either use a different function like: write_spmatrix_to_sparse_tensor \"\"\"writes a scipy sparse matrix to a sparse tensor\"\"\" see this issue for more details. or first convert the output of tfidfvectorizer to a dense numpy array and then use your above code xtrain_tfidf = tfidf_vect.transform(x_train).toarray() buf = io.bytesio() smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train) ... ...",
        "Answer_original_content_gpt_summary":"The answer suggests two possible solutions for the user's challenges with training text data for ticket classification on Amazon SageMaker. The first solution is to use a different function like \"write_spmatrix_to_sparse_tensor\" to write a scipy sparse matrix to a sparse tensor. The second solution is to convert the output of tfidfvectorizer to a dense numpy array and then use the code provided.",
        "Answer_preprocessed_content":"the output of is a scipy sparse matrix, not a simple numpy array. so either use a different function like writes a scipy sparse matrix to a sparse tensor see this issue for more details. or first convert the output of to a dense numpy array and then use your above code"
    },
    {
        "Question_id":54334462.0,
        "Question_title":"How can I quickly debug a SageMaker training script?",
        "Question_body":"<p>When running an ML training job in Amazon SageMaker, the training script is \"deployed\" and given an ML training instance, which takes about 10 minutes to spin up and get the data it needs. <\/p>\n\n<p>I can only get one error message from the training job, then it dies and the instance is killed along with it. <\/p>\n\n<p>After I make a change to the training script to fix it, I need to deploy and run it which takes another 10 minutes or so.<\/p>\n\n<p>How can I accomplish this faster, or keep the training instance running?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1548272148563,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":1902.0,
        "Owner_creation_time":1416193017423,
        "Owner_last_access_time":1663696463112,
        "Owner_reputation":880.0,
        "Owner_up_votes":211.0,
        "Owner_down_votes":2.0,
        "Owner_views":111.0,
        "Answer_body":"<p>It seems that you are running a training job using one of the SageMaker frameworks. Given that, you can use the \"local mode\" feature of SageMaker, which will run your training job (specifically the container) locally in your notebook instance. That way, you can iterate on your script until it works. Then you can move on to the remote training cluster to train the model against the whole dataset if needed. To use local mode, you just set the instance type to \"local\". More details about local mode can be found at <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#sagemaker-python-sdk-overview<\/a> and the blog post: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1548282026927,
        "Answer_score":5.0,
        "Owner_location":"Gensokyo",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54334462",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i quickly debug a training script?; Content: when running an ml training job in , the training script is \"deployed\" and given an ml training instance, which takes about 10 minutes to spin up and get the data it needs. i can only get one error message from the training job, then it dies and the instance is killed along with it. after i make a change to the training script to fix it, i need to deploy and run it which takes another 10 minutes or so. how can i accomplish this faster, or keep the training instance running?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with debugging a training script, as they are only able to get one error message before the instance is killed and they have to wait 10 minutes for the instance to spin up again.",
        "Question_preprocessed_content":"Title: how can i quickly debug a training script?; Content: when running an ml training job in , the training script is deployed and given an ml training instance, which takes about minutes to spin up and get the data it needs. i can only get one error message from the training job, then it dies and the instance is killed along with it. after i make a change to the training script to fix it, i need to deploy and run it which takes another minutes or so. how can i accomplish this faster, or keep the training instance running?",
        "Answer_original_content":"it seems that you are running a training job using one of the frameworks. given that, you can use the \"local mode\" feature of , which will run your training job (specifically the container) locally in your notebook instance. that way, you can iterate on your script until it works. then you can move on to the remote training cluster to train the model against the whole dataset if needed. to use local mode, you just set the instance type to \"local\". more details about local mode can be found at https:\/\/github.com\/aws\/-python-sdk#-python-sdk-overview and the blog post: https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon--local-mode-to-train-on-your-notebook-instance\/",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of debugging a training script with limited error messages include using the \"local mode\" feature of the framework being used, which allows for running the training job locally in the notebook instance to iterate on the script until it works. Once the script is working, the user can move on to the remote training cluster to train the model against the whole dataset if needed. The instance type should be set to \"local\" to use local mode. More details about local mode can be found at the provided links.",
        "Answer_preprocessed_content":"it seems that you are running a training job using one of the frameworks. given that, you can use the local mode feature of , which will run your training job locally in your notebook instance. that way, you can iterate on your script until it works. then you can move on to the remote training cluster to train the model against the whole dataset if needed. to use local mode, you just set the instance type to local . more details about local mode can be found at and the blog post"
    },
    {
        "Question_id":51088145.0,
        "Question_title":"Azure Python SDK & Machine Learning Studio Web Service Batch Execution Snippet: TypeError",
        "Question_body":"<p><strong>First Issue resolved, please read scroll down to EDIT2<\/strong><\/p>\n\n<p>I'm trying to access a Web Service deployed via Azure Machine Learning Studio, using the Batch Execution-Sample Code for Python on the bottom of below page:<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs<\/a><\/p>\n\n<p>I have already fixed an Issue according to this question (replaced BlobService by BlobBlockService and so on):<\/p>\n\n<p><a href=\"https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs<\/a><\/p>\n\n<p>And I also have entered the API-Key, Container-Name, URL, account_key and account_name according to the instructions.<\/p>\n\n<p>However it seems that today the Code Snippet is even more outdated than it was back then because I receive a different error now:<\/p>\n\n<pre><code>File \"C:\/Users\/Alex\/Desktop\/scripts\/BatchExecution.py\", line 80, in uploadFileToBlob\n    blob_service = asb.BlockBlobService(account_name=storage_account_name, account_key=storage_account_key)\n\n  File \"C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\azure\\storage\\blob\\blockblobservice.py\", line 145, in __init__\n\n  File \"C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\azure\\storage\\blob\\baseblobservice.py\", line 205, in __init__\n\nTypeError: get_service_parameters() got an unexpected keyword argument 'token_credential'\n<\/code><\/pre>\n\n<p>I also noticed, that when installing the Azure SDK for Python via pip, I get the following warnings in the end of the process (installation is successful however):<\/p>\n\n<pre><code>azure-storage-queue 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n\nazure-storage-file 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n\nazure-storage-blob 1.3.0 has requirement azure-storage-common&lt;1.4.0,&gt;=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible.\n<\/code><\/pre>\n\n<p>I can't find anything about all this in the latest documentation for the Python SDK (the word 'token_credential' is not even contained):<\/p>\n\n<p><a href=\"https:\/\/media.readthedocs.org\/pdf\/azure-storage\/latest\/azure-storage.pdf\" rel=\"nofollow noreferrer\">https:\/\/media.readthedocs.org\/pdf\/azure-storage\/latest\/azure-storage.pdf<\/a><\/p>\n\n<p>Does anyone have a clue what's going wrong during the installation or why the type-error with the 'token_credential' pops up during execution?<\/p>\n\n<p>Or does anyone know how I can install the necessary version of azure-storage-common or azure-storage-blob?<\/p>\n\n<p>EDIT: Here's a my code (however not-reproducible because I changed the keys before posting)<\/p>\n\n<pre><code># How this works:\n#\n# 1. Assume the input is present in a local file (if the web service accepts input)\n# 2. Upload the file to an Azure blob - you\"d need an Azure storage account\n# 3. Call BES to process the data in the blob. \n# 4. The results get written to another Azure blob.\n\n# 5. Download the output blob to a local file\n#\n# Note: You may need to download\/install the Azure SDK for Python.\n# See: http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/python-how-to-install\/\n\nimport urllib\n# If you are using Python 3+, import urllib instead of urllib2\n\nimport json\nimport time\nimport azure.storage.blob as asb          # replaces BlobService by BlobBlockService\n\n\ndef printHttpError(httpError):\n    print(\"The request failed with status code: \" + str(httpError.code))\n\n    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n    print(httpError.info())\n\n    print(json.loads(httpError.read()))\n    return\n\n\ndef saveBlobToFile(blobUrl, resultsLabel):\n    output_file = \"myresults.csv\" # Replace this with the location you would like to use for your output file\n    print(\"Reading the result from \" + blobUrl)\n    try:\n        # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n        response = urllib.request.urlopen(blobUrl)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    with open(output_file, \"w+\") as f:\n        f.write(response.read())\n    print(resultsLabel + \" have been written to the file \" + output_file)\n    return\n\n\ndef processResults(result):\n\n\n    first = True\n    results = result[\"Results\"]\n    for outputName in results:\n        result_blob_location = results[outputName]\n        sas_token = result_blob_location[\"SasBlobToken\"]\n        base_url = result_blob_location[\"BaseLocation\"]\n        relative_url = result_blob_location[\"RelativeLocation\"]\n\n        print(\"The results for \" + outputName + \" are available at the following Azure Storage location:\")\n        print(\"BaseLocation: \" + base_url)\n        print(\"RelativeLocation: \" + relative_url)\n        print(\"SasBlobToken: \" + sas_token)\n\n\n        if (first):\n            first = False\n            url3 = base_url + relative_url + sas_token\n            saveBlobToFile(url3, \"The results for \" + outputName)\n    return\n\n\n\ndef uploadFileToBlob(input_file, input_blob_name, storage_container_name, storage_account_name, storage_account_key):\n    blob_service = asb.BlockBlobService(account_name=storage_account_name, account_key=storage_account_key)\n\n    print(\"Uploading the input to blob storage...\")\n    data_to_upload = open(input_file, \"r\").read()\n    blob_service.put_blob(storage_container_name, input_blob_name, data_to_upload, x_ms_blob_type=\"BlockBlob\")\n\ndef invokeBatchExecutionService():\n    storage_account_name = \"storage1\" # Replace this with your Azure Storage Account name\n    storage_account_key = \"kOveEtQMoP5zbUGfFR47\" # Replace this with your Azure Storage Key\n    storage_container_name = \"input\" # Replace this with your Azure Storage Container name\n    connection_string = \"DefaultEndpointsProtocol=https;AccountName=\" + storage_account_name + \";AccountKey=\" + storage_account_key #\"DefaultEndpointsProtocol=https;AccountName=mayatostorage1;AccountKey=aOYA2P5VQPR3ZQCl+aWhcGhDRJhsR225teGGBKtfXWwb2fNEo0CrhlwGWdfbYiBTTXPHYoKZyMaKuEAU8A\/Fzw==;EndpointSuffix=core.windows.net\"\n    api_key = \"5wUaln7n99rt9k+enRLG2OrhSsr9VLeoCfh0q3mfYo27hfTCh32f10PsRjJtuA==\" # Replace this with the API key for the web service\n    url = \"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/306bc1f050\/services\/61670382104542bc9533a920830b263c\/jobs\" #\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/services\/61670382104542bc9533a920830b263c\/jobs\/job_id\/start?api-version=2.0\"\n\n\n\n    uploadFileToBlob(r\"C:\\Users\\Alex\\Desktop\\16_da.csv\", # Replace this with the location of your input file\n                     \"input1datablob.csv\", # Replace this with the name you would like to use for your Azure blob; this needs to have the same extension as the input file \n                     storage_container_name, storage_account_name, storage_account_key)\n\n    payload =  {\n\n        \"Inputs\": {\n\n            \"input1\": { \"ConnectionString\": connection_string, \"RelativeLocation\": \"\/\" + storage_container_name + \"\/input1datablob.csv\" },\n        },     \n\n        \"Outputs\": {\n\n            \"output1\": { \"ConnectionString\": connection_string, \"RelativeLocation\": \"\/\" + storage_container_name + \"\/output1results.csv\" },\n        },\n        \"GlobalParameters\": {\n}\n    }\n\n    body = str.encode(json.dumps(payload))\n    headers = { \"Content-Type\":\"application\/json\", \"Authorization\":(\"Bearer \" + api_key)}\n    print(\"Submitting the job...\")\n\n    # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n\n    # submit the job\n    req = urllib.request.Request(url + \"?api-version=2.0\", body, headers)\n    try:\n        response = urllib.request.urlopen(req)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    result = response.read()\n    job_id = result[1:-1] # remove the enclosing double-quotes\n    print(\"Job ID: \" + job_id)\n\n\n    # If you are using Python 3+, replace urllib2 with urllib.request in the following code\n    # start the job\n    print(\"Starting the job...\")\n    req = urllib.request.Request(url + \"\/\" + job_id + \"\/start?api-version=2.0\", \"\", headers)\n    try:\n        response = urllib.request.urlopen(req)\n    except urllib.request.HTTPError:\n        printHttpError(urllib.HTTPError)\n        return\n\n    url2 = url + \"\/\" + job_id + \"?api-version=2.0\"\n\n    while True:\n        print(\"Checking the job status...\")\n        # If you are using Python 3+, replace urllib2 with urllib.request in the follwing code\n        req = urllib.request.Request(url2, headers = { \"Authorization\":(\"Bearer \" + api_key) })\n\n        try:\n            response = urllib.request.urlopen(req)\n        except urllib.request.HTTPError:\n            printHttpError(urllib.HTTPError)\n            return    \n\n        result = json.loads(response.read())\n        status = result[\"StatusCode\"]\n        if (status == 0 or status == \"NotStarted\"):\n            print(\"Job \" + job_id + \" not yet started...\")\n        elif (status == 1 or status == \"Running\"):\n            print(\"Job \" + job_id + \" running...\")\n        elif (status == 2 or status == \"Failed\"):\n            print(\"Job \" + job_id + \" failed!\")\n            print(\"Error details: \" + result[\"Details\"])\n            break\n        elif (status == 3 or status == \"Cancelled\"):\n            print(\"Job \" + job_id + \" cancelled!\")\n            break\n        elif (status == 4 or status == \"Finished\"):\n            print(\"Job \" + job_id + \" finished!\")\n\n            processResults(result)\n            break\n        time.sleep(1) # wait one second\n    return\n\ninvokeBatchExecutionService()\n<\/code><\/pre>\n\n<p>EDIT 2: The above issue has been resolved thanks to jon and the csv gets uploaded in blob storage.<\/p>\n\n<p>However now there is an HTTPError, when the job gets submitted in Line 130:<\/p>\n\n<pre><code>   raise HTTPError(req.full_url, code, msg, hdrs, fp)  HTTPError: Bad Request\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1530205297260,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":499.0,
        "Owner_creation_time":1513445313103,
        "Owner_last_access_time":1646340377376,
        "Owner_reputation":29.0,
        "Owner_up_votes":11.0,
        "Owner_down_votes":0.0,
        "Owner_views":10.0,
        "Answer_body":"<p>I think the code they give may be pretty old at this point.<\/p>\n\n<p>The <a href=\"https:\/\/pypi.org\/project\/azure-storage-blob\/#history\" rel=\"nofollow noreferrer\">latest version<\/a> of <code>azure.storage.blob<\/code> is 1.3. So perhaps a <code>pip install azure.storage.blob --update<\/code> or simply uninstalling and reinstalling would help.<\/p>\n\n<p>Once you got the latest version, try using the <code>create_blob_from_text<\/code> method to load the file to your storage container.<\/p>\n\n<pre><code>from azure.storage.blob import BlockBlobService\n\nblobService = BlockBlobService(account_name=\"accountName\", account_key=\"accountKey)\n\nblobService.create_blob_from_text(\"containerName\", \"fileName\", csv_file)\n<\/code><\/pre>\n\n<p>Hope that works to help lead you down the right path, but if not we can work through it. :)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1530266278528,
        "Answer_score":1.0,
        "Owner_location":"NE",
        "Question_last_edit_time":1530279219952,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51088145",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure python sdk & machine learning studio web service batch execution snippet: typeerror; Content: first issue resolved, please read scroll down to edit2 i'm trying to access a web service deployed via studio, using the batch execution-sample code for python on the bottom of below page: https:\/\/studio..net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs i have already fixed an issue according to this question (replaced blobservice by blobblockservice and so on): https:\/\/studio..net\/apihelp\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/webservices\/e4e3d2d32ec347ae9a829b200f7d31cd\/endpoints\/61670382104542bc9533a920830b263c\/jobs and i also have entered the api-key, container-name, url, account_key and account_name according to the instructions. however it seems that today the code snippet is even more outdated than it was back then because i receive a different error now: file \"c:\/users\/alex\/desktop\/scripts\/batchexecution.py\", line 80, in uploadfiletoblob blob_service = asb.blockblobservice(account_name=storage_account_name, account_key=storage_account_key) file \"c:\\users\\alex\\anaconda3\\lib\\site-packages\\azure\\storage\\blob\\blockblobservice.py\", line 145, in __init__ file \"c:\\users\\alex\\anaconda3\\lib\\site-packages\\azure\\storage\\blob\\baseblobservice.py\", line 205, in __init__ typeerror: get_service_parameters() got an unexpected keyword argument 'token_credential' i also noticed, that when installing the azure sdk for python via pip, i get the following warnings in the end of the process (installation is successful however): azure-storage-queue 1.3.0 has requirement azure-storage-common<1.4.0,>=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible. azure-storage-file 1.3.0 has requirement azure-storage-common<1.4.0,>=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible. azure-storage-blob 1.3.0 has requirement azure-storage-common<1.4.0,>=1.3.0, but you'll have azure-storage-common 1.1.0 which is incompatible. i can't find anything about all this in the latest documentation for the python sdk (the word 'token_credential' is not even contained): https:\/\/media.readthedocs.org\/pdf\/azure-storage\/latest\/azure-storage.pdf does anyone have a clue what's going wrong during the installation or why the type-error with the 'token_credential' pops up during execution? or does anyone know how i can install the necessary version of azure-storage-common or azure-storage-blob? edit: here's a my code (however not-reproducible because i changed the keys before posting) # how this works: # # 1. assume the input is present in a local file (if the web service accepts input) # 2. upload the file to an azure blob - you\"d need an azure storage account # 3. call bes to process the data in the blob. # 4. the results get written to another azure blob. # 5. download the output blob to a local file # # note: you may need to download\/install the azure sdk for python. # see: http:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/python-how-to-install\/ import urllib # if you are using python 3+, import urllib instead of urllib2 import json import time import azure.storage.blob as asb # replaces blobservice by blobblockservice def printhttperror(httperror): print(\"the request failed with status code: \" + str(httperror.code)) # print the headers - they include the requert id and the timestamp, which are useful for debugging the failure print(httperror.info()) print(json.loads(httperror.read())) return def saveblobtofile(bloburl, resultslabel): output_file = \"myresults.csv\" # replace this with the location you would like to use for your output file print(\"reading the result from \" + bloburl) try: # if you are using python 3+, replace urllib2 with urllib.request in the following code response = urllib.request.urlopen(bloburl) except urllib.request.httperror: printhttperror(urllib.httperror) return with open(output_file, \"w+\") as f: f.write(response.read()) print(resultslabel + \" have been written to the file \" + output_file) return def processresults(result): first = true results = result[\"results\"] for outputname in results: result_blob_location = results[outputname] sas_token = result_blob_location[\"sasblobtoken\"] base_url = result_blob_location[\"baselocation\"] relative_url = result_blob_location[\"relativelocation\"] print(\"the results for \" + outputname + \" are available at the following azure storage location:\") print(\"baselocation: \" + base_url) print(\"relativelocation: \" + relative_url) print(\"sasblobtoken: \" + sas_token) if (first): first = false url3 = base_url + relative_url + sas_token saveblobtofile(url3, \"the results for \" + outputname) return def uploadfiletoblob(input_file, input_blob_name, storage_container_name, storage_account_name, storage_account_key): blob_service = asb.blockblobservice(account_name=storage_account_name, account_key=storage_account_key) print(\"uploading the input to blob storage...\") data_to_upload = open(input_file, \"r\").read() blob_service.put_blob(storage_container_name, input_blob_name, data_to_upload, x_ms_blob_type=\"blockblob\") def invokebatchexecutionservice(): storage_account_name = \"storage1\" # replace this with your azure storage account name storage_account_key = \"koveetqmop5zbugffr47\" # replace this with your azure storage key storage_container_name = \"input\" # replace this with your azure storage container name connection_string = \"defaultendpointsprotocol=https;accountname=\" + storage_account_name + \";accountkey=\" + storage_account_key #\"defaultendpointsprotocol=https;accountname=mayatostorage1;accountkey=aoya2p5vqpr3zqcl+awhcghdrjhsr225teggbktfxwwb2fneo0crhlwgwdfbyibttxphyokzymakueau8a\/fzw==;endpointsuffix=core.windows.net\" api_key = \"5wualn7n99rt9k+enrlg2orhssr9vleocfh0q3mfyo27hftch32f10psrjjtua==\" # replace this with the api key for the web service url = \"https:\/\/ussouthcentral.services..net\/workspaces\/306bc1f050\/services\/61670382104542bc9533a920830b263c\/jobs\" #\"https:\/\/ussouthcentral.services..net\/workspaces\/306bc1f050ba4cdba0dbc6cc561c6ab0\/services\/61670382104542bc9533a920830b263c\/jobs\/job_id\/start?api-version=2.0\" uploadfiletoblob(r\"c:\\users\\alex\\desktop\\16_da.csv\", # replace this with the location of your input file \"input1datablob.csv\", # replace this with the name you would like to use for your azure blob; this needs to have the same extension as the input file storage_container_name, storage_account_name, storage_account_key) payload = { \"inputs\": { \"input1\": { \"connectionstring\": connection_string, \"relativelocation\": \"\/\" + storage_container_name + \"\/input1datablob.csv\" }, }, \"outputs\": { \"output1\": { \"connectionstring\": connection_string, \"relativelocation\": \"\/\" + storage_container_name + \"\/output1results.csv\" }, }, \"globalparameters\": { } } body = str.encode(json.dumps(payload)) headers = { \"content-type\":\"application\/json\", \"authorization\":(\"bearer \" + api_key)} print(\"submitting the job...\") # if you are using python 3+, replace urllib2 with urllib.request in the following code # submit the job req = urllib.request.request(url + \"?api-version=2.0\", body, headers) try: response = urllib.request.urlopen(req) except urllib.request.httperror: printhttperror(urllib.httperror) return result = response.read() job_id = result[1:-1] # remove the enclosing double-quotes print(\"job id: \" + job_id) # if you are using python 3+, replace urllib2 with urllib.request in the following code # start the job print(\"starting the job...\") req = urllib.request.request(url + \"\/\" + job_id + \"\/start?api-version=2.0\", \"\", headers) try: response = urllib.request.urlopen(req) except urllib.request.httperror: printhttperror(urllib.httperror) return url2 = url + \"\/\" + job_id + \"?api-version=2.0\" while true: print(\"checking the job status...\") # if you are using python 3+, replace urllib2 with urllib.request in the follwing code req = urllib.request.request(url2, headers = { \"authorization\":(\"bearer \" + api_key) }) try: response = urllib.request.urlopen(req) except urllib.request.httperror: printhttperror(urllib.httperror) return result = json.loads(response.read()) status = result[\"statuscode\"] if (status == 0 or status == \"notstarted\"): print(\"job \" + job_id + \" not yet started...\") elif (status == 1 or status == \"running\"): print(\"job \" + job_id + \" running...\") elif (status == 2 or status == \"failed\"): print(\"job \" + job_id + \" failed!\") print(\"error details: \" + result[\"details\"]) break elif (status == 3 or status == \"cancelled\"): print(\"job \" + job_id + \" cancelled!\") break elif (status == 4 or status == \"finished\"): print(\"job \" + job_id + \" finished!\") processresults(result) break time.sleep(1) # wait one second return invokebatchexecutionservice() edit 2: the above issue has been resolved thanks to jon and the csv gets uploaded in blob storage. however now there is an httperror, when the job gets submitted in line 130: raise httperror(req.full_url, code, msg, hdrs, fp) httperror: bad request",
        "Question_original_content_gpt_summary":"The user encountered multiple challenges while attempting to access a web service deployed via Azure Machine Learning Studio, including an unexpected TypeError, warnings during installation, and an HTTPError when submitting the job.",
        "Question_preprocessed_content":"Title: azure python sdk & machine learning studio web service batch execution snippet typeerror; Content: first issue resolved, please read scroll down to edit i'm trying to access a web service deployed via studio, using the batch code for python on the bottom of below page i have already fixed an issue according to this question and i also have entered the url, and according to the instructions. however it seems that today the code snippet is even more outdated than it was back then because i receive a different error now i also noticed, that when installing the azure sdk for python via pip, i get the following warnings in the end of the process i can't find anything about all this in the latest documentation for the python sdk does anyone have a clue what's going wrong during the installation or why the with the pops up during execution? or does anyone know how i can install the necessary version of or edit here's a my code edit the above issue has been resolved thanks to jon and the csv gets uploaded in blob storage. however now there is an httperror, when the job gets submitted in line",
        "Answer_original_content":"i think the code they give may be pretty old at this point. the latest version of azure.storage.blob is 1.3. so perhaps a pip install azure.storage.blob --update or simply uninstalling and reinstalling would help. once you got the latest version, try using the create_blob_from_text method to load the file to your storage container. from azure.storage.blob import blockblobservice blobservice = blockblobservice(account_name=\"accountname\", account_key=\"accountkey) blobservice.create_blob_from_text(\"containername\", \"filename\", csv_file) hope that works to help lead you down the right path, but if not we can work through it. :)",
        "Answer_original_content_gpt_summary":"Possible solutions include updating the version of azure.storage.blob, uninstalling and reinstalling it, and using the create_blob_from_text method to load the file to the storage container. The user can also try using the provided code to access the web service and work through any remaining issues.",
        "Answer_preprocessed_content":"i think the code they give may be pretty old at this point. the latest version of is so perhaps a or simply uninstalling and reinstalling would help. once you got the latest version, try using the method to load the file to your storage container. hope that works to help lead you down the right path, but if not we can work through it."
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML - Notebook - Jupyter Kernel Error - No Kernel Connecl",
        "Question_body":"In Azure ML i am following the tutorial to execute a notebook and when i open the notebook and have a valid compute (running as well as the green icon) it shows an error 'Jupyter Kernel Error'. On the far right of the screen it says 'No Kernel Connnected'. What is needed to connect the kernel?",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1595605454123,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/52057\/azure-ml-notebook-jupyter-kernel-error-no-kernel-c.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-28T02:40:35.287Z",
                "Answer_score":0,
                "Answer_body":"Hi,\nmay I know which tutorial you are mentioning? If you are facing issue with notebooks.azure.com then the most likely scenario is your project is not running. Please navigate back to the project page and run your project before opening the required notebook in a new window which should show all the options to select the kernel and run a cell.\n\nThanks,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-19T06:18:42.667Z",
                "Answer_score":0,
                "Answer_body":"In new ML workspace you need to setup the computing resource to run the jupyter. The resource can be single VM, cluster (available set) or container set. You also can select using GPU or not.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":0.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - notebook - jupyter kernel error - no kernel connecl; Content: in i am following the tutorial to execute a notebook and when i open the notebook and have a valid compute (running as well as the green icon) it shows an error 'jupyter kernel error'. on the far right of the screen it says 'no kernel connnected'. what is needed to connect the kernel?",
        "Question_original_content_gpt_summary":"The user is encountering an error with their Jupyter notebook, where the kernel is not connecting and displaying an error message.",
        "Question_preprocessed_content":"Title: notebook jupyter kernel error no kernel connecl; Content: in i am following the tutorial to execute a notebook and when i open the notebook and have a valid compute it shows an error 'jupyter kernel error'. on the far right of the screen it says 'no kernel connnected'. what is needed to connect the kernel?",
        "Answer_original_content":"hi, may i know which tutorial you are mentioning? if you are facing issue with notebooks.azure.com then the most likely scenario is your project is not running. please navigate back to the project page and run your project before opening the required notebook in a new window which should show all the options to select the kernel and run a cell. thanks, yutong in new ml workspace you need to setup the computing resource to run the jupyter. the resource can be single vm, cluster (available set) or container set. you also can select using gpu or not.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n1. If the user is facing an issue with notebooks.azure.com, the most likely scenario is that their project is not running. They should navigate back to the project page and run their project before opening the required notebook in a new window, which should show all the options to select the kernel and run a cell.\n\n2. In the new ml workspace, the user needs to set up the computing resource to run the Jupyter notebook. The resource can be a single VM, cluster (available set), or container set. They can also select whether to use GPU or not.",
        "Answer_preprocessed_content":"hi, may i know which tutorial you are mentioning? if you are facing issue with then the most likely scenario is your project is not running. please navigate back to the project page and run your project before opening the required notebook in a new window which should show all the options to select the kernel and run a cell. thanks, yutong in new ml workspace you need to setup the computing resource to run the jupyter. the resource can be single vm, cluster or container set. you also can select using gpu or not."
    },
    {
        "Question_id":null,
        "Question_title":"Usage of Sagemaker Processing Job Manifest File",
        "Question_body":"I have a processing Job that uses input files saved in different folders of a S3 Bucket and use the Manifest file within the processing Job to copy it to \/opt\/ml\/processing\/input Folder.\n\nThis works perfectly fine when i have all the files in one folder but wont work when they are under the same prefix but under different folders.\n\nFollowing the steps listed in the url https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_S3DataSource.html\n\n[ {\"prefix\": \"s3:\/\/customer_bucket\/some\/prefix\/\"},\n\n\"relative\/path\/to\/custdata-1\",\n\n\"relative\/path\/custdata-2\",\n\n...\n\n\"relative\/path\/custdata-N\"\n\n]\n\nIf i have all the input files in \"relative\/path1\/custdata-1\" The job works fine but if i add another one \"relative\/path2\/custdata-2\", there is no file copied and my script fails with no such file or directory.\n\nAny suggestions or advise on this will be very helpful.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1646687511442,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":322.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAKFZ3-NLSIuWWSesoqRPRQ\/usage-of-sagemaker-processing-job-manifest-file",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-10T04:38:42.624Z",
                "Answer_score":0,
                "Answer_body":"I've used Processing Job ManifestFile inputs successfully in the past with multiple relative folders (for e.g. the \"Extract clean input images\" section of this notebook - sorry for citing a large\/sprawling sample, there are probably simpler ones out there).\n\nNot sure exactly what could be going wrong here so I'll try to describe using the feature as I think of it and hope that helps:\n\nGiven an S3 bucket containing:\n\ns3:\/\/customer_bucket\/some\/prefix\/relative\/path1\/custdata-1\ns3:\/\/customer_bucket\/some\/prefix\/relative\/path2\/custdata-2\n\n\n...and a manifest file like:\n\n[ { \"prefix\":  \"s3:\/\/customer_bucket\/some\/prefix\/\" },\n  \"relative\/path1\/custdata-1\",\n  \"relative\/path2\/custdata-2\"\n]\n\n...for a processing input something like the below (or equivalent if you're using boto3\/etc instead of the SageMaker Python SDK):\n\nProcessingInput(\n    destination=\"\/opt\/ml\/processing\/input\/mycoolinput\",\n    input_name=\"mycoolinput\",\n    s3_data_type=\"ManifestFile\",\n    source=\"s3:\/\/path-to-your-manifest-file\",\n)\n\n...I'd expect your processing job to see files:\n\n\/opt\/ml\/processing\/input\/mycoolinput\/relative\/path1\/custdata-1\n\/opt\/ml\/processing\/input\/mycoolinput\/relative\/path2\/custdata-2\n\n\nSo in this sense it is possible to have files under the same prefix with different subfolders. In the above mentioned sample, the raw_s3uri prefix contains credit card agreement PDFs categorized into folders by bank\/provider - e.g. {raw_s3uri}\/Bank1\/Card1.pdf, {raw_s3uri}\/CreditUnion2\/Disclosures.pdf, etc.\n\nTo my knowledge it's not possible to have multiple { \"prefix\": \"...\" } entries in your manifest, but as I understood it didn't sound like you were trying to do that.\n\nApart from double-checking this overall setup (and maybe using Python os.walk() to recursively print() out the folder contents as your Processing job sees them), the only other thing I could suggest is to check if your S3 object keys have any special characters in them that could be causing issues when mapping to a local filesystem - such as files\/folders with spaces at the end, or characters that aren't usually allowed in filenames?",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: usage of processing job manifest file; Content: i have a processing job that uses input files saved in different folders of a s3 bucket and use the manifest file within the processing job to copy it to \/opt\/ml\/processing\/input folder. this works perfectly fine when i have all the files in one folder but wont work when they are under the same prefix but under different folders. following the steps listed in the url https:\/\/docs.aws.amazon.com\/\/latest\/apireference\/api_s3datasource.html [ {\"prefix\": \"s3:\/\/customer_bucket\/some\/prefix\/\"}, \"relative\/path\/to\/custdata-1\", \"relative\/path\/custdata-2\", ... \"relative\/path\/custdata-n\" ] if i have all the input files in \"relative\/path1\/custdata-1\" the job works fine but if i add another one \"relative\/path2\/custdata-2\", there is no file copied and my script fails with no such file or directory. any suggestions or advise on this will be very helpful.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with using a processing job manifest file to copy input files saved in different folders of a s3 bucket to the \/opt\/ml\/processing\/input folder.",
        "Question_preprocessed_content":"Title: usage of processing job manifest file; Content: i have a processing job that uses input files saved in different folders of a s bucket and use the manifest file within the processing job to copy it to folder. this works perfectly fine when i have all the files in one folder but wont work when they are under the same prefix but under different folders. following the steps listed in the url , if i have all the input files in the job works fine but if i add another one there is no file copied and my script fails with no such file or directory. any suggestions or advise on this will be very helpful.",
        "Answer_original_content":"i've used processing job manifestfile inputs successfully in the past with multiple relative folders (for e.g. the \"extract clean input images\" section of this notebook - sorry for citing a large\/sprawling sample, there are probably simpler ones out there). not sure exactly what could be going wrong here so i'll try to describe using the feature as i think of it and hope that helps: given an s3 bucket containing: s3:\/\/customer_bucket\/some\/prefix\/relative\/path1\/custdata-1 s3:\/\/customer_bucket\/some\/prefix\/relative\/path2\/custdata-2 ...and a manifest file like: [ { \"prefix\": \"s3:\/\/customer_bucket\/some\/prefix\/\" }, \"relative\/path1\/custdata-1\", \"relative\/path2\/custdata-2\" ] ...for a processing input something like the below (or equivalent if you're using boto3\/etc instead of the python sdk): processinginput( destination=\"\/opt\/ml\/processing\/input\/mycoolinput\", input_name=\"mycoolinput\", s3_data_type=\"manifestfile\", source=\"s3:\/\/path-to-your-manifest-file\", ) ...i'd expect your processing job to see files: \/opt\/ml\/processing\/input\/mycoolinput\/relative\/path1\/custdata-1 \/opt\/ml\/processing\/input\/mycoolinput\/relative\/path2\/custdata-2 so in this sense it is possible to have files under the same prefix with different subfolders. in the above mentioned sample, the raw_s3uri prefix contains credit card agreement pdfs categorized into folders by bank\/provider - e.g. {raw_s3uri}\/bank1\/card1.pdf, {raw_s3uri}\/creditunion2\/disclosures.pdf, etc. to my knowledge it's not possible to have multiple { \"prefix\": \"...\" } entries in your manifest, but as i understood it didn't sound like you were trying to do that. apart from double-checking this overall setup (and maybe using python os.walk() to recursively print() out the folder contents as your processing job sees them), the only other thing i could suggest is to check if your s3 object keys have any special characters in them that could be causing issues when mapping to a local filesystem - such as files\/folders with spaces at the end, or characters that aren't usually allowed in filenames?",
        "Answer_original_content_gpt_summary":"The answer suggests that the user can use a processing job manifest file to copy input files saved in different folders of an s3 bucket to the \/opt\/ml\/processing\/input folder. The answer provides an example of how to use the feature and suggests double-checking the overall setup and checking for special characters in the s3 object keys that could be causing issues.",
        "Answer_preprocessed_content":"i've used processing job manifestfile inputs successfully in the past with multiple relative folders . not sure exactly what could be going wrong here so i'll try to describe using the feature as i think of it and hope that helps given an s bucket containing a manifest file like , a processing input something like the below processinginput expect your processing job to see files so in this sense it is possible to have files under the same prefix with different subfolders. in the above mentioned sample, the prefix contains credit card agreement pdfs categorized into folders by etc. to my knowledge it's not possible to have multiple entries in your manifest, but as i understood it didn't sound like you were trying to do that. apart from this overall setup to recursively print out the folder contents as your processing job sees them , the only other thing i could suggest is to check if your s object keys have any special characters in them that could be causing issues when mapping to a local filesystem such as with spaces at the end, or characters that aren't usually allowed in filenames?"
    },
    {
        "Question_id":null,
        "Question_title":"Dvc get git@github.com:*.git folder giving ERROR: unexpected error - Response payload is not completed",
        "Question_body":"<p>I have successfully added a folder with 2gb of files using DVC to an s3 bucket. After that I tried to get the folder using the dvc get command. It downloaded half the folder correctly and after that the download stopped and gave me this message: ERROR: Unexpected error - Response payload not completed<br>\nAny suggestions on how I can resolve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1634238227604,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":355.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-get-git-github-com-git-folder-giving-error-unexpected-error-response-payload-is-not-completed\/919",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-15T21:38:57.691Z",
                "Answer_body":"<p>For the record: this was discussed on discord <a href=\"https:\/\/discord.com\/channels\/485586884165107732\/485596304961962003\/898534803362701322\" class=\"inline-onebox\">Discord<\/a> and seems to have been a network issue.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: get git@github.com:*.git folder giving error: unexpected error - response payload is not completed; Content: i have successfully added a folder with 2gb of files using to an s3 bucket. after that i tried to get the folder using the get command. it downloaded half the folder correctly and after that the download stopped and gave me this message: error: unexpected error - response payload not completed any suggestions on how i can resolve this issue?",
        "Question_original_content_gpt_summary":"The user encountered an unexpected error while attempting to download a folder from an S3 bucket using the get command.",
        "Question_preprocessed_content":"Title: get folder giving error unexpected error response payload is not completed; Content: i have successfully added a folder with gb of files using to an s bucket. after that i tried to get the folder using the get command. it downloaded half the folder correctly and after that the download stopped and gave me this message error unexpected error response payload not completed any suggestions on how i can resolve this issue?",
        "Answer_original_content":"for the record: this was discussed on discord discord and seems to have been a network issue.",
        "Answer_original_content_gpt_summary":"There is no specific solution mentioned in the answer. However, it suggests that the error might have been caused by a network issue.",
        "Answer_preprocessed_content":"for the record this was discussed on discord discord and seems to have been a network issue."
    },
    {
        "Question_id":38563051.0,
        "Question_title":"Getting the images produced by AzureML experiments back",
        "Question_body":"<p>I have created a toy example in Azure.\nI have the following dataset:<\/p>\n\n<pre><code>  amounts       city code user_id\n1    2.95 Colleferro  100     999\n2    2.95    Subiaco  100     111\n3   14.95   Avellino  101     333\n4   14.95 Colleferro  101     999\n5   14.95  Benevento  101     444\n6  -14.95    Subiaco  110     111\n7  -14.95   Sgurgola  110     555\n8  -14.95       Roma  110     666\n9  -14.95 Colleferro  110     999\n<\/code><\/pre>\n\n<p>I create an AzureML experiment that simply plots the column of the amounts.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/TgRTW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TgRTW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The code into the R script module is the following:<\/p>\n\n<pre><code>data.set &lt;- maml.mapInputPort(1) # class: data.frame  \n#-------------------\nplot(data.set$amounts);\ntitle(\"This title is a very long title. That is not a problem for R, but it becomes a problem when Azure manages it in the visualization.\")\n#-------------------\nmaml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>Now, if you click on the right output port of the R script and then on \"Visualize\"<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/pTkSH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pTkSH.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>you will see the Azure page where the outputs are shown.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Iv5GM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iv5GM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now, the following happens:<\/p>\n\n<ol>\n<li>The plot is <em>stucked<\/em> into an estabilished space (example: the title is cut!!!)<\/li>\n<li>The image produced is a <em>low resolution<\/em> one.<\/li>\n<li>The JSON produced by Azure is \"dirty\" (making the <em>decoding<\/em> in C# difficult).<\/li>\n<\/ol>\n\n<p>It seems that this is not the best way to get the images produced by the AzureML experiment. <\/p>\n\n<p>Possible solution: I would like <\/p>\n\n<blockquote>\n  <p>to send the picture produced in my experiment to a space like the blob\n  storage. <\/p>\n<\/blockquote>\n\n<p>This would be also a great solution when I have a web-app and I have to pick the image produced by Azure and put it on my Web App page.\nDo you know if there is a way to send the image somewhere?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1469435784157,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":593.0,
        "Owner_creation_time":1436432728608,
        "Owner_last_access_time":1663607665487,
        "Owner_reputation":809.0,
        "Owner_up_votes":109.0,
        "Owner_down_votes":0.0,
        "Owner_views":361.0,
        "Answer_body":"<p>To saving the images into Azure Blob Storage with R, you need to do two steps, which include getting the images from the R device output of <code>Execute R Script<\/code> and uploading the images to Blob Storage.<\/p>\n\n<p>There are two ways to implement the steps above.<\/p>\n\n<ol>\n<li><p>You can publish the experiment as a webservice, then get the images with base64 encoding from the response of the webservice request and use Azure Blob Storage <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/dd179451.aspx\" rel=\"nofollow\">REST API<\/a> with R to upload the images. Please refer to the article <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/10\/24\/how-to-retrieve-r-data-visualization-from-azure-machine-learning\/\" rel=\"nofollow\">How to retrieve R data visualization from Azure Machine Learning<\/a>.<\/p><\/li>\n<li><p>You can directly add a module in C# to get &amp; upload the images from the output of <code>Execute R Script<\/code>. Please refer to the article <a href=\"https:\/\/blogs.msdn.microsoft.com\/data_insights_global_practice\/2015\/12\/15\/accessing-a-visual-generated-from-r-code-in-azureml\/\" rel=\"nofollow\">Accessing a Visual Generated from R Code in AzureML<\/a>.<\/p><\/li>\n<\/ol>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1469518311968,
        "Answer_score":1.0,
        "Owner_location":"Colleferro, Italy",
        "Question_last_edit_time":1469500135808,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38563051",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: getting the images produced by experiments back; Content: i have created a toy example in azure. i have the following dataset: amounts city code user_id 1 2.95 colleferro 100 999 2 2.95 subiaco 100 111 3 14.95 avellino 101 333 4 14.95 colleferro 101 999 5 14.95 benevento 101 444 6 -14.95 subiaco 110 111 7 -14.95 sgurgola 110 555 8 -14.95 roma 110 666 9 -14.95 colleferro 110 999 i create an experiment that simply plots the column of the amounts. the code into the r script module is the following: data.set <- maml.mapinputport(1) # class: data.frame #------------------- plot(data.set$amounts); title(\"this title is a very long title. that is not a problem for r, but it becomes a problem when azure manages it in the visualization.\") #------------------- maml.mapoutputport(\"data.set\"); now, if you click on the right output port of the r script and then on \"visualize\" you will see the azure page where the outputs are shown. now, the following happens: the plot is stucked into an estabilished space (example: the title is cut!!!) the image produced is a low resolution one. the json produced by azure is \"dirty\" (making the decoding in c# difficult). it seems that this is not the best way to get the images produced by the experiment. possible solution: i would like to send the picture produced in my experiment to a space like the blob storage. this would be also a great solution when i have a web-app and i have to pick the image produced by azure and put it on my web app page. do you know if there is a way to send the image somewhere?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with getting the images produced by their experiments back, including being stuck in a space, having a low resolution, and having a \"dirty\" JSON produced by Azure.",
        "Question_preprocessed_content":"Title: getting the images produced by experiments back; Content: i have created a toy example in azure. i have the following dataset i create an experiment that simply plots the column of the amounts. the code into the r script module is the following now, if you click on the right output port of the r script and then on visualize you will see the azure page where the outputs are shown. now, the following happens the plot is stucked into an estabilished space the image produced is a low resolution one. the json produced by azure is dirty . it seems that this is not the best way to get the images produced by the experiment. possible solution i would like to send the picture produced in my experiment to a space like the blob storage. this would be also a great solution when i have a and i have to pick the image produced by azure and put it on my web app page. do you know if there is a way to send the image somewhere?",
        "Answer_original_content":"to saving the images into azure blob storage with r, you need to do two steps, which include getting the images from the r device output of execute r script and uploading the images to blob storage. there are two ways to implement the steps above. you can publish the experiment as a webservice, then get the images with base64 encoding from the response of the webservice request and use azure blob storage rest api with r to upload the images. please refer to the article how to retrieve r data visualization from . you can directly add a module in c# to get & upload the images from the output of execute r script. please refer to the article accessing a visual generated from r code in .",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of getting images produced by experiments back include saving the images into Azure blob storage with R, which involves getting the images from the R device output and uploading them to blob storage. This can be done by publishing the experiment as a webservice and getting the images with base64 encoding from the response of the webservice request, or by adding a module in C# to get and upload the images from the output of execute R script.",
        "Answer_preprocessed_content":"to saving the images into azure blob storage with r, you need to do two steps, which include getting the images from the r device output of and uploading the images to blob storage. there are two ways to implement the steps above. you can publish the experiment as a webservice, then get the images with base encoding from the response of the webservice request and use azure blob storage rest api with r to upload the images. please refer to the article how to retrieve r data visualization from . you can directly add a module in c to get & upload the images from the output of . please refer to the article accessing a visual generated from r code in ."
    },
    {
        "Question_id":null,
        "Question_title":"Azure TabularDataset wrongly loads Parquet?",
        "Question_body":"Below I give a concrete example where azureml Python api fails to correctly read Parquet files.\nMore precisely, the data gets displaced. It may clarify this issue where data could not be publicly shared posted by @KengoWada-6837.\n\nSetup: Python 3.8 + azureml-core=1.36.0 + azureml-dataprep=2.26.0 + pyarrow=7.0.0\n\nThe data is attached182488-error.log\n\nThe code demonstrating the issue is given below.\nIt uses an input data to create a table of strings along with some None values, stores in Parquet format, and reads either directly or through TabularDataset.\n\n from azureml.core import Workspace, Dataset\n import tempfile\n import pandas as pd\n import hashlib\n    \n # prepare data: list of hashs with some None values\n df = pd.read_csv(\"error.log\")\n mask = df.isna().any(1)\n df.id = df.id.map(lambda s:hashlib.sha512(str(s).encode()).hexdigest() if s else None)\n df.loc[mask,'id'] = None\n # configure Azure storage\n ws = Workspace.from_config()\n dstore = ws.datastores.get('my_datastore')\n dstore_path = 'my_path'\n target = (dstore,dstore_path)\n # write to Azure storage\n with tempfile.TemporaryDirectory() as tmpdir:\n     df.to_parquet(f'{tmpdir}\/df.parquet')\n     ds=Dataset.File.upload_directory(tmpdir,target,overwrite=True)\n    \n # read by two ways: download and open in pandas or use the Azure connector\n with tempfile.TemporaryDirectory() as tmpdir:\n     ds=Dataset.File.from_files(target)\n     ds.download(tmpdir)\n     df1 = pd.read_parquet(tmpdir)\n     ds = Dataset.Tabular.from_parquet_files(target)\n     df2 = ds.to_pandas_dataframe()\n    \n # comparison fails, the data seems displaced :-(\n pd.testing.assert_frame_equal(df1,df2)\n\n\n\n\n\n\n\nFWD: @ramr-msft",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1647121761043,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/769866\/azure-dataflow-wrongly-loads-parquet.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-14T10:59:31.277Z",
                "Answer_score":0,
                "Answer_body":"@MaciejSkorski-9112 Thanks for the question. Please raise an issue in the following link to check by product team.\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure tabulardataset wrongly loads parquet?; Content: below i give a concrete example where python api fails to correctly read parquet files. more precisely, the data gets displaced. it may clarify this issue where data could not be publicly shared posted by @kengowada-6837. setup: python 3.8 + -core=1.36.0 + -dataprep=2.26.0 + pyarrow=7.0.0 the data is attached182488-error.log the code demonstrating the issue is given below. it uses an input data to create a table of strings along with some none values, stores in parquet format, and reads either directly or through tabulardataset. from .core import workspace, dataset import tempfile import pandas as pd import hashlib # prepare data: list of hashs with some none values df = pd.read_csv(\"error.log\") mask = df.isna().any(1) df.id = df.id.map(lambda s:hashlib.sha512(str(s).encode()).hexdigest() if s else none) df.loc[mask,'id'] = none # configure azure storage ws = workspace.from_config() dstore = ws.datastores.get('my_datastore') dstore_path = 'my_path' target = (dstore,dstore_path) # write to azure storage with tempfile.temporarydirectory() as tmpdir: df.to_parquet(f'{tmpdir}\/df.parquet') ds=dataset.file.upload_directory(tmpdir,target,overwrite=true) # read by two ways: download and open in pandas or use the azure connector with tempfile.temporarydirectory() as tmpdir: ds=dataset.file.from_files(target) ds.download(tmpdir) df1 = pd.read_parquet(tmpdir) ds = dataset.tabular.from_parquet_files(target) df2 = ds.to_pandas_dataframe() # comparison fails, the data seems displaced :-( pd.testing.assert_frame_equal(df1,df2) fwd: @ramr-msft",
        "Question_original_content_gpt_summary":"The user encountered challenges with Azure TabularDataset wrongly loading parquet files, resulting in data displacement.",
        "Question_preprocessed_content":"Title: azure tabulardataset wrongly loads parquet?; Content: below i give a concrete example where python api fails to correctly read parquet files. more precisely, the data gets displaced. it may clarify this issue where data could not be publicly shared posted by setup python + + + the data is the code demonstrating the issue is given below. it uses an input data to create a table of strings along with some none values, stores in parquet format, and reads either directly or through tabulardataset. from import workspace, dataset import tempfile import pandas as pd import hashlib prepare data list of hashs with some none values df mask if s else none none configure azure storage ws dstore target write to azure storage with as tmpdir read by two ways download and open in pandas or use the azure connector with as tmpdir df ds df comparison fails, the data seems displaced fwd",
        "Answer_original_content":"@maciejskorski-9112 thanks for the question. please raise an issue in the following link to check by product team. https:\/\/github.com\/azure\/machinelearningnotebooks\/issues",
        "Answer_original_content_gpt_summary":"Possible solution: The answer suggests raising an issue with the Azure product team through a GitHub link provided. This could potentially lead to a resolution of the challenge faced by the user with Azure TabularDataset wrongly loading parquet files and resulting in data displacement.",
        "Answer_preprocessed_content":"thanks for the question. please raise an issue in the following link to check by product team."
    },
    {
        "Question_id":38077884.0,
        "Question_title":"Can Azure calculate confidence interval for regressions?",
        "Question_body":"<p>I plan to try different regression methods provided by Azure ML Studio to predict numeric values. I wonder if it is possible to get the predictions together with corresponding confidence intervals. In other words, I would like the regression function to tell me not only the expected value (prediction) but also how confident it (the model) is about this value. Does Azure regression support this functionality?<\/p>\n\n<p><strong>ADDED<\/strong><\/p>\n\n<p>A related question. Can build in \"regressors\" estimate probability density functions? For example for a given case (a row in a data table) I would like to have not only a single number as a prediction (expected value) but also probabilities of all possible values.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1467121249227,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":332.0,
        "Owner_creation_time":1262870449816,
        "Owner_last_access_time":1652799411040,
        "Owner_reputation":116085.0,
        "Owner_up_votes":820.0,
        "Owner_down_votes":37.0,
        "Owner_views":4661.0,
        "Answer_body":"<p>Currently, you will have to use R or python within Azure ML for confidence interval <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1467268176063,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":1467123550236,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38077884",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can azure calculate confidence interval for regressions?; Content: i plan to try different regression methods provided by studio to predict numeric values. i wonder if it is possible to get the predictions together with corresponding confidence intervals. in other words, i would like the regression function to tell me not only the expected value (prediction) but also how confident it (the model) is about this value. does azure regression support this functionality? added a related question. can build in \"regressors\" estimate probability density functions? for example for a given case (a row in a data table) i would like to have not only a single number as a prediction (expected value) but also probabilities of all possible values.",
        "Question_original_content_gpt_summary":"The user is wondering if Azure regression can provide predictions with corresponding confidence intervals and estimate probability density functions.",
        "Question_preprocessed_content":"Title: can azure calculate confidence interval for regressions?; Content: i plan to try different regression methods provided by studio to predict numeric values. i wonder if it is possible to get the predictions together with corresponding confidence intervals. in other words, i would like the regression function to tell me not only the expected value but also how confident it is about this value. does azure regression support this functionality? added a related question. can build in regressors estimate probability density functions? for example for a given case i would like to have not only a single number as a prediction but also probabilities of all possible values.",
        "Answer_original_content":"currently, you will have to use r or python within for confidence interval",
        "Answer_original_content_gpt_summary":"The answer suggests that currently, to obtain confidence intervals for predictions and estimate probability density functions in Azure regression, the user will have to use R or Python.",
        "Answer_preprocessed_content":"currently, you will have to use r or python within for confidence interval"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker GroundTruth Interface - option to skip a task and then return",
        "Question_body":"Customer wants to configure the SageMaker Ground Truth interface seen by the workers such that the labeler can navigate to previous or next tasks. For example, if one is labelling images, they could skip the current image, label the next one, and then return to the skipped image. The Ground Truth interface does not seem to have this capability. Is there an option for it that I missed? I could not find anything about it here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-labeling.html.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1596055479000,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":68.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU_S8ylg4UQdKh76o3zp3dWQ\/sage-maker-ground-truth-interface-option-to-skip-a-task-and-then-return",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-29T21:28:39.000Z",
                "Answer_score":0,
                "Answer_body":"Currently, there is no functionality to skip a task and go back to it later. However, you could add a field like\n\n[ ] this task was skipped\n\nwhere the annotator could check the box for those items to be reviewed and processed at another time.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: groundtruth interface - option to skip a task and then return; Content: customer wants to configure the ground truth interface seen by the workers such that the labeler can navigate to previous or next tasks. for example, if one is labelling images, they could skip the current image, label the next one, and then return to the skipped image. the ground truth interface does not seem to have this capability. is there an option for it that i missed? i could not find anything about it here: https:\/\/docs.aws.amazon.com\/\/latest\/dg\/sms-data-labeling.html.",
        "Question_original_content_gpt_summary":"The user is looking for a way to configure the ground truth interface so that labelers can skip a task and then return to it.",
        "Question_preprocessed_content":"Title: groundtruth interface option to skip a task and then return; Content: customer wants to configure the ground truth interface seen by the workers such that the labeler can navigate to previous or next tasks. for example, if one is labelling images, they could skip the current image, label the next one, and then return to the skipped image. the ground truth interface does not seem to have this capability. is there an option for it that i missed? i could not find anything about it here",
        "Answer_original_content":"currently, there is no functionality to skip a task and go back to it later. however, you could add a field like [ ] this task was skipped where the annotator could check the box for those items to be reviewed and processed at another time.",
        "Answer_original_content_gpt_summary":"Possible solution: Add a checkbox field labeled \"This task was skipped\" for annotators to mark items that need to be reviewed and processed at a later time. However, there is currently no built-in functionality to skip a task and return to it later in the ground truth interface.",
        "Answer_preprocessed_content":"currently, there is no functionality to skip a task and go back to it later. however, you could add a field like this task was skipped where the annotator could check the box for those items to be reviewed and processed at another time."
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Designer error: Dataset initialization failed",
        "Question_body":"Getting following error when submiting Azure ML designer pipeline, following azure tutorial explore-data\n\n\n\n\nError msg:\nDataset initialization failed: Waiting for mount point to be ready has timed out. Check if fuse device is available on your system.\nIf mounting on remote targets, see https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-train-with-datasets#mount-files-to-remote-compute-targets",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1606556238047,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/178715\/azure-ml-designer-error-dataset-initialization-fai.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-30T08:48:28.543Z",
                "Answer_score":0,
                "Answer_body":"I had this problem only in us east region. I continue study in another region for now.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: designer error: dataset initialization failed; Content: getting following error when submiting designer pipeline, following azure tutorial explore-data error msg: dataset initialization failed: waiting for mount point to be ready has timed out. check if fuse device is available on your system. if mounting on remote targets, see https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-train-with-datasets#mount-files-to-remote-compute-targets",
        "Question_original_content_gpt_summary":"The user encountered an error when submitting a designer pipeline, with an Azure tutorial explore-data error message indicating that dataset initialization failed due to a timeout waiting for a mount point to be ready.",
        "Question_preprocessed_content":"Title: designer error dataset initialization failed; Content: getting following error when submiting designer pipeline, following azure tutorial error msg dataset initialization failed waiting for mount point to be ready has timed out. check if fuse device is available on your system. if mounting on remote targets, see",
        "Answer_original_content":"i had this problem only in us east region. i continue study in another region for now.",
        "Answer_original_content_gpt_summary":"Possible solutions extracted from the answer are:\n- Study in another region for now.\n- Avoid using the us east region.",
        "Answer_preprocessed_content":"i had this problem only in us east region. i continue study in another region for now."
    },
    {
        "Question_id":null,
        "Question_title":"Sweep command",
        "Question_body":"<p>Hi,<br>\nI am trying to create a sweep YAML config but the hp are not passed as cli args to my script.<\/p>\n<p>The yaml:<\/p>\n<pre data-code-wrap=\"yaml\"><code class=\"lang-nohighlight\">\nprogram: src.stance_detection.bert.simpletransformers.sweep_tests\nmethod: grid\nmetric:\n  goal: minimize\n  name: loss\nparameters:\n  n:\n    values: [1, 2]\ncommand:\n  - ${env}\n  - ${interpreter}\n  - \"-m\"\n  - ${program}\n  - ${args_no_boolean_flags}\n<\/code><\/pre>\n<p>The script:<\/p>\n<pre><code class=\"lang-python\">import wandb\nfrom pprint import pprint\nimport sys\n\nif __name__ == \"__main__\":\n    pprint(sys.argv)\n    wandb.init()\n    wandb.log({\"loss\": 0})\n<\/code><\/pre>\n<p>I was expecting <code>sys.argv<\/code> to contain <code>--n=[value]<\/code>. What am I doing wrong?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1657793218155,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":142.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-command\/2749",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-14T14:41:22.428Z",
                "Answer_body":"<p>It seems to have been a bug in wandb. Updating to <code>0.12.21 <\/code> fixed it.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-15T19:01:19.174Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/maxreimer\">@maxreimer<\/a> ,  glad you were able to successfully resolve this. Please do reach back out again with any additional questions.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-13T19:02:04.327Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sweep command; Content: hi, i am trying to create a sweep yaml config but the hp are not passed as cli args to my script. the yaml: program: src.stance_detection.bert.simpletransformers.sweep_tests method: grid metric: goal: minimize name: loss parameters: n: values: [1, 2] command: - ${env} - ${interpreter} - \"-m\" - ${program} - ${args_no_boolean_flags} the script: import from pprint import pprint import sys if __name__ == \"__main__\": pprint(sys.argv) .init() .log({\"loss\": 0}) i was expecting sys.argv to contain --n=[value]. what am i doing wrong?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with creating a sweep yaml config, as the hyperparameters are not being passed as CLI args to their script.",
        "Question_preprocessed_content":"Title: sweep command; Content: hi, i am trying to create a sweep yaml config but the hp are not passed as cli args to my script. the yaml the script i was expecting to contain . what am i doing wrong?",
        "Answer_original_content":"it seems to have been a bug in . updating to 0.12.21 fixed it. hi @maxreimer , glad you were able to successfully resolve this. please do reach back out again with any additional questions. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"Solution: Updating to version 0.12.21 seems to have fixed the bug that was causing the hyperparameters to not be passed as CLI args to the script.",
        "Answer_preprocessed_content":"it seems to have been a bug in . updating to fixed it. hi , glad you were able to successfully resolve this. please do reach back out again with any additional questions. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"[fixed] Cannot use Sweeps - metric value [null]",
        "Question_body":"<p>Hi! <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>I am using <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/boosting\/Using_W%26B_Sweeps_with_XGBoost.ipynb#scrollTo=VCRlDRL6_5aA\" rel=\"noopener nofollow ugc\">this notebook<\/a> as a tutorial to log sweeps into <a href=\"https:\/\/wandb.ai\/andrada\/AI4Code?workspace=user-andrada\">my Dashboard<\/a>.<\/p>\n<p>Everything works very well besides the Metric, which loggs with the value Null (<a href=\"https:\/\/wandb.ai\/andrada\/AI4Code\/sweeps\/6831zeoz?workspace=user-andrada\">see this run for details<\/a>)<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ed36db8c7cb03fbacf376fc1167163c09034ba2.png\" data-download-href=\"\/uploads\/short-url\/8XMAnSbrBXqOrZuGJcQDg6tQrGq.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ed36db8c7cb03fbacf376fc1167163c09034ba2_2_690x249.png\" alt=\"image\" data-base62-sha1=\"8XMAnSbrBXqOrZuGJcQDg6tQrGq\" width=\"690\" height=\"249\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ed36db8c7cb03fbacf376fc1167163c09034ba2_2_690x249.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ed36db8c7cb03fbacf376fc1167163c09034ba2_2_1035x373.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ed36db8c7cb03fbacf376fc1167163c09034ba2_2_1380x498.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ed36db8c7cb03fbacf376fc1167163c09034ba2_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1471\u00d7532 98.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>My process is as follows:<\/p>\n<p><strong>Create a train function<\/strong><\/p>\n<pre><code class=\"lang-auto\">def train_XGBRanker(config_defaults):\n    \n    # \ud83d\udc1d W&amp;B Experiment\n    config_defaults.update(CONFIG)\n    run = wandb.init(project='AI4Code', name='xgbRanker', config=config_defaults)\n    config = wandb.config\n    \n    # Initiate the model\n    model = XGBRanker(tree_method = config.tree_method,\n                      booster=config.booster,\n                      objective=config.objective,\n                      random_state=config.random_state, \n                      learning_rate=config.learning_rate,\n                      colsample_bytree=config.colsample_bytree, \n                      eta=config.eta, \n                      max_depth=config.max_depth, \n                      n_estimators=config.n_estimators, \n                      subsample=config.subsample,\n                      min_child_weight=config.min_child_weight)\n\n    # Train the model\n    model.fit(X_train, y_train, group=groups, verbose=True)\n\n    # Create df containing the cell_id and the prediction\n    predict = pd.DataFrame({\"cell_id\" : df_valid[\"cell_id\"],\n                            \"pred\" : model.predict(X_valid)}, index = df_valid.index)\n\n    # Sort (using the predicted rank) and then group\n    predict = predict.sort_values(by = ['id', 'pred'], ascending = [False, True])\\\n                        .groupby('id')['cell_id'].apply(list)\n\n    # Create the same but for actual data\n    actual = df_valid.sort_values(by = ['id', 'rank'], ascending = [False, True])\\\n                            .groupby('id')['cell_id'].apply(list)\n\n    # Kendall Metric\n    metric = kendall_tau(actual, predict)\n    print(clr.S+\"Kendall Tau\"+clr.E, metric)\n    wandb.log({\"kendall_tau\": np.float(metric)})\n<\/code><\/pre>\n<p><strong>try a first baseline experiment<\/strong><\/p>\n<pre><code class=\"lang-auto\">config_defaults = {\"tree_method\":'hist',\n                   \"booster\":'gbtree',\n                   \"objective\":'rank:pairwise',\n                   \"random_state\":24, \n                   \"learning_rate\":0.1,\n                   \"colsample_bytree\":0.9, \n                   \"eta\":0.05, \n                   \"max_depth\":6, \n                   \"n_estimators\":110, \n                   \"subsample\":0.75,\n                   \"min_child_weight\":10}\n\ntrain_XGBRanker(config_defaults)\n<\/code><\/pre>\n<p>which returns a <code>kendall_tau<\/code> of 0.5479588742699661 (so the metric isn\u2019t null).<\/p>\n<p><strong>and then I am running the Sweeps as follows:<\/strong><\/p>\n<pre><code class=\"lang-auto\"># Sweep Config\nsweep_config = {\n    \"method\": \"random\", # grid for all\n    \"metric\": {\n      \"name\": \"kendall_tau\",\n      \"goal\": \"maximize\"   \n    },\n    \"parameters\": {\n        \"booster\": {\n            \"values\": [\"gbtree\",\"gblinear\"]\n        },\n        \"max_depth\": {\n            \"values\": [3, 6, 9, 12]\n        },\n        \"learning_rate\": {\n            \"values\": [0.1, 0.05, 0.2]\n        },\n        \"subsample\": {\n            \"values\": [1, 0.5, 0.3]\n        }\n    }\n}\n\n# Sweep ID\nsweep_id = wandb.sweep(sweep_config, project=\"AI4Code\")\n\n# \ud83d\udc1d RUN SWEEPS\nconfig_defaults = {\"tree_method\":'hist',\n                   \"booster\":'gbtree',\n                   \"objective\":'rank:pairwise',\n                   \"random_state\":24, \n                   \"learning_rate\":0.1,\n                   \"colsample_bytree\":0.9, \n                   \"eta\":0.05, \n                   \"max_depth\":6, \n                   \"n_estimators\":110, \n                   \"subsample\":0.75,\n                   \"min_child_weight\":10}\n\n# count = the number of trials to run\nwandb.agent(sweep_id, train_XGBRanker(config_defaults), count=8)\n<\/code><\/pre>\n<p>Could you please advise? I don\u2019t know if this is related, but I also can\u2019t run the sweeps for more than a <code>count=5<\/code> as I get the following error:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/88e725be8086de062b815ebe6c25710b0ba23b89.png\" data-download-href=\"\/uploads\/short-url\/jx6amgBaQRJiD3N6IIcvKyT4sNH.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/88e725be8086de062b815ebe6c25710b0ba23b89.png\" alt=\"image\" data-base62-sha1=\"jx6amgBaQRJiD3N6IIcvKyT4sNH\" width=\"690\" height=\"487\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/88e725be8086de062b815ebe6c25710b0ba23b89_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">922\u00d7652 21.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Thank you lots!<br>\nAndrada<\/p>\n<p>UPDATE:<\/p>\n<p>The issue was from the fact that I was having arguments within the <code>train_XGBRanker()<\/code> - moving <code>config_defaults<\/code> from outside the function to in the function an then passing it to <code>wandb.agent(sweep_id, train_XGBRanker, count=20)<\/code> did the job.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1654710108211,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":244.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/fixed-cannot-use-sweeps-metric-value-null\/2576",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-09T19:31:59.719Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/andrada\">@andrada<\/a> ,<\/p>\n<p>Thank you for writing in with your question and providing an update. I\u2019m glad you were able to successfully resolve this issue. Please do reach back out again if you run into any other issues.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_score":11.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-08T19:32:48.769Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: [fixed] cannot use sweeps - metric value [null]; Content: hi! i am using this notebook as a tutorial to log sweeps into my dashboard. everything works very well besides the metric, which loggs with the value null (see this run for details) image1471\u00d7532 98.8 kb my process is as follows: create a train function def train_xgbranker(config_defaults): # \ud83d\udc1d w&b experiment config_defaults.update(config) run = .init(project='ai4code', name='xgbranker', config=config_defaults) config = .config # initiate the model model = xgbranker(tree_method = config.tree_method, booster=config.booster, objective=config.objective, random_state=config.random_state, learning_rate=config.learning_rate, colsample_bytree=config.colsample_bytree, eta=config.eta, max_depth=config.max_depth, n_estimators=config.n_estimators, subsample=config.subsample, min_child_weight=config.min_child_weight) # train the model model.fit(x_train, y_train, group=groups, verbose=true) # create df containing the cell_id and the prediction predict = pd.dataframe({\"cell_id\" : df_valid[\"cell_id\"], \"pred\" : model.predict(x_valid)}, index = df_valid.index) # sort (using the predicted rank) and then group predict = predict.sort_values(by = ['id', 'pred'], ascending = [false, true])\\ .groupby('id')['cell_id'].apply(list) # create the same but for actual data actual = df_valid.sort_values(by = ['id', 'rank'], ascending = [false, true])\\ .groupby('id')['cell_id'].apply(list) # kendall metric metric = kendall_tau(actual, predict) print(clr.s+\"kendall tau\"+clr.e, metric) .log({\"kendall_tau\": np.float(metric)}) try a first baseline experiment config_defaults = {\"tree_method\":'hist', \"booster\":'gbtree', \"objective\":'rank:pairwise', \"random_state\":24, \"learning_rate\":0.1, \"colsample_bytree\":0.9, \"eta\":0.05, \"max_depth\":6, \"n_estimators\":110, \"subsample\":0.75, \"min_child_weight\":10} train_xgbranker(config_defaults) which returns a kendall_tau of 0.5479588742699661 (so the metric isn\u2019t null). and then i am running the sweeps as follows: # sweep config sweep_config = { \"method\": \"random\", # grid for all \"metric\": { \"name\": \"kendall_tau\", \"goal\": \"maximize\" }, \"parameters\": { \"booster\": { \"values\": [\"gbtree\",\"gblinear\"] }, \"max_depth\": { \"values\": [3, 6, 9, 12] }, \"learning_rate\": { \"values\": [0.1, 0.05, 0.2] }, \"subsample\": { \"values\": [1, 0.5, 0.3] } } } # sweep id sweep_id = .sweep(sweep_config, project=\"ai4code\") # \ud83d\udc1d run sweeps config_defaults = {\"tree_method\":'hist', \"booster\":'gbtree', \"objective\":'rank:pairwise', \"random_state\":24, \"learning_rate\":0.1, \"colsample_bytree\":0.9, \"eta\":0.05, \"max_depth\":6, \"n_estimators\":110, \"subsample\":0.75, \"min_child_weight\":10} # count = the number of trials to run .agent(sweep_id, train_xgbranker(config_defaults), count=8) could you please advise? i don\u2019t know if this is related, but i also can\u2019t run the sweeps for more than a count=5 as i get the following error: image922\u00d7652 21.9 kb thank you lots! andrada update: the issue was from the fact that i was having arguments within the train_xgbranker() - moving config_defaults from outside the function to in the function an then passing it to .agent(sweep_id, train_xgbranker, count=20) did the job.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with using sweeps to log metric values into their dashboard, as the metric value is logging as null despite the user's attempts to troubleshoot the issue.",
        "Question_preprocessed_content":"Title: cannot use sweeps metric value ; Content: hi! i am using this notebook as a tutorial to log sweeps into my dashboard. everything works very well besides the metric, which loggs with the value null image kb my process is as follows create a train function try a first baseline experiment which returns a of . and then i am running the sweeps as follows could you please advise? i dont know if this is related, but i also cant run the sweeps for more than a as i get the following error image kb thank you lots! andrada update the issue was from the fact that i was having arguments within the moving from outside the function to in the function an then passing it to did the job.",
        "Answer_original_content":"hi @andrada , thank you for writing in with your question and providing an update. im glad you were able to successfully resolve this issue. please do reach back out again if you run into any other issues. regards, mohammad this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"There are no solutions provided in the answer as it is a generic response thanking the user for their question and indicating that the topic is closed.",
        "Answer_preprocessed_content":"hi , thank you for writing in with your question and providing an update. im glad you were able to successfully resolve this issue. please do reach back out again if you run into any other issues. regards, mohammad this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"xgboost sagemaker batch transform job output in multiple lines",
        "Question_body":"Hello,\n\nI've just trained a churn prediction model with XGBoost algorithm, based on the SageMaker example notebooks. I've created SageMaker batch transformation jobs using this model using input from CSV file with multiple records, however the output file is a single record CSV containing all the inferences in a single comma separated row. The result is that I'm not able to use the \"Join source\" feature with \"Input - Merge input data with job output\" since the input and output files must match the number of records. I've tried with different batch job configurations but I always get the same single line output file.\n\nDo you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? Is this a restriction from the XGBoost algorithm built-in implementation?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1599771185000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":199.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUYz7Bz_5sTmG0uBaqlt7J_g\/xgboost-sagemaker-batch-transform-job-output-in-multiple-lines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-11T02:38:30.000Z",
                "Answer_score":0,
                "Answer_body":"Sounds like a configuration issue, this algorithm should be able to output proper output CSVs.\n\nAre you using accept=\"text\/csv\" and assemble_with=\"Line\" on your Transformer? Is your strategy set to SingleRecord or MultiRecord?\n\nAnd split_type=\"Line\", content_type=\"text\/csv\" on the .transform() call?\n\nI have had custom algorithms accidentally output row vectors instead of column vectors for multi-record batches in the past (because they gave a 1D output which the default serializer interpreted as a row), but not built-in algorithms.\n\nDropping to SingleRecord could be a last resort (forcing Batch Transform itself to handle the serialization), but would decrease efficiency\/speed.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: xgboost batch transform job output in multiple lines; Content: hello, i've just trained a churn prediction model with xgboost algorithm, based on the example notebooks. i've created batch transformation jobs using this model using input from csv file with multiple records, however the output file is a single record csv containing all the inferences in a single comma separated row. the result is that i'm not able to use the \"join source\" feature with \"input - merge input data with job output\" since the input and output files must match the number of records. i've tried with different batch job configurations but i always get the same single line output file. do you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? is this a restriction from the xgboost algorithm built-in implementation?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with their xgboost batch transformation job output, which is being outputted as a single line in a CSV file, preventing them from using the \"join source\" feature to merge input and output data.",
        "Question_preprocessed_content":"Title: xgboost batch transform job output in multiple lines; Content: hello, i've just trained a churn prediction model with xgboost algorithm, based on the example notebooks. i've created batch transformation jobs using this model using input from csv file with multiple records, however the output file is a single record csv containing all the inferences in a single comma separated row. the result is that i'm not able to use the join source feature with input merge input data with job output since the input and output files must match the number of records. i've tried with different batch job configurations but i always get the same single line output file. do you know if is there any configuration that allows me to merge input and output in order to have a direct association between an input column with its inference result? is this a restriction from the xgboost algorithm implementation?",
        "Answer_original_content":"sounds like a configuration issue, this algorithm should be able to output proper output csvs. are you using accept=\"text\/csv\" and assemble_with=\"line\" on your transformer? is your strategy set to singlerecord or multirecord? and split_type=\"line\", content_type=\"text\/csv\" on the .transform() call? i have had custom algorithms accidentally output row vectors instead of column vectors for multi-record batches in the past (because they gave a 1d output which the default serializer interpreted as a row), but not built-in algorithms. dropping to singlerecord could be a last resort (forcing batch transform itself to handle the serialization), but would decrease efficiency\/speed.",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge with xgboost batch transformation job output include checking the configuration settings such as accept, assemble_with, strategy, split_type, and content_type. The user may need to ensure that the output is in the proper format, such as using column vectors for multi-record batches. If all else fails, the user may need to drop to singlerecord, but this could decrease efficiency and speed.",
        "Answer_preprocessed_content":"sounds like a configuration issue, this algorithm should be able to output proper output csvs. are you using and on your transformer? is your strategy set to singlerecord or multirecord? and on the .transform call? i have had custom algorithms accidentally output row vectors instead of column vectors for batches in the past , but not algorithms. dropping to singlerecord could be a last resort , but would decrease"
    },
    {
        "Question_id":56456463.0,
        "Question_title":"Unable to ignore .DS_Store files in DVC",
        "Question_body":"<p>I use DVC to track my media files. I use MacOS and I want\".DS_Store\" files to be ignored by DVC. According to DVC documentation I can achieve it with  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">.dvcignore<\/a>. I created <code>.dvcignore<\/code> file with \".DS_Store\" rule. However every time \".DS_Store\" is created <code>dvc status<\/code> still says that content has changed<\/p>\n\n<p>Here is the little test to reproduce my issue:<\/p>\n\n<pre><code>$ git init\n$ dvc init\n\n# create directory to store data\n# and track it's content with DVC\n$ mkdir data\n$ dvc add data\n\n# Ignore .DS_Store files created by MacOS\n$ echo \".DS_Store\" &gt; .dvcignore\n\n# create .DS_Store in data dir\n$ touch \"data\/.DS_Store\"\n<\/code><\/pre>\n\n<p>If I understand DVC documentation correctly then <code>dvc status<\/code> should print something like \"Pipeline is up to date. Nothing to reproduce\". However <code>dvc status<\/code> gives me:<\/p>\n\n<pre><code>data.dvc:\n        changed outs:\n                modified:           data\n<\/code><\/pre>\n\n<p>How I can really ignore \".DS_Store\" files?<\/p>\n\n<p><strong>UPDATE:<\/strong> The .dvcignore support noticeably improved in latest versions and the problem is no more relevant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2.0,
        "Question_creation_time":1559722020690,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":326.0,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_reputation":784.0,
        "Owner_up_votes":32.0,
        "Owner_down_votes":0.0,
        "Owner_views":77.0,
        "Answer_body":"<p>The current implementation of <code>.dvcignore<\/code> is very limited. Read more on it <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Please, mention that you are interested in this feature here - <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1876\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/issues\/1876<\/a>. That would help our team to prioritize issues properly.<\/p>\n\n<p>The possible workaround for now would be to use one of these approaches - <a href=\"https:\/\/stackoverflow.com\/questions\/18015978\/how-to-stop-creating-ds-store-on-mac\">How to stop creating .DS_Store on Mac?<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1559758177416,
        "Answer_score":3.0,
        "Owner_location":"Russia",
        "Question_last_edit_time":1568948355943,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56456463",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unable to ignore .ds_store files in ; Content: i use to track my media files. i use macos and i want\".ds_store\" files to be ignored by . according to documentation i can achieve it with .ignore. i created .ignore file with \".ds_store\" rule. however every time \".ds_store\" is created status still says that content has changed here is the little test to reproduce my issue: $ git init $ init # create directory to store data # and track it's content with $ mkdir data $ add data # ignore .ds_store files created by macos $ echo \".ds_store\" > .ignore # create .ds_store in data dir $ touch \"data\/.ds_store\" if i understand documentation correctly then status should print something like \"pipeline is up to date. nothing to reproduce\". however status gives me: data.: changed outs: modified: data how i can really ignore \".ds_store\" files? update: the .ignore support noticeably improved in latest versions and the problem is no more relevant.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with ignoring \".ds_store\" files in their media files tracking using , despite following the documentation's instructions.",
        "Question_preprocessed_content":"Title: unable to ignore files in ; Content: i use to track my media files. i use macos and i files to be ignored by . according to documentation i can achieve it with .ignore. i created file with rule. however every time is created still says that content has changed here is the little test to reproduce my issue if i understand documentation correctly then should print something like pipeline is up to date. nothing to reproduce . however gives me how i can really ignore files? update the .ignore support noticeably improved in latest versions and the problem is no more relevant.",
        "Answer_original_content":"the current implementation of .ignore is very limited. read more on it here. please, mention that you are interested in this feature here - https:\/\/github.com\/iterative\/\/issues\/1876. that would help our team to prioritize issues properly. the possible workaround for now would be to use one of these approaches - how to stop creating .ds_store on mac?",
        "Answer_original_content_gpt_summary":"Possible solutions to the challenge of ignoring \".ds_store\" files in media files tracking using the current implementation of .ignore are limited. The user can read more about it and express their interest in this feature on the provided link to help the team prioritize issues properly. For now, a possible workaround is to use one of the approaches suggested in the link on how to stop creating .ds_store on Mac.",
        "Answer_preprocessed_content":"the current implementation of is very limited. read more on it here. please, mention that you are interested in this feature here that would help our team to prioritize issues properly. the possible workaround for now would be to use one of these approaches how to stop creating on mac?"
    },
    {
        "Question_id":null,
        "Question_title":"adjusting sagemaker xgboost project to tensorflow (or even just different folder name)",
        "Question_body":"I have sagemaker xgboost project template \"build, train, deploy\" working, but I'd like to modify if to use tensorflow instead of xgboost. First up I was just trying to change the abalone folder to topic to reflect the data we are working with.\n\nI was experimenting with trying to change the topic\/pipeline.py file like so\n\n    image_uri = sagemaker.image_uris.retrieve(\n        framework=\"tensorflow\",\n        region=region,\n        version=\"1.0-1\",\n        py_version=\"py3\",\n        instance_type=training_instance_type,\n    )\n\n\ni.e. just changing the framework name from \"xgboost\" to \"tensorflow\", but then when I run the following from a notebook:\n\nfrom pipelines.topic.pipeline import get_pipeline\n\n\npipeline = get_pipeline(\n    region=region,\n    role=role,\n    default_bucket=default_bucket,\n    model_package_group_name=model_package_group_name,\n    pipeline_name=pipeline_name,\n)\n\n\nI get the following error\n\nValueError                                Traceback (most recent call last)\n<ipython-input-5-6343f00c3471> in <module>\n      7     default_bucket=default_bucket,\n      8     model_package_group_name=model_package_group_name,\n----> 9     pipeline_name=pipeline_name,\n     10 )\n\n~\/topic-models-no-monitoring-p-rboparx6tdeg\/sagemaker-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, sagemaker_project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type)\n    188         version=\"1.0-1\",\n    189         py_version=\"py3\",\n--> 190         instance_type=training_instance_type,\n    191     )\n    192     tf_train = Estimator(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/utilities.py in wrapper(*args, **kwargs)\n    197                 logger.warning(warning_msg_template, arg_name, func_name, type(value))\n    198                 kwargs[arg_name] = value.default_value\n--> 199         return func(*args, **kwargs)\n    200 \n    201     return wrapper\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in retrieve(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config)\n    152             if inference_tool == \"neuron\":\n    153                 _framework = f\"{framework}-{inference_tool}\"\n--> 154         config = _config_for_framework_and_scope(_framework, image_scope, accelerator_type)\n    155 \n    156     original_version = version\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _config_for_framework_and_scope(framework, image_scope, accelerator_type)\n    277         image_scope = available_scopes[0]\n    278 \n--> 279     _validate_arg(image_scope, available_scopes, \"image scope\")\n    280     return config if \"scope\" in config else config[image_scope]\n    281 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/image_uris.py in _validate_arg(arg, available_options, arg_name)\n    443             \"Unsupported {arg_name}: {arg}. You may need to upgrade your SDK version \"\n    444             \"(pip install -U sagemaker) for newer {arg_name}s. Supported {arg_name}(s): \"\n--> 445             \"{options}.\".format(arg_name=arg_name, arg=arg, options=\", \".join(available_options))\n    446         )\n    447 \n\nValueError: Unsupported image scope: None. You may need to upgrade your SDK version (pip install -U sagemaker) for newer image scopes. Supported image scope(s): eia, inference, training.\n\n\nI was skeptical that the upgrade suggested by the error message would fix this, but gave it a try:\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npipelines 0.0.1 requires sagemaker==2.93.0, but you have sagemaker 2.110.0 which is incompatible.\n\n\nSo that seems like I can't upgrade sagemaker without changing pipelines, and it's not clear that's the right thing to do - like this project template may be all designed around those particular ealier libraries.\n\nBut so is it that the \"framework\" name should be different, e.g. \"tf\"? Or is there some other setting that needs changing in order to allow me to get a tensorflow pipeline ...?\n\nHowever I find that if I use the existing abalone\/pipeline.py file I can change the framework to \"tensorflow\" and there's no problem running that particular step in the notebook.\n\nI've searched all the files in the project to try and find any dependency on the abalone folder name, and the closest I came was in codebuild-buildspec.yml but that hasn't helped.\n\nHas anyone else successfully changed the folder name from abalone to something else, or am I stuck with abalone if I want to make progress?\n\nMany thanks in advance\n\np.s. is there a slack community for sagemaker studio anywhere?\n\np.p.s. I have tried changing all instances of the term \"Abalone\" to \"Topic\" within the topic\/pipeline.py file (matching case as appropriate) to no avail\n\np.p.p.s. I discovered that I can get an error free run of getting the pipeline from a unit test:\n\nimport pytest\n\nfrom pipelines.topic.pipeline import *\n\nregion = 'eu-west-1'\nrole = 'arn:aws:iam::398371982844:role\/SageMakerExecutionRole'\ndefault_bucket = 'sagemaker-eu-west-1-398371982844'\nmodel_package_group_name = 'TopicModelPackageGroup-Example'\npipeline_name = 'TopicPipeline-Example'\n\ndef test_pipeline():\n    pipeline = get_pipeline(\n        region=region,\n        role=role,\n        default_bucket=default_bucket,\n        model_package_group_name=model_package_group_name,\n        pipeline_name=pipeline_name,\n    )\n\n\nand strangely if I go to a different copy of the notebook, everything runs fine, there ... so I have two seemingly identical ipynb notebooks, and in one of them when I switch to trying to get a topic pipeline I get the above error, and in the other, I get no error at all, very strange\n\np.p.p.p.s. I also notice that conda list returns very different results depending on whether I run it in the notebook or the terminal ... but the conda list results are identical for the two notebooks ...",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1664391665708,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":46.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUAL9Vn9abQ6KKCs2ASwwmzg\/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-29T11:24:21.266Z",
                "Answer_score":1,
                "Answer_body":"Hi! I see two parts in your question:\n\nHow to use Tensorflow in a SageMaker estimator to train and deploy a model\nHow to adapt a SageMaker MLOps template to your data and code\n\nTensorflow estimator is slightly different from XGBoost estimator, and the easiest way to work with it is not by using sagemaker.image_uris.retrieve(framework=\"tensorflow\",...), but to use sagemaker.tensorflow.TensorFlow estimator instead.\n\nThese are the two examples, which will be useful for you:\n\nTrain an MNIST model with TensorFlow\nDeploy a Trained TensorFlow V2 Model\n\nAs for updating the MLOps template, I recommend you to go through the comprehensive self-service lab on SageMaker Pipelines.\n\nIt shows you how to update the source directory from abalone to customer_churn. In your case it will be the topic.\n\nP. S. As for a Slack channel, to my best knowledge, this re:Post forum now is the best place to ask any questions on Amazon SageMaker, including SageMaker Studio.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: adjusting xgboost project to tensorflow (or even just different folder name); Content: i have xgboost project template \"build, train, deploy\" working, but i'd like to modify if to use tensorflow instead of xgboost. first up i was just trying to change the abalone folder to topic to reflect the data we are working with. i was experimenting with trying to change the topic\/pipeline.py file like so image_uri = .image_uris.retrieve( framework=\"tensorflow\", region=region, version=\"1.0-1\", py_version=\"py3\", instance_type=training_instance_type, ) i.e. just changing the framework name from \"xgboost\" to \"tensorflow\", but then when i run the following from a notebook: from pipelines.topic.pipeline import get_pipeline pipeline = get_pipeline( region=region, role=role, default_bucket=default_bucket, model_package_group_name=model_package_group_name, pipeline_name=pipeline_name, ) i get the following error valueerror traceback (most recent call last) in 7 default_bucket=default_bucket, 8 model_package_group_name=model_package_group_name, ----> 9 pipeline_name=pipeline_name, 10 ) ~\/topic-models-no-monitoring-p-rboparx6tdeg\/-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, _project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type) 188 version=\"1.0-1\", 189 py_version=\"py3\", --> 190 instance_type=training_instance_type, 191 ) 192 tf_train = estimator( \/opt\/conda\/lib\/python3.7\/site-packages\/\/workflow\/utilities.py in wrapper(*args, **kwargs) 197 logger.warning(warning_msg_template, arg_name, func_name, type(value)) 198 kwargs[arg_name] = value.default_value --> 199 return func(*args, **kwargs) 200 201 return wrapper \/opt\/conda\/lib\/python3.7\/site-packages\/\/image_uris.py in retrieve(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config) 152 if inference_tool == \"neuron\": 153 _framework = f\"{framework}-{inference_tool}\" --> 154 config = _config_for_framework_and_scope(_framework, image_scope, accelerator_type) 155 156 original_version = version \/opt\/conda\/lib\/python3.7\/site-packages\/\/image_uris.py in _config_for_framework_and_scope(framework, image_scope, accelerator_type) 277 image_scope = available_scopes[0] 278 --> 279 _validate_arg(image_scope, available_scopes, \"image scope\") 280 return config if \"scope\" in config else config[image_scope] 281 \/opt\/conda\/lib\/python3.7\/site-packages\/\/image_uris.py in _validate_arg(arg, available_options, arg_name) 443 \"unsupported {arg_name}: {arg}. you may need to upgrade your sdk version \" 444 \"(pip install -u ) for newer {arg_name}s. supported {arg_name}(s): \" --> 445 \"{options}.\".format(arg_name=arg_name, arg=arg, options=\", \".join(available_options)) 446 ) 447 valueerror: unsupported image scope: none. you may need to upgrade your sdk version (pip install -u ) for newer image scopes. supported image scope(s): eia, inference, training. i was skeptical that the upgrade suggested by the error message would fix this, but gave it a try: error: pip's dependency resolver does not currently take into account all the packages that are installed. this behaviour is the source of the following dependency conflicts. pipelines 0.0.1 requires ==2.93.0, but you have 2.110.0 which is incompatible. so that seems like i can't upgrade without changing pipelines, and it's not clear that's the right thing to do - like this project template may be all designed around those particular ealier libraries. but so is it that the \"framework\" name should be different, e.g. \"tf\"? or is there some other setting that needs changing in order to allow me to get a tensorflow pipeline ...? however i find that if i use the existing abalone\/pipeline.py file i can change the framework to \"tensorflow\" and there's no problem running that particular step in the notebook. i've searched all the files in the project to try and find any dependency on the abalone folder name, and the closest i came was in codebuild-buildspec.yml but that hasn't helped. has anyone else successfully changed the folder name from abalone to something else, or am i stuck with abalone if i want to make progress? many thanks in advance p.s. is there a slack community for studio anywhere? p.p.s. i have tried changing all instances of the term \"abalone\" to \"topic\" within the topic\/pipeline.py file (matching case as appropriate) to no avail p.p.p.s. i discovered that i can get an error free run of getting the pipeline from a unit test: import pytest from pipelines.topic.pipeline import * region = 'eu-west-1' role = 'arn:aws:iam::398371982844:role\/executionrole' default_bucket = '-eu-west-1-398371982844' model_package_group_name = 'topicmodelpackagegroup-example' pipeline_name = 'topicpipeline-example' def test_pipeline(): pipeline = get_pipeline( region=region, role=role, default_bucket=default_bucket, model_package_group_name=model_package_group_name, pipeline_name=pipeline_name, ) and strangely if i go to a different copy of the notebook, everything runs fine, there ... so i have two seemingly identical ipynb notebooks, and in one of them when i switch to trying to get a topic pipeline i get the above error, and in the other, i get no error at all, very strange p.p.p.p.s. i also notice that conda list returns very different results depending on whether i run it in the notebook or the terminal ... but the conda list results are identical for the two notebooks ...",
        "Question_original_content_gpt_summary":"The user is encountering challenges while attempting to adjust an existing xgboost project template to use TensorFlow instead, including issues with changing the folder name, upgrading the SDK version, and conflicting dependencies.",
        "Question_preprocessed_content":"Title: adjusting xgboost project to tensorflow ; Content: i have xgboost project template build, train, deploy working, but i'd like to modify if to use tensorflow instead of xgboost. first up i was just trying to change the abalone folder to topic to reflect the data we are working with. i was experimenting with trying to change the file like so framework tensorflow , region region, just changing the framework name from xgboost to tensorflow , but then when i run the following from a notebook from import pipeline region region, role role, i get the following error valueerror traceback in in role, estimator type return func return wrapper in retrieve if neuron config version in image scope return config if scope in config else in unsupported . you may need to upgrade your sdk version pip install u for newer s. supported arg arg, options , valueerror unsupported image scope none. you may need to upgrade your sdk version for newer image scopes. supported image scope eia, inference, training. i was skeptical that the upgrade suggested by the error message would fix this, but gave it a try error pip's dependency resolver does not currently take into account all the packages that are installed. this behaviour is the source of the following dependency conflicts. pipelines requires but you have which is incompatible. so that seems like i can't upgrade without changing pipelines, and it's not clear that's the right thing to do like this project template may be all designed around those particular ealier libraries. but so is it that the framework name should be different, tf ? or is there some other setting that needs changing in order to allow me to get a tensorflow pipeline however i find that if i use the existing file i can change the framework to tensorflow and there's no problem running that particular step in the notebook. i've searched all the files in the project to try and find any dependency on the abalone folder name, and the closest i came was in but that hasn't helped. has anyone else successfully changed the folder name from abalone to something else, or am i stuck with abalone if i want to make progress? many thanks in advance is there a slack community for studio anywhere? i have tried changing all instances of the term abalone to topic within the file to no avail i discovered that i can get an error free run of getting the pipeline from a unit test import pytest from import region role def pipeline region region, role role, and strangely if i go to a different copy of the notebook, everything runs fine, there so i have two seemingly identical ipynb notebooks, and in one of them when i switch to trying to get a topic pipeline i get the above error, and in the other, i get no error at all, very strange i also notice that conda list returns very different results depending on whether i run it in the notebook or the terminal but the conda list results are identical for the two notebooks",
        "Answer_original_content":"hi! i see two parts in your question: how to use tensorflow in a estimator to train and deploy a model how to adapt a mlops template to your data and code tensorflow estimator is slightly different from xgboost estimator, and the easiest way to work with it is not by using .image_uris.retrieve(framework=\"tensorflow\",...), but to use .tensorflow.tensorflow estimator instead. these are the two examples, which will be useful for you: train an mnist model with tensorflow deploy a trained tensorflow v2 model as for updating the mlops template, i recommend you to go through the comprehensive self-service lab on pipelines. it shows you how to update the source directory from abalone to customer_churn. in your case it will be the topic. p. s. as for a slack channel, to my best knowledge, this re:post forum now is the best place to ask any questions on , including studio.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer include using TensorFlow estimator instead of xgboost estimator, and following a self-service lab on pipelines to update the mlops template. The answer also provides two examples for training and deploying a TensorFlow model.",
        "Answer_preprocessed_content":"hi! i see two parts in your question how to use tensorflow in a estimator to train and deploy a model how to adapt a mlops template to your data and code tensorflow estimator is slightly different from xgboost estimator, and the easiest way to work with it is not by using but to use estimator instead. these are the two examples, which will be useful for you train an mnist model with tensorflow deploy a trained tensorflow v model as for updating the mlops template, i recommend you to go through the comprehensive lab on pipelines. it shows you how to update the source directory from abalone to in your case it will be the topic. p. s. as for a slack channel, to my best knowledge, this re post forum now is the best place to ask any questions on , including studio."
    },
    {
        "Question_id":null,
        "Question_title":"How do we create learning virtual machine AI assistants with smart home control and self-driving vehicle funtionality?",
        "Question_body":"I am developing Conscious Quantum Coding Living AI Virtual Assistants to help with everything.\n\nJodi, The AI Motor Home\nJodi will be an integrative, quantum coded, learning\/self-improving, online\/cloud, virtual machine, life conscious Living AI assistant who fully controls, and self drives, an RV\/Motor home\n\nHow would you create a Living AI assistant for a motor home?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1591266810177,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/31993\/how-do-we-create-learning-virtual-machine-ai-assis.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-04T23:06:32.373Z",
                "Answer_score":0,
                "Answer_body":"Thanks for reaching out. The Azure Bot Service may be useful for your scenario. I encourage you to check out our documentation on Virtual Assistant and Template Outline for best practices. There are also videos available to help you get started. Feel free to followup with any particular questions or concerns for the community to chime in. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":19.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do we create learning virtual machine ai assistants with smart home control and self-driving vehicle funtionality?; Content: i am developing conscious quantum coding living ai virtual assistants to help with everything. jodi, the ai motor home jodi will be an integrative, quantum coded, learning\/self-improving, online\/cloud, virtual machine, life conscious living ai assistant who fully controls, and self drives, an rv\/motor home how would you create a living ai assistant for a motor home?",
        "Question_original_content_gpt_summary":"The user is developing a conscious quantum-coded, learning and self-improving virtual machine AI assistant to help with everything, including controlling and self-driving an RV\/motor home.",
        "Question_preprocessed_content":"Title: how do we create learning virtual machine ai assistants with smart home control and vehicle funtionality?; Content: i am developing conscious quantum coding living ai virtual assistants to help with everything. jodi, the ai motor home jodi will be an integrative, quantum coded, virtual machine, life conscious living ai assistant who fully controls, and self drives, an home how would you create a living ai assistant for a motor home?",
        "Answer_original_content":"thanks for reaching out. the azure bot service may be useful for your scenario. i encourage you to check out our documentation on virtual assistant and template outline for best practices. there are also videos available to help you get started. feel free to followup with any particular questions or concerns for the community to chime in. thanks.",
        "Answer_original_content_gpt_summary":"Possible solutions from the answer are:\n\n- The Azure Bot Service may be useful for developing a virtual machine AI assistant.\n- Check out the documentation on virtual assistant and template outline for best practices.\n- Watch videos available to help get started.\n\nIn summary, the answer suggests using the Azure Bot Service and checking out the documentation and videos to develop a virtual machine AI assistant.",
        "Answer_preprocessed_content":"thanks for reaching out. the azure bot service may be useful for your scenario. i encourage you to check out our documentation on virtual assistant and template outline for best practices. there are also videos available to help you get started. feel free to followup with any particular questions or concerns for the community to chime in. thanks."
    },
    {
        "Question_id":null,
        "Question_title":"Registered for aws sagemaker studio but not able to create account",
        "Question_body":"Hi I received my aws sagemaker studio approval to create an account 1 hr ago When I go to the link to create an account it says my email has not been approved Even though I have an email to the contrary\n\nHow do I contact amazon to sort this out?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1652237658633,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":70.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSB0wO0OiQ_irUdCx9ppjbg\/registered-for-aws-sagemaker-studio-but-not-able-to-create-account",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-11T15:42:07.722Z",
                "Answer_score":0,
                "Answer_body":"Hi, If you requested a SageMaker Studio Lab free account, then please create an issue on this link.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: registered for studio but not able to create account; Content: hi i received my studio approval to create an account 1 hr ago when i go to the link to create an account it says my email has not been approved even though i have an email to the contrary how do i contact amazon to sort this out?",
        "Question_original_content_gpt_summary":"The user is unable to create an account for Amazon Studio despite having received approval to do so, and is seeking assistance to resolve the issue.",
        "Question_preprocessed_content":"Title: registered for studio but not able to create account; Content: hi i received my studio approval to create an account hr ago when i go to the link to create an account it says my email has not been approved even though i have an email to the contrary how do i contact amazon to sort this out?",
        "Answer_original_content":"hi, if you requested a studio lab free account, then please create an issue on this link.",
        "Answer_original_content_gpt_summary":"Possible solution: The user can create an issue on the provided link to resolve the issue of not being able to create an account for Amazon Studio despite receiving approval.",
        "Answer_preprocessed_content":"hi, if you requested a studio lab free account, then please create an issue on this link."
    },
    {
        "Question_id":null,
        "Question_title":"Add a remote directory without adding to the cache",
        "Question_body":"<p>We have a corpus licensed from a third party that is stored in our S3 bucket.  The corpus is fixed and should never change, so versioning is not much of a concern.  I\u2019d like to integrate it into my repository so that when someone checks it out from git, they can run a simple command like <code>dvc checkout<\/code> and it will download the corpus.  However, since it is already stored in S3 and should never change, I would prefer that it doesn\u2019t get copied into our remote cache.  But, I\u2019m not sure this is really possible with <code>dvc<\/code>.<\/p>\n<p>One solution might be to use something like <code>dvc run -n get-corpus -O [local-corpus-path] dvc get-url [remote-corpus-path] [local-corpus-path]<\/code>.  Then (I think) someone can just run <code>dvc repro get-corpus<\/code> or any downstream tasks, and dvc will get the corpus.  But, will this work?  And is this the best\/recommended way?<\/p>",
        "Question_answer_count":16,
        "Question_comment_count":null,
        "Question_creation_time":1595379583111,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":2148.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/add-a-remote-directory-without-adding-to-the-cache\/452",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-22T02:46:21.820Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nimrand\">@nimrand<\/a><\/p>\n<blockquote>\n<p>By <em>corpus<\/em>, I assume you mean a data set (probably related to NLP). Some sort of ground truth data source.<\/p>\n<\/blockquote>\n<p>Seems like you need a download tool? Would <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/s3\/cp.html\"><code>aws s3 cp<\/code><\/a> work for you? Installing <code>aws<\/code> CLI and even a shell script to wrap the exact download command could be integrated to your repo.<\/p>\n<p>Wrapping it with DVC is possible but I\u2019n not sure I understand the use case. Can you share more context if needed?<\/p>\n<p>Thanks<\/p>",
                "Answer_score":14.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T12:50:09.368Z",
                "Answer_body":"<p>The corpus is a large directory of text documents that are representative of various domains, from which we want to develop language models.<\/p>\n<p>The missing context, I guess, is that I\u2019m already using <code>dvc<\/code> for other projects, and I\u2019d like to standardize our repos on using <code>dvc<\/code> for large data dependencies in all our projects across the organization.  In previous projects, I used <code>dvc add [large data file]<\/code> and pushed to a remote cache on S3.  So, when a person checks out the repo, they just need to run <code>dvc pull<\/code> to get the data files they need to run the code.<\/p>\n<p>In this case, however, the large data file already exists in centralized location and should never change, so I\u2019d like to avoid making a copy of the data in the remote cache just for the sake of integrating it with <code>dvc<\/code>.  Ideally, when someone downloads the repo and does a <code>dvc pull<\/code> it would download the corpus rom the existing S3 source, but from reading the docs I\u2019m not sure this is possible.  The alternative I came up with was to wrap it in a task, as I proposed in my original post.<\/p>\n<p>As for <code>aws s3 cp<\/code>, is there any advantage to that rather than using <code>dvc get-url<\/code> if we can already assume that users of the repo have <code>dvc<\/code> installed?<\/p>",
                "Answer_score":18.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T13:28:02.520Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nimrand\">@nimrand<\/a>!<br>\nI think there is no way to not have this data and still be able to pull it, though I think there is workaround for your use case.<\/p>\n<p>What you can do is to create a stage that does not have a dependency (callback stage).<br>\nIn your case it would look like that:<br>\n<code>dvc run -n download_data --outs-no-cache data aws s3 cp s3:\/\/s3_data_path data<\/code><br>\nthat way we preserve the knowledge how to get the data, if you use <code>get-url<\/code>, dvc will not preserve information where did you get the data.<\/p>\n<p>Now there is one problem with this approach - callback stages are considered to be always changed, and if you later <code>dvc repro<\/code> stage depending on callback stage output, <code>dvc<\/code> will try to rerun this stage - copy the file all over.<br>\nTo prevent that you can use <code>dvc freeze download_data<\/code> - dvc wont try to rerun this stage until you <code>unfreeze<\/code> it.<br>\nSo then, when you share your project with frozen <code>dwonload_data<\/code> -  someone will get an error that <code>data<\/code> does not exist, but if he\/she <code>dvc unfreeze download_data &amp;&amp; dvc repro some_later_stage<\/code> - dvc will try to download and repro the process.<\/p>\n<p>To illustrate this, I created following example:<\/p>\n<pre><code class=\"lang-auto\">#!\/bin\/bash\n\n# data to import\nrm big_data\necho data &gt;&gt; big_data\nmain=$(pwd)\n\nrm -rf repo\nmkdir repo\n\nset -x\npushd repo\ngit init --quiet\ndvc init --quiet\n\n# download data\ndvc run -n download_data --outs-no-cache data \"cp $main\/big_data data\"\ndvc freeze download_data # so that dvc will not try to download each time repro is called\ndvc run -n process -d data -o result \"cat data &gt;&gt; result &amp;&amp; echo processed &gt;&gt; result\"\n\n#everything is present, no problem\ndvc repro process\n\n#remove data, repro fails\nrm data\ndvc repro process\n\n#unfreeze stage, be able to reproduce pipeline\ndvc unfreeze download_data\ndvc repro process\n<\/code><\/pre>",
                "Answer_score":18.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T14:36:08.632Z",
                "Answer_body":"<p>I\u2019m surprised that <code>dvc<\/code> would always try to rerun that task, even when the data it produces already exists locally.  Is this always the case when a stage depends on an <code>--outs-no-cache<\/code> of another stage?  You called it a \u201ccallback stage\u201d.  I\u2019m not familiar with that term.<\/p>\n<p>Also, how is <code>dvc run -n download_data --outs-no-cache data aws s3 cp s3:\/\/s3_data_path<\/code> different from <code>dvc run -n download_data --outs-no-cache data dvc get-url s3:\/\/s3_data_path data<\/code>.  Doesn\u2019t <code>aws s3 cp s3:\/\/s3_data_path<\/code> and  <code>dvc get-url s3:\/\/s3_data_path data<\/code> do the same thing?<\/p>",
                "Answer_score":13.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T15:16:46.703Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/nimrand\">@nimrand<\/a><\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"nimrand\" data-post=\"5\" data-topic=\"452\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/n\/5f8ce5\/40.png\" class=\"avatar\"> nimrand:<\/div>\n<blockquote>\n<p>Also, how is <code>dvc run -...<\/code><\/p>\n<\/blockquote>\n<\/aside>\n<p>Well it is possible to do that too, I missed this possibility. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>So to answer your first question:<\/p>\n<ul>\n<li>\n<code>callback stage<\/code>: stage that does not have dependencies<\/li>\n<\/ul>\n<p>dvc will always rerun <code>callback stages<\/code>.<br>\n<code>--outs-no-cache<\/code> does not matter in this case.<br>\nThe reason for this behavior is that <code>callback stages<\/code> have been introduced to let user execute code that verifies whether something changed and let that information into the pipeline.<br>\nExample: checking if there are new entries in log storage to trigger importing and processing them.<\/p>\n<p>As to your project: you might also just use <code>data<\/code> as a dependency in your first pipeline. Dependencies are not added under DVC control (outputs do), so it won\u2019t go into the cache. Upon reproducing pipeline with no <code>data<\/code> you will get information that it does not exist.<\/p>",
                "Answer_score":33.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T16:00:45.458Z",
                "Answer_body":"<p>Interesting ideas about using a stage with no dependencies, or an <a href=\"https:\/\/dvc.org\/doc\/user-guide\/external-dependencies\">external dependency<\/a> in a 2-stage pipeline Pawel. But if the goal is to download the data with <code>dvc pull<\/code> like in the other repos <a class=\"mention\" href=\"\/u\/nimrand\">@nimrand<\/a> has, that wouldn\u2019t help, as <code>dvc repro<\/code> is needed instead instead.<\/p>\n<p>The problem is that for <code>dvc pull<\/code> to work, the data would need to be pushed first, meaning added manually to the workspace, tracked by DVC (<code>dvc add<\/code> for example), and <code>dvc push<\/code>ed to some remote storage. Another way to put in the workspace and track it in a single step would be <code>dvc import-url<\/code>. But both these methods duplicate the remote storage of the data set.<\/p>\n<p>Another possible workaround is to add the data to the project without moving it, as an <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\">external output<\/a>. This implies setting up an external cache in the S3 location first, and then do something like <code>dvc add s3:\/\/s3_data_path<\/code>. I\u2019m not sure what happens on the S3 at that point though, the data may be duplicated anyway <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/confused.png?v=9\" title=\":confused:\" class=\"emoji\" alt=\":confused:\"> and <code>dvc pull<\/code> still wouldn\u2019t download it to the workspace, as it\u2019s added externally (never pushed in the first place).<\/p>",
                "Answer_score":7.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T16:21:31.092Z",
                "Answer_body":"<blockquote>\n<p>dvc will always rerun  <code>callback stages<\/code> .<br>\n<code>--outs-no-cache<\/code>  does not matter in this case.<br>\nThe reason for this behavior is that  <code>callback stages<\/code>  have been introduced to let user execute code that verifies whether something changed and let that information into the pipeline.<br>\nExample: checking if there are new entries in log storage to trigger importing and processing them.<\/p>\n<\/blockquote>\n<p>I would expect the default would be to not rerun stages that don\u2019t have dependencies, and that <code>callback stage<\/code> could be made by using <code>--always-changed<\/code>.  I suppose I could add a dummy dependency to the <code>get-corpus<\/code> task, but that\u2019s a bit clunky.<\/p>\n<p>It seems that my use-case wont work with <code>dvc pull<\/code> without duplicating the data, but maybe making it work via a <code>dvc repro<\/code> is the next best thing.<\/p>",
                "Answer_score":32.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T17:16:04.412Z",
                "Answer_body":"<p>OK. And like I mentioned, please consider <a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url\">https:\/\/dvc.org\/doc\/command-reference\/import-url<\/a> as another alternative. It will download the data and create a .dvc file for it, as if it was tracked with <code>dvc add<\/code>, so you can use it as a dependency for further stages and have repro download it automatically if\/when needed.<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T18:19:53.356Z",
                "Answer_body":"<p>Just re-read that page\u2026<\/p>\n<p>In it, they give an example that says <code>import-url<\/code> is equivalent to<\/p>\n<p><code>dvc run -n download_data           -d https:\/\/data.dvc.org\/get-started\/data.xml           -o data.xml           wget https:\/\/data.dvc.org\/get-started\/data.xml -O data.xml<\/code><\/p>\n<p>I think that\u2019s <em>almost<\/em> what I want, except we\u2019d change the first <code>-o<\/code> parameter to <code>-O<\/code> to avoid copying the file into the local cache.<\/p>\n<p>But, since it declares a dependency on a remote location, how does it know when its changed other than re-downloading the files every time?<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T19:14:14.413Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"nimrand\" data-post=\"10\" data-topic=\"452\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/n\/5f8ce5\/40.png\" class=\"avatar\"> nimrand:<\/div>\n<blockquote>\n<p>I think that\u2019s <em>almost<\/em> what I want, except we\u2019d change the first <code>-o<\/code> parameter to <code>-O<\/code> to avoid copying the file into the local cache.<\/p>\n<\/blockquote>\n<\/aside>\n<p>This is not such a big of a deal really. <code>import-url<\/code> will download it into the cache directly, and then link the file to the workspace (as long as it\u2019s supported by the file system, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">this doc<\/a> for more info.), so there\u2019s no duplication. <code>-O<\/code> is meant more for small files you want to track with Git or experimental files you don\u2019t want to track at all.<\/p>\n<aside class=\"quote no-group\" data-username=\"nimrand\" data-post=\"10\" data-topic=\"452\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/n\/5f8ce5\/40.png\" class=\"avatar\"> nimrand:<\/div>\n<blockquote>\n<p>But, since it declares a dependency on a remote location, how does it know when its changed other than re-downloading the files every time?<\/p>\n<\/blockquote>\n<\/aside>\n<p>DVC checks the <a href=\"https:\/\/en.wikipedia.org\/wiki\/HTTP_ETag#Strong_and_weak_validation\">eTag<\/a> of the original S3 data source when it needs to determine this e.g. at <code>dvc status<\/code>, <code>dvc repro<\/code>, etc. (You can use <code>dvc update<\/code> to bring it up to date, BTW.) But I thought your data set is not expected to ever change anyway?<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T20:04:46.442Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"jorgeorpinel\" data-post=\"11\" data-topic=\"452\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jorgeorpinel\/40\/46_2.png\" class=\"avatar\"> jorgeorpinel:<\/div>\n<blockquote>\n<p>This is not such a big of a deal really. <code>import-url<\/code> will download it into the cache directly, and then link the file to the workspace (as long as it\u2019s supported by the file system, see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization?_ga=2.18380281.1393504728.1595257554-337925788.1593564350\" rel=\"nofollow noopener\">this doc<\/a> for more info.), so there\u2019s no duplication. <code>-O<\/code> is meant more for small files you want to track with Git or experimental files you don\u2019t want to track at all.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Except it will then copy the corpus to the remote cache when I do a <code>dvc push<\/code>, which is what I was trying to avoid, right?  Or, am I missing something?<\/p>",
                "Answer_score":12.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T20:25:00.519Z",
                "Answer_body":"<p>Also, when I try running <code>dvc import-url [s3-url] [local-path]<\/code> I get a <code>Current operation was unsuccessful because 's3:\/\/duolingo-research-data\/det\/COCA' requires existing cache on 's3' remote.<\/code>  It is not clear to me what setting up a \u201ccache on \u2018s3\u2019 remote\u201d requires.<\/p>",
                "Answer_score":26.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T20:54:21.397Z",
                "Answer_body":"<p>No, it won\u2019t be pushed to any remote as it\u2019s technically an external output in the .dvc file.<\/p>\n<aside class=\"quote no-group\" data-username=\"nimrand\" data-post=\"13\" data-topic=\"452\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/n\/5f8ce5\/40.png\" class=\"avatar\"> nimrand:<\/div>\n<blockquote>\n<p>when I try running <code>dvc import-url [s3-url] [local-path]<\/code> I get a <code>Current operation was unsuccessful because 's3:\/\/duolingo-research-data\/det\/COCA' requires existing cache on 's3' remote.<\/code> It is not clear to me what setting up a \u201ccache on \u2018s3\u2019 remote\u201d requires.<\/p>\n<\/blockquote>\n<\/aside>\n<p><img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/face_with_hand_over_mouth.png?v=10\" title=\":face_with_hand_over_mouth:\" class=\"emoji\" alt=\":face_with_hand_over_mouth:\"> That sounds like a bug\u2026 Mind opening a report to <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/new?template=bug_report.md\" class=\"inline-onebox\">Sign in to GitHub \u00b7 GitHub<\/a> ? Sorry about that.<\/p>",
                "Answer_score":16.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T21:22:18.044Z",
                "Answer_body":"<p>Done: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/4261\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/issues\/4261<\/a><\/p>",
                "Answer_score":11.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-22T22:01:38.261Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"jorgeorpinel\" data-post=\"14\" data-topic=\"452\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jorgeorpinel\/40\/46_2.png\" class=\"avatar\"> jorgeorpinel:<\/div>\n<blockquote>\n<p>No, it won\u2019t be pushed to any remote as it\u2019s technically an external output in the .dvc file.<\/p>\n<\/blockquote>\n<\/aside>\n<p>I\u2019m not sure exactly what an \u201cexternal\u201d output is, but based on what you\u2019ve said it sounds like <code>import-url<\/code> (when its fixed) gives me exactly what I want.<\/p>\n<p>I think the documentation about how external resources work is a bit confusing, and then the error about requiring an \u201cexisting cache\u201d really threw me for a loop.  Good to know its a bug.<\/p>",
                "Answer_score":16.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-24T16:53:14.876Z",
                "Answer_body":"<p>You are right about our external data docs, we should probably review them. We actually have an old issue about this\u2026 <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/520\">https:\/\/github.com\/iterative\/dvc.org\/issues\/520<\/a> feel free to upvote (like) and\/or comment there.<\/p>\n<p>As for the bug you bumped into, please follow <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/4144\">https:\/\/github.com\/iterative\/dvc\/issues\/4144<\/a> instead, as the one I opened was found to be a duplicate.<\/p>\n<p>Best<\/p>",
                "Answer_score":11.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: add a remote directory without adding to the cache; Content: we have a corpus licensed from a third party that is stored in our s3 bucket. the corpus is fixed and should never change, so versioning is not much of a concern. i\u2019d like to integrate it into my repository so that when someone checks it out from git, they can run a simple command like checkout and it will download the corpus. however, since it is already stored in s3 and should never change, i would prefer that it doesn\u2019t get copied into our remote cache. but, i\u2019m not sure this is really possible with . one solution might be to use something like run -n get-corpus -o [local-corpus-path] get-url [remote-corpus-path] [local-corpus-path]. then (i think) someone can just run repro get-corpus or any downstream tasks, and will get the corpus. but, will this work? and is this the best\/recommended way?",
        "Question_original_content_gpt_summary":"The user is looking for a way to add a remote directory to their repository without adding it to the cache, as the corpus stored in the remote directory should never change.",
        "Question_preprocessed_content":"Title: add a remote directory without adding to the cache; Content: we have a corpus licensed from a third party that is stored in our s bucket. the corpus is fixed and should never change, so versioning is not much of a concern. id like to integrate it into my repository so that when someone checks it out from git, they can run a simple command like and it will download the corpus. however, since it is already stored in s and should never change, i would prefer that it doesnt get copied into our remote cache. but, im not sure this is really possible with . one solution might be to use something like . then someone can just run or any downstream tasks, and will get the corpus. but, will this work? and is this the way?",
        "Answer_original_content":"hi @nimrand by corpus, i assume you mean a data set (probably related to nlp). some sort of ground truth data source. seems like you need a download tool? would aws s3 cp work for you? installing aws cli and even a shell script to wrap the exact download command could be integrated to your repo. wrapping it with is possible but in not sure i understand the use case. can you share more context if needed? thanks the corpus is a large directory of text documents that are representative of various domains, from which we want to develop language models. the missing context, i guess, is that im already using for other projects, and id like to standardize our repos on using for large data dependencies in all our projects across the organization. in previous projects, i used add [large data file] and pushed to a remote cache on s3. so, when a person checks out the repo, they just need to run pull to get the data files they need to run the code. in this case, however, the large data file already exists in centralized location and should never change, so id like to avoid making a copy of the data in the remote cache just for the sake of integrating it with . ideally, when someone downloads the repo and does a pull it would download the corpus rom the existing s3 source, but from reading the docs im not sure this is possible. the alternative i came up with was to wrap it in a task, as i proposed in my original post. as for aws s3 cp, is there any advantage to that rather than using get-url if we can already assume that users of the repo have installed? hi @nimrand! i think there is no way to not have this data and still be able to pull it, though i think there is workaround for your use case. what you can do is to create a stage that does not have a dependency (callback stage). in your case it would look like that: run -n download_data --outs-no-cache data aws s3 cp s3:\/\/s3_data_path data that way we preserve the knowledge how to get the data, if you use get-url, will not preserve information where did you get the data. now there is one problem with this approach - callback stages are considered to be always changed, and if you later repro stage depending on callback stage output, will try to rerun this stage - copy the file all over. to prevent that you can use freeze download_data - wont try to rerun this stage until you unfreeze it. so then, when you share your project with frozen dwonload_data - someone will get an error that data does not exist, but if he\/she unfreeze download_data && repro some_later_stage - will try to download and repro the process. to illustrate this, i created following example: #!\/bin\/bash # data to import rm big_data echo data >> big_data main=$(pwd) rm -rf repo mkdir repo set -x pushd repo git init --quiet init --quiet # download data run -n download_data --outs-no-cache data \"cp $main\/big_data data\" freeze download_data # so that will not try to download each time repro is called run -n process -d data -o result \"cat data >> result && echo processed >> result\" #everything is present, no problem repro process #remove data, repro fails rm data repro process #unfreeze stage, be able to reproduce pipeline unfreeze download_data repro process im surprised that would always try to rerun that task, even when the data it produces already exists locally. is this always the case when a stage depends on an --outs-no-cache of another stage? you called it a callback stage. im not familiar with that term. also, how is run -n download_data --outs-no-cache data aws s3 cp s3:\/\/s3_data_path different from run -n download_data --outs-no-cache data get-url s3:\/\/s3_data_path data. doesnt aws s3 cp s3:\/\/s3_data_path and get-url s3:\/\/s3_data_path data do the same thing? @nimrand nimrand: also, how is run -... well it is possible to do that too, i missed this possibility. so to answer your first question: callback stage: stage that does not have dependencies will always rerun callback stages. --outs-no-cache does not matter in this case. the reason for this behavior is that callback stages have been introduced to let user execute code that verifies whether something changed and let that information into the pipeline. example: checking if there are new entries in log storage to trigger importing and processing them. as to your project: you might also just use data as a dependency in your first pipeline. dependencies are not added under control (outputs do), so it wont go into the cache. upon reproducing pipeline with no data you will get information that it does not exist. interesting ideas about using a stage with no dependencies, or an external dependency in a 2-stage pipeline pawel. but if the goal is to download the data with pull like in the other repos @nimrand has, that wouldnt help, as repro is needed instead instead. the problem is that for pull to work, the data would need to be pushed first, meaning added manually to the workspace, tracked by ( add for example), and pushed to some remote storage. another way to put in the workspace and track it in a single step would be import-url. but both these methods duplicate the remote storage of the data set. another possible workaround is to add the data to the project without moving it, as an external output. this implies setting up an external cache in the s3 location first, and then do something like add s3:\/\/s3_data_path. im not sure what happens on the s3 at that point though, the data may be duplicated anyway and pull still wouldnt download it to the workspace, as its added externally (never pushed in the first place). will always rerun callback stages . --outs-no-cache does not matter in this case. the reason for this behavior is that callback stages have been introduced to let user execute code that verifies whether something changed and let that information into the pipeline. example: checking if there are new entries in log storage to trigger importing and processing them. i would expect the default would be to not rerun stages that dont have dependencies, and that callback stage could be made by using --always-changed. i suppose i could add a dummy dependency to the get-corpus task, but thats a bit clunky. it seems that my use-case wont work with pull without duplicating the data, but maybe making it work via a repro is the next best thing. ok. and like i mentioned, please consider https:\/\/.org\/doc\/command-reference\/import-url as another alternative. it will download the data and create a . file for it, as if it was tracked with add, so you can use it as a dependency for further stages and have repro download it automatically if\/when needed. just re-read that page in it, they give an example that says import-url is equivalent to run -n download_data -d https:\/\/data..org\/get-started\/data.xml -o data.xml wget https:\/\/data..org\/get-started\/data.xml -o data.xml i think thats almost what i want, except wed change the first -o parameter to -o to avoid copying the file into the local cache. but, since it declares a dependency on a remote location, how does it know when its changed other than re-downloading the files every time? nimrand: i think thats almost what i want, except wed change the first -o parameter to -o to avoid copying the file into the local cache. this is not such a big of a deal really. import-url will download it into the cache directly, and then link the file to the workspace (as long as its supported by the file system, see this doc for more info.), so theres no duplication. -o is meant more for small files you want to track with git or experimental files you dont want to track at all. nimrand: but, since it declares a dependency on a remote location, how does it know when its changed other than re-downloading the files every time? checks the etag of the original s3 data source when it needs to determine this e.g. at status, repro, etc. (you can use update to bring it up to date, btw.) but i thought your data set is not expected to ever change anyway? jorgeorpinel: this is not such a big of a deal really. import-url will download it into the cache directly, and then link the file to the workspace (as long as its supported by the file system, see this doc for more info.), so theres no duplication. -o is meant more for small files you want to track with git or experimental files you dont want to track at all. except it will then copy the corpus to the remote cache when i do a push, which is what i was trying to avoid, right? or, am i missing something? also, when i try running import-url [s3-url] [local-path] i get a current operation was unsuccessful because 's3:\/\/duolingo-research-data\/det\/coca' requires existing cache on 's3' remote. it is not clear to me what setting up a cache on s3 remote requires. no, it wont be pushed to any remote as its technically an external output in the . file. nimrand: when i try running import-url [s3-url] [local-path] i get a current operation was unsuccessful because 's3:\/\/duolingo-research-data\/det\/coca' requires existing cache on 's3' remote. it is not clear to me what setting up a cache on s3 remote requires. that sounds like a bug mind opening a report to sign in to github github ? sorry about that. done: https:\/\/github.com\/iterative\/\/issues\/4261 jorgeorpinel: no, it wont be pushed to any remote as its technically an external output in the . file. im not sure exactly what an external output is, but based on what youve said it sounds like import-url (when its fixed) gives me exactly what i want. i think the documentation about how external resources work is a bit confusing, and then the error about requiring an existing cache really threw me for a loop. good to know its a bug. you are right about our external data docs, we should probably review them. we actually have an old issue about this https:\/\/github.com\/iterative\/.org\/issues\/520 feel free to upvote (like) and\/or comment there. as for the bug you bumped into, please follow https:\/\/github.com\/iterative\/\/issues\/4144 instead, as the one i opened was found to be a duplicate. best",
        "Answer_original_content_gpt_summary":"Possible solutions to adding a remote directory to a repository without adding it to the cache include using a download tool like AWS S3 cp, wrapping it in a task, creating a stage with no dependency, or using import-url. The import-url command can download the data and create a . file for it, as if it was tracked with add, so you can use it as a dependency for further stages and have repro download it automatically if\/when needed. However, the documentation about how external resources work is a bit confusing, and there is a bug that requires an existing cache on the S3 remote.",
        "Answer_preprocessed_content":"hi by corpus, i assume you mean a data set . some sort of ground truth data source. seems like you need a download tool? would work for you? installing cli and even a shell script to wrap the exact download command could be integrated to your repo. wrapping it with is possible but in not sure i understand the use case. can you share more context if needed? thanks the corpus is a large directory of text documents that are representative of various domains, from which we want to develop language models. the missing context, i guess, is that im already using for other projects, and id like to standardize our repos on using for large data dependencies in all our projects across the organization. in previous projects, i used and pushed to a remote cache on s . so, when a person checks out the repo, they just need to run to get the data files they need to run the code. in this case, however, the large data file already exists in centralized location and should never change, so id like to avoid making a copy of the data in the remote cache just for the sake of integrating it with . ideally, when someone downloads the repo and does a it would download the corpus rom the existing s source, but from reading the docs im not sure this is possible. the alternative i came up with was to wrap it in a task, as i proposed in my original post. as for , is there any advantage to that rather than using if we can already assume that users of the repo have installed? hi i think there is no way to not have this data and still be able to pull it, though i think there is workaround for your use case. what you can do is to create a stage that does not have a dependency . in your case it would look like that that way we preserve the knowledge how to get the data, if you use , will not preserve information where did you get the data. now there is one problem with this approach callback stages are considered to be always changed, and if you later stage depending on callback stage output, will try to rerun this stage copy the file all over. to prevent that you can use wont try to rerun this stage until you it. so then, when you share your project with frozen someone will get an error that does not exist, but if will try to download and repro the process. to illustrate this, i created following example im surprised that would always try to rerun that task, even when the data it produces already exists locally. is this always the case when a stage depends on an of another stage? you called it a callback stage. im not familiar with that term. also, how is different from . doesnt and do the same thing? nimrand also, how is well it is possible to do that too, i missed this possibility. so to answer your first question stage that does not have dependencies will always rerun . does not matter in this case. the reason for this behavior is that have been introduced to let user execute code that verifies whether something changed and let that information into the pipeline. example checking if there are new entries in log storage to trigger importing and processing them. as to your project you might also just use as a dependency in your first pipeline. dependencies are not added under control , so it wont go into the cache. upon reproducing pipeline with no you will get information that it does not exist. interesting ideas about using a stage with no dependencies, or an external dependency in a pipeline pawel. but if the goal is to download the data with like in the other repos has, that wouldnt help, as is needed instead instead. the problem is that for to work, the data would need to be pushed first, meaning added manually to the workspace, tracked by , and ed to some remote storage. another way to put in the workspace and track it in a single step would be . but both these methods duplicate the remote storage of the data set. another possible workaround is to add the data to the project without moving it, as an external output. this implies setting up an external cache in the s location first, and then do something like . im not sure what happens on the s at that point though, the data may be duplicated anyway and still wouldnt download it to the workspace, as its added externally . will always rerun . does not matter in this case. the reason for this behavior is that have been introduced to let user execute code that verifies whether something changed and let that information into the pipeline. example checking if there are new entries in log storage to trigger importing and processing them. i would expect the default would be to not rerun stages that dont have dependencies, and that could be made by using . i suppose i could add a dummy dependency to the task, but thats a bit clunky. it seems that my wont work with without duplicating the data, but maybe making it work via a is the next best thing. ok. and like i mentioned, please consider as another alternative. it will download the data and create a . file for it, as if it was tracked with , so you can use it as a dependency for further stages and have repro download it automatically needed. just that page in it, they give an example that says is equivalent to i think thats almost what i want, except wed change the first parameter to to avoid copying the file into the local cache. but, since it declares a dependency on a remote location, how does it know when its changed other than the files every time? nimrand i think thats almost what i want, except wed change the first parameter to to avoid copying the file into the local cache. this is not such a big of a deal really. will download it into the cache directly, and then link the file to the workspace , so theres no duplication. is meant more for small files you want to track with git or experimental files you dont want to track at all. nimrand but, since it declares a dependency on a remote location, how does it know when its changed other than the files every time? checks the etag of the original s data source when it needs to determine this at , , etc. but i thought your data set is not expected to ever change anyway? jorgeorpinel this is not such a big of a deal really. will download it into the cache directly, and then link the file to the workspace , so theres no duplication. is meant more for small files you want to track with git or experimental files you dont want to track at all. except it will then copy the corpus to the remote cache when i do a , which is what i was trying to avoid, right? or, am i missing something? also, when i try running i get a it is not clear to me what setting up a cache on s remote requires. no, it wont be pushed to any remote as its technically an external output in the . file. nimrand when i try running i get a it is not clear to me what setting up a cache on s remote requires. that sounds like a bug mind opening a report to sign in to github github ? sorry about that. done jorgeorpinel no, it wont be pushed to any remote as its technically an external output in the . file. im not sure exactly what an external output is, but based on what youve said it sounds like gives me exactly what i want. i think the documentation about how external resources work is a bit confusing, and then the error about requiring an existing cache really threw me for a loop. good to know its a bug. you are right about our external data docs, we should probably review them. we actually have an old issue about this feel free to upvote comment there. as for the bug you bumped into, please follow instead, as the one i opened was found to be a duplicate. best"
    }
]