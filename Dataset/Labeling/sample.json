[
    {
        "Question_id":33708725.0,
        "Question_title":"Azure Machine learning: error with multiclass classification algo",
        "Question_body":"<p>I have <a href=\"https:\/\/yadi.sk\/i\/--Vkm7FTkTAxY\" rel=\"nofollow noreferrer\">training set<\/a> and <a href=\"https:\/\/yadi.sk\/i\/wYu0ZsmukTAw3\" rel=\"nofollow noreferrer\">test set<\/a> (csv files with header), in which I have to classify each value. There is 118.000 uniq values in X column, and only about 13000 in y1 column, so there will be 13000 categories.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Qc1i8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Qc1i8.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>From Training set I need only <code>X<\/code> and <code>y1<\/code> column to train model. I need to classify X value to one of categories (find normal from of initial word). I tried all multi algo but failed trying to evaluate model.<\/p>\n\n<p>Visualizing Score model return this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EnDZq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EnDZq.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What can be a problem, it just returns -2 code as error and this <a href=\"https:\/\/yadi.sk\/i\/I6WiEoCGkTCXc\" rel=\"nofollow noreferrer\">log<\/a><\/p>\n\n<p>UPD1: By Metadata Editor module under Project Column Module made column y1  as categorical,  nothing seems to be changed<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1447506491170,
        "Question_favorite_count":null,
        "Question_last_edit_time":1454300637020,
        "Question_score":0.0,
        "Question_view_count":75.0,
        "Answer_body":"<p><strong>Moncef<\/strong> provided <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/aabb9080-adf8-4fc0-b332-2a3c8ee29a28\/azure-machine-learning-error-with-multiclass-classification-algo?forum=MachineLearning\" rel=\"nofollow\">here<\/a> the solution to my problem:<\/p>\n\n<p>The point is that Azure has limitations on maximum categories 8192, this is why the number should be decreased by R or python modules or own evaluation module may be created. Or there is another way, evaluation step may be skipped, because model`ve been trained successfully. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/33708725",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1448271863192,
        "Question_original_content":"error multiclass classif algo train set test set csv file header classifi valu uniq valu column column categori train set need column train model need classifi valu categori normal initi word tri multi algo fail try evalu model visual score model return problem return code error log upd metadata editor modul project column modul column categor chang",
        "Question_preprocessed_content":"error multiclass classif algo train set test set classifi valu uniq valu column column categori train set need column train model need classifi valu categori tri multi algo fail try evalu model visual score model return problem return code error log upd metadata editor modul project column modul column categor chang",
        "Question_gpt_summary_original":"The user is facing challenges with Azure Machine Learning's multiclass classification algorithm. They have a training set and a test set with a large number of unique values in the X column and only about 13,000 categories in the y1 column. The user has tried all multi algorithms but failed to evaluate the model. The model returns a -2 code error and a log file. The user has also tried making the y1 column categorical using the Metadata Editor module, but it did not solve the problem.",
        "Question_gpt_summary":"user face challeng multiclass classif algorithm train set test set larg number uniqu valu column categori column user tri multi algorithm fail evalu model model return code error log file user tri make column categor metadata editor modul solv problem",
        "Answer_original_content":"moncef provid solut problem point azur limit maximum categori number decreas python modul evalu modul creat wai evalu step skip model train successfulli",
        "Answer_preprocessed_content":"moncef provid solut problem point azur limit maximum categori number decreas python modul evalu modul creat wai evalu step skip model train successfulli",
        "Answer_gpt_summary_original":"possible solutions to the challenge of classifying 118,000 unique values into 13,000 categories are to decrease the number of categories using r or python modules or creating an evaluation module, or to skip the evaluation step altogether if the model has been trained successfully. additionally, it is noted that azure has a limitation on maximum categories of 8192.",
        "Answer_gpt_summary":"possibl solut challeng classifi uniqu valu categori decreas number categori python modul creat evalu modul skip evalu step altogeth model train successfulli addition note azur limit maximum categori"
    },
    {
        "Question_id":null,
        "Question_title":"Save audio file from speech to text stream",
        "Question_body":"I am using @Google-cloud\/speech for streaming audio from the browser to my nodejs backend.\nI would like to save the recorded audio.\nI see no option to do so. Any suggestions? Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646292900000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Save-audio-file-from-speech-to-text-stream\/td-p\/398993\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-08T13:17:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hey,\u00a0\n\nYou shall probably use other packages for recording such as recordrtc as mentioned at [1].\u00a0\u00a0\n\n[1]\u00a0https:\/\/www.leeboonstra.dev\/chatbots\/building-your-own-voice-ai-3\/"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save audio file speech text stream googl cloud speech stream audio browser nodej backend like save record audio option suggest thank",
        "Question_preprocessed_content":"save audio file speech text stream stream audio browser nodej backend like save record audio option suggest thank",
        "Question_gpt_summary_original":"The user is facing a challenge in saving recorded audio from a speech to text stream using @Google-cloud\/speech in their nodejs backend as they cannot find an option to do so. They are seeking suggestions to overcome this challenge.",
        "Question_gpt_summary":"user face challeng save record audio speech text stream googl cloud speech nodej backend option seek suggest overcom challeng",
        "Answer_original_content":"hei shall probabl us packag record recordrtc mention http leeboonstra dev chatbot build voic",
        "Answer_preprocessed_content":"hei shall probabl us packag record recordrtc mention",
        "Answer_gpt_summary_original":"possible solution: the user can use other packages such as recordrtc for recording audio files from a speech to text stream in their node.js backend.",
        "Answer_gpt_summary":"possibl solut user us packag recordrtc record audio file speech text stream node backend"
    },
    {
        "Question_id":73661090.0,
        "Question_title":"How do I give Vertex AI pipeline component permissions?",
        "Question_body":"<p>In a Vertex AI pipeline component,I try:<\/p>\n<pre><code>def my_comp(project_id: str, location: str, endpoint_id: str, endpoint: Output[Artifact]):\n    import google.cloud.aiplatform as aip\n    endpoints = aip.Endpoint.list()\n...\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>'aiplatform.endpoints.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/...\n<\/code><\/pre>\n<p>My service account has owner permissions, and it works outside of the component. What do I need to do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1662721061203,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":"<p>This permission denied on resource issue can be resolved by using import statement:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73661090",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663223342092,
        "Question_original_content":"pipelin compon permiss pipelin compon try def comp project str locat str endpoint str endpoint output artifact import googl cloud aiplatform aip endpoint aip endpoint list give aiplatform endpoint list deni resourc aiplatform googleapi com project servic account owner permiss work outsid compon need",
        "Question_preprocessed_content":"pipelin compon permiss pipelin compon try give servic account owner permiss work outsid compon need",
        "Question_gpt_summary_original":"The user is encountering a challenge in giving Vertex AI pipeline component permissions. They are receiving an error message stating that 'aiplatform.endpoints.list' is denied on a resource, despite having owner permissions on their service account. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user encount challeng give pipelin compon permiss receiv error messag state aiplatform endpoint list deni resourc despit have owner permiss servic account user seek advic resolv issu",
        "Answer_original_content":"permiss deni resourc issu resolv import statement googl cloud import aiplatform aiplatform",
        "Answer_preprocessed_content":"permiss deni resourc issu resolv import statement",
        "Answer_gpt_summary_original":"the solution to the permission denied issue when giving pipeline component permissions is to use the import statement \"from google.cloud import aiplatform_v1 as aiplatform\".",
        "Answer_gpt_summary":"solut permiss deni issu give pipelin compon permiss us import statement googl cloud import aiplatform aiplatform"
    },
    {
        "Question_id":null,
        "Question_title":"Invitation to join Microsoft Community Champions Program - Azure",
        "Question_body":"Are you a Microsoft Azure Technology Expert? Do you resonate with the idea of using your skills to help customers, create Impact and also get recognized?\n\nIf your answer is yes \u2013 you have an opportunity to be part of the Microsoft Azure Community Champions family! This is a thriving community of 100+ members made of Microsoft employees, MVPs, Suppliers, and other experts who enjoy solving latest customer problems in Microsoft Q&A technical site and thereby learn hands-on.\n\nFor your voluntary contributions you can also receive exciting rewards and recognitions which includes monthly $50 gift cards, being featured on Microsoft branded social media & leaderboards, considerations for Microsoft MVP award, Microsoft NDA with Private Preview access and more.\n\nVisit Microsoft Community Champions for Q&A | Microsoft Docs to learn more and become an Azure Community Champion!\n\nSee you on the other side!!\n\nSpecial invitation to:\n\n@carlzhao-msft @TchimwaSougang-3249 @LiHongMSFT-3908 @MarkBrownMSFT @LuDaiMSFT-0289 @dominicbetts @SujithreddyKomma-8764 @BertZhoumsft-7490 @ZehuiYaoMSFT-7151 @SeeyaXi-msft @VickyKumarMindtreeConsultingPVTLTD-5545 @chbeier @JasonPan-MSFT @OlafHelper-2800 @AzureAaronHughes @lukemurraynz @sadomovalex @NaomiNNN @EricBoyd @sql-articles @JingyangLi @MOXiMOX-2301 @msfthiker @NewbieJones-6218 @Viorel-1 @clivewatson-9831 @RichMatheisen-8856 @Chungsun-1776 @JamesLongworth @kaaven @MotoX80 @RishabhMishra-9205 @rkiss @SrikanthRana @67603284 @AkashChopra-0052 @JimmySalian-2011 @AgaveJoe @Yufeishao-0810 @ZoeHui @Jason-MSFT @Sheena-MSFT @Bruce-SqlWork @martinta @LynnNiu-5125\n\nThank You to all the Microsoft Q&A Community Champions.\nThey are helping make Microsoft Q&A a vibrant place for learning.\n\n@AlbertoMorillo @soysoliscarlos @ricardosolisvillegas-4678 @AlanKinane @ManuPhilip @NandanHegde-7720 @AndrewBlumhardt-1137 @sikumars @PratikSomaiya @TakahitoIwasa @DavidBroggy-5270 @KamleshKumar @AndreasBaumgarten @michev @AndyDavid @SandervandeVelde42 @BrunoLucas-9843 @shivapatpi-MSFT @DillonJS @martins-jackson @SubashriVasudevan-1752 @Samy-7940 @pituach @AndriyBilous @stan @CristianSPIRIDON72 @DSPatrick @nasreen-akter @Sam-Cogan @rbrundritt @BjoernPeters @ErlandSommarskog @sreejukg @msrini-MSFT @VidyaNarasimhan-2409 @SudiptaChakraborty-1767 @ZollnerD @cooldadtx @AlistairRoss-msft @MarkKromer-MSFT @ScottAzureRTOS @maserg @Dev073 @nhcloud @TomPhillips-1744 @IoTGirl @JaliyaUdagedara @JohndeuMSFT @PradeepKommaraju-MSFT @MatthijsvdVeer @HasanSavran-7728 @VinodKumar-0434 @AliSufyan-1625 @CdricPerion-5162 @learn2skills @VineetKumarGupta-6574 @Vinodh247-1375 @Thameur-BOURBITA @kashyapa @PierreLucGiguere-5297 @DanGuzman @derhoppe @MartinCairney-6481 @piaudonn @yagmoth555 @eepyaich @GeorgeChrysovalantisGrammatikos-8518 @KenMaxwell-4349 @RahulJindal-2267 @SubbuKonathala @VaibhavChaudhari @vukasinterzic @GeorgeMoise-0315 @YugandharMunagala-MSFT @arnavsharma @MatthewBrowne-7065 @pvanberlo @Saggiehaim @SaiGunaranjan @GlenScales-6756 @jasjitchopra @RahulTherayil @ShamirCharania-2101 @Yuvaraj-MSFT @abhishekshaha @adiazcan @damianoandresini @demiliani @hugobarona @jloudon @Joecarlyle @JoseBenjaminSolisNolasco-2402 @JuanSobrado-3258 @mgessenay @oscarmh @RyanJohnson-MSFT @Sean-Liming @sqlarcher @SruthiSaranyaKarthikeyan-7489 @StoyanChalakov @VigneshSukumar-0037 @MohammedAltamashKhan-3285 @LuisRodriguez-MSFT",
        "Question_answer_count":7,
        "Question_comment_count":5,
        "Question_creation_time":1660030627450,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":11.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/960307\/invitation-to-join-microsoft-community-champions-p.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-09T08:21:01.273Z",
                "Answer_score":0,
                "Answer_body":"Thanks for the invitation - happy to help ;-)",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-10T02:55:34.273Z",
                "Answer_score":0,
                "Answer_body":"Thanks for the invitation\nI'm happy to help",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-10T05:00:55.343Z",
                "Answer_score":0,
                "Answer_body":"Thanks for the Invite\nHappy to get in",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-16T06:27:37.377Z",
                "Answer_score":0,
                "Answer_body":"Happy to Part of this.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-19T14:02:03.447Z",
                "Answer_score":0,
                "Answer_body":"The thing is, am now part of the ITS GREATNESS, AZURE TEAM ready to instill Azure in them\/me\/us, haha Thanks for the inv!!!!",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-10T12:17:26.737Z",
                "Answer_score":0,
                "Answer_body":"@tbgangav-MSFT\nThank you for the invitation. it's pleasure to be part of you and share my knowledge in this forum.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-26T12:02:05.217Z",
                "Answer_score":0,
                "Answer_body":"Very happy to be here! All the best!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":112.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"invit join microsoft commun champion program azur microsoft azur technolog expert reson idea skill help custom creat impact recogn answer ye opportun microsoft azur commun champion famili thrive commun member microsoft employe mvp supplier expert enjoi solv latest custom problem microsoft technic site learn hand voluntari contribut receiv excit reward recognit includ monthli gift card featur microsoft brand social media leaderboard consider microsoft mvp award microsoft nda privat preview access visit microsoft commun champion microsoft doc learn azur commun champion special invit carlzhao msft tchimwasougang lihongmsft markbrownmsft ludaimsft dominicbett sujithreddykomma bertzhoumsft zehuiyaomsft seeyaxi msft vickykumarmindtreeconsultingpvtltd chbeier jasonpan msft olafhelp azureaaronhugh lukemurraynz sadomovalex naomi ericboyd sql articl jingyangli moximox msfthiker newbiejon viorel clivewatson richmatheisen chungsun jameslongworth kaaven motox rishabhmishra rkiss srikanthrana akashchopra jimmysalian agavejo yufeishao zoehui jason msft sheena msft bruce sqlwork martinta lyiu thank microsoft commun champion help microsoft vibrant place learn albertomorillo soysoliscarlo ricardosolisvillega alankinan manuphilip nandanhegd andrewblumhardt sikumar pratiksomaiya takahitoiwasa davidbroggi kamleshkumar andreasbaumgarten michev andydavid sandervandeveld brunoluca shivapatpi msft dillonj martin jackson subashrivasudevan sami pituach andriybil stan cristianspiridon dspatrick nasreen akter sam cogan rbrundritt bjoernpet erlandsommarskog sreejukg msrini msft vidyanarasimhan sudiptachakraborti zollnerd cooldadtx alistairross msft markkrom msft scottazurerto maserg dev nhcloud tomphillip iotgirl jaliyaudagedara johndeumsft pradeepkommaraju msft matthijsvdv hasansavran vinodkumar alisufyan cdricperion learnskil vineetkumargupta vinodh thameur bourbita kashyapa pierrelucgiguer danguzman derhopp martincairnei piaudonn yagmoth eepyaich georgechrysovalantisgrammatiko kenmaxwel rahuljind subbukonathala vaibhavchaudhari vukasinterz georgemois yugandharmunagala msft arnavsharma matthewbrown pvanberlo saggiehaim saigunaranjan glenscal jasjitchopra rahultherayil shamircharania yuvaraj msft abhishekshaha adiazcan damianoandresini demiliani hugobarona jloudon joecarlyl josebenjaminsolisnolasco juansobrado mgessenai oscarmh ryanjohnson msft sean lime sqlarcher sruthisaranyakarthikeyan stoyanchalakov vigneshsukumar mohammedaltamashkhan luisrodriguez msft",
        "Question_preprocessed_content":"invit join microsoft commun champion program azur microsoft azur technolog expert reson idea skill help custom creat impact recogn answer ye opportun microsoft azur commun champion famili thrive commun member microsoft employe mvp supplier expert enjoi solv latest custom problem microsoft technic site learn voluntari contribut receiv excit reward recognit includ monthli gift card featur microsoft brand social media leaderboard consider microsoft mvp award microsoft nda privat preview access visit microsoft commun champion microsoft doc learn azur commun champion special invit thank microsoft commun champion help microsoft vibrant place learn",
        "Question_gpt_summary_original":"The text invites Microsoft Azure Technology Experts to join the Microsoft Azure Community Champions program, which is a community of experts who help customers solve problems on the Microsoft Q&A technical site. The program offers rewards and recognition for voluntary contributions. The text also includes a list of current Microsoft Q&A Community Champions who are helping to make the site a vibrant place for learning.",
        "Question_gpt_summary":"text invit microsoft azur technolog expert join microsoft azur commun champion program commun expert help custom solv problem microsoft technic site program offer reward recognit voluntari contribut text includ list current microsoft commun champion help site vibrant place learn",
        "Answer_original_content":"thank invit happi help thank invit happi help thank invit happi happi thing great azur team readi instil azur haha thank inv tbgangav msft thank invit pleasur share knowledg forum happi best",
        "Answer_preprocessed_content":"thank invit happi help thank invit happi help thank invit happi happi thing great azur team readi instil azur haha thank inv thank invit pleasur share knowledg forum happi best",
        "Answer_gpt_summary_original":"the answer does not provide any specific solutions, but it shows that the user is willing to join the microsoft azure community champions program and contribute to the microsoft q&a technical site. the user expresses gratitude for the invitation and is excited to share their knowledge and be part of the community.",
        "Answer_gpt_summary":"answer provid specif solut show user will join microsoft azur commun champion program contribut microsoft technic site user express gratitud invit excit share knowledg commun"
    },
    {
        "Question_id":null,
        "Question_title":"Memory issue",
        "Question_body":"Hi friends, Iam facing this error recently - The replica workerpool0-0 ran out-of-memory and exited with a non-zero status of 137(SIGKILL). Kindly help me , i am using 800GB , still getting this error   ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655778180000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":221.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Memory-issue\/td-p\/433242\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-29T07:35:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"This error normally indicates an issue with the code rather than being an Out Of Memory Exception in the service side.\n\nIs there any other error that you can share that is showing in the VM logs?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"memori issu friend iam face error recent replica workerpool ran memori exit non zero statu sigkil kindli help get error",
        "Question_preprocessed_content":"memori issu friend iam face error recent replica ran exit statu sigkil kindli help get error",
        "Question_gpt_summary_original":"The user is facing a memory issue where the replica workerpool0-0 ran out of memory and exited with a non-zero status of 137(SIGKILL), despite using 800GB. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face memori issu replica workerpool ran memori exit non zero statu sigkil despit user seek help resolv issu",
        "Answer_original_content":"error normal indic issu code memori except servic error share show log",
        "Answer_preprocessed_content":"error normal indic issu code memori except servic error share show log",
        "Answer_gpt_summary_original":"possible solutions: \n- check for any other errors in the vm logs that may be causing the issue.\n- review the code to identify any potential issues that may be causing the out-of-memory error.",
        "Answer_gpt_summary":"possibl solut check error log caus issu review code identifi potenti issu caus memori error"
    },
    {
        "Question_id":null,
        "Question_title":"Loosen azureml-dataprep requirements to cloudpickle<=2.0.0",
        "Question_body":"Hi,\n\nI couldn\u2019t find a specific github repo for azureml-dataprep so I decided to also write you here. Can you forward it to the devs?\n\n\n\n\nazureml-dataprep (which is a depedency for azureml-dataset-runtime) has requirement cloudpickle<2.0.0 and >=1.1.0. However there is to my knowledage no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0. cloudpickle==2.0.0 introduces some very effective tools for serializing helper scripts which is very helful when working with azureml. So azureml-dataprep should allow cloudpickle<=2.0.0\n\nIntro to new cloudpickle:\nhttps:\/\/github.com\/cloudpipe\/cloudpickle#overriding-pickles-serialization-mechanism-for-importable-constructs\nPR:\nhttps:\/\/github.com\/cloudpipe\/cloudpickle\/pull\/417\nGithub issue:\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1637",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637242355487,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@ThomasH-1455\n\nThank you so much for the contribute, I have sent an email to the author for the PR review and merge.\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/632441\/loosen-azureml-dataprep-requirements-to-cloudpickl.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-19T02:48:45.06Z",
                "Answer_score":0,
                "Answer_body":"@ThomasH-1455\n\nThank you so much for the contribute, I have sent an email to the author for the PR review and merge.\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1637290125060,
        "Question_original_content":"loosen dataprep requir cloudpickl specif github repo dataprep decid write forward dev dataprep deped dataset runtim requir cloudpickl knowledag break featur go cloudpickl cloudpickl cloudpickl introduc effect tool serial helper script hel work dataprep allow cloudpickl intro new cloudpickl http github com cloudpip cloudpickl overrid pickl serial mechan import construct http github com cloudpip cloudpickl pull github issu http github com azur machinelearningnotebook issu",
        "Question_preprocessed_content":"loosen dataprep requir specif github repo dataprep decid write forward dev dataprep requir knowledag break featur go introduc effect tool serial helper script hel work dataprep allow intro new cloudpickl github issu",
        "Question_gpt_summary_original":"The user is facing a challenge with the dependency requirements of azureml-dataprep, which is a dependency for azureml-dataset-runtime. The current requirement for cloudpickle is <2.0.0 and >=1.1.0, but the user believes that there are no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0, and that the new version introduces helpful tools for serializing helper scripts. The user is requesting that azureml-dataprep should allow cloudpickle<=2.0.0.",
        "Question_gpt_summary":"user face challeng depend requir dataprep depend dataset runtim current requir cloudpickl user believ break featur go cloudpickl cloudpickl new version introduc help tool serial helper script user request dataprep allow cloudpickl",
        "Answer_original_content":"thomash thank contribut sent email author review merg hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"thank contribut sent email author review merg hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_gpt_summary_original":"there are no explicit solutions provided in the answer. the answer is thanking someone for their contribution and suggesting ways to stay updated on the discussion.",
        "Answer_gpt_summary":"explicit solut provid answer answer thank contribut suggest wai stai updat discuss"
    },
    {
        "Question_id":42766263.0,
        "Question_title":"How to Find Azure ML Experiment based on Deployed Web Service",
        "Question_body":"<p>I have a list of ML experiments which I have created in Azure Machine Learning Studio.  I have deployed them as web services (the new version, not classic).  <\/p>\n\n<p>How can I go into Azure Machine Learning Web Services, click on a web service (which was deployed from an experiment), then navigate back to the experiment \/ predictive model which feeds it?<\/p>\n\n<p>The only link I can find between the two is by updating the web service from the predictive experiment, which then confirms what the web service is. I can see that the \"ExperimentId\" is a GUID in the URL when in the experiment and the web service, so hopefully this is possible.<\/p>\n\n<p>My reasoning is that relying on matching naming conventions, etc., to select the appropriate model to update is subject to human error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1489415750577,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":7.0,
        "Question_view_count":224.0,
        "Answer_body":"<p>The <em>new<\/em> web service does not store any information about the experiment or workspace that was deployed (not all <em>new<\/em> web services are deployed from an experiment).<\/p>\n\n<p>Here are the options available to track the relationship between the experiment and a <em>new<\/em> web service.<\/p>\n\n<h2>last deployment<\/h2>\n\n<p>However, the experiment keeps track of the <strong>last<\/strong> <em>new<\/em> web service that was deployed from the experiment. each deployment to a <em>new<\/em> web service overwrites this value.<\/p>\n\n<p>The value is stored in the experiment graph. One way to get the graph is to use the powershell module <a href=\"http:\/\/aka.ms\/amlps\" rel=\"nofollow noreferrer\">amlps<\/a><\/p>\n\n<p><code>Export-AmlExperimentGraph -ExperimentId &lt;Experiment Id&gt; -OutputFile e.json<\/code><\/p>\n\n<p><strong>e.json<\/strong><\/p>\n\n<pre><code>{\n\"ExperimentId\":\"&lt;Experiment Id&gt;\",\n\/\/ . . .\n\"WebService\":{\n\/\/ . . .\n\"ArmWebServiceId\":\"&lt;Arm Id&gt;\"\n},\n\/\/ . . . \n}\n<\/code><\/pre>\n\n<h2>azure resource tags<\/h2>\n\n<p>The tags feature for Azure resources is supported by the <em>new<\/em> web services. Setting a <code>tag<\/code> on the web service programmatically, with powershell or via the azure portal UX can be used to store a reference to the experiment on the <em>new<\/em> web service.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42766263",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1490283526688,
        "Question_original_content":"experi base deploi web servic list experi creat studio deploi web servic new version classic web servic click web servic deploi experi navig experi predict model feed link updat web servic predict experi confirm web servic experimentid guid url experi web servic hopefulli possibl reason reli match name convent select appropri model updat subject human error",
        "Question_preprocessed_content":"experi base deploi web servic list experi creat studio deploi web servic web servic click web servic navig experi predict model feed link updat web servic predict experi confirm web servic experimentid guid url experi web servic hopefulli possibl reason reli match name convent select appropri model updat subject human error",
        "Question_gpt_summary_original":"The user is facing a challenge in finding the Azure ML experiment that feeds a deployed web service. They are unable to navigate back to the experiment from the web service and are relying on matching naming conventions, which is subject to human error. The user is looking for a way to link the web service to the experiment using the ExperimentId GUID in the URL.",
        "Question_gpt_summary":"user face challeng find experi feed deploi web servic unabl navig experi web servic reli match name convent subject human error user look wai link web servic experi experimentid guid url",
        "Answer_original_content":"new web servic store inform experi workspac deploi new web servic deploi experi option avail track relationship experi new web servic deploy experi keep track new web servic deploi experi deploy new web servic overwrit valu valu store experi graph wai graph us powershel modul amlp export amlexperimentgraph experimentid outputfil json json experimentid webservic armwebserviceid azur resourc tag tag featur azur resourc support new web servic set tag web servic programmat powershel azur portal store refer experi new web servic",
        "Answer_preprocessed_content":"new web servic store inform experi workspac deploi option avail track relationship experi new web servic deploy experi keep track new web servic deploi experi deploy new web servic overwrit valu valu store experi graph wai graph us powershel modul amlp azur resourc tag tag featur azur resourc support new web servic set web servic programmat powershel azur portal store refer experi new web servic",
        "Answer_gpt_summary_original":"possible solutions to track the relationship between an experiment and a new web service are: \n1. use the last deployment, as the experiment keeps track of the last new web service that was deployed from the experiment. \n2. use the experiment graph, which can be obtained using the powershell module amlps export-amlexperimentgraph. \n3. use azure resource tags, which can be set programmatically or via the azure portal ux to store a reference to the experiment on the new web service.",
        "Answer_gpt_summary":"possibl solut track relationship experi new web servic us deploy experi keep track new web servic deploi experi us experi graph obtain powershel modul amlp export amlexperimentgraph us azur resourc tag set programmat azur portal store refer experi new web servic"
    },
    {
        "Question_id":null,
        "Question_title":"Clean Data Error",
        "Question_body":"I'm getting an error trying to clean a data module in Azure Machine Learning Studio (classic).\n\nBelow is the full log. I've tried logging out and back into my Azure account to no avail. Is there something wrong with my credentials?\n\nRecord Starts at UTC 09\/18\/2021 02:19:08:\n\n\nRun the job:\"\/dll \"Microsoft.Analytics.Modules.CleanMissingData.Dll, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.Analytics.Modules.CleanMissingData.Dll.CleanMissingData;Run\" \/Output0 \"....\\Cleaned dataset\\Cleaned dataset.dataset\" \/Output1 \"....\\Cleaning transformation\\Cleaning transformation.itransform\" \/inputData \"....\\Dataset\\Dataset.csv\" \/columnsToClean \"%7B%22isFilter%22%3Atrue%2C%22rules%22%3A%5B%7B%22ruleType%22%3A%22ColumnNames%22%2C%22columns%22%3A%5B%22symboling%22%5D%2C%22exclude%22%3Afalse%7D%5D%7D\" \/minRatio \"0\" \/maxRatio \"1\" \/cleaningMode \"Replace with mean\" \/colsWithAllMissing \"Remove\" \/indicatorColumns \"False\" \/ContextFile \"...._context\\ContextFile.txt\"\"\n[Start] Program::Main\n[Start] DataLabModuleDescriptionParser::ParseModuleDescriptionString\n[Stop] DataLabModuleDescriptionParser::ParseModuleDescriptionString. Duration = 00:00:00.0024833\n[Start] DllModuleMethod::DllModuleMethod\n[Stop] DllModuleMethod::DllModuleMethod. Duration = 00:00:00.0000264\n[Start] DllModuleMethod::Execute\n[Start] DataLabModuleBinder::BindModuleMethod\n[Verbose] moduleMethodDescription Microsoft.Analytics.Modules.CleanMissingData.Dll, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.Analytics.Modules.CleanMissingData.Dll.CleanMissingData;Run\n[Verbose] assemblyFullName Microsoft.Analytics.Modules.CleanMissingData.Dll, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca\n[Start] DataLabModuleBinder::LoadModuleAssembly\n[Verbose] Loaded moduleAssembly Microsoft.Analytics.Modules.CleanMissingData.Dll, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca\n[Stop] DataLabModuleBinder::LoadModuleAssembly. Duration = 00:00:00.0090461\n[Verbose] moduleTypeName Microsoft.Analytics.Modules.CleanMissingData.Dll.CleanMissingData\n[Verbose] moduleMethodName Run\n[Information] Module FriendlyName : Clean Missing Data\n[Information] Module Release Status : Release\n[Stop] DataLabModuleBinder::BindModuleMethod. Duration = 00:00:00.0102317\n[Start] ParameterArgumentBinder::InitializeParameterValues\n[Verbose] parameterInfos count = 10\n[Verbose] parameterInfos[0] name = inputData , type = Microsoft.Numerics.Data.Local.DataTable\n[Start] DataTableCsvHandler::HandleArgumentString\n[Stop] DataTableCsvHandler::HandleArgumentString. Duration = 00:00:00.1604789\n[Verbose] parameterInfos[1] name = columnsToClean , type = Microsoft.Analytics.Modules.Common.Dll.ColumnSelection\n[Verbose] parameterInfos[2] name = minRatio , type = System.Double\n[Verbose] Converted string '0' to value of type System.Double\n[Verbose] parameterInfos[3] name = maxRatio , type = System.Double\n[Verbose] Converted string '1' to value of type System.Double\n[Verbose] parameterInfos[4] name = cleaningMode , type = Microsoft.Analytics.Modules.CleanMissingData.Dll.CleanMissingData+CleanMissingDataHandlingPolicy\n[Verbose] Converted string 'Replace with mean' to enum of type Microsoft.Analytics.Modules.CleanMissingData.Dll.CleanMissingData+CleanMissingDataHandlingPolicy\n[Verbose] parameterInfos[5] name = replacementValue , type = System.String\n[Verbose] Set optional parameter replacementValue value to NULL\n[Verbose] parameterInfos[6] name = colsWithAllMissing , type = Microsoft.Analytics.Modules.CleanMissingData.Dll.CleanMissingData+ColumnsWithAllValuesMissing\n[Verbose] Converted string 'Remove' to enum of type Microsoft.Analytics.Modules.CleanMissingData.Dll.CleanMissingData+ColumnsWithAllValuesMissing\n[Verbose] parameterInfos[7] name = indicatorColumns , type = System.Boolean\n[Verbose] Converted string 'False' to value of type System.Boolean\n[Verbose] parameterInfos[8] name = iterations , type = System.Int32\n[Verbose] Set optional parameter iterations value to NULL\n[Verbose] parameterInfos[9] name = iterationsPCA , type = System.Int32\n[Verbose] Set optional parameter iterationsPCA value to NULL\n[Stop] ParameterArgumentBinder::InitializeParameterValues. Duration = 00:00:00.4304001\n[Verbose] Begin invoking method Run ...\n[Verbose] End invoking method Run\n[Start] DataLabOutputManager::ManageModuleReturnValue\n[Verbose] moduleReturnType = System.Tuple`2[T1,T2]\n[Start] DataLabOutputManager::ConvertTupleOutputToFiles\n[Verbose] tupleType = System.Tuple`2[Microsoft.Numerics.Data.Local.DataTable,Microsoft.Analytics.MachineLearning.ITransform`2[Microsoft.Numerics.Data.Local.DataTable,Microsoft.Numerics.Data.Local.DataTable]]\n[Verbose] outputName Output0\n[Start] DataTableDatasetHandler::HandleOutput\n[Start] SidecarFiles::CreateVisualizationFiles\n[Information] Creating Cleaned dataset.visualization with key visualization...\n[Stop] SidecarFiles::CreateVisualizationFiles. Duration = 00:00:00.0704111\n[Start] SidecarFiles::CreateDatatableSchemaFile\n[Information] SidecarFiles::CreateDatatableSchemaFile creating \"....\\Cleaned dataset\\Cleaned dataset.schema\"\n[Stop] SidecarFiles::CreateDatatableSchemaFile. Duration = 00:00:00.0071185\n[Start] SidecarFiles::CreateMetadataFile\n[Information] SidecarFiles::CreateMetadataFile creating \"....\\Cleaned dataset\\Cleaned dataset.metadata\"\n[Stop] SidecarFiles::CreateMetadataFile. Duration = 00:00:00.0019639\n[Stop] DataTableDatasetHandler::HandleOutput. Duration = 00:00:00.1898522\n[Verbose] outputName Output1\n[Start] CustomSerializationHandler::HandleOutput\n[Start] DotNetSerializationHandler::HandleOutput\n[Start] SidecarFiles::CreateRuntimeInfoFile\n[Information] SidecarFiles::CreateRuntimeInfoFile creating \"....\\Cleaning transformation\\Cleaning transformation.runtimeinfo\"\n[Information] SidecarFileWritter::WriteRuntimeInfoToFile setting Language info for \"Microsoft.Analytics.Modules.CleanMissingData.Dll.CleaningMVTransform\"\n[ModuleOutput] SidecarFileWritter::WriteRuntimeInfoToFile setting Language info for \"Microsoft.Analytics.Modules.CleanMissingData.Dll.CleaningMVTransform\"\n[ModuleOutput] Setting Languge to DotNet.\n[Stop] SidecarFiles::CreateRuntimeInfoFile. Duration = 00:00:00.0016640\n[Start] SidecarFiles::CreateMetadataFile\n[Information] SidecarFiles::CreateMetadataFile creating \"....\\Cleaning transformation\\Cleaning transformation.metadata\"\n[Stop] SidecarFiles::CreateMetadataFile. Duration = 00:00:00.0003313\n[Stop] DotNetSerializationHandler::HandleOutput. Duration = 00:00:00.0045472\n[Stop] CustomSerializationHandler::HandleOutput. Duration = 00:00:00.0050090\n[Stop] DataLabOutputManager::ConvertTupleOutputToFiles. Duration = 00:00:00.2003385\n[Stop] DataLabOutputManager::ManageModuleReturnValue. Duration = 00:00:00.2017888\n[Verbose] {\"InputParameters\":{\"DataTable\":[{\"Rows\":205,\"Columns\":26,\"estimatedSize\":12574720,\"ColumnTypes\":{\"System.Int32\":5,\"System.Nullable`1[System.Int32]\":4,\"System.String\":10,\"System.Double\":5,\"System.Nullable`1[System.Double]\":2},\"IsComplete\":true,\"Statistics\":{\"0\":[0.8341463414634146,1.0,-2.0,3.0,1.2453068281055315,6.0,0.0],\"1\":[122.0,115.0,65.0,256.0,35.442167530553256,51.0,41.0],\"2\":[22,0],\"3\":[2,0],\"4\":[2,0],\"5\":[2,2],\"6\":[5,0],\"7\":[3,0],\"8\":[2,0],\"9\":[98.756585365853581,97.0,86.6,120.9,6.0217756850255721,53.0,0.0],\"10\":[174.04926829268285,173.2,141.1,208.1,12.337288526555183,75.0,0.0],\"11\":[65.907804878048722,65.5,60.3,72.3,2.1452038526871831,44.0,0.0],\"12\":[53.724878048780596,54.1,47.8,59.8,2.4435219699049036,49.0,0.0],\"13\":[2555.5658536585365,2414.0,1488.0,4066.0,520.68020350163874,171.0,0.0],\"14\":[7,0],\"15\":[7,0],\"16\":[126.90731707317073,120.0,61.0,326.0,41.642693438179847,44.0,0.0],\"17\":[8,0],\"18\":[3.3297512437810943,3.31,2.54,3.94,0.27353873182959904,38.0,4.0],\"19\":[3.2554228855721377,3.29,2.07,4.17,0.31671745337703111,36.0,4.0],\"20\":[10.142536585365862,9.0,7.0,23.0,3.9720403218632976,32.0,0.0],\"21\":[104.25615763546799,95.0,48.0,288.0,39.714368786793578,59.0,2.0],\"22\":[5125.3694581280788,5200.0,4150.0,6600.0,479.33455983341668,23.0,2.0],\"23\":[25.219512195121951,24.0,13.0,49.0,6.5421416530016216,29.0,0.0],\"24\":[30.751219512195121,30.0,16.0,54.0,6.886443130941827,30.0,0.0],\"25\":[13207.129353233831,10295.0,5118.0,45400.0,7947.066341939274,186.0,4.0]}}],\"Generic\":{\"columnsToClean\":\"{\\\"isFilter\\\":true,\\\"rules\\\":[{\\\"ruleType\\\":\\\"ColumnNames\\\",\\\"columns\\\":[\\\"symboling\\\"],\\\"exclude\\\":false}]}\",\"minRatio\":0.0,\"maxRatio\":1.0,\"cleaningMode\":\"ReplaceWithMean\",\"colsWithAllMissing\":\"Remove\",\"indicatorColumns\":false}},\"OutputParameters\":[\"Parameter with no known logging method, Microsoft.Analytics.Modules.CleanMissingData.Dll.CleaningMVTransform\",{\"Rows\":205,\"Columns\":26,\"estimatedSize\":0,\"ColumnTypes\":{\"System.Int32\":5,\"System.Nullable`1[System.Int32]\":4,\"System.String\":10,\"System.Double\":5,\"System.Nullable`1[System.Double]\":2},\"IsComplete\":true,\"Statistics\":{\"0\":[0.8341463414634146,1.0,-2.0,3.0,1.2453068281055315,6.0,0.0],\"1\":[122.0,115.0,65.0,256.0,35.442167530553256,51.0,41.0],\"2\":[22,0],\"3\":[2,0],\"4\":[2,0],\"5\":[2,2],\"6\":[5,0],\"7\":[3,0],\"8\":[2,0],\"9\":[98.756585365853581,97.0,86.6,120.9,6.0217756850255721,53.0,0.0],\"10\":[174.04926829268285,173.2,141.1,208.1,12.337288526555183,75.0,0.0],\"11\":[65.907804878048722,65.5,60.3,72.3,2.1452038526871831,44.0,0.0],\"12\":[53.724878048780596,54.1,47.8,59.8,2.4435219699049036,49.0,0.0],\"13\":[2555.5658536585365,2414.0,1488.0,4066.0,520.68020350163874,171.0,0.0],\"14\":[7,0],\"15\":[7,0],\"16\":[126.90731707317073,120.0,61.0,326.0,41.642693438179847,44.0,0.0],\"17\":[8,0],\"18\":[3.3297512437810943,3.31,2.54,3.94,0.27353873182959904,38.0,4.0],\"19\":[3.2554228855721377,3.29,2.07,4.17,0.31671745337703111,36.0,4.0],\"20\":[10.142536585365862,9.0,7.0,23.0,3.9720403218632976,32.0,0.0],\"21\":[104.25615763546799,95.0,48.0,288.0,39.714368786793578,59.0,2.0],\"22\":[5125.3694581280788,5200.0,4150.0,6600.0,479.33455983341668,23.0,2.0],\"23\":[25.219512195121951,24.0,13.0,49.0,6.5421416530016216,29.0,0.0],\"24\":[30.751219512195121,30.0,16.0,54.0,6.886443130941827,30.0,0.0],\"25\":[13207.129353233831,10295.0,5118.0,45400.0,7947.066341939274,186.0,4.0]}}],\"ModuleType\":\"Microsoft.Analytics.Modules.CleanMissingData.Dll\",\"ModuleVersion\":\" Version=6.0.0.0\",\"AdditionalModuleInfo\":\"Microsoft.Analytics.Modules.CleanMissingData.Dll, Version=6.0.0.0, Culture=neutral, PublicKeyToken=69c3241e6f0468ca;Microsoft.Analytics.Modules.CleanMissingData.Dll.CleanMissingData;Run\",\"Errors\":\"\",\"Warnings\":[],\"Duration\":\"00:00:00.7323002\"}\n[Stop] DllModuleMethod::Execute. Duration = 00:00:00.7557727\n[Stop] Program::Main. Duration = 00:00:00.8973350\nModule finished after a runtime of 00:00:00.9843864 with exit code 0\nExecution failed due to exception:taskStatusCode=400. Failed to upload W:\\jw\\e\\Cleaned dataset\\Cleaned dataset.dataset to Uri experimentoutput\/4a90c8cd-cc1d-4de0-97b2-aebb1285e140\/4a90c8cd-cc1d-4de0-97b2-aebb1285e140.dataset. This is an Azure storage request failure with status code 404. The request id is 5052585f-501e-000b-2933-acd43e000000 and the error message is The specified resource does not exist.. Possible reasons for such failure: (1) Invalid storage account or credential (2) Invalid SAS token (3) Concurrent jobs trying to upload files to the same blob at the same time. If those are not your case, please consider it as a transient error and retry.\n\n\nRecord Ends at UTC 09\/18\/2021 02:19:09.",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1631932138293,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/557185\/import-data-error-2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-20T05:59:40.243Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nThanks for reaching to us about this issue. I have the same issue because of my bad setting for cleaning mode.\n\nFor replace with mean:\nCalculates the column mean and uses the mean as the replacement value for each missing value in the column.\n\nApplies only to columns that have Integer, Double, or Boolean data types. See the Technical Notes section for more information.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/clean-missing-data#bkmk_TechNotes\n\nYou may contain other types of data.\n\nMy solution is I used \"MICE\" instead. Could you please check on your data types or change your mode?\n\nHope this helps.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-20T16:28:19.043Z",
                "Answer_score":0,
                "Answer_body":"Thank you YutongTie-MSFT but unfortunately that does not appear to be the case. Several new experiment attempts have been failing me lately and given the 400 code errors I'm noticing in the logs, I suspect the issue is on Microsoft's side.\n\nTo verify further, here I run the experiment again with only one numerical column selected:\n\n======\n\n======\n\n======\n\n======\n\nMind you, I'm using the basic Automobile price data (Raw) that appears under Sample Datasets. In review of my log file, I suspect this to be potentially the most relevant piece (emphasis mine):\n\n\n\n\nExecution failed due to exception:taskStatusCode=400. Failed to upload [...].dataset to Uri [...].dataset. This is an Azure storage request failure with status code 404. The request id is [...] and the error message is The specified resource does not exist.. Possible reasons for such failure: (1) Invalid storage account or credential (2) Invalid SAS token (3) Concurrent jobs trying to upload files to the same blob at the same time. If those are not your case, please consider it as a transient error and retry.\n\n\n\n\nQuestions for Follow Up:\n\nCan you run my experiment and verify it for you?\n\n\nIf functional, how do I check \"possible reasons\" (1), (2), and (3)?\n\n\nIf not, how do we escalate the issue?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-20T18:31:05.827Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI followed your settings but everything seems work well for me as below:\nReplace by mean:\n\n\nMICE:\n\n\nCould you please rebuild one to try? I am in South Central US.\n\n\nPlease let me know if you still have this issue.\n\nRegards,\nYutong",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"clean data error get error try clean data modul studio classic log tri log azur account avail wrong credenti record start utc run job dll microsoft analyt modul cleanmissingdata dll version cultur neutral publickeytoken cefca microsoft analyt modul cleanmissingdata dll cleanmissingdata run output clean dataset clean dataset dataset output clean transform clean transform itransform inputdata dataset dataset csv columnstoclean isfilt atru rule ruletyp columnnam column symbol exclud afals minratio maxratio cleaningmod replac mean colswithallmiss remov indicatorcolumn fals contextfil context contextfil txt start program main start datalabmoduledescriptionpars parsemoduledescriptionstr stop datalabmoduledescriptionpars parsemoduledescriptionstr durat start dllmodulemethod dllmodulemethod stop dllmodulemethod dllmodulemethod durat start dllmodulemethod execut start datalabmodulebind bindmodulemethod verbos modulemethoddescript microsoft analyt modul cleanmissingdata dll version cultur neutral publickeytoken cefca microsoft analyt modul cleanmissingdata dll cleanmissingdata run verbos assemblyfullnam microsoft analyt modul cleanmissingdata dll version cultur neutral publickeytoken cefca start datalabmodulebind loadmoduleassembl verbos load moduleassembl microsoft analyt modul cleanmissingdata dll version cultur neutral publickeytoken cefca stop datalabmodulebind loadmoduleassembl durat verbos moduletypenam microsoft analyt modul cleanmissingdata dll cleanmissingdata verbos modulemethodnam run inform modul friendlynam clean miss data inform modul releas statu releas stop datalabmodulebind bindmodulemethod durat start parameterargumentbind initializeparametervalu verbos parameterinfo count verbos parameterinfo inputdata type microsoft numer data local datat start datatablecsvhandl handleargumentstr stop datatablecsvhandl handleargumentstr durat verbos parameterinfo columnstoclean type microsoft analyt modul common dll columnselect verbos parameterinfo minratio type doubl verbos convert string valu type doubl verbos parameterinfo maxratio type doubl verbos convert string valu type doubl verbos parameterinfo cleaningmod type microsoft analyt modul cleanmissingdata dll cleanmissingdata cleanmissingdatahandlingpolici verbos convert string replac mean enum type microsoft analyt modul cleanmissingdata dll cleanmissingdata cleanmissingdatahandlingpolici verbos parameterinfo replacementvalu type string verbos set option paramet replacementvalu valu null verbos parameterinfo colswithallmiss type microsoft analyt modul cleanmissingdata dll cleanmissingdata columnswithallvaluesmiss verbos convert string remov enum type microsoft analyt modul cleanmissingdata dll cleanmissingdata columnswithallvaluesmiss verbos parameterinfo indicatorcolumn type boolean verbos convert string fals valu type boolean verbos parameterinfo iter type int verbos set option paramet iter valu null verbos parameterinfo iterationspca type int verbos set option paramet iterationspca valu null stop parameterargumentbind initializeparametervalu durat verbos begin invok method run verbos end invok method run start datalaboutputmanag managemodulereturnvalu verbos modulereturntyp tupl start datalaboutputmanag converttupleoutputtofil verbos tupletyp tupl microsoft numer data local datat microsoft analyt machinelearn itransform microsoft numer data local datat microsoft numer data local datat verbos outputnam output start datatabledatasethandl handleoutput start sidecarfil createvisualizationfil inform creat clean dataset visual kei visual stop sidecarfil createvisualizationfil durat start sidecarfil createdatatableschemafil inform sidecarfil createdatatableschemafil creat clean dataset clean dataset schema stop sidecarfil createdatatableschemafil durat start sidecarfil createmetadatafil inform sidecarfil createmetadatafil creat clean dataset clean dataset metadata stop sidecarfil createmetadatafil durat stop datatabledatasethandl handleoutput durat verbos outputnam output start customserializationhandl handleoutput start dotnetserializationhandl handleoutput start sidecarfil createruntimeinfofil inform sidecarfil createruntimeinfofil creat clean transform clean transform runtimeinfo inform sidecarfilewritt writeruntimeinfotofil set languag info microsoft analyt modul cleanmissingdata dll cleaningmvtransform moduleoutput sidecarfilewritt writeruntimeinfotofil set languag info microsoft analyt modul cleanmissingdata dll cleaningmvtransform moduleoutput set langug dotnet stop sidecarfil createruntimeinfofil durat start sidecarfil createmetadatafil inform sidecarfil createmetadatafil creat clean transform clean transform metadata stop sidecarfil createmetadatafil durat stop dotnetserializationhandl handleoutput durat stop customserializationhandl handleoutput durat stop datalaboutputmanag converttupleoutputtofil durat stop datalaboutputmanag managemodulereturnvalu durat verbos inputparamet datat row column estimateds columntyp int nullabl int string doubl nullabl doubl iscomplet true statist gener columnstoclean isfilt true rule ruletyp columnnam column symbol exclud fals minratio maxratio cleaningmod replacewithmean colswithallmiss remov indicatorcolumn fals outputparamet paramet known log method microsoft analyt modul cleanmissingdata dll cleaningmvtransform row column estimateds columntyp int nullabl int string doubl nullabl doubl iscomplet true statist moduletyp microsoft analyt modul cleanmissingdata dll modulevers version additionalmoduleinfo microsoft analyt modul cleanmissingdata dll version cultur neutral publickeytoken cefca microsoft analyt modul cleanmissingdata dll cleanmissingdata run error warn durat stop dllmodulemethod execut durat stop program main durat modul finish runtim exit code execut fail except taskstatuscod fail upload clean dataset clean dataset dataset uri experimentoutput accd ccd aebb accd ccd aebb dataset azur storag request failur statu code request acd error messag specifi resourc exist possibl reason failur invalid storag account credenti invalid sa token concurr job try upload file blob time case consid transient error retri record end utc",
        "Question_preprocessed_content":"clean data error get error try clean data modul studio log tri log azur account avail wrong credenti record start utc run cultur neutral isfilt atru rule ruletyp columnnam column symbol exclud afals replac mean remov fals start program main start datalabmoduledescriptionpars parsemoduledescriptionstr stop datalabmoduledescriptionpars parsemoduledescriptionstr durat start dllmodulemethod dllmodulemethod stop dllmodulemethod dllmodulemethod durat start dllmodulemethod execut start datalabmodulebind bindmodulemethod verbos modulemethoddescript cultur neutral verbos assemblyfullnam cultur neutral publickeytoken start datalabmodulebind loadmoduleassembl verbos load moduleassembl cultur neutral publickeytoken stop datalabmodulebind loadmoduleassembl durat verbos moduletypenam verbos modulemethodnam run inform modul friendlynam clean miss data inform modul releas statu releas stop datalabmodulebind bindmodulemethod durat start parameterargumentbind initializeparametervalu verbos parameterinfo count verbos parameterinfo inputdata type start datatablecsvhandl handleargumentstr stop datatablecsvhandl handleargumentstr durat verbos parameterinfo columnstoclean type verbos parameterinfo minratio type verbos convert string valu type verbos parameterinfo maxratio type verbos convert string valu type verbos parameterinfo cleaningmod type verbos convert string replac mean enum type verbos parameterinfo replacementvalu type verbos set option paramet replacementvalu valu null verbos parameterinfo colswithallmiss type verbos convert string remov enum type verbos parameterinfo indicatorcolumn type verbos convert string fals valu type verbos parameterinfo iter type verbos set option paramet iter valu null verbos parameterinfo iterationspca type verbos set option paramet iterationspca valu null stop parameterargumentbind initializeparametervalu durat verbos begin invok method run verbos end invok method run start datalaboutputmanag managemodulereturnvalu verbos modulereturntyp start datalaboutputmanag converttupleoutputtofil verbos tupletyp verbos outputnam output start datatabledatasethandl handleoutput start sidecarfil createvisualizationfil inform creat clean kei stop sidecarfil createvisualizationfil durat start sidecarfil createdatatableschemafil inform sidecarfil createdatatableschemafil creat stop sidecarfil createdatatableschemafil durat start sidecarfil createmetadatafil inform sidecarfil createmetadatafil creat stop sidecarfil createmetadatafil durat stop datatabledatasethandl handleoutput durat verbos outputnam output start customserializationhandl handleoutput start dotnetserializationhandl handleoutput start sidecarfil createruntimeinfofil inform sidecarfil createruntimeinfofil creat inform sidecarfilewritt writeruntimeinfotofil set languag info moduleoutput sidecarfilewritt writeruntimeinfotofil set languag info moduleoutput set langug dotnet stop sidecarfil createruntimeinfofil durat start sidecarfil createmetadatafil inform sidecarfil createmetadatafil creat stop sidecarfil createmetadatafil durat stop dotnetserializationhandl handleoutput durat stop customserializationhandl handleoutput durat stop datalaboutputmanag converttupleoutputtofil durat stop datalaboutputmanag managemodulereturnvalu durat verbos known log method cultur neutral stop dllmodulemethod execut durat stop program main durat modul finish runtim exit code execut fail except taskstatuscod fail upload uri azur storag request failur statu code request error messag specifi resourc possibl reason failur invalid storag account credenti invalid sa token concurr job try upload file blob time case consid transient error retri record end utc",
        "Question_gpt_summary_original":"The user encountered an error while trying to clean a data module in Azure Machine Learning Studio (classic). The error occurred while trying to upload the cleaned dataset to Azure storage, and the possible reasons for the failure include invalid storage account or credential, invalid SAS token, or concurrent jobs trying to upload files to the same blob at the same time.",
        "Question_gpt_summary":"user encount error try clean data modul studio classic error occur try upload clean dataset azur storag possibl reason failur includ invalid storag account credenti invalid sa token concurr job try upload file blob time",
        "Answer_original_content":"hello thank reach issu issu bad set clean mode replac mean calcul column mean us mean replac valu miss valu column appli column integ doubl boolean data type technic note section inform http doc microsoft com azur machin learn studio modul refer clean miss data bkmk technot contain type data solut mice instead check data type chang mode hope help regard yutong thank yutongti msft unfortun appear case new experi attempt fail late given code error notic log suspect issu microsoft verifi run experi numer column select mind basic automobil price data raw appear sampl dataset review log file suspect potenti relev piec emphasi execut fail except taskstatuscod fail upload dataset uri dataset azur storag request failur statu code request error messag specifi resourc exist possibl reason failur invalid storag account credenti invalid sa token concurr job try upload file blob time case consid transient error retri question follow run experi verifi function check possibl reason escal issu hello follow set work replac mean mice rebuild try south central let know issu regard yutong",
        "Answer_preprocessed_content":"hello thank reach issu issu bad set clean mode replac mean calcul column mean us mean replac valu miss valu column appli column integ doubl boolean data type technic note section inform contain type data solut mice instead check data type chang mode hope help regard yutong thank unfortun appear case new experi attempt fail late given code error notic log suspect issu microsoft verifi run experi numer column select mind basic automobil price data appear sampl dataset review log file suspect potenti relev piec execut fail except taskstatuscod fail upload dataset uri dataset azur storag request failur statu code request error messag specifi resourc possibl reason failur invalid storag account credenti invalid sa token concurr job try upload file blob time case consid transient error retri question follow run experi verifi function check possibl reason escal issu hello follow set work replac mean mice rebuild try south central let know issu regard yutong",
        "Answer_gpt_summary_original":"there are two possible solutions mentioned in the answer. the first solution is to check the data types or change the mode to \"mice\" if the error is due to bad settings for cleaning mode. the second solution is to verify the possible reasons for the 400 code errors, which could be due to invalid storage account or credential, invalid sas token, or concurrent jobs trying to upload files to the same blob at the same time. if none of these reasons apply, it could be a transient error and should be retried.",
        "Answer_gpt_summary":"possibl solut mention answer solut check data type chang mode mice error bad set clean mode second solut verifi possibl reason code error invalid storag account credenti invalid sa token concurr job try upload file blob time reason appli transient error retri"
    },
    {
        "Question_id":null,
        "Question_title":"Attaching custom image to user (not domain) in SageMaker Studio",
        "Question_body":"Is there a way to attach a custom image to just the user (not the domain) in SageMaker Studio.\n\nDocumentation states 'To make a custom SageMaker image available to all users within a domain, you attach the image to the domain. To make an image available to a single user, you attach the image to the user's profile.' https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-byoi.html\n\nWhen I 'edit user', I dont see a way to attach a custom image. Is there a way to do this?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607576593000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"You can attach custom images to user profiles via the APIs to create\/update user profiles.\n\nMore info:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateUserProfile.html https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateUserProfile.html",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUhP1jmxpAQi6X0eDIUI2JKA\/attaching-custom-image-to-user-not-domain-in-sage-maker-studio",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-10T09:36:50.000Z",
                "Answer_score":1,
                "Answer_body":"You can attach custom images to user profiles via the APIs to create\/update user profiles.\n\nMore info:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateUserProfile.html https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_UpdateUserProfile.html",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1607593010000,
        "Question_original_content":"attach custom imag user domain studio wai attach custom imag user domain studio document state custom imag avail user domain attach imag domain imag avail singl user attach imag user profil http doc aw amazon com latest studio byoi html edit user dont wai attach custom imag wai",
        "Question_preprocessed_content":"attach custom imag user studio wai attach custom imag user studio document state custom imag avail user domain attach imag domain imag avail singl user attach imag user edit user dont wai attach custom imag wai",
        "Question_gpt_summary_original":"The user is facing a challenge in attaching a custom image to a single user in SageMaker Studio, as the documentation only provides instructions for attaching images to domains or user profiles. The user has attempted to edit the user's profile but has not found a way to attach a custom image. The user is seeking assistance in finding a solution to this challenge.",
        "Question_gpt_summary":"user face challeng attach custom imag singl user studio document provid instruct attach imag domain user profil user attempt edit user profil wai attach custom imag user seek assist find solut challeng",
        "Answer_original_content":"attach custom imag user profil api creat updat user profil info http doc aw amazon com latest apirefer api createuserprofil html http doc aw amazon com latest apirefer api updateuserprofil html",
        "Answer_preprocessed_content":"attach custom imag user profil api user profil info",
        "Answer_gpt_summary_original":"possible solutions to attaching custom images to user profiles in studio include using the apis to create or update user profiles. more information on how to do this can be found in the provided links to the aws documentation.",
        "Answer_gpt_summary":"possibl solut attach custom imag user profil studio includ api creat updat user profil inform provid link aw document"
    },
    {
        "Question_id":37363883.0,
        "Question_title":"Azure Machine learning - Strip top X rows from dataset",
        "Question_body":"<p>I have a plain text csv file, which i am trying to read in Azure ML studio - the file format is pretty much like this<\/p>\n\n<pre><code>Geolife trajectory\nWGS 84\nAltitude is in Feet\nReserved 3\n0,2,255,My Track,0,0,2,8421376\n0\n39.984702,116.318417,0,492,39744.1201851852,2008-10-23,02:53:04\n39.984683,116.31845,0,492,39744.1202546296,2008-10-23,02:53:10\n39.984686,116.318417,0,492,39744.1203125,2008-10-23,02:53:15\n39.984688,116.318385,0,492,39744.1203703704,2008-10-23,02:53:20\n39.984655,116.318263,0,492,39744.1204282407,2008-10-23,02:53:25\n39.984611,116.318026,0,493,39744.1204861111,2008-10-23,02:53:30\n<\/code><\/pre>\n\n<p>The real data starts from Line 7, how could i strip it off, these files need to be downloaded on the fly so I don't think i would like to strip off the data by some code.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1463839280200,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Answer_body":"<p>What is your source location - SQL or Blob or http?<\/p>\n\n<p>If SQL, then you can use query to start from line 6.<\/p>\n\n<p>If Blob\/http, I would suggest reading a file as a single column TSV format, use simple R\/Python script to drop first 6 rows and convert to csv<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37363883",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1463946230187,
        "Question_original_content":"strip row dataset plain text csv file try read studio file format pretti like geolif trajectori wg altitud feet reserv track real data start line strip file need download fly think like strip data code",
        "Question_preprocessed_content":"strip row dataset plain text csv file try read studio file format pretti like real data start line strip file need download fly think like strip data code",
        "Question_gpt_summary_original":"The user is facing a challenge in reading a plain text csv file in Azure ML studio. The file contains some unnecessary data in the first few rows, and the user wants to strip it off without using any code as the files need to be downloaded on the fly.",
        "Question_gpt_summary":"user face challeng read plain text csv file studio file contain unnecessari data row user want strip code file need download fly",
        "Answer_original_content":"sourc locat sql blob http sql us queri start line blob http suggest read file singl column tsv format us simpl python script drop row convert csv",
        "Answer_preprocessed_content":"sourc locat sql blob http sql us queri start line suggest read file singl column tsv format us simpl script drop row convert csv",
        "Answer_gpt_summary_original":"the possible solutions to strip the top 6 rows from a plain text csv file are: \n1. if the source location is sql, use a query to start from line 6.\n2. if the source location is blob or http, read the file as a single column tsv format, use a simple r\/python script to drop the first 6 rows, and convert it to csv.",
        "Answer_gpt_summary":"possibl solut strip row plain text csv file sourc locat sql us queri start line sourc locat blob http read file singl column tsv format us simpl python script drop row convert csv"
    },
    {
        "Question_id":null,
        "Question_title":"How to register Managed SQL Instance as a datastore in Azure Machine Learning workspace?",
        "Question_body":"I am trying to connect a database within the Managed SQL Instance as a datastore. Managed Instance is not among the datastore type options so I tried to select the Azure SQL database and \"enter manually\" option for account selection. When I enter the full hostname of the managed instance which is something like \"xxx-xxxx-xxx-xxxx-sql.45bsa4569.database.windows.net\" it complaints due to dot \".\" that comes after 'sql'. Since I do not have control over that part of the hostname (Azure adds something like '.45bsa4569' on the managed instance name itself) I do not know how to fix it.\n\nQuestions:\n1. Is it possible to register a managed instance as a datastore by using the method above? (Azure SQL database and 'enter manually' options).\n2. If possible, how to handle the '.' in the host name so that it does not complain?\n\nFYI: The managed instance is behind a vnet so I am using service principal to authenticate.",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1603811671907,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/141428\/how-to-register-managed-sql-instance-as-a-datastor.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-28T12:16:20.347Z",
                "Answer_score":0,
                "Answer_body":"@EnginKapti-9217 Thanks for the question. Please follow the repo and snapshot.\n\nSupported data storage types:https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types\n\nAzure SQL MI is not officially supported per doc .\nIt's possible to create a datastore on Azure SQL MI, by providing connection string to API register_azure_sql_database, but unexpected behavior may occur.\n\nWe have forwarded to the product team and AzureML data team is looking into this issue, and will update if we find any work around.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-12T12:59:15.757Z",
                "Answer_score":1,
                "Answer_body":"@azure-machine-learning",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-12T12:59:24.057Z",
                "Answer_score":0,
                "Answer_body":"[azure-machine-learning]",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"regist manag sql instanc datastor workspac try connect databas manag sql instanc datastor manag instanc datastor type option tri select azur sql databas enter manual option account select enter hostnam manag instanc like sql bsa databas window net complaint dot come sql control hostnam azur add like bsa manag instanc know fix question possibl regist manag instanc datastor method azur sql databas enter manual option possibl handl host complain fyi manag instanc vnet servic princip authent",
        "Question_preprocessed_content":"regist manag sql instanc datastor workspac try connect databas manag sql instanc datastor manag instanc datastor type option tri select azur sql databas enter manual option account select enter hostnam manag instanc like complaint dot come sql control hostnam know fix question possibl regist manag instanc datastor method possibl handl host complain fyi manag instanc vnet servic princip authent",
        "Question_gpt_summary_original":"The user is facing challenges in registering a Managed SQL Instance as a datastore in Azure Machine Learning workspace. The Managed Instance is not among the datastore type options, and when the user tries to select the Azure SQL database and \"enter manually\" option for account selection, it complains due to the dot \".\" that comes after 'sql'. The user does not have control over that part of the hostname, and therefore, does not know how to fix it. The Managed Instance is behind a vnet, and the user is using a service principal to authenticate.",
        "Question_gpt_summary":"user face challeng regist manag sql instanc datastor workspac manag instanc datastor type option user tri select azur sql databas enter manual option account select complain dot come sql user control hostnam know fix manag instanc vnet user servic princip authent",
        "Answer_original_content":"enginkapti thank question follow repo snapshot support data storag type http doc microsoft com azur machin learn access data support data storag servic type azur sql offici support doc possibl creat datastor azur sql provid connect string api regist azur sql databas unexpect behavior occur forward product team data team look issu updat work azur machin learn azur machin learn",
        "Answer_preprocessed_content":"thank question follow repo snapshot support data storag azur sql offici support doc possibl creat datastor azur sql provid connect string api unexpect behavior occur forward product team data team look issu updat work",
        "Answer_gpt_summary_original":"possible solutions are not explicitly mentioned in the answer. however, the answer suggests that azure sql mi is not officially supported for registering as a datastore in  workspace. it is possible to create a datastore on azure sql mi by providing a connection string to the api register_azure_sql_database, but unexpected behavior may occur. the data team is looking into this issue and will update if they find any workarounds.",
        "Answer_gpt_summary":"possibl solut explicitli mention answer answer suggest azur sql offici support regist datastor workspac possibl creat datastor azur sql provid connect string api regist azur sql databas unexpect behavior occur data team look issu updat workaround"
    },
    {
        "Question_id":64170759.0,
        "Question_title":"Pyathena is super slow compared to querying from Athena",
        "Question_body":"<p>I run a query from AWS <strong>Athena console<\/strong> and takes 10s.\nThe same query run from <strong>Sagemaker<\/strong> using <strong>PyAthena<\/strong> takes 155s.\nIs PyAthena slowing it down or is the data transfer from Athena to sagemaker so time consuming?<\/p>\n<p>What could I do to speed this up?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601638106830,
        "Question_favorite_count":null,
        "Question_last_edit_time":1601641259400,
        "Question_score":3.0,
        "Question_view_count":3188.0,
        "Answer_body":"<p>Just figure out a way of boosting the queries:<\/p>\n<p>Before I was trying:<\/p>\n<pre><code>import pandas as pd\nfrom pyathena import connect\n\nconn = connect(s3_staging_dir=STAGIN_DIR,\n             region_name=REGION)\npd.read_sql(QUERY, conn)\n# takes 160s\n<\/code><\/pre>\n<p>Figured out that using a <em>PandasCursor<\/em> instead of a <em>connection<\/em> is way faster<\/p>\n<pre><code>import pandas as pd\npyathena import connect\nfrom pyathena.pandas.cursor import PandasCursor\n\ncursor = connect(s3_staging_dir=STAGIN_DIR,\n                 region_name=REGION,\n                 cursor_class=PandasCursor).cursor()\ndf = cursor.execute(QUERY).as_pandas()\n# takes 12s\n<\/code><\/pre>\n<p>Ref: <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46\" rel=\"noreferrer\">https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1629380324768,
        "Answer_score":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64170759",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1601639323903,
        "Question_original_content":"pyathena super slow compar queri athena run queri aw athena consol take queri run pyathena take pyathena slow data transfer athena time consum speed",
        "Question_preprocessed_content":"pyathena super slow compar queri athena run queri aw athena consol take queri run pyathena take pyathena slow data transfer athena time consum speed",
        "Question_gpt_summary_original":"The user is facing challenges with slow query performance when using PyAthena to query data from Athena compared to querying directly from the Athena console. The user is unsure whether PyAthena is causing the slowdown or if the data transfer from Athena to Sagemaker is the issue. The user is seeking advice on how to improve query performance.",
        "Question_gpt_summary":"user face challeng slow queri perform pyathena queri data athena compar queri directli athena consol user unsur pyathena caus slowdown data transfer athena issu user seek advic improv queri perform",
        "Answer_original_content":"figur wai boost queri try import panda pyathena import connect conn connect stage dir stagin dir region region read sql queri conn take figur pandascursor instead connect wai faster import panda pyathena import connect pyathena panda cursor import pandascursor cursor connect stage dir stagin dir region region cursor class pandascursor cursor cursor execut queri panda take ref http github com laughingman pyathena issu",
        "Answer_preprocessed_content":"figur wai boost queri try figur pandascursor instead connect wai faster ref",
        "Answer_gpt_summary_original":"the answer suggests using a pandas cursor instead of a connection to speed up the query process when using pyathena. the user can modify their code to use the pandas cursor and execute the query, which should significantly reduce the query time.",
        "Answer_gpt_summary":"answer suggest panda cursor instead connect speed queri process pyathena user modifi code us panda cursor execut queri significantli reduc queri time"
    },
    {
        "Question_id":null,
        "Question_title":"error - unable to find ggplot function",
        "Question_body":"Hi, could assist to advise me?\nR-script in Azure machine.\n\nerror message: unable to find ggplot function.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655169164353,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/887802\/error-unable-to-find-ggplot-function.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-14T01:55:55.307Z",
                "Answer_score":0,
                "Answer_body":"@DawnChin-3362 - Welcome to Microsoft Q&A and thanks for reaching out.\n\nIf you ever see the Error in ggplot(...): could not find function \"ggplot\", it suggests that this ggplot() function is not available because the package that holds the function (ggplot2) did not load with library(ggplot2).\n\nTherefore, you cannot utilize the ggplot() function without that ggplot2 package being loaded first.\n\nHope this helps. and please feel free to reach out if you have any further questions.\n\n\n\n\nIf the above response was helpful, please feel free to \"Accept as Answer\" and \"Upvote\" the same so it can be beneficial to the community.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error unabl ggplot function assist advis script azur machin error messag unabl ggplot function",
        "Question_preprocessed_content":"error unabl ggplot function assist advis azur machin error messag unabl ggplot function",
        "Question_gpt_summary_original":"The user encountered an error message while using an R-script in Azure machine, stating that the ggplot function could not be found.",
        "Question_gpt_summary":"user encount error messag script azur machin state ggplot function",
        "Answer_original_content":"dawnchin welcom microsoft thank reach error ggplot function ggplot suggest ggplot function avail packag hold function ggplot load librari ggplot util ggplot function ggplot packag load hope help feel free reach question respons help feel free accept answer upvot benefici commun",
        "Answer_preprocessed_content":"welcom microsoft thank reach error function ggplot suggest ggplot function avail packag hold function load librari util ggplot function ggplot packag load hope help feel free reach question respons help feel free accept answer upvot benefici commun",
        "Answer_gpt_summary_original":"the solution to the error message encountered when using the ggplot function in an r-script on an azure machine is to load the ggplot2 package with the library(ggplot2) command before using the ggplot() function.",
        "Answer_gpt_summary":"solut error messag encount ggplot function script azur machin load ggplot packag librari ggplot command ggplot function"
    },
    {
        "Question_id":null,
        "Question_title":"Deployemnt Time out error in AKS and Endpoint stuck in \"Transitioning\" state.",
        "Question_body":"Working on the deployment of 170 ML models using ML studio and azure Kubernetes service which is referred on the below doc link \"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-deploy-azure-kubernetes-service.md\".\n\nWe are training the model using python script with the custom environment and we are registering the ml model on the Azure ML services. Once we register the mode we are deploying it on the AKS by using the container images.\n\nWhile deploying the ML model we are able to deploy up to 10 to 11 models per pod for each Node in AKS. When we try to deploy the model on the same node we are getting deployment timeout error and we are getting the below error message.\n\n\n\n\nFor deploying the model in Azure Kubernetes Service using python language with below sample code.\n\n\n\n  #  Create an environment and add conda dependencies to it and for this creating our environment and building the custom container image.\n         myenv = Environment(name = Deployment_name)\n         myenv.python.conda_dependencies = CondaDependencies.create(pip_packages)\n        \n            \n     #  Inference_Conifiguration\n         inf_config = InferenceConfig(environment= myenv, entry_script='.\/Script_file.py')\n        \n        \n     # Deployment_Conifiguration\n         deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1, cpu_cores_limit = 2, memory_gb_limit = 2, traffic_percentile = 10)\n        \n     #  AKS cluster compute target \n         aks_target = ComputeTarget(ws, 'pipeline')\n           \n        \n    #  Deploying the model in AKS server\n           service = Model.deploy(ws, Deployment_name, model_1, inf_config,\n                       deployment_config, aks_target, overwrite=True)\n        \n            service.wait_for_deployment(show_output=True)\n\n\n\nWe also checked on the azure documentation and we could able to find any configuration or deployment setup for aks nodes.\n\n\n\n\nCan you please provide us more clarification regarding \"The number of models to be deployed is limited to 1,000 models per deployment (per container)\" and Can you please give insight\/feedback on how to increase the number of ml models that can be deployed in each node in Azure Kubernetes Service? Thanks!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630906705210,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/540740\/deployemnt-time-out-error-in-aks-and-endpoint-stuc.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-06T18:02:32.08Z",
                "Answer_score":1,
                "Answer_body":"Hello @DanMarculescu-1199 ,\nCan you kindly take a look at the similar post which was answered with relevant documentation .\nhttps:\/\/docs.microsoft.com\/en-us\/answers\/questions\/540001\/how-many-models-can-be-deployed-in-single-node-in.html\n\nLet us know if that helps !\n\nRegards,\nShiva.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":17.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deployemnt time error ak endpoint stuck transit state work deploy model studio azur kubernet servic refer doc link http github com microsoftdoc azur doc blob master articl machin learn deploi azur kubernet servic train model python script custom environ regist model servic regist mode deploi ak contain imag deploi model abl deploi model pod node ak try deploi model node get deploy timeout error get error messag deploi model azur kubernet servic python languag sampl code creat environ add conda depend creat environ build custom contain imag myenv environ deploy myenv python conda depend condadepend creat pip packag infer conifigur inf config inferenceconfig environ myenv entri script script file deploy conifigur deploy config akswebservic deploi configur cpu core memori cpu core limit memori limit traffic percentil ak cluster comput target ak target computetarget pipelin deploi model ak server servic model deploi deploy model inf config deploy config ak target overwrit true servic wait deploy output true check azur document abl configur deploy setup ak node provid clarif number model deploi limit model deploy contain insight feedback increas number model deploi node azur kubernet servic thank",
        "Question_preprocessed_content":"deployemnt time error ak endpoint stuck transit state work deploy model studio azur kubernet servic refer doc link train model python script custom environ regist model servic regist mode deploi ak contain imag deploi model abl deploi model pod node ak try deploi model node get deploy timeout error get error messag deploi model azur kubernet servic python languag sampl code creat environ add conda depend creat environ build custom contain imag myenv environ inferenceconfig ak cluster comput target computetarget deploi model ak server servic overwrit true check azur document abl configur deploy setup ak node provid clarif number model deploi limit model deploy increas number model deploi node azur kubernet servic thank",
        "Question_gpt_summary_original":"The user is facing challenges while deploying 170 ML models using ML studio and Azure Kubernetes Service. They are able to deploy up to 10-11 models per pod for each node in AKS, but when they try to deploy more models on the same node, they encounter a deployment timeout error. The user is seeking clarification on the limit of 1,000 models per deployment (per container) and is looking for feedback on how to increase the number of ML models that can be deployed in each node in Azure Kubernetes Service.",
        "Question_gpt_summary":"user face challeng deploi model studio azur kubernet servic abl deploi model pod node ak try deploi model node encount deploy timeout error user seek clarif limit model deploy contain look feedback increas number model deploi node azur kubernet servic",
        "Answer_original_content":"hello danmarculescu kindli look similar post answer relev document http doc microsoft com answer question model deploi singl node html let know help regard shiva",
        "Answer_preprocessed_content":"hello kindli look similar post answer relev document let know help regard shiva",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer, but the responder suggests checking out a similar post that was answered with relevant documentation. the documentation may provide helpful information on how to deploy a large number of ml models using azure kubernetes service without encountering deployment time out errors and endpoint stuck in \"transitioning\" state.",
        "Answer_gpt_summary":"specif solut provid answer respond suggest check similar post answer relev document document provid help inform deploi larg number model azur kubernet servic encount deploy time error endpoint stuck transit state"
    },
    {
        "Question_id":null,
        "Question_title":"Databricks mlflow model serving networking",
        "Question_body":"Hi there, I'm trying to register a ML model to the mlflow model registry to be served from an Azure web app. I'm wondering if the databricks networking configuration will apply to the model api endpoint, as in, calls to the api from outside the VNET which the databricks is deployed to with private endpoints and disabled public access will be rejected and the call from within the vnet integrated web app will be successful?\n\nThank you!",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1662480716897,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hello @ynh-9693,\n\nThanks for the question and using MS Q&A platform.\n\nI'm wondering if the databricks networking configuration will apply to the model api endpoint\n\nYes, it will. The single node cluster on which the model is hosted (classic), is deployed in data plane and will have a private IP.\n\n\nI should be able to call the model with the url <databricks-instance>\/model\/<registered-model-name>\/<model-version>\/invocations, my question is whether this url will have the same restrictions as the databricks where it resides\n\nI believe, this should work as long as you have not defined any IP access list. The PAT will let you authenticate.\n\nbut I can't find information for IP restrictions\n\nIP access lists - Azure Databricks | Microsoft Docs\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is jhow you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/996163\/databricks-mlflow-model-serving-networking.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-13T05:14:35.633Z",
                "Answer_score":1,
                "Answer_body":"Hello @ynh-9693,\n\nThanks for the question and using MS Q&A platform.\n\nI'm wondering if the databricks networking configuration will apply to the model api endpoint\n\nYes, it will. The single node cluster on which the model is hosted (classic), is deployed in data plane and will have a private IP.\n\n\nI should be able to call the model with the url <databricks-instance>\/model\/<registered-model-name>\/<model-version>\/invocations, my question is whether this url will have the same restrictions as the databricks where it resides\n\nI believe, this should work as long as you have not defined any IP access list. The PAT will let you authenticate.\n\nbut I can't find information for IP restrictions\n\nIP access lists - Azure Databricks | Microsoft Docs\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is jhow you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":30.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663046075632,
        "Question_original_content":"databrick model serv network try regist model model registri serv azur web app wonder databrick network configur appli model api endpoint call api outsid vnet databrick deploi privat endpoint disabl public access reject vnet integr web app success thank",
        "Question_preprocessed_content":"databrick model serv network try regist model model registri serv azur web app wonder databrick network configur appli model api endpoint call api outsid vnet databrick deploi privat endpoint disabl public access reject vnet integr web app success thank",
        "Question_gpt_summary_original":"The user is facing challenges in registering an ML model to the mlflow model registry to be served from an Azure web app. They are unsure if the databricks networking configuration will apply to the model API endpoint, and if calls to the API from outside the VNET will be rejected while calls from within the VNET integrated web app will be successful.",
        "Question_gpt_summary":"user face challeng regist model model registri serv azur web app unsur databrick network configur appli model api endpoint call api outsid vnet reject call vnet integr web app success",
        "Answer_original_content":"hello ynh thank question platform wonder databrick network configur appli model api endpoint ye singl node cluster model host classic deploi data plane privat abl model url model invoc question url restrict databrick resid believ work long defin access list pat let authent inform restrict access list azur databrick microsoft doc hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur jhow volunt moder",
        "Answer_preprocessed_content":"hello thank question platform wonder databrick network configur appli model api endpoint ye singl node cluster model host deploi data plane privat abl model url question url restrict databrick resid believ work long defin access list pat let authent inform restrict access list azur databrick microsoft doc hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur jhow volunt moder",
        "Answer_gpt_summary_original":"the answer suggests that the databricks networking configuration will apply to the model api endpoint. the model is hosted on a single node cluster with a private ip, and the url to call the model should be \/model\/\/\/invocations. the answer also mentions that ip access lists may restrict access, but there is no information available on this. the answer provides a link to azure databricks documentation for more information.",
        "Answer_gpt_summary":"answer suggest databrick network configur appli model api endpoint model host singl node cluster privat url model model invoc answer mention access list restrict access inform avail answer provid link azur databrick document inform"
    },
    {
        "Question_id":null,
        "Question_title":"How to fix database disk image is malform",
        "Question_body":"<p>Hi,<br>\nHow to fix in this errot:<br>\nERROR: unexpected error - database disk image is malformed<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1614622947594,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":276.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-fix-database-disk-image-is-malform\/691",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-01T18:24:28.464Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/merve\">@merve<\/a> . Please post <code>$ dvc doctor<\/code> output and add <code>-v<\/code> to the command that gave you that error and post full log for it.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-01T18:29:26.396Z",
                "Answer_body":"<p>\u2026<br>\n2021-03-01 18:26:19,932 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,932 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,932 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,933 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,933 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,933 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,933 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,933 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,933 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,934 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,934 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,934 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,934 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/fd\/bcaeb97ec78f1f769ed86daaa47f5b\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,934 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/34\/8b4f28c78afc5c8bd17a4c44bca6f4\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,934 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/50\/125ec853355732ca7ba36dfbee759e\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,935 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/b2\/7d2a2599b431b51c456ab828300dd4\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,935 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/c7\/fc45a4f47a6764f1efb1376c5d0d56\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,935 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/c1\/fa6762bd8a289d8f84b86de6a99d30\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,935 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/b5\/4a2a2c78cb87324be68119258146a9\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,935 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/73\/12baf37cc83f4b19dcfc4d0884ba7d\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,935 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/3d\/6233dc79c74063b7bd9ea90eee5300\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,935 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/97\/1524f85e560709b71ffe28dfc6014c\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,936 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,936 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,936 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,936 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,936 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,936 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,937 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,937 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,937 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,937 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,937 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,937 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/ae\/385509503eb52989789d5627a3c3aa\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,938 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,938 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,938 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,938 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/f9\/1a03e0e8cd30be73e407771515a1ac\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,938 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/04\/a312b03bb10807962e3d21e0b2345d\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,938 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/b1\/3ddd277a4d00eba53080e783d9e1c0\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,939 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/77\/379a250fc5405c4ff09f28448bc0b7\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,939 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/57\/cd0fd0340d8f16ba3a929bc84a3d67\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,939 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/61\/b8bc15abefa474dc41678c4921f40a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,939 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/61\/b8bc15abefa474dc41678c4921f40a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,939 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/61\/b8bc15abefa474dc41678c4921f40a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,939 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/61\/b8bc15abefa474dc41678c4921f40a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,940 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/61\/b8bc15abefa474dc41678c4921f40a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,940 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/61\/b8bc15abefa474dc41678c4921f40a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,940 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/61\/b8bc15abefa474dc41678c4921f40a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,940 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/61\/b8bc15abefa474dc41678c4921f40a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,940 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/61\/b8bc15abefa474dc41678c4921f40a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,940 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/45\/7469cf0c84dff9fcf1aa5619db16d1\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:19,941 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/28\/a368342a05839b68282a10c4df55f0\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:22,134 DEBUG: Path \u2018\/home\/issd\/dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\u2019 inode \u2018704446465\u2019<br>\n2021-03-01 18:26:22,135 DEBUG: fetched: [(\u201868b163ddca4cab63fbb57661fce485da\u2019, \u20181256073424\u2019, \u20187b06542fa765ea71f585c8f0a7ff401e.dir\u2019, \u20181614622753215537408\u2019)]<br>\n2021-03-01 18:26:22,139 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/7b\/06542fa765ea71f585c8f0a7ff401e.dir\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:22,142 DEBUG: Saving \u2018dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\u2019 to \u2018.dvc\/cache\/7b\/06542fa765ea71f585c8f0a7ff401e.dir\u2019.<br>\n2021-03-01 18:26:22,143 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/7b\/06542fa765ea71f585c8f0a7ff401e.dir\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:22,143 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/a1\/4241ccf2999bc5c8ec3c7096ea7d0a\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:22,262 DEBUG: Cache type \u2018reflink\u2019 is not supported: reflink is not supported<br>\n2021-03-01 18:26:22,264 DEBUG: Created \u2018copy\u2019: .dvc\/cache\/.cache_type_test_file \u2192 dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/.EPYcYwH5t94MmHMwKib8J6<br>\n2021-03-01 18:26:22,265 DEBUG: Removing \u2018\/home\/issd\/dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/.EPYcYwH5t94MmHMwKib8J6\u2019<br>\n2021-03-01 18:26:22,265 DEBUG: Removing \u2018\/home\/issd\/.dvc\/cache\/.cache_type_test_file\u2019<br>\n2021-03-01 18:26:22,265 DEBUG: Skipping copying for \u2018\/home\/issd\/dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/2.avi\u2019, since it is not a symlink or a hardlink.<br>\n2021-03-01 18:26:22,266 DEBUG: Path \u2018dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/2.avi\u2019 inode \u2018704446468\u2019<br>\n2021-03-01 18:26:22,266 DEBUG: fetched: [(\u20181614611664072704256\u2019, \u20181048658318\u2019, \u2018a14241ccf2999bc5c8ec3c7096ea7d0a\u2019, \u20181614622753226607616\u2019)]<br>\n2021-03-01 18:26:22,267 DEBUG: Path \u2018.dvc\/cache\/a1\/4241ccf2999bc5c8ec3c7096ea7d0a\u2019 inode \u2018271620\u2019<br>\n2021-03-01 18:26:22,267 DEBUG: fetched: [(\u20181614173000334131200\u2019, \u20181048658318\u2019, \u2018a14241ccf2999bc5c8ec3c7096ea7d0a\u2019, \u20181614622753226980352\u2019)]<br>\n2021-03-01 18:26:22,267 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/bb\/edcd847daadf290f1277ab78cab0b0\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:22,268 DEBUG: Skipping copying for \u2018\/home\/issd\/dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/SC_02.json\u2019, since it is not a symlink or a hardlink.<br>\n2021-03-01 18:26:22,268 DEBUG: Path \u2018dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/SC_02.json\u2019 inode \u2018704446469\u2019<br>\n2021-03-01 18:26:22,268 DEBUG: fetched: [(\u20181614611664120704512\u2019, \u20181536261\u2019, \u2018bbedcd847daadf290f1277ab78cab0b0\u2019, \u20181614622753227555584\u2019)]<br>\n2021-03-01 18:26:22,269 DEBUG: Path \u2018.dvc\/cache\/bb\/edcd847daadf290f1277ab78cab0b0\u2019 inode \u2018271622\u2019<br>\n2021-03-01 18:26:22,269 DEBUG: fetched: [(\u20181614173000390131968\u2019, \u20181536261\u2019, \u2018bbedcd847daadf290f1277ab78cab0b0\u2019, \u20181614622753227661056\u2019)]<br>\n2021-03-01 18:26:22,269 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/02\/6f88e275bf4909df069d793c2e60f2\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:22,269 DEBUG: Skipping copying for \u2018\/home\/issd\/dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/images\/PED-SC-02_V1-frame10000.jpg\u2019, since it is not a symlink or a hardlink.<br>\n2021-03-01 18:26:22,270 DEBUG: Path \u2018dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/images\/PED-SC-02_V1-frame10000.jpg\u2019 inode \u2018779321500\u2019<br>\n2021-03-01 18:26:22,270 DEBUG: fetched: [(\u20181614611664260704000\u2019, \u2018128359\u2019, \u2018026f88e275bf4909df069d793c2e60f2\u2019, \u20181614622753228241152\u2019)]<br>\n2021-03-01 18:26:22,271 DEBUG: Path \u2018.dvc\/cache\/02\/6f88e275bf4909df069d793c2e60f2\u2019 inode \u2018271623\u2019<br>\n2021-03-01 18:26:22,271 DEBUG: fetched: [(\u20181614173000402131968\u2019, \u2018128359\u2019, \u2018026f88e275bf4909df069d793c2e60f2\u2019, \u20181614622753228406784\u2019)]<br>\n2021-03-01 18:26:22,271 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/7c\/4967cfff4e8bfac7c9562fdccfb1bc\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:22,271 DEBUG: Skipping copying for \u2018\/home\/issd\/dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/images\/PED-SC-02_V1-frame100000.jpg\u2019, since it is not a symlink or a hardlink.<br>\n2021-03-01 18:26:22,272 DEBUG: Path \u2018dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/images\/PED-SC-02_V1-frame100000.jpg\u2019 inode \u2018779321501\u2019<br>\n2021-03-01 18:26:22,272 DEBUG: fetched: [(\u20181614611664268704000\u2019, \u2018134209\u2019, \u20187c4967cfff4e8bfac7c9562fdccfb1bc\u2019, \u20181614622753228962304\u2019)]<br>\n2021-03-01 18:26:22,272 DEBUG: Path \u2018.dvc\/cache\/7c\/4967cfff4e8bfac7c9562fdccfb1bc\u2019 inode \u2018271624\u2019<br>\n2021-03-01 18:26:22,273 DEBUG: fetched: [(\u20181614173000414132480\u2019, \u2018134209\u2019, \u20187c4967cfff4e8bfac7c9562fdccfb1bc\u2019, \u20181614622753229066240\u2019)]<br>\n2021-03-01 18:26:22,273 DEBUG: Assuming \u2018\/home\/issd\/.dvc\/cache\/b6\/239e20a793a9d496d24dd3778495c9\u2019 is unchanged since it is read-only<br>\n2021-03-01 18:26:22,274 DEBUG: Skipping copying for \u2018\/home\/issd\/dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/images\/PED-SC-02_V1-frame100100.jpg\u2019, since it is not a symlink or a hardlink.<br>\n2021-03-01 18:26:22,274 DEBUG: Path \u2018dataset\/DATASETS\/READY\/TRAIN\/PEDESTRIAN\/SMARTCARD\/VERSION1\/02\/images\/PED-SC-02_V1-frame100100.jpg\u2019 inode \u2018779321502\u2019<br>\n2021-03-01 18:26:22,275 DEBUG: fetched: [(5710,)]<br>\n2021-03-01 18:26:22,371 ERROR: unexpected error - database disk image is malformed<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-08T11:12:00.517Z",
                "Answer_body":"<p>Deleting tmp folder fixed this issue for me<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"fix databas disk imag malform fix errot error unexpect error databas disk imag malform",
        "Question_preprocessed_content":"fix databas disk imag malform fix errot error unexpect error databas disk imag malform",
        "Question_gpt_summary_original":"The user is encountering an error message stating \"database disk image is malformed\" and is seeking guidance on how to fix it.",
        "Question_gpt_summary":"user encount error messag state databas disk imag malform seek guidanc fix",
        "Answer_original_content":"merv post doctor output add command gave error post log debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bcaebecffeddfb unchang read debug assum home issd cach bfcafccbdacbcaf unchang read debug assum home issd cach eccabadfbee unchang read debug assum home issd cach dabbcabdd unchang read debug assum home issd cach fcafafefbcdd unchang read debug assum home issd cach fabdadfbdead unchang read debug assum home issd cach aaccbbea unchang read debug assum home issd cach bafccfbdcfcdbad unchang read debug assum home issd cach dccbbdea unchang read debug assum home issd cach febffedfcc unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach ebdacaa unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach aeecdbeeaac unchang read debug assum home issd cach abbbedebd unchang read debug assum home issd cach adebaedec unchang read debug assum home issd cach afccfffbcb unchang read debug assum home issd cach cdfddfbaabcad unchang read debug assum home issd cach bbcabefadccfa unchang read debug assum home issd cach bbcabefadccfa unchang read debug assum home issd cach bbcabefadccfa unchang read debug assum home issd cach bbcabefadccfa unchang read debug assum home issd cach bbcabefadccfa unchang read debug assum home issd cach bbcabefadccfa unchang read debug assum home issd cach bbcabefadccfa unchang read debug assum home issd cach bbcabefadccfa unchang read debug assum home issd cach bbcabefadccfa unchang read debug assum home issd cach cfcdfffcfaadbd unchang read debug assum home issd cach aabacdff unchang read debug path home issd dataset dataset readi train pedestrian smartcard version inod debug fetch bddcacabfbbfceda bfaeafcfaff dir debug assum home issd cach faeafcfaff dir unchang read debug save dataset dataset readi train pedestrian smartcard version cach faeafcfaff dir debug assum home issd cach faeafcfaff dir unchang read debug assum home issd cach ccfbccecceada unchang read debug cach type reflink support reflink support debug creat copi cach cach type test file dataset dataset readi train pedestrian smartcard version epycywhtmmhmwkibj debug remov home issd dataset dataset readi train pedestrian smartcard version epycywhtmmhmwkibj debug remov home issd cach cach type test file debug skip copi home issd dataset dataset readi train pedestrian smartcard version avi symlink hardlink debug path dataset dataset readi train pedestrian smartcard version avi inod debug fetch accfbccecceada debug path cach ccfbccecceada inod debug fetch accfbccecceada debug assum home issd cach edcddaadffabcabb unchang read debug skip copi home issd dataset dataset readi train pedestrian smartcard version json symlink hardlink debug path dataset dataset readi train pedestrian smartcard version json inod debug fetch bbedcddaadffabcabb debug path cach edcddaadffabcabb inod debug fetch bbedcddaadffabcabb debug assum home issd cach febfdfdcef unchang read debug skip copi home issd dataset dataset readi train pedestrian smartcard version imag ped frame jpg symlink hardlink debug path dataset dataset readi train pedestrian smartcard version imag ped frame jpg inod debug fetch febfdfdcef debug path cach febfdfdcef inod debug fetch febfdfdcef debug assum home issd cach cebfaccfdccfbbc unchang read debug skip copi home issd dataset dataset readi train pedestrian smartcard version imag ped frame jpg symlink hardlink debug path dataset dataset readi train pedestrian smartcard version imag ped frame jpg inod debug fetch ccebfaccfdccfbbc debug path cach cebfaccfdccfbbc inod debug fetch ccebfaccfdccfbbc debug assum home issd cach eaaddddc unchang read debug skip copi home issd dataset dataset readi train pedestrian smartcard version imag ped frame jpg symlink hardlink debug path dataset dataset readi train pedestrian smartcard version imag ped frame jpg inod debug fetch error unexpect error databas disk imag malform delet tmp folder fix issu",
        "Answer_preprocessed_content":"post output add command gave error post log debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug assum unchang debug path inod debug fetch debug assum unchang debug save debug assum unchang debug assum unchang debug cach type reflink support reflink support debug creat copi debug remov debug remov debug skip copi symlink hardlink debug path inod debug fetch debug path inod debug fetch debug assum unchang debug skip copi symlink hardlink debug path inod debug fetch debug path inod debug fetch debug assum unchang debug skip copi symlink hardlink debug path inod debug fetch debug path inod debug fetch debug assum unchang debug skip copi symlink hardlink debug path inod debug fetch debug path inod debug fetch debug assum unchang debug skip copi symlink hardlink debug path inod debug fetch error unexpect error databas disk imag malform delet tmp folder fix issu",
        "Answer_gpt_summary_original":"possible solutions:\n- posting the $ doctor output and adding -v to the command that gave the error and posting the full log for it.\n- deleting the tmp folder.",
        "Answer_gpt_summary":"possibl solut post doctor output ad command gave error post log delet tmp folder"
    },
    {
        "Question_id":55257580.0,
        "Question_title":"SageMaker NodeJS's SDK is not locking the API Version",
        "Question_body":"<p>I am running some code in AWS Lambda that dynamically creates SageMaker models.\nI am locking Sagemaker's API version like so:<\/p>\n\n<p><code>const sagemaker = new AWS.SageMaker({apiVersion: '2017-07-24'});<\/code><\/p>\n\n<p>And here's the code to create the model:<\/p>\n\n<pre><code>await sagemaker.createModel({\n        ExecutionRoleArn: 'xxxxxx',\n        ModelName: sageMakerConfigId,\n        Containers: [{\n            Image: ecrUrl\n        }]\n    }).promise()\n<\/code><\/pre>\n\n<p>This code runs just fine locally with <code>aws-sdk<\/code> on <code>2.418.0<\/code>. <\/p>\n\n<p>However, when this code is deployed to Lambda, it doesn't work due to some validation errors upon creating the model:<\/p>\n\n<blockquote>\n  <ul>\n  <li>MissingRequiredParameter: Missing required key 'PrimaryContainer' in params<\/li>\n  <li>UnexpectedParameter: Unexpected key 'Containers' found in params<\/li>\n  <\/ul>\n<\/blockquote>\n\n<p>Is anyone aware of existing bugs in the <code>aws-sdk<\/code> for NodeJS using the SDK provided by AWS in the Lambda context? I believe the SDK available inside AWS Lambda is more up-to-date than <code>2.418.0<\/code> but apparently there are compatibility issues.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1553074522987,
        "Question_favorite_count":null,
        "Question_last_edit_time":1553082788147,
        "Question_score":1.0,
        "Question_view_count":277.0,
        "Answer_body":"<p>As you've noticed the 'embedded' lambda version of the aws-sdk lags behind. It's actually on <code>2.290.0<\/code> (you can see the full details on the environment here: <a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/current-supported-versions.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/current-supported-versions.html<\/a>)<\/p>\n\n<p>You can see here: <a href=\"https:\/\/github.com\/aws\/aws-sdk-js\/blame\/master\/clients\/sagemaker.d.ts\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/aws-sdk-js\/blame\/master\/clients\/sagemaker.d.ts<\/a> that it is not until <code>2.366.0<\/code> that the params for this method included <code>Containers<\/code> and did not require <code>PrimaryContainer<\/code>.<\/p>\n\n<p>As you've noted, the <em>workaround<\/em> is to deploy your lambda with the <code>aws-sdk<\/code> version that you're using. This is sometimes noted as a best practice, as it pins the <code>aws-sdk<\/code> on the functionality you've built and tested against.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55257580",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1553117574836,
        "Question_original_content":"nodej sdk lock api version run code aw lambda dynam creat model lock api version like const new aw apivers code creat model await createmodel executionrolearn modelnam configid contain imag ecrurl promis code run fine local aw sdk code deploi lambda work valid error creat model missingrequiredparamet miss requir kei primarycontain param unexpectedparamet unexpect kei contain param awar exist bug aw sdk nodej sdk provid aw lambda context believ sdk avail insid aw lambda date appar compat issu",
        "Question_preprocessed_content":"nodej sdk lock api version run code aw lambda dynam creat model lock api version like code creat model code run fine local code deploi lambda work valid error creat model missingrequiredparamet miss requir kei primarycontain param unexpectedparamet unexpect kei contain param awar exist bug nodej sdk provid aw lambda context believ sdk avail insid aw lambda appar compat issu",
        "Question_gpt_summary_original":"The user is encountering validation errors when creating SageMaker models in AWS Lambda using the AWS SDK for NodeJS. The errors include missing required parameters and unexpected parameters. The user suspects that the SDK available inside AWS Lambda is more up-to-date than the version they are using locally, causing compatibility issues.",
        "Question_gpt_summary":"user encount valid error creat model aw lambda aw sdk nodej error includ miss requir paramet unexpect paramet user suspect sdk avail insid aw lambda date version local caus compat issu",
        "Answer_original_content":"notic embed lambda version aw sdk lag actual detail environ http doc aw amazon com lambda latest current support version html http github com aw aw sdk blame master client param method includ contain requir primarycontain note workaround deploi lambda aw sdk version note best practic pin aw sdk function built test",
        "Answer_preprocessed_content":"notic embed lambda version lag actual param method includ requir note workaround deploi lambda version note best practic pin function built test",
        "Answer_gpt_summary_original":"the solution to encountering validation errors when running code in aws lambda that dynamically creates models is to deploy the lambda with the aws sdk version that was used during development and testing. this is considered a best practice as it ensures that the aws sdk is pinned to the functionality that was built and tested against. the embedded lambda version of the aws sdk lags behind and may not include the necessary parameters for certain methods.",
        "Answer_gpt_summary":"solut encount valid error run code aw lambda dynam creat model deploi lambda aw sdk version develop test consid best practic ensur aw sdk pin function built test embed lambda version aw sdk lag includ necessari paramet certain method"
    },
    {
        "Question_id":null,
        "Question_title":"How to provide fold information in WandbCallback?",
        "Question_body":"<p>How to provide fold information in multi-fold training?<\/p>\n<p>I am trying to do something like this:<\/p>\n<pre><code class=\"lang-auto\">for fold in range(5):\nmodel.fit()\n<\/code><\/pre>\n<p>So it actually creates a single graph where steps are continued from the last executed step of previous epoch.<br>\nFor ref see below:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5.png\" data-download-href=\"\/uploads\/short-url\/bUQfG2SdqWgNhB5QoDwBGjbyy3P.png?dl=1\" title=\"Screenshot 2021-12-23 at 12.19.28 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5_2_690x382.png\" alt=\"Screenshot 2021-12-23 at 12.19.28 PM\" data-base62-sha1=\"bUQfG2SdqWgNhB5QoDwBGjbyy3P\" width=\"690\" height=\"382\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5_2_690x382.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5_2_1035x573.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2021-12-23 at 12.19.28 PM<\/span><span class=\"informations\">1098\u00d7608 28.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1640242246866,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":214.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-provide-fold-information-in-wandbcallback\/1599",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-26T17:38:34.237Z",
                "Answer_body":"<p>I am thinking about this as well. I am about to give each fold a run name in wandb but not sure it is a good practice.<\/p>",
                "Answer_score":2.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-27T18:09:52.185Z",
                "Answer_body":"<p>Not sure if I fully understand the question, you\u2019d like to train on different folds, but continue training the model, as opposed to training a fresh model on each fold?<\/p>\n<p>If you\u2019re resuming the run you could update a <code>fold<\/code> config variable at the beginning of each for loop, so you would know the latest fold the model is training on?<\/p>\n<pre><code class=\"lang-auto\">for fold in range(5):\n    wandb.config.update({'fold':fold})\n    model.fit()\n\n<\/code><\/pre>\n<p>What other fold information were you hoping to log?<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-04T13:05:22.890Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"harveenchadha\" data-post=\"1\" data-topic=\"1599\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sea2.discourse-cdn.com\/business7\/user_avatar\/community.wandb.ai\/harveenchadha\/40\/575_2.png\" class=\"avatar\"> harveenchadha:<\/div>\n<blockquote>\n<p>How to provide fold information in multi-fold<\/p>\n<\/blockquote>\n<\/aside>\n<p>I\u2019d love to help more here if you could clarify what you\u2019re hoping would happen.<\/p>\n<p>You can customise your x-axis using x-axis expressions if that\u2019s all you want to do:<\/p>\n<p>Here you can see I\u2019m redefining the x-axis to be <code>step - (fold*num_steps)<\/code> where <code>config:num_steps<\/code> is the number of steps in each fold (100 in this case), fold is the current fold, and <code>_step<\/code> is the internal step logged by wandb.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/ed686bc99de10d283012548a0979562bad514c2e.png\" data-download-href=\"\/uploads\/short-url\/xScSEGZ1o1ynq5HNnGOUgCYtis6.png?dl=1\" title=\"image\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ed686bc99de10d283012548a0979562bad514c2e_2_690x242.png\" alt=\"image\" data-base62-sha1=\"xScSEGZ1o1ynq5HNnGOUgCYtis6\" width=\"690\" height=\"242\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ed686bc99de10d283012548a0979562bad514c2e_2_690x242.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ed686bc99de10d283012548a0979562bad514c2e_2_1035x363.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ed686bc99de10d283012548a0979562bad514c2e_2_1380x484.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ed686bc99de10d283012548a0979562bad514c2e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1540\u00d7541 75.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:07.232Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"provid fold inform callback provid fold inform multi fold train try like fold rang model fit actual creat singl graph step continu execut step previou epoch ref screenshot",
        "Question_preprocessed_content":"provid fold inform callback provid fold inform train try like actual creat singl graph step continu execut step previou epoch ref screenshot",
        "Question_gpt_summary_original":"The user is trying to provide fold information in multi-fold training using WandbCallback, but is facing challenges in creating a single graph where steps are continued from the last executed step of the previous epoch. The user has provided a screenshot for reference.",
        "Question_gpt_summary":"user try provid fold inform multi fold train callback face challeng creat singl graph step continu execut step previou epoch user provid screenshot refer",
        "Answer_original_content":"think fold run sure good practic sure fulli understand question youd like train differ fold continu train model oppos train fresh model fold your resum run updat fold config variabl begin loop know latest fold model train fold rang config updat fold fold model fit fold inform hope log harveenchadha provid fold inform multi fold love help clarifi your hope happen customis axi axi express that want redefin axi step fold num step config num step number step fold case fold current fold step intern step log imag topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"think fold run sure good practic sure fulli understand question youd like train differ fold continu train model oppos train fresh model fold your resum run updat config variabl begin loop know latest fold model train fold inform hope log harveenchadha provid fold inform love help clarifi your hope happen customis express that want redefin number step fold fold current fold intern step log imag topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the answer suggests updating a fold config variable at the beginning of each for loop to know the latest fold the model is training on. the user can also customize the x-axis using x-axis expressions to redefine the x-axis to be step - (fold*num_steps) where config:num_steps is the number of steps in each fold.",
        "Answer_gpt_summary":"answer suggest updat fold config variabl begin loop know latest fold model train user custom axi axi express redefin axi step fold num step config num step number step fold"
    },
    {
        "Question_id":null,
        "Question_title":"Does Azure Ml support training deep learning models like yolov3, faster R-CNN, Deeplabv3+, Mask R-CNN",
        "Question_body":"Does the azure ml supports training and inference task for deep learning models for object detection, semantic segmentation models.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1594372489527,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/45193\/does-azure-ml-support-training-deep-learning-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-10T09:26:16.96Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nAzure Machine Learning appears to support Yolov3 and Faster R-CNN:\nhttps:\/\/azure.microsoft.com\/en-us\/blog\/new-azure-machine-learning-updates-simplify-and-accelerate-the-ml-lifecycle\n\nhttps:\/\/devblogs.microsoft.com\/cse\/2017\/10\/24\/bird-detection-with-azure-ml-workbench\n\n\n\n\nBest regards,\nLeon",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-10T20:57:55.633Z",
                "Answer_score":0,
                "Answer_body":"Thanks for reaching out. Yes, Azure ML supports deep learning models for object detection using ONNX. Please review this document for more details. Models from many frameworks including TensorFlow, PyTorch, SciKit-Learn, Keras, Chainer, MXNet, MATLAB, and SparkML can be exported or converted to the standard ONNX format. Once the models are in the ONNX format, they can be run on a variety of platforms\/devices including Azure Machine Learning services.\n\n\n\n\nMany models including object detection can be represented as ONNX models and can be obtained through the following ways:\n\nTrain a new ONNX model in Azure Machine Learning or by using automated Machine Learning capabilities\n\n\nConvert existing model from another format to ONNX\n\n\nGet a pre-trained ONNX model from the ONNX Model Zoo\n\n\nGenerate a customized ONNX model from Azure Custom Vision service\n\nFurthermore, you can deploy, manage, and monitor your ONNX models in Azure ML. Please check out the following examples on how-to-use-azureml\/deployment\/onnx in Python as well as samples in other programming languages. Let us know if you have any further questions or concerns. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":32.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"support train deep learn model like yolov faster cnn deeplabv mask cnn support train infer task deep learn model object detect semant segment model",
        "Question_preprocessed_content":"support train deep learn model like yolov faster deeplabv mask support train infer task deep learn model object detect semant segment model",
        "Question_gpt_summary_original":"The user is seeking information on whether Azure ML supports training and inference tasks for deep learning models such as YOLOv3, Faster R-CNN, Deeplabv3+, and Mask R-CNN for object detection and semantic segmentation.",
        "Question_gpt_summary":"user seek inform support train infer task deep learn model yolov faster cnn deeplabv mask cnn object detect semant segment",
        "Answer_original_content":"appear support yolov faster cnn http azur microsoft com blog new azur machin learn updat simplifi acceler lifecycl http devblog microsoft com cse bird detect azur workbench best regard leon thank reach ye support deep learn model object detect onnx review document detail model framework includ tensorflow pytorch scikit learn kera chainer mxnet matlab sparkml export convert standard onnx format model onnx format run varieti platform devic includ servic model includ object detect repres onnx model obtain follow wai train new onnx model autom machin learn capabl convert exist model format onnx pre train onnx model onnx model zoo gener custom onnx model azur custom vision servic furthermor deploi manag monitor onnx model check follow exampl us deploy onnx python sampl program languag let know question concern thank",
        "Answer_preprocessed_content":"appear support yolov faster best regard leon thank reach ye support deep learn model object detect onnx review document detail model framework includ tensorflow pytorch kera chainer mxnet matlab sparkml export convert standard onnx format model onnx format run varieti includ servic model includ object detect repres onnx model obtain follow wai train new onnx model autom machin learn capabl convert exist model format onnx onnx model onnx model zoo gener custom onnx model azur custom vision servic furthermor deploi manag monitor onnx model check follow exampl python sampl program languag let know question concern thank",
        "Answer_gpt_summary_original":"possible solutions from the answer include:\n-  supports yolov3 and faster r-cnn for object detection.\n- deep learning models for object detection can be supported using onnx format, which can be obtained through various ways such as training a new model, converting an existing model, or getting a pre-trained model from the onnx model zoo.\n- onnx models can be deployed, managed, and monitored in azure.",
        "Answer_gpt_summary":"possibl solut answer includ support yolov faster cnn object detect deep learn model object detect support onnx format obtain wai train new model convert exist model get pre train model onnx model zoo onnx model deploi manag monitor azur"
    },
    {
        "Question_id":null,
        "Question_title":"Join over different tables in a run",
        "Question_body":"<p>Hello,<\/p>\n<p>I am looking at <a href=\"https:\/\/wandb.ai\/stacey\/mnist-viz\/reports\/Visualize-Predictions-over-Time--Vmlldzo1OTQxMTk\">this example<\/a> where at each epoch a table is generated to represent a dataset (images, ground truth) along with the model prediction and is then logged to be able to visualize the model prediction at every epoch.<\/p>\n<p>It looks redundant and bandwidth-hungry to log the images at every epoch. I would like to have a way to log the dataset as a table only once with the columns (id, image, ground truth), then at every epoch log only a  table with the model predictions i.e. with columns (id, prediction), then on the UI join the two tables on the \u201cid\u201d key.<\/p>\n<p>This does not seem to be possible at the moment. Has anyone tried something similar? Is it really standard to log a whole dataset at every evaluation step?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1673361282853,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":64.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/join-over-different-tables-in-a-run\/3670",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-11T15:53:33.620Z",
                "Answer_body":"<p>There is a way to log images just once. Basically, you log a table without the model predictions and then log a new table that references these images. Actually the integrations with lightning and keras do this.<\/p>\n<p>Basically, you do this in 3 steps.<\/p>\n<ul>\n<li>Log a Table into an Artifact<\/li>\n<\/ul>\n<pre><code class=\"lang-python\">at = wandb.Artifact(\"evaluation_data\", type=\"data\") \nds_table = wandb.Table(columns = [\"image\", \"label\"], data=data)\nds_at.add(ds_table,  \"dataset_table\")\nwandb.log_artifact(at)\n<\/code><\/pre>\n<ul>\n<li>then you grab this artifact and recover the table:<\/li>\n<\/ul>\n<pre><code class=\"lang-python\">at = wandb.use_artifact(\"evaluation_data\", type='data')\n\n# grab the ds table\nds_table = at.get(\"dataset_table\")\nindex = ds_table.get_index()\n<\/code><\/pre>\n<ul>\n<li>Finally, you create a new Table and reference (index) the values from the referenced table.<\/li>\n<\/ul>\n<pre><code class=\"lang-python\"># create a new predictions table\npreds_table = wandb.Table(columns=[\"image\",  \"label\", \"predictions\"])\n\n# then we fill the new table with the values from the `ds_table`\nfor idx in index:\n  pred = preds[idx]\n  row = [ds_table.data[idx][0], ds_table.data[idx][1], pred.argmax()]\n  self.preds_table.add_data(*row)\n\n# finally we log the new predictions table to a new Artifact\npred_artifact = wandb.Artifact(f\"run_{wandb.run.id}_preds\",  type=\"evaluation\")\npred_artifact.add(preds_table,  \"model_predictions\")\nwandb.log_artifact(pred_artifact)\n<\/code><\/pre>\n<p>It is pretty verbose, but it keeps track of the lineage.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-11T16:41:46.566Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/skandermoalla\">@skandermoalla<\/a>, it\u2019s possible to log the dataset only once, and for subsequent epochs, use referencing to access the logged dataset. Thus you need to upload the dataset only once.<\/p>\n<p>It\u2019s already used in MMDetection, MMSegmentation, MMClassification and new W&amp;B Keras Eval callback:<\/p>\n<ul>\n<li>MMDetection - <a href=\"https:\/\/github.com\/open-mmlab\/mmdetection\/blob\/master\/mmdet\/core\/hook\/wandblogger_hook.py\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">mmdetection\/wandblogger_hook.py at master \u00b7 open-mmlab\/mmdetection \u00b7 GitHub<\/a>\n<\/li>\n<li>For a simpler example check out Keras WandbEvalCallback: <a href=\"https:\/\/github.com\/wandb\/wandb\/blob\/0095a42b9a1ccf43c26ae09c2c7d52e727c9fd3d\/wandb\/integration\/keras\/callbacks\/tables_builder.py#L10\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">wandb\/tables_builder.py at 0095a42b9a1ccf43c26ae09c2c7d52e727c9fd3d \u00b7 wandb\/wandb \u00b7 GitHub<\/a>\n<\/li>\n<\/ul>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"join differ tabl run hello look exampl epoch tabl gener repres dataset imag ground truth model predict log abl visual model predict epoch look redund bandwidth hungri log imag epoch like wai log dataset tabl column imag ground truth epoch log tabl model predict column predict join tabl kei possibl moment tri similar standard log dataset evalu step thank",
        "Question_preprocessed_content":"join differ tabl run hello look exampl epoch tabl gener repres dataset model predict log abl visual model predict epoch look redund log imag epoch like wai log dataset tabl column epoch log tabl model predict column join tabl kei possibl moment tri similar standard log dataset evalu step thank",
        "Question_gpt_summary_original":"The user is facing challenges in logging a dataset and model predictions in a more efficient way. They want to avoid logging images at every epoch and instead log the dataset as a table with columns (id, image, ground truth) and only log a table with model predictions at every epoch with columns (id, prediction). However, they are unable to find a way to join the two tables on the \"id\" key and are questioning if it is standard to log a whole dataset at every evaluation step.",
        "Question_gpt_summary":"user face challeng log dataset model predict effici wai want avoid log imag epoch instead log dataset tabl column imag ground truth log tabl model predict epoch column predict unabl wai join tabl kei question standard log dataset evalu step",
        "Answer_original_content":"wai log imag basic log tabl model predict log new tabl refer imag actual integr lightn kera basic step log tabl artifact artifact evalu data type data tabl tabl column imag label data data add tabl dataset tabl log artifact grab artifact recov tabl us artifact evalu data type data grab tabl tabl dataset tabl index tabl index final creat new tabl refer index valu referenc tabl creat new predict tabl pred tabl tabl column imag label predict new tabl valu tabl idx index pred pred idx row tabl data idx tabl data idx pred argmax self pred tabl add data row final log new predict tabl new artifact pred artifact artifact run run pred type evalu pred artifact add pred tabl model predict log artifact pred artifact pretti verbos keep track lineag hei skandermoalla possibl log dataset subsequ epoch us referenc access log dataset need upload dataset mmdetect mmsegment mmclassif new kera eval callback mmdetect mmdetect logger hook master open mmlab mmdetect github simpler exampl check kera evalcallback tabl builder abaccfcaeccdecfdd github",
        "Answer_preprocessed_content":"wai log imag basic log tabl model predict log new tabl refer imag actual integr lightn kera basic step log tabl artifact grab artifact recov tabl final creat new tabl refer valu referenc tabl pretti verbos keep track lineag hei possibl log dataset subsequ epoch us referenc access log dataset need upload dataset mmdetect mmsegment mmclassif new kera eval callback mmdetect master github simpler exampl check kera evalcallback ccf github",
        "Answer_gpt_summary_original":"possible solutions to the user's problem are provided in the answer. one way is to log the dataset only once and then use referencing to access the logged dataset for subsequent epochs. this can be done by logging a table without model predictions and then logging a new table that references these images. the integrations with lightning and keras already do this. the process involves three steps: logging a table into an artifact, grabbing the artifact and recovering the table, and creating a new table and referencing the values from the referenced table. the lineage is kept track of during this process.",
        "Answer_gpt_summary":"possibl solut user problem provid answer wai log dataset us referenc access log dataset subsequ epoch log tabl model predict log new tabl refer imag integr lightn kera process involv step log tabl artifact grab artifact recov tabl creat new tabl referenc valu referenc tabl lineag kept track process"
    },
    {
        "Question_id":null,
        "Question_title":"Azure endpoint in decimal notation",
        "Question_body":"Hello, I've set up an Azure endpoint and I'm trying to communicate with it using some old software that can only read decimal notation. The scientific notation the endpoint occasionally delivers is breaking it. Is there a way to configure the endpoint to return only decimal notation? Ideally just with the correct header like \"application\/jsonlegacy\" or something?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639603918890,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@JonathanHorton-4850 Returning a decimal value from an endpoint should be possible. I think this depends on the training of the experiment if the ML studio is used. I have an experiment which returns decimals. You can use a similar setup with Apply transformation module or Apply Math operation if using the newer version of the studio.\n\n\n\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/665285\/azure-endpoint-in-decimal-notation.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-16T06:58:54.83Z",
                "Answer_score":1,
                "Answer_body":"@JonathanHorton-4850 Returning a decimal value from an endpoint should be possible. I think this depends on the training of the experiment if the ML studio is used. I have an experiment which returns decimals. You can use a similar setup with Apply transformation module or Apply Math operation if using the newer version of the studio.\n\n\n\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639637934830,
        "Question_original_content":"azur endpoint decim notat hello set azur endpoint try commun old softwar read decim notat scientif notat endpoint occasion deliv break wai configur endpoint return decim notat ideal correct header like applic jsonlegaci",
        "Question_preprocessed_content":"azur endpoint decim notat hello set azur endpoint try commun old softwar read decim notat scientif notat endpoint occasion deliv break wai configur endpoint return decim notat ideal correct header like",
        "Question_gpt_summary_original":"The user is facing a challenge with an Azure endpoint that occasionally delivers scientific notation, which is causing issues with their old software that can only read decimal notation. They are seeking a solution to configure the endpoint to return only decimal notation, preferably with the correct header.",
        "Question_gpt_summary":"user face challeng azur endpoint occasion deliv scientif notat caus issu old softwar read decim notat seek solut configur endpoint return decim notat prefer correct header",
        "Answer_original_content":"jonathanhorton return decim valu endpoint possibl think depend train experi studio experi return decim us similar setup appli transform modul appli math oper newer version studio answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"return decim valu endpoint possibl think depend train experi studio experi return decim us similar setup appli transform modul appli math oper newer version studio answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"the answer suggests that returning a decimal value from an endpoint should be possible and it depends on the training of the experiment if the ml studio is used. the user can use a similar setup with apply transformation module or apply math operation if using the newer version of the studio.",
        "Answer_gpt_summary":"answer suggest return decim valu endpoint possibl depend train experi studio user us similar setup appli transform modul appli math oper newer version studio"
    },
    {
        "Question_id":63128111.0,
        "Question_title":"Sagemaker Object2Vec training samples per second",
        "Question_body":"<p>I am using Sagemaker Object2Vec to train on data of size 2GB.<\/p>\n<p>ml.p2.xlarge instance took 12 hours to train the data on 4 epochs going at the speed of 5000 samples\/sec.<\/p>\n<p>Now, I am using a higher level instance ml.p2.16xlarge and it only trains at 400 samples\/sec with this in the logs<\/p>\n<pre><code>[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:739: only 114 out of 240 GPU pairs are enabled direct access. It may affect the performance. You can set MXNET_ENABLE_GPU_P2P=0 to turn it off\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .vvvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: v.vvvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vv.vvvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvv.vvvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvv.vvvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvv.vvv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvv.vv.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvv.v.......\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: vvvvvvvv........\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: ..........vvvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........v.vvvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vv.vvvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvv.vvv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvv.vv\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvv.v\n\n2020-07-27T23:03:49.956-07:00\n[06:03:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsMXNet\/AIAlgorithmsMXNet-1.3.x_Cuda_10.1.x.672.0\/AL2012\/generic-flavor\/src\/src\/kvstore\/.\/.\/comm.h:748: .........vvvvvv.\n<\/code><\/pre>\n<p>There are about 50 million samples.<\/p>\n<p>What can I do to correct this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595917104483,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":59.0,
        "Answer_body":"<p>2 ideas:<\/p>\n<ol>\n<li>Before increasing the GPU count, grow batch size so that a single\nGPU is as busy as possible<\/li>\n<li>Use P3 instances instead of P2. P3 is more recent, has more memory, more CUDA cores, faster memory bandwidth and NVLink inter-GPU connections. Though it's more\nexpensive by hour, your total training cost may be much smaller if\nproperly tuned<\/li>\n<\/ol>\n<p>Also, if your problem involves sparse updates, meaning if just a small fraction of all tokens appear in a given mini-batch, you can try using <code>token_embedding_storage_type = 'row_sparse'<\/code>, which I think refers to using sparse gradient updates like described here <a href=\"https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/apache-mxnet\/learning-embeddings-for-music-recommendation-with-mxnets-sparse-api-5698f4d7d8<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63128111",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1595937519408,
        "Question_original_content":"objectvec train sampl second objectvec train data size xlarg instanc took hour train data epoch go speed sampl sec higher level instanc xlarg train sampl sec log opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm gpu pair enabl direct access affect perform set mxnet enabl gpu turn opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm opt brazil pkg cach packag aialgorithmsmxnet aialgorithmsmxnet cuda gener flavor src src kvstore comm million sampl correct",
        "Question_preprocessed_content":"object vec train sampl second object vec train data size instanc took hour train data epoch go speed higher level instanc train log million sampl correct",
        "Question_gpt_summary_original":"The user is facing challenges with the Sagemaker Object2Vec training, as the higher level instance is only training at 400 samples\/sec, which is significantly slower than the previous instance that trained at 5000 samples\/sec. The logs show a message that only 114 out of 240 GPU pairs are enabled direct access, which may affect performance. The user has 50 million samples to train and is seeking advice on how to correct the issue.",
        "Question_gpt_summary":"user face challeng objectvec train higher level instanc train sampl sec significantli slower previou instanc train sampl sec log messag gpu pair enabl direct access affect perform user million sampl train seek advic correct issu",
        "Answer_original_content":"idea increas gpu count grow batch size singl gpu busi possibl us instanc instead recent memori cuda core faster memori bandwidth nvlink inter gpu connect expens hour total train cost smaller properli tune problem involv spars updat mean small fraction token appear given mini batch try token embed storag type row spars think refer spars gradient updat like describ http medium com apach mxnet learn embed music recommend mxnet spars api fdd",
        "Answer_preprocessed_content":"idea increas gpu count grow batch size singl gpu busi possibl us instanc instead recent memori cuda core faster memori bandwidth nvlink connect expens hour total train cost smaller properli tune problem involv spars updat mean small fraction token appear given try think refer spars gradient updat like describ",
        "Answer_gpt_summary_original":"possible solutions to improve object2vec training are to increase the batch size before increasing the gpu count, use p3 instances instead of p2, and use sparse gradient updates if the problem involves sparse updates. p3 instances have more memory, more cuda cores, faster memory bandwidth, and nvlink inter-gpu connections, which can reduce the total training cost if properly tuned. using token_embedding_storage_type = 'row_sparse' can also help with sparse updates.",
        "Answer_gpt_summary":"possibl solut improv objectvec train increas batch size increas gpu count us instanc instead us spars gradient updat problem involv spars updat instanc memori cuda core faster memori bandwidth nvlink inter gpu connect reduc total train cost properli tune token embed storag type row spars help spars updat"
    },
    {
        "Question_id":null,
        "Question_title":"Does dvc work for live streaming data versioning and batch data versioning ? If yes, can someone explain briefly",
        "Question_body":"<p>I couldn\u2019t find any dvc documentation for batch data, and live streaming data versioning.<br>\nIs it possible in dvc to track streaming data and also fetch data in batch or time travel data?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1619414992269,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":796.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-work-for-live-streaming-data-versioning-and-batch-data-versioning-if-yes-can-someone-explain-briefly\/738",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-26T05:52:19.927Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/veeresh\">@veeresh<\/a>,<\/p>\n<p>DVC could be used to maintain snapshots of a growing dataset every X time (prob not real-time for a raw stream though) IF the dataset growth looks like adding\/removing files. That would work efficiently because DVC de-duplicates data storage at the file level (see <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>). So in that narrow case, it\u2019s technically possible.<\/p>\n<p>That said, there may be better tools for timed data back-ups. And I would question whether snapshots of a data stream should be considered versions: You could say they\u2019re all parts of the same version, which you just haven\u2019t obtained completely yet (maybe never will). If the stream includes updates\/corrections to previously received data points though, then that would more clearly represent versioning IMO.<\/p>\n<p>This is all pretty conceptual though. Feel free to share a more specific use case you have in mind for more concrete answer <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_score":51.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-26T06:40:24.955Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> ,<\/p>\n<p>Thanks for explaining.<\/p>\n<p>I was looking if something similar to delta lake time travel is possible in dvc. (<a href=\"https:\/\/databricks.com\/blog\/2019\/02\/04\/introducing-delta-time-travel-for-large-scale-data-lakes.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Introducing Delta Time Travel for Large Scale Data Lakes - The Databricks Blog<\/a>)<\/p>\n<p>Lets say there are 2 folders which keep updating every day (steaming),<br>\n-dogs<br>\n-cats<br>\nBasically, I want to get data between some time period (between 2 dates) , this is not possible in dvc right?<\/p>",
                "Answer_score":46.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-26T06:44:01.795Z",
                "Answer_body":"<p>DVC uses Git as the underlying versioning layer. In Git you decide when and what to include in your commits. You can only switch between the commits that you have registered yourself.<\/p>\n<p>The new Experiments features do include some automatic tracking of multiple project versions but again that has a different purpose (ML <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\">experiment management<\/a>).<\/p>\n<p>Thanks<\/p>",
                "Answer_score":51.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"work live stream data version batch data version ye explain briefli document batch data live stream data version possibl track stream data fetch data batch time travel data",
        "Question_preprocessed_content":"work live stream data version batch data version ye explain briefli document batch data live stream data version possibl track stream data fetch data batch time travel data",
        "Question_gpt_summary_original":"The user is facing challenges in finding documentation for using dvc for batch data versioning and live streaming data versioning. They are unsure if it is possible to track streaming data and fetch data in batch or time travel data using dvc.",
        "Question_gpt_summary":"user face challeng find document batch data version live stream data version unsur possibl track stream data fetch data batch time travel data",
        "Answer_original_content":"veeresh maintain snapshot grow dataset time prob real time raw stream dataset growth look like ad remov file work effici duplic data storag file level http org doc user guid larg dataset optim narrow case technic possibl said better tool time data up question snapshot data stream consid version theyr part version havent obtain complet mayb stream includ updat correct previous receiv data point clearli repres version imo pretti conceptu feel free share specif us case mind concret answer jorgeorpinel thank explain look similar delta lake time travel possibl introduc delta time travel larg scale data lake databrick blog let folder updat dai steam dog cat basic want data time period date possibl right us git underli version layer git decid includ commit switch commit regist new experi featur includ automat track multipl project version differ purpos experi manag thank",
        "Answer_preprocessed_content":"maintain snapshot grow dataset time dataset growth look like file work effici data storag file level narrow case technic possibl said better tool time data question snapshot data stream consid version theyr part version havent obtain complet stream includ previous receiv data point clearli repres version imo pretti conceptu feel free share specif us case mind concret answer thank explain look similar delta lake time travel possibl let folder updat dai dog cat basic want data time period possibl right us git underli version layer git decid includ commit switch commit regist new experi featur includ automat track multipl project version differ purpos thank",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer include using snapshots to maintain versions of a growing dataset every x time, which could work efficiently for adding\/removing files. however, there may be better tools for timed data backups. the answer also suggests that if the stream includes updates\/corrections to previously received data points, then that would more clearly represent versioning. additionally, the answer mentions that uses git as the underlying versioning layer, but switching between commits is limited to those registered by the user. the new experiments features include some automatic tracking of multiple project versions, but that has a different purpose (ml experiment management).",
        "Answer_gpt_summary":"possibl solut mention answer includ snapshot maintain version grow dataset time work effici ad remov file better tool time data backup answer suggest stream includ updat correct previous receiv data point clearli repres version addition answer mention us git underli version layer switch commit limit regist user new experi featur includ automat track multipl project version differ purpos experi manag"
    },
    {
        "Question_id":60382704.0,
        "Question_title":"Convert csv into parquet in kedro",
        "Question_body":"<p>I have pretty big CSV that would not fit into memory, and I need to convert it into .parquet file to work with vaex.<\/p>\n\n<p>Here is my catalog:<\/p>\n\n<pre><code>raw_data:\n    type: kedro.contrib.io.pyspark.SparkDataSet\n    filepath: data\/01_raw\/data.csv\n    file_format: csv\n\nparquet_data:\n    type: ParquetLocalDataSet\n    filepath: data\/02_intermediate\/data.parquet\n<\/code><\/pre>\n\n<p>node:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def convert_to_parquet(data: SparkDataSet) -&gt; ParquetLocalDataSet:\n    return data.coalesce(1)\n<\/code><\/pre>\n\n<p>and a pipeline:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=convert_to_parquet,\n                inputs=\"raw_data\",\n                outputs=\"parquet_data\",\n                name=\"data_to_parquet\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<p>But if I do <code>kedro run<\/code> I receive this error <code>kedro.io.core.DataSetError: Failed while saving data to data set ParquetLocalDataSet(engine=auto, filepath=data\/02_intermediate\/data.parquet, save_args={}).\n'DataFrame' object has no attribute 'to_parquet'<\/code><\/p>\n\n<p>What should I fix to get my dataset converted?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582572566660,
        "Question_favorite_count":null,
        "Question_last_edit_time":1583421031536,
        "Question_score":2.0,
        "Question_view_count":498.0,
        "Answer_body":"<p>You could try the following. This has worked for me in the past.<\/p>\n\n<pre><code>parquet_data:\n    type: kedro.contrib.io.pyspark.SparkDataSet\n    file_format: 'parquet'\n    filepath: data\/02_intermediate\/data.parquet\n    save_args:\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60382704",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582574347452,
        "Question_original_content":"convert csv parquet pretti big csv fit memori need convert parquet file work vaex catalog raw data type contrib pyspark sparkdataset filepath data raw data csv file format csv parquet data type parquetlocaldataset filepath data intermedi data parquet node def convert parquet data sparkdataset parquetlocaldataset return data coalesc pipelin def creat pipelin kwarg return pipelin node func convert parquet input raw data output parquet data data parquet run receiv error core dataseterror fail save data data set parquetlocaldataset engin auto filepath data intermedi data parquet save arg datafram object attribut parquet fix dataset convert",
        "Question_preprocessed_content":"convert csv parquet pretti big csv fit memori need convert parquet file work vaex catalog node pipelin receiv error fix dataset convert",
        "Question_gpt_summary_original":"The user is facing a challenge of converting a large CSV file into a .parquet file to work with vaex. They have created a catalog, node, and pipeline for the conversion, but when they run the pipeline, they receive an error stating that the DataFrame object has no attribute 'to_parquet'. The user is seeking guidance on how to fix this issue and successfully convert their dataset.",
        "Question_gpt_summary":"user face challeng convert larg csv file parquet file work vaex creat catalog node pipelin convers run pipelin receiv error state datafram object attribut parquet user seek guidanc fix issu successfulli convert dataset",
        "Answer_original_content":"try follow work past parquet data type contrib pyspark sparkdataset file format parquet filepath data intermedi data parquet save arg",
        "Answer_preprocessed_content":"try follow work past",
        "Answer_gpt_summary_original":"the solution to the challenge of converting a large csv file into a .parquet file to work with vaex and receiving an error when running the pipeline is to try using the following code: parquet_data: type: .contrib.io.pyspark.sparkdataset file_format: 'parquet' filepath: data\/02_intermediate\/data.parquet save_args. this solution has worked for the person providing the answer in the past.",
        "Answer_gpt_summary":"solut challeng convert larg csv file parquet file work vaex receiv error run pipelin try follow code parquet data type contrib pyspark sparkdataset file format parquet filepath data intermedi data parquet save arg solut work person provid answer past"
    },
    {
        "Question_id":56647549.0,
        "Question_title":"MLflow Error while deploying the Model to local REST server",
        "Question_body":"<blockquote>\n  <p><strong>System Details:<\/strong><\/p>\n  \n  <p>Operating System: Ubuntu 19.04<\/p>\n  \n  <p>Anaconda version: 2019.03<\/p>\n  \n  <p>Python version: 3.7.3<\/p>\n  \n  <p>mlflow version: 1.0.0<\/p>\n<\/blockquote>\n\n<p><strong>Steps to Reproduce:<\/strong> <a href=\"https:\/\/mlflow.org\/docs\/latest\/tutorial.html\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/tutorial.html<\/a><\/p>\n\n<p><strong>Error at line\/command:<\/strong> <code>mlflow models serve -m [path_to_model] -p 1234<\/code><\/p>\n\n<p><strong>Error:<\/strong>\nCommand 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1<\/p>\n\n<p><strong>Terminal Log:<\/strong><\/p>\n\n<pre><code>(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# mlflow models serve -m $(pwd) -p 1234\n2019\/06\/18 16:15:16 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2019\/06\/18 16:15:17 INFO mlflow.pyfunc.backend: === Running command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app'\nbash: activate: No such file or directory\nTraceback (most recent call last):\n  File \"\/root\/anaconda3\/envs\/mlflow\/bin\/mlflow\", line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/models\/cli.py\", line 43, in serve\n    host=host)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 76, in serve\n    command_env=command_env)\n  File \"\/root\/anaconda3\/envs\/mlflow\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/backend.py\", line 147, in _execute_in_conda_env\n    command, rc\nException: Command 'source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1&gt;&amp;2 &amp;&amp; gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\n(mlflow) root@user:\/home\/user\/mlflow\/mlflow\/examples\/sklearn_elasticnet_wine\/mlruns\/0\/e3dd02d5d84545ffab858db13ede7366\/artifacts\/model# \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1560855399150,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1840.0,
        "Answer_body":"<p>Following the steps mentioned in the GitHub Issue <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">1507<\/a> (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/1507\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/1507<\/a>) I was able to resolve this issue.<\/p>\n\n<p>In reference to this post, the \"<strong>anaconda\/bin\/<\/strong>\" directory is never added to the list of environment variables i.e. PATH variable. In order to resolve this issue, add the \"<strong>else<\/strong>\" part of conda initialize code block from ~\/.bashrc file to your PATH variable.<\/p>\n\n<pre><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'conda init' !!\n__conda_setup=\"$('\/home\/atulk\/anaconda3\/bin\/conda' 'shell.bash' 'hook' 2&gt; \/dev\/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__conda_setup\"\nelse\n    if [ -f \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\" ]; then\n        . \"\/home\/atulk\/anaconda3\/etc\/profile.d\/conda.sh\"\n    else\n        export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"\n    fi\nfi\nunset __conda_setup\n# &lt;&lt;&lt; conda initialize &lt;&lt;&lt;\n<\/code><\/pre>\n\n<p>In this case, I added <strong>export PATH=\"\/home\/atulk\/anaconda3\/bin:$PATH\"<\/strong> to the PATH variable. However, this is just a temporary fix until the issue is fixed in the project.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56647549",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1561730574528,
        "Question_original_content":"error deploi model local rest server detail oper ubuntu anaconda version python version version step reproduc http org doc latest tutori html error line command model serv path model error command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app return non zero return code return code termin log root user home user exampl sklearn elasticnet wine mlrun eddddffabdbed artifact model model serv pwd info model cli select backend flavor python function info pyfunc backend run command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app bash activ file directori traceback recent file root anaconda env bin line sy exit cli file root anaconda env lib python site packag click core line return self main arg kwarg file root anaconda env lib python site packag click core line main self invok ctx file root anaconda env lib python site packag click core line invok return process result sub ctx command invok sub ctx file root anaconda env lib python site packag click core line invok return process result sub ctx command invok sub ctx file root anaconda env lib python site packag click core line invok return ctx invok self callback ctx param file root anaconda env lib python site packag click core line invok return callback arg kwarg file root anaconda env lib python site packag model cli line serv host host file root anaconda env lib python site packag pyfunc backend line serv command env command env file root anaconda env lib python site packag pyfunc backend line execut conda env command except command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app return non zero return code return code root user home user exampl sklearn elasticnet wine mlrun eddddffabdbed artifact model",
        "Question_preprocessed_content":"error deploi model local rest server detail oper ubuntu anaconda version python version version step reproduc error error command sourc activ bfb dce gunicorn return non zero return code return code termin log",
        "Question_gpt_summary_original":"The user encountered an error while deploying a model to a local REST server using MLflow. The error occurred at the command \"mlflow models serve -m [path_to_model] -p 1234\" and returned a non-zero return code. The terminal log shows that the error was caused by a missing file or directory for the command \"source activate mlflow-c4536834c2e6e0e2472b58bfb28dce35b4bd0be6 1>&2 && gunicorn --timeout 60 -b 127.0.0.1:1234 -w 4 mlflow.pyfunc.scoring_server.wsgi:app\".",
        "Question_gpt_summary":"user encount error deploi model local rest server error occur command model serv path model return non zero return code termin log show error caus miss file directori command sourc activ cceeebbfbdcebbdb gunicorn timeout pyfunc score server wsgi app",
        "Answer_original_content":"follow step mention github issu http github com issu abl resolv issu refer post anaconda bin directori ad list environ variabl path variabl order resolv issu add conda initi code block bashrc file path variabl conda initi content block manag conda init conda setup home atulk anaconda bin conda shell bash hook dev null eval conda setup home atulk anaconda profil conda home atulk anaconda profil conda export path home atulk anaconda bin path unset conda setup conda initi case ad export path home atulk anaconda bin path path variabl temporari fix issu fix project",
        "Answer_preprocessed_content":"follow step mention github issu abl resolv issu refer post directori ad list environ variabl path variabl order resolv issu add conda initi code block file path variabl case ad export path variabl temporari fix issu fix project",
        "Answer_gpt_summary_original":"the solution to the error encountered while deploying a model to a local rest server on an ubuntu 19.04 operating system using anaconda 2019.03 and python 3.7.3 is to add the \"else\" part of conda initialize code block from ~\/.bashrc file to the path variable. the user added export path=\"\/home\/atulk\/anaconda3\/bin:$path\" to the path variable as a temporary fix until the issue is fixed in the project.",
        "Answer_gpt_summary":"solut error encount deploi model local rest server ubuntu oper anaconda python add conda initi code block bashrc file path variabl user ad export path home atulk anaconda bin path path variabl temporari fix issu fix project"
    },
    {
        "Question_id":null,
        "Question_title":"MLOps Query regarding SageMaker Project template and Pipelines",
        "Question_body":"Hi MLOps Gurus,\n\nI'd like to seek guidance on my below situation.\n\nThis is regarding Sagemaker Project creation in AWS. The use case is to take final model (built by DS team) from S3 and do all sorts of Bias analysis using Clarify and upon acceptance of its response(Bias & Explainability reports) by Data Scientists, deploy to Model Monitor. Now, as there is no prebuilt Sagemaker template that caters to my use case, how shall I initiate this whole process and how shall I create a ML project at first place.\n\nAlso, how shall I initiate a notebook from within my cloudformation template for DS Team to kick off from Service Catalogue to automate the above process?\n\nAny pointers would be greatly appreciated.\n\nRegards, Nikhil",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645816119455,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":93.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbOWLdrrNStmzDzTMOcdG3Q\/ml-ops-query-regarding-sage-maker-project-template-and-pipelines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-27T13:02:12.192Z",
                "Answer_score":0,
                "Answer_body":"To learn more about building custom SageMaker projects templates, I usually suggest a read of Build Custom SageMaker Project Templates \u2013 Best Practices, and also to check if the use case of interest is already present in this collection of custom templates: Custom Project Templates in SageMaker.\n\nI'm not really sure I understand:\n\nAlso, how shall I initiate a notebook from within my cloudformation template for DS Team to kick off from Service Catalogue to automate the above process?",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"mlop queri project templat pipelin mlop guru like seek guidanc situat project creation aw us case final model built team sort bia analysi clarifi accept respons bia explain report data scientist deploi model monitor prebuilt templat cater us case shall initi process shall creat project place shall initi notebook cloudform templat team kick servic catalogu autom process pointer greatli appreci regard nikhil",
        "Question_preprocessed_content":"mlop queri project templat pipelin mlop guru like seek guidanc situat project creation aw us case final model sort bia analysi clarifi accept respons data scientist deploi model monitor prebuilt templat cater us case shall initi process shall creat project place shall initi notebook cloudform templat team kick servic catalogu autom process pointer greatli appreci regard nikhil",
        "Question_gpt_summary_original":"The user is seeking guidance on how to create a Sagemaker project in AWS for their use case, which involves taking a final model from S3 and conducting bias analysis using Clarify before deploying to Model Monitor. They are facing challenges as there is no prebuilt Sagemaker template that caters to their use case and they are unsure how to initiate the process and create an ML project. They are also seeking advice on how to initiate a notebook from within their cloudformation template for the DS team to automate the process.",
        "Question_gpt_summary":"user seek guidanc creat project aw us case involv take final model conduct bia analysi clarifi deploi model monitor face challeng prebuilt templat cater us case unsur initi process creat project seek advic initi notebook cloudform templat team autom process",
        "Answer_original_content":"learn build custom project templat usual suggest read build custom project templat best practic check us case present collect custom templat custom project templat sure understand shall initi notebook cloudform templat team kick servic catalogu autom process",
        "Answer_preprocessed_content":"learn build custom project templat usual suggest read build custom project templat best practic check us case present collect custom templat custom project templat sure understand shall initi notebook cloudform templat team kick servic catalogu autom process",
        "Answer_gpt_summary_original":"the answer suggests reading the best practices for building custom project templates and checking if the desired use case is already present in the collection of custom templates. however, the answer is not clear on how to initiate a notebook from within a cloudformation template to automate the process.",
        "Answer_gpt_summary":"answer suggest read best practic build custom project templat check desir us case present collect custom templat answer clear initi notebook cloudform templat autom process"
    },
    {
        "Question_id":65494496.0,
        "Question_title":"How to load a model using the object \"mlflow.tracking.client.MlflowClient\"?",
        "Question_body":"<p>I'm stuck with the MLFlow model registry. Does anyone know how to load a model using the object &quot;mlflow.tracking.client.MlflowClient&quot;?<\/p>\n<p>I would like to do a predict after with that. I'm sure I'm wrong somewhere because I've already done that in the past. I'm not able to find it in the doc, in the web.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1609255468723,
        "Question_favorite_count":null,
        "Question_last_edit_time":1611139575950,
        "Question_score":0.0,
        "Question_view_count":608.0,
        "Answer_body":"<p>You'll have to make use of <code>mlflow.&lt;model_flavor&gt;.load_model()<\/code> to load a given model from the Model Registry. For example:<\/p>\n<pre><code>import mlflow.pyfunc\n\nmodel = mlflow.pyfunc.load_model(\n          model_uri=&quot;models:\/&lt;model_name&gt;\/&lt;model_version&gt;&quot;\n          )\n\nmodel.predict(...)\n<\/code><\/pre>\n<p>With <code>mlflow.tracking.client.MlflowClient<\/code> you can retrieve metadata about a model from the model registry, but for retrieving the actual model you will need to use <code>mlflow.&lt;model_flavor&gt;.load_model<\/code>. For example, you could use the MlflowClient to get the download URI for a given model, and then use <code>mlflow.&lt;flavor&gt;.load_model<\/code> to retrieve that model.<\/p>\n<pre><code>model_uri = client.get_model_version_download_uri(&quot;&lt;model_name&gt;&quot;, &lt;version&gt;)\nmodel = mlflow.pyfunc.load_model(model_uri)\n\nmodel.predict(...)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65494496",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1609791959680,
        "Question_original_content":"load model object track client client stuck model registri know load model object track client client like predict sure wrong past abl doc web",
        "Question_preprocessed_content":"load model object stuck model registri know load model object like predict sure wrong past abl doc web",
        "Question_gpt_summary_original":"The user is facing challenges with loading a model using the \"mlflow.tracking.client.MlflowClient\" object and is seeking assistance to perform a prediction. The user has previously done this but is unable to find the necessary information in the documentation or online.",
        "Question_gpt_summary":"user face challeng load model track client client object seek assist perform predict user previous unabl necessari inform document onlin",
        "Answer_original_content":"us load model load given model model registri exampl import pyfunc model pyfunc load model model uri model model predict track client client retriev metadata model model registri retriev actual model need us load model exampl us client download uri given model us load model retriev model model uri client model version download uri model pyfunc load model model uri model predict",
        "Answer_preprocessed_content":"us load given model model registri exampl retriev metadata model model registri retriev actual model need us exampl us client download uri given model us retriev model",
        "Answer_gpt_summary_original":"the solution to the user's problem of loading a model using \".tracking.client.client\" is to use \".<model_flavor>.load_model()\" instead. the user can import \".pyfunc\" and use \".pyfunc.load_model()\" to load a given model from the model registry. the user can also use \".tracking.client.client\" to retrieve metadata about a model from the model registry, but to retrieve the actual model, they will need to use \".<model_flavor>.load_model\". the user can get the download uri for a given model using the client and then use \".<flavor>.load_model\" to retrieve that model.",
        "Answer_gpt_summary":"solut user problem load model track client client us load model instead user import pyfunc us pyfunc load model load given model model registri user us track client client retriev metadata model model registri retriev actual model need us load model user download uri given model client us load model retriev model"
    },
    {
        "Question_id":null,
        "Question_title":"Error while calling W&B API iinternal database error (<Response [500]>)",
        "Question_body":"<p>I am running four experiments from the same system (a google cloud VM) and while one is running fine: three have frozen (no progress but program still active\/has not errored out). Curious if anyone knows how to fix this?<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/19876a837127852045cb70c484da66f5a3c28d52.png\" alt=\"image\" data-base62-sha1=\"3DQ3Zmd1Ly9oF4hn1gpJ6jJ9bB8\" width=\"681\" height=\"130\"><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1655361075016,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":789.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-while-calling-w-b-api-iinternal-database-error-response-500\/2624",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-18T00:25:13.707Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/usman391\">@usman391<\/a> ,<\/p>\n<p>Thank you for reaching out with your support request. We will be glad to look into this to determine the cause.<br>\nCan you please provide the following:<\/p>\n<ul>\n<li>Description of the experiment you are running (single runs from four agents, or parallel processes?)<\/li>\n<li>Sample code for you how you are initializing\/executing runs<\/li>\n<li>\n<code>debug<\/code> logs for the runs this error is occurring, they live in the <code>WANDB_DIR<\/code> which defaults to <code>.\/wandb<\/code> in your project folder.<\/li>\n<\/ul>\n<p>Please attach the code sample\/logs here or send  directly to me at <a href=\"mailto:mohammad.bakir@wandb.com\">mohammad.bakir@wandb.com<\/a>.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_score":56.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-18T16:08:37.108Z",
                "Answer_body":"<p>Thanks for the response Mohammad Bakir.  The error actually went away after I rebooted the system and has not occurred again since. If it occurs again, I will let you know.<\/p>",
                "Answer_score":11.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-22T20:41:43.593Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/usman391\">@usman391<\/a> , thank you for updating us that this error has gone away. Yes please do let us know if this occurs again. I will mark this matter closed in the meantime.<\/p>",
                "Answer_score":11.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-21T20:42:26.367Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":20.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error call api iintern databas error run experi googl cloud run fine frozen progress program activ error curiou know fix",
        "Question_preprocessed_content":"error call api iintern databas error run experi run fine frozen curiou know fix",
        "Question_gpt_summary_original":"The user is encountering challenges with running experiments on a Google Cloud VM, where three out of four experiments have frozen and are not making progress, while one is running fine. The user is seeking help to fix this issue.",
        "Question_gpt_summary":"user encount challeng run experi googl cloud experi frozen make progress run fine user seek help fix issu",
        "Answer_original_content":"usman thank reach support request glad look determin caus provid follow descript experi run singl run agent parallel process sampl code initi execut run debug log run error occur live dir default project folder attach code sampl log send directli mohammad bakir com regard mohammad thank respons mohammad bakir error actual went awai reboot occur occur let know usman thank updat error gone awai ye let know occur mark matter close meantim topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank reach support request glad look determin caus provid follow descript experi run sampl code run log run error occur live default project folder attach code send directli regard mohammad thank respons mohammad bakir error actual went awai reboot occur occur let know thank updat error gone awai ye let know occur mark matter close meantim topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer as the user reported that the error went away after rebooting the system. the support team requested more information about the experiment, sample code, and debug logs to determine the cause of the error if it occurs again in the future. the matter was marked as closed, and new replies are no longer allowed.",
        "Answer_gpt_summary":"solut provid answer user report error went awai reboot support team request inform experi sampl code debug log determin caus error occur futur matter mark close new repli longer allow"
    },
    {
        "Question_id":null,
        "Question_title":"Testing Data Problem",
        "Question_body":"Any ideas what this could be? Trying to upload a test data to test the model.",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1647528251100,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/776565\/testing-data-problem.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-04T09:23:03.263Z",
                "Answer_score":0,
                "Answer_body":"Update about this case:\n\nDuring the support session, support team confirmed that PG has identified a bug where the target column in test data, which ideally should be optional, is currently required. PG will roll out the fix in the next python SDK release.\n\nSorry for the experience.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks a lot.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"test data problem idea try upload test data test model",
        "Question_preprocessed_content":"test data problem idea try upload test data test model",
        "Question_gpt_summary_original":"The user is facing challenges while trying to upload test data to test a model.",
        "Question_gpt_summary":"user face challeng try upload test data test model",
        "Answer_original_content":"updat case support session support team confirm identifi bug target column test data ideal option current requir roll fix python sdk releas sorri experi regard yutong kindli accept answer feel help thank lot",
        "Answer_preprocessed_content":"updat case support session support team confirm identifi bug target column test data ideal option current requir roll fix python sdk releas sorri experi regard yutong kindli accept answer feel help thank lot",
        "Answer_gpt_summary_original":"possible solutions: wait for the next python sdk release where the bug will be fixed.",
        "Answer_gpt_summary":"possibl solut wait python sdk releas bug fix"
    },
    {
        "Question_id":null,
        "Question_title":"How to get all stdout from log",
        "Question_body":"<p>I am trying to get all the standard output of my run in log. I can see this screen on the web console:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/5\/5fbacd8534d6292b5ce0a30a167fe896d30488a9.png\" data-download-href=\"\/uploads\/short-url\/dERA8zxAjiNltMHn8FqM9a3SvUt.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/5\/5fbacd8534d6292b5ce0a30a167fe896d30488a9.png\" alt=\"image\" data-base62-sha1=\"dERA8zxAjiNltMHn8FqM9a3SvUt\" width=\"690\" height=\"337\" data-dominant-color=\"494B4D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">776\u00d7380 69.4 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>However, I can\u2019t copy and paste all of the text (it only copies the current screen) and inspect element also shows that when I scroll down, other things are cleared from memory<\/p>\n<p>I would also be interested if there\u2019s a way to get this stdout from the API, too<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1676574169175,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":96.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-get-all-stdout-from-log\/3895",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-17T00:27:53.547Z",
                "Answer_body":"<p>Ah, I now see there\u2019s a download button top right:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/231873544bd099d36a0fefeb254f4dc66e4bea8b.png\" alt=\"image\" data-base62-sha1=\"50t5YGeiOCpCcEFcULgzy4GOJu3\" width=\"195\" height=\"239\"><\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-17T18:07:08.204Z",
                "Answer_body":"<p>Hello Arthur!<\/p>\n<p>Looks like you were able to find the solution! Is there anything else that you need help with?<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-18T12:11:13.319Z",
                "Answer_body":"<p>Actually yes - is there any way to get this from an API call?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stdout log try standard output run log screen web consol imag copi past text copi current screen inspect element show scroll thing clear memori interest there wai stdout api",
        "Question_preprocessed_content":"stdout log try standard output run log screen web consol imag copi past text inspect element show scroll thing clear memori interest there wai stdout api",
        "Question_gpt_summary_original":"The user is facing challenges in getting all the standard output of their run in a log. They are unable to copy and paste all the text as it only copies the current screen and other things are cleared from memory when they scroll down. The user is also interested in knowing if there is a way to get the stdout from the API.",
        "Question_gpt_summary":"user face challeng get standard output run log unabl copi past text copi current screen thing clear memori scroll user interest know wai stdout api",
        "Answer_original_content":"there download button right hello arthur look like abl solut need help actual ye wai api",
        "Answer_preprocessed_content":"there download button right hello arthur look like abl solut need help actual ye wai api",
        "Answer_gpt_summary_original":"possible solution: the user can download the entire output log by clicking on the download button located at the top right corner of the web console. however, if the user wants to get the output log through an api call, the answer does not provide a solution.",
        "Answer_gpt_summary":"possibl solut user download entir output log click download button locat right corner web consol user want output log api answer provid solut"
    },
    {
        "Question_id":null,
        "Question_title":"How to stop weights & biases (wandb) from creating random tmp files?",
        "Question_body":"<p>I have a million tmp files due to wandb on my home folder. I don\u2019t know why. Why are they being created &amp; how do I stop it?<\/p>\n<pre><code class=\"lang-auto\">anaconda\t\t\t\t\t   tmpa0wf77f_wandb-artifacts  tmpmv2ewoi1wandb-media\nanaconda.sh\t\t\t\t\t   tmpa3t655lewandb-media      tmpmvhp3i6ewandb-media\ndata\t\t\t\t\t\t   tmpa6roiz80wandb\t       tmpmwszwea4\ndebug-cli.brando9.log\t\t\t\t   tmpal7kticfwandb-artifacts  tmpn0p49w1hwandb-media\ndiversity-for-predictive-success-of-meta-learning  tmpalsnd4g1wandb-media      tmpnbxtnojbwandb-artifacts\ndiv_install_miniconda\t\t\t\t   tmpambb3rm9wandb-artifacts  tmpngpy96dawandb-artifacts\ndiv_install.out\t\t\t\t\t   tmpapco0xetwandb-media      tmpnqezeggnwandb-media\niit-term-synthesis\t\t\t\t   tmpaqf80v_hwandb-media      tmpo3cug5nzwandb-media\nmain.sh.e449240\t\t\t\t\t   tmpaqigpze6wandb-artifacts  tmpoc4x6l22wandb\nmain.sh.e457075\t\t\t\t\t   tmpaw1kvgtgwandb-media      tmpofql583uwandb-media\nmain.sh.e760266\t\t\t\t\t   tmpay63rbxgwandb-media      tmponqiggzswandb-artifacts\nmain.sh.err748250\t\t\t\t   tmpb2clycf5\t\t       tmpoqem6uclwandb-media\nmain.sh.err849450\t\t\t\t   tmpbbrfn_kmwandb-artifacts  tmpoqth0mgpwandb-media\nmain.sh.err923818\t\t\t\t   tmpbcxatqdiwandb-artifacts  tmppbd5bfm_wandb\nmain.sh.err962904\t\t\t\t   tmpbgewkz10wandb\t       tmppbnpm41gwandb-media\nmain.sh.o449240\t\t\t\t\t   tmpbsd96o99wandb-media      tmppwxmebn1\nmain.sh.o457075\t\t\t\t\t   tmpbtlp8zomwandb\t       tmpq396kfo1wandb-artifacts\nmain.sh.o748250\t\t\t\t\t   tmpby3a9u8ywandb\t       tmpq8jryat0wandb-media\nmain.sh.o760266\t\t\t\t\t   tmpc45e2nlxwandb-media      tmpqc65bfs0wandb\nmain.sh.o849450\t\t\t\t\t   tmpc4m5b21_\t\t       tmpqexdhp6gwandb-artifacts\nmain.sh.o923818\t\t\t\t\t   tmpcap20jmdwandb-media      tmpqh3uu7v2wandb-media\nmain.sh.o950686\t\t\t\t\t   tmpcl2sb6j_wandb\t       tmpqh99a72vwandb-media\nmain.sh.o962904\t\t\t\t\t   tmpcsncx8x4wandb-media      tmpqmim4sxywandb\nminiconda\t\t\t\t\t   tmpd7dhluxmwandb\t       tmpqpfcq9uwwandb\nminiconda.sh\t\t\t\t\t   tmpdbbb3hw_wandb-artifacts  tmpqtds4jdiwandb-artifacts\nnohup.out\t\t\t\t\t   tmpdfrjyk90wandb-media      tmp_qz8pu0xwandb-artifacts\nnohup.out449240\t\t\t\t\t   tmpdhqwaxygwandb\t       tmpr98qj7auwandb\nnohup.out457075\t\t\t\t\t   tmpdpj3bfz0wandb-artifacts  tmprfwooa22wandb-artifacts\nnohup.out760266\t\t\t\t\t   tmpdqzzy7v3\t\t       tmpri9xu8i_wandb-media\npycoq\t\t\t\t\t\t   tmpdr6fbpctwandb\t       tmprj4g0kkhwandb\ntest.py\t\t\t\t\t\t   tmpejwo7axlwandb\t       tmp_rla0cb9wandb-media\ntmp\t\t\t\t\t\t   tmpekqp7b2dwandb-media      tmprmrasn0fwandb-media\ntmp03kmjan0wandb\t\t\t\t   tmpf3pk0_3t\t\t       tmpr_yrhzj_wandb\ntmp07zhon11wandb-media\t\t\t\t   tmpf4w8yhsswandb-media      tmprzxltg0lwandb\ntmp0pkwjwg8wandb\t\t\t\t   tmpf_6vd6hkwandb-media      tmps0beul64wandb-media\ntmp0ypuhnktwandb-media\t\t\t\t   tmpf7vuwlipwandb\t       tmps5qf0_w0wandb\ntmp0zk3_ok1wandb\t\t\t\t   tmpfc8ltujrwandb-media      tmpsp2djjg6wandb-artifacts\ntmp14xa24j_wandb\t\t\t\t   tmpfmcmwgb8\t\t       tmpsqe0vylnwandb\ntmp1f3gqdq1wandb-media\t\t\t\t   tmpfqhl6c9vwandb\t       tmpstniop3twandb-media\ntmp1hmrx3xnwandb\t\t\t\t   tmpfvkvyklpwandb-media      tmpsv3n4fi7wandb-media\ntmp1nxq8dmowandb\t\t\t\t   tmpfxuc2zwjwandb-artifacts  tmp_t3mkuy4\ntmp1r2xah97wandb-media\t\t\t\t   tmpg051c49z\t\t       tmptb0urf26wandb\ntmp1sdb3vnqwandb-media\t\t\t\t   tmpg16e6zpxwandb-media      tmptgq1h308wandb-media\ntmp1wq9i7tmwandb-media\t\t\t\t   tmpg2qfjo5pwandb-artifacts  tmpthtghn1wwandb-media\ntmp27k3evykwandb-artifacts\t\t\t   tmpg34wt2g1wandb-media      tmptkp9qpgxwandb-media\ntmp2ncmg9jmwandb-media\t\t\t\t   tmpggaltim9wandb-media      tmptqn9w7rawandb-artifacts\ntmp2qxmugpjwandb-media\t\t\t\t   tmpgj6gyqw6wandb-media      tmptsqb0lwrwandb\ntmp2w92xlzowandb\t\t\t\t   tmpgpv_1hxk\t\t       tmptub9i1zzwandb-media\ntmp39lds7tywandb-media\t\t\t\t   tmpgswv7jpn\t\t       tmpu0k6cuycwandb-media\ntmp3ncj9tdewandb-artifacts\t\t\t   tmpgvz0_o1h\t\t       tmpu6uv_y0pwandb\ntmp3qlpfrylwandb-media\t\t\t\t   tmpgyarr2jxwandb\t       tmpumz7hmaiwandb-artifacts\ntmp3snbanfnwandb\t\t\t\t   tmph6m9dpa_wandb\t       tmpun08cdmwwandb-artifacts\ntmp3xrxd920wandb-artifacts\t\t\t   tmph8n3b36swandb-media      tmp_uqnbz5n\ntmp3zmnx6jxwandb-artifacts\t\t\t   tmphddkq3_3wandb\t       tmpurv7_fe2wandb\ntmp4103eum2wandb\t\t\t\t   tmphmva83y4wandb\t       tmpuwoxzzfvwandb-media\ntmp421qmhu3wandb\t\t\t\t   tmphs6erdxrwandb-media      tmpvb5bk2js\ntmp48khxd0nwandb-artifacts\t\t\t   tmphshrf9juwandb-artifacts  tmpvd_wklrtwandb\ntmp49fv73y2wandb-media\t\t\t\t   tmpi31q87a0wandb-artifacts  tmpvg_71vtdwandb-media\ntmp49sad_g1wandb-artifacts\t\t\t   tmpiu05wr2_wandb\t       tmpvlxyr3eawandb-media\ntmp4c4800_xwandb-media\t\t\t\t   tmpivnhmojfwandb\t       tmpvqmyjo4pwandb-media\ntmp4clbe6xvwandb-media\t\t\t\t   tmpj16iv0rbwandb-media      tmpw10pvrxxwandb-media\ntmp4nuizjduwandb-media\t\t\t\t   tmpj4nmef2_wandb-media      tmpw8eaus7xwandb-media\ntmp5aiik94rwandb-media\t\t\t\t   tmpj6k4pajlwandb-artifacts  tmpw97zp6pqwandb-media\ntmp5jusc1czwandb-media\t\t\t\t   tmpjetcrm92wandb-media      tmpwkzzglljwandb-media\ntmp5ks7vxpqwandb\t\t\t\t   tmp_jfnbfwcwandb-artifacts  tmpwlpoppuwwandb-media\ntmp5ss5gfoqwandb-media\t\t\t\t   tmpjhcfo3sjwandb-media      tmpwok9yxtqwandb-media\ntmp61l257guwandb-media\t\t\t\t   tmpjhkja0n4wandb-media      tmpwqbb7793wandb\ntmp66a_30crwandb\t\t\t\t   tmpjq3bc0iywandb-media      tmpwu7oid1swandb-media\ntmp6_95ss09\t\t\t\t\t   tmpjseq6pjrwandb\t       tmpwwmlqm3gwandb-artifacts\ntmp6eb3e1v_wandb-artifacts\t\t\t   tmpjywyihxswandb\t       tmpwys0txyz\ntmp6ev3bw0kwandb-media\t\t\t\t   tmpk7eb9cxxwandb-artifacts  tmpx0i8_uxdwandb-media\ntmp6j_pagmjwandb-media\t\t\t\t   tmpki8mvo7pwandb\t       tmpxby6g44swandb-media\ntmp6uz84wzpwandb\t\t\t\t   tmpkiqc2rxywandb-media      tmpxdsg3tk8wandb-artifacts\ntmp7dmpqrecwandb\t\t\t\t   tmpklsmildcwandb-media      tmpxm2j1915wandb\ntmp7fzpg3pjwandb-artifacts\t\t\t   tmpkvhsusnzwandb-artifacts  tmp_xmydpcnwandb-media\ntmp7iafm3cywandb-media\t\t\t\t   tmpkvt13pjiwandb\t       tmpxpj1qkhnwandb-media\ntmp7m0tkcx7wandb\t\t\t\t   tmpkxhoutmnwandb\t       tmpxqnwoio_wandb-media\ntmp7p7ko5c1\t\t\t\t\t   tmpl32i_q8cwandb-artifacts  tmpy3sbukw0wandb-artifacts\ntmp7xmnpnjxwandb-media\t\t\t\t   tmpldwit_dswandb-media      tmpy4tqgd9q\ntmp80lef2dvwandb\t\t\t\t   tmplf9oolt5wandb\t       tmpy5mlqvf2\ntmp89e0j4bjwandb-artifacts\t\t\t   tmplgmiofgnwandb-artifacts  tmpy5y0mxbrwandb\ntmp8h7rchd9wandb-artifacts\t\t\t   tmplw1n5b69wandb-media      tmpydoskv75wandb\ntmp8l4njuz2\t\t\t\t\t   tmplx9285iywandb\t       tmpyx791iakwandb-media\ntmp8lxb4u_0wandb\t\t\t\t   tmp_lzx3b9dwandb-media      tmpyy2hv95pwandb-artifacts\ntmp8lyo8smzwandb\t\t\t\t   tmpm1c4zy4twandb-artifacts  tmpz0gx4ikiwandb-media\ntmp8q4h8lu7wandb-artifacts\t\t\t   tmpm2755ginwandb-artifacts  tmpz26cajmh\ntmp_8uvnuf2wandb\t\t\t\t   tmpm56u1aa5wandb-media      tmpz5s198hnwandb-artifacts\ntmp96o0qfii\t\t\t\t\t   tmpm9gk_r6swandb-media      tmpz6oxqu4vwandb-media\ntmp974f9ciawandb-media\t\t\t\t   tmpm_9gv20owandb-media      tmpzgz2lbnmwandb\ntmp98ec7tz8wandb-media\t\t\t\t   tmpmdak3eqkwandb-media      tmpzyqel_hcwandb-artifacts\ntmp9i4bx28vwandb\t\t\t\t   tmpmekovp_5wandb-artifacts  tmpzzjlqqh8wandb-media\ntmp9l8xrnlqwandb-media\t\t\t\t   tmp_mpucxdiwandb-artifacts  ultimate-utils\ntmp9y_56adfwandb-media\t\t\t\t   tmpmsn3sy8mwandb-artifacts  wandb\n<\/code><\/pre>\n<h3>\n<a name=\"additional-files-1\" class=\"anchor\" href=\"#additional-files-1\"><\/a>Additional Files<\/h3>\n<p><em>No response<\/em><\/p>\n<h3>\n<a name=\"environment-2\" class=\"anchor\" href=\"#environment-2\"><\/a>Environment<\/h3>\n<p>WandB version:<br>\n(metalearning_gpu) brando9~ $ python<br>\nPython 3.9.13 (main, Oct 13 2022, 21:15:33)<br>\n[GCC 11.2.0] :: Anaconda, Inc. on linux<br>\nType \u201chelp\u201d, \u201ccopyright\u201d, \u201ccredits\u201d or \u201clicense\u201d for more information.<\/p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>import wandb<br>\nwandb.<strong>version<\/strong><br>\n\u20180.13.5\u2019<\/p>\n<\/blockquote>\n<\/blockquote>\n<\/blockquote>\n<p>OS: ubuntu\/linux<\/p>\n<p>(metalearning_gpu) brando9~ $ cat \/etc\/os-release<br>\nNAME=\u201cUbuntu\u201d<br>\nVERSION=\u201c16.04.7 LTS (Xenial Xerus)\u201d<br>\nID=ubuntu<br>\nID_LIKE=debian<br>\nPRETTY_NAME=\u201cUbuntu 16.04.7 LTS\u201d<br>\nVERSION_ID=\u201c16.04\u201d<br>\nHOME_URL=\u201c<a href=\"http:\/\/www.ubuntu.com\/\" rel=\"noopener nofollow ugc\">http:\/\/www.ubuntu.com\/<\/a>\u201d<br>\nSUPPORT_URL=\u201c<a href=\"http:\/\/help.ubuntu.com\/\" rel=\"noopener nofollow ugc\">http:\/\/help.ubuntu.com\/<\/a>\u201d<br>\nBUG_REPORT_URL=\u201c<a href=\"http:\/\/bugs.launchpad.net\/ubuntu\/\" rel=\"noopener nofollow ugc\">http:\/\/bugs.launchpad.net\/ubuntu\/<\/a>\u201d<br>\nVERSION_CODENAME=xenial<br>\nUBUNTU_CODENAME=xenial<\/p>\n<p>Python version: 3.9.13<\/p>\n<p>Versions of relevant libraries:<\/p>\n<hr>\n<p>related:<\/p>\n<ul>\n<li>cross: <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4535\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: Random tmp files being made -- why? \u00b7 Issue #4535 \u00b7 wandb\/wandb \u00b7 GitHub<\/a>\n<\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/74566670\/how-to-stop-weights-biases-wandb-from-creating-random-tmp-files\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">machine learning - How to stop weights &amp; biases (wandb) from creating random tmp files? - Stack Overflow<\/a><\/li>\n<\/ul>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1669328009419,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":275.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-stop-weights-biases-wandb-from-creating-random-tmp-files\/3460",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-29T14:08:44.794Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/brando\">@brando<\/a> thank you for reporting this. Besides <a href=\"https:\/\/docs.wandb.ai\/guides\/artifacts\/storage\">these directories<\/a> which are controlled by the environment variables <code>WANDB_DIR<\/code>, <code>WANDB_CACHE_DIR<\/code> and <code>WANDB_CONFIG_DIR<\/code>, the wandb SDK in some cases such as logging <code>wandb.Image<\/code> objects or Artifacts it will additionally write some temporary files in the <code>tmp<\/code> directory for code\u2019s efficiency reasons.<\/p>\n<p>It seems that in your system, these were created in your home directory. Could you please post here the output of <code>ls -lah \/t*<\/code> to check the write permissions, and of <code>pwd<\/code> from within the directory where these files were generated? I have filed a feature request to our engineering team to expose an environment variable for this directory so that it can be controlled where to write the temp files. Please let me know if that would help. Also, in terms of logged data, has everything uploaded fine in W&amp;B, and in that case would you need any help to remove these temp files?<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-30T10:27:41.843Z",
                "Answer_body":"<p>HI!<\/p>\n<p>I\u2019m encountering this logging thing in my \/tmp folder when downloading artifacts. From your explanation, I guess it\u2019s the intended behaviour, but is there any way to just avoid wandb to produce these files?<\/p>\n<p>Thanks!<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-04T10:33:07.848Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jceamoran\">@jceamoran<\/a> indeed that\u2019s intended and it isn\u2019t currently possible to switch this off. You can however remove these files once your training has completed and everything has synced with W&amp;B App. May I please ask what\u2019s the reason that you wouldn\u2019t want to have some temporary files written in your disk? I can add this context to an existing feature request to allow users control the location of this directory, would this work for you?<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-05T09:17:13.110Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/thanos-wandb\">@thanos-wandb<\/a> !<\/p>\n<p>I was just wondering how to make my deployments lighter. Nothing I cannot live with. However, being able to choose location of these files would be an interesting feature tho!<\/p>\n<p>Thanks for your answer <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":11.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-08T15:29:01.511Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jceamoran\">@jceamoran<\/a> thanks for the context, good to hear it\u2019s not currently blocking you. You could in the meantime call a command as follows, to cleanup the \/tmp folder after your training has finished and synced everything to W&amp;B App:<br>\n<code>os.system('find \/tmp -type d -name \"*wandb*\" -exec rm -rf {} \\;')<\/code><\/p>\n<p>However, I understand that this might not be ideal to be done by you, so I have also increased the requests for this feature, and we will reach out to you here on any related updates!<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-15T15:23:54.059Z",
                "Answer_body":"<p>Hi, I just noticed this behavior too.  We have literally millions of wandb directories in \/tmp, so just listing the contents of the directory is slow, even though the total space they take up is only a few GB.  I guess it would be nice if wandb did not produce so much trash, and also if it does, maybe they could be stored in per-user subdirectories of \/tmp instead of directly in \/tmp<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stop creat random tmp file million tmp file home folder dont know creat stop anaconda tmpawff artifact tmpmvewoi media anaconda tmpatl media tmpmvhpie media data tmparoiz tmpmwszwea debug cli brando log tmpalkticf artifact tmpnpwh media divers predict success meta learn tmpalsndg media tmpnbxtnojb artifact div instal miniconda tmpambbrm artifact tmpngpyda artifact div instal tmpapcoxet media tmpnqezeggn media iit term synthesi tmpaqfv media tmpocugnz media main tmpaqigpz artifact tmpocxl main tmpawkvgtg media tmpofqlu media main tmpayrbxg media tmponqiggz artifact main err tmpbclycf tmpoqemucl media main err tmpbbrfn artifact tmpoqthmgp media main err tmpbcxatqdi artifact tmppbdbfm main err tmpbgewkz tmppbnpmg media main tmpbsdo media tmppwxmebn main tmpbtlpzom tmpqkfo artifact main tmpbyaui tmpqjryat media main tmpcenlx media tmpqcbf main tmpcmb tmpqexdhpg artifact main tmpcapjmd media tmpqhuuv media main tmpclsbj tmpqhav media main tmpcsncxx media tmpqmimsxi miniconda tmpddhluxm tmpqpfcquw miniconda tmpdhw artifact tmpqtdsjdi artifact nohup tmpdfrjyk media tmp qzpux artifact nohup tmpdhqwaxyg tmprqjau nohup tmpdpjbfz artifact tmprfwooa artifact nohup tmpdqzzyv tmprixui media pycoq tmpdrfbpct tmprjgkkh test tmpejwoaxl tmp rlacb media tmp tmpekqpbd media tmprmrasnf media tmpkmjan tmpfpk tmpr yrhzj tmpzhon media tmpfwyhss media tmprzxltgl tmppkwjwg tmpf vdhk media tmpsbeul media tmpypuhnkt media tmpfvuwlip tmpsqf tmpzk tmpfcltujr media tmpspdjjg artifact tmpxaj tmpfmcmwgb tmpsqevyln tmpfgqdq media tmpfqhlcv tmpstniopt media tmphmrxxn tmpfvkvyklp media tmpsvnfi media tmpnxqdmo tmpfxuczwj artifact tmp tmkui tmprxah media tmpgcz tmptburf tmpsdbvnq media tmpgezpx media tmptgqh media tmpwqitm media tmpgqfjop artifact tmpthtghnw media tmpkevyk artifact tmpgwtg media tmptkpqpgx media tmpncmgjm media tmpggaltim media tmptqnwra artifact tmpqxmugpj media tmpgjgyqw media tmptsqblwr tmpwxlzo tmpgpv hxk tmptubizz media tmpldsty media tmpgswvjpn tmpukcuyc media tmpncjtde artifact tmpgvz tmpuuv tmpqlpfryl media tmpgyarrjx tmpumzhmai artifact tmpsnbanfn tmphmdpa tmpuncdmw artifact tmpxrxd artifact tmphnb media tmp uqnbzn tmpzmnxjx artifact tmphddkq tmpurv tmpeum tmphmvai tmpuwoxzzfv media tmpqmhu tmphserdxr media tmpvbbkj tmpkhxdn artifact tmphshrfju artifact tmpvd wklrt tmpfvy media tmpiqa artifact tmpvg vtd media tmpsad artifact tmpiuwr tmpvlxyrea media tmpc media tmpivnhmojf tmpvqmyjop media tmpclbexv media tmpjivrb media tmpwpvrxx media tmpnuizjdu media tmpjnmef media tmpweausx media tmpaiikr media tmpjkpajl artifact tmpwzppq media tmpjusccz media tmpjetcrm media tmpwkzzgllj media tmpksvxpq tmp jfnbfwc artifact tmpwlpoppuw media tmpssgfoq media tmpjhcfosj media tmpwokyxtq media tmplgu media tmpjhkjan media tmpwqbb tmpa tmpjqbcii media tmpwuoid media tmp tmpjseqpjr tmpwwmlqmg artifact tmpebev artifact tmpjywyihx tmpwystxyz tmpevbwk media tmpkebcxx artifact tmpxi uxd media tmpj pagmj media tmpkimvop tmpxbyg media tmpuzwzp tmpkiqcrxi media tmpxdsgtk artifact tmpdmpqrec tmpklsmildc media tmpxmj tmpfzpgpj artifact tmpkvhsusnz artifact tmp xmydpcn media tmpiafmci media tmpkvtpji tmpxpjqkhn media tmpmtkcx tmpkxhoutmn tmpxqnwoio media tmppkoc tmpli artifact tmpysbukw artifact tmpxmnpnjx media tmpldwit media tmpytqgdq tmplefdv tmplfoolt tmpymlqvf tmpejbj artifact tmplgmiofgn artifact tmpyymxbr tmphrchd artifact tmplwnb media tmpydoskv tmplnjuz tmplxii tmpyxiak media tmplxbu tmp lzxbd media tmpyyhvp artifact tmplyosmz tmpmczyt artifact tmpzgxiki media tmpqhlu artifact tmpmgin artifact tmpzcajmh tmp uvnuf tmpmuaa media tmpzshn artifact tmpoqfii tmpmgk media tmpzoxquv media tmpfcia media tmpm gvo media tmpzgzlbnm tmpectz media tmpmdakeqk media tmpzyqel artifact tmpibxv tmpmekovp artifact tmpzzjlqqh media tmplxrnlq media tmp mpucxdi artifact ultim util tmpy adf media tmpmsnsym artifact addit file respons environ version metalearn gpu brando python python main oct gcc anaconda linux type help copyright credit licens inform import version ubuntu linux metalearn gpu brando cat releas ubuntu version lt xenial xeru ubuntu like debian pretti ubuntu lt version home url http ubuntu com support url http help ubuntu com bug report url http bug launchpad net ubuntu version codenam xenial ubuntu codenam xenial python version version relev librari relat cross cli random tmp file issu github machin learn stop creat random tmp file stack overflow",
        "Question_preprocessed_content":"stop creat random tmp file million tmp file home folder dont know creat stop addit file respons environ version brando python python gcc anaconda linux type help copyright credit licens inform import version brando cat ubuntu lt ubuntu lt python version version relev librari relat cross random tmp file issu github machin learn stop creat random tmp file stack overflow",
        "Question_gpt_summary_original":"The user is facing the challenge of having a large number of tmp files created by wandb in their home folder and is unsure why they are being created and how to stop it. The user is using Ubuntu 16.04.7 LTS and Python 3.9.13 with wandb version 0.13.5.",
        "Question_gpt_summary":"user face challeng have larg number tmp file creat home folder unsur creat stop user ubuntu lt python version",
        "Answer_original_content":"brando thank report directori control environ variabl dir cach dir config dir sdk case log imag object artifact addition write temporari file tmp directori code effici reason creat home directori post output lah check write permiss pwd directori file gener file featur request engin team expos environ variabl directori control write temp file let know help term log data upload fine case need help remov temp file encount log thing tmp folder download artifact explan guess intend behaviour wai avoid produc file thank jceamoran that intend isnt current possibl switch remov file train complet sync app ask what reason wouldnt want temporari file written disk add context exist featur request allow user control locat directori work thano wonder deploy lighter live abl choos locat file interest featur tho thank answer jceamoran thank context good hear current block meantim command follow cleanup tmp folder train finish sync app tmp type exec understand ideal increas request featur reach relat updat notic behavior liter million directori tmp list content directori slow total space guess nice produc trash mayb store user subdirectori tmp instead directli tmp",
        "Answer_preprocessed_content":"thank report directori control environ variabl sdk case log object artifact addition write temporari file directori code effici reason creat home directori post output check write permiss directori file gener file featur request engin team expos environ variabl directori control write temp file let know help term log data upload fine case need help remov temp file encount log thing folder download artifact explan guess intend behaviour wai avoid produc file thank that intend isnt current possibl switch remov file train complet sync app ask what reason wouldnt want temporari file written disk add context exist featur request allow user control locat directori work wonder deploy lighter live abl choos locat file interest featur tho thank answer thank context good hear current block meantim command follow cleanup folder train finish sync app understand ideal increas request featur reach relat updat notic behavior liter million directori list content directori slow total space guess nice produc trash mayb store subdirectori instead directli",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n1. check the write permissions of the tmp directory and the directory where the files were generated.\n2. remove the temporary files manually after the training has completed and synced everything to w&b app.\n3. request the engineering team to expose an environment variable for the directory where the temp files are written so that it can be controlled.\n4. request the feature to allow users to control the location of the directory where the temp files are written.\n5. store the temp files in per-user subdirectories of \/tmp instead of directly in \/tmp.",
        "Answer_gpt_summary":"possibl solut extract answer check write permiss tmp directori directori file gener remov temporari file manual train complet sync app request engin team expos environ variabl directori temp file written control request featur allow user control locat directori temp file written store temp file user subdirectori tmp instead directli tmp"
    },
    {
        "Question_id":62215724.0,
        "Question_title":"Does Kedro support Checkpointing\/Caching of Results?",
        "Question_body":"<p>Let's say we have multiple long running pipeline nodes.\nIt seems quite straight forward to checkpoint or cache the intermediate results, so when nodes after a checkpoint are changed or added only these nodes must be executed again.<\/p>\n\n<p>Does Kedro provide functionality to make sure, that when I run the pipeline only those steps are \nexecuted that have changed?\nAlso the reverse, is there a way to make sure, that all steps that have changed are executed?<\/p>\n\n<p>Let's say a pipeline producing some intermediate result changed, will it be executed, when i execute a pipeline depending on the output of the first?<\/p>\n\n<p><strong>TL;DR:<\/strong> Does Kedro have <code>makefile<\/code>-like tracking of what needs to be done and what not?<\/p>\n\n<p>I think my question is similar to <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/341\" rel=\"nofollow noreferrer\">issue #341<\/a>, but I do not require support of cyclic graphs.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591361290233,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":367.0,
        "Answer_body":"<p>You might want to have a look at the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.IncrementalDataSet.html\" rel=\"nofollow noreferrer\">IncrementalDataSet<\/a> alongside the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">partitioned dataset<\/a> documentation, specifically the section on <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_user_guide\/08_advanced_io.html#incremental-loads-with-incrementaldataset\" rel=\"nofollow noreferrer\">incremental loads with the incremental dataset<\/a> which has a notion of \"checkpointing\", although checkpointing is a manual step and not automated like <code>makefile<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62215724",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1591362515296,
        "Question_original_content":"support checkpoint cach result let multipl long run pipelin node straight forward checkpoint cach intermedi result node checkpoint chang ad node execut provid function sure run pipelin step execut chang revers wai sure step chang execut let pipelin produc intermedi result chang execut execut pipelin depend output makefil like track need think question similar issu requir support cyclic graph",
        "Question_preprocessed_content":"support result let multipl long run pipelin node straight forward checkpoint cach intermedi result node checkpoint chang ad node execut provid function sure run pipelin step execut chang revers wai sure step chang execut let pipelin produc intermedi result chang execut execut pipelin depend output like track need think question similar issu requir support cyclic graph",
        "Question_gpt_summary_original":"The user is inquiring if Kedro supports checkpointing or caching of intermediate results to avoid re-executing long running pipeline nodes. They are also asking if Kedro has functionality to track changes and only execute the necessary steps, as well as if changes to intermediate results will trigger the execution of dependent pipelines.",
        "Question_gpt_summary":"user inquir support checkpoint cach intermedi result avoid execut long run pipelin node ask function track chang execut necessari step chang intermedi result trigger execut depend pipelin",
        "Answer_original_content":"want look incrementaldataset alongsid partit dataset document specif section increment load increment dataset notion checkpoint checkpoint manual step autom like makefil",
        "Answer_preprocessed_content":"want look incrementaldataset alongsid partit dataset document specif section increment load increment dataset notion checkpoint checkpoint manual step autom like",
        "Answer_gpt_summary_original":"the solution to checkpoint or cache intermediate results for efficient pipeline runs is to use the \"incrementaldataset\" alongside the partitioned dataset documentation. the \"incremental dataset\" has a feature called \"checkpointing\" which can be used to manually checkpoint the intermediate results. however, it is not automated like makefile.",
        "Answer_gpt_summary":"solut checkpoint cach intermedi result effici pipelin run us incrementaldataset alongsid partit dataset document increment dataset featur call checkpoint manual checkpoint intermedi result autom like makefil"
    },
    {
        "Question_id":null,
        "Question_title":"DVC push to SSH remote ERROR: No such file",
        "Question_body":"<p>I do the Getting started guide.<br>\nWhen it is time to add a remote, instead of proposed S3 storage, I use an SSH remote that I add using the following command:<\/p>\n<pre><code>dvc remote add -d storage ssh:\/\/ws\/hddb\/data\/dvc-tutorial\n<\/code><\/pre>\n<p>where ws is a hostname of a workstation with the SSH access enabled, and <code>hddb\/data\/dvc-tutorial<\/code> is the path to the directory in which I would like to keep the data. I actually execute this command being on the workstation through SSH (I can recursively ssh to this workstation when I am already ssh\u2019ed to it).<\/p>\n<p>Then I have to execute the following command to use passphrase-protected SSH keys:<\/p>\n<pre><code>dvc remote modify --local storage password my-lovely-passphrase\n<\/code><\/pre>\n<p>When I do <code>dvc push<\/code> to transfer <code>data\/data.xml<\/code> from the Getting started guide, it fails when the following error:<\/p>\n<pre><code class=\"lang-auto\">\u279c dvc push                                             \nERROR: failed to transfer 'md5: 22a1a2931c8370d3aeedd7183606fd7f' - [Errno 2] No such file or directory: No such file\nERROR: failed to push data to the cloud - 1 files failed to upload\n<\/code><\/pre>\n<p>What should I do to make an SSH remote work? Thank you!<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1654677490086,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":283.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-to-ssh-remote-error-no-such-file\/1203",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-08T09:17:20.147Z",
                "Answer_body":"<p>Hey there,<br>\nit may be due to the username (and possibly port, if it\u2019s not 22) missing from the ssh remote url. I\u2019d try again with:<\/p>\n<p>dvc remote add -d storage ssh:\/\/username@ws\/hddb\/data\/dvc-tutorial<\/p>",
                "Answer_score":11.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-08T09:26:37.698Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dtrifiro\">@dtrifiro<\/a>. I\u2019ve just tried it with username added but it still gives the same error<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-08T09:28:36.269Z",
                "Answer_body":"<p>Are you able to login using <code>ssh username@ws<\/code>? Does the path exist on the workstation and does the user you\u2019re connecting as have the permissions to read\/write to the given path?<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-08T09:38:40.105Z",
                "Answer_body":"<p>Ooops, yeah, the problem was with the write permissions of the users for the directory path <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nThe error message was a bit misleading in that sense.<br>\nDear <a class=\"mention\" href=\"\/u\/dtrifiro\">@dtrifiro<\/a> thank you very much!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-08T09:41:02.870Z",
                "Answer_body":"<p>I agree, the message is a bit misleading. If you want, you could open up an issue on github so that we can keep track of that.<\/p>\n<p>Happy to hear that everything is working <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":15.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"push ssh remot error file get start guid time add remot instead propos storag us ssh remot add follow command remot add storag ssh hddb data tutori hostnam workstat ssh access enabl hddb data tutori path directori like data actual execut command workstat ssh recurs ssh workstat sshed execut follow command us passphras protect ssh kei remot modifi local storag password love passphras push transfer data data xml get start guid fail follow error push error fail transfer aacdaeeddfdf errno file directori file error fail push data cloud file fail upload ssh remot work thank",
        "Question_preprocessed_content":"push ssh remot error file get start guid time add remot instead propos storag us ssh remot add follow command hostnam workstat ssh access enabl path directori like data actual execut command workstat ssh execut follow command us ssh kei transfer get start guid fail follow error ssh remot work thank",
        "Question_gpt_summary_original":"The user encountered an error while trying to push data to an SSH remote using DVC. The error message indicates that the file being transferred cannot be found. The user added the remote using the correct command and modified it to use passphrase-protected SSH keys. The user is seeking advice on how to resolve the issue and make the SSH remote work.",
        "Question_gpt_summary":"user encount error try push data ssh remot error messag indic file transfer user ad remot correct command modifi us passphras protect ssh kei user seek advic resolv issu ssh remot work",
        "Answer_original_content":"hei usernam possibl port miss ssh remot url try remot add storag ssh usernam hddb data tutori dtrifiro iv tri usernam ad give error abl login ssh usernam path exist workstat user your connect permiss read write given path yeah problem write permiss user directori path error messag bit mislead sens dear dtrifiro thank agre messag bit mislead want open issu github track happi hear work",
        "Answer_preprocessed_content":"hei usernam miss ssh remot url try remot add storag iv tri usernam ad give error abl login path exist workstat user your connect permiss given path yeah problem write permiss user directori path error messag bit mislead sens dear thank agre messag bit mislead want open issu github track happi hear work",
        "Answer_gpt_summary_original":"the possible solutions to the error \"no such file or directory\" when pushing data to an ssh remote are: \n1. check if the username and port are missing from the ssh remote url and add them if necessary.\n2. verify if the path exists on the workstation and if the user has the necessary permissions to read\/write to the given path. \n3. check the write permissions of the users for the directory path. \n4. open up an issue on github to keep track of the misleading error message.",
        "Answer_gpt_summary":"possibl solut error file directori push data ssh remot check usernam port miss ssh remot url add necessari verifi path exist workstat user necessari permiss read write given path check write permiss user directori path open issu github track mislead error messag"
    },
    {
        "Question_id":32422626.0,
        "Question_title":"Part of speech tagging and entity recognition - python",
        "Question_body":"<p>I want to perform part of speech tagging and entity recognition in python similar to Maxent_POS_Tag_Annotator and Maxent_Entity_Annotator functions of openNLP in R.  I would prefer a code in python which takes input as textual sentence and gives output as different features- like number of \"CC\", number of \"CD\", number of \"DT\" etc.. CC, CD, DT are POS tags as used in Penn Treebank. So there should be 36 columns\/features for POS tagging corresponding to 36 POS tags as in <a href=\"http:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\" rel=\"nofollow\">Penn Treebank POS<\/a>. I want to implement this on Azure ML \"Execute Python Script\" module and Azure ML supports python 2.7.7. I heard nltk in python may does the job, but I am a beginner on python. Any help would be appreciated. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1441535794287,
        "Question_favorite_count":null,
        "Question_last_edit_time":1441776651843,
        "Question_score":0.0,
        "Question_view_count":1014.0,
        "Answer_body":"<p>Take a look at <a href=\"http:\/\/www.nltk.org\/book\/ch05.html\" rel=\"nofollow\">NTLK book<\/a>, Categorizing and Tagging Words section.<\/p>\n\n<p>Simple example, it uses the Penn Treebank tagset:<\/p>\n\n<pre><code>from nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\npos_tag(word_tokenize(\"John's big idea isn't all that bad.\")) \n\n[('John', 'NNP'),\n(\"'s\", 'POS'),\n ('big', 'JJ'),\n ('idea', 'NN'),\n ('is', 'VBZ'),\n (\"n't\", 'RB'),\n ('all', 'DT'),\n ('that', 'DT'),\n ('bad', 'JJ'),\n ('.', '.')]\n<\/code><\/pre>\n\n<p>Then you can use<\/p>\n\n<pre><code>from collections import defaultdict\ncounts = defaultdict(int)\nfor (word, tag) in pos_tag(word_tokenize(\"John's big idea isn't all that bad.\")):\n    counts[tag] += 1\n<\/code><\/pre>\n\n<p>to get frequencies:<\/p>\n\n<pre><code>defaultdict(&lt;type 'int'&gt;, {'JJ': 2, 'NN': 1, 'POS': 1, '.': 1, 'RB': 1, 'VBZ': 1, 'DT': 2, 'NNP': 1})\n<\/code><\/pre>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32422626",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1441539548048,
        "Question_original_content":"speech tag entiti recognit python want perform speech tag entiti recognit python similar maxent po tag annot maxent entiti annot function opennlp prefer code python take input textual sentenc give output differ featur like number number number po tag penn treebank column featur po tag correspond po tag penn treebank po want implement execut python script modul support python heard nltk python job beginn python help appreci",
        "Question_preprocessed_content":"speech tag entiti recognit python want perform speech tag entiti recognit python similar function opennlp prefer code python take input textual sentenc give output differ featur like number number number po tag penn treebank po tag correspond po tag penn treebank po want implement execut python script modul support python heard nltk python job beginn python help appreci",
        "Question_gpt_summary_original":"The user is facing a challenge in performing part of speech tagging and entity recognition in Python, similar to the Maxent_POS_Tag_Annotator and Maxent_Entity_Annotator functions of openNLP in R. They require a code in Python that takes input as a textual sentence and gives output as different features, such as the number of \"CC\", \"CD\", \"DT\", etc. The user wants to implement this on Azure ML \"Execute Python Script\" module, which supports Python 2.7.7. Although they have heard that nltk in Python may do the job, they are a beginner in Python and would appreciate any help.",
        "Question_gpt_summary":"user face challeng perform speech tag entiti recognit python similar maxent po tag annot maxent entiti annot function opennlp requir code python take input textual sentenc give output differ featur number user want implement execut python script modul support python heard nltk python job beginn python appreci help",
        "Answer_original_content":"look ntlk book categor tag word section simpl exampl us penn treebank tagset nltk tag import po tag nltk token import word token po tag word token john big idea isn bad john nnp po big idea vbz bad us collect import defaultdict count defaultdict int word tag po tag word token john big idea isn bad count tag frequenc defaultdict po vbz nnp",
        "Answer_preprocessed_content":"look ntlk book categor tag word section simpl exampl us penn treebank tagset us frequenc",
        "Answer_gpt_summary_original":"the answer suggests using the nltk book and its categorizing and tagging words section to perform part of speech tagging and entity recognition in python. it provides a simple example using the penn treebank tagset and shows how to get frequencies using the defaultdict module.",
        "Answer_gpt_summary":"answer suggest nltk book categor tag word section perform speech tag entiti recognit python provid simpl exampl penn treebank tagset show frequenc defaultdict modul"
    },
    {
        "Question_id":null,
        "Question_title":"Upgrade mlflow",
        "Question_body":"The following error is thrown when I upgrade mlflow from 1.13.1 to 1.18.0\n\n\nalembic.util.exc.CommandError: Can't locate revision identified by 'a8c4a736bde6' mlflow\n\n\n\nWould deleting the alembic version fix the issue or is there any alternate solution?\n\n\nThank you and Warm Regards",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624850016000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":114.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CX4PiIGYZiE",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-28T03:36:33",
                "Answer_body":"The issue is resolved - please ignore, I upgraded my mlflow version to the latest which was 1.7.2 in the conda environment from where I was trying to fire mlflow db uprade command\u00a0\u00a0\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"upgrad follow error thrown upgrad alemb util exc commanderror locat revis identifi acabd delet alemb version fix issu altern solut thank warm regard",
        "Question_preprocessed_content":"upgrad follow error thrown upgrad locat revis identifi bde delet alemb version fix issu altern solut thank warm regard",
        "Question_gpt_summary_original":"The user encountered an error when upgrading mlflow from version 1.13.1 to 1.18.0, specifically an error related to locating a revision identified by 'a8c4a736bde6' mlflow. The user is seeking advice on whether deleting the alembic version would resolve the issue or if there is an alternate solution.",
        "Question_gpt_summary":"user encount error upgrad version specif error relat locat revis identifi acabd user seek advic delet alemb version resolv issu altern solut",
        "Answer_original_content":"issu resolv ignor upgrad version latest conda environ try uprad command",
        "Answer_preprocessed_content":"issu resolv ignor upgrad version latest conda environ try uprad command",
        "Answer_gpt_summary_original":"Solution: The user resolved the issue by upgrading their mlflow version to the latest version (1.7.2) in their conda environment, which allowed them to successfully run the mlflow db upgrade command. No alternative solutions were mentioned.",
        "Answer_gpt_summary":"solut user resolv issu upgrad version latest version conda environ allow successfulli run upgrad command altern solut mention"
    },
    {
        "Question_id":null,
        "Question_title":"Request GPU Quota increase",
        "Question_body":"For my Machine Learning Experiments I need a dedicated VM with GPU. They are not available in my region (only low priority VMs). The system tells me to request a quota increase but if I go to the quota increase support page I can only select additional CPU's not a GPU for a dedicated VM. How do I request a GPU quota increase?",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1615999382250,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/319120\/request-gpu-quota-increase.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-18T00:57:11.537Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nSorry for the confuse description. Could you please try as below with \"Machine Learning Service\" and describe your need with the support engineer? I am also checking internally to see if there any direct way to increase the quota.\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"request gpu quota increas machin learn experi need dedic gpu avail region low prioriti vm tell request quota increas quota increas support page select addit cpu gpu dedic request gpu quota increas",
        "Question_preprocessed_content":"request gpu quota increas machin learn experi need dedic gpu avail region tell request quota increas quota increas support page select addit cpu gpu dedic request gpu quota increas",
        "Question_gpt_summary_original":"The user needs a dedicated VM with GPU for their machine learning experiments, but they are not available in their region. The system suggests requesting a quota increase, but the user can only select additional CPUs and not a GPU for a dedicated VM on the quota increase support page. The user is seeking guidance on how to request a GPU quota increase.",
        "Question_gpt_summary":"user need dedic gpu machin learn experi avail region suggest request quota increas user select addit cpu gpu dedic quota increas support page user seek guidanc request gpu quota increas",
        "Answer_original_content":"hello sorri confus descript try machin learn servic need support engin check intern direct wai increas quota regard yutong",
        "Answer_preprocessed_content":"hello sorri confus descript try machin learn servic need support engin check intern direct wai increas quota regard yutong",
        "Answer_gpt_summary_original":"possible solutions: \n1. try requesting a gpu quota increase through \"machine learning service\" and describe the need to the support engineer. \n2. yutong is checking internally to see if there is any direct way to increase the quota. \n\nsummary: \nthe user can try requesting a gpu quota increase through \"machine learning service\" and describe their need to the support engineer. yutong is also checking internally to see if there is any direct way to increase the quota.",
        "Answer_gpt_summary":"possibl solut try request gpu quota increas machin learn servic need support engin yutong check intern direct wai increas quota summari user try request gpu quota increas machin learn servic need support engin yutong check intern direct wai increas quota"
    },
    {
        "Question_id":53213596.0,
        "Question_title":"How to execute python from conda environment by dvc run",
        "Question_body":"<p>I have an environment of conda configurated with python 3.6 and dvc is installed there, but when I try to execute dvc run with python, dvc call the python version of main installation of conda and not find the installed libraries.<\/p>\n\n<pre><code>$ conda activate py36\n$ python --version\nPython 3.6.6 :: Anaconda custom (64-bit)\n$ dvc run python --version\nRunning command:\n    python --version\nPython 3.7.0\nSaving information to 'Dvcfile'.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1541699972677,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":6.0,
        "Question_view_count":351.0,
        "Answer_body":"<p>The version 0.24.3 of dvc correct this problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53213596",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1549414531500,
        "Question_original_content":"execut python conda environ run environ conda configur python instal try execut run python python version main instal conda instal librari conda activ python version python anaconda custom bit run python version run command python version python save inform file",
        "Question_preprocessed_content":"execut python conda environ run environ conda configur python instal try execut run python python version main instal conda instal librari",
        "Question_gpt_summary_original":"The user is facing challenges in executing python from a conda environment using dvc run. When attempting to execute dvc run with python, dvc calls the python version of the main installation of conda and is unable to find the installed libraries.",
        "Question_gpt_summary":"user face challeng execut python conda environ run attempt execut run python call python version main instal conda unabl instal librari",
        "Answer_original_content":"version correct problem",
        "Answer_preprocessed_content":"version correct problem",
        "Answer_gpt_summary_original":"the solution to the challenge of executing python from a conda environment is to upgrade to version 0.24.3, which fixes the issue of the 'run' command calling the main installation of conda instead of the installed libraries.",
        "Answer_gpt_summary":"solut challeng execut python conda environ upgrad version fix issu run command call main instal conda instead instal librari"
    },
    {
        "Question_id":73113256.0,
        "Question_title":"Hyperparameter data types and scales not being validated",
        "Question_body":"<p>On past week, I was implementing some code to <a href=\"https:\/\/github.com\/explosion\/spaCy\/discussions\/11126#discussioncomment-3191163\" rel=\"nofollow noreferrer\">tune hyperparameters on a spaCy model, using Vertex AI<\/a>. From that experience, I have several questions, but since they might no be directly related to each other, I decided to open one case per each question.<\/p>\n<p>In this case, I would like to understand what is exactly going on, when I set the following hyperparameters, in some HP tuning job:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/w4C78.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/w4C78.png\" alt=\"hyperparameters\" \/><\/a><\/p>\n<p>Notice <strong>both examples have been purposedly written 'wrongly' to trigger an error but 'eerily', they don't<\/strong> (UPDATE: at least with my current understanding of the docs). I have the sensation that <em>&quot;Vertex AI does not make any validation of the inserted values, they just run whatever you write, and trigger an error only if the values don't actually make ANY sense&quot;<\/em>. Allow me to insert a couple of comments on each example:<\/p>\n<ul>\n<li><code>dropout<\/code>: This variable should be <em>&quot;scaled linearly between 0 and 1&quot;<\/em> ... However what I can see in the HP tuning jobs, are values <em>&quot;scaled linearly between 0.1 and 0.3, and nothing in the interval 0.3 to 0.5&quot;<\/em>. Now this reasoning is a bit naive, as I am not 100% sure if <a href=\"https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization\" rel=\"nofollow noreferrer\">this algorithm<\/a> had to do in the values selection, or <em>&quot;Google Console understood I only had the interval [0.1,0.3] to choose values from&quot;<\/em>. (UPDATE) Plus, how can a variable be &quot;discrete and linear&quot; at the same time?<\/li>\n<li><code>batch_size<\/code>: I think I know what's going on with this one, I just want to confirm: 3 categorical values (&quot;500&quot;, &quot;1000&quot; &amp; &quot;2000&quot;) are being selected &quot;as they are&quot;, since they have a SHP of &quot;UNESPECIFIED&quot;.<\/li>\n<\/ul>\n<p>(*) Notice both the HP names, as well as their values, were just &quot;examples on the spot&quot;, they don't intend to be &quot;good starting points&quot;. HP tuning initial values selection is NOT the point of this query.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7,
        "Question_creation_time":1658770653850,
        "Question_favorite_count":null,
        "Question_last_edit_time":1660741967556,
        "Question_score":0.0,
        "Question_view_count":133.0,
        "Answer_body":"<p>If the type is Categorical, then the scale type is irrelevant and ignored. If the type is DoubleValueSpec, IntegerValueSpec, or DiscreteValueSpec, then the scale type will govern which values are picked more often.<\/p>\n<p>Regarding how a variable can be both Discrete and have a scale: Discrete variables are still numeric in nature. For example, if the discrete values are <code>[1, 10, 100]<\/code>, the ScaleType will determine whether the optimization algorithm considers &quot;distance&quot; between 1 and 10 versus 10 and 100 the same (if logarithmic) or smaller (if linear).<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1660850228732,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73113256",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660669810152,
        "Question_original_content":"hyperparamet data type scale valid past week implement code tune hyperparamet spaci model experi question directli relat decid open case question case like understand exactli go set follow hyperparamet tune job notic exampl purposedli written wrongli trigger error eerili updat current understand doc sensat valid insert valu run write trigger error valu actual sens allow insert coupl comment exampl dropout variabl scale linearli tune job valu scale linearli interv reason bit naiv sure algorithm valu select googl consol understood interv choos valu updat plu variabl discret linear time batch size think know go want confirm categor valu select shp unespecifi notic name valu exampl spot intend good start point tune initi valu select point queri thank",
        "Question_preprocessed_content":"hyperparamet data type scale valid past week implement code tune hyperparamet spaci model experi question directli relat decid open case question case like understand exactli go set follow hyperparamet tune job notic exampl purposedli written wrongli trigger error eerili sensat valid insert valu run write trigger error valu actual sens allow insert coupl comment exampl variabl scale linearli tune job valu scale linearli interv reason bit naiv sure algorithm valu select googl consol understood interv choos valu plu variabl discret linear time think know go want confirm categor valu select shp unespecifi notic name valu exampl spot intend good start point tune initi valu select point queri thank",
        "Question_gpt_summary_original":"The user is facing challenges with hyperparameter tuning on a spaCy model using Vertex AI. They are concerned that Vertex AI does not validate the data types and scales of the hyperparameters, and only triggers an error if the values do not make sense. The user provides examples of hyperparameters, such as dropout and batch size, that are not being validated properly. They are unsure if the algorithm or Google Console is responsible for the selection of values. The user is seeking clarification on the issue.",
        "Question_gpt_summary":"user face challeng hyperparamet tune spaci model concern valid data type scale hyperparamet trigger error valu sens user provid exampl hyperparamet dropout batch size valid properli unsur algorithm googl consol respons select valu user seek clarif issu",
        "Answer_original_content":"type categor scale type irrelev ignor type doublevaluespec integervaluespec discretevaluespec scale type govern valu pick variabl discret scale discret variabl numer natur exampl discret valu scaletyp determin optim algorithm consid distanc versu logarithm smaller linear",
        "Answer_preprocessed_content":"type categor scale type irrelev ignor type doublevaluespec integervaluespec discretevaluespec scale type govern valu pick variabl discret scale discret variabl numer natur exampl discret valu scaletyp determin optim algorithm consid distanc versu smaller",
        "Answer_gpt_summary_original":"the answer suggests that if the type of hyperparameter is categorical, then the scale type is irrelevant. however, if the type is doublevaluespec, integervaluespec, or discretevaluespec, then the scale type will determine which values are picked more often. additionally, the answer clarifies that discrete variables are still numeric in nature and the scaletype will determine how the optimization algorithm considers the distance between the values.",
        "Answer_gpt_summary":"answer suggest type hyperparamet categor scale type irrelev type doublevaluespec integervaluespec discretevaluespec scale type determin valu pick addition answer clarifi discret variabl numer natur scaletyp determin optim algorithm consid distanc valu"
    },
    {
        "Question_id":61984217.0,
        "Question_title":"Invalid base64: \"{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}\" when testing Amazon SageMaker model endpoint using the AWS CLI",
        "Question_body":"<p>I am new to Amazon SageMaker and I am closely following this tutorial <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-a-machine-learning-powered-rest-api-with-amazon-api-gateway-mapping-templates-and-amazon-sagemaker\/<\/a> to create a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker<\/p>\n\n<p>when I run the following command on terminal (<strong>Step 2 of the Tutorial<\/strong> )<\/p>\n\n<pre><code>aws sagemaker-runtime invoke-endpoint \\\n  --endpoint-name &lt;endpoint-name&gt; \\\n  --body '{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}' \\\n  --content-type application\/json \\\n  --accept application\/json \\\n  results\n<\/code><\/pre>\n\n<p>I get the following <strong>Error:<\/strong> <code>Invalid base64: \"{\"instances\": [{\"in0\":[863],\"in1\":[882]}]}\"<\/code>\nMy endpoint is <code>InService<\/code> on the SageMaker console and the example Jupyter notebook run successfully. (I also substituted <code>&lt;endpoint-name&gt;<\/code> with the actual name - same error received with\/without quotations around the name) <\/p>\n\n<p>Using <strong>zsh<\/strong> here is the aws cli version:<\/p>\n\n<pre><code>aws --version\naws-cli\/2.0.15 Python\/3.7.4 Darwin\/19.4.0 botocore\/2.0.0dev19\n<\/code><\/pre>\n\n<p>Wondering what the problem could be. Any help is appreciated<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1590313799700,
        "Question_favorite_count":null,
        "Question_last_edit_time":1590314948843,
        "Question_score":3.0,
        "Question_view_count":1117.0,
        "Answer_body":"<p>The problem is that the body contents is being expected to be base 64 encoded, try base64 encoding the body before passing it to the invoke statement.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61984217",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1590314994627,
        "Question_original_content":"invalid base instanc test model endpoint aw cli new close follow tutori http aw amazon com blog machin learn creat machin learn power rest api amazon api gatewai map templat amazon creat machin learn power rest api amazon api gatewai map templat run follow command termin step tutori runtim invok endpoint endpoint bodi instanc content type applic json accept applic json result follow error invalid base instanc endpoint inservic consol exampl jupyt notebook run successfulli substitut actual error receiv quotat zsh aw cli version aw version aw cli python darwin botocor dev wonder problem help appreci",
        "Question_preprocessed_content":"invalid base instanc test model endpoint aw cli new close follow tutori creat machin rest api amazon api gatewai map templat run follow command termin follow error endpoint consol exampl jupyt notebook run successfulli zsh aw cli version wonder problem help appreci",
        "Question_gpt_summary_original":"The user encountered an error \"Invalid base64\" when testing an Amazon SageMaker model endpoint using the AWS CLI. The user followed a tutorial to create a machine learning-powered REST API with Amazon API Gateway mapping templates and Amazon SageMaker. The endpoint is InService on the SageMaker console and the example Jupyter notebook run successfully. The user is using zsh and AWS CLI version 2.0.15.",
        "Question_gpt_summary":"user encount error invalid base test model endpoint aw cli user follow tutori creat machin learn power rest api amazon api gatewai map templat endpoint inservic consol exampl jupyt notebook run successfulli user zsh aw cli version",
        "Answer_original_content":"problem bodi content expect base encod try base encod bodi pass invok statement",
        "Answer_preprocessed_content":"problem bodi content expect base encod try base encod bodi pass invok statement",
        "Answer_gpt_summary_original":"the solution to the \"invalid base64\" error when testing a model endpoint using the aws cli is to base64 encode the body contents before passing it to the invoke statement.",
        "Answer_gpt_summary":"solut invalid base error test model endpoint aw cli base encod bodi content pass invok statement"
    },
    {
        "Question_id":54992434.0,
        "Question_title":"MissingRequiredParameter: Missing required key 'FunctionName' in params",
        "Question_body":"<p>I'm working on a supervised machine learning problem, and I am setting up a custom labeling task to send out to Amazon Mechanical Turk for human annotation.<\/p>\n\n<p>I have uploaded the data to AWS S3 in the json-lines (<code>.jsonl<\/code>) format as follows, pursuant to the instructions as specified in the AWS documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html<\/a>: <\/p>\n\n<pre><code>{\"source\": \"value0\"}\n{\"source\": \"value1\"}    \n{\"source\": \"value2\"}\n...\n{\"source\": \"value2\"}\n<\/code><\/pre>\n\n<p>When I click on the default text classification template, I can see my data come through and everything appears to work.<\/p>\n\n<p>However, I am getting the following error when I attempt to use the custom annotation task template interface: <code>MissingRequiredParameter: Missing required key 'FunctionName' in params<\/code> <\/p>\n\n<p>The error resembles an AWS Lambda error, except the strange thing is that I am not using AWS Lambda. Suggestions for how to proceed? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1551737856450,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":7776.0,
        "Answer_body":"<p>I am from the engineering team and happy to help you here. I think the issue is not related to the manifest as it looks correct to me. The error suggests that you may haven't provided a correct lambda ARN for pre or post labeling task. Please see this doc for more details: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html<\/a><\/p>\n\n<p>I can also help further if you can send me details on how you starting the job and what parameters you are sending.  <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54992434",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1552342307056,
        "Question_original_content":"missingrequiredparamet miss requir kei functionnam param work supervis machin learn problem set custom label task send amazon mechan turk human annot upload data aw json line jsonl format follow pursuant instruct specifi aw document http doc aw amazon com latest sm data input html sourc valu sourc valu sourc valu sourc valu click default text classif templat data come appear work get follow error attempt us custom annot task templat interfac missingrequiredparamet miss requir kei functionnam param error resembl aw lambda error strang thing aw lambda suggest proce",
        "Question_preprocessed_content":"missingrequiredparamet miss requir kei functionnam param work supervis machin learn problem set custom label task send amazon mechan turk human annot upload data aw format follow pursuant instruct specifi aw document click default text classif templat data come appear work get follow error attempt us custom annot task templat interfac error resembl aw lambda error strang thing aw lambda suggest proce",
        "Question_gpt_summary_original":"The user is encountering an error message \"MissingRequiredParameter: Missing required key 'FunctionName' in params\" while setting up a custom labeling task for a supervised machine learning problem on Amazon Mechanical Turk. The user has uploaded the data to AWS S3 in the json-lines format and is not using AWS Lambda. The user is seeking suggestions on how to proceed.",
        "Question_gpt_summary":"user encount error messag missingrequiredparamet miss requir kei functionnam param set custom label task supervis machin learn problem amazon mechan turk user upload data aw json line format aw lambda user seek suggest proce",
        "Answer_original_content":"engin team happi help think issu relat manifest look correct error suggest haven provid correct lambda arn pre post label task doc detail http doc aw amazon com latest sm custom templat step html help send detail start job paramet send",
        "Answer_preprocessed_content":"engin team happi help think issu relat manifest look correct error suggest haven provid correct lambda arn pre post label task doc detail help send detail start job paramet send",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- check if the correct lambda arn for pre or post labeling task is provided.\n- refer to the documentation for more details on custom templates.\n- provide details on how the job is started and what parameters are being sent to get further help.",
        "Answer_gpt_summary":"possibl solut answer check correct lambda arn pre post label task provid refer document detail custom templat provid detail job start paramet sent help"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Studio - create domain error",
        "Question_body":"A customer is trying to setup Sagemaker studio. He is following our published instructions to set up using IAM: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-iam.html\n\nBut is getting an error: User: arn:aws:iam:xxxx:user\/user1 is not authorized to perform: sagemaker:CreateDomain on resource: arn:aws:sagemaker: us-east-2:xxxx:domain\/yyyy\n\nHe has admin priviledges on the account and AmazonSageMakerFullAccess. We noticed that the AmazonSageMakerFullAccess policy actually has a limitation. You can perform all sagemaker actions, but not on a resource with arn \u201carn:aws:sagemaker:::domain\/*\u201d. We confirmed there are no other domains in that region with the CLI as you are only allowed one \u2013 so that isn\u2019t blocking. And aws sagemaker list-user-profiles returns no user profiles.\n\nHas anyone seen that error before or know the workaround? Should he create a custom policy to enable creating domains or would there be any implications of that? Are there specific permissions he should have so as to onboard using IAM?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586796156000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":792.0,
        "Answer_body":"A user with admin privileges would have access to \"iam:CreateServiceLinkedRole\" and \"sagemaker:CreateDomain\" actions, unless SCPs or permissions boundaries are involved. However, for the purpose of onboarding Amazon SageMaker Studio with limited permissions, I would grant the user least privilege by reviewing Control Access to the Amazon SageMaker API by Using Identity-based Policies and Actions, Resources, and Condition Keys for Amazon SageMaker documentation:\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"sagemaker:CreateDomain\",\n    \"Resource\": \"arn:aws:sagemaker:<REGION>:<ACCOUNT-ID>:domain\/*\"\n}\n\n\nNOTE: An AWS account is limited to one Domain, per region, see CreateDomain.\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"iam:CreateServiceLinkedRole\",\n    \"Resource\": \"*\",\n    \"Condition\": {\n        \"StringEquals\": {\n            \"iam:AWSServiceName\": \"sagemaker.amazonaws.com\"\n        }\n    }\n}\n\n\nCheers!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyWQfPusnSHG6Ujfzx27o1w\/sagemaker-studio-create-domain-error",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-04-13T19:51:10.000Z",
                "Answer_score":1,
                "Answer_body":"A user with admin privileges would have access to \"iam:CreateServiceLinkedRole\" and \"sagemaker:CreateDomain\" actions, unless SCPs or permissions boundaries are involved. However, for the purpose of onboarding Amazon SageMaker Studio with limited permissions, I would grant the user least privilege by reviewing Control Access to the Amazon SageMaker API by Using Identity-based Policies and Actions, Resources, and Condition Keys for Amazon SageMaker documentation:\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"sagemaker:CreateDomain\",\n    \"Resource\": \"arn:aws:sagemaker:<REGION>:<ACCOUNT-ID>:domain\/*\"\n}\n\n\nNOTE: An AWS account is limited to one Domain, per region, see CreateDomain.\n\n{\n    \"Effect\": \"Allow\",\n    \"Action\": \"iam:CreateServiceLinkedRole\",\n    \"Resource\": \"*\",\n    \"Condition\": {\n        \"StringEquals\": {\n            \"iam:AWSServiceName\": \"sagemaker.amazonaws.com\"\n        }\n    }\n}\n\n\nCheers!",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1586807470000,
        "Question_original_content":"studio creat domain error custom try setup studio follow publish instruct set iam http doc aw amazon com latest onboard iam html get error user arn aw iam user user author perform createdomain resourc arn aw east domain admin priviledg account amazonfullaccess notic amazonfullaccess polici actual limit perform action resourc arn arn aw domain confirm domain region cli allow isnt block list user profil return user profil seen error know workaround creat custom polici enabl creat domain implic specif permiss onboard iam",
        "Question_preprocessed_content":"studio creat domain error custom try setup studio follow publish instruct set iam get error user author perform createdomain resourc arn aw admin priviledg account amazonfullaccess notic amazonfullaccess polici actual limit perform action resourc arn confirm domain region cli allow isnt block return user profil seen error know workaround creat custom polici enabl creat domain implic specif permiss onboard iam",
        "Question_gpt_summary_original":"The user is encountering an error while setting up Sagemaker Studio using IAM, despite having admin privileges and AmazonSageMakerFullAccess. The AmazonSageMakerFullAccess policy has a limitation that prevents actions on a resource with arn \u201carn:aws:sagemaker:::domain\/*\u201d. There are no other domains in the region, and the CLI returns no user profiles. The user is seeking a workaround, including creating a custom policy or identifying specific permissions needed for onboarding using IAM.",
        "Question_gpt_summary":"user encount error set studio iam despit have admin privileg amazonfullaccess amazonfullaccess polici limit prevent action resourc arn arn aw domain domain region cli return user profil user seek workaround includ creat custom polici identifi specif permiss need onboard iam",
        "Answer_original_content":"user admin privileg access iam createservicelinl createdomain action scp permiss boundari involv purpos onboard studio limit permiss grant user privileg review control access api ident base polici action resourc condit kei document effect allow action createdomain resourc arn aw domain note aw account limit domain region createdomain effect allow action iam createservicelinl resourc condit stringequ iam awsservicenam amazonaw com cheer",
        "Answer_preprocessed_content":"user admin privileg access iam createservicelinl createdomain action scp permiss boundari involv purpos onboard studio limit permiss grant user privileg review control access api polici action resourc condit kei document effect allow action createdomain resourc note aw account limit domain region createdomain effect allow action iam createservicelinl resourc condit cheer",
        "Answer_gpt_summary_original":"possible solutions to the error encountered while setting up studio using iam include granting the user least privilege by reviewing control access to the api using identity-based policies and actions, resources, and condition keys. a user with admin privileges would have access to \"iam:createservicelinle\" and \":createdomain\" actions, unless scps or permissions boundaries are involved. additionally, the user can use the following documentation to create an allow effect for the \"iam:createservicelinle\" and \":createdomain\" actions: { \"effect\": \"allow\", \"action\": \":createdomain\", \"resource\": \"arn:aws::::domain\/*\" } and { \"effect\": \"allow\", \"action\": \"iam:createservicelinle\", \"resource\": \"*\", \"condition\": { \"stringequals\": { \"iam:awsservicename\": \".amazonaws.com\" } } }.",
        "Answer_gpt_summary":"possibl solut error encount set studio iam includ grant user privileg review control access api ident base polici action resourc condit kei user admin privileg access iam createservicelinl createdomain action scp permiss boundari involv addition user us follow document creat allow effect iam createservicelinl createdomain action effect allow action createdomain resourc arn aw domain effect allow action iam createservicelinl resourc condit stringequ iam awsservicenam amazonaw com"
    },
    {
        "Question_id":null,
        "Question_title":"All records are lost in a project without any action",
        "Question_body":"<p>Dear Sir or Madam,<\/p>\n<p>Sorry for bothering you, I think there is an error in one of my wandb projects and the records of all runs were lost. The account is nbower0707, email 1155156871@link.cuhk.edu.hk, and the project name is ocp22.<\/p>\n<p>Everything worked fine before today, and I did a lot of experiments on this project. I\u2019m uploading records of my metric around every 5000 steps, and the result validation metric plot should be something like  figure 1 shows(continuous lines of records, with multiple data points) I\u2019m uploading the corresponding metrics every 2500 steps, and wandb displayed all results fine yesterday (either undergoing or finished runs)<\/p>\n<p>However, when I check the plot today, the record of metric in all runs were (completely or partly) lost, except for some small isolated data points left (as figure 2 and 3 shows).<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d.jpeg\" data-download-href=\"\/uploads\/short-url\/n0TMrYL9SyvpaH1YKsBmceDhhRb.jpeg?dl=1\" title=\"Picture 1\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg\" alt=\"Picture 1\" data-base62-sha1=\"n0TMrYL9SyvpaH1YKsBmceDhhRb\" width=\"414\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_414x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_621x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_828x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/a14c097f39437a5f49c44e628d5d3e3d92490c4d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Picture 1<\/span><span class=\"informations\">2337\u00d72818 348 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I tried to use <strong>wandb sync<\/strong> from the local file, and upload the runs to a new project, the result is still the same.<\/p>\n<p>I didn\u2019t do any specific operations regarding wandb logging process or on the website. The project consist of runs uploaded from different machines, therefore it wouldn\u2019t be mistakenly deletion\/ false operation offline. And the phenomenon of lost of data also occurs on old runs that finished weeks ago.<\/p>\n<p>Please let me know if you have any suggestions on this error, and if the records could be recovered.<\/p>\n<p>Your time and patience are sincerely appreciated.<\/p>\n<p>Bowen Wang<\/p>",
        "Question_answer_count":13,
        "Question_comment_count":0,
        "Question_creation_time":1661403318517,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":162.0,
        "Answer_body":"<p>Hey all,<\/p>\n<p>Our engineering team looked into this and rolled back some changes, everything should be working fine now.<\/p>\n<p>Please let us know if this issue persists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/all-records-are-lost-in-a-project-without-any-action\/2993",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-25T05:10:38.981Z",
                "Answer_body":"<p>I have the same problem\u2026<\/p>",
                "Answer_score":9.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T06:26:28.134Z",
                "Answer_body":"<p>Me too. From night to morning the runs graphs miss a lot of data points in the validation section and I also noted that the resize of the panels in that section doesn\u2019t work properly<\/p>",
                "Answer_score":4.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T06:27:35.035Z",
                "Answer_body":"<p>Same problem here, the plots are weird and loses a lot of data points<\/p>",
                "Answer_score":14.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T06:44:26.346Z",
                "Answer_body":"<p>Me too\u2026 It seem to be  same problem<\/p>",
                "Answer_score":4.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T07:43:03.396Z",
                "Answer_body":"<p>Hi Everyone,<\/p>\n<p>Apologies for the inconvenience here. We are looking into the issue - any links to workspaces where you see this currently would be greatly appreciated.<\/p>\n<p>Thanks,<br>\nRamit<br>\nW&amp;B Support<\/p>",
                "Answer_score":44.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T08:19:07.481Z",
                "Answer_body":"<p>Mine is <a href=\"https:\/\/wandb.ai\/niansong1996\/cot-codegen?workspace=user-niansong1996\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>",
                "Answer_score":4.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T08:20:00.625Z",
                "Answer_body":"<p>Same problem here, plots look similar to the ones shown<\/p>",
                "Answer_score":4.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T08:45:43.885Z",
                "Answer_body":"<p>Here\u2019s <a href=\"https:\/\/wandb.ai\/johnminelli\/TwoWaySinth\">mine<\/a><\/p>",
                "Answer_score":4.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T09:41:56.499Z",
                "Answer_body":"<p>The same issue\u2026<br>\nYesterday  they were fine\u2026<\/p>",
                "Answer_score":9.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T09:46:40.765Z",
                "Answer_body":"<p>Same issue  <a href=\"https:\/\/wandb.ai\/ecotoxformer\/fish-EC50-MOR?workspace=user-styrbjornkall\">here<\/a>. Though the charts look fine when opened in their respective run, just not in the combined workspace\u2026<\/p>",
                "Answer_score":4.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T16:49:12.578Z",
                "Answer_body":"<p>Now it\u2019s fine for me, thank you for the support<\/p>",
                "Answer_score":8.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T21:07:26.293Z",
                "Answer_body":"<p>Hey all,<\/p>\n<p>Our engineering team looked into this and rolled back some changes, everything should be working fine now.<\/p>\n<p>Please let us know if this issue persists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":47.6,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-10-24T21:08:14.661Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1661461646292,
        "Question_original_content":"record lost project action dear sir madam sorri bother think error project record run lost account nbower email link cuhk edu project ocp work fine todai lot experi project upload record metric step result valid metric plot like figur show continu line record multipl data point upload correspond metric step displai result fine yesterdai undergo finish run check plot todai record metric run complet partli lost small isol data point left figur show pictur tri us sync local file upload run new project result didnt specif oper log process websit project consist run upload differ machin wouldnt mistakenli delet fals oper offlin phenomenon lost data occur old run finish week ago let know suggest error record recov time patienc sincer appreci bowen wang",
        "Question_preprocessed_content":"record lost project action dear sir madam sorri bother think error project record run lost account nbower email project ocp work fine todai lot experi project upload record metric step result valid metric plot like figur show upload correspond metric step displai result fine yesterdai check plot todai record metric run lost small isol data point left pictur tri us sync local file upload run new project result didnt specif oper log process websit project consist run upload differ machin wouldnt mistakenli delet fals oper offlin phenomenon lost data occur old run finish week ago let know suggest error record recov time patienc sincer appreci bowen wang",
        "Question_gpt_summary_original":"The user has encountered a challenge where all records of their runs in a wandb project named \"ocp22\" have been lost without any action on their part. The user had been uploading records of their metric every 5000 steps and the validation metric plot was displaying fine until today. The records of the metric in all runs were lost, except for some small isolated data points. The user tried to use \"wandb sync\" from the local file and upload the runs to a new project, but the result was still the same. The user did not perform any specific operations regarding the wandb logging process or on the website, and the phenomenon of lost data also occurred on old runs that finished weeks ago. The user is seeking suggestions on how to recover the lost records.",
        "Question_gpt_summary":"user encount challeng record run project name ocp lost action user upload record metric step valid metric plot displai fine todai record metric run lost small isol data point user tri us sync local file upload run new project result user perform specif oper log process websit phenomenon lost data occur old run finish week ago user seek suggest recov lost record",
        "Answer_original_content":"hei engin team look roll chang work fine let know issu persist thank ramit",
        "Answer_preprocessed_content":"hei engin team look roll chang work fine let know issu persist thank ramit",
        "Answer_gpt_summary_original":"possible solutions: \n- the engineering team rolled back some changes that may have caused the loss of records. \n- the issue should be resolved now. \n- the user is encouraged to report if the issue persists.",
        "Answer_gpt_summary":"possibl solut engin team roll chang caus loss record issu resolv user encourag report issu persist"
    },
    {
        "Question_id":null,
        "Question_title":"Unable to load .ipynb file in Azure Machine Learning Workspace",
        "Question_body":"Hey all,\n\nI experienced the issue below. To summarize, I cannot open .ipynb file in my azure machine learning workspace.\n\nI have tried and ensure that the notebooks are under ~\/cloudfiles\/code\/Users\/ folder so it is visible to Jupyter environment.\n\nCan anyone give suggestions\/guidance on how to resolve the issue?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1642489248987,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/699144\/unable-to-load-ipynb-file-in-azure-machine-learnin.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-19T01:50:07.83Z",
                "Answer_score":0,
                "Answer_body":"Hi, if you're still experiencing this issue, have you tried a different browser (private browsing) to see if it helps? Are you able to connect to a kernel? When you create a new file, are you still getting same error?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"unabl load ipynb file workspac hei experienc issu summar open ipynb file workspac tri ensur notebook cloudfil code user folder visibl jupyt environ suggest guidanc resolv issu",
        "Question_preprocessed_content":"unabl load ipynb file workspac hei experienc issu summar open ipynb file workspac tri ensur notebook folder visibl jupyt environ resolv issu",
        "Question_gpt_summary_original":"The user is unable to open .ipynb files in their Azure Machine Learning Workspace despite ensuring that the files are in the correct folder. They are seeking suggestions or guidance on how to resolve the issue.",
        "Question_gpt_summary":"user unabl open ipynb file workspac despit ensur file correct folder seek suggest guidanc resolv issu",
        "Answer_original_content":"experienc issu tri differ browser privat brows help abl connect kernel creat new file get error",
        "Answer_preprocessed_content":"experienc issu tri differ browser help abl connect kernel creat new file get error",
        "Answer_gpt_summary_original":"possible solutions to the issue of being unable to load a .ipynb file in the workspace include trying a different browser (private browsing), checking if the user is able to connect to a kernel, and creating a new file to see if the error persists.",
        "Answer_gpt_summary":"possibl solut issu unabl load ipynb file workspac includ try differ browser privat brows check user abl connect kernel creat new file error persist"
    },
    {
        "Question_id":null,
        "Question_title":"Problem with DVC and Azure + tenant_id, client_id and client_secret",
        "Question_body":"<p>I am on the DVC version: 2.31.0 (pip)<br>\nMy command are :<br>\npip install \u2018dvc[azure]\u2019<br>\ndvc init --no-scm<br>\ndvc remote add -d myremote azure:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/<br>\ndvc remote modify --local myremote tenant_id \u201c12345678-1234-1234-1234-123456789012\u201d<br>\ndvc remote modify --local myremote client_id \u201cabcdefgh-1234-1234-1234-123456789012\u201d<br>\ndvc remote modify --local myremote client_secret \u201cAAAAA~AAAAAAAAAAAAAAAAAAAAA_AAAAAAAAAAAA\u201d<\/p>\n<p>dvc add an_image.jpg<br>\ndvc push<\/p>\n<p>All the tenant_id, client_id and client_secret are the same than used with azcopy :<br>\nI use azcopy with this variables without probleme :<br>\nexport AZCOPY_SPA_APPLICATION_ID=\u201cabcdefgh-1234-1234-1234-123456789012\u201d<br>\nexport AZCOPY_SPA_CLIENT_SECRET=\u201cAAAAA~AAAAAAAAAAAAAAAAAAAAA_AAAAAAAAAAAA\u201d<br>\nexport AZCOPY_TENANT_ID=\u201c12345678-1234-1234-1234-123456789012\u201d<br>\nazcopy login --service-principal --application-id $AZCOPY_SPA_APPLICATION_ID --tenant-id $AZCOPY_TENANT_ID<br>\nazcopy copy \u2018.\/01_RAW_From_Data_Lake\/*\u2019 \u2018<a href=\"https:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/\" class=\"inline-onebox-loading\" rel=\"noopener nofollow ugc\">https:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/<\/a>\u2019 --recursive<\/p>\n<p>Why the dvc push return this error message :<br>\nERROR: configuration error - Authentication to Azure Blob Storage requires either account_name or connection_string.<br>\nERROR: Authentication to Azure Blob Storage requires either account_name or connection_string.<br>\nLearn more about configuration settings at <a href=\"https:\/\/man.dvc.org\/remote\/modify\" rel=\"noopener nofollow ugc\">https:\/\/man.dvc.org\/remote\/modify<\/a>.<br>\n\/<br>\nThe tenant_id, client_id and client_secret are enough for login with azcopy. where is the mistake with DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1666890271387,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":94.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/problem-with-dvc-and-azure-tenant-id-client-id-and-client-secret\/1376",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-28T07:54:22.393Z",
                "Answer_body":"<p>Finally I founded the solution (the account_name is the name at the beginning of the url azure:\/\/xxxxxxx.core.windows.net):<br>\ndvc init --no-scm<br>\ndvc remote add -d myremote azure:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/<br>\n<strong>dvc remote modify --local myremote account_name \u201cxxxxxxx\u201d<\/strong><br>\ndvc remote modify --local myremote tenant_id \u201c12345678-1234-1234-1234-123456789012\u201d<br>\ndvc remote modify --local myremote client_id \u201cabcdefgh-1234-1234-1234-123456789012\u201d<br>\ndvc remote modify --local myremote client_secret \u201cAAAAA~AAAAAAAAAAAAAAAAAAAAA_AAAAAAAAAAAA\u201d<\/p>\n<p>dvc add an_image.jpg<br>\ndvc push<\/p>\n<p>is equivalent to :<br>\nexport AZCOPY_SPA_APPLICATION_ID=\u201cabcdefgh-1234-1234-1234-123456789012\u201d<br>\nexport AZCOPY_SPA_CLIENT_SECRET=\u201cAAAAA~AAAAAAAAAAAAAAAAAAAAA_AAAAAAAAAAAA\u201d<br>\nexport AZCOPY_TENANT_ID=\u201c12345678-1234-1234-1234-123456789012\u201d<br>\nazcopy login --service-principal --application-id $AZCOPY_SPA_APPLICATION_ID --tenant-id $AZCOPY_TENANT_ID<br>\nazcopy copy \u2018.\/01_RAW_From_Data_Lake\/*\u2019 \u2018<a href=\"https:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/%E2%80%99\" class=\"inline-onebox-loading\" rel=\"noopener nofollow ugc\">https:\/\/xxxxxxx.core.windows.net\/yyyyy\/zzzz-dvc-dataset\/\u2019<\/a> --recursive<\/p>",
                "Answer_score":61.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"problem azur tenant client client secret version pip command pip instal azur init scm remot add myremot azur core window net dataset remot modifi local myremot tenant remot modifi local myremot client abcdefgh remot modifi local myremot client secret add imag jpg push tenant client client secret azcopi us azcopi variabl problem export azcopi spa applic abcdefgh export azcopi spa client secret export azcopi tenant azcopi login servic princip applic azcopi spa applic tenant azcopi tenant azcopi copi raw data lake http core window net dataset recurs push return error messag error configur error authent azur blob storag requir account connect string error authent azur blob storag requir account connect string learn configur set http man org remot modifi tenant client client secret login azcopi mistak",
        "Question_preprocessed_content":"problem azur version command pip instal init remot add myremot remot modifi myremot remot modifi myremot remot modifi myremot add push azcopi us azcopi variabl problem export export export azcopi login azcopi copi push return error messag error configur error authent azur blob storag requir error authent azur blob storag requir learn configur set login azcopi mistak",
        "Question_gpt_summary_original":"The user is encountering an error while trying to push an image to Azure Blob Storage using DVC. The error message states that authentication to Azure Blob Storage requires either account_name or connection_string. The user has already provided tenant_id, client_id, and client_secret, which were used successfully with azcopy. The user is unsure where the mistake is with DVC.",
        "Question_gpt_summary":"user encount error try push imag azur blob storag error messag state authent azur blob storag requir account connect string user provid tenant client client secret successfulli azcopi user unsur mistak",
        "Answer_original_content":"final found solut account begin url azur core window net init scm remot add myremot azur core window net dataset remot modifi local myremot account remot modifi local myremot tenant remot modifi local myremot client abcdefgh remot modifi local myremot client secret add imag jpg push equival export azcopi spa applic abcdefgh export azcopi spa client secret export azcopi tenant azcopi login servic princip applic azcopi spa applic tenant azcopi tenant azcopi copi raw data lake http core window net dataset recurs",
        "Answer_preprocessed_content":"final found solut init remot add myremot remot modifi myremot remot modifi myremot remot modifi myremot remot modifi myremot add push equival export export export azcopi login azcopi copi",
        "Answer_gpt_summary_original":"the solution to the authentication issue with azure blob storage involves using the account name at the beginning of the url, and modifying the remote with the tenant_id, client_id, and client_secret. additionally, exporting the azcopy_spa_application_id, azcopy_spa_client_secret, and azcopy_tenant_id variables and using them with the azcopy login command can help with authentication. finally, using the azcopy copy command with the correct source and destination urls can help with copying files to azure blob storage.",
        "Answer_gpt_summary":"solut authent issu azur blob storag involv account begin url modifi remot tenant client client secret addition export azcopi spa applic azcopi spa client secret azcopi tenant variabl azcopi login command help authent final azcopi copi command correct sourc destin url help copi file azur blob storag"
    },
    {
        "Question_id":60637170.0,
        "Question_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Question_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1583933260433,
        "Question_favorite_count":null,
        "Question_last_edit_time":1584005920356,
        "Question_score":4.0,
        "Question_view_count":1681.0,
        "Answer_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1584011988083,
        "Answer_score":-2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1584005785480,
        "Question_original_content":"pass argument score file deploi model deploi train model aci endpoint python sdk creat score file like file call argument pass like train file interpret argpars pass argument code creat inferenceconfig environ obvious work fall extra docker file step core conda depend import condadepend core environ import environ core model import inferenceconfig env environ host environ env python conda depend condadepend creat conda packag scikit learn pip packag default score script score model model infer config inferenceconfig entri script score script environ env ad score refer love us argument script remov import import argpars def init global model parser argpars argumentpars descript load sklearn model parser add argument model dest model requir true arg parser pars known arg model path model model path model arg model model joblib load model path def run raw data try data json load raw data data data arrai data result model predict data return result tolist except result str return result interest hear thought",
        "Question_preprocessed_content":"pass argument score file deploi model deploi train model aci endpoint python sdk creat file like file call argument pass interpret pass argument code creat inferenceconfig environ obvious work fall extra docker file step ad refer love us argument script interest hear thought",
        "Question_gpt_summary_original":"The user is facing a challenge in passing arguments to the score.py file when deploying a trained model to an ACI endpoint on Azure Machine Learning using the Python SDK. They have created the score.py file and want it to be called with an argument that can be interpreted using argparse, but they are unable to find a way to pass arguments. The user has shared their code for creating the InferenceConfig environment and the score.py file for reference.",
        "Question_gpt_summary":"user face challeng pass argument score file deploi train model aci endpoint python sdk creat score file want call argument interpret argpars unabl wai pass argument user share code creat inferenceconfig environ score file refer",
        "Answer_original_content":"deploi environ model regist deploi ipynb inferenceconfig class accept sourc directori entri script paramet sourc directori path folder contain file score addit file creat imag multi model regist deploi ipynb code snippet creat inferenceconfig sourc directori entri script core webservic import webservic core model import inferenceconfig core environ import environ myenv environ conda specif myenv file path myenv yml infer config inferenceconfig entri script score environ myenv servic model deploi workspac sklearn mnist svc model model infer config infer config deploy config aciconfig servic wait deploy output true print servic score uri",
        "Answer_preprocessed_content":"deploi environ inferenceconfig class accept paramet path folder contain addit file creat imag code snippet creat inferenceconfig",
        "Answer_gpt_summary_original":"the answer suggests that the user can deploy using environments and provides a link to a notebook with instructions. it also mentions the use of the inferenceconfig class, which accepts source_directory and entry_script parameters. the answer provides a link to another notebook with code snippets on how to create inferenceconfig with source_directory and entry_script. additionally, the answer includes code snippets for creating an environment and deploying the model.",
        "Answer_gpt_summary":"answer suggest user deploi environ provid link notebook instruct mention us inferenceconfig class accept sourc directori entri script paramet answer provid link notebook code snippet creat inferenceconfig sourc directori entri script addition answer includ code snippet creat environ deploi model"
    },
    {
        "Question_id":null,
        "Question_title":"Difference in processing time between Azure Machine Learning Studio and Azure Machine Learning Studio (classic)",
        "Question_body":"I used to use Azure Machine Learning Studio (classic).\nCreating the same workout in Azure Machine Learning Studio takes about 20 times longer than classic.\nVirtual machine size is Standard_DS3_v2 (4 core\u300114 GB RAM\u300128 GB disk).\nSteps that have been executed once will be processed quickly from the next time onward, but steps that have been changed even slightly will take 20 times longer than classic.\n\nHow can I process at the same speed as classic?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635432791567,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for your feedback. AML classic studio appears to be faster in some cases because it uses a Fixed Compute (and always available). However, AML Classic lacks flexibility and scalability that the new platform offers. With designer, you have greater flexibility but depending on the task (e.g. smaller tasks), the processing time may seem longer than classic due to overhead for preparing each step. For smaller tasks, majority of execution time is spent on overhead. Furthermore, when input data changes, it may take longer. If no changes are made, the pipeline would automatically use the cached result of that module, so it should be faster compared to the first run. The product team are aware of this limitation and working to improve the experience. For compute heavy tasks, we recommend you pick a larger VM to improve processing speeds. Please review this document for ways to Optimize Data Processing. Feel free to submit feedback directly to the product team by using the 'smiley' feedback icon in Azure ML Studio. Other Similar Posts: (1), (2).\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/607943\/difference-in-processing-time-between-azure-machin.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-28T22:04:25.073Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for your feedback. AML classic studio appears to be faster in some cases because it uses a Fixed Compute (and always available). However, AML Classic lacks flexibility and scalability that the new platform offers. With designer, you have greater flexibility but depending on the task (e.g. smaller tasks), the processing time may seem longer than classic due to overhead for preparing each step. For smaller tasks, majority of execution time is spent on overhead. Furthermore, when input data changes, it may take longer. If no changes are made, the pipeline would automatically use the cached result of that module, so it should be faster compared to the first run. The product team are aware of this limitation and working to improve the experience. For compute heavy tasks, we recommend you pick a larger VM to improve processing speeds. Please review this document for ways to Optimize Data Processing. Feel free to submit feedback directly to the product team by using the 'smiley' feedback icon in Azure ML Studio. Other Similar Posts: (1), (2).\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1635458665072,
        "Question_original_content":"differ process time studio studio classic us studio classic creat workout studio take time longer classic virtual machin size standard core ram disk step execut process quickli time onward step chang slightli time longer classic process speed classic",
        "Question_preprocessed_content":"differ process time studio studio us studio creat workout studio take time longer classic virtual machin size step execut process quickli time onward step chang slightli time longer classic process speed classic",
        "Question_gpt_summary_original":"The user is facing challenges with the processing time in Azure Machine Learning Studio compared to Azure Machine Learning Studio (classic). The user has observed that creating the same workflow in Azure Machine Learning Studio takes about 20 times longer than in classic. The virtual machine size used is Standard_DS3_v2 with 4 cores, 14 GB RAM, and 28 GB disk. The user has noticed that steps that have been executed once are processed quickly from the next time onward, but steps that have been changed even slightly take 20 times longer than classic. The user is seeking a solution to process at the same speed as classic.",
        "Question_gpt_summary":"user face challeng process time studio compar studio classic user observ creat workflow studio take time longer classic virtual machin size standard core ram disk user notic step execut process quickli time onward step chang slightli time longer classic user seek solut process speed classic",
        "Answer_original_content":"thank feedback aml classic studio appear faster case us fix comput avail aml classic lack flexibl scalabl new platform offer design greater flexibl depend task smaller task process time longer classic overhead prepar step smaller task major execut time spent overhead furthermor input data chang longer chang pipelin automat us cach result modul faster compar run product team awar limit work improv experi comput heavi task recommend pick larger improv process speed review document wai optim data process feel free submit feedback directli product team smilei feedback icon studio similar post kindli accept answer inform help thank",
        "Answer_preprocessed_content":"thank feedback aml classic studio appear faster case us fix comput aml classic lack flexibl scalabl new platform offer design greater flexibl depend task process time longer classic overhead prepar step smaller task major execut time spent overhead furthermor input data chang longer chang pipelin automat us cach result modul faster compar run product team awar limit work improv experi comput heavi task recommend pick larger improv process speed review document wai optim data process feel free submit feedback directli product team smilei feedback icon studio similar post kindli accept answer inform help thank",
        "Answer_gpt_summary_original":"possible solutions to the user's issue of slower processing time in studio compared to studio (classic) include using a larger virtual machine for compute-heavy tasks, optimizing data processing, and submitting feedback to the product team. the answer also explains that while classic may appear faster in some cases, it lacks the flexibility and scalability of the new platform. the product team is aware of the limitations and is working to improve the experience.",
        "Answer_gpt_summary":"possibl solut user issu slower process time studio compar studio classic includ larger virtual machin comput heavi task optim data process submit feedback product team answer explain classic appear faster case lack flexibl scalabl new platform product team awar limit work improv experi"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning - Deploy model with DockerFile",
        "Question_body":"Hi all,\n\nI'm trying to deploy a model with the azure-cli-ml with the following command:\n\naz ml model deploy --name local-test-endpoint --model name:version --compute-type local --ic inference_config.json --dc deployment_config_local.json -g rg --workspace-name amw\n\nwith the following inference configuration:\n\ninference_config.json\n\n\n\n {\n     \"entryScript\": \"score.py\",\n     \"environment\": {\n         \"docker\": {\n             \"arguments\": [],\n             \"baseDockerfile\": \"dockerFile\",\n             \"baseImage\": null,\n             \"enabled\": true,\n             \"sharedVolumes\": true,\n             \"shmSize\": \"2g\"\n         },\n         \"name\": \"test-environment\",\n         \"python\": {\n             \"baseCondaEnvironment\": null,\n             \"condaDependencies\": {\n                 \"channels\": [\n                     \"conda-forge\"\n                 ],\n                 \"dependencies\": [\n                     \"python=3.7\"\n                 ],\n                 \"name\": \"project_environment\"\n             },\n             \"condaDependenciesFile\": null,\n             \"interpreterPath\": \"python\",\n             \"userManagedDependencies\": false\n         },\n         \"version\": \"1\"\n     }\n }\n\n\n\n\nand this is the dockerFile:\n\n\n\n FROM pytorch\/pytorch:1.11.0-cuda11.3-cudnn8-runtime\n    \n RUN pip install \\\n     'azureml-mlflow==1.37.0' \\\n     'mlflow-skinny' \\\n     'pytorch-accelerated>=0.1.22' \\\n     'torchmetrics>=0.7.2' \\\n     'func_to_script' \\\n     'albumentations==1.1.0' \\\n     'pandas==1.3.4' \\\n     'matplotlib' \\\n     'sklearn'\n\n\n\nRunning the deploy command I get the following error message:\n\n\n\n\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\nfatal: not a git repository (or any parent up to mount point \/)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\nunexpected dockerfile format\nfailed to run step ID: acb_step_0: failed to scan dependencies: exit status 1\n\nI get the same error even deploying to ACI and AKS.\nWhat am I doing wrong in the configuration?\n\nThanks!",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1654794549783,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/883631\/azure-machine-learning-deploy-model-with-dockerfil.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-14T09:43:51.18Z",
                "Answer_score":1,
                "Answer_body":"@GCocci Thanks for the details. Can you please add more details about the version that you are using? environment creation in v1 requires you to provide the dockerfile INLINE, not as a reference. We highly recommend using v2.\n\n\"baseDockerfile\": \"dockerFile\",\n\nHere is the document to troubleshoot environment image builds.\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deploi model dockerfil try deploi model azur cli follow command model deploi local test endpoint model version comput type local infer config json deploy config local json workspac amw follow infer configur infer config json entryscript score environ docker argument basedockerfil dockerfil baseimag null enabl true sharedvolum true shmsize test environ python basecondaenviron null condadepend channel conda forg depend python project environ condadependenciesfil null interpreterpath python usermanageddepend fals version dockerfil pytorch pytorch cuda cudnn runtim run pip instal skinni pytorch acceler torchmetr func script albument panda matplotlib sklearn run deploi command follow error messag stop filesystem boundari git discoveri filesystem set fatal git repositori parent mount point stop filesystem boundari git discoveri filesystem set unexpect dockerfil format fail run step acb step fail scan depend exit statu error deploi aci ak wrong configur thank",
        "Question_preprocessed_content":"deploi model dockerfil try deploi model follow command model deploi version local amw follow infer configur python condadependenciesfil null interpreterpath python usermanageddepend fals version dockerfil run pip instal matplotlib sklearn run deploi command follow error messag stop filesystem boundari fatal git repositori stop filesystem boundari unexpect dockerfil format fail run step fail scan depend exit statu error deploi aci ak wrong configur thank",
        "Question_gpt_summary_original":"The user is encountering an error while trying to deploy a model with Azure Machine Learning using a DockerFile. The error message states that the DockerFile format is unexpected and the scan dependencies failed, resulting in the deployment failure. The user is seeking assistance in identifying the issue with the configuration.",
        "Question_gpt_summary":"user encount error try deploi model dockerfil error messag state dockerfil format unexpect scan depend fail result deploy failur user seek assist identifi issu configur",
        "Answer_original_content":"gcocci thank detail add detail version environ creation requir provid dockerfil inlin refer highli recommend basedockerfil dockerfil document troubleshoot environ imag build answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"thank detail add detail version environ creation requir provid dockerfil inlin refer highli recommend basedockerfil dockerfil document troubleshoot environ imag build answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"the possible solutions to the challenge of encountering an error message related to the dockerfile format while deploying a model with azure-cli-ml are to provide the dockerfile inline instead of as a reference and to use version 2 instead of version 1. the user is also directed to a troubleshooting document for environment image builds.",
        "Answer_gpt_summary":"possibl solut challeng encount error messag relat dockerfil format deploi model azur cli provid dockerfil inlin instead refer us version instead version user direct troubleshoot document environ imag build"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker instances keep awakening and charge the credit",
        "Question_body":"I have tried Data Wrangler in Sagemaker last month and close the service. A few weeks later I have noticed the credit was charge $1 every hour and just realized that the Data Wranger auto-save the flow every minute. So, I deleted the unsaved flow and shut down all the services and instances according to advice on these two links :\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-shutdown.html\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-cleanup.html\n\nThen, I left the Sagemaker untouched for the whole month of May, and just got back to the console yesterday. This is what I found out for May's bill:\n\nAmazon SageMaker RunInstance $531.74\nDetail\tUsage\tTotal\n$0.00 for Host:ml.m5.xlarge per hour under monthly free tier\t125.000 Hrs\t$0.00\n$0.00 for Notebk:ml.t2.medium per hour under monthly free tier\t107.056 Hrs\t$0.00\n$0.00 per Data Wrangler Interactive ml.m5.4xlarge hour under monthly free tier\t25.000 Hrs\t$0.00\n$0.23 per Hosting ml.m5.xlarge hour in US East (N. Virginia)\t88.997 Hrs\t$20.47\n$0.922 per Data Wrangler Interactive ml.m5.4xlarge hour in US East (N. Virginia)\t554.521 Hrs\t$511.27\n\nSo, with another attempt, I installed an extension to automatically shut down idle kernels and set the limit to 10 min from advice here: https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-costs-by-automatically-shutting-down-idle-resources-within-amazon-sagemaker-studio\/ Checked the cost in usage report, it turns out that the service was shut down after installing the extension but then it revoked itself after 5 hours later (during my sleep time). There's still cost from Studio although with less charge than previous one.\n\nService\tOperation\tUsageType\tStartTime\tEndTime\tUsageValue\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/24\/2022 23:00\t5\/25\/2022 0:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 0:00\t5\/25\/2022 1:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 1:00\t5\/25\/2022 2:00\t1\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 2:00\t5\/25\/2022 3:00\t0.76484417\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 8:00\t5\/25\/2022 9:00\t0.36636722\nAmazonSageMaker\tRunInstance\tUSE1-Studio_DW:KernelGateway-ml.m5.4xlarge\t5\/25\/2022 9:00\t5\/25\/2022 10:00\t0.38959556\n\nDuring this time, I'm sure that there're no running instances, running apps, kernel sessions or terminal sessions. I even deleted the user profile. Last thing I haven't tried is to set up scheduled shutdown coz I think the services should not cause difficulty to our life that much. Any advice for any effective action to completely shutdown the Sagemaker instance? Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653535822137,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":306.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjCMOSHaPR4WwWP1SoFzzng\/sagemaker-instances-keep-awakening-and-charge-the-credit",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-26T16:33:36.084Z",
                "Answer_score":0,
                "Answer_body":"Hi, you can shut down SageMaker Studio resources per the documentation here - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-run-and-manage-shut-down.html (you've linked to Studio Lab documentation, so making sure). In addition, I see you've been charged for hosting and you've since deleted the endpoints, and don't see the hosting charges after deleting them.\n\nFor data wrangler, once you have the flow saved, you need to shut down the app (closing the window does not automatically shut down the app). Note that if you open the DW flow later, it does start a compute instance, which you'll then have to shut down.\n\nIf you've deleted the user profile (and associated apps), you shouldn't be seeing any more Studio charges for that user profile. If you still see the DW charges (and there's no other user profile), please cut a ticket to support for further investigation.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"instanc awaken charg credit tri data wrangler month close servic week later notic credit charg hour realiz data wranger auto save flow minut delet unsav flow shut servic instanc accord advic link http doc aw amazon com latest studio lab us shutdown html http doc aw amazon com latest cleanup html left untouch month got consol yesterdai runinst usag total host xlarg hour monthli free tier hr notebk medium hour monthli free tier hr data wrangler interact xlarg hour monthli free tier hr host xlarg hour east virginia hr data wrangler interact xlarg hour east virginia hr attempt instal extens automat shut idl kernel set limit min advic http aw amazon com blog machin learn save cost automat shut idl resourc amazon studio check cost usag report turn servic shut instal extens revok hour later sleep time cost studio charg previou servic oper usagetyp starttim endtim usagevalu amazon runinst us studio kernelgatewai xlarg amazon runinst us studio kernelgatewai xlarg amazon runinst us studio kernelgatewai xlarg amazon runinst us studio kernelgatewai xlarg amazon runinst us studio kernelgatewai xlarg amazon runinst us studio kernelgatewai xlarg time sure run instanc run app kernel session termin session delet user profil thing haven tri set schedul shutdown coz think servic caus difficulti life advic effect action complet shutdown instanc thank",
        "Question_preprocessed_content":"instanc awaken charg credit tri data wrangler month close servic week later notic credit charg hour realiz data wranger flow minut delet unsav flow shut servic instanc accord advic link left untouch month got consol yesterdai runinst usag total hour monthli free tier hr hour monthli free tier hr data wrangler interact hour monthli free tier hr host hour east hr data wrangler interact hour east hr attempt instal extens automat shut idl kernel set limit min advic check cost usag report turn servic shut instal extens revok hour later cost studio charg previou servic oper usagetyp starttim endtim usagevalu amazon runinst amazon runinst amazon runinst amazon runinst amazon runinst amazon runinst time sure run instanc run app kernel session termin session delet user profil thing haven tri set schedul shutdown coz think servic caus difficulti life advic effect action complet shutdown instanc thank",
        "Question_gpt_summary_original":"The user encountered challenges with Sagemaker instances that kept awakening and charging the credit, even after shutting down all services and instances. The user tried installing an extension to automatically shut down idle kernels, but the service revoked itself after five hours. The user is seeking advice on an effective action to completely shut down the Sagemaker instance.",
        "Question_gpt_summary":"user encount challeng instanc kept awaken charg credit shut servic instanc user tri instal extens automat shut idl kernel servic revok hour user seek advic effect action complet shut instanc",
        "Answer_original_content":"shut studio resourc document http doc aw amazon com latest notebook run manag shut html link studio lab document make sure addit charg host delet endpoint host charg delet data wrangler flow save need shut app close window automat shut app note open flow later start comput instanc shut delet user profil associ app shouldn see studio charg user profil charg user profil cut ticket support investig",
        "Answer_preprocessed_content":"shut studio resourc document addit charg host delet endpoint host charg delet data wrangler flow save need shut app note open flow later start comput instanc shut delet user profil shouldn see studio charg user profil charg cut ticket support investig",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge with aws instances continually awakening and charging their credit include shutting down studio resources according to the documentation provided, ensuring that endpoints are deleted to avoid hosting charges, and properly shutting down the data wrangler app to avoid compute instance charges. if the user has deleted the associated user profile and still sees charges, they should contact support for further investigation.",
        "Answer_gpt_summary":"possibl solut user challeng aw instanc continu awaken charg credit includ shut studio resourc accord document provid ensur endpoint delet avoid host charg properli shut data wrangler app avoid comput instanc charg user delet associ user profil see charg contact support investig"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML 'Designer': how to view logistic regression model coefficients \/ intercept",
        "Question_body":"Using Azure ML Designer it is easy to create a model using the Two-Class Logistic Regression & Train Model components. However it does not seem to be possible to view the regression coefficients \/ intercept (ie. the weights applied to the feature values within the model). How can we go about viewing the model coefficients? Are they stored in one of the Train Model output files (eg. data.ilearner) in a way that can be viewed \/ exported to a human readable format?\n\nNote: this question relates to the Azure Machine Learning Studio (not the older 'classic' version where I believe it was possible to 'right-click' and visualise the model coefficients).",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607433503623,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/190478\/azure-ml-39designer39-how-to-view-logistic-regress.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-09T05:26:10.807Z",
                "Answer_score":0,
                "Answer_body":"Thanks for reaching out. These are the metrics reported when evaluating binary classification models. You can view the results by clicking evaluate model > visualize > evaluation results. Hope this helps.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-05T20:37:41.743Z",
                "Answer_score":0,
                "Answer_body":"Any update on this? Is it still not possible to get the coefficients for the trained linear regression model?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"design view logist regress model coeffici intercept design easi creat model class logist regress train model compon possibl view regress coeffici intercept weight appli featur valu model view model coeffici store train model output file data ilearn wai view export human readabl format note question relat studio older classic version believ possibl right click visualis model coeffici",
        "Question_preprocessed_content":"design view logist regress model coeffici intercept design easi creat model logist regress train model compon possibl view regress coeffici intercept view model coeffici store train model output file wai view export human readabl format note question relat studio",
        "Question_gpt_summary_original":"The user is facing a challenge in viewing the regression coefficients and intercept of a model created using the Two-Class Logistic Regression & Train Model components in Azure ML Designer. They are unsure if the coefficients are stored in the Train Model output files and if they can be exported to a human-readable format. The user notes that this question is specific to Azure Machine Learning Studio and not the older 'classic' version.",
        "Question_gpt_summary":"user face challeng view regress coeffici intercept model creat class logist regress train model compon design unsur coeffici store train model output file export human readabl format user note question specif studio older classic version",
        "Answer_original_content":"thank reach metric report evalu binari classif model view result click evalu model visual evalu result hope help updat possibl coeffici train linear regress model",
        "Answer_preprocessed_content":"thank reach metric report evalu binari classif model view result click evalu model visual evalu result hope help updat possibl coeffici train linear regress model",
        "Answer_gpt_summary_original":"possible solutions: \n- click on \"evaluate model\" > \"visualize\" > \"evaluation results\" to view the logistic regression model coefficients and intercepts in designer. \n\nsummary: \nto view the logistic regression model coefficients and intercepts in designer, the user can click on \"evaluate model\" > \"visualize\" > \"evaluation results\".",
        "Answer_gpt_summary":"possibl solut click evalu model visual evalu result view logist regress model coeffici intercept design summari view logist regress model coeffici intercept design user click evalu model visual evalu result"
    },
    {
        "Question_id":null,
        "Question_title":"Show single lines in groups",
        "Question_body":"<p>Hello,<br>\nI want to have a way to organise my runs so that I can know some parameters of the runs already by the name. I can do that by using group_by and then I can see the different parameter for each run<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7aefbd3b28fbf0ac496352f40ceb75e0975e1f00.png\" alt=\"image\" data-base62-sha1=\"hxxTc29jObSNMn9oZg3e64ZOA4U\" width=\"269\" height=\"195\"><\/p>\n<p>But this also means all the runs inside a group, is it possible to group but still see the different runs (optionally all runs in a group with the same color?)<br>\nThanks,<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1649968185659,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":70.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/show-single-lines-in-groups\/2245",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-15T10:48:05.606Z",
                "Answer_body":"<p>Hey Oren, this is not possible with grouping. I\u2019d suggest to pass the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/init\">name<\/a> argument when invoking wandb.init(). You can format the string with the values of the wandb.config dict.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-16T17:47:21.269Z",
                "Answer_body":"<p>Thanks,<br>\nThis is a shame, is it possible to add it as a feature request?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-27T15:31:01.255Z",
                "Answer_body":"<p>Hey Oren, sorry about the late reply. I\u2019ve shared your request with the team. Will notify you once there are updates on this.<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-26T20:26:52.905Z",
                "Answer_body":"<p>Hey Oren, another workaround I\u2019d suggest is to pin columns to achieve what you want.<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-15T17:48:07.417Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"singl line group hello want wai organis run know paramet run group differ paramet run mean run insid group possibl group differ run option run group color thank",
        "Question_preprocessed_content":"singl line group hello want wai organis run know paramet run differ paramet run mean run insid group possibl group differ run thank",
        "Question_gpt_summary_original":"The user wants to organize their runs by using group_by to see different parameters for each run. However, this means that all runs inside a group are grouped together, and the user is looking for a way to group while still being able to see the different runs, optionally with the same color.",
        "Question_gpt_summary":"user want organ run group differ paramet run mean run insid group group user look wai group abl differ run option color",
        "Answer_original_content":"hei oren possibl group suggest pass argument invok init format string valu config dict thank shame possibl add featur request hei oren sorri late repli iv share request team notifi updat hei oren workaround suggest pin column achiev want topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei oren possibl group suggest pass argument invok init format string valu config dict thank shame possibl add featur request hei oren sorri late repli iv share request team notifi updat hei oren workaround suggest pin column achiev want topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. pass the name argument when invoking .init() and format the string with the values of the .config dict to identify different parameters associated with each run.\n2. request the feature to add grouping as a feature.\n3. pin columns as a workaround to achieve the desired result.",
        "Answer_gpt_summary":"possibl solut answer pass argument invok init format string valu config dict identifi differ paramet associ run request featur add group featur pin column workaround achiev desir result"
    },
    {
        "Question_id":null,
        "Question_title":"While running an Azure ML Experiement, I get \"File Not found\" error when attempting to find ODBC driver for python pyodbc.connect command",
        "Question_body":"Hello:\n\nThis is the python command. It works fine in my local Windows environment and I can connect to the remote Azure SQL DB that I want to connect to.\n\nsql_conn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server}; ...\n\nHowever, when I run my az ml submit-run CLI command in order to run an experiment in Azure Machine Learning, I get this error.:\n\n[01000] [unixODBC][Driver Manager]Can't open lib 'ODBC Driver 17 for SQL Server' : file not found\n\nSo, it seems that AML is running its containers in Linux. Ok....I have no problem with that, but how do I configure either my <filename>.runconfig or my .yaml file to tell the Linux machine where to find the driver?\n\nI can't really connect to a virtual machine and install this driver if I use AML, right? It's needs to be done in one of the aforementioned files, right?\n\nDan",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1594589100353,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/45912\/while-running-an-azure-ml-experiement-i-get-file-n.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-13T12:10:47.957Z",
                "Answer_score":0,
                "Answer_body":"@DanWellisch-1648 Are you using a Jupyter notebook from Azure ML portal to run the commands to connect to a SQL DB? If Yes, you can install the required library on your notebook VM and try to connect again.\n\n\n\n\nIn this case you should have enabled SSH access to your compute instance to login and install the required library.\n\n\n\n\n\n=>Enable SSH access\n\n\n\n\n\n\n\n\n=>Login to the terminal of the instance using the link after the compute is created. Install the library if not available already\n\n\nsudo dpkg -l|grep msodbcsql # If this command return blank the you know that the driver indeed is missing on your server and that you need to install it\n\n #And to install this driver you can run the following:\n         \n sudo ACCEPT_EULA=Y apt-get install msodbcsql17\n\n\n\n\n\n\n\n=>The following files should be available which should help to configure and use the drivers.\n\n\n\n\nFirst a new directory located in \/opt\/microsoft\/msodbcsql17\/ this is the directory where the libraries are installed\n\n\n\n\nSecond a file in \/etc\/ called odbcinst.ini that will host the paths for the libraries and that python will need\n\n\n\n\nThird the main library that is hosted in\/opt\/microsoft\/msodbcsql17\/lib64\/libmsodbcsql-17.4.so.1.1 and this library should have all the dynamic libraries attached by default.\n\n\n\n\n\n\n\nYou can then run the experiment again to check if the connection works.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-13T15:28:23.887Z",
                "Answer_score":0,
                "Answer_body":"Hello:\n\nI am NOT running from a Jupiter Notebook. I am running on my laptop with the az ml submit-run CLI command which specifies my python code, run config, .yml file.\n\nBut, if I create an Azure notebook while logged in to the portal, and run through the steps you mentioned, that would configure my compute for me and my AZ ML Submit-Run CLI command should then find it's ODBC library, correct?\n\nDan",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-13T15:56:13.76Z",
                "Answer_score":0,
                "Answer_body":"@DanWellisch-1648 You can always from within your train script use pyodbc to access on prem SQL server as long as there\u2019s network connectivity.\nYou will need to define a custom docker image to configure the driver if you use on demand AML compute. It\u2019s easier to use VM:\n\n\n\n\nSee this: https:\/\/github.com\/mkleehammer\/pyodbc\/wiki\/Connecting-to-SQL-Server-from-Linux\n\n\n\n\n\nor",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":45.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run experi file error attempt odbc driver python pyodbc connect command hello python command work fine local window environ connect remot azur sql want connect sql conn pyodbc connect driver odbc driver sql server run submit run cli command order run experi error unixodbc driver manag open lib odbc driver sql server file aml run contain linux problem configur runconfig yaml file tell linux machin driver connect virtual machin instal driver us aml right need aforement file right dan",
        "Question_preprocessed_content":"run experi file error attempt odbc driver python command hello python command work fine local window environ connect remot azur sql want connect driver sql server run cli command order run experi open lib odbc driver sql server file aml run contain linux problem configur runconfig yaml file tell linux machin driver connect virtual machin instal driver us aml right need aforement file right dan",
        "Question_gpt_summary_original":"The user is encountering a \"File Not found\" error when attempting to find the ODBC driver for the pyodbc.connect command while running an Azure ML experiment. The command works fine in the user's local Windows environment, but when running the az ml submit-run CLI command to run the experiment in Azure Machine Learning, the error occurs. The user suspects that AML is running its containers in Linux and is unsure how to configure the .runconfig or .yaml file to tell the Linux machine where to find the driver. The user cannot connect to a virtual machine to install the driver and needs to do it in one of the aforementioned files.",
        "Question_gpt_summary":"user encount file error attempt odbc driver pyodbc connect command run experi command work fine user local window environ run submit run cli command run experi error occur user suspect aml run contain linux unsur configur runconfig yaml file tell linux machin driver user connect virtual machin instal driver need aforement file",
        "Answer_original_content":"danwellisch jupyt notebook portal run command connect sql ye instal requir librari notebook try connect case enabl ssh access comput instanc login instal requir librari enabl ssh access login termin instanc link comput creat instal librari avail sudo dpkg grep msodbcsql command return blank know driver miss server need instal instal driver run follow sudo accept eula apt instal msodbcsql follow file avail help configur us driver new directori locat opt microsoft msodbcsql directori librari instal second file call odbcinst ini host path librari python need main librari host opt microsoft msodbcsql lib libmsodbcsql librari dynam librari attach default run experi check connect work hello run jupit notebook run laptop submit run cli command specifi python code run config yml file creat azur notebook log portal run step mention configur comput submit run cli command odbc librari correct dan danwellisch train script us pyodbc access prem sql server long there network connect need defin custom docker imag configur driver us demand aml comput easier us http github com mkleehamm pyodbc wiki connect sql server linux",
        "Answer_preprocessed_content":"jupyt notebook portal run command connect sql ye instal requir librari notebook try connect case enabl ssh access comput instanc login instal requir librari enabl ssh access login termin instanc link comput creat instal librari avail sudo dpkg grep msodbcsql command return blank know driver miss server need instal instal driver run follow sudo instal msodbcsql follow file avail help configur us driver new directori locat directori librari instal second file call host path librari python need main librari host librari dynam librari attach default run experi check connect work hello run jupit notebook run laptop cli command specifi python code run config yml file creat azur notebook log portal run step mention configur comput cli command odbc librari correct dan train script us pyodbc access prem sql server long there network connect need defin custom docker imag configur driver us demand aml comput easier us",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- enable ssh access to the compute instance and install the required library if not available already.\n- install the msodbcsql17 driver by running the command \"sudo accept_eula=y apt-get install msodbcsql17\".\n- check if the following files are available: a new directory located in \/opt\/microsoft\/msodbcsql17\/, a file in \/etc\/ called odbcinst.ini, and the main library that is hosted in \/opt\/microsoft\/msodbcsql17\/lib64\/libmsodbcsql-17.4.so.1.1.\n- use a custom docker image to configure the driver if using on-demand aml compute.\n- use vm to connect to the sql server from linux.",
        "Answer_gpt_summary":"possibl solut extract answer enabl ssh access comput instanc instal requir librari avail instal msodbcsql driver run command sudo accept eula apt instal msodbcsql check follow file avail new directori locat opt microsoft msodbcsql file call odbcinst ini main librari host opt microsoft msodbcsql lib libmsodbcsql us custom docker imag configur driver demand aml comput us connect sql server linux"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Inference for Tensorflow Base64 Input Error through API Gateway",
        "Question_body":"When I am trying to call my Sagemaker TF endpoint using API Gateway -> Lambda Func by passing a Base 64 String (an image) I am getting an unsupported string error. I also tried with application\/Json but I am still getting the error. Need Suggestion.\n\nIn Notebook Instance this is how my input looks: <CODE> input = { 'instances': [{\"b64\": \"iV\"}] }\n\nIn Lambda function I am doing this: <CODE>\n\ninstance = [{\"b64\": \"b64string\"}] pleasework=json.dumps({\"instances\": instance}) response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME_BASE64,ContentType='string',Accept='string' ,Body=pleasework)\n\nERROR: Inference Error: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from primary with message \"{\"error\": \"Unsupported Media Type: string\"}\".\n\nIncase if I pass application\/json I get this error:\n\nReceived client error (400) from primary with message \"{ \"error\": \"Failed to process element: 0 of 'instances' list. Error: INVALID_ARGUMENT: JSON Value: {\\n \"b64\": \"iV\"\\n} Type: Object is not of expected type: uint8\"}\"",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659593262466,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":91.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCAh0uL1NRmCcZANpWnZd7A\/sagemaker-inference-for-tensorflow-base-64-input-error-through-api-gateway",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-05T18:18:56.773Z",
                "Answer_score":0,
                "Answer_body":"I would suggest testing invoking your model locally first and confirming what the input your model is expecting using the saved_model CLI. Kindly see this link: https:\/\/www.tensorflow.org\/guide\/saved_model#the_savedmodel_format_on_disk\n\nThen when invoking the model confirm that instance is in the correct input format shape your model expects.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"infer tensorflow base input error api gatewai try endpoint api gatewai lambda func pass base string imag get unsupport string error tri applic json get error need suggest notebook instanc input look input instanc lambda function instanc bstring pleasework json dump instanc instanc respons runtim invok endpoint endpointnam endpoint base contenttyp string accept string bodi pleasework error infer error error occur modelerror call invokeendpoint oper receiv client error primari messag error unsupport media type string incas pass applic json error receiv client error primari messag error fail process element instanc list error invalid argument json valu type object expect type uint",
        "Question_preprocessed_content":"infer tensorflow base input error api gatewai try endpoint api gatewai lambda func pass base string get unsupport string error tri get error need suggest notebook instanc input look",
        "Question_gpt_summary_original":"The user is encountering an error when trying to call their Sagemaker TF endpoint using API Gateway and Lambda Func by passing a Base 64 String (an image). They have tried passing the input as application\/Json but are still getting the error. The error message indicates that the input is an unsupported media type or an invalid argument.",
        "Question_gpt_summary":"user encount error try endpoint api gatewai lambda func pass base string imag tri pass input applic json get error error messag indic input unsupport media type invalid argument",
        "Answer_original_content":"suggest test invok model local confirm input model expect save model cli kindli link http tensorflow org guid save model savedmodel format disk invok model confirm instanc correct input format shape model expect",
        "Answer_preprocessed_content":"suggest test invok model local confirm input model expect cli kindli link invok model confirm instanc correct input format shape model expect",
        "Answer_gpt_summary_original":"the solution suggested is to test invoking the model locally and confirming the input format shape that the model expects using the saved_model cli.",
        "Answer_gpt_summary":"solut suggest test invok model local confirm input format shape model expect save model cli"
    },
    {
        "Question_id":null,
        "Question_title":"how to trigger sagemaker pipeline via code change in github?",
        "Question_body":"based on the aws docs and sample code provided here, https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/6299535b80b44ef0b61b95c979b1511157965810\/sagemaker-pipelines\/tabular\/customizing_build_train_deploy_project\/modelbuild\/pipelines\/customer_churn\/pipeline.py, I can wrap different ML steps like training into a step , in a sagemaker pipeline code. after that, i can define sagemaker projects to create a CI\/CD pipeline to build and deploy models. the examples i saw builds\/deploy model in aws code commit\/pipeline. is it possible to move this part to github or gitlab for repository and then deploy from gitlab or other tools like jenkins. are there any examples or code samples to do this?? also, instead of working in sagemaker studio IDE, is it possible to set up code repo in github or gitlab and we use our own IDE to push changes to the pipeline or sagemkaer projects code to build\/deploy model? and somehow hook that into studio such that for example , we make a change to hyperparameter value in gitlab, that will trigger the pipeline in studio?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663125520507,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":42.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaQEgFCVaTxCY_CINaOXSsw\/how-to-trigger-sagemaker-pipeline-via-code-change-in-github",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-14T21:18:41.617Z",
                "Answer_score":0,
                "Answer_body":"You can create SageMaker projects using third party source code repositories such as Gitlab, Github etc. Here's a blog that walks through a solution - https:\/\/aws.amazon.com\/blogs\/machine-learning\/create-amazon-sagemaker-projects-using-third-party-source-control-and-jenkins\/ If you did not want to use Projects, you can still use pipelines with your source code. You can create an Amazon EventBridge rule to trigger a SageMaker pipeline execution based on events or a given schedule. See an example here - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipeline-eventbridge.html",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"trigger pipelin code chang github base aw doc sampl code provid http github com aw amazon exampl blob bbefbbcb pipelin tabular custom build train deploi project modelbuild pipelin custom churn pipelin wrap differ step like train step pipelin code defin project creat pipelin build deploi model exampl saw build deploi model aw code commit pipelin possibl github gitlab repositori deploi gitlab tool like jenkin exampl code sampl instead work studio id possibl set code repo github gitlab us id push chang pipelin sagemka project code build deploi model hook studio exampl chang hyperparamet valu gitlab trigger pipelin studio",
        "Question_preprocessed_content":"trigger pipelin code chang github base aw doc sampl code provid wrap differ step like train step pipelin code defin project creat pipelin build deploi model exampl saw model aw code possibl github gitlab repositori deploi gitlab tool like jenkin exampl code sampl instead work studio id possibl set code repo github gitlab us id push chang pipelin sagemka project code model hook studio exampl chang hyperparamet valu gitlab trigger pipelin studio",
        "Question_gpt_summary_original":"The user is facing challenges in triggering a Sagemaker pipeline via code change in Github. They have successfully wrapped different ML steps like training into a step in a Sagemaker pipeline code and defined Sagemaker projects to create a CI\/CD pipeline to build and deploy models. However, they are unsure if it is possible to move the build\/deploy model part to Github or Gitlab for repository and then deploy from Gitlab or other tools like Jenkins. They are also looking for examples or code samples to do this and wondering if it is possible to set up a code repo in Github or Gitlab and use their own IDE to push changes to the pipeline or Sagemaker projects code to build\/deploy models. They are also looking for a way to hook that into Sagemaker Studio such that a change to hyperparameter value in Gitlab will trigger the pipeline in Studio.",
        "Question_gpt_summary":"user face challeng trigger pipelin code chang github successfulli wrap differ step like train step pipelin code defin project creat pipelin build deploi model unsur possibl build deploi model github gitlab repositori deploi gitlab tool like jenkin look exampl code sampl wonder possibl set code repo github gitlab us id push chang pipelin project code build deploi model look wai hook studio chang hyperparamet valu gitlab trigger pipelin studio",
        "Answer_original_content":"creat project parti sourc code repositori gitlab github blog walk solut http aw amazon com blog machin learn creat amazon project parti sourc control jenkin want us project us pipelin sourc code creat amazon eventbridg rule trigger pipelin execut base event given schedul exampl http doc aw amazon com latest pipelin eventbridg html",
        "Answer_preprocessed_content":"creat project parti sourc code repositori gitlab github blog walk solut want us project us pipelin sourc code creat amazon eventbridg rule trigger pipelin execut base event given schedul exampl",
        "Answer_gpt_summary_original":"possible solutions to trigger a pipeline via code changes in github or gitlab and use ide to push changes to the pipeline or  projects code to build\/deploy models are: \n1. create  projects using third-party source control repositories such as gitlab, github, etc. \n2. use pipelines with your source code and create an amazon eventbridge rule to trigger a pipeline execution based on events or a given schedule.",
        "Answer_gpt_summary":"possibl solut trigger pipelin code chang github gitlab us id push chang pipelin project code build deploi model creat project parti sourc control repositori gitlab github us pipelin sourc code creat amazon eventbridg rule trigger pipelin execut base event given schedul"
    },
    {
        "Question_id":63127521.0,
        "Question_title":"Deploying Model to Kubernetes",
        "Question_body":"<p>I am trying to deploy a model to Kubernetes in Azure Machine Learning Studio, it was working for a while, but now, it fails during deployment, the error message is as follows:<\/p>\n<pre><code>Deploy: Failed on step WaitServiceCreating. Details: AzureML service API error. \nYour container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r.\nFrom the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally.\nPlease refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\n<\/code><\/pre>\n<pre class=\"lang-js prettyprint-override\"><code>{\n    &quot;code&quot;: &quot;KubernetesDeploymentFailed&quot;,\n    &quot;statusCode&quot;: 400,\n    &quot;message&quot;: &quot;Kubernetes Deployment failed&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n            &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1595913837627,
        "Question_favorite_count":null,
        "Question_last_edit_time":1595945366092,
        "Question_score":1.0,
        "Question_view_count":186.0,
        "Answer_body":"<p>It seems it was a bug, got corrected by itself today. Closing this question now<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63127521",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1596104635683,
        "Question_original_content":"deploi model kubernet try deploi model kubernet studio work fail deploy error messag follow deploi fail step waitservicecr detail servic api error contain applic crash caus error score file init function check log contain instanc pipelin creat aml sdk run print servic log servic object fetch log try run imag viennaglob azurecr aeffebdcc latest local refer http aka debugimag servic launch fail inform code kubernetesdeploymentfail statuscod messag kubernet deploy fail detail code crashloopbackoff messag contain applic crash caus error score file init function check log contain instanc pipelin creat aml sdk run print servic log servic object fetch log nyou try run imag viennaglob azurecr aeffebdcc latest local refer http aka debugimag servic launch fail inform",
        "Question_preprocessed_content":"deploi model kubernet try deploi model kubernet studio work fail deploy error messag follow",
        "Question_gpt_summary_original":"The user is facing challenges while deploying a model to Kubernetes in Azure Machine Learning Studio. The deployment fails during the process and the error message suggests that the container application crashed, which could be due to errors in the scoring file's init() function. The user is advised to check the logs for the container instance and run print(service.get_logs()) to fetch the logs. The user can also try running the image locally and refer to the provided link for more information.",
        "Question_gpt_summary":"user face challeng deploi model kubernet studio deploy fail process error messag suggest contain applic crash error score file init function user advis check log contain instanc run print servic log fetch log user try run imag local refer provid link inform",
        "Answer_original_content":"bug got correct todai close question",
        "Answer_preprocessed_content":"bug got correct todai close question",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer as the issue was resolved on its own.",
        "Answer_gpt_summary":"solut provid answer issu resolv"
    },
    {
        "Question_id":70565147.0,
        "Question_title":"AWS sagemaker datawrangler continues to be used after closing everything",
        "Question_body":"<p>I used data wrangler for maybe 3h a week ago, and I open my account today to see that Ive been charged for 6 days worth of data wrangler usage. Basically it was running in the background the whole time. The first 25h were part of free tier then I got charged for the rest of the time. I dont have any endpoints to close so whats the issue? I dont care about the costs, I know I can talk to support to get the charges reversed but they dont seem to know whats going on because they havent helped me at all.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641209554070,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":36.0,
        "Answer_body":"<p>After going over <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-shut-down.html\" rel=\"nofollow noreferrer\">the docs<\/a>, I found that I needed to shut down the wrangler instance under Running Instances and Kernels button.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1641216587632,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70565147",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641216477460,
        "Question_original_content":"datawrangl continu close data wrangler mayb week ago open account todai iv charg dai worth data wrangler usag basic run background time free tier got charg rest time dont endpoint close what issu dont care cost know talk support charg revers dont know what go havent help",
        "Question_preprocessed_content":"datawrangl continu close data wrangler mayb week ago open account todai iv charg dai worth data wrangler usag basic run background time free tier got charg rest time dont endpoint close what issu dont care cost know talk support charg revers dont know what go havent help",
        "Question_gpt_summary_original":"The user encountered a challenge where AWS Sagemaker Data Wrangler continued to run in the background and resulted in unexpected charges for 6 days of usage, even though the user only used it for 3 hours a week ago. The user is unsure why this happened and has not been able to get help from support.",
        "Question_gpt_summary":"user encount challeng data wrangler continu run background result unexpect charg dai usag user hour week ago user unsur happen abl help support",
        "Answer_original_content":"go doc need shut wrangler instanc run instanc kernel button",
        "Answer_preprocessed_content":"go doc need shut wrangler instanc run instanc kernel button",
        "Answer_gpt_summary_original":"solution: the user needs to shut down the wrangler instance under the \"running instances and kernels\" button to avoid being charged for datawrangler usage even after closing everything.",
        "Answer_gpt_summary":"solut user need shut wrangler instanc run instanc kernel button avoid charg datawrangl usag close"
    },
    {
        "Question_id":58323029.0,
        "Question_title":"Azure ML deploy locally: tarfile.ReadError: file could not be opened successfully",
        "Question_body":"<p>I am trying to deploy(local) using this line:<\/p>\n\n<pre><code>local_service = Model.deploy(ws, \"test\", [model], inference_config, deployment_config)\n<\/code><\/pre>\n\n<p>Then I get this output in the terminal:<\/p>\n\n<pre><code>tarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WXbPe.png\" rel=\"nofollow noreferrer\">Screenshot of the output<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570710941163,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":77.0,
        "Answer_body":"<p>There was a bug with the retry logic when files were being uploaded. That bug has since been fixed, so updating your SDK should fix the issue.<\/p>\n\n<p>Similar post: <a href=\"https:\/\/stackoverflow.com\/questions\/57854136\/registering-and-downloading-a-fasttext-bin-model-fails-with-azure-machine-learn\">Registering and downloading a fastText .bin model fails with Azure Machine Learning Service<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58323029",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1570725963836,
        "Question_original_content":"deploi local tarfil readerror file open successfulli try deploi local line local servic model deploi test model infer config deploy config output termin tarfil readerror file open successfulli screenshot output",
        "Question_preprocessed_content":"deploi local file open successfulli try deploi line output termin screenshot output",
        "Question_gpt_summary_original":"The user is encountering a challenge while trying to deploy a local service using Azure ML. The error message \"tarfile.ReadError: file could not be opened successfully\" is displayed in the terminal, preventing the deployment from being successful.",
        "Question_gpt_summary":"user encount challeng try deploi local servic error messag tarfil readerror file open successfulli displai termin prevent deploy success",
        "Answer_original_content":"bug retri logic file upload bug fix updat sdk fix issu similar post regist download fasttext bin model fail servic",
        "Answer_preprocessed_content":"bug retri logic file upload bug fix updat sdk fix issu similar post regist download fasttext bin model fail servic",
        "Answer_gpt_summary_original":"possible solution: updating the sdk should fix the issue as there was a bug with the retry logic when files were being uploaded.",
        "Answer_gpt_summary":"possibl solut updat sdk fix issu bug retri logic file upload"
    },
    {
        "Question_id":null,
        "Question_title":"If on Azure any reason not to use the Azure ML platform vs a dedicated VM and azure functions",
        "Question_body":"I come from a full stack dev background and I don't know that much about ML, just evaluating a proposal. We have Azure web apps, Azure SQL and Azure BI for our SaaS. Seems it would make more sense to use the Azure ML platform and have it create the private endpoints and manage deployment, less setup...\nThis is more for text parsing, good and bad messages and suggestion like spam detection.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603468321273,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/137047\/if-on-azure-any-reason-not-to-use-the-azure-ml-pla.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-23T20:42:31.993Z",
                "Answer_score":0,
                "Answer_body":"Hi, if you're going to be building and deploying ML solutions\/pipelines, Azure ML offers great flexibility. AML offers code, no-code, and auto-ml options for ml workflows. AML also enables you to manage all of your ML resources in your workspace and offers MLOPs. Also, the ability to deploy a machine learning model to Azure Functions is currently in preview. Hope this helps.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"azur reason us platform dedic azur function come stack dev background know evalu propos azur web app azur sql azur saa sens us platform creat privat endpoint manag deploy setup text pars good bad messag suggest like spam detect",
        "Question_preprocessed_content":"azur reason us platform dedic azur function come stack dev background know evalu propos azur web app azur sql azur saa sens us platform creat privat endpoint manag deploy text pars good bad messag suggest like spam detect",
        "Question_gpt_summary_original":"The user is evaluating a proposal for implementing machine learning in their SaaS. They are considering using the Azure ML platform but are unsure if it is better than using a dedicated VM and Azure functions. The user's main goal is to implement text parsing for spam detection.",
        "Question_gpt_summary":"user evalu propos implement machin learn saa consid platform unsur better dedic azur function user main goal implement text pars spam detect",
        "Answer_original_content":"go build deploi solut pipelin offer great flexibl aml offer code code auto option workflow aml enabl manag resourc workspac offer mlop abil deploi machin learn model azur function current preview hope help",
        "Answer_preprocessed_content":"go build deploi offer great flexibl aml offer code option workflow aml enabl manag resourc workspac offer mlop abil deploi machin learn model azur function current preview hope help",
        "Answer_gpt_summary_original":"the answer does not provide any solutions related to the original question. instead, it discusses the benefits of using  (aml) for building and deploying machine learning solutions and pipelines. it also mentions that aml offers code, no-code, and auto-ml options for ml workflows, and enables users to manage all of their ml resources in their workspace and offers mlops. additionally, it mentions that the ability to deploy a machine learning model to azure functions is currently in preview.",
        "Answer_gpt_summary":"answer provid solut relat origin question instead discuss benefit aml build deploi machin learn solut pipelin mention aml offer code code auto option workflow enabl user manag resourc workspac offer mlop addition mention abil deploi machin learn model azur function current preview"
    },
    {
        "Question_id":null,
        "Question_title":"open a csv file",
        "Question_body":"Hello,\n\nI have a very basic question. I have a CSV file saved in a blob. I need to open that file in ML Studio. whenever I reference the file I just get an error that the file doesn't exist. Please help",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636324623930,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/618167\/open-a-csv-file.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-08T10:12:28.853Z",
                "Answer_score":0,
                "Answer_body":"@KamilaThompson-6591 Thanks for the question. AML datasets enables customers to benefit from our storage, compute and framework agnostic data access solution. Additionally with the launch of identity based access, customers can just pass the url directly to create a dataset (without the need for create a datastore). This avoids storing credentials in AML or managing data access yourself.\n\nHere is the document to Connect to data with the Azure Machine Learning studio.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"open csv file hello basic question csv file save blob need open file studio refer file error file exist help",
        "Question_preprocessed_content":"open csv file hello basic question csv file save blob need open file studio refer file error file exist help",
        "Question_gpt_summary_original":"The user is facing challenges in opening a CSV file saved in a blob in ML Studio. Despite referencing the file, they receive an error message indicating that the file does not exist.",
        "Question_gpt_summary":"user face challeng open csv file save blob studio despit referenc file receiv error messag indic file exist",
        "Answer_original_content":"kamilathompson thank question aml dataset enabl custom benefit storag comput framework agnost data access solut addition launch ident base access custom pass url directli creat dataset need creat datastor avoid store credenti aml manag data access document connect data studio",
        "Answer_preprocessed_content":"thank question aml dataset enabl custom benefit storag comput framework agnost data access solut addition launch ident base access custom pass url directli creat dataset avoid store credenti aml manag data access document connect data studio",
        "Answer_gpt_summary_original":"possible solutions from the answer include using aml datasets to access the csv file stored in a blob, utilizing identity-based access to create a dataset without storing credentials in aml, and referring to a document for connecting to data with the studio.",
        "Answer_gpt_summary":"possibl solut answer includ aml dataset access csv file store blob util ident base access creat dataset store credenti aml refer document connect data studio"
    },
    {
        "Question_id":null,
        "Question_title":"AzureML Notebooks: how to access data from an experiment",
        "Question_body":"Hi, I am new to Azure ML, and I have been trying to replicate the same structure presented in the MNIST tutorial, but I don't understand how to adapt it to my case.\n\nI am running a python file from the experiment, but I don't understand how I can access data that is currently in a folder in the cloud file system from the script running in the experiment.\nI have found many examples about accessing one single .csv file, but my data is made of many images.\n\nFrom my understanding I should first load the folder to a datastore, then use Dataset.File.upload_directory to create a dataset containing my folder, and here is how I tried to do it:\n\n # Create dataset from data directory\n datastore = Datastore.get(ws, 'workspaceblobstore')\n dataset = Dataset.File.upload_directory(path_data, target, pattern=None, overwrite=False, show_progress=True)\n    \n file_dataset = dataset.register(workspace=ws, name='reduced_classification_dataset',\n                                                  description='reduced_classification_dataset',\n                                                  create_new_version=True)\n\n\n\nBut then I don't understand if and how I can access this data like a normal file system from my python script, or I need further steps to be able to do that.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1614762558803,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@Matzof Thanks for the question. Please follow the below code for writing.\n\n    datastore = ## get your defined in Workspace as Datastore \n datastore.upload(src_dir='.\/files\/to\/copy\/...',\n                  target_path='target\/directory',\n                  overwrite=True)\n\n\n\nDatastore.upload only support blob and fileshare. For adlsgen2 upload, you can try our new dataset upload API:\n\n\n\n from azureml.core import Dataset, Datastore\n datastore = Datastore.get(workspace, 'mayadlsgen2')\n Dataset.File.upload_directory(src_dir='.\/data', target=(datastore,'data'))\n\n\n\n\nPandas is integrated with fsspec which provides Pythonic implementation for filesystems including s3, gcs, and Azure. You can check the source for Azure here: dask\/adlfs: fsspec-compatible Azure Datake and Azure Blob Storage access (github.com). With this you can use normal filesystem operations like ls, glob, info, etc.\n\nYou can find an example (for reading data) here: azureml-examples\/1.intro-to-dask.ipynb at main \u00b7 Azure\/azureml-examples (github.com)\n\nWriting is essentially the same as reading, you need to switch the protocol to abfs (or az), slightly modify how you're accessing the data, and provide credentials unless your blob has public write access.\n\nYou can use the Azure ML Datastore to retrieve credentials like this (taken from example):",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/296661\/azureml-notebooks-how-to-access-data-from-an-exper.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-04T04:48:33.09Z",
                "Answer_score":0,
                "Answer_body":"@Matzof Thanks for the question. Please follow the below code for writing.\n\n    datastore = ## get your defined in Workspace as Datastore \n datastore.upload(src_dir='.\/files\/to\/copy\/...',\n                  target_path='target\/directory',\n                  overwrite=True)\n\n\n\nDatastore.upload only support blob and fileshare. For adlsgen2 upload, you can try our new dataset upload API:\n\n\n\n from azureml.core import Dataset, Datastore\n datastore = Datastore.get(workspace, 'mayadlsgen2')\n Dataset.File.upload_directory(src_dir='.\/data', target=(datastore,'data'))\n\n\n\n\nPandas is integrated with fsspec which provides Pythonic implementation for filesystems including s3, gcs, and Azure. You can check the source for Azure here: dask\/adlfs: fsspec-compatible Azure Datake and Azure Blob Storage access (github.com). With this you can use normal filesystem operations like ls, glob, info, etc.\n\nYou can find an example (for reading data) here: azureml-examples\/1.intro-to-dask.ipynb at main \u00b7 Azure\/azureml-examples (github.com)\n\nWriting is essentially the same as reading, you need to switch the protocol to abfs (or az), slightly modify how you're accessing the data, and provide credentials unless your blob has public write access.\n\nYou can use the Azure ML Datastore to retrieve credentials like this (taken from example):",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1614833313088,
        "Question_original_content":"notebook access data experi new try replic structur present mnist tutori understand adapt case run python file experi understand access data current folder cloud file script run experi exampl access singl csv file data imag understand load folder datastor us dataset file upload directori creat dataset contain folder tri creat dataset data directori datastor datastor workspaceblobstor dataset dataset file upload directori path data target pattern overwrit fals progress true file dataset dataset regist workspac reduc classif dataset descript reduc classif dataset creat new version true understand access data like normal file python script need step abl",
        "Question_preprocessed_content":"notebook access data experi new try replic structur present mnist tutori understand adapt case run python file experi understand access data current folder cloud file script run experi exampl access singl csv file data imag understand load folder datastor us creat dataset contain folder tri creat dataset data directori datastor workspaceblobstor dataset target pattern overwrit fals understand access data like normal file python script need step abl",
        "Question_gpt_summary_original":"The user is facing challenges in accessing data from a folder in the cloud file system in Azure ML. They are trying to replicate the structure presented in the MNIST tutorial but are unsure how to adapt it to their case, which involves many images. They have attempted to create a dataset from the data directory but are unsure how to access it like a normal file system from their python script.",
        "Question_gpt_summary":"user face challeng access data folder cloud file try replic structur present mnist tutori unsur adapt case involv imag attempt creat dataset data directori unsur access like normal file python script",
        "Answer_original_content":"matzof thank question follow code write datastor defin workspac datastor datastor upload src dir file copi target path target directori overwrit true datastor upload support blob fileshar adlsgen upload try new dataset upload api core import dataset datastor datastor datastor workspac mayadlsgen dataset file upload directori src dir data target datastor data panda integr fsspec provid python implement filesystem includ gc azur check sourc azur dask adlf fsspec compat azur datak azur blob storag access github com us normal filesystem oper like glob info exampl read data exampl intro dask ipynb main azur exampl github com write essenti read need switch protocol abf slightli modifi access data provid credenti blob public write access us datastor retriev credenti like taken exampl",
        "Answer_preprocessed_content":"thank question follow code write datastor defin workspac datastor overwrit true support blob fileshar adlsgen upload try new dataset upload api core import dataset datastor datastor mayadlsgen target datastor data panda integr fsspec provid python implement filesystem includ gc azur check sourc azur azur datak azur blob storag access us normal filesystem oper like glob info exampl main write essenti read need switch protocol abf slightli modifi access data provid credenti blob public write access us datastor retriev credenti like",
        "Answer_gpt_summary_original":"possible solutions from the answer include using the datastore.upload method to upload files to the target directory, trying the new dataset upload api for adlsgen2 upload, and using pandas integrated with fsspec to access cloud file systems like azure. additionally, the answer suggests modifying how data is accessed and providing credentials if necessary.",
        "Answer_gpt_summary":"possibl solut answer includ datastor upload method upload file target directori try new dataset upload api adlsgen upload panda integr fsspec access cloud file system like azur addition answer suggest modifi data access provid credenti necessari"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Studio JupyterServer App does not load",
        "Question_body":"After months of seamless work in SageMaker Studio, the JupyterServer App won't load for the last 4 days. The Control Panel shows that the JupyterServer is in \"Pending\" or \"Failed\" state after I try to launch the app. When clicking \"Launch app\", the screen shows that:\n\n\"The JupyterServer app default encountered a problem and was stopped.\"\nThe \"Restart Now\" button is visible, but pressing this results in the same behaviour. I created a new JupyterServer App and it experiences the same problem under that account. I use a different account for another project and the JupyterServer under that account works perfectly. I even mounted the EFS associated with the App on an EC2 instance and deleted some files to reduce the EFS volume but it did not help (it was 995 MB and as far as I know, 5GB is the default limit).\n\nI found a post stating the same problem from 2 years ago, but could not follow the advice to delete the app and create a new one, since the Delete option is not available in the Action dropdown (https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sage-maker-studio-will-not-load). All apps that I create is \"default\".\n\nPlease help, how could I overcome this and access Jupyter Lab again? Thank you.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1656519218822,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":85.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5Da9xot8TwST0MP_8uRV2A\/sagemaker-studio-jupyter-server-app-does-not-load",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-30T03:09:11.417Z",
                "Answer_score":0,
                "Answer_body":"Even if you're not seeing the \"Delete app\" option in console (because the app Failed), the good news is that the \"Restart\" button should be doing the same thing for you: So how can you find out more about what's breaking here and hopefully fix it?\n\nTo see logs, you can open the \/aws\/sagemaker\/studio log group in Amazon CloudWatch: Here you should find log streams like {DomainID}\/{UserProfileName}\/JupyterServer\/default and (if you have a lifecycle configuration script set up?) {DomainID}\/{UserProfileName}\/JupyterServer\/default\/LifecycleConfigOnStart.\n\nIf you do have a custom lifecycle configuration script set up, this can often be a point of failure: I'd suggest trying to detach it and\/or adding some extra flags like set -ux to help debug what might be going wrong in it. Also since SageMaker Studio recently launched JupyterLab v3 support in parallel to JLv1. If you've been experimenting with both versions, it's worth checking which version your user is currently configured to use, and remembering that some setup scripts which might work on one version could break on another.\n\nYour user's EFS home folder (and any additional setup the LCC script does) will be the only data persisted between JupyterServer launches, so content could be another point of failure. It sounds like you started to explore this already.\n\nYou could try to delete (or otherwise edit) the user's ~\/.jupyter folder from EFS to clear any customized Jupyter configuration settings that might be causing problems during start-up. Again, this may be useful if you're using any features or extensions for which the Jupyter configuration API changed between JLv1 and v3.\nI haven't found overall data volume to cause these kind of start-up problems myself so far. I have seen some cases where having a git repository with many active changes (e.g. thousands) causes a UI slowdown when working in the repository's folder, but I haven't seen it prevent the actual start-up I think?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-01T19:57:55.897Z",
                "Answer_score":0,
                "Answer_body":"@Alex_T thank you for the reply. The CloudWatch logs don't show any sign of failing notebooks, and I did not have any LCC either. The final solution was the following:\n\nI mounted the EFS associated to the SageMaker domain on an EC2 instance and made a backup of all notebooks and other files (saved them on my local computer too).\nThen, I deleted the SageMaker domain by following these steps: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-delete-domain.html (via CLI)\nCreated a new SageMaker domain. The JupyterServer App starts now.\nFinally, I mounted the new EFS to the EC2 instance and uploaded the notebooks and files. They are all visible and working in Jupyter.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"studio jupyterserv app load month seamless work studio jupyterserv app won load dai control panel show jupyterserv pend fail state try launch app click launch app screen show jupyterserv app default encount problem stop restart button visibl press result behaviour creat new jupyterserv app experi problem account us differ account project jupyterserv account work perfectli mount ef associ app instanc delet file reduc ef volum help far know default limit post state problem year ago follow advic delet app creat new delet option avail action dropdown http repost aw question quxosaetzqbk towjjvsmq sage maker studio load app creat default help overcom access jupyt lab thank",
        "Question_preprocessed_content":"studio jupyterserv app load month seamless work studio jupyterserv app won load dai control panel show jupyterserv pend fail state try launch app click launch app screen show jupyterserv app default encount problem restart button visibl press result behaviour creat new jupyterserv app experi problem account us differ account project jupyterserv account work perfectli mount ef associ app instanc delet file reduc ef volum help post state problem year ago follow advic delet app creat new delet option avail action dropdown app creat default help overcom access jupyt lab thank",
        "Question_gpt_summary_original":"The user is facing challenges with the JupyterServer App in SageMaker Studio, which has not been loading for the past four days. The Control Panel shows that the JupyterServer is in \"Pending\" or \"Failed\" state after trying to launch the app. The user has tried creating a new JupyterServer App, reducing the EFS volume, and mounting it on an EC2 instance, but the problem persists. The user is unable to delete the app and create a new one, as the Delete option is not available in the Action dropdown.",
        "Question_gpt_summary":"user face challeng jupyterserv app studio load past dai control panel show jupyterserv pend fail state try launch app user tri creat new jupyterserv app reduc ef volum mount instanc problem persist user unabl delet app creat new delet option avail action dropdown",
        "Answer_original_content":"see delet app option consol app fail good new restart button thing break hopefulli fix log open aw studio log group amazon cloudwatch log stream like domainid userprofilenam jupyterserv default lifecycl configur script set domainid userprofilenam jupyterserv default lifecycleconfigonstart custom lifecycl configur script set point failur suggest try detach ad extra flag like set help debug go wrong studio recent launch jupyterlab support parallel jlv experi version worth check version user current configur us rememb setup script work version break user ef home folder addit setup lcc script data persist jupyterserv launch content point failur sound like start explor try delet edit user jupyt folder ef clear custom jupyt configur set caus problem start us featur extens jupyt configur api chang jlv haven overal data volum caus kind start problem far seen case have git repositori activ chang thousand caus slowdown work repositori folder haven seen prevent actual start think alex thank repli cloudwatch log sign fail notebook lcc final solut follow mount ef associ domain instanc backup notebook file save local delet domain follow step http doc aw amazon com latest studio delet domain html cli creat new domain jupyterserv app start final mount new ef instanc upload notebook file visibl work jupyt",
        "Answer_preprocessed_content":"see delet app option consol good new restart button thing break hopefulli fix log open log group amazon cloudwatch log stream like custom lifecycl configur script set point failur suggest try detach ad extra flag like set help debug go wrong studio recent launch jupyterlab support parallel jlv experi version worth check version user current configur us rememb setup script work version break user ef home folder data persist jupyterserv launch content point failur sound like start explor try delet user folder ef clear custom jupyt configur set caus problem us featur extens jupyt configur api chang jlv haven overal data volum caus kind problem far seen case have git repositori activ chang caus slowdown work repositori folder haven seen prevent actual think thank repli cloudwatch log sign fail notebook lcc final solut follow mount ef associ domain instanc backup notebook file delet domain follow step creat new domain jupyterserv app start final mount new ef instanc upload notebook file visibl work jupyt",
        "Answer_gpt_summary_original":"possible solutions to the issue of jupyterserver app being stuck in a \"pending\" or \"failed\" state in  studio are: checking the logs in amazon cloudwatch, detaching or debugging the custom lifecycle configuration script, checking the jupyterlab version and clearing any customized jupyter configuration settings, and deleting and recreating the domain. additionally, mounting the efs associated with the domain on an ec2 instance and backing up all notebooks and files before deleting the domain can help preserve the data.",
        "Answer_gpt_summary":"possibl solut issu jupyterserv app stuck pend fail state studio check log amazon cloudwatch detach debug custom lifecycl configur script check jupyterlab version clear custom jupyt configur set delet recreat domain addition mount ef associ domain instanc back notebook file delet domain help preserv data"
    },
    {
        "Question_id":58194899.0,
        "Question_title":"AWS SageMaker SKLearn entry point in a subdirectory?",
        "Question_body":"<p>Can I specify SageMaker estimator's entry point script to be in a subdirectory? So far, it fails for me. Here is what I want to do:<\/p>\n\n<pre><code>sklearn = SKLearn(\n    entry_point=\"RandomForest\/my_script.py\",\n    source_dir=\"..\/\",\n    hyperparameters={...\n<\/code><\/pre>\n\n<p>I want to do this so I don't have to break my directory structure. I have some modules, which I use in several sagemaker projects, and each project lives in its own directory:<\/p>\n\n<pre><code>my_git_repo\/\n\n  RandomForest\/\n    my_script.py\n    my_sagemaker_notebook.ipynb\n\n  TensorFlow\/\n    my_script.py\n    my_other_sagemaker_notebook.ipynb\n\nmodule_imported_in_both_scripts.py\n<\/code><\/pre>\n\n<p>If I try to run this, SageMaker fails because it seems to parse the name of the entry point script to make a module name out of it, and it does not do a good job:<\/p>\n\n<pre><code>\/usr\/bin\/python3 -m RandomForest\/my_script --bootstrap True --case nf_2 --max_features 0.5 --min_impurity_decrease 5.323785009485933e-06 --model_name model --n_estimators 455 --oob_score True\n\n...\n\n\/usr\/bin\/python3: No module named RandomForest\/my_script\n\n<\/code><\/pre>\n\n<p>Anyone knows a way around this other than putting <code>my_script.py<\/code> in the <code>source_dir<\/code>?<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/54314876\/aws-sagemaker-sklearn-entry-point-allow-multiple-script\">Related to this question<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1569987546700,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":843.0,
        "Answer_body":"<p>Unfortunately, this is a gap in functionality. There is some related work in <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/pull\/941<\/a> which should also solve this issue, but for now, you do need to put <code>my_script.py<\/code> in <code>source_dir<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58194899",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1571939829096,
        "Question_original_content":"sklearn entri point subdirectori specifi estim entri point script subdirectori far fail want sklearn sklearn entri point randomforest script sourc dir hyperparamet want break directori structur modul us project project live directori git repo randomforest script notebook ipynb tensorflow script notebook ipynb modul import script try run fail pars entri point script modul good job usr bin python randomforest script bootstrap true case max featur min impur decreas model model estim oob score true usr bin python modul name randomforest script know wai put script sourc dir relat question",
        "Question_preprocessed_content":"sklearn entri point subdirectori specifi estim entri point script subdirectori far fail want want break directori structur modul us project project live directori try run fail pars entri point script modul good job know wai put relat question",
        "Question_gpt_summary_original":"The user is facing a challenge in specifying the entry point script for the SageMaker estimator to be in a subdirectory. The user wants to maintain their directory structure and avoid breaking it, but SageMaker fails to parse the name of the entry point script to make a module name out of it. This results in a module not found error. The user is seeking a solution to this problem other than putting the script in the source directory.",
        "Question_gpt_summary":"user face challeng specifi entri point script estim subdirectori user want maintain directori structur avoid break fail pars entri point script modul result modul error user seek solut problem put script sourc directori",
        "Answer_original_content":"unfortun gap function relat work http github com aw python sdk pull solv issu need script sourc dir",
        "Answer_preprocessed_content":"unfortun gap function relat work solv issu need",
        "Answer_gpt_summary_original":"possible solutions: \n- there is no direct solution to the problem of specifying an estimator's entry point script to be in a subdirectory when using sklearn.\n- there is some related work in https:\/\/github.com\/aws\/-python-sdk\/pull\/941 which may solve this issue.\n- for now, the only solution is to put the script in the source directory.",
        "Answer_gpt_summary":"possibl solut direct solut problem specifi estim entri point script subdirectori sklearn relat work http github com aw python sdk pull solv issu solut script sourc directori"
    },
    {
        "Question_id":null,
        "Question_title":"How could I upload notebooks to my AzureML workspace programatically",
        "Question_body":"Would like to upload Jupyter notebooks from different sources like GitHub into my workspace either directly or through my local machine (download locally first and then upload) but I would like to do it programmatically. Either with the AzureML SDK or azure cli",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651101620807,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. You can use compute instance terminal in AML notebooks to clone the GitHub repo. There's currently no option to upload notebooks to your workspace programmatically using sdk or cli.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/829311\/how-could-i-upload-notebooks-to-my-azureml-workspa.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-28T00:07:22.393Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. You can use compute instance terminal in AML notebooks to clone the GitHub repo. There's currently no option to upload notebooks to your workspace programmatically using sdk or cli.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651104442392,
        "Question_original_content":"upload notebook workspac programat like upload jupyt notebook differ sourc like github workspac directli local machin download local upload like programmat sdk azur cli",
        "Question_preprocessed_content":"upload notebook workspac programat like upload jupyt notebook differ sourc like github workspac directli local machin like programmat sdk azur cli",
        "Question_gpt_summary_original":"The user is facing challenges in uploading Jupyter notebooks from various sources like GitHub to their AzureML workspace programmatically using either the AzureML SDK or Azure CLI.",
        "Question_gpt_summary":"user face challeng upload jupyt notebook sourc like github workspac programmat sdk azur cli",
        "Answer_original_content":"thank reach us comput instanc termin aml notebook clone github repo current option upload notebook workspac programmat sdk cli kindli accept answer inform help thank",
        "Answer_preprocessed_content":"thank reach us comput instanc termin aml notebook clone github repo current option upload notebook workspac programmat sdk cli kindli accept answer inform help thank",
        "Answer_gpt_summary_original":"possible solution: the user can clone the github repository using the compute instance terminal in aml notebooks. however, there is currently no option to upload notebooks to their workspace programmatically using sdk or cli.",
        "Answer_gpt_summary":"possibl solut user clone github repositori comput instanc termin aml notebook current option upload notebook workspac programmat sdk cli"
    },
    {
        "Question_id":30115812.0,
        "Question_title":"Error while running Azure Machine Learning web service but the experiment works fine",
        "Question_body":"<p>I have created the Azure ML experiment with R script module \nit works fine while we run the experiment but\n when we publish the web service it throws error http 500 \n ( I believe the error is causing in the R script module because other modules are running fine in web service but i can't debug the problem<\/p>\n\n<blockquote>\n  <p>Http status code: 500, Timestamp: Fri, 08 May 2015 04:23:14 GMT<\/p>\n<\/blockquote>\n\n<p>Also is there any limitation in r e.g. some function which wont work in web service<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1431059608047,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1446192965568,
        "Question_score":1.0,
        "Question_view_count":415.0,
        "Answer_body":"<p>I found the problem. I was facing this error because the R module in the Azure ML was was taking variable as the other data type and not producing any outputs results which i was checking through for loop which is why i was getting this error.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30115812",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1439539745912,
        "Question_original_content":"error run web servic experi work fine creat experi script modul work fine run experi publish web servic throw error http believ error caus script modul modul run fine web servic debug problem http statu code timestamp fri gmt limit function wont work web servic",
        "Question_preprocessed_content":"error run web servic experi work fine creat experi script modul work fine run experi publish web servic throw error http believ error caus script modul modul run fine web servic debug problem http statu code timestamp fri gmt limit function wont work web servic",
        "Question_gpt_summary_original":"The user has encountered an error while running an Azure Machine Learning web service. The experiment works fine, but when the web service is published, it throws an HTTP 500 error. The user suspects that the error is caused by the R script module, but cannot debug the problem. Additionally, the user is unsure if there are any limitations in R functions that may not work in a web service.",
        "Question_gpt_summary":"user encount error run web servic experi work fine web servic publish throw http error user suspect error caus script modul debug problem addition user unsur limit function work web servic",
        "Answer_original_content":"problem face error modul take variabl data type produc output result check loop get error",
        "Answer_preprocessed_content":"problem face error modul take variabl data type produc output result check loop get error",
        "Answer_gpt_summary_original":"the solution to the http 500 error encountered when publishing a web service created with an r script module is to check if the r module is taking variables as the correct data type and producing output results. the error may occur if the r module is not producing any output results, which can be checked through a for loop.",
        "Answer_gpt_summary":"solut http error encount publish web servic creat script modul check modul take variabl correct data type produc output result error occur modul produc output result check loop"
    },
    {
        "Question_id":null,
        "Question_title":"Error while consuming the deployed web service through python",
        "Question_body":"I have tried consuming the web service with python with the link how-to-consume-web-service. I'm getting an error\n\npredict() missing 1 required positional argument: 'X'\n\n\nI have trained the model to predict only one field \" DESCRIPTION\" and two input fields \"CUSTOMERCODE\", \"DESCRIPTION\"\n\nwhen I try to predict with input data with the code below:\n\n\n\nimport requests\nimport json\n\n\nURL for the web service\n\nscoring_uri = 'xxx'\n# If the service is authenticated, set the key or token\nkey = 'xxx'\n\n\nTwo sets of data to score, so we get two results back\n\ndata = {\"data\":\n[\n[\n\"10000\",\n\"CAPPUCINO\"\n],\n[\n\"12345\",\n\"CAFFINE\"\n]\n]\n}\ninput_data = json.dumps(data)\nheaders = {'Content-Type': 'application\/json'}\nheaders['Authorization'] = f'Bearer {key}'\nresp = requests.post(scoring_uri, input_data, headers=headers)\nprint(resp.text)",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1613996362460,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/282967\/error-while-consuming-the-deployed-web-service-thr.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-25T18:44:25.18Z",
                "Answer_score":0,
                "Answer_body":"It seems your input is CUSTOMERCODE and target is DESCRIPTION, however, you are entering data for the target column when it is expecting only CUSTOMERCODE as input. Hope this helps!",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-02T02:43:22.363Z",
                "Answer_score":0,
                "Answer_body":"Hi, can you try testing your model locally first to view the results and the expected format for your test data? It seems you performed some transformations when fitting the model, so you need to ensure that you are providing your test data with the expected shape and dimension of the array. Hope this helps!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error consum deploi web servic python tri consum web servic python link consum web servic get error predict miss requir posit argument train model predict field descript input field customercod descript try predict input data code import request import json url web servic score uri servic authent set kei token kei set data score result data data cappucino caffin input data json dump data header content type applic json header author bearer kei resp request post score uri input data header header print resp text",
        "Question_preprocessed_content":"error consum deploi web servic python tri consum web servic python link get error predict miss requir posit argument train model predict field descript input field customercod descript try predict input data code import request import json url web servic servic authent set kei token kei set data score result data header header bearer resp header header",
        "Question_gpt_summary_original":"The user encountered an error while consuming a deployed web service through Python. The error message indicates that the \"predict()\" function is missing a required positional argument. The user trained the model to predict one field and two input fields. When attempting to predict with input data, the user received an error message.",
        "Question_gpt_summary":"user encount error consum deploi web servic python error messag indic predict function miss requir posit argument user train model predict field input field attempt predict input data user receiv error messag",
        "Answer_original_content":"input customercod target descript enter data target column expect customercod input hope help try test model local view result expect format test data perform transform fit model need ensur provid test data expect shape dimens arrai hope help",
        "Answer_preprocessed_content":"input customercod target descript enter data target column expect customercod input hope help try test model local view result expect format test data perform transform fit model need ensur provid test data expect shape dimens arrai hope help",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n1. the user needs to ensure that they are entering data only for the customercode column as input, instead of the target column.\n2. the user should test their model locally first to view the results and the expected format for their test data.\n3. the user needs to ensure that they are providing their test data with the expected shape and dimension of the array, as they performed some transformations when fitting the model.",
        "Answer_gpt_summary":"possibl solut answer user need ensur enter data customercod column input instead target column user test model local view result expect format test data user need ensur provid test data expect shape dimens arrai perform transform fit model"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb.plot.confusion_matrix() just show a Table!",
        "Question_body":"<p>Hello,<\/p>\n<p>I used this code to create a confusion matrix:<\/p>\n<pre><code class=\"lang-auto\"># confusion matrix\n        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<p>However, Wanda\u2019s website only shows a table instead of the confusion matrix. This is a screenshot from the issue:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046.png\" data-download-href=\"\/uploads\/short-url\/8UGiFwpsOZ6Pivp7qmFXgL1OuvI.png?dl=1\" title=\"Screenshot from 2022-01-09 20-58-35\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png\" alt=\"Screenshot from 2022-01-09 20-58-35\" data-base62-sha1=\"8UGiFwpsOZ6Pivp7qmFXgL1OuvI\" width=\"690\" height=\"249\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_690x249.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1035x373.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_1380x498.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3e79ae0d1bed34de85b7a5a0f60df3ac167ed046_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2022-01-09 20-58-35<\/span><span class=\"informations\">1741\u00d7629 29.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1641781829922,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":565.0,
        "Answer_body":"<aside class=\"quote no-group\" data-username=\"fdaliran\" data-post=\"1\" data-topic=\"1744\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/f\/73ab20\/40.png\" class=\"avatar\"> fdaliran:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-plot-confusion-matrix-just-show-a-table\/1744",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-10T10:12:33.312Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"fdaliran\" data-post=\"1\" data-topic=\"1744\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/f\/73ab20\/40.png\" class=\"avatar\"> fdaliran:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">        wandb.log({\"confusion-matrix-test\": wandb.plot.confusion_matrix(\n            probs=None,\n            y_true=all_gt, preds=all_pre,\n            class_names=classes_names)})\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>If you click the section called \u201cCustom Charts\u201d above the Table, it\u2019ll show the line plot that you\u2019ve logged.<\/p>\n<p>Logging the Table also is expected behaviour because this will allow users to interactively explore the logged data in a W&amp;B Table after logging it.<\/p>",
                "Answer_score":2.8,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-03-11T10:12:41.284Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641809553312,
        "Question_original_content":"plot confus matrix tabl hello code creat confus matrix confus matrix log confus matrix test plot confus matrix prob true pred pre class name class name wanda websit show tabl instead confus matrix screenshot issu screenshot",
        "Question_preprocessed_content":"tabl hello code creat confus matrix wanda websit show tabl instead confus matrix screenshot issu screenshot",
        "Question_gpt_summary_original":"The user is facing a challenge with the Wandb.plot.confusion_matrix() function as it only displays a table instead of a confusion matrix on the Wandb website.",
        "Question_gpt_summary":"user face challeng plot confus matrix function displai tabl instead confus matrix websit",
        "Answer_original_content":"fdaliran log confus matrix test plot confus matrix prob true pred pre class name class name click section call custom chart tabl itll line plot youv log log tabl expect behaviour allow user interact explor log data tabl log",
        "Answer_preprocessed_content":"fdaliran click section call custom chart tabl itll line plot youv log log tabl expect behaviour allow user interact explor log data tabl log",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- click the section called custom charts above the table to show the line plot that was logged.\n- interactively explore the logged data in a w&b table after logging it.",
        "Answer_gpt_summary":"possibl solut answer click section call custom chart tabl line plot log interact explor log data tabl log"
    },
    {
        "Question_id":56857709.0,
        "Question_title":"How to change experiment being tracked by Databricks \"runs\" tab?",
        "Question_body":"<p>I'm trying to use the mlflow databricks integration, specifically the tracking API. Normally, I can view past runs info in the handy sidebar of a notebook, as you can see <a href=\"https:\/\/i.stack.imgur.com\/JWY1c.png\" rel=\"nofollow noreferrer\">here<\/a> and which I got from the <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/quick-start.html#mlflow-mlflow-quick-start-python\" rel=\"nofollow noreferrer\">tutorial<\/a>. However, what I want now is to use multiple notebooks to send runs to the same experiment. Additionally, I would like to view the results of all these common runs in each of the notebooks. To do this, I need to change the (default) experiment tracked by the \"runs\" tab. <\/p>\n\n<p>Ultimately, my question boils down to the following: how can I set the experiment being tracked by the \"runs\" tab? I have tried using <code>mlflow.set_tracking_uri<\/code> and <code>mlflow.set_experiment(mlflow_experiment_name)<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1562089677383,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":257.0,
        "Answer_body":"<p>I don't believe this is possible today, as the design choice is to associate the runs tab to the notebook experiment.  From the <a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/tracking.html#notebook-experiments\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>Every Python and R notebook in a Databricks workspace has its own experiment. When you use MLflow in a notebook, it records runs in the notebook experiment.<\/p>\n<p>A notebook experiment shares the same name and ID as its corresponding notebook. The notebook ID is the numerical identifier at the end of a Notebook URL.<\/p>\n<\/blockquote>\n<p>You can create experiments independent of the notebook experiment and log runs to it from different sources.  You'll still have to open up the tracking UI to explore the results though.<\/p>\n<p>In other words, you can send multiple runs from different notebooks to the same experiment, but today you cannot log multiple runs to the 'Runs' tab in a specific notebook.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1592644375060,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56857709",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1566518807736,
        "Question_original_content":"chang experi track databrick run tab try us databrick integr specif track api normal view past run info handi sidebar notebook got tutori want us multipl notebook send run experi addition like view result common run notebook need chang default experi track run tab ultim question boil follow set experi track run tab tri set track uri set experi experi",
        "Question_preprocessed_content":"chang experi track databrick run tab try us databrick integr specif track api normal view past run info handi sidebar notebook got tutori want us multipl notebook send run experi addition like view result common run notebook need chang experi track run tab ultim question boil follow set experi track run tab tri",
        "Question_gpt_summary_original":"The user is facing a challenge in changing the experiment being tracked by the \"runs\" tab in Databricks while using the mlflow databricks integration. They want to use multiple notebooks to send runs to the same experiment and view the results of all these common runs in each of the notebooks, which requires changing the default experiment tracked by the \"runs\" tab. The user has tried using mlflow.set_tracking_uri and mlflow.set_experiment(mlflow_experiment_name) but has not been successful.",
        "Question_gpt_summary":"user face challeng chang experi track run tab databrick databrick integr want us multipl notebook send run experi view result common run notebook requir chang default experi track run tab user tri set track uri set experi experi success",
        "Answer_original_content":"believ possibl todai design choic associ run tab notebook experi doc python notebook databrick workspac experi us notebook record run notebook experi notebook experi share correspond notebook notebook numer identifi end notebook url creat experi independ notebook experi log run differ sourc open track explor result word send multipl run differ notebook experi todai log multipl run run tab specif notebook",
        "Answer_preprocessed_content":"believ possibl todai design choic associ run tab notebook experi doc python notebook databrick workspac experi us notebook record run notebook experi notebook experi share correspond notebook notebook numer identifi end notebook url creat experi independ notebook experi log run differ sourc open track explor result word send multipl run differ notebook experi todai log multipl run run tab specif notebook",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n- it is not possible to track multiple notebooks to the same experiment and view the results in each of the notebooks by setting the experiment being tracked by the \"runs\" tab.\n- every python and r notebook in a databricks workspace has its own experiment, and when you use `` in a notebook, it records runs in the notebook experiment.\n- you can create experiments independent of the notebook experiment and log runs to it from different sources, but you'll still have to open up the tracking ui to explore the results.\n- in other words, you can send multiple runs from different notebooks to the same experiment, but today you cannot log multiple runs to the 'runs' tab in a specific notebook.",
        "Answer_gpt_summary":"possibl solut answer possibl track multipl notebook experi view result notebook set experi track run tab python notebook databrick workspac experi us notebook record run notebook experi creat experi independ notebook experi log run differ sourc open track explor result word send multipl run differ notebook experi todai log multipl run run tab specif notebook"
    },
    {
        "Question_id":null,
        "Question_title":"Model for Predicting Earthquakes",
        "Question_body":"I am planning to create an Azure Machine Learning model to Predict the number of Earthquakes based on past historical data. I am asked to use one of the Regression techniques. Which regression model should I be using? Can I use Logistic Regression, or Poisson Regression, or Fast Forest Quantile Regression?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600174009770,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/95792\/model-for-predicting-earthquakes.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-15T13:46:48.727Z",
                "Answer_score":1,
                "Answer_body":"Hello, thanks for reaching out. Since you want to predict event counts, Poisson Regression would be appropriate. As a future reference, refer to this document for choosing the right algorithm for your predictive analytics model. Hope this helps.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"model predict earthquak plan creat model predict number earthquak base past histor data ask us regress techniqu regress model us logist regress poisson regress fast forest quantil regress",
        "Question_preprocessed_content":"model predict earthquak plan creat model predict number earthquak base past histor data ask us regress techniqu regress model us logist regress poisson regress fast forest quantil regress",
        "Question_gpt_summary_original":"The user is planning to create an Azure Machine Learning model to predict the number of earthquakes based on past historical data. They are unsure which regression technique to use and are considering options such as Logistic Regression, Poisson Regression, or Fast Forest Quantile Regression.",
        "Question_gpt_summary":"user plan creat model predict number earthquak base past histor data unsur regress techniqu us consid option logist regress poisson regress fast forest quantil regress",
        "Answer_original_content":"hello thank reach want predict event count poisson regress appropri futur refer refer document choos right algorithm predict analyt model hope help",
        "Answer_preprocessed_content":"hello thank reach want predict event count poisson regress appropri futur refer refer document choos right algorithm predict analyt model hope help",
        "Answer_gpt_summary_original":"possible solution: the answer suggests using poisson regression as an appropriate model for predicting the number of earthquakes based on past historical data. additionally, the answer recommends referring to a document for choosing the right algorithm for predictive analytics models.",
        "Answer_gpt_summary":"possibl solut answer suggest poisson regress appropri model predict number earthquak base past histor data addition answer recommend refer document choos right algorithm predict analyt model"
    },
    {
        "Question_id":63766714.0,
        "Question_title":"Run.get_context() gives the same run id",
        "Question_body":"<p>I am submitting the training through a script file. Following is the content of the <code>train.py<\/code> script. Azure ML is treating all these as one run (instead of run per alpha value as coded below) as <code>Run.get_context()<\/code> is returning the same Run id.<\/p>\n<p><strong>train.py<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.opendatasets import Diabetes\nfrom azureml.core import Run\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.externals import joblib\n\nimport math\nimport os\nimport logging\n\n# Load dataset\ndataset = Diabetes.get_tabular_dataset()\nprint(dataset.take(1))\n\ndf = dataset.to_pandas_dataframe()\ndf.describe()\n\n# Split X (independent variables) &amp; Y (target variable)\nx_df = df.dropna()      # Remove rows that have missing values\ny_df = x_df.pop(&quot;Y&quot;)    # Y is the label\/target variable\n\nx_train, x_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=66)\nprint('Original dataset size:', df.size)\nprint(&quot;Size after dropping 'na':&quot;, x_df.size)\nprint(&quot;Training split size: &quot;, x_train.size)\nprint(&quot;Test split size: &quot;, x_test.size)\n\n# Training\nalphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\n# Create and log interactive runs\n\noutput_dir = os.path.join(os.getcwd(), 'outputs')\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run = Run.get_context()\n    print(&quot;Started run: &quot;, run.id)\n    run.log(&quot;train_split_size&quot;, x_train.size)\n    run.log(&quot;test_split_size&quot;, x_train.size)\n    run.log(&quot;alpha_value&quot;, hyperparam_alpha)\n\n    # Train\n    print(&quot;Train ...&quot;)\n    model = Ridge(hyperparam_alpha)\n    model.fit(X = x_train, y = y_train)\n    \n    # Predict\n    print(&quot;Predict ...&quot;)\n    y_pred = model.predict(X = x_test)\n\n    # Calculate &amp; log error\n    rmse = math.sqrt(mean_squared_error(y_true = y_test, y_pred = y_pred))\n    run.log(&quot;rmse&quot;, rmse)\n    print(&quot;rmse&quot;, rmse)\n\n    # Serialize the model to local directory\n    if not os.path.isdir(output_dir):\n        os.makedirs(output_dir, exist_ok=True) \n\n    print(&quot;Save model ...&quot;)\n    model_name = &quot;model_alpha_&quot; + str(hyperparam_alpha) + &quot;.pkl&quot; # Pickle file\n    file_path = os.path.join(output_dir, model_name)\n    joblib.dump(value = model, filename = file_path)\n\n    # Upload the model\n    run.upload_file(name = model_name, path_or_stream = file_path)\n\n    # Complete the run\n    run.complete()\n<\/code><\/pre>\n<p><strong>Experiments view<\/strong>\n<a href=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6xAG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>Authoring code (i.e. control plane)<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nfrom azureml.core import Workspace, Experiment, RunConfiguration, ScriptRunConfig, VERSION, Run\n\nws = Workspace.from_config()\nexp = Experiment(workspace = ws, name = &quot;diabetes-local-script-file&quot;)\n\n# Create new run config obj\nrun_local_config = RunConfiguration()\n\n# This means that when we run locally, all dependencies are already provided.\nrun_local_config.environment.python.user_managed_dependencies = True\n\n# Create new script config\nscript_run_cfg = ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    run_config = run_local_config) \n\nrun = exp.submit(script_run_cfg)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1599411710610,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1599438192360,
        "Question_score":3.0,
        "Question_view_count":2523.0,
        "Answer_body":"<h2>Short Answer<\/h2>\n<h3>Option 1: create child runs within run<\/h3>\n<p><code>run = Run.get_context()<\/code> assigns the run object of the run that you're currently in to <code>run<\/code>. So in every iteration of the hyperparameter search, you're logging to the same run. To solve this, you need to create child (or sub-) runs for each hyperparameter value. You can do this with <code>run.child_run()<\/code>. Below is the template for making this happen.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>run = Run.get_context()\n\nfor hyperparam_alpha in alphas:\n    # Get the experiment run context\n    run_child = run.child_run()\n    print(&quot;Started run: &quot;, run_child.id)\n    run_child.log(&quot;train_split_size&quot;, x_train.size)\n<\/code><\/pre>\n<p>On the <code>diabetes-local-script-file<\/code> Experiment page, you can see that Run <code>9<\/code> was the parent run and Runs <code>10-19<\/code> were the child runs if you click &quot;Include child runs&quot; page. There is also a &quot;Child runs&quot; tab on Run 9 details page.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/5IMcz.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<h2>Long answer<\/h2>\n<p>I highly recommend abstracting the hyperparameter search away from the data plane (i.e. <code>train.py<\/code>) and into the control plane (i.e. &quot;authoring code&quot;). This becomes especially valuable as training time increases and you can arbitrarily parallelize and also choose Hyperparameters more intelligently by using Azure ML's <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\" rel=\"noreferrer\"><code>Hyperdrive<\/code><\/a>.<\/p>\n<h3>Option 2 Create runs from control plane<\/h3>\n<p>Remove the loop from your code, add the code like below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd\" rel=\"noreferrer\">full data and control here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import argparse\nfrom pprint import pprint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--alpha', type=float, default=0.5)\nargs = parser.parse_args()\nprint(&quot;all args:&quot;)\npprint(vars(args))\n\n# use the variable like this\nmodel = Ridge(args.alpha)\n<\/code><\/pre>\n<p>below is how to submit a single run using a script argument. To submit multiple runs, just use a loop in the control plane.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>alphas = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] # Define hyperparameters\n\nlist_rcs = [ScriptRunConfig(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    script = 'train.py',\n    arguments=['--alpha',a],\n    run_config = run_local_config) for a in alphas]\n\nlist_runs = [exp.submit(rc) for rc in list_rcs]\n\n<\/code><\/pre>\n<h3>Option 3 Hyperdrive (IMHO the recommended approach)<\/h3>\n<p>In this way you outsource the hyperparameter source to <code>Hyperdrive<\/code>. The UI will also report results exactly how you want them, and via the API you can easily download the best model.  Note you can't use this locally anymore and must use <code>AMLCompute<\/code>, but to me it is a worthwhile trade-off.<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#configure-experiment\" rel=\"noreferrer\">This is a great overview<\/a>. Excerpt below (<a href=\"https:\/\/gist.github.com\/swanderz\/f5c0dc1aefc796d37f6bc3600f2ae3cd#file-hyperdrive-ipynb\" rel=\"noreferrer\">full code here<\/a>)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>param_sampling = GridParameterSampling( {\n        &quot;alpha&quot;: choice(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n    }\n)\n\nestimator = Estimator(\n    source_directory =  os.path.join(os.getcwd(), 'code'),\n    entry_script = 'train.py',\n    compute_target=cpu_cluster,\n    environment_definition=Environment.get(workspace=ws, name=&quot;AzureML-Tutorial&quot;)\n)\n\nhyperdrive_run_config = HyperDriveConfig(estimator=estimator,\n                          hyperparameter_sampling=param_sampling, \n                          policy=None,\n                          primary_metric_name=&quot;rmse&quot;, \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n                          max_total_runs=10,\n                          max_concurrent_runs=4)\n\nrun = exp.submit(hyperdrive_run_config)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q1AhJ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63766714",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1599436122230,
        "Question_original_content":"run context give run submit train script file follow content train script treat run instead run alpha valu code run context return run train opendataset import diabet core import run sklearn model select import train test split sklearn linear model import ridg sklearn metric import mean squar error sklearn extern import joblib import math import import log load dataset dataset diabet tabular dataset print dataset dataset panda datafram split independ variabl target variabl dropna remov row miss valu pop label target variabl train test train test train test split test size random state print origin dataset size size print size drop size print train split size train size print test split size test size train alpha defin hyperparamet creat log interact run output dir path join getcwd output hyperparam alpha alpha experi run context run run context print start run run run log train split size train size run log test split size train size run log alpha valu hyperparam alpha train print train model ridg hyperparam alpha model fit train train predict print predict pred model predict test calcul log error rmse math sqrt mean squar error true test pred pred run log rmse rmse print rmse rmse serial model local directori path isdir output dir makedir output dir exist true print save model model model alpha str hyperparam alpha pkl pickl file file path path join output dir model joblib dump valu model filenam file path upload model run upload file model path stream file path complet run run complet experi view author code control plane import core import workspac experi runconfigur scriptrunconfig version run workspac config exp experi workspac diabet local script file creat new run config obj run local config runconfigur mean run local depend provid run local config environ python user manag depend true creat new script config script run cfg scriptrunconfig sourc directori path join getcwd code script train run config run local config run exp submit script run cfg run wait complet output true",
        "Question_preprocessed_content":"give run submit train script file follow content script treat run return run experi view author code",
        "Question_gpt_summary_original":"The user is encountering a challenge where Azure ML is treating all runs as one run instead of one run per alpha value as coded, as Run.get_context() is returning the same Run id.",
        "Question_gpt_summary":"user encount challeng treat run run instead run alpha valu code run context return run",
        "Answer_original_content":"short answer option creat child run run run run context assign run object run current run iter hyperparamet search log run solv need creat child sub run hyperparamet valu run child run templat make happen run run context hyperparam alpha alpha experi run context run child run child run print start run run child run child log train split size train size diabet local script file experi page run parent run run child run click includ child run page child run tab run detail page long answer highli recommend abstract hyperparamet search awai data plane train control plane author code especi valuabl train time increas arbitrarili parallel choos hyperparamet intellig hyperdr option creat run control plane remov loop code add code like data control import argpars pprint import pprint parser argpars argumentpars parser add argument alpha type float default arg parser pars arg print arg pprint var arg us variabl like model ridg arg alpha submit singl run script argument submit multipl run us loop control plane alpha defin hyperparamet list rc scriptrunconfig sourc directori path join getcwd code script train argument alpha run config run local config alpha list run exp submit list rc option hyperdr imho recommend approach wai outsourc hyperparamet sourc hyperdr report result exactli want api easili download best model note us local anymor us amlcomput worthwhil trade great overview excerpt code param sampl gridparametersampl alpha choic estim estim sourc directori path join getcwd code entri script train comput target cpu cluster environ definit environ workspac tutori hyperdr run config hyperdriveconfig estim estim hyperparamet sampl param sampl polici primari metric rmse primari metric goal primarymetricgo maxim max total run max concurr run run exp submit hyperdr run config run wait complet output true",
        "Answer_preprocessed_content":"short answer option creat child run run assign run object run current iter hyperparamet search log run solv need creat child run hyperparamet valu templat make happen experi page run parent run run child run click includ child run page child run tab run detail page long answer highli recommend abstract hyperparamet search awai data plane control plane especi valuabl train time increas arbitrarili parallel choos hyperparamet intellig option creat run control plane remov loop code add code like submit singl run script argument submit multipl run us loop control plane option hyperdr wai outsourc hyperparamet sourc report result exactli want api easili download best model note us local anymor us worthwhil great overview excerpt",
        "Answer_gpt_summary_original":"the answer provides three possible solutions to the challenge of run.get_context() returning the same run id. the first solution is to create child runs within the run using run.child_run(). the second solution is to create runs from the control plane. the third solution is to use hyperdrive, which outsources the hyperparameter source to hyperdrive and reports results via the ui.",
        "Answer_gpt_summary":"answer provid possibl solut challeng run context return run solut creat child run run run child run second solut creat run control plane solut us hyperdr outsourc hyperparamet sourc hyperdr report result"
    },
    {
        "Question_id":69935219.0,
        "Question_title":"How to search a set of normally distributed parameters using optuna?",
        "Question_body":"<p>I'm trying to optimize a custom model (no fancy ML whatsoever) that has 13 parameters, 12 of which I know to be normally distributed. I've gotten decent results using the <code>hyperopt<\/code> library:<\/p>\n<pre><code>space = {\n    'B1': hp.normal('B1', B1['mean'], B1['std']),\n    'B2': hp.normal('B2', B2['mean'], B2['std']),\n    'C1': hp.normal('C1', C1['mean'], C1['std']),\n    'C2': hp.normal('C2', C2['mean'], C2['std']),\n    'D1': hp.normal('D1', D1['mean'], D1['std']),\n    'D2': hp.normal('D2', D2['mean'], D2['std']),\n    'E1': hp.normal('E1', E1['mean'], E1['std']),\n    'E2': hp.normal('E2', E2['mean'], E2['std']),\n    'F1': hp.normal('F1', F1['mean'], F1['std']),\n    'F2': hp.normal('F2', F2['mean'], F2['std'])\n}\n<\/code><\/pre>\n<p>where I can specify the shape of the search space per parameter to be normally distributed.<\/p>\n<p>I've got 32 cores and the default <code>Trials()<\/code> object only uses one of them.  <code>Hyperopt<\/code> suggests two ways to parallelize the search process, both of which I could not get to work on my windows machine for the life of me, so I've given up and want to try a different framework.<\/p>\n<p>Even though Bayesian Hyper Parameter Optimization as far as I know is based on the idea that values are distributed according to a distribution, and the normal distribution is so prevalent that it is literally called normal. I cannot find a way to specify to <code>Optuna<\/code> that my parameters have a <code>mean<\/code> and a <code>standard deviation<\/code>.<\/p>\n<p>How do i tell <code>Optuna<\/code> the <code>mean<\/code> and <code>standard deviation<\/code> of my parameters?<\/p>\n<p>The only distributions I can find in the documentation are: <code>suggest_uniform()<\/code>, <code>suggest_loguniform()<\/code> and <code>suggest_discrete_uniform()<\/code>.<\/p>\n<p>Please tell me if I am somehow misunderstanding loguniform distrubution (It looks somewhat similar, but I can't specify a standard deviation?) or the pruning process.<\/p>\n<p>As you might be able to tell from my text, I've spent a frustrating amount of time trying to figure this out and gotten exactly nowhere, any help will be highly appreciated!<\/p>\n<p>Special thanks to dankal444 for this elegant solution (i will replace the mean and std with my own values):<\/p>\n<pre><code>from scipy.special import erfinv\nspace = {\n    'B1': (erfinv(trial.suggest_float('B1', -1, 1))-mean)*std,\n    'B2': ...\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1636666668057,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1636679968800,
        "Question_score":0.0,
        "Question_view_count":248.0,
        "Answer_body":"<p>You can cheat <code>optuna<\/code> by using uniform distribution and transforming it into normal distribution. To do that one of the method is <a href=\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.erfinv.html\" rel=\"nofollow noreferrer\">inversed error function<\/a> implemented in <code>scipy<\/code>.<\/p>\n<p>Function takes uniform distribution from in range &lt;-1, 1&gt; and converts it to standard normal distribution<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import special\n\n\nx = np.linspace(-1, 1)\nplt.plot(x, special.erfinv(x))\nplt.xlabel('$x$')\nplt.ylabel('$erf(x)$')\n\nmean = 2\nstd = 3\nrandom_uniform_data = np.random.uniform(-1 + 0.00001, 1-0.00001, 1000)\nrandom_gaussianized_data = (special.erfinv(random_uniform_data) - mean) * std\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\naxes[0].hist(random_uniform_data, 30)\naxes[1].hist(random_gaussianized_data, 30)\naxes[0].set_title('uniform distribution samples')\naxes[1].set_title('erfinv(uniform distribution samples)')\nplt.show()\n<\/code><\/pre>\n<p>This is how the function looks like:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/E363H.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E363H.png\" alt=\"inverse error function\" \/><\/a><\/p>\n<p>And below example of transforming of uniform distribution into normal with custom mean and standard deviation (see code above)<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rspUX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rspUX.png\" alt=\"transforming uniform to normal distribution\" \/><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69935219",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1636670797220,
        "Question_original_content":"search set normal distribut paramet try optim custom model fanci whatsoev paramet know normal distribut gotten decent result hyperopt librari space normal mean std normal mean std normal mean std normal mean std normal mean std normal mean std normal mean std normal mean std normal mean std normal mean std specifi shape search space paramet normal distribut got core default trial object us hyperopt suggest wai parallel search process work window machin life given want try differ framework bayesian hyper paramet optim far know base idea valu distribut accord distribut normal distribut preval liter call normal wai specifi paramet mean standard deviat tell mean standard deviat paramet distribut document suggest uniform suggest loguniform suggest discret uniform tell misunderstand loguniform distrubut look somewhat similar specifi standard deviat prune process abl tell text spent frustrat time try figur gotten exactli help highli appreci special thank dankal eleg solut replac mean std valu scipi special import erfinv space erfinv trial suggest float mean std",
        "Question_preprocessed_content":"search set normal distribut paramet try optim custom model paramet know normal distribut gotten decent result librari specifi shape search space paramet normal distribut got core default object us suggest wai parallel search process work window machin life given want try differ framework bayesian hyper paramet optim far know base idea valu distribut accord distribut normal distribut preval liter call normal wai specifi paramet tell paramet distribut document tell misunderstand loguniform distrubut prune process abl tell text spent frustrat time try figur gotten exactli help highli appreci special thank dankal eleg solut",
        "Question_gpt_summary_original":"The user is trying to optimize a custom model with 13 parameters, 12 of which are normally distributed. They have tried using the hyperopt library but could not get the parallelization to work on their Windows machine. They want to try a different framework, but cannot find a way to specify the mean and standard deviation of their parameters in Optuna. The only distributions available in Optuna are suggest_uniform(), suggest_loguniform(), and suggest_discrete_uniform(). The user is seeking help to figure out how to specify the mean and standard deviation of their parameters in Optuna.",
        "Question_gpt_summary":"user try optim custom model paramet normal distribut tri hyperopt librari parallel work window machin want try differ framework wai specifi mean standard deviat paramet distribut avail suggest uniform suggest loguniform suggest discret uniform user seek help figur specifi mean standard deviat paramet",
        "Answer_original_content":"cheat uniform distribut transform normal distribut method invers error function implement scipi function take uniform distribut rang convert standard normal distribut import matplotlib pyplot plt import numpi scipi import special linspac plt plot special erfinv plt xlabel plt ylabel erf mean std random uniform data random uniform random gaussian data special erfinv random uniform data mean std fig ax plt subplot figsiz ax hist random uniform data ax hist random gaussian data ax set titl uniform distribut sampl ax set titl erfinv uniform distribut sampl plt function look like exampl transform uniform distribut normal custom mean standard deviat code",
        "Answer_preprocessed_content":"cheat uniform distribut transform normal distribut method invers error function implement function take uniform distribut rang convert standard normal distribut function look like exampl transform uniform distribut normal custom mean standard deviat",
        "Answer_gpt_summary_original":"The answer suggests using a uniform distribution and transforming it into a normal distribution using the inversed error function implemented in scipy. The code provided shows an example of how to transform a uniform distribution into a normal distribution with custom mean and standard deviation.",
        "Answer_gpt_summary":"answer suggest uniform distribut transform normal distribut invers error function implement scipi code provid show exampl transform uniform distribut normal distribut custom mean standard deviat"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML: Upload File to Step Run's Output - Authentication Error",
        "Question_body":"During a PythonScriptStep in an Azure ML Pipeline, I'm saving a model as joblib pickle dump to a directory in a Blob Container in the Azure Blob Storage which I've created during the setup of the Azure ML Workspace. Afterwards I'm trying to upload this model file to the step run's output directory using\n\nRun.upload_file (name, path_or_stream)\n\n\n\n(for the function's documentation, see https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#upload-file-name--path-or-stream--datastore-name-none-)\n\nSome time ago when I created the script using the azureml-sdk version 1.18.0, everything worked fine. Now, I've updated the script's functionalities and upgraded the azureml-sdk to version 1.33.0 during the process and the upload function now runs into the following error:\n\nTraceback (most recent call last):\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/upload.py\", line 64, in upload_blob_from_stream\n    validate_content=True)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py\", line 93, in execute_func_with_reset\n    return ClientBase._execute_func_internal(backoff, retries, module_logger, func, reset_func, *args, **kwargs)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py\", line 367, in _execute_func_internal\n    left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py\", line 399, in _handle_retry\n    raise error\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py\", line 358, in _execute_func_internal\n    response = func(*args, **kwargs)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 614, in create_blob_from_stream\n    initialization_vector=iv\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py\", line 98, in _upload_blob_chunks\n    range_ids = [f.result() for f in futures]\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py\", line 98, in <listcomp>\n    range_ids = [f.result() for f in futures]\n  File \"\/opt\/miniconda\/lib\/python3.7\/concurrent\/futures\/_base.py\", line 435, in result\n    return self.__get_result()\n  File \"\/opt\/miniconda\/lib\/python3.7\/concurrent\/futures\/_base.py\", line 384, in __get_result\n    raise self._exception\n  File \"\/opt\/miniconda\/lib\/python3.7\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py\", line 210, in process_chunk\n    return self._upload_chunk_with_progress(chunk_offset, chunk_bytes)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py\", line 224, in _upload_chunk_with_progress\n    range_id = self._upload_chunk(chunk_offset, chunk_data)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py\", line 269, in _upload_chunk\n    timeout=self.timeout,\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py\", line 1013, in _put_block\n    self._perform_request(request)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 432, in _perform_request\n    raise ex\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 357, in _perform_request\n    raise ex\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py\", line 343, in _perform_request\n    HTTPError(response.status, response.message, response.headers, response.body))\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/common\/_error.py\", line 115, in _http_error_handler\n    raise ex\nazure.common.AzureHttpError: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. ErrorCode: AuthenticationFailed\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthenticationFailed<\/Code><Message>Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\nRequestId:5d4e1b7e-c01e-0070-0d47-9bf8a0000000\nTime:2021-08-27T13:30:02.2685991Z<\/Message><AuthenticationErrorDetail>Signature did not match. String to sign used was rcw\n2021-08-27T13:19:56Z\n2021-08-28T13:29:56Z\n\/blob\/mystorage\/azureml\/ExperimentRun\/dcid.98d11a7b-2aac-4bc0-bd64-bb4d72e0e0be\/outputs\/models\/Model.pkl\n2019-07-07\nb\n<\/AuthenticationErrorDetail><\/Error>\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/...\/azureml-setup\/context_manager_injector.py\", line 243, in execute_with_context\n    runpy.run_path(sys.argv[0], globals(), run_name=\"__main__\")\n  File \"\/opt\/miniconda\/lib\/python3.7\/runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"\/opt\/miniconda\/lib\/python3.7\/runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"\/opt\/miniconda\/lib\/python3.7\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py\", line 318, in <module>\n    main()\n  File \"401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py\", line 286, in main\n    path_or_stream=model_path)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/core\/run.py\", line 53, in wrapped\n    return func(self, *args, **kwargs)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/core\/run.py\", line 1989, in upload_file\n    datastore_name=datastore_name)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py\", line 114, in upload_artifact\n    return self.upload_artifact_from_path(artifact, *args, **kwargs)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py\", line 107, in upload_artifact_from_path\n    return self.upload_artifact_from_stream(stream, *args, **kwargs)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py\", line 99, in upload_artifact_from_stream\n    content_type=content_type, session=session)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py\", line 88, in upload_stream_to_existing_artifact\n    timeout=TIMEOUT, backoff=BACKOFF_START, retries=RETRY_LIMIT)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/upload.py\", line 71, in upload_blob_from_stream\n    raise AzureMLException._with_error(azureml_error, inner_exception=e)\nazureml._common.exceptions.AzureMLException: AzureMLException:\n    Message: Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\n    StorageAccount: mystorage\n    ContainerName: azureml\n    StatusCode: 403\n    InnerException Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. ErrorCode: AuthenticationFailed\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthenticationFailed<\/Code><Message>Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\nRequestId:5d4e1b7e-c01e-0070-0d47-9bf8a0000000\nTime:2021-08-27T13:30:02.2685991Z<\/Message><AuthenticationErrorDetail>Signature did not match. String to sign used was rcw\n2021-08-27T13:19:56Z\n2021-08-28T13:29:56Z\n\/blob\/mystorage\/azureml\/ExperimentRun\/dcid.98d11a7b-2aac-4bc0-bd64-bb4d72e0e0be\/outputs\/models\/Model.pkl\n2019-07-07\nb\n<\/AuthenticationErrorDetail><\/Error>\n    ErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\\n\\tStorageAccount: mystorage\\n\\tContainerName: azureml\\n\\tStatusCode: 403\",\n        \"inner_error\": {\n            \"code\": \"Auth\",\n            \"inner_error\": {\n                \"code\": \"Authorization\"\n            }\n        }\n    }\n}\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py\", line 318, in <module>\n    main()\n  File \"401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py\", line 286, in main\n    path_or_stream=model_path)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/core\/run.py\", line 53, in wrapped\n    return func(self, *args, **kwargs)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/core\/run.py\", line 1989, in upload_file\n    datastore_name=datastore_name)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py\", line 114, in upload_artifact\n    return self.upload_artifact_from_path(artifact, *args, **kwargs)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py\", line 107, in upload_artifact_from_path\n    return self.upload_artifact_from_stream(stream, *args, **kwargs)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py\", line 99, in upload_artifact_from_stream\n    content_type=content_type, session=session)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py\", line 88, in upload_stream_to_existing_artifact\n    timeout=TIMEOUT, backoff=BACKOFF_START, retries=RETRY_LIMIT)\n  File \"\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/upload.py\", line 71, in upload_blob_from_stream\n    raise AzureMLException._with_error(azureml_error, inner_exception=e)\nUserScriptException: UserScriptException:\n    Message: Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\n    StorageAccount: mystorage\n    ContainerName: azureml\n    StatusCode: 403\n    InnerException AzureMLException:\n    Message: Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\n    StorageAccount: mystorage\n    ContainerName: azureml\n    StatusCode: 403\n    InnerException Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. ErrorCode: AuthenticationFailed\n<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>AuthenticationFailed<\/Code><Message>Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\nRequestId:5d4e1b7e-c01e-0070-0d47-9bf8a0000000\nTime:2021-08-27T13:30:02.2685991Z<\/Message><AuthenticationErrorDetail>Signature did not match. String to sign used was rcw\n2021-08-27T13:19:56Z\n2021-08-28T13:29:56Z\n\/blob\/mystorage\/azureml\/ExperimentRun\/dcid.98d11a7b-2aac-4bc0-bd64-bb4d72e0e0be\/outputs\/models\/Model.pkl\n2019-07-07\nb\n<\/AuthenticationErrorDetail><\/Error>\n    ErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\\n\\tStorageAccount: verovisionstorage\\n\\tContainerName: azureml\\n\\tStatusCode: 403\",\n        \"inner_error\": {\n            \"code\": \"Auth\",\n            \"inner_error\": {\n                \"code\": \"Authorization\"\n            }\n        }\n    }\n}\n    ErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\\n\\tStorageAccount: mystorage\\n\\tContainerName: azureml\\n\\tStatusCode: 403\"\n    }\n}\n\n\n\n\nAs far as I can tell from the code of the azureml.core.Run class and the subsequent function calls, the Run object tries to upload the file to the step run's output directory using SAS-Token-Authentication (which fails). This documentation article is linked in the code (but I don't know if this relates to the issue): https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/create-service-sas#service-sas-example\n\nDid anybody encounter this error as well and knows what causes it or how it can be resolved?\n\nBest,\nJonas",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1630074811383,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/530817\/azure-ml-upload-file-to-step-run39s-output-authent.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-31T16:24:16.313Z",
                "Answer_score":0,
                "Answer_body":"@Jonas-4379 Thanks for the details. This is a bug with the azureml-sdk-version V1.33 and will update once this is fixed.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"upload file step run output authent error pythonscriptstep pipelin save model joblib pickl dump directori blob contain azur blob storag creat setup workspac try upload model file step run output directori run upload file path stream function document http doc microsoft com python api core core run class view azur upload file path stream datastor time ago creat script sdk version work fine updat script function upgrad sdk version process upload function run follow error traceback recent file opt miniconda lib python site packag file util upload line upload blob stream valid content true file opt miniconda lib python site packag restclient clientbas line execut func reset return clientbas execut func intern backoff retri modul logger func reset func arg kwarg file opt miniconda lib python site packag restclient clientbas line execut func intern left retri cl handl retri left retri total retri error logger func file opt miniconda lib python site packag restclient clientbas line handl retri rais error file opt miniconda lib python site packag restclient clientbas line execut func intern respons func arg kwarg file opt miniconda lib python site packag vendor azur storag blob blockblobservic line creat blob stream initi vector file opt miniconda lib python site packag vendor azur storag blob upload chunk line upload blob chunk rang id result futur file opt miniconda lib python site packag vendor azur storag blob upload chunk line rang id result futur file opt miniconda lib python concurr futur base line result return self result file opt miniconda lib python concurr futur base line result rais self except file opt miniconda lib python concurr futur thread line run result self self arg self kwarg file opt miniconda lib python site packag vendor azur storag blob upload chunk line process chunk return self upload chunk progress chunk offset chunk byte file opt miniconda lib python site packag vendor azur storag blob upload chunk line upload chunk progress rang self upload chunk chunk offset chunk data file opt miniconda lib python site packag vendor azur storag blob upload chunk line upload chunk timeout self timeout file opt miniconda lib python site packag vendor azur storag blob blockblobservic line block self perform request request file opt miniconda lib python site packag vendor azur storag common storagecli line perform request rais file opt miniconda lib python site packag vendor azur storag common storagecli line perform request rais file opt miniconda lib python site packag vendor azur storag common storagecli line perform request httperror respons statu respons messag respons header respons bodi file opt miniconda lib python site packag vendor azur storag common error line http error handler rais azur common azurehttperror server fail authent request sure valu author header form correctli includ signatur errorcod authenticationfail authenticationfailedserv fail authent request sure valu author header form correctli includ signatur requestid debe bfa time zsignatur match string sign rcw blob mystorag experimentrun dcid dab aac bbdeeb output model model pkl handl except except occur traceback recent file mnt batch task share root job setup context manag injector line execut context runpi run path sy argv global run main file opt miniconda lib python runpi line run path pkg pkg script fname file opt miniconda lib python runpi line run modul code mod mod spec pkg script file opt miniconda lib python runpi line run code exec code run global file aml pipelin time seri model train azur cpu line main file aml pipelin time seri model train azur cpu line main path stream model path file opt miniconda lib python site packag core run line wrap return func self arg kwarg file opt miniconda lib python site packag core run line upload file datastor datastor file opt miniconda lib python site packag restclient artifact client line upload artifact return self upload artifact path artifact arg kwarg file opt miniconda lib python site packag restclient artifact client line upload artifact path return self upload artifact stream stream arg kwarg file opt miniconda lib python site packag restclient artifact client line upload artifact stream content type content type session session file opt miniconda lib python site packag restclient artifact client line upload stream exist artifact timeout timeout backoff backoff start retri retri limit file opt miniconda lib python site packag file util upload line upload blob stream rais except error error inner except common except except except messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set storageaccount mystorag containernam statuscod innerexcept server fail authent request sure valu author header form correctli includ signatur errorcod authenticationfail authenticationfailedserv fail authent request sure valu author header form correctli includ signatur requestid debe bfa time zsignatur match string sign rcw blob mystorag experimentrun dcid dab aac bbdeeb output model model pkl errorrespons error code usererror messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set tstorageaccount mystorag tcontainernam tstatuscod inner error code auth inner error code author handl except except occur traceback recent file aml pipelin time seri model train azur cpu line main file aml pipelin time seri model train azur cpu line main path stream model path file opt miniconda lib python site packag core run line wrap return func self arg kwarg file opt miniconda lib python site packag core run line upload file datastor datastor file opt miniconda lib python site packag restclient artifact client line upload artifact return self upload artifact path artifact arg kwarg file opt miniconda lib python site packag restclient artifact client line upload artifact path return self upload artifact stream stream arg kwarg file opt miniconda lib python site packag restclient artifact client line upload artifact stream content type content type session session file opt miniconda lib python site packag restclient artifact client line upload stream exist artifact timeout timeout backoff backoff start retri retri limit file opt miniconda lib python site packag file util upload line upload blob stream rais except error error inner except userscriptexcept userscriptexcept messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set storageaccount mystorag containernam statuscod innerexcept except messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set storageaccount mystorag containernam statuscod innerexcept server fail authent request sure valu author header form correctli includ signatur errorcod authenticationfail authenticationfailedserv fail authent request sure valu author header form correctli includ signatur requestid debe bfa time zsignatur match string sign rcw blob mystorag experimentrun dcid dab aac bbdeeb output model model pkl errorrespons error code usererror messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set tstorageaccount verovisionstorag tcontainernam tstatuscod inner error code auth inner error code author errorrespons error code usererror messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set tstorageaccount mystorag tcontainernam tstatuscod far tell code core run class subsequ function call run object tri upload file step run output directori sa token authent fail document articl link code know relat issu http doc microsoft com rest api storageservic creat servic sa servic sa exampl anybodi encount error know caus resolv best jona",
        "Question_preprocessed_content":"upload file step run output authent error pythonscriptstep pipelin save model joblib pickl dump directori blob contain azur blob storag creat setup workspac try upload model file step run output directori function document time ago creat script sdk version work fine updat script function upgrad sdk version process upload function run follow error traceback file line file line return retri func arg kwarg file line error logger func file line rais error file line respons func file line file line file line file line result return file line rais file line run result file line return file line file line file line file line rais file line rais file line file line rais server fail authent request sure valu author header form correctli includ signatur errorcod authenticationfail server fail authent request sure valu author header form correctli includ signatur match string sign rcw handl except except occur traceback file line global file line file line file line exec file line main file line main file line wrap return func file line file line return arg kwarg file line return arg kwarg file line session session file line timeout timeout file line rais except messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set storageaccount mystorag containernam statuscod innerexcept server fail authent request sure valu author header form correctli includ signatur errorcod authenticationfail server fail authent request sure valu author header form correctli includ signatur match string sign rcw errorrespons error handl except except occur traceback file line main file line main file line wrap return func file line file line return arg kwarg file line return arg kwarg file line session session file line timeout timeout file line rais userscriptexcept userscriptexcept messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set storageaccount mystorag containernam statuscod innerexcept except messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set storageaccount mystorag containernam statuscod innerexcept server fail authent request sure valu author header form correctli includ signatur errorcod authenticationfail server fail authent request sure valu author header form correctli includ signatur match string sign rcw errorrespons error errorrespons error far tell code class subsequ function call run object tri upload file step run output directori document articl link code anybodi encount error know caus resolv best jona",
        "Question_gpt_summary_original":"The user is encountering an authentication error while trying to upload a model file to the step run's output directory using the Run.upload_file() function in an Azure ML Pipeline. The error message suggests that the server failed to authenticate the request, and the value of the Authorization header is not formed correctly, including the signature. The user has updated the script's functionalities and upgraded the azureml-sdk to version 1.33.0, which may have caused the issue. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount authent error try upload model file step run output directori run upload file function pipelin error messag suggest server fail authent request valu author header form correctli includ signatur user updat script function upgrad sdk version caus issu user seek help resolv issu",
        "Answer_original_content":"jona thank detail bug sdk version updat fix",
        "Answer_preprocessed_content":"thank detail bug updat fix",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the response acknowledges the issue and states that it is a bug with the sdk version being used, and that an update will be provided once the bug is fixed.",
        "Answer_gpt_summary":"solut provid answer respons acknowledg issu state bug sdk version updat provid bug fix"
    },
    {
        "Question_id":null,
        "Question_title":"If I'm logging metrics for MLOps from a test pipeline, how do I create a separate api key for that?",
        "Question_body":"<p>How do I create a separate API so that I can log metrics from test pipelines? It doesn\u2019t make sense to use a personal key for that.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1675375672133,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":47.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adgudime\">@adgudime<\/a>, thanks for your question! You can use a <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/general#what-is-a-service-account-and-why-is-it-useful\">service account<\/a> for this purpose,  could you please check if this would work for you? Thanks!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/if-im-logging-metrics-for-mlops-from-a-test-pipeline-how-do-i-create-a-separate-api-key-for-that\/3803",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-06T12:37:26.565Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adgudime\">@adgudime<\/a>, thanks for your question! You can use a <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/general#what-is-a-service-account-and-why-is-it-useful\">service account<\/a> for this purpose,  could you please check if this would work for you? Thanks!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2023-02-07T01:52:50.222Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/luis_bergua1\">@luis_bergua1<\/a> this would work perfectly. Thank you!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-07T08:40:29.901Z",
                "Answer_body":"<p>Great to hear it <a class=\"mention\" href=\"\/u\/adgudime\">@adgudime<\/a>! I\u2019ll then go ahead and close this ticket for now but please feel free to re-open it if you have any other question related. Thanks!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1675687046564,
        "Question_original_content":"log metric mlop test pipelin creat separ api kei creat separ api log metric test pipelin doesnt sens us person kei",
        "Question_preprocessed_content":"log metric mlop test pipelin creat separ api kei creat separ api log metric test pipelin doesnt sens us person kei",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a separate API key for logging metrics from test pipelines, as using a personal key for this purpose is not feasible.",
        "Question_gpt_summary":"user face challeng creat separ api kei log metric test pipelin person kei purpos feasibl",
        "Answer_original_content":"adgudim thank question us servic account purpos check work thank",
        "Answer_preprocessed_content":"thank question us servic account purpos check work thank",
        "Answer_gpt_summary_original":"possible solution: the user can create a service account to log metrics from test pipelines.",
        "Answer_gpt_summary":"possibl solut user creat servic account log metric test pipelin"
    },
    {
        "Question_id":58807540.0,
        "Question_title":"How to convert Spark data frame to Pandas and back in Kedro?",
        "Question_body":"<p>I'm trying to understand what is the optimal way in Kedro to convert Spark dataframe coming out of one node into Pandas required as input for another node without creating a redundant conversion step.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573500781437,
        "Question_favorite_count":null,
        "Question_last_edit_time":1573553002123,
        "Question_score":3.0,
        "Question_view_count":800.0,
        "Answer_body":"<p>Kedro currently supports 2 strategies for that:<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/04_data_catalog.html#transcoding-datasets\" rel=\"nofollow noreferrer\">Transcoding<\/a> feature<\/h3>\n\n<p>This requires one to define two <code>DataCatalog<\/code> entries for the same dataset, working with the same file in a common format (Parquet, JSON, CSV, etc.), in your <code>catalog.yml<\/code>:<\/p>\n\n<pre><code>my_dataframe@spark:\n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: data\/02_intermediate\/data.parquet\n\nmy_dataframe@pandas:\n  type: ParquetLocalDataSet\n  filepath: data\/02_intermediate\/data.parquet\n<\/code><\/pre>\n\n<p>And then use them in the pipeline like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func1, \"spark_input\", \"my_dataframe@spark\"),\n    node(my_func2, \"my_dataframe@pandas\", \"output\"),\n])\n<\/code><\/pre>\n\n<p>In this case, <code>kedro<\/code> understands that <code>my_dataframe<\/code> is the same dataset in both cases and resolves the node execution order properly. At the same time, <code>kedro<\/code> would use the <code>SparkDataSet<\/code> implementation for saving and <code>ParquetLocalDataSet<\/code> for loading, so the first node should output <code>pyspark.sql.DataFrame<\/code>, while the second node would receive a <code>pandas.Dataframe<\/code>.<\/p>\n\n<h3>Using <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.pandas_to_spark.html\" rel=\"nofollow noreferrer\">Pandas to Spark<\/a> and <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.contrib.decorators.spark_to_pandas.html\" rel=\"nofollow noreferrer\">Spark to Pandas<\/a> node decorators<\/h3>\n\n<p><strong>Note:<\/strong> <code>Spark &lt;-&gt; Pandas<\/code> in-memory conversion is <a href=\"https:\/\/stackoverflow.com\/a\/47536675\/3364156\">notorious<\/a> for its memory demands, so this is a viable option only if the dataframe is known to be small.<\/p>\n\n<p>One can decorate the node as per the docs:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from spark import get_spark\nfrom kedro.contrib.decorators import pandas_to_spark\n\n@pandas_to_spark(spark_session)\ndef my_func3(data):\n    data.show() # data is pyspark.sql.DataFrame\n<\/code><\/pre>\n\n<p>Or even the whole pipeline:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Pipeline([\n    node(my_func4, \"pandas_input\", \"some_output\"),\n    ...\n]).decorate(pandas_to_spark)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1573501243163,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58807540",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573500781436,
        "Question_original_content":"convert spark data frame panda try understand optim wai convert spark datafram come node panda requir input node creat redund convers step",
        "Question_preprocessed_content":"convert spark data frame panda try understand optim wai convert spark datafram come node panda requir input node creat redund convers step",
        "Question_gpt_summary_original":"The user is facing a challenge in converting a Spark data frame to Pandas and back in Kedro without creating a redundant conversion step.",
        "Question_gpt_summary":"user face challeng convert spark data frame panda creat redund convers step",
        "Answer_original_content":"current support strategi transcod featur requir defin datacatalog entri dataset work file common format parquet json csv catalog yml datafram spark type contrib pyspark sparkdataset filepath data intermedi data parquet datafram panda type parquetlocaldataset filepath data intermedi data parquet us pipelin like pipelin node func spark input datafram spark node func datafram panda output case understand datafram dataset case resolv node execut order properli time us sparkdataset implement save parquetlocaldataset load node output pyspark sql datafram second node receiv panda datafram panda spark spark panda node decor note spark panda memori convers notori memori demand viabl option datafram known small decor node doc spark import spark contrib decor import panda spark panda spark spark session def func data data data pyspark sql datafram pipelin pipelin node func panda input output decor panda spark",
        "Answer_preprocessed_content":"current support strategi transcod featur requir defin entri dataset work file common format us pipelin like case understand dataset case resolv node execut order properli time us implement save load node output second node receiv panda spark spark panda node decor note convers notori memori demand viabl option datafram known small decor node doc pipelin",
        "Answer_gpt_summary_original":"the answer suggests two strategies for converting a spark dataframe into a pandas dataframe without creating a redundant conversion step. the first strategy involves using the transcoding feature and defining two datacatalog entries for the same dataset. the second strategy involves using pandas to spark and spark to pandas node decorators. however, the answer notes that the second strategy is only viable if the dataframe is small due to the memory demands of in-memory conversion.",
        "Answer_gpt_summary":"answer suggest strategi convert spark datafram panda datafram creat redund convers step strategi involv transcod featur defin datacatalog entri dataset second strategi involv panda spark spark panda node decor answer note second strategi viabl datafram small memori demand memori convers"
    },
    {
        "Question_id":null,
        "Question_title":"MLlofw artifacts to differnt environments",
        "Question_body":"Hi team,,\n\n\u00a0 I have three subscription how does mlflow communicate between dev ,stagging and prod envs. please let me know what the process to move mlflow artifacts to different environments with three separate azure data bricks subscriptions\n\n\u00a0\n\n\u00a0\n\nRegards,\n\nNaveen",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1648020576000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":14.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/-AQJqwq1l1c",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-30T01:05:26",
                "Answer_body":"Within the context of MLflow, you can register your model and then transition it between staging, production, and archived within a single environment.\u00a0 \u00a0You can share models across workspaces and\/or access the MLflow tracking server from outside Azure Databricks per different subscriptions.\n\n\nHTH!\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/1f444e5e-7aa7-4a0e-8917-eeea8166f0ebn%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2022-03-30T01:29:43",
                "Answer_body":"Thank you Denny Lee\n\ue5d3"
            },
            {
                "Answer_creation_time":"2022-03-30T01:36:09",
                "Answer_body":"Glad to help!\u00a0 Hope all is well!\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"mllofw artifact differnt environ team subscript commun dev stag prod env let know process artifact differ environ separ azur data brick subscript regard naveen",
        "Question_preprocessed_content":"mllofw artifact differnt environ team subscript commun dev stag prod env let know process artifact differ environ separ azur data brick subscript regard naveen",
        "Question_gpt_summary_original":"The user is facing a challenge in understanding how MLflow can communicate between three different environments (dev, staging, and prod) with separate Azure Databricks subscriptions. They are seeking guidance on the process of moving MLflow artifacts between these environments.",
        "Question_gpt_summary":"user face challeng understand commun differ environ dev stage prod separ azur databrick subscript seek guidanc process move artifact environ",
        "Answer_original_content":"context regist model transit stage product archiv singl environ share model workspac access track server outsid azur databrick differ subscript hth receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user fee afebn googlegroup com thank denni lee glad help hope",
        "Answer_preprocessed_content":"context regist model transit stage product archiv singl environ share model workspac access track server outsid azur databrick differ subscript hth receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit thank denni lee glad help hope",
        "Answer_gpt_summary_original":"Solution: \n- Within the context of MLflow, you can register your model and then transition it between staging, production, and archived within a single environment. \n- You can share models across workspaces and\/or access the MLflow tracking server from outside Azure Databricks per different subscriptions.",
        "Answer_gpt_summary":"solut context regist model transit stage product archiv singl environ share model workspac access track server outsid azur databrick differ subscript"
    },
    {
        "Question_id":null,
        "Question_title":"Speech to text work",
        "Question_body":"  I am a user of Speech to Text. I use it in order to get a written text from the interviews and courses I shoot myself. After that I correct the text manually. So, in Russian it works fine, however, 20-30 percents of the words are incorrect. Moreover, there are no Russian punctuation at all.  So I get the speech to text transcript, then I create the perfect transcript out of this with correct words and punctuations. All I want to know is how I can improve Speech toText by using the perfect transcript I have already corrected? Where I can send that data to?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636330440000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":325.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-text-work\/td-p\/175139\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-09T09:36:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\u00a0\n\nIf you're using the Google Cloud Speech-to-Text API [1] and encounter text quality problem, I would suggest that you can report the issue at the Issue Tracker [2] with the reproduction details for the support to further look into issue with you to improve the quality.\u00a0\n\n[1]\u00a0https:\/\/cloud.google.com\/speech-to-text\n[2]\u00a0https:\/\/cloud.google.com\/support\/docs\/issue-trackers"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"speech text work user speech text us order written text interview cours shoot correct text manual russian work fine percent word incorrect russian punctuat speech text transcript creat perfect transcript correct word punctuat want know improv speech totext perfect transcript correct send data",
        "Question_preprocessed_content":"speech text work user speech text us order written text interview cours shoot correct text manual russian work fine percent word incorrect russian punctuat speech text transcript creat perfect transcript correct word punctuat want know improv speech totext perfect transcript correct send data",
        "Question_gpt_summary_original":"The user is facing challenges with the accuracy of Speech to Text software when transcribing interviews and courses in Russian. They report that 20-30% of the words are incorrect and there is no Russian punctuation. The user manually corrects the transcript and is seeking ways to improve the software using their corrected transcript. They are looking for information on where to send this data.",
        "Question_gpt_summary":"user face challeng accuraci speech text softwar transcrib interview cours russian report word incorrect russian punctuat user manual correct transcript seek wai improv softwar correct transcript look inform send data",
        "Answer_original_content":"googl cloud speech text api encount text qualiti problem suggest report issu issu tracker reproduct detail support look issu improv qualiti http cloud googl com speech text http cloud googl com support doc issu tracker",
        "Answer_preprocessed_content":"googl cloud api encount text qualiti problem suggest report issu issu tracker reproduct detail support look issu improv qualiti",
        "Answer_gpt_summary_original":"possible solution: the user can report the speech-to-text accuracy and punctuation issues they are facing while using the google cloud speech-to-text api at the issue tracker with reproduction details. the support team can then work with the user to improve the quality.",
        "Answer_gpt_summary":"possibl solut user report speech text accuraci punctuat issu face googl cloud speech text api issu tracker reproduct detail support team work user improv qualiti"
    },
    {
        "Question_id":70397010.0,
        "Question_title":"What would stop credentials from validation on a ClearML server?",
        "Question_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639763350487,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":194.0,
        "Answer_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1640162037740,
        "Question_original_content":"stop credenti valid server set server gcp sub domain approach access domain http app mydomain com http api mydomain com http file mydomain com browser think correct respons connect python sdk init follow error backend api session session loginerror fail get token error http api mydomain com bad request like caus error",
        "Question_preprocessed_content":"stop credenti valid server set server gcp approach access domain browser think correct respons connect python sdk follow error like caus error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to connect to a ClearML server using the python SDK via clearml-init. The error message indicates that the credentials are not being validated due to a Bad Request error. The user is seeking assistance in identifying the likely causes of this error.",
        "Question_gpt_summary":"user encount error try connect server python sdk init error messag indic credenti valid bad request error user seek assist identifi like caus error",
        "Answer_original_content":"follow discuss load balanc block request payload fix work allow method chang post request environ variabl",
        "Answer_preprocessed_content":"follow discuss load balanc block request payload fix work allow method chang request environ variabl",
        "Answer_gpt_summary_original":"the solution to the issue of credentials not validating on a server set up in gcp resulting in a \"bad request\" error is to change the method from a get request to a post request via an environment variable. this is because the load balancer being used is blocking get requests with a payload.",
        "Answer_gpt_summary":"solut issu credenti valid server set gcp result bad request error chang method request post request environ variabl load balanc block request payload"
    },
    {
        "Question_id":68034523.0,
        "Question_title":"How to download artifacts from mlflow in python",
        "Question_body":"<p>I am creating an mlflow experiment which logs a logistic regression model together with a metric and an artifact.<\/p>\n<pre><code>import mlflow\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_fscore_support\n\nwith mlflow.start_run(run_name=run_name, experiment_id=experiment_id):\n\n        logreg = LogisticRegression()\n        logreg.fit(x_train, y_train)\n        print('training over', flush=True)\n        y_pred = logreg.predict(x_test)\n        mlflow.sklearn.log_model(logreg, &quot;model&quot;)\n   \n        mlflow.log_metric(&quot;f1&quot;, precision_recall_fscore_support(y_test, y_pred, average='weighted')[2])\n        mlflow.log_artifact(x_train.to_csv('train.csv')\n<\/code><\/pre>\n<p>for some data (<code>x_train, y_train, x_test, y_test<\/code>)<\/p>\n<p>Is there any way to access the artifacts for that specific experiment_id for this run_name and read the <code>train.csv<\/code> and also read the <code>model<\/code> ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1624016436840,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":5182.0,
        "Answer_body":"<p>There is a <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.download_artifacts\" rel=\"noreferrer\">download_artifacts function<\/a> that allows you to get access to the logged artifact:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>local_path = client.download_artifacts(run_id, &quot;train.csv&quot;, local_dir)\n<\/code><\/pre>\n<p>The model artifact could either downloaded using the same function (there should be the object called <code>model\/model.pkl<\/code> (for scikit-learn, or something else), or you can load model by run:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>loaded_model = mlflow.pyfunc.load_model(f&quot;runs:\/{run_id}\/model&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68034523",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1624022186892,
        "Question_original_content":"download artifact python creat experi log logist regress model metric artifact import sklearn linear model import logisticregress sklearn metric import precis recal fscore support start run run run experi experi logreg logisticregress logreg fit train train print train flush true pred logreg predict test sklearn log model logreg model log metric precis recal fscore support test pred averag weight log artifact train csv train csv data train train test test wai access artifact specif experi run read train csv read model",
        "Question_preprocessed_content":"download artifact python creat experi log logist regress model metric artifact data wai access artifact specif read read",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing artifacts for a specific mlflow experiment in Python. They want to read the 'train.csv' and 'model' for a particular run name and experiment ID.",
        "Question_gpt_summary":"user face challeng access artifact specif experi python want read train csv model particular run experi",
        "Answer_original_content":"download artifact function allow access log artifact local path client download artifact run train csv local dir model artifact download function object call model model pkl scikit learn load model run load model pyfunc load model run run model",
        "Answer_preprocessed_content":"function allow access log artifact model artifact download function load model run",
        "Answer_gpt_summary_original":"the solution to download artifacts from an experiment in python includes using the download_artifacts function to access the logged artifact, and loading the model either by downloading it using the same function or by using the .pyfunc.load_model method.",
        "Answer_gpt_summary":"solut download artifact experi python includ download artifact function access log artifact load model download function pyfunc load model method"
    },
    {
        "Question_id":null,
        "Question_title":"TensorFlow 2 Object Detection API with W&B",
        "Question_body":"<p>Hello everyone! Firstime using Weights and biases. Came from 2 minute papers and  was pleasantly surprised when I heard 3 blue 1 brown in the promo video also! Must be a good tool if 2 of my favorite YouTubers are involved!<\/p>\n<p>Since I\u2019m a new user I included all of my resources links and images in this paste bin:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/pastebin.com\/wQgmjADX\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/0\/074c06983b1528839dce1ba4483d63e9070d13fa.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https:\/\/pastebin.com\/wQgmjADX\" target=\"_blank\" rel=\"noopener nofollow ugc\">Pastebin<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https:\/\/pastebin.com\/wQgmjADX\" target=\"_blank\" rel=\"noopener nofollow ugc\">[1] TensorFlow 2 Object Detection API:...<\/a><\/h3>\n\n  <p>Pastebin.com is the number one paste tool since 2002. Pastebin is a website where you can store text online for a set period of time.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<p><em>the last link in the above paste is the link to the imgur images<\/em><\/p>\n<p>I\u2019m developing a Car Object Detection model for a university project using the TensorFlow 2 Object Detection API [1].<br>\nTo get to the point I\u2019m at now where I have a trained model for object detection I was following this tutorial: [2]<\/p>\n<p>My question is how can I Integrate W&amp;B into a \"TensorFlow 2 Object Detection API \" workflow?<\/p>\n<p>I\u2019ve searched around the internet and only found these 2 related questions:<\/p>\n<p>[3]<\/p>\n<p>[4] (first top comment is the same question)<\/p>\n<p>Both of the above sources are unanswered. So I\u2019m sure that some other people in the future might come across this same unanswered problem.<\/p>\n<p>From what I understand W&amp;B works based on callbacks from the model.fit function like so:<\/p>\n<p>[6]<\/p>\n<p>But the \"TensorFlow 2 Object Detection API \" doesn\u2019t directly use the model.fit function but it calls a python script like so:<\/p>\n<p>[7]<\/p>\n<p>(For training I then paste this command into the terminal alternatively I could also just paste it into jupyter)<\/p>\n<p>I asked chat GPT about this problem and this is the answer it provided me:<\/p>\n<p>[8]<\/p>\n<p>I tried Chat GPT\u2019s solution but the results are a bit weird:<br>\nFirst successful run:<br>\nlogs:<br>\n[9]<\/p>\n<p>Charts:<br>\n[10]<\/p>\n<p>My question is what is the correct way to integrate W&amp;B into the \"TensorFlow 2 Object Detection API \"  workflow? Have I done it correctly but am I missing something? Also, where do I specify that W&amp;B should keep track of the loss and other variables or is that done automatically?<\/p>\n<p>Here is what my training looks like in the terminal for more info:<\/p>\n<p>Executing the training command:<br>\n[11]<br>\n\u2026<br>\n<em>A lot of skipped terminal lines<\/em><br>\n\u2026<br>\nEpcoh result logging after the training kicks off:<br>\n[12]<\/p>\n<p>I hope someone can help me out and thanks for all the help I advance!<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1675019613564,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":95.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/tensorflow-2-object-detection-api-with-w-b\/3770",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-02T18:37:51.099Z",
                "Answer_body":"<p>Hi Matija!<\/p>\n<p>Thank you so much for using Weights and Biases! <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nI will ask around internally and see if we have any docs or examples on this. So far I haven\u2019t been able to find anything as well\u2026<\/p>\n<p>I\u2019ll let you know if I find something asap!<\/p>\n<p>Cheers,<br>\nArtsiom<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-03T15:59:03.612Z",
                "Answer_body":"<p>Hi Matija,<\/p>\n<p>I have asked around and it seems like TFOD is deprecated and we will not be creating a new report\/example on it any time soon. What some of my engineers did suggest though, if using Tensorflow is not a compulsion for you, is using MMDetection. Which also has a better example for it.<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-07T20:04:03.949Z",
                "Answer_body":"<p>Hi Matija,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-13T19:14:40.271Z",
                "Answer_body":"<p>Hi Matija, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-14T15:38:09.221Z",
                "Answer_body":"<p>Hey,<br>\nI\u2019ve also tried integrating wandb with tensorflow object detection api but couldn\u2019t find any proper resource on it. My organization primarily uses tensorflow object detection api so it would be hard to change to anything else.So please provoide some suggestions or a working example(would be great) for tensorflow object detection api.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-14T15:40:24.085Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/artsiom\">@artsiom<\/a> ,<br>\nI\u2019ve also tried integrating wandb with tensorflow object detection api but couldn\u2019t find any proper resource on it. My organization primarily uses tensorflow object detection api so it would be hard to change to anything else.So please provoide some suggestions or a working example(would be great) for tensorflow object detection api.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"tensorflow object detect api hello firstim came minut paper pleasantli surpris heard blue brown promo video good tool favorit youtub involv new user includ resourc link imag past bin pastebin tensorflow object detect api pastebin com number past tool pastebin websit store text onlin set period time link past link imgur imag develop car object detect model univers project tensorflow object detect api point train model object detect follow tutori question integr tensorflow object detect api workflow iv search internet relat question comment question sourc unansw sure peopl futur come unansw problem understand work base callback model fit function like tensorflow object detect api doesnt directli us model fit function call python script like train past command termin altern past jupyt ask chat gpt problem answer provid tri chat gpt solut result bit weird success run log chart question correct wai integr tensorflow object detect api workflow correctli miss specifi track loss variabl automat train look like termin info execut train command lot skip termin line epcoh result log train kick hope help thank help advanc",
        "Question_preprocessed_content":"tensorflow object detect api hello firstim came minut paper pleasantli surpris heard blue brown promo video good tool favorit youtub involv new user includ resourc link imag past bin pastebin tensorflow object detect number past tool pastebin websit store text onlin set period time link past link imgur imag develop car object detect model univers project tensorflow object detect api point train model object detect follow tutori question integr tensorflow object detect api workflow iv search internet relat question sourc unansw sure peopl futur come unansw problem understand work base callback function like tensorflow object detect api doesnt directli us function call python script like train past command termin altern past jupyt ask chat gpt problem answer provid tri chat gpt solut result bit weird success run log chart question correct wai integr tensorflow object detect api workflow correctli miss specifi track loss variabl automat train look like termin info execut train command lot skip termin line epcoh result log train kick hope help thank help advanc",
        "Question_gpt_summary_original":"The user is facing challenges in integrating Weights and Biases (W&B) into their TensorFlow 2 Object Detection API workflow. They have searched for solutions online but have not found any satisfactory answers. They have tried a solution suggested by Chat GPT but the results were not satisfactory. The user is unsure about the correct way to integrate W&B into their workflow and how to specify that W&B should keep track of the loss and other variables.",
        "Question_gpt_summary":"user face challeng integr tensorflow object detect api workflow search solut onlin satisfactori answer tri solut suggest chat gpt result satisfactori user unsur correct wai integr workflow specifi track loss variabl",
        "Answer_original_content":"matija thank ask intern doc exampl far havent abl ill let know asap cheer artsiom matija ask like tfod deprec creat new report exampl time soon engin suggest tensorflow compuls mmdetect better exampl cheer artsiom matija want follow support request heard let know assist issu resolv best matija heard go close request like open convers let know hei iv tri integr tensorflow object detect api proper resourc organ primarili us tensorflow object detect api hard chang provoid suggest work exampl great tensorflow object detect api hei artsiom iv tri integr tensorflow object detect api proper resourc organ primarili us tensorflow object detect api hard chang provoid suggest work exampl great tensorflow object detect api",
        "Answer_preprocessed_content":"matija thank ask intern doc exampl far havent abl ill let know asap cheer artsiom matija ask like tfod deprec creat new time soon engin suggest tensorflow compuls mmdetect better exampl cheer artsiom matija want follow support request heard let know assist issu resolv best matija heard go close request like convers let know hei iv tri integr tensorflow object detect api proper resourc organ primarili us tensorflow object detect api hard chang provoid suggest work exampl tensorflow object detect api hei iv tri integr tensorflow object detect api proper resourc organ primarili us tensorflow object detect api hard chang provoid suggest work exampl tensorflow object detect api",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer. the user is having difficulty integrating  into a tensorflow 2 object detection api workflow, and the responder suggests using mmdetection instead of tfod. however, the user mentions that their organization primarily uses tensorflow object detection api, so it would be hard to change to anything else. the responder promises to look for any documentation or examples on the integration and will update the user if they find anything.",
        "Answer_gpt_summary":"specif solut provid answer user have difficulti integr tensorflow object detect api workflow respond suggest mmdetect instead tfod user mention organ primarili us tensorflow object detect api hard chang respond promis look document exampl integr updat user"
    },
    {
        "Question_id":null,
        "Question_title":"Cannot compare artifcats anymore",
        "Question_body":"<p>Hello,<\/p>\n<p>I work a project using GANs to generate images. At each iteration I would log artifacts containing the generated images. Up until now I could visualize the generated images in a table and compare multiple artifact versions visually by joining the tables in the artifact view.<br>\nIt seems that view has been updated and this functionality does not exist anymore or is hidden. Could anyone help me find back this functionality in case it still exists. Thank you!<\/p>\n<p>Mahmoud<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658850713295,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":312.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cannot-compare-artifcats-anymore\/2800",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-28T16:26:28.637Z",
                "Answer_body":"<p>Hi Mahmoud,<\/p>\n<p>This is a known issue that we have and there\u2019s a ticket out to fix this. I\u2019ll let you know when I receive an update on it.<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-24T15:51:58.398Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"compar artifcat anymor hello work project gan gener imag iter log artifact contain gener imag visual gener imag tabl compar multipl artifact version visual join tabl artifact view view updat function exist anymor hidden help function case exist thank mahmoud",
        "Question_preprocessed_content":"compar artifcat anymor hello work project gan gener imag iter log artifact contain gener imag visual gener imag tabl compar multipl artifact version visual join tabl artifact view view updat function exist anymor hidden help function case exist thank mahmoud",
        "Question_gpt_summary_original":"The user is facing a challenge in comparing multiple artifact versions visually in a table view after an update to the artifact view. The user is seeking help to find back this functionality if it still exists.",
        "Question_gpt_summary":"user face challeng compar multipl artifact version visual tabl view updat artifact view user seek help function exist",
        "Answer_original_content":"mahmoud known issu there ticket fix ill let know receiv updat warmli lesli topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"mahmoud known issu there ticket fix ill let know receiv updat warmli lesli topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the response acknowledges the issue and informs the user that there is a ticket to fix it. the user will be updated when there is progress on the issue.",
        "Answer_gpt_summary":"solut provid answer respons acknowledg issu inform user ticket fix user updat progress issu"
    },
    {
        "Question_id":59882941.0,
        "Question_title":"ValueError: no SavedModel bundles found! when trying to deploy a TF2.0 model to SageMaker",
        "Question_body":"<p>I'm trying to deploy a TF2.0 model to SageMaker. So far, I managed to train the model and save it into an S3 bucket but when I'm calling the <code>.deploy()<\/code> method, I get the following error from cloud Watch <\/p>\n\n<p><code>ValueError: no SavedModel bundles found!<\/code><\/p>\n\n<p>Here is my training script: <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tf_model\"), save_format=\"tf\")\n\ndef model_fn(model_dir):\n    classifier = tf.keras.models.load_model(os.path.join(model_dir, \"tf_model\"))\n    return classifier\n<\/code><\/pre>\n\n<p>And here is the code that I wrote into Colab <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='tensorflow_estimator.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='ml.m5.large',\n                          framework_version='2.0.0', \n                          sagemaker_session=sagemaker_session,\n                          output_path=s3_output_location,\n                          hyperparameters={'epochs': 1,\n                                           'batch_size': 30,\n                                           'learning_rate': 0.001},\n                          py_version='py3')\n\n\ntf_estimator.fit({\"train\":train_data})\n\nfrom sagemaker.tensorflow.serving import Model\n\nmodel = Model(model_data='s3:\/\/path\/to\/model.tar.gz', \n              role=role,\n              framework_version=\"2.0.0\",\n              sagemaker_session=sagemaker_session)\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n<\/code><\/pre>\n\n<p>I already tried to look at <a href=\"https:\/\/stackoverflow.com\/questions\/57172147\/no-savedmodel-bundles-found-on-tensorflow-hub-model-deployment-to-aws-sagemak\">this thread<\/a> but I actually don't have the problem of versions in my tar.gz file as the structure is the following : <\/p>\n\n<pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 saved_model.pb\n\u2514\u2500\u2500 variables\n    \u251c\u2500\u2500 variables.data-00000-of-00001\n    \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>I feel I might be wrong when defining <code>model_fn()<\/code> in my training script but definitely don't what to replace that with. Would you guys have an idea? <\/p>\n\n<p>Thanks a lot for your help!  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1579796394240,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":943.0,
        "Answer_body":"<p>I actually tried to modify my training script to the following : <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n\n<\/code><\/pre>\n\n<p>It seems that it's important to have a numerical name for your folder:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Save the model\nmodel.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1590696546172,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59882941",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1579799847967,
        "Question_original_content":"valueerror savedmodel bundl try deploi model try deploi model far manag train model save bucket call deploi method follow error cloud watch valueerror savedmodel bundl train script code add tensorflow estim file import argpars import import pathlib import tensorflow main parser argpars argumentpars hyperparamet sent client pass command line argument script parser add argument epoch type int default parser add argument batch size type int default parser add argument learn rate type float default data model output directori parser add argument output data dir type str default environ output data dir parser add argument model dir type str default environ model dir parser add argument train type str default environ channel train parser add argument test type str default environ channel test arg parser pars known arg print arg format arg file path pathlib path arg train print folder content item path iterdir print directori format item imag imag list path glob imag path str path path list path glob transform imag tensor def preprocess load imag path imag read file path imag imag decod jpeg imag channel imag imag resiz imag return imag appli preprocess function path data dataset tensor slice imag path imag path map preprocess load imag map label label data path iterdir data dir label data label index label enumer label label index label print label index format label index imag label label index path parent path list path glob creat dataset label data dataset tensor slice imag label zip train label dataset data dataset zip imag label shuffl dataset batch shuffl len imag batch arg batch size creat pre train model base model kera applic inceptionv input shape includ fals weight imagenet base model trainabl fals model kera sequenti base model kera layer globalaveragepoolingd kera layer dens len label activ softmax initi learn rate arg learn rate schedul kera optim schedul exponentialdecai initi learn rate decai step decai rate staircas true model compil optim kera optim adam learn rate schedul loss kera loss sparsecategoricalcrossentropi metric kera metric sparsecategoricalaccuraci train model model fit epoch arg epoch save model model save path join arg model dir model save format def model model dir classifi kera model load model path join model dir model return classifi code wrote colab tensorflow import tensorflow estim tensorflow entri point tensorflow estim role role train instanc count train instanc type larg framework version session session output path output locat hyperparamet epoch batch size learn rate version estim fit train train data tensorflow serv import model model model model data path model tar role role framework version session session predictor model deploi initi instanc count instanc type larg tri look thread actual problem version tar file structur follow asset save model variabl variabl data variabl index feel wrong defin model train script definit replac gui idea thank lot help",
        "Question_preprocessed_content":"valueerror savedmodel bundl try deploi model try deploi model far manag train model save bucket call method follow error cloud watch train script code wrote colab tri look thread actual problem version file structur follow feel wrong defin train script definit replac gui idea thank lot help",
        "Question_gpt_summary_original":"The user is encountering an error when trying to deploy a TensorFlow 2.0 model to SageMaker. The error message \"ValueError: no SavedModel bundles found!\" is displayed when calling the .deploy() method. The user has already trained the model and saved it to an S3 bucket. The user has also tried to look for solutions on Stack Overflow but has not found a solution. The user suspects that the issue may be with the model_fn() function in the training script but is unsure of how to fix it.",
        "Question_gpt_summary":"user encount error try deploi tensorflow model error messag valueerror savedmodel bundl displai call deploi method user train model save bucket user tri look solut stack overflow solut user suspect issu model function train script unsur fix",
        "Answer_original_content":"actual tri modifi train script follow code add tensorflow estim file import argpars import import pathlib import tensorflow main parser argpars argumentpars hyperparamet sent client pass command line argument script parser add argument epoch type int default parser add argument batch size type int default parser add argument learn rate type float default data model output directori parser add argument output data dir type str default environ output data dir parser add argument model dir type str default environ model dir parser add argument train type str default environ channel train parser add argument test type str default environ channel test arg parser pars known arg print arg format arg file path pathlib path arg train print folder content item path iterdir print directori format item imag imag list path glob imag path str path path list path glob transform imag tensor def preprocess load imag path imag read file path imag imag decod jpeg imag channel imag imag resiz imag return imag appli preprocess function path data dataset tensor slice imag path imag path map preprocess load imag map label label data path iterdir data dir label data label index label enumer label label index label print label index format label index imag label label index path parent path list path glob creat dataset label data dataset tensor slice imag label zip train label dataset data dataset zip imag label shuffl dataset batch shuffl len imag batch arg batch size creat pre train model base model kera applic inceptionv input shape includ fals weight imagenet base model trainabl fals model kera sequenti base model kera layer globalaveragepoolingd kera layer dens len label activ softmax initi learn rate arg learn rate schedul kera optim schedul exponentialdecai initi learn rate decai step decai rate staircas true model compil optim kera optim adam learn rate schedul loss kera loss sparsecategoricalcrossentropi metric kera metric sparsecategoricalaccuraci train model model fit epoch arg epoch save model model save path join arg model dir tensorflow model save format import numer folder save model model save path join arg model dir tensorflow model save format",
        "Answer_preprocessed_content":"actual tri modifi train script follow import numer folder",
        "Answer_gpt_summary_original":"the answer suggests modifying the training script by adding code to a tensorflow_estimator.py file. it also recommends ensuring that the saved model has a numerical name for the folder. the modified script includes hyperparameters, data, model, and output directories, and trains and saves the model.",
        "Answer_gpt_summary":"answer suggest modifi train script ad code tensorflow estim file recommend ensur save model numer folder modifi script includ hyperparamet data model output directori train save model"
    },
    {
        "Question_id":64097278.0,
        "Question_title":"Why is the field \"compute target\" for data drift monitoring in Azure ML studio still blank whereas I have a compute instance?",
        "Question_body":"<p>I have created a compute instance:<\/p>\n<p>Virtual machine size\nSTANDARD_DS3_V2 (4 Cores, 14 GB RAM, 28 GB Disk)<\/p>\n<p>Processing Unit\nCPU - General purpose<\/p>\n<p>But, I'm not able to access it when trying to set it for data drift monitoring.\nThe dropdown list is empty. I can't understand why. Can you help me please?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pf10h.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601276103867,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":88.0,
        "Answer_body":"<p>I found the answer. You must give a <strong>cluster<\/strong> compute instance to do data drift in Azure Machine Learning Studio. As it is not clear, I'm planning to add something in the documentation of Azure.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64097278",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1601278985528,
        "Question_original_content":"field comput target data drift monitor studio blank comput instanc creat comput instanc virtual machin size standard core ram disk process unit cpu gener purpos abl access try set data drift monitor dropdown list understand help",
        "Question_preprocessed_content":"field comput target data drift monitor studio blank comput instanc creat comput instanc virtual machin size process unit cpu gener purpos abl access try set data drift monitor dropdown list understand help",
        "Question_gpt_summary_original":"The user has created a compute instance in Azure ML studio but is unable to access it when trying to set it for data drift monitoring as the \"compute target\" field is still blank and the dropdown list is empty. The user is seeking help to understand why this is happening.",
        "Question_gpt_summary":"user creat comput instanc studio unabl access try set data drift monitor comput target field blank dropdown list user seek help understand happen",
        "Answer_original_content":"answer cluster comput instanc data drift studio clear plan add document azur",
        "Answer_preprocessed_content":"answer cluster comput instanc data drift studio clear plan add document azur",
        "Answer_gpt_summary_original":"solution: to resolve the issue of the \"compute target\" field being blank for data drift monitoring in studio, the user must provide a cluster compute instance. the answer suggests that this information is not clear in the azure documentation and the user plans to add it.",
        "Answer_gpt_summary":"solut resolv issu comput target field blank data drift monitor studio user provid cluster comput instanc answer suggest inform clear azur document user plan add"
    },
    {
        "Question_id":null,
        "Question_title":"Adding input and output parameter to a DatabricksStep",
        "Question_body":"@romungi-MSFT\n\nHello,\n\nWe are using AMLS for creating and registering a pipeline which runs on a pre-defined Databricks cluster.\nIn the AMLS workspace, there is our Databricks notebook which should be executed in the DatabricksStep.\n\nWe want to save a file into a Blob-storage container. Therefore, we have added the parameters \"outputs\" and \"notebook_params\" to the DatabricksStep:\n\n\nWe would like to know how we can retrieve the output folder path within the Databricks notebook with the name \"basic_DatabricksStep_script.py\".\nWith PythonScriptStep this worked using the following commands:\n\n import argparse\n parser = argparse.ArgumentParser()\n parser.add_argument('output', type=str, dest='output', default='output', help='given output data folder name') \n args = parser.parse_args()\n output_data_folder_path = args.output\n\n\n\nHow will this work with a DatabricksStep?\n\nWe are aware of this notebook, but we need additional support to solve our issue.\nIt would be great if you could provide exemplary code and also show us how we can add the input parameter to the DatabricksStep so that we can read Datasets which are registered in AMLS.\n\n\n\n\nThank you in advance for your efforts!\n\n\n\n\nWith best regards\nAlex",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1654710154713,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/881980\/adding-input-and-output-parameter-to-a-databrickss.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-10T09:15:46.96Z",
                "Answer_score":0,
                "Answer_body":"@romungi-MSFT\n\n\n\n\nThank you for the answer.\n\nWe are still not able to save a file into a Blob storage container.\nIt is important that we use the DatabricksStep, but irrelevant whether the Notebook (Python script basic_DatabricksStep_script.py) is in AMLS or the Databricks workspace.\n\nHow we try to save a file into the Blob storage container:\n\nHere is the Python script, which should be executed in the DatabricksStep\n\n %%writefile $source_directory\/basic_DatabricksStep_script.py\n    \n dbutils.widgets.get(\"input\")\n i = getArgument(\"input\")\n print (\"Param -\\'input':\")\n print (i)\n    \n dbutils.widgets.get(\"output\")\n dbutils.widgets.get(\"output\")\n o = getArgument(\"output\")\n print (\"Param -\\'output':\")\n print (o)\n data = [('value1', 'value2')]\n df2 = spark.createDataFrame(data)\n    \n z = o + \"\/output.txt\"\n df2.write.csv(z)\n\n\n\nThis is how we define the DatabricksStep\n\n\n\n def_blob_store = Datastore(ws, \"input_datastore\")\n step_1_input = DataReference(datastore=def_blob_store, path_on_datastore=\"dbtest\",\n                                      data_reference_name=\"input\")\n    \n output_data_folder_name = \"output\"\n output_data_folder = PipelineData(output_data_folder_name, Datastore.get(ws, \"output_datastore\"))\n     \n dbNbWithExistingClusterStep = DatabricksStep(\n     name=\"DBFSReferenceWithExisting\",\n     run_name='DBFS_Reference_With_Existing',\n     source_directory = source_directory,\n     python_script_name = \"basic_DatabricksStep_script.py\",\n  inputs=[step_1_input],\n     outputs=[output_data_folder],\n     compute_target=databricks_compute,\n     existing_cluster_id=\"XXXXXX\",\n     allow_reuse=True,\n     permit_cluster_restart=True\n )\n\n\n\n\nHere is a picture for making it clearer what we want to achieve:\n\n\n\n\n\nCurrently, our pipeline is not getting built by AMLS even though we followed the examples of the official GitHub notebook for learning about the DatabricksStep class.\nCan you make our pipeline work, please?\n\n\n\n\nThank you in advance for your support!\n\n\n\n\nWith best regards,\nAlex",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":17.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"ad input output paramet databricksstep romungi msft hello aml creat regist pipelin run pre defin databrick cluster aml workspac databrick notebook execut databricksstep want save file blob storag contain ad paramet output notebook param databricksstep like know retriev output folder path databrick notebook basic databricksstep script pythonscriptstep work follow command import argpars parser argpars argumentpars parser add argument output type str dest output default output help given output data folder arg parser pars arg output data folder path arg output work databricksstep awar notebook need addit support solv issu great provid exemplari code add input paramet databricksstep read dataset regist aml thank advanc effort best regard alex",
        "Question_preprocessed_content":"ad input output paramet databricksstep hello aml creat regist pipelin run databrick cluster aml workspac databrick notebook execut databricksstep want save file contain ad paramet output databricksstep like know retriev output folder path databrick notebook pythonscriptstep work follow command import argpars parser type str dest output default output help given output data folder arg work databricksstep awar notebook need addit support solv issu great provid exemplari code add input paramet databricksstep read dataset regist aml thank advanc effort best regard alex",
        "Question_gpt_summary_original":"The user is facing challenges in adding input and output parameters to a DatabricksStep in AMLS. They want to save a file into a Blob-storage container and retrieve the output folder path within the Databricks notebook. They are also seeking help in adding input parameters to the DatabricksStep to read datasets registered in AMLS. The user is requesting exemplary code and additional support to solve their issue.",
        "Question_gpt_summary":"user face challeng ad input output paramet databricksstep aml want save file blob storag contain retriev output folder path databrick notebook seek help ad input paramet databricksstep read dataset regist aml user request exemplari code addit support solv issu",
        "Answer_original_content":"romungi msft thank answer abl save file blob storag contain import us databricksstep irrelev notebook python script basic databricksstep script aml databrick workspac try save file blob storag contain python script execut databricksstep writefil sourc directori basic databricksstep script dbutil widget input getargu input print param input print dbutil widget output dbutil widget output getargu output print param output print data valu valu spark createdatafram data output txt write csv defin databricksstep def blob store datastor input datastor step input datarefer datastor def blob store path datastor dbtest data refer input output data folder output output data folder pipelinedata output data folder datastor output datastor dbnbwithexistingclusterstep databricksstep dbfsreferencewithexist run dbf refer exist sourc directori sourc directori python script basic databricksstep script input step input output output data folder comput target databrick comput exist cluster allow reus true permit cluster restart true pictur make clearer want achiev current pipelin get built aml follow exampl offici github notebook learn databricksstep class pipelin work thank advanc support best regard alex",
        "Answer_preprocessed_content":"thank answer abl save file blob storag contain import us databricksstep irrelev notebook aml databrick workspac try save file blob storag contain python script execut databricksstep writefil getargu print print getargu print print data defin databricksstep datastor output dbnbwithexistingclusterstep databricksstep pictur make clearer want achiev current pipelin get built aml follow exampl offici github notebook learn databricksstep class pipelin work thank advanc support best regard alex",
        "Answer_gpt_summary_original":"the user is trying to save a file into a blob storage container using databricksstep, but is encountering challenges. the answer provides a python script that defines the databricksstep and shows how to save a file into the blob storage container. the user is also requesting help to make their pipeline work.",
        "Answer_gpt_summary":"user try save file blob storag contain databricksstep encount challeng answer provid python script defin databricksstep show save file blob storag contain user request help pipelin work"
    },
    {
        "Question_id":65991587.0,
        "Question_title":"AzureDevOPS ML Error: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories",
        "Question_body":"<p>I am trying to create an Azure DEVOPS ML Pipeline. The following code works 100% fine on Jupyter Notebooks, but when I run it in Azure Devops I get this error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;src\/my_custom_package\/data.py&quot;, line 26, in &lt;module&gt;\n    ws = Workspace.from_config()\n  File &quot;\/opt\/hostedtoolcache\/Python\/3.8.7\/x64\/lib\/python3.8\/site-packages\/azureml\/core\/workspace.py&quot;, line 258, in from_config\n    raise UserErrorException('We could not find config.json in: {} or in its parent directories. '\nazureml.exceptions._azureml_exception.UserErrorException: UserErrorException:\n    Message: We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;We could not find config.json in: \/home\/vsts\/work\/1\/s or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.&quot;\n    }\n}\n<\/code><\/pre>\n<p>The code is:<\/p>\n<pre><code>#import\nfrom sklearn.model_selection import train_test_split\nfrom azureml.core.workspace import Workspace\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.experiment import Experiment\nfrom datetime import date\nfrom azureml.core import Workspace, Dataset\n\n\n\nimport pandas as pd\nimport numpy as np\nimport logging\n\n#getdata\nsubscription_id = 'mysubid'\nresource_group = 'myrg'\nworkspace_name = 'mlplayground'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='correctData')\n\n\n#auto ml\nws = Workspace.from_config()\n\n\nautoml_settings = {\n    &quot;iteration_timeout_minutes&quot;: 2880,\n    &quot;experiment_timeout_hours&quot;: 48,\n    &quot;enable_early_stopping&quot;: True,\n    &quot;primary_metric&quot;: 'spearman_correlation',\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO,\n    &quot;n_cross_validations&quot;: 5,\n    &quot;max_concurrent_iterations&quot;: 4,\n    &quot;max_cores_per_iteration&quot;: -1,\n}\n\n\n\ncpu_cluster_name = &quot;computecluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=cpu_cluster_name)\nprint(compute_target)\nautoml_config = AutoMLConfig(task='regression',\n                             compute_target = compute_target,\n                             debug_log='automated_ml_errors.log',\n                             training_data = dataset,\n                             label_column_name=&quot;paidInDays&quot;,\n                             **automl_settings)\n\ntoday = date.today()\nd4 = today.strftime(&quot;%b-%d-%Y&quot;)\n\nexperiment = Experiment(ws, &quot;myexperiment&quot;+d4)\nremote_run = experiment.submit(automl_config, show_output = True)\n\nfrom azureml.widgets import RunDetails\nRunDetails(remote_run).show()\n\nremote_run.wait_for_completion()\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1612177533493,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":2339.0,
        "Answer_body":"<p>There is something weird happening on your code, you are getting the data from a first workspace (<code>workspace = Workspace(subscription_id, resource_group, workspace_name)<\/code>), then using the resources from a second one (<code>ws = Workspace.from_config()<\/code>). I would suggest avoiding having code relying on two different workspaces, especially when you know that an underlying datasource can be registered (linked) to multiple workspaces (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#create-and-register-datastores\" rel=\"nofollow noreferrer\">documentation<\/a>).<\/p>\n<p>In general using a <code>config.json<\/code> file when instantiating a <code>Workspace<\/code> object will result in an interactive authentication. When your code will be processed and you will have a log asking you to reach a specific URL and enter a code. This will use your Microsoft account to verify that you are authorized to access the Azure resource (in this case your <code>Workspace('mysubid', 'myrg', 'mlplayground')<\/code>). This has its limitations when you start deploying the code onto virtual machines or agents, you will not always manually check the logs, access the URL and authenticate yourself.<\/p>\n<p>For this matter it is strongly recommended setting up more advanced authentication methods and personally I would suggest using the service principal one since it is simple, convinient and secure if done properly.\nYou can follow Azure's official documentation <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#configure-a-service-principal\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65991587",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1612865840128,
        "Question_original_content":"azuredevop error config json home vst work parent directori try creat azur devop pipelin follow code work fine jupyt notebook run azur devop error traceback recent file src custom packag data line workspac config file opt hostedtoolcach python lib python site packag core workspac line config rais usererrorexcept config json parent directori except except usererrorexcept usererrorexcept messag config json home vst work parent directori provid path config file ensur config json exist parent directori innerexcept errorrespons error code usererror messag config json home vst work parent directori provid path config file ensur config json exist parent directori code import sklearn model select import train test split core workspac import workspac train automl import automlconfig core comput import computetarget amlcomput core comput target import computetargetexcept core experi import experi datetim import date core import workspac dataset import panda import numpi import log getdata subscript mysubid resourc group myrg workspac mlplayground workspac workspac subscript resourc group workspac dataset dataset workspac correctdata auto workspac config automl set iter timeout minut experi timeout hour enabl earli stop true primari metric spearman correl featur auto verbos log info cross valid max concurr iter max core iter cpu cluster computeclust comput target computetarget workspac cpu cluster print comput target automl config automlconfig task regress comput target comput target debug log autom error log train data dataset label column paidindai automl set todai date todai todai strftime experi experi myexperi remot run experi submit automl config output true widget import rundetail rundetail remot run remot run wait complet",
        "Question_preprocessed_content":"azuredevop error parent directori try creat azur devop pipelin follow code work fine jupyt notebook run azur devop error code",
        "Question_gpt_summary_original":"The user is encountering an error while trying to create an Azure DevOps ML Pipeline. The error message indicates that the config.json file cannot be found in the specified directory or its parent directories. The code works fine on Jupyter Notebooks but not on Azure DevOps. The user has provided the code used for the pipeline.",
        "Question_gpt_summary":"user encount error try creat azur devop pipelin error messag indic config json file specifi directori parent directori code work fine jupyt notebook azur devop user provid code pipelin",
        "Answer_original_content":"weird happen code get data workspac workspac workspac subscript resourc group workspac resourc second workspac config suggest avoid have code reli differ workspac especi know underli datasourc regist link multipl workspac document gener config json file instanti workspac object result interact authent code process log ask reach specif url enter code us microsoft account verifi author access azur resourc case workspac mysubid myrg mlplayground limit start deploi code virtual machin agent manual check log access url authent matter strongli recommend set advanc authent method person suggest servic princip simpl convini secur properli follow azur offici document",
        "Answer_preprocessed_content":"weird happen code get data workspac resourc second suggest avoid have code reli differ workspac especi know underli datasourc regist multipl workspac gener file instanti object result interact authent code process log ask reach specif url enter code us microsoft account verifi author access azur resourc limit start deploi code virtual machin agent manual check log access url authent matter strongli recommend set advanc authent method person suggest servic princip simpl convini secur properli follow azur offici document",
        "Answer_gpt_summary_original":"the solution to the error message indicating that config.json could not be found in the specified directory is to avoid having code relying on two different workspaces. it is recommended to use more advanced authentication methods, such as the service principal one, which is simple, convenient, and secure if done properly. azure's official documentation can be followed for this purpose.",
        "Answer_gpt_summary":"solut error messag indic config json specifi directori avoid have code reli differ workspac recommend us advanc authent method servic princip simpl conveni secur properli azur offici document follow purpos"
    },
    {
        "Question_id":63947132.0,
        "Question_title":"Trouble connecting to AMLS web service on AKS using Python requests",
        "Question_body":"<p>I am having trouble contacting an AMLS web service hosted on AKS in a vnet. I am able to successfully provision AKS and deploy the models, but I am not able to access the web service using the Python requests module:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>headers = {'Content-Type':'application\/json',\n           'Authorization': 'Bearer ' + &lt;AKS_KEY&gt;}\nresp = requests.post(&lt;AKS_URI&gt;, json={&quot;data&quot;:{&quot;x&quot;: &quot;1&quot;}}, headers=headers)\nprint(resp.text)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>Error: HTTPConnectionPool(host='', port=80): Max retries exceeded with url: &lt;AKS_URL&gt; (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f33f6035a10&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))<\/p>\n<\/blockquote>\n<p>However, I am able to successfully connect to the web service using Postman:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>curl --location --request POST &lt;AKS_URI&gt; \\\n--header 'Authorization: Bearer &lt;AKS_KEY&gt;' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;data&quot;: {&quot;x&quot;: &quot;1&quot;}}'\n<\/code><\/pre>\n<p>If I load the AKS service in my AMLS workspace <code>aks_service.run()<\/code> also gives me the same error message. I don't have these problems when I deploy without vnet integration.<\/p>\n<p>What could be causing this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_creation_time":1600383486567,
        "Question_favorite_count":null,
        "Question_last_edit_time":1600397592800,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":"<p>I fixed this by adding an inbound security rule enabled for the scoring endpoint in the NSG group that controls the virtual network.<\/p>\n<p>This should be done so that the scoring endpoint can be called from outside the virtual network (see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet\" rel=\"nofollow noreferrer\">documentation<\/a>), but apparently Postman can figure out how to access the endpoint without this security rule!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63947132",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1600397953580,
        "Question_original_content":"troubl connect aml web servic ak python request have troubl contact aml web servic host ak vnet abl successfulli provis ak deploi model abl access web servic python request modul header content type applic json author bearer resp request post json data header header print resp text follow error error httpconnectionpool host port max retri exceed url caus newconnectionerror fail establish new connect errno connect time abl successfulli connect web servic postman curl locat request post header author bearer header content type applic json data raw data load ak servic aml workspac ak servic run give error messag problem deploi vnet integr caus",
        "Question_preprocessed_content":"troubl connect aml web servic ak python request have troubl contact aml web servic host ak vnet abl successfulli provis ak deploi model abl access web servic python request modul follow error error httpconnectionpool max retri exceed url abl successfulli connect web servic postman load ak servic aml workspac give error messag problem deploi vnet integr caus",
        "Question_gpt_summary_original":"The user is having trouble accessing an AMLS web service hosted on AKS in a vnet using the Python requests module. They are able to provision AKS and deploy the models successfully, but encounter a \"Connection timed out\" error when attempting to access the web service. However, they are able to connect to the web service using Postman. The same error message is received when loading the AKS service in the AMLS workspace. The user does not encounter these problems when deploying without vnet integration.",
        "Question_gpt_summary":"user have troubl access aml web servic host ak vnet python request modul abl provis ak deploi model successfulli encount connect time error attempt access web servic abl connect web servic postman error messag receiv load ak servic aml workspac user encount problem deploi vnet integr",
        "Answer_original_content":"fix ad inbound secur rule enabl score endpoint nsg group control virtual network score endpoint call outsid virtual network document appar postman figur access endpoint secur rule",
        "Answer_preprocessed_content":"fix ad inbound secur rule enabl score endpoint nsg group control virtual network score endpoint call outsid virtual network appar postman figur access endpoint secur rule",
        "Answer_gpt_summary_original":"the solution to the challenge of connecting to an amls web service hosted on aks in a vnet using the python requests module is to add an inbound security rule enabled for the scoring endpoint in the nsg group that controls the virtual network. this allows the scoring endpoint to be called from outside the virtual network. however, it seems that postman can access the endpoint without this security rule.",
        "Answer_gpt_summary":"solut challeng connect aml web servic host ak vnet python request modul add inbound secur rule enabl score endpoint nsg group control virtual network allow score endpoint call outsid virtual network postman access endpoint secur rule"
    },
    {
        "Question_id":73728499.0,
        "Question_title":"How to update an existing model in AWS sagemaker >= 2.0",
        "Question_body":"<p>I have an XGBoost model currently in production using AWS sagemaker and making real time inferences. After a while, I would like to update the model with a newer one trained on more data and keep everything as is (e.g. same endpoint, same inference procedure, so really no changes aside from the model itself)<\/p>\n<p>The current deployment procedure is the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.xgboost.model import XGBoostModel\nfrom sagemaker.xgboost.model import XGBoostPredictor\n\nxgboost_model = XGBoostModel(\n    model_data = &lt;S3 url&gt;,\n    role = &lt;sagemaker role&gt;,\n    entry_point = 'inference.py',\n    source_dir = 'src',\n    code_location = &lt;S3 url of other dependencies&gt;\n    framework_version='1.5-1',\n    name = model_name)\n\nxgboost_model.deploy(\n    instance_type='ml.c5.large',\n    initial_instance_count=1,\n    endpoint_name = model_name)\n<\/code><\/pre>\n<p>Now that I updated the model a few weeks later, I would like to re-deploy it. I am aware that the <code>.deploy()<\/code> method creates an endpoint and an endpoint configuration so it does it all. I cannot simply re-run my script again since I would encounter an error.<\/p>\n<p>In previous versions of sagemaker I could have updated the model with an extra argument passed to the <code>.deploy()<\/code> method called <code>update_endpoint = True<\/code>. In sagemaker &gt;=2.0 this is a no-op. Now, in sagemaker &gt;= 2.0, I need to use the predictor object as stated in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html\" rel=\"nofollow noreferrer\">documentation<\/a>. So I try the following :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>predictor = XGBoostPredictor(model_name)\npredictor.update_endpoint(model_name= model_name)\n<\/code><\/pre>\n<p>Which actually updates the endpoint according to a new endpoint configuration. However, I do not know what it is updating... I do not specify in the above 2 lines of code that we need to considering the new <code>xgboost_model<\/code> trained on more data...  so where do I tell the update to take a more recent model?<\/p>\n<p>Thank you!<\/p>\n<p><strong>Update<\/strong><\/p>\n<p>I believe that I need to be looking at production variants as stated in their documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-ab-testing.html\" rel=\"nofollow noreferrer\">here<\/a>. However, their whole tutorial is based on the amazon sdk for python (boto3) which has artifacts that are hard to manage when I have difference entry points for each model variant (e.g. different <code>inference.py<\/code> scripts).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663233254363,
        "Question_favorite_count":null,
        "Question_last_edit_time":1663319367852,
        "Question_score":0.0,
        "Question_view_count":41.0,
        "Answer_body":"<p>Since I found an answer to my own question I will post it here for those who encounter the same problem.<\/p>\n<p>I ended up re-coding all my deployment script using the boto3 SDK rather than the sagemaker SDK (or a mix of both as some documentation suggest).<\/p>\n<p>Here's the whole script that shows how to create a sagemaker model object, an endpoint configuration and an endpoint to deploy the model on for the first time. In addition, it shows what to do how to update the endpoint with a newer model (which was my main question)<\/p>\n<p>Here's the code to do all 3 in case you want to bring your own model and update it safely in production using sagemaker :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\nimport time\nfrom datetime import datetime\nfrom sagemaker import image_uris\nfrom fileManager import *  # this is a local script for helper functions\n\n# name of zipped model and zipped inference code\nCODE_TAR = 'your_inference_code_and_other_artifacts.tar.gz'\nMODEL_TAR = 'your_saved_xgboost_model.tar.gz'\n\n# sagemaker params\nsmClient = boto3.client('sagemaker')\nsmRole = &lt;your_sagemaker_role&gt;\nbucket = sagemaker.Session().default_bucket()\n\n# deploy algorithm\nclass Deployer:\n\n    def __init__(self, modelName, deployRetrained=False):\n        self.modelName=modelName\n        self.deployRetrained = deployRetrained\n        self.prefix = &lt;S3_model_path_prefix&gt;\n    \n    def deploy(self):\n        '''\n        Main method to create a sagemaker model, create an endpoint configuration and deploy the model. If deployRetrained\n        param is set to True, this method will update an already existing endpoint.\n        '''\n        # define model name and endpoint name to be used for model deployment\/update\n        model_name = self.modelName + &lt;any_suffix&gt;\n        endpoint_config_name = self.modelName + '-%s' %datetime.now().strftime('%Y-%m-%d-%HH%M')\n        endpoint_name = self.modelName\n        \n        # deploy model for the first time\n        if not self.deployRetrained:\n            print('Deploying for the first time')\n\n            # here you should copy and zip the model dependencies that you may have (such as preprocessors, inference code, config code...)\n            # mine were zipped into the file called CODE_TAR\n\n            # upload model and model artifacts needed for inference to S3\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create sagemaker model and endpoint configuration\n            self.createSagemakerModel(model_name)\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # deploy model and wait while endpoint is being created\n            self.createEndpoint(endpoint_name, endpoint_config_name)\n            self.waitWhileCreating(endpoint_name)\n        \n        # update model\n        else:\n            print('Updating existing model')\n\n            # upload model and model artifacts needed for inference (here the old ones are replaced)\n            # make sure to make a backup in S3 if you would like to keep the older models\n            # we replace the old ones and keep the same names to avoid having to recreate a sagemaker model with a different name for the update!\n            uploadFile(list_files=[MODEL_TAR, CODE_TAR], prefix = self.prefix)\n\n            # create a new endpoint config that takes the new model\n            self.createEndpointConfig(endpoint_config_name, model_name)\n\n            # update endpoint\n            self.updateEndpoint(endpoint_name, endpoint_config_name)\n\n            # wait while endpoint updates then delete outdated endpoint config once it is InService\n            self.waitWhileCreating(endpoint_name)\n            self.deleteOutdatedEndpointConfig(model_name, endpoint_config_name)\n\n    def createSagemakerModel(self, model_name):\n        ''' \n        Create a new sagemaker Model object with an xgboost container and an entry point for inference using boto3 API\n        '''\n        # Retrieve that inference image (container)\n        docker_container = image_uris.retrieve(region=region, framework='xgboost', version='1.5-1')\n\n        # Relative S3 path to pre-trained model to create S3 model URI\n        model_s3_key = f'{self.prefix}\/'+ MODEL_TAR\n\n        # Combine bucket name, model file name, and relate S3 path to create S3 model URI\n        model_url = f's3:\/\/{bucket}\/{model_s3_key}'\n\n        # S3 path to the necessary inference code\n        code_url = f's3:\/\/{bucket}\/{self.prefix}\/{CODE_TAR}'\n        \n        # Create a sagemaker Model object with all its artifacts\n        smClient.create_model(\n            ModelName = model_name,\n            ExecutionRoleArn = smRole,\n            PrimaryContainer = {\n                'Image': docker_container,\n                'ModelDataUrl': model_url,\n                'Environment': {\n                    'SAGEMAKER_PROGRAM': 'inference.py', #inference.py is at the root of my zipped CODE_TAR\n                    'SAGEMAKER_SUBMIT_DIRECTORY': code_url,\n                }\n            }\n        )\n    \n    def createEndpointConfig(self, endpoint_config_name, model_name):\n        ''' \n        Create an endpoint configuration (only for boto3 sdk procedure) and set production variants parameters.\n        Each retraining procedure will induce a new variant name based on the endpoint configuration name.\n        '''\n        smClient.create_endpoint_config(\n            EndpointConfigName=endpoint_config_name,\n            ProductionVariants=[\n                {\n                    'VariantName': endpoint_config_name,\n                    'ModelName': model_name,\n                    'InstanceType': INSTANCE_TYPE,\n                    'InitialInstanceCount': 1\n                }\n            ]\n        )\n\n    def createEndpoint(self, endpoint_name, endpoint_config_name):\n        '''\n        Deploy the model to an endpoint\n        '''\n        smClient.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name)\n    \n    def deleteOutdatedEndpointConfig(self, name_check, current_endpoint_config):\n        '''\n        Automatically detect and delete endpoint configurations that contain a string 'name_check'. This method can be used\n        after a retrain procedure to delete all previous endpoint configurations but keep the current one named 'current_endpoint_config'.\n        '''\n        # get a list of all available endpoint configurations\n        all_configs = smClient.list_endpoint_configs()['EndpointConfigs']\n\n        # loop over the names of endpoint configs\n        names_list = []\n        for config_dict in all_configs:\n            endpoint_config_name = config_dict['EndpointConfigName']\n\n            # get only endpoint configs that contain name_check in them and save names to a list\n            if name_check in endpoint_config_name:\n                names_list.append(endpoint_config_name)\n        \n        # remove the current endpoint configuration from the list (we do not want to detele this one since it is live)\n        names_list.remove(current_endpoint_config)\n\n        for name in names_list:\n            try:\n                smClient.delete_endpoint_config(EndpointConfigName=name)\n                print('Deleted endpoint configuration for %s' %name)\n            except:\n                print('INFO : No endpoint configuration was found for %s' %endpoint_config_name)\n\n    def updateEndpoint(self, endpoint_name, endpoint_config_name):\n        ''' \n        Update existing endpoint with a new retrained model\n        '''\n        smClient.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=endpoint_config_name,\n            RetainAllVariantProperties=True)\n    \n    def waitWhileCreating(self, endpoint_name):\n        ''' \n        While the endpoint is being created or updated sleep for 60 seconds.\n        '''\n        # wait while creating or updating endpoint\n        status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n        print('Status: %s' %status)\n        while status != 'InService' and status !='Failed':\n            time.sleep(60)\n            status = smClient.describe_endpoint(EndpointName=endpoint_name)['EndpointStatus']\n            print('Status: %s' %status)\n        \n        # in case of a deployment failure raise an error\n        if status == 'Failed':\n            raise ValueError('Endpoint failed to deploy')\n\nif __name__==&quot;__main__&quot;:\n    deployer = Deployer('churnmodel', deployRetrained=True)\n    deployer.deploy()\n<\/code><\/pre>\n<p>Final comments :<\/p>\n<ul>\n<li><p>The sagemaker <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/realtime-endpoints-deployment.html\" rel=\"nofollow noreferrer\">documentation<\/a> mentions all this but fails to state that you can provide an 'entry_point' to the <code>create_model<\/code> method as well as a 'source_dir' for inference dependencies (e.g. normalization artifacts). It can be done as seen in <code>PrimaryContainer<\/code> argument.<\/p>\n<\/li>\n<li><p>my <code>fileManager.py<\/code> script just contains basic functions to make tar files, upload and download to and from my S3 paths. To simplify the class, I have not included them in.<\/p>\n<\/li>\n<li><p>The method deleteOutdatedEndpointConfig may seem like an overkill with an unnecessary loop and checks, I do so because I have multiple endpoint configurations to handle and wanted to remove the ones that weren't live AND contain the string <code>name_check<\/code> (I do not know the exact name of the configuration since there is a datetime suffix). Feel free to simplify it or remove it all together if you feel like it.<\/p>\n<\/li>\n<\/ul>\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1663925308150,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73728499",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663924957328,
        "Question_original_content":"updat exist model xgboost model current product make real time infer like updat model newer train data endpoint infer procedur chang asid model current deploy procedur follow xgboost model import xgboostmodel xgboost model import xgboostpredictor xgboost model xgboostmodel model data role entri point infer sourc dir src code locat framework version model xgboost model deploi instanc type larg initi instanc count endpoint model updat model week later like deploi awar deploi method creat endpoint endpoint configur simpli run script encount error previou version updat model extra argument pass deploi method call updat endpoint true need us predictor object state document try follow predictor xgboostpredictor model predictor updat endpoint model model actual updat endpoint accord new endpoint configur know updat specifi line code need consid new xgboost model train data tell updat recent model thank updat believ need look product variant state document tutori base amazon sdk python boto artifact hard manag differ entri point model variant differ infer script",
        "Question_preprocessed_content":"updat exist model xgboost model current product make real time infer like updat model newer train data current deploy procedur follow updat model week later like awar method creat endpoint endpoint configur simpli script encount error previou version updat model extra argument pass method call need us predictor object state document try follow actual updat endpoint accord new endpoint configur know specifi line code need consid new train tell updat recent model thank updat believ need look product variant state document tutori base amazon sdk python artifact hard manag differ entri point model variant",
        "Question_gpt_summary_original":"The user is facing a challenge in updating an existing XGBoost model in AWS Sagemaker >= 2.0 while keeping everything else the same, including the endpoint and inference procedure. The user is aware that the .deploy() method creates an endpoint and endpoint configuration, and cannot simply re-run the script again. In previous versions of Sagemaker, the user could have updated the model with an extra argument passed to the .deploy() method called update_endpoint = True, but in Sagemaker >= 2.0, this is a no-op. The user is trying to use the predictor object to update the endpoint configuration, but is unsure how to specify the new xgboost_model trained on more data. The user is considering looking at production variants, but is hesitant due to differences in entry points for each model variant.",
        "Question_gpt_summary":"user face challeng updat exist xgboost model keep includ endpoint infer procedur user awar deploi method creat endpoint endpoint configur simpli run script previou version user updat model extra argument pass deploi method call updat endpoint true user try us predictor object updat endpoint configur unsur specifi new xgboost model train data user consid look product variant hesit differ entri point model variant",
        "Answer_original_content":"answer question post encount problem end code deploy script boto sdk sdk mix document suggest script show creat model object endpoint configur endpoint deploi model time addit show updat endpoint newer model main question code case want bring model updat safe product import boto import time datetim import datetim import imag uri filemanag import local script helper function zip model zip infer code code tar infer code artifact tar model tar save xgboost model tar param smclient boto client smrole bucket session default bucket deploi algorithm class deploy def init self modelnam deployretrain fals self modelnam modelnam self deployretrain deployretrain self prefix def deploi self main method creat model creat endpoint configur deploi model deployretrain param set true method updat exist endpoint defin model endpoint model deploy updat model self modelnam endpoint config self modelnam datetim strftime endpoint self modelnam deploi model time self deployretrain print deploi time copi zip model depend preprocessor infer code config code zip file call code tar upload model model artifact need infer uploadfil list file model tar code tar prefix self prefix creat model endpoint configur self createmodel model self createendpointconfig endpoint config model deploi model wait endpoint creat self createendpoint endpoint endpoint config self waitwhilecr endpoint updat model print updat exist model upload model model artifact need infer old on replac sure backup like older model replac old on name avoid have recreat model differ updat uploadfil list file model tar code tar prefix self prefix creat new endpoint config take new model self createendpointconfig endpoint config model updat endpoint self updateendpoint endpoint endpoint config wait endpoint updat delet outdat endpoint config inservic self waitwhilecr endpoint self deleteoutdatedendpointconfig model endpoint config def createmodel self model creat new model object xgboost contain entri point infer boto api retriev infer imag contain docker contain imag uri retriev region region framework xgboost version rel path pre train model creat model uri model kei self prefix model tar combin bucket model file relat path creat model uri model url bucket model kei path necessari infer code code url bucket self prefix code tar creat model object artifact smclient creat model modelnam model executionrolearn smrole primarycontain imag docker contain modeldataurl model url environ program infer infer root zip code tar submit directori code url def createendpointconfig self endpoint config model creat endpoint configur boto sdk procedur set product variant paramet retrain procedur induc new variant base endpoint configur smclient creat endpoint config endpointconfignam endpoint config productionvari variantnam endpoint config modelnam model instancetyp instanc type initialinstancecount def createendpoint self endpoint endpoint config deploi model endpoint smclient creat endpoint endpointnam endpoint endpointconfignam endpoint config def deleteoutdatedendpointconfig self check current endpoint config automat detect delet endpoint configur contain string check method retrain procedur delet previou endpoint configur current name current endpoint config list avail endpoint configur config smclient list endpoint config endpointconfig loop name endpoint config name list config dict config endpoint config config dict endpointconfignam endpoint config contain check save name list check endpoint config name list append endpoint config remov current endpoint configur list want detel live name list remov current endpoint config name list try smclient delet endpoint config endpointconfignam print delet endpoint configur print info endpoint configur endpoint config def updateendpoint self endpoint endpoint config updat exist endpoint new retrain model smclient updat endpoint endpointnam endpoint endpointconfignam endpoint config retainallvariantproperti true def waitwhilecr self endpoint endpoint creat updat sleep second wait creat updat endpoint statu smclient endpoint endpointnam endpoint endpointstatu print statu statu statu inservic statu fail time sleep statu smclient endpoint endpointnam endpoint endpointstatu print statu statu case deploy failur rais error statu fail rais valueerror endpoint fail deploi main deploy deploy churnmodel deployretrain true deploy deploi final comment document mention fail state provid entri point creat model method sourc dir infer depend normal artifact seen primarycontain argument filemanag script contain basic function tar file upload download path simplifi class includ method deleteoutdatedendpointconfig like overkil unnecessari loop check multipl endpoint configur handl want remov on weren live contain string check know exact configur datetim suffix feel free simplifi remov feel like hope help",
        "Answer_preprocessed_content":"answer question post encount problem end deploy script boto sdk sdk script show creat model object endpoint configur endpoint deploi model time addit show updat endpoint newer model code case want bring model updat safe product final comment document mention fail state provid method infer depend seen argument script contain basic function tar file upload download path simplifi class includ method deleteoutdatedendpointconfig like overkil unnecessari loop check multipl endpoint configur handl want remov on weren live contain string feel free simplifi remov feel like hope help",
        "Answer_gpt_summary_original":"the answer provides a solution to updating an existing model in >= 2.0 using the boto3 sdk. the solution involves creating a model object, an endpoint configuration, and an endpoint to deploy the model on for the first time. to update the endpoint with a newer model, the user needs to upload the new model and model artifacts needed for inference to s3, create a new endpoint configuration that takes the new model, and update the endpoint. the answer also includes code to do all three.",
        "Answer_gpt_summary":"answer provid solut updat exist model boto sdk solut involv creat model object endpoint configur endpoint deploi model time updat endpoint newer model user need upload new model model artifact need infer creat new endpoint configur take new model updat endpoint answer includ code"
    },
    {
        "Question_id":65168915.0,
        "Question_title":"AWS SageMaker - How to load trained sklearn model to serve for inference?",
        "Question_body":"<p>I am trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions. All I want to use sagemaker for, is to deploy and server model I had serialised using <code>joblib<\/code>, nothing more. every blog I have read and sagemaker python documentation showed that sklearn model had to be trained on sagemaker in order to be deployed in sagemaker.<\/p>\n<p>When I was going through the SageMaker documentation I learned that sagemaker does let users <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">load a serialised model<\/a> stored in S3 as shown below:<\/p>\n<pre><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>And this is what documentation says about the argument <code>model_dir<\/code>:<\/p>\n<blockquote>\n<p>SageMaker will inject the directory where your model files and\nsub-directories, saved by save, have been mounted. Your model function\nshould return a model object that can be used for model serving.<\/p>\n<\/blockquote>\n<p>This again means that training has to be done on sagemaker.<\/p>\n<p>So, is there a way I can just specify the S3 location of my serialised model and have sagemaker de-serialise(or load) the model from S3 and use it for inference?<\/p>\n<h2>EDIT 1:<\/h2>\n<p>I used code in the answer to my application and I got below error when trying to deploy from notebook of SageMaker studio. I believe SageMaker is screaming that training wasn't done on SageMaker.<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-6662bbae6010&gt; in &lt;module&gt;\n      1 predictor = model.deploy(\n      2     initial_instance_count=1,\n----&gt; 3     instance_type='ml.m4.xlarge'\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\n    770         &quot;&quot;&quot;\n    771         removed_kwargs(&quot;update_endpoint&quot;, kwargs)\n--&gt; 772         self._ensure_latest_training_job()\n    773         self._ensure_base_job_name()\n    774         default_name = name_from_base(self.base_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in _ensure_latest_training_job(self, error_message)\n   1128         &quot;&quot;&quot;\n   1129         if self.latest_training_job is None:\n-&gt; 1130             raise ValueError(error_message)\n   1131 \n   1132     delete_endpoint = removed_function(&quot;delete_endpoint&quot;)\n\nValueError: Estimator is not associated with a training job\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n# from sagemaker.pytorch import PyTorchModel\nfrom sagemaker.sklearn import SKLearn\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nsm_role = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\n\nmodel_file = &quot;s3:\/\/sagemaker-manual-bucket\/sm_model_artifacts\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = SKLearn(model_data=model_file,\n                entry_point='inference.py',\n                name='rf_try_1',\n                role=sm_role,\n                source_dir='code',\n                framework_version='0.20.0',\n                instance_count=1,\n                instance_type='ml.m4.xlarge',\n                predictor_cls=AnalysisClass)\npredictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607263332140,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1607399645212,
        "Question_score":3.0,
        "Question_view_count":3221.0,
        "Answer_body":"<p>Yes you can. AWS documentation focuses on end-to-end from training to deployment in SageMaker which makes the impression that training has to be done on sagemaker. AWS documentation and examples should have clear separation among Training in Estimator, Saving and loading model, and Deployment model to SageMaker Endpoint.<\/p>\n<h2>SageMaker Model<\/h2>\n<p>You need to create the <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\" rel=\"nofollow noreferrer\">AWS::SageMaker::Model<\/a> resource which refers to the &quot;model&quot; you have trained <strong>and more<\/strong>. AWS::SageMaker::Model is in CloudFormation document but it is only to explain what AWS resource you need.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">CreateModel<\/a> API creates a SageMaker model resource. The parameters specifie the docker image to use, model location in S3, IAM role to use, etc. See <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts\" rel=\"nofollow noreferrer\">How SageMaker Loads Your Model Artifacts<\/a>.<\/p>\n<h3>Docker image<\/h3>\n<p>Obviously you need the framework e.g. ScikitLearn, TensorFlow, PyTorch, etc that you used to train your model to get inferences. You need a docker image that has the framework, and HTTP front end to respond to the prediction calls. See <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\" rel=\"nofollow noreferrer\">Using the SageMaker Training and Inference Toolkits<\/a>.<\/p>\n<p>To build the image is not easy. Hence AWS provides pre-built images called <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\" rel=\"nofollow noreferrer\">AWS Deep Learning Containers<\/a> and available images are in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">Github<\/a>.<\/p>\n<p>If your framework and the version is listed there, you can use it as the image. Otherwise you need to build by yourself. See <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-mlops-workshop\/blob\/master\/lab\/01_CreateAlgorithmContainer\/01_Creating%20a%20Classifier%20Container.ipynb\" rel=\"nofollow noreferrer\">Building a docker container for training\/deploying our classifier<\/a>.<\/p>\n<h2>SageMaker Python SDK for Frameworks<\/h2>\n<p>Create SageMaker Model by yourself using API is hard. Hence AWS SageMaker Python SDK has provided utilities to create the SageMaker models for several frameworks. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/index.html\" rel=\"nofollow noreferrer\">Frameworks<\/a> for available frameworks. If it is not there, you may still be able to use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.FrameworkModel\" rel=\"nofollow noreferrer\">sagemaker.model.FrameworkModel<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\">Model<\/a> to load your trained model. For your case, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html\" rel=\"nofollow noreferrer\">Using Scikit-learn with the SageMaker Python SDK<\/a>.<\/p>\n<h3>model.tar.gz<\/h3>\n<p>For instance if you used PyTorch and save the model as model.pth. To load the model and the inference code to get the prediction from the model, you need to create a model.tar.gz file. The structure inside the model.tar.gz is explained in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">Model Directory Structure<\/a>. If you use Windows, beware of the CRLF to LF. AWS SageMaker runs in *NIX environment. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-the-directory-structure-for-your-model-files\" rel=\"nofollow noreferrer\">Create the directory structure for your model files<\/a>.<\/p>\n<pre><code>|- model.pth        # model file is inside \/ directory.\n|- code\/            # Code artefacts must be inside \/code\n  |- inference.py   # Your inference code for the framework\n  |- requirements.txt  # only for versions 1.3.1 and higher. Name must be &quot;requirements.txt&quot;\n<\/code><\/pre>\n<p>Save the tar.gz file in S3. Make sure of the IAM role to access the S3 bucket and objects.<\/p>\n<h3>Loading model and get inference<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-a-pytorchmodel-object\" rel=\"nofollow noreferrer\">Create a PyTorchModel object<\/a>. When instantiating the PyTorchModel class, SageMaker automatically selects the AWS Deep Learning Container image for PyTorch for the version specified in <strong>framework_version<\/strong>. If the image for the version does not exist, then it fails. This has not been documented in AWS but need to be aware of. SageMaker then internally calls the CreateModel API with the S3 model file location and the AWS Deep Learning Container image URL.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nrole = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\nmodel_file = &quot;s3:\/\/YOUR_BUCKET\/YOUR_FOLDER\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = PyTorchModel(\n    model_data=model_file,\n    name='YOUR_MODEL_NAME_WHATEVER',\n    role=role,\n    entry_point='inference.py',\n    source_dir='code',              # Location of the inference code\n    framework_version='1.5.0',      # Availble AWS Deep Learning PyTorch container version must be specified\n    predictor_cls=AnalysisClass     # To specify the HTTP request body format (application\/json)\n)\n\npredictor = model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\ntest_data = {&quot;body&quot;: &quot;YOUR PREDICTION REQUEST&quot;}\nprediction = predictor.predict(test_data)\n<\/code><\/pre>\n<p>By default, SageMaker uses NumPy as the serialization format. To be able to use JSON, need to specify the serializer and content_type. Instead of using RealTimePredictor class, you can specify them to predictor.<\/p>\n<pre><code>predictor.serializer=json_serializer\npredictor.predict(test_data)\n<\/code><\/pre>\n<p>Or<\/p>\n<pre><code>predictor.serializer=None # As the serializer is None, predictor won't serialize the data\nserialized_test_data=json.dumps(test_data) \npredictor.predict(serialized_test_data)\n<\/code><\/pre>\n<h3>Inference code sample<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-input\" rel=\"nofollow noreferrer\">Process Model Input<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#get-predictions-from-a-pytorch-model\" rel=\"nofollow noreferrer\">Get Predictions from a PyTorch Model<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-output\" rel=\"nofollow noreferrer\">Process Model Output<\/a>. The prediction request is sent as JSON in HTTP request body in this example.<\/p>\n<pre><code>import os\nimport sys\nimport datetime\nimport json\nimport torch\nimport numpy as np\n\nCONTENT_TYPE_JSON = 'application\/json'\n\ndef model_fn(model_dir):\n    # SageMaker automatically load the model.tar.gz from the S3 and \n    # mount the folders inside the docker container. The  'model_dir'\n    # points to the root of the extracted tar.gz file.\n\n    model_path = f'{model_dir}\/'\n    \n    # Load the model\n    # You can load whatever from the Internet, S3, wherever &lt;--- Answer to your Question\n    # NO Need to use the model in tar.gz. You can place a dummy model file.\n    ...\n\n    return model\n\n\ndef predict_fn(input_data, model):\n    # Do your inference\n    ...\n\ndef input_fn(serialized_input_data, content_type=CONTENT_TYPE_JSON):\n    input_data = json.loads(serialized_input_data)\n    return input_data\n\n\ndef output_fn(prediction_output, accept=CONTENT_TYPE_JSON):\n    if accept == CONTENT_TYPE_JSON:\n        return json.dumps(prediction_output), accept\n    raise Exception('Unsupported content type') \n<\/code><\/pre>\n<h2>Related<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">Using Models Trained Outside of Amazon SageMaker\n<\/a><\/li>\n<\/ul>\n<h2>Note<\/h2>\n<p>SageMaker team keeps changing the implementations and the documentations are frequently obsolete. When you are sure you did follow the documents and it does not work, obsolete documentation is quite likely. In such case, need to clarify with AWS support, or open an issue in the Github.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1630389412780,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65168915",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1607293074048,
        "Question_original_content":"load train sklearn model serv infer try deploi model train sklearn endpoint serv api predict want us deploi server model serialis joblib blog read python document show sklearn model train order deploi go document learn let user load serialis model store shown def model model dir clf joblib load path join model dir model joblib return clf document sai argument model dir inject directori model file sub directori save save mount model function return model object model serv mean train wai specifi locat serialis model serialis load model us infer edit code answer applic got error try deploi notebook studio believ scream train wasn valueerror traceback recent predictor model deploi initi instanc count instanc type xlarg opt conda lib python site packag estim deploi self initi instanc count instanc type serial deseri acceler type endpoint us compil model wait model km kei data captur config tag kwarg remov kwarg updat endpoint kwarg self ensur latest train job self ensur base job default base self base job opt conda lib python site packag estim ensur latest train job self error messag self latest train job rais valueerror error messag delet endpoint remov function delet endpoint valueerror estim associ train job code import import execut role pytorch import pytorchmodel sklearn import sklearn predictor import realtimepredictor json serial json deseri role execut role iam role run access ecr model file manual bucket model artifact model tar tar suffix class analysisclass realtimepredictor def init self endpoint session super init endpoint session session serial json serial deseri json deseri abl us json serial content type applic json abl send json http bodi model sklearn model data model file entri point infer try role role sourc dir code framework version instanc count instanc type xlarg predictor cl analysisclass predictor model deploi initi instanc count instanc type xlarg",
        "Question_preprocessed_content":"load train sklearn model serv infer try deploi model train sklearn endpoint serv api predict want us deploi server model serialis blog read python document show sklearn model train order deploi go document learn let user load serialis model store shown document sai argument inject directori model file save save mount model function return model object model serv mean train wai specifi locat serialis model load model us infer edit code answer applic got error try deploi notebook studio believ scream train wasn code",
        "Question_gpt_summary_original":"The user is trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions using AWS SageMaker. However, the user has encountered challenges as every blog and SageMaker documentation suggests that the sklearn model has to be trained on SageMaker in order to be deployed in SageMaker. The user is looking for a way to specify the S3 location of the serialized model and have SageMaker de-serialize (or load) the model from S3 and use it for inference. The user has also encountered an error when trying to deploy the model from the notebook of SageMaker studio, which suggests that training was not done on SageMaker.",
        "Question_gpt_summary":"user try deploi model train sklearn endpoint serv api predict user encount challeng blog document suggest sklearn model train order deploi user look wai specifi locat serial model serial load model us infer user encount error try deploi model notebook studio suggest train",
        "Answer_original_content":"ye aw document focus end end train deploy make impress train aw document exampl clear separ train estim save load model deploy model endpoint model need creat aw model resourc refer model train aw model cloudform document explain aw resourc need createmodel api creat model resourc paramet specifi docker imag us model locat iam role us load model artifact docker imag obvious need framework scikitlearn tensorflow pytorch train model infer need docker imag framework http end respond predict call infer toolkit train infer toolkit build imag easi aw provid pre built imag call aw deep learn contain avail imag github framework version list us imag need build build docker contain train deploi classifi python sdk framework creat model api hard python sdk provid util creat model framework framework avail framework abl us model frameworkmodel model load train model case scikit learn python sdk model tar instanc pytorch save model model pth load model infer code predict model need creat model tar file structur insid model tar explain model directori structur us window bewar crlf run nix environ creat directori structur model file model pth model file insid directori code code artefact insid code infer infer code framework requir txt version higher requir txt save tar file sure iam role access bucket object load model infer creat pytorchmodel object instanti pytorchmodel class automat select aw deep learn contain imag pytorch version specifi framework version imag version exist fail document aw need awar intern call createmodel api model file locat aw deep learn contain imag url import import execut role pytorch import pytorchmodel predictor import realtimepredictor json serial json deseri role execut role iam role run access ecr model file bucket folder model tar tar suffix class analysisclass realtimepredictor def init self endpoint session super init endpoint session session serial json serial deseri json deseri abl us json serial content type applic json abl send json http bodi model pytorchmodel model data model file model role role entri point infer sourc dir code locat infer code framework version availbl aw deep learn pytorch contain version specifi predictor cl analysisclass specifi http request bodi format applic json predictor model deploi initi instanc count instanc type xlarg test data bodi predict request predict predictor predict test data default us numpi serial format abl us json need specifi serial content type instead realtimepredictor class specifi predictor predictor serial json serial predictor predict test data predictor serial serial predictor won serial data serial test data json dump test data predictor predict serial test data infer code sampl process model input predict pytorch model process model output predict request sent json http request bodi exampl import import sy import datetim import json import torch import numpi content type json applic json def model model dir automat load model tar mount folder insid docker contain model dir point root extract tar file model path model dir load model load internet answer question need us model tar place dummi model file return model def predict input data model infer def input serial input data content type content type json input data json load serial input data return input data def output predict output accept content type json accept content type json return json dump predict output accept rais except unsupport content type relat model train outsid note team keep chang implement document frequent obsolet sure follow document work obsolet document like case need clarifi aw support open issu github",
        "Answer_preprocessed_content":"ye aw document focus train deploy make impress train aw document exampl clear separ train estim save load model deploy model endpoint model need creat aw model resourc refer model train aw model cloudform document explain aw resourc need createmodel api creat model resourc paramet specifi docker imag us model locat iam role us load model artifact docker imag obvious need framework scikitlearn tensorflow pytorch train model infer need docker imag framework http end respond predict call infer toolkit train infer toolkit build imag easi aw provid imag call aw deep learn contain avail imag github framework version list us imag need build build docker contain classifi python sdk framework creat model api hard python sdk provid util creat model framework framework avail framework abl us model load train model case python sdk instanc pytorch save model load model infer code predict model need creat file structur insid explain model directori structur us window bewar crlf run nix environ creat directori structur model file save file sure iam role access bucket object load model infer creat pytorchmodel object instanti pytorchmodel class automat select aw deep learn contain imag pytorch version specifi imag version exist fail document aw need awar intern call createmodel api model file locat aw deep learn contain imag url default us numpi serial format abl us json need specifi serial instead realtimepredictor class specifi predictor infer code sampl process model input predict pytorch model process model output predict request sent json http request bodi exampl relat model train outsid note team keep chang implement document frequent obsolet sure follow document work obsolet document like case need clarifi aw support open issu github",
        "Answer_gpt_summary_original":"possible solutions to deploying a model trained with sklearn to an endpoint and serving it as an api for predictions without having to train it on  include creating an aws model resource, specifying the docker image to use, model location in s3, iam role to use, etc. aws provides pre-built images called aws deep learning containers, and available images are in github. python sdk for frameworks create model by yourself using api is hard. hence python sdk has provided utilities to create the models for several frameworks. to load the model and the inference code to get the prediction from the model, you need to create a model.tar.gz file. the structure inside the model.tar.gz is explained in the model directory structure. save the tar.gz file in s3. make sure of the iam role to access the s3 bucket and objects.",
        "Answer_gpt_summary":"possibl solut deploi model train sklearn endpoint serv api predict have train includ creat aw model resourc specifi docker imag us model locat iam role us aw provid pre built imag call aw deep learn contain avail imag github python sdk framework creat model api hard python sdk provid util creat model framework load model infer code predict model need creat model tar file structur insid model tar explain model directori structur save tar file sure iam role access bucket object"
    },
    {
        "Question_id":null,
        "Question_title":"Waiting for wandb.init",
        "Question_body":"<p>Hi, the code ran 3 days agoand it worked, but after I changed some variables and it failed with: \u201cwandb: W&amp;B API key is configured. Use <code>wandb login --relogin<\/code> to force relogin wandb: - Waiting for wandb.init()\u2026wandb: \\ Waiting for wandb.init()\u2026wandb: | Waiting for wandb.init()\u2026wandb: \/ Waiting for wandb.init()\u2026wandb: - Waiting for wandb.init()\u2026Traceback (most recent call last):\u201d. Now after I changed it back the error persists.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1673289804453,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":143.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/waiting-for-wandb-init\/3658",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-10T00:26:11.354Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/gammill-chance\">@gammill-chance<\/a> , happy to help. Could you  expand a bit on your experiment setup and which arguments are you setting in <code>wandb.init()<\/code>, additionally:<\/p>\n<ul>\n<li>Does the output of <code>wandb status<\/code> correspond correctly to the <code>entity<\/code>,<code> host_url<\/code>, and is the api key set there correctly?<\/li>\n<li>Try deleting the <code>~\/.netrc<\/code>file that stored your wandb login info and api key and relogin<\/li>\n<li>Uninstall\/reinstall wandb<\/li>\n<\/ul>\n<p>If the above doesn\u2019t work please provide the <code>debug.log<\/code> and <code>debug-internal.log<\/code> files for the failing runs located in the wandb folder of your project directory and we will take a closer look.<\/p>",
                "Answer_score":20.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-10T16:09:54.716Z",
                "Answer_body":"<p>I am new to the Wandb coding, but for the init it is as follows,<br>\nwandb.init (project=\u2018U-Net\u2019,entity= \u2018gammill-chance\u2019, resume=\u2018allow\u2019,  anonymous=\u2018allow\u2019, name= \u2018check_8\u2019).<br>\nI will add that when logging in to wandb in the command console the command wandb login key, has not been working, so instead I used wandb.login(). Additionally I did not know about wandb status and I decided to run the neural network without wandb for the time being, so I can respond to that hopefully on Friday. After that I will try what you suggested and I will get back to you when I know if it works. Thank you!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-18T20:05:10.461Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/gammill-chance\">@gammill-chance<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"wait init code ran dai agoand work chang variabl fail api kei configur us login relogin forc relogin wait init wait init wait init wait init wait init traceback recent chang error persist",
        "Question_preprocessed_content":"wait init code ran dai agoand work chang variabl fail api kei configur us forc relogin wait init wait init wait init wait init wait init traceback chang error persist",
        "Question_gpt_summary_original":"The user is encountering challenges with the wandb.init() function after changing some variables in the code. The error message suggests that the API key needs to be reconfigured, but even after changing the variables back, the error persists.",
        "Question_gpt_summary":"user encount challeng init function chang variabl code error messag suggest api kei need reconfigur chang variabl error persist",
        "Answer_original_content":"gammil chanc happi help expand bit experi setup argument set init addition output statu correspond correctli entiti host url api kei set correctli try delet netrcfil store login info api kei relogin uninstal reinstal doesnt work provid debug log debug intern log file fail run locat folder project directori closer look new code init follow init project net entiti gammil chanc resum allow anonym allow check add log command consol command login kei work instead login addition know statu decid run neural network time respond hopefulli fridai try suggest know work thank gammil chanc heard go close request like open convers let know",
        "Answer_preprocessed_content":"happi help expand bit experi setup argument set addition output correspond correctli api kei set correctli try delet file store login info api kei relogin doesnt work provid file fail run locat folder project directori closer look new code init follow init add log command consol command login kei work instead login addition know statu decid run neural network time respond hopefulli fridai try suggest know work thank heard go close request like convers let know",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer are: \n- checking the experiment setup and arguments being set in .init()\n- verifying if the output of status corresponds correctly to the entity, host_url, and if the api key is set correctly\n- trying to delete the ~\/.netrcfile that stored login info and api key and re-login\n- uninstalling\/reinstalling if the above doesn't work\n- providing the debug.log and debug-internal.log files for the failing runs located in the folder of the project directory\n- using .login() instead of the command login key\n- running the neural network without status for the time being\n- re-opening the conversation if needed\n\noverall, the answer provides some troubleshooting steps to resolve the issue with the code being stuck in a loop of \"waiting for .init()\" errors.",
        "Answer_gpt_summary":"possibl solut mention answer check experi setup argument set init verifi output statu correspond correctli entiti host url api kei set correctli try delet netrcfil store login info api kei login uninstal reinstal work provid debug log debug intern log file fail run locat folder project directori login instead command login kei run neural network statu time open convers need overal answer provid troubleshoot step resolv issu code stuck loop wait init error"
    },
    {
        "Question_id":null,
        "Question_title":"Why can't I delete multiple runs in W&B at once",
        "Question_body":"<p>As shown in the picture below, I selected multiple runs but still can\u2019t hit delete, which was ok before, did I set something wrong?<br>\nThank you for your help!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe.jpeg\" data-download-href=\"\/uploads\/short-url\/1tihGWyfSAk2zp9Y2O4VmZiNf3g.jpeg?dl=1\" title=\"20211218195831\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe_2_690x283.jpeg\" alt=\"20211218195831\" data-base62-sha1=\"1tihGWyfSAk2zp9Y2O4VmZiNf3g\" width=\"690\" height=\"283\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe_2_690x283.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe_2_1035x424.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/0a51eb43f9c351b2492802fcef3dd96b673771fe_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">20211218195831<\/span><span class=\"informations\">1072\u00d7441 33.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1640127473795,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":205.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/why-cant-i-delete-multiple-runs-in-w-b-at-once\/1585",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-22T13:30:12.386Z",
                "Answer_body":"<p>Hey there, this is a known issue that we are working on right now. I\u2019ll notify you once this is fixed.<\/p>",
                "Answer_score":11.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-28T17:01:19.641Z",
                "Answer_body":"<p>Hey there, can you check if the issue still persists?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-30T01:10:49.998Z",
                "Answer_body":"<p>I\u2019m sorry I just saw your reply, this problem has been solved,  thanks!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-28T01:11:08.297Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"delet multipl run shown pictur select multipl run hit delet set wrong thank help",
        "Question_preprocessed_content":"delet multipl run shown pictur select multipl run hit delet set wrong thank help",
        "Question_gpt_summary_original":"The user is facing a challenge in deleting multiple runs at once in W&B, despite selecting them. They are seeking help to understand if they have set something wrong.",
        "Question_gpt_summary":"user face challeng delet multipl run despit select seek help understand set wrong",
        "Answer_original_content":"hei known issu work right ill notifi fix hei check issu persist sorri saw repli problem solv thank topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei known issu work right ill notifi fix hei check issu persist sorri saw repli problem solv thank topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there was a known issue with w&b where users were unable to delete multiple runs despite selecting them. the issue has been fixed and the user is advised to check if the problem still persists. the topic was automatically closed after 60 days and new replies are no longer allowed.",
        "Answer_gpt_summary":"known issu user unabl delet multipl run despit select issu fix user advis check problem persist topic automat close dai new repli longer allow"
    },
    {
        "Question_id":71221741.0,
        "Question_title":"ValidationException in Sagemaker pipeline creation",
        "Question_body":"<p>I am new to Sagmaker. I am creating a pipeline in sagemaker where I initialize the number of epochs as a pipeline parameter. But when I upsert, it shows this error.\nCheck the following code for reference, please.<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\npipeline = Pipeline(\nname=f&quot;a_name&quot;,\nparameters=[\n    training_instance_type,\n    training_instance_count,\n    epoch_count,\n    hugging_face_model_name,\n    endpoint_instance_type,\n    endpoint_instance_type_alternate,\n],\nsteps=[step_train, step_register, step_deploy_lambda],\nsagemaker_session=sagemaker_session,\n<\/code><\/pre>\n<p>)<\/p>\n<p>Error - ---<\/p>\n<pre><code>---------------------------------------------------------------------------\nClientError                               Traceback (most recent call last)\n&lt;ipython-input-54-138a517611f0&gt; in &lt;module&gt;\n----&gt; 1 pipeline.upsert(role_arn=role)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in upsert(self, role_arn, description, tags, parallelism_config)\n    217         &quot;&quot;&quot;\n    218         try:\n--&gt; 219             response = self.create(role_arn, description, tags, parallelism_config)\n    220         except ClientError as e:\n    221             error = e.response[&quot;Error&quot;]\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in create(self, role_arn, description, tags, parallelism_config)\n    119             Tags=tags,\n    120         )\n--&gt; 121         return self.sagemaker_session.sagemaker_client.create_pipeline(**kwargs)\n    122 \n    123     def _create_args(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _api_call(self, *args, **kwargs)\n    389                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)\n    390             # The &quot;self&quot; in this scope is referring to the BaseClient.\n--&gt; 391             return self._make_api_call(operation_name, kwargs)\n    392 \n    393         _api_call.__name__ = str(py_operation_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _make_api_call(self, operation_name, api_params)\n    717             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)\n    718             error_class = self.exceptions.from_code(error_code)\n--&gt; 719             raise error_class(parsed_response, operation_name)\n    720         else:\n    721             return parsed_response\n\nClientError: An error occurred (ValidationException) when calling the CreatePipeline operation: Cannot assign property reference [Parameters.EpochCount] to argument of type [String]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645534662633,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":608.0,
        "Answer_body":"<p>I replace<\/p>\n<pre><code>epoch_count = ParameterInteger(name=&quot;EpochCount&quot;, default_value=1)\n<\/code><\/pre>\n<p>with<\/p>\n<pre><code>epoch_count = ParameterString(name=&quot;EpochCount&quot;, default_value=&quot;1&quot;)\n<\/code><\/pre>\n<p>And it works. Maybe we can only use an integer in pipeline parameters from the sagemaker notebook. But epoch_count is being used in the docker container, which is not directly something of Sagemaker, and that's my understanding.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1649157854310,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71221741",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645641627683,
        "Question_original_content":"validationexcept pipelin creation new sagmak creat pipelin initi number epoch pipelin paramet upsert show error check follow code refer epoch count parameterinteg epochcount default valu pipelin pipelin paramet train instanc type train instanc count epoch count hug face model endpoint instanc type endpoint instanc type altern step step train step regist step deploi lambda session session error clienterror traceback recent pipelin upsert role arn role opt conda lib python site packag workflow pipelin upsert self role arn descript tag parallel config try respons self creat role arn descript tag parallel config clienterror error respons error opt conda lib python site packag workflow pipelin creat self role arn descript tag parallel config tag tag return self session client creat pipelin kwarg def creat arg opt conda lib python site packag botocor client api self arg kwarg accept keyword argument oper self scope refer basecli return self api oper kwarg api str oper opt conda lib python site packag botocor client api self oper api param error code pars respons error code error class self except code error code rais error class pars respons oper return pars respons clienterror error occur validationexcept call createpipelin oper assign properti refer paramet epochcount argument type string",
        "Question_preprocessed_content":"validationexcept pipelin creation new sagmak creat pipelin initi number epoch pipelin paramet upsert show error check follow code refer error",
        "Question_gpt_summary_original":"The user encountered a ValidationException error while creating a pipeline in Sagemaker. The error occurred when the user initialized the number of epochs as a pipeline parameter and tried to upsert. The error message indicates that the property reference [Parameters.EpochCount] cannot be assigned to an argument of type [String].",
        "Question_gpt_summary":"user encount validationexcept error creat pipelin error occur user initi number epoch pipelin paramet tri upsert error messag indic properti refer paramet epochcount assign argument type string",
        "Answer_original_content":"replac epoch count parameterinteg epochcount default valu epoch count parameterstr epochcount default valu work mayb us integ pipelin paramet notebook epoch count docker contain directli understand",
        "Answer_preprocessed_content":"replac work mayb us integ pipelin paramet notebook docker contain directli understand",
        "Answer_gpt_summary_original":"solution: replace the parameterinteger with parameterstring for the epoch count in the pipeline. it seems that only integers can be used as pipeline parameters from the notebook. however, the epoch count is being used in the docker container, which is not directly related to the notebook.",
        "Answer_gpt_summary":"solut replac parameterinteg parameterstr epoch count pipelin integ pipelin paramet notebook epoch count docker contain directli relat notebook"
    },
    {
        "Question_id":null,
        "Question_title":"(unsolved) How to set spacing between monitoring points(default 30s)?",
        "Question_body":"<p>The default interval of monitoring points for GPU utilization is the 30s. How can I set this time?<\/p>\n<p>In my model, the latency of one batch is shorter than the 30s. So I don\u2019t think this interval is suitable.<br>\nCan I modify it by setting any variable?<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_creation_time":1642469560749,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":219.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/unsolved-how-to-set-spacing-between-monitoring-points-default-30s\/1780",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-18T04:08:20.421Z",
                "Answer_body":"<p>I modify the two function <code>sample_rate_seconds(self)<\/code> and <code>sample_rate_average(self)<\/code> in <code>stats.py<\/code>. The path of <code>stats.py<\/code> is <code>&lt;Python_Path&gt;\\Lib\\site-packages\\wandb\\sdk\\internal\\stats.py<\/code>.<\/p>\n<p>And the default value is 2 and 15. I don\u2019t know how it works, so I set it as small as possible.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-18T05:26:57.685Z",
                "Answer_body":"<p>There are some notes in code,<\/p>\n<blockquote>\n<p>\u201c\u201d\u201cSample system stats every this many seconds, defaults to 2, min is 0.5\"\u201d\"<br>\n\u201c\u201d\u201cThe number of samples to average before pushing, defaults to 15 valid range (2:30)\u201d\"\"<\/p>\n<\/blockquote>\n<p>Finally, the interval is 1s. Have any other way to make the interval shorter? Like 0.5s?<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T12:01:14.369Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-22T15:14:20.158Z",
                "Answer_body":"<p>Hi Boqian, according to lines 85-87 in this code (<a href=\"https:\/\/github.com\/wandb\/client-ng\/blob\/fa51b2fbaf12a22a3e48f8c9c2c157642986946b\/wandb\/internal\/stats.py#L85\" class=\"inline-onebox-loading\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/wandb\/client-ng\/blob\/fa51b2fbaf12a22a3e48f8c9c2c157642986946b\/wandb\/internal\/stats.py#L85<\/a>) we can\u2019t return anything faster than one second. However, if you\u2019re willing to edit the client code on your instance to where you make the interval shorter on your machine you can.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-22T16:43:44.280Z",
                "Answer_body":"",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-25T16:45:53.277Z",
                "Answer_body":"<p>Hi Boqian,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-28T18:07:46.648Z",
                "Answer_body":"<p>Hi Boqian, I\u2019m going to close this request because we haven\u2019t heard back from you but let me know if you need any further help!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-02T11:50:58.796Z",
                "Answer_body":"<p>Thanks a lot!<\/p>\n<p>I see, but I am still confused about why the min value is 0.5. Why not set it shorter? like 0.1?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-02T11:56:35.323Z",
                "Answer_body":"<p>Do you have any idea about how to show the GPU utilization per 0.1 seconds?<br>\nI think it is hard to continuously modify the client code to achieve that.<br>\nThanks!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"unsolv set space monitor point default default interv monitor point gpu util set time model latenc batch shorter dont think interv suitabl modifi set variabl",
        "Question_preprocessed_content":"set space monitor point default interv monitor point gpu util set time model latenc batch shorter dont think interv suitabl modifi set variabl",
        "Question_gpt_summary_original":"The user is facing a challenge with the default interval of monitoring points for GPU utilization, which is set at 30 seconds. The user's model has a shorter batch latency, making the 30-second interval unsuitable. The user is seeking a way to modify the interval by setting a variable.",
        "Question_gpt_summary":"user face challeng default interv monitor point gpu util set second user model shorter batch latenc make second interv unsuit user seek wai modifi interv set variabl",
        "Answer_original_content":"modifi function sampl rate second self sampl rate averag self stat path stat lib site packag sdk intern stat default valu dont know work set small possibl note code sampl stat second default min number sampl averag push default valid rang final interv wai interv shorter like topic automat close dai repli new repli longer allow boqian accord line code http github com client blob fabfbafaaefcccb intern stat return faster second your will edit client code instanc interv shorter machin boqian want follow support request heard let know assist issu resolv boqian go close request havent heard let know need help thank lot confus min valu set shorter like idea gpu util second think hard continu modifi client code achiev thank",
        "Answer_preprocessed_content":"modifi function path default valu dont know work set small possibl note code sampl stat second default min number sampl averag push default valid rang final interv wai interv shorter like topic automat close dai repli new repli longer allow boqian accord line code return faster second your will edit client code instanc interv shorter machin boqian want follow support request heard let know assist issu resolv boqian go close request havent heard let know need help thank lot confus min valu set shorter like idea gpu util second think hard continu modifi client code achiev thank",
        "Answer_gpt_summary_original":"possible solutions to modify the default 30s interval of monitoring points for gpu utilization in the model are to modify the sample_rate_seconds and sample_rate_average functions in stats.py, which are located in the python_path\\lib\\site-packages\\sdk\\internal\\stats.py path. the default values are 2 and 15, respectively, and the minimum valid range is 2 seconds. however, the interval cannot be shorter than 1 second due to the code's limitations. editing the client code on the user's instance to make the interval shorter is possible, but it requires continuous modification.",
        "Answer_gpt_summary":"possibl solut modifi default interv monitor point gpu util model modifi sampl rate second sampl rate averag function stat locat python path lib site packag sdk intern stat path default valu respect minimum valid rang second interv shorter second code limit edit client code user instanc interv shorter possibl requir continu modif"
    },
    {
        "Question_id":null,
        "Question_title":"AWS Sagemaker: UnexpectedStatusException: Compilation job Failed. Reason: ClientError: InputConfiguration: Please make sure input config is correct - Input 1 of node StatefulPartitionedCall was passed",
        "Question_body":"I am trying to convert a pre-trained (NASNETMobile) model into AWS Neo Optimized model. I am flowing [https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/] this link. Only difference is that I am using Tensorflow 2.6 and Python 3.7.10. Other dependencies are boto 2.49.0 boto3 1.20.30 botocore 1.23.30 keras 2.7.0 Keras-Preprocessing 1.1.2 numpy 1.19.5 safety 1.10.3 sagemaker 2.74.0 sagemaker-pyspark 1.4.2 tensorflow 2.6.0 tensorflow-cpu 2.6.0 tensorflow-estimator 2.6.0 tensorflow-gpu 2.6.0 tensorflow-io-gcs-filesystem 0.23.1 tensorflow-serving-api 2.6.0\n\nAs per link, I got stuck in step 7. Getting error->\n\nfrom tensorflow.keras.applications import nasnet\nfrom keras.preprocessing import image\nimport h5py\nimport numpy as np\nfrom keras.models import load_model\n\n# Load h5 file\nloaded_model = load_model('myy_model.h5')\n\nmodel_version = '2'\nexport_dir = 'export\/Servo\/' + model_version\n\n# Save in pb format\nloaded_model.save(export_dir)\n\n# Save as tar file\nimport tarfile\nmodel_archive = 'model.tar.gz'\nwith tarfile.open(model_archive, mode='w:gz') as archive:\n    archive.add('export', recursive=True)\n\nfrom sagemaker import get_execution_role\nfrom sagemaker import Session\n\nrole = get_execution_role()\nsess = Session()\nregion = sess.boto_region_name\nbucket = sess.default_bucket()\n\nmodel_data = sess.upload_data(path=model_archive, key_prefix='model')\n\nfrom sagemaker.tensorflow import TensorFlowModel\n\nsagemaker_model = TensorFlowModel(model_data=model_data, \n                      framework_version=tf_framework_version,\n                      role=role)\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n\n#Load  jpeg image from local and set target size to 224 x 224\nimg = image.load_img('eagle.jpg', target_size=(224, 224))\n\n#convert image to array\ninput_img = image.img_to_array(img)\ninput_img = np.expand_dims(input_img, axis=0)\ninput_img = nasnet.preprocess_input(input_img)\n\n\npredict_img = predictor.predict(input_img)\npredict_img\n\n\/\/ Till this point every thing is working\n\ninstance_family = 'ml_c5'\nframework = 'tensorflow'\ncompilation_job_name = 'keras-compile-16'\n# output path for compiled model artifact\ncompiled_model_path = 's3:\/\/{}\/{}\/output'.format(bucket, compilation_job_name)\n\ndata_shape = {'input_2':[1,224,224,3]}\n\n\/\/ Failing in below line\noptimized_estimator = sagemaker_model.compile(target_instance_family=instance_family,\n                                         input_shape=data_shape,\n                                         job_name=compilation_job_name,\n                                         role=role,\n                                         framework=framework,\n                                         framework_version=tf_framework_version,\n                                         output_path=compiled_model_path\n                                        )\n\n\n\ndata_shape = {'input_2':[1,224,224,3]} \"input_2\" is right name. have checked in model plus https:\/\/netron.app\/\n\nFull Error Logs:\n\nUnexpectedStatusException Traceback (most recent call last) <ipython-input-140-c16ae7ba24de> in <module> 5 framework=framework, 6 framework_version=tf_framework_version, ----> 7 output_path=compiled_model_path 8 )\n\n~\/anaconda3\/envs\/tensorflow2_p37\/lib\/python3.7\/site-packages\/sagemaker\/model.py in compile(self, target_instance_family, input_shape, output_path, role, tags, job_name, compile_max_run, framework, framework_version, target_platform_os, target_platform_arch, target_platform_accelerator, compiler_options) 659 ) 660 self.sagemaker_session.compile_model(**config) --> 661 job_status = self.sagemaker_session.wait_for_compilation_job(job_name) 662 self.model_data = job_status[\"ModelArtifacts\"][\"S3ModelArtifacts\"] 663 if target_instance_family is not None:\n\n~\/anaconda3\/envs\/tensorflow2_p37\/lib\/python3.7\/site-packages\/sagemaker\/session.py in wait_for_compilation_job(self, job, poll) 3224 \"\"\" 3225 desc = _wait_until(lambda: _compilation_job_status(self.sagemaker_client, job), poll) -> 3226 self._check_job_status(job, desc, \"CompilationJobStatus\") 3227 return desc 3228\n\n~\/anaconda3\/envs\/tensorflow2_p37\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name) 3341 ), 3342 allowed_statuses=[\"Completed\", \"Stopped\"], -> 3343 actual_status=status, 3344 ) 3345\n\nUnexpectedStatusException: Error for Compilation job keras-compile-14: Failed. Reason: ClientError: InputConfiguration: Please make sure input config is correct - Input 1 of node StatefulPartitionedCall was passed float from stem_conv1\/kernel:0 incompatible with expected resource.\n\nThanks in Advance",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643795001851,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":28.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvCjrBvqAQzGwEyZjXDwE7g\/aws-sagemaker-unexpected-status-exception-compilation-job-failed-reason-client-error-input-configuration-please-make-sure-input-config-is-correct-input-1-of-node-stateful-partitioned-call-was-passed",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-05T15:00:55.100Z",
                "Answer_score":0,
                "Answer_body":"It is hard to say if the input tensor name is wrong without having access to the notebook and the other artifacts but I guess the name of the input tensor might be wrong. TF uses dynamic names for the tensors when you don't specify and the sequence is incremented each time you load the model using the same runtime session. It starts with input_1 then input_2 and so on. Another point is that you're using the estimator to invoke Neo. This library is sometimes outdated. I prefer to use boto3 instead. I wrote a sample code to test the compilation. The following code is working perfectly with your model:\n\n## Load a pre-trained Keras model and export in Tensorflow format\nimport tensorflow as tf\nfrom tensorflow.keras.applications import nasnet\nimport numpy as np\nmodel = tf.keras.applications.nasnet.NASNetMobile(weights='imagenet')\nx = tf.random.uniform((1, 224, 224, 3))\ny = model(x)\nexport_dir = 'export\/1'\ntf.saved_model.save(model, export_dir)\n\n## Put the model into a tar ball and send to S3\nimport sagemaker\nimport tarfile\nimport io\n\nmodel_archive = 'model.tar.gz'\nmodel_name='nasnet-mobile'\nimg_size=224\nsagemaker_session = sagemaker.Session()\ndefault_bucket = sagemaker_session.default_bucket()\n\nwith io.BytesIO() as f:\n    with tarfile.open(fileobj=f, mode=\"w:gz\") as tar:\n        tar.add('export')\n        tar.list()\n    f.seek(0)\n    s3_uri = sagemaker_session.upload_string_as_file_body(f.read(), default_bucket, f\"models\/{model_name}\/model.tar.gz\")\n    print(s3_uri)\n\n## Now kick-off a compilation job\nimport time\nimport boto3\nimport sagemaker\n\nrole = sagemaker.get_execution_role()\nsm_client = boto3.client('sagemaker')\n\nframework='tensorflow'\nimg_size=224\ninput_shape=f\"1,{img_size},{img_size},3\"\ncompilation_job_name = f'{model_name}-{framework}-{int(time.time()*1000)}'\n\nsm_client.create_compilation_job(\n    CompilationJobName=compilation_job_name,\n    RoleArn=role,\n    InputConfig={\n        'S3Uri': s3_uri,\n        'DataInputConfig': f'{{\"{model.layers[0].name}\": [{input_shape}]}}',\n        'Framework': framework.upper(),\n        'FrameworkVersion': '2.4'\n    },\n    OutputConfig={\n        'S3OutputLocation': f's3:\/\/{default_bucket}\/{model_name}-{framework}\/optimized\/',\n        'TargetDevice': 'ml_c5',\n        # Comment or change the following line depending on your edge device\n        # Jetson Xavier: sm_72; Jetson Nano: sm_53\n        #'CompilerOptions': '{\"trt-ver\": \"7.1.3\", \"cuda-ver\": \"10.2\", \"gpu-code\": \"sm_72\"}' # Jetpack 4.4.1\n    },\n    StoppingCondition={ 'MaxRuntimeInSeconds': 18000 }\n)\nwhile True:\n    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n        print('Running...')\n    else:\n        print(resp['CompilationJobStatus'], compilation_job_name)\n        break\n    time.sleep(5)\n\n\nJust notice that I'm using FrameworkVersion = 2.4. There is not information about that in your sample.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"unexpectedstatusexcept compil job fail reason clienterror inputconfigur sure input config correct input node statefulpartitionedcal pass try convert pre train nasnetmobil model aw neo optim model flow http aw amazon com blog machin learn deploi train kera tensorflow model amazon link differ tensorflow python depend boto boto botocor kera kera preprocess numpi safeti pyspark tensorflow tensorflow cpu tensorflow estim tensorflow gpu tensorflow gc filesystem tensorflow serv api link got stuck step get error tensorflow kera applic import nasnet kera preprocess import imag import hpy import numpi kera model import load model load file load model load model myi model model version export dir export servo model version save format load model save export dir save tar file import tarfil model archiv model tar tarfil open model archiv mode archiv archiv add export recurs true import execut role import session role execut role sess session region sess boto region bucket sess default bucket model data sess upload data path model archiv kei prefix model tensorflow import tensorflowmodel model tensorflowmodel model data model data framework version framework version role role predictor model deploi initi instanc count instanc type xlarg load jpeg imag local set target size img imag load img eagl jpg target size convert imag arrai input img imag img arrai img input img expand dim input img axi input img nasnet preprocess input input img predict img predictor predict input img predict img till point thing work instanc famili framework tensorflow compil job kera compil output path compil model artifact compil model path output format bucket compil job data shape input fail line optim estim model compil target instanc famili instanc famili input shape data shape job compil job role role framework framework framework version framework version output path compil model path data shape input input right check model plu http netron app error log unexpectedstatusexcept traceback recent framework framework framework version framework version output path compil model path anaconda env tensorflow lib python site packag model compil self target instanc famili input shape output path role tag job compil max run framework framework version target platform target platform arch target platform acceler compil option self session compil model config job statu self session wait compil job job self model data job statu modelartifact smodelartifact target instanc famili anaconda env tensorflow lib python site packag session wait compil job self job poll desc wait lambda compil job statu self client job poll self check job statu job desc compilationjobstatu return desc anaconda env tensorflow lib python site packag session check job statu self job desc statu kei allow status complet stop actual statu statu unexpectedstatusexcept error compil job kera compil fail reason clienterror inputconfigur sure input config correct input node statefulpartitionedcal pass float stem conv kernel incompat expect resourc thank advanc",
        "Question_preprocessed_content":"unexpectedstatusexcept compil job fail reason clienterror inputconfigur sure input config correct input node statefulpartitionedcal pass try convert model aw neo optim model flow link differ tensorflow python depend boto boto botocor kera numpi safeti pyspark tensorflow link got stuck step get import nasnet import imag import import numpi import load file save format save tar file import tarfil mode archiv recurs true import import session role sess session region bucket tensorflow import tensorflowmodel role role predictor load jpeg imag local set target size img convert imag arrai axi till point thing work framework tensorflow output path compil model artifact fail line role role framework framework right check model plu error log unexpectedstatusexcept traceback framework framework compil job poll desc job poll desc compilationjobstatu return desc job desc stop unexpectedstatusexcept error compil job fail reason clienterror inputconfigur sure input config correct input node statefulpartitionedcal pass float incompat expect resourc thank advanc",
        "Question_gpt_summary_original":"The user encountered an error while trying to convert a pre-trained model into an AWS Neo Optimized model using AWS Sagemaker. The error occurred during step 7 of the process and was related to the input configuration of the model. The error message indicated that the input configuration was incorrect and that the input passed to node StatefulPartitionedCall was incompatible with the expected resource. The user provided a detailed list of dependencies used in the process.",
        "Question_gpt_summary":"user encount error try convert pre train model aw neo optim model error occur step process relat input configur model error messag indic input configur incorrect input pass node statefulpartitionedcal incompat expect resourc user provid detail list depend process",
        "Answer_original_content":"hard input tensor wrong have access notebook artifact guess input tensor wrong us dynam name tensor specifi sequenc increment time load model runtim session start input input point estim invok neo librari outdat prefer us boto instead wrote sampl code test compil follow code work perfectli model load pre train kera model export tensorflow format import tensorflow tensorflow kera applic import nasnet import numpi model kera applic nasnet nasnetmobil weight imagenet random uniform model export dir export save model save model export dir model tar ball send import import tarfil import model archiv model tar model nasnet mobil img size session session default bucket session default bucket bytesio tarfil open fileobj mode tar tar add export tar list seek uri session upload string file bodi read default bucket model model model tar print uri kick compil job import time import boto import role execut role client boto client framework tensorflow img size input shape img size img size compil job model framework int time time client creat compil job compilationjobnam compil job rolearn role inputconfig suri uri datainputconfig model layer input shape framework framework upper frameworkvers outputconfig soutputloc default bucket model framework optim targetdevic comment chang follow line depend edg devic jetson xavier jetson nano compileropt trt ver cuda ver gpu code jetpack stoppingcondit maxruntimeinsecond true resp client compil job compilationjobnam compil job resp compilationjobstatu start inprogress print run print resp compilationjobstatu compil job break time sleep notic frameworkvers inform sampl",
        "Answer_preprocessed_content":"hard input tensor wrong have access notebook artifact guess input tensor wrong us dynam name tensor specifi sequenc increment time load model runtim session start point estim invok neo librari outdat prefer us boto instead wrote sampl code test compil follow code work perfectli model load kera model export tensorflow format import tensorflow import nasnet import numpi model model model tar ball send import import tarfil import session mode tar compil job import time import boto import role framework tensorflow rolearn role inputconfig uri datainputconfig framework frameworkvers outputconfig outputloc targetdevic comment chang follow line depend edg devic jetson xavier jetson nano compileropt jetpack stoppingcondit maxruntimeinsecond true resp resp print break notic frameworkvers inform sampl",
        "Answer_gpt_summary_original":"possible solutions from the answer include checking if the input tensor name is correct, using boto3 instead of the estimator library, and using the provided sample code to test the compilation. the sample code involves loading a pre-trained keras model, exporting it in tensorflow format, putting it into a tar ball and sending it to s3, and kicking off a compilation job. the framework version used in the sample code is 2.4.",
        "Answer_gpt_summary":"possibl solut answer includ check input tensor correct boto instead estim librari provid sampl code test compil sampl code involv load pre train kera model export tensorflow format put tar ball send kick compil job framework version sampl code"
    },
    {
        "Question_id":null,
        "Question_title":"predict() missing 1 required positional argument: 'X' while consuming a deployed web service through python in azure",
        "Question_body":"I'm trying to consume a web service that I deployed and I get predict() missing 1 required positional argument: 'X' error. Here is a link for reference about m previous question: error-while-consuming-the-deployed-web-service-thr.html\n\n\n\n\n\nHere is my train.py file\n\ndf = pd.read_csv('prediction_data01.csv')\ndf = df[pd.notnull(df['DESCRIPTION'])]\ndf = df[pd.notnull(df['CUSTOMERCODE'])]\ncol = ['CUSTOMERCODE', 'DESCRIPTION']\ndf = df[col]\ndf.columns = ['CUSTOMERCODE', 'DESCRIPTION']\ndf['category_id'] = df['DESCRIPTION'].factorize()[0]\n\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 4), stop_words='english')\nfeatures = tfidf.fit_transform(df.DESCRIPTION).toarray()\nlabels = df.category_id\n\n\ndf = df.applymap(str)\nX_train, X_test, y_train, y_test = train_test_split(df['CUSTOMERCODE'], df['DESCRIPTION'], random_state=0)\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\n\nclf = MultinomialNB().fit(X_train_tfidf, y_train)\nos.makedirs(\".\/outputs\", exist_ok=True)\njoblib.dump(clf, 'prediction-model.pickle')\n\nHere is my score.py file:\n\ndef init():\nglobal model\n# AZUREML_MODEL_DIR is an environment variable created during deployment.\n# It is the path to the model folder (.\/azureml-models\/$MODEL_NAME\/$VERSION)\n# For multiple models, it points to the folder containing all deployed models (.\/azureml-models)\nmodel_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), \"prediction-model.pickle\")\nmodel = joblib.load(model_path)\n\n\ndef run(raw_data):\ndata = np.array(json.loads(raw_data)['data'])\n# make prediction\ny_hat = model.predict(data)\n# you can return any data type as long as it is JSON-serializable\nreturn y_hat.tolist()",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1614169733667,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/286859\/predict-missing-1-required-positional-argument-39x.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-25T18:46:23.327Z",
                "Answer_score":0,
                "Answer_body":"Please review my response on this thread, thanks!",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-02T02:42:35.233Z",
                "Answer_score":0,
                "Answer_body":"Hi, can you try testing your model locally first to view the results and the expected format for your test data? It seems you performed some transformations when fitting the model, so you need to ensure that you are providing your test data with the expected shape and dimension of the array. Hope this helps!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-05T05:47:40.24Z",
                "Answer_score":0,
                "Answer_body":"Hi, I have tested the model results locally and it's working fine. I predicted the results of the model and with the below code.\n\nclf = MultinomialNB().fit(X_train_tfidf, y_train)\nwith open(\"prediction.pickle\", \"wb\") as f:\npickle.dump(MultinomialNB, f)\nprint(clf.predict(count_vect.transform([\"18339\"])))\n\nI'm able to predict successfully with the above code and also I'm able to predict with loading the saved model pickle file using the below code.\n\n\npickle_in = open(\"prediction.pickle\", \"rb\")\nMultinomial_model = pickle.load(pickle_in)\nclf = Multinomial_model().fit(X_train_tfidf, y_train)\nprint(clf.predict(count_vect.transform([\"18339\"])))\n\nI get this error -- fit() missing 1 required positional argument: 'y'-- when I do not use parenthesis in the above code fit method.\n\nclf = Multinomial_model.fit(X_train_tfidf, y_train)\n\nHope you understand the issue. Thanks for the continuous response. I think the issue is also with the score.py. Any help is appreciated Thanks",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"predict miss requir posit argument consum deploi web servic python azur try consum web servic deploi predict miss requir posit argument error link refer previou question error consum deploi web servic thr html train file read csv predict data csv notnul descript notnul customercod col customercod descript col column customercod descript categori descript factor tfidf tfidfvector sublinear true min norm encod latin ngram rang stop word english featur tfidf fit transform descript toarrai label categori applymap str train test train test train test split customercod descript random state count vect countvector train count count vect fit transform train tfidf transform tfidftransform train tfidf tfidf transform fit transform train count clf multinomialnb fit train tfidf train makedir output exist true joblib dump clf predict model pickl score file def init global model model dir environ variabl creat deploy path model folder model model version multipl model point folder contain deploi model model model path path join getenv model dir predict model pickl model joblib load model path def run raw data data arrai json load raw data data predict hat model predict data return data type long json serializ return hat tolist",
        "Question_preprocessed_content":"predict miss requir posit argument consum deploi web servic python azur try consum web servic deploi predict miss requir posit argument error link refer previou question file col tfidf norm featur label countvector tfidftransform clf file def init global model environ variabl creat deploy path model folder multipl model point folder contain deploi model model def data predict return data type long return",
        "Question_gpt_summary_original":"The user is encountering an error \"predict() missing 1 required positional argument: 'X'\" while trying to consume a web service that was deployed. The user has shared their train.py and score.py files for reference.",
        "Question_gpt_summary":"user encount error predict miss requir posit argument try consum web servic deploi user share train score file refer",
        "Answer_original_content":"review respons thread thank try test model local view result expect format test data perform transform fit model need ensur provid test data expect shape dimens arrai hope help test model result local work fine predict result model code clf multinomialnb fit train tfidf train open predict pickl pickl dump multinomialnb print clf predict count vect transform abl predict successfulli code abl predict load save model pickl file code pickl open predict pickl multinomi model pickl load pickl clf multinomi model fit train tfidf train print clf predict count vect transform error fit miss requir posit argument us parenthesi code fit method clf multinomi model fit train tfidf train hope understand issu thank continu respons think issu score help appreci thank",
        "Answer_preprocessed_content":"review respons thread thank try test model local view result expect format test data perform transform fit model need ensur provid test data expect shape dimens arrai hope help test model result local work fine predict result model code clf abl predict successfulli code abl predict load save model pickl file code clf error fit miss requir posit argument us parenthesi code fit method clf hope understand issu thank continu respons think issu help appreci thank",
        "Answer_gpt_summary_original":"the answer suggests testing the model locally to ensure that the test data is in the expected shape and dimension of the array. the user has successfully predicted results using the code provided and loading the saved model pickle file. the error \"fit() missing 1 required positional argument: 'y'\" occurs when the parenthesis are not used in the fit method. the issue may also be with the score.py file.",
        "Answer_gpt_summary":"answer suggest test model local ensur test data expect shape dimens arrai user successfulli predict result code provid load save model pickl file error fit miss requir posit argument occur parenthesi fit method issu score file"
    },
    {
        "Question_id":null,
        "Question_title":"How to automate dvc pull request for a single file?",
        "Question_body":"<p>We are using dvc for heavy AI-ML model files in our gitlab repository.<br>\nLets say, with the help of DVC, we can easily push a model file \u2018X\u2019 to cloud but while pulling    same file on some other server, we have to use command \u201cdvc pull X\u201d.<\/p>\n<p>Currently, we run this command \u201cdvc pull X\u201d manually everytime we update our file X. Since, we dont want to pull all the updated files on cloud therefore it is necessary for us to specify \u2018X\u2019 in our dvc pull requests.<\/p>\n<p>My question is how can we automate this dvc pull request in our CI yaml file for a single file X, if this X is a variable for our file name ?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1613106444368,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":2111.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-automate-dvc-pull-request-for-a-single-file\/666",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-12T07:28:40.351Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/somya\">@Somya<\/a>,<\/p>\n<p>Could you set the file name as an environment variable in the CI system? (Perhaps via <a href=\"https:\/\/docs.gitlab.com\/ee\/ci\/variables\/#custom-environment-variables\" class=\"inline-onebox\">GitLab CI\/CD environment variables | GitLab<\/a> if you\u2019re using that)<\/p>\n<p>Then if the env var name is <code>FILE_NAME<\/code>, you can<\/p>\n<pre><code class=\"lang-auto\">$ dvc pull $FILE_NAME\n<\/code><\/pre>\n<p>(from the CI job script)<\/p>\n<p>Depending on your use case though, you may want to check out the <code>dvc get<\/code> or <code>dvc import<\/code> commands: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\">https:\/\/dvc.org\/doc\/command-reference\/get<\/a>, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/import\">https:\/\/dvc.org\/doc\/command-reference\/import<\/a>.<\/p>",
                "Answer_score":53.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-16T10:45:36.864Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a>,<br>\nThanks for your reply.<br>\nRegarding the solution based on environment variable, the file needs to be pushed from server 1 and get pulled on server 2, and if we will set file name in an environment variable then we will be doing that on server 2 and my problem will still persist. How can I communicate this environment variable between 2 branches of my gitlab repository ?<\/p>\n<p>I will definitely checkout dvc get and dvc import as suggested.<\/p>\n<p>Thanks and Regards<\/p>",
                "Answer_score":32.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-16T13:02:32.067Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/somya\">@Somya<\/a><\/p>\n<blockquote>\n<p>How can I communicate this environment variable between 2 branches of my gitlab repository ?<\/p>\n<\/blockquote>\n<p>Is it about communication between branches, or rather two different builds?<br>\nAre they run in parallel, or is one depending on another?<\/p>",
                "Answer_score":17.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T04:50:24.138Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a>,<br>\nMy use case involves one feature branch which is then merged with \u201cmaster\u201d branch.<br>\nFeature branch is just a clone of master branch on my local machine from which I will push my dvc file. And master branch means deployment of same repository on some other server with master branch. I want to pull a dvc file from master branch.<br>\nTo answer your question, master branch is dependent on feature branch and do not run in parallel.<\/p>",
                "Answer_score":17.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T11:01:19.850Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/somya\">@Somya<\/a><br>\nAnd how does your workflow look like?<\/p>\n<ol>\n<li>You do some changes in feature branch<\/li>\n<li>You push the changed data with dvc<\/li>\n<li>You  push the git changes into master branch in core repo<\/li>\n<li>You update your master branch on server 2<br>\nAnd<\/li>\n<li>You want to update only the stage that has been updated in step 3.<br>\nAm I right so far?<\/li>\n<\/ol>",
                "Answer_score":76.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-17T12:03:53.604Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a>,<br>\nMy workflow looks like following :<\/p>\n<ol>\n<li>I do some changes in feature branch<\/li>\n<li>I push the changed data with dvc<\/li>\n<li>I merge master branch from feature branch using merge request feature of gitlab<\/li>\n<li>Git pull all the changes in repository deployed on server 2 with master branch (Continuous integration written in gitlab config yaml file)<\/li>\n<\/ol>\n<p>But as I pushed a dvc file from my feature branch, I would like to pull that file from master branch on server 2. A simple command \u201cdvc pull $filename\u201d can do that trick on server 2 but how can I automate this command if $filename is not fixed.<\/p>",
                "Answer_score":46.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"autom pull request singl file heavi model file gitlab repositori let help easili push model file cloud pull file server us command pull current run command pull manual everytim updat file dont want pull updat file cloud necessari specifi pull request question autom pull request yaml file singl file variabl file",
        "Question_preprocessed_content":"autom pull request singl file heavi model file gitlab repositori let help easili push model file cloud pull file server us command pull current run command pull manual everytim updat file dont want pull updat file cloud necessari specifi pull request question autom pull request yaml file singl file variabl file",
        "Question_gpt_summary_original":"The user is facing a challenge in automating the dvc pull request for a single file in their gitlab repository. They currently have to manually run the command \"dvc pull X\" every time they update the file, and they want to automate this process in their CI yaml file. The challenge is to specify the file name 'X' as a variable in the automation process.",
        "Question_gpt_summary":"user face challeng autom pull request singl file gitlab repositori current manual run command pull time updat file want autom process yaml file challeng specifi file variabl autom process",
        "Answer_original_content":"somya set file environ variabl gitlab environ variabl gitlab your env var file pull file job script depend us case want check import command http org doc command refer http org doc command refer import jorgeorpinel thank repli solut base environ variabl file need push server pull server set file environ variabl server problem persist commun environ variabl branch gitlab repositori definit checkout import suggest thank regard somya commun environ variabl branch gitlab repositori commun branch differ build run parallel depend paffciu us case involv featur branch merg master branch featur branch clone master branch local machin push file master branch mean deploy repositori server master branch want pull file master branch answer question master branch depend featur branch run parallel somya workflow look like chang featur branch push chang data push git chang master branch core repo updat master branch server want updat stage updat step right far paffciu workflow look like follow chang featur branch push chang data merg master branch featur branch merg request featur gitlab git pull chang repositori deploi server master branch continu integr written gitlab config yaml file push file featur branch like pull file master branch server simpl command pull filenam trick server autom command filenam fix",
        "Answer_preprocessed_content":"set file environ variabl env var job script depend us case want check command thank repli solut base environ variabl file need push server pull server set file environ variabl server problem persist commun environ variabl branch gitlab repositori definit checkout import suggest thank regard commun environ variabl branch gitlab repositori commun branch differ build run parallel depend us case involv featur branch merg master branch featur branch clone master branch local machin push file master branch mean deploy repositori server master branch want pull file master branch answer question master branch depend featur branch run parallel workflow look like chang featur branch push chang data push git chang master branch core repo updat master branch server want updat stage updat step right far workflow look like follow chang featur branch push chang data merg master branch featur branch merg request featur gitlab git pull chang repositori deploi server master branch push file featur branch like pull file master branch server simpl command pull filenam trick server autom command filenam fix",
        "Answer_gpt_summary_original":"possible solutions from the answer include setting the file name as an environment variable in the ci system, using the get or import commands, and automating the pull command with a variable file name. the user may also need to communicate the environment variable between two branches of their gitlab repository. the workflow involves making changes in a feature branch, pushing the changes, merging the master branch, and pulling the changes in the deployed repository on server 2.",
        "Answer_gpt_summary":"possibl solut answer includ set file environ variabl import command autom pull command variabl file user need commun environ variabl branch gitlab repositori workflow involv make chang featur branch push chang merg master branch pull chang deploi repositori server"
    },
    {
        "Question_id":68463080.0,
        "Question_title":"how create azure machine learning scoring image using local package",
        "Question_body":"<p>I have pkl package saved in my azure devops repository<\/p>\n<p>using below code it searches for package in workspace.\nHow to provide package saved in repository<\/p>\n<pre><code> ws = Workspace.get(\n         name=workspace_name,\n         subscription_id=subscription_id,\n        resource_group=resource_group,\n        auth=cli_auth)\n\nimage_config = ContainerImage.image_configuration(\n    execution_script=&quot;score.py&quot;,\n    runtime=&quot;python-slim&quot;,\n    conda_file=&quot;conda.yml&quot;,\n    description=&quot;Image with ridge regression model&quot;,\n    tags={&quot;area&quot;: &quot;ml&quot;, &quot;type&quot;: &quot;dev&quot;},\n)\n\nimage = Image.create(\n    name=image_name,  models=[model], image_config=image_config, workspace=ws\n)\n\nimage.wait_for_creation(show_output=True)\n\nif image.creation_state != &quot;Succeeded&quot;:\n    raise Exception(&quot;Image creation status: {image.creation_state}&quot;)\n\nprint(\n    &quot;{}(v.{} [{}]) stored at {} with build log {}&quot;.format(\n        image.name,\n        image.version,\n        image.creation_state,\n        image.image_location,\n        image.image_build_log_uri,\n    )\n)\n\n# Writing the image details to \/aml_config\/image.json\nimage_json = {}\nimage_json[&quot;image_name&quot;] = image.name\nimage_json[&quot;image_version&quot;] = image.version\nimage_json[&quot;image_location&quot;] = image.image_location\nwith open(&quot;aml_config\/image.json&quot;, &quot;w&quot;) as outfile:\n    json.dump(image_json, outfile)\n<\/code><\/pre>\n<p>I tried to provide path to models but its fails saying package not found<\/p>\n<p>models = $(System.DefaultWorkingDirectory)\/package_model.pkl<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626831896507,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":140.0,
        "Answer_body":"<p>Register model:\nRegister a file or folder as a model by calling <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none--datasets-none--model-framework-none--model-framework-version-none--child-paths-none-\" rel=\"nofollow noreferrer\">Model.register()<\/a>.<\/p>\n<p>In addition to the content of the model file itself, your registered model will also store model metadata -- model description, tags, and framework information -- that will be useful when managing and deploying models in your workspace. Using tags, for instance, you can categorize your models and apply filters when listing models in your workspace.<\/p>\n<pre><code>model = Model.register(workspace=ws,\n                       model_name='',                # Name of the registered model in your workspace.\n                       model_path='',  # Local file to upload and register as a model.\n                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n                       sample_input_dataset=input_dataset,\n                       sample_output_dataset=output_dataset,\n                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n                       description='Ridge regression model to predict diabetes progression.',\n                       tags={'area': 'diabetes', 'type': 'regression'})\n\nprint('Name:', model.name)\nprint('Version:', model.version)\n<\/code><\/pre>\n<p>Deploy machine learning models to Azure: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python<\/a><\/p>\n<p>To Troubleshooting remote model deployment Please follow the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?tabs=azcli#function-fails-get_model_path\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BL0Nm.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1627278873550,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68463080",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1627276170736,
        "Question_original_content":"creat score imag local packag pkl packag save azur devop repositori code search packag workspac provid packag save repositori workspac workspac subscript subscript resourc group resourc group auth cli auth imag config containerimag imag configur execut script score runtim python slim conda file conda yml descript imag ridg regress model tag area type dev imag imag creat imag model model imag config imag config workspac imag wait creation output true imag creation state succeed rais except imag creation statu imag creation state print store build log format imag imag version imag creation state imag imag locat imag imag build log uri write imag detail aml config imag json imag json imag json imag imag imag json imag version imag version imag json imag locat imag imag locat open aml config imag json outfil json dump imag json outfil tri provid path model fail sai packag model defaultworkingdirectori packag model pkl",
        "Question_preprocessed_content":"creat score imag local packag pkl packag save azur devop repositori code search packag workspac provid packag save repositori tri provid path model fail sai packag model",
        "Question_gpt_summary_original":"The user is facing challenges in creating an Azure machine learning scoring image using a local package. They have saved a pkl package in their Azure DevOps repository and are trying to search for the package in the workspace using the provided code. However, they are unable to provide the package saved in the repository and are encountering errors when trying to provide the path to the models.",
        "Question_gpt_summary":"user face challeng creat score imag local packag save pkl packag azur devop repositori try search packag workspac provid code unabl provid packag save repositori encount error try provid path model",
        "Answer_original_content":"regist model regist file folder model call model regist addit content model file regist model store model metadata model descript tag framework inform us manag deploi model workspac tag instanc categor model appli filter list model workspac model model regist workspac model regist model workspac model path local file upload regist model model framework model framework scikitlearn framework creat model model framework version sklearn version version scikit learn creat model sampl input dataset input dataset sampl output dataset output dataset resourc configur resourceconfigur cpu memori descript ridg regress model predict diabet progress tag area diabet type regress print model print version model version deploi machin learn model azur http doc microsoft com azur machin learn deploi tab python troubleshoot remot model deploy follow document",
        "Answer_preprocessed_content":"regist model regist file folder model call addit content model file regist model store model metadata model descript tag framework inform us manag deploi model workspac tag instanc categor model appli filter list model workspac deploi machin learn model azur troubleshoot remot model deploy follow document",
        "Answer_gpt_summary_original":"the solution to the challenge of creating a scoring image using a local package saved in azure devops repository is to register the model by calling model.register(). this will store model metadata such as model description, tags, and framework information that will be useful when managing and deploying models in the workspace. additionally, the user can deploy machine learning models to azure by following the documentation provided. for troubleshooting remote model deployment, the user can refer to the document as well.",
        "Answer_gpt_summary":"solut challeng creat score imag local packag save azur devop repositori regist model call model regist store model metadata model descript tag framework inform us manag deploi model workspac addition user deploi machin learn model azur follow document provid troubleshoot remot model deploy user refer document"
    },
    {
        "Question_id":58995329.0,
        "Question_title":"Artifact storage and MLFLow on remote server",
        "Question_body":"<p>I am trying to get MLFlow on another machine in a local network to run and I would like to ask for some help because I don't know what to do now.<\/p>\n\n<p>I have a mlflow server running on a <em>server<\/em>. The mlflow server is running under my user on the <em>server<\/em> and has been started like this: <\/p>\n\n<pre><code>mlflow server --host 0.0.0.0 --port 9999 --default-artifact-root sftp:\/\/&lt;MYUSERNAME&gt;@&lt;SERVER&gt;:&lt;PATH\/TO\/DIRECTORY\/WHICH\/EXISTS&gt;\n<\/code><\/pre>\n\n<p>My program which should log all the data to the mlflow server looks like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow import log_metric, log_param, log_artifact, set_tracking_uri\n\nif __name__ == \"__main__\":\n    remote_server_uri = '&lt;SERVER&gt;' # this value has been replaced\n    set_tracking_uri(remote_server_uri)\n    # Log a parameter (key-value pair)\n    log_param(\"param1\", 5)\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(\"foo\", 1)\n    log_metric(\"foo\", 2)\n    log_metric(\"foo\", 3)\n\n    # Log an artifact (output file)\n    with open(\"output.txt\", \"w\") as f:\n        f.write(\"Hello world!\")\n    log_artifact(\"output.txt\")\n\n<\/code><\/pre>\n\n<p>The parameters get and metrics get transfered to the server but not the artifacts. Why is that so?<\/p>\n\n<p>Note on the SFTP part:\nI can log in via SFTP and the pysftp package is installed<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1574429522373,
        "Question_favorite_count":null,
        "Question_last_edit_time":1574433828620,
        "Question_score":9.0,
        "Question_view_count":3483.0,
        "Answer_body":"<p>I don't know if I will get an answer to my problem but I did <em>solved<\/em> it this way.<\/p>\n\n<p>On the server I created the directory <code>\/var\/mlruns<\/code>. I pass this directory to mlflow via <code>--backend-store-uri file:\/\/\/var\/mlruns<\/code><\/p>\n\n<p>Then I mount this directory via e.g. <code>sshfs<\/code> on my local machine under the same path.<\/p>\n\n<p>I don't like this solution but it solved the problem good enough for now.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58995329",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1575967801236,
        "Question_original_content":"artifact storag remot server try machin local network run like ask help know server run server server run user server start like server host port default artifact root sftp program log data server look like import log metric log param log artifact set track uri main remot server uri valu replac set track uri remot server uri log paramet kei valu pair log param param log metric metric updat run log metric foo log metric foo log metric foo log artifact output file open output txt write hello world log artifact output txt paramet metric transfer server artifact note sftp log sftp pysftp packag instal",
        "Question_preprocessed_content":"artifact storag remot server try machin local network run like ask help know server run server server run user server start like program log data server look like paramet metric transfer server artifact note sftp log sftp pysftp packag instal",
        "Question_gpt_summary_original":"The user is facing a challenge in transferring artifacts to a remote server using MLFlow. The parameters and metrics are successfully transferred, but the artifacts are not. The MLFlow server is running on a server under the user's account, and the program to log data to the server is written in Python. The user has noted that they can log in via SFTP, and the pysftp package is installed.",
        "Question_gpt_summary":"user face challeng transfer artifact remot server paramet metric successfulli transfer artifact server run server user account program log data server written python user note log sftp pysftp packag instal",
        "Answer_original_content":"know answer problem solv wai server creat directori var mlrun pass directori backend store uri file var mlrun mount directori sshf local machin path like solut solv problem good",
        "Answer_preprocessed_content":"know answer problem solv wai server creat directori pass directori mount directori local machin path like solut solv problem good",
        "Answer_gpt_summary_original":"the solution to the challenge of artifact storage and transfer on a remote server is to create a directory on the server, pass it via --backend-store-uri, and mount it on the local machine using sshfs. although this solution is not ideal, it has solved the problem for now.",
        "Answer_gpt_summary":"solut challeng artifact storag transfer remot server creat directori server pass backend store uri mount local machin sshf solut ideal solv problem"
    },
    {
        "Question_id":null,
        "Question_title":"Custom settings for wandb.Object3D",
        "Question_body":"<p>I wonder if <a class=\"mention-group notify\" href=\"\/groups\/team\">@team<\/a> can add some custom seetings for wandb.Object3D.<br>\nAf first I tried to use Plotly to achieve custom 3D point cloud visualization, but I saw team says Plotly is not supported now in github issue.<br>\nFor example, point size, backgorund color, etc.<br>\nIt would be really nice for 3D task.<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1651459164070,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":180.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/custom-settings-for-wandb-object3d\/2351",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-04T13:52:37.046Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/justice\">@justice<\/a>, I can put in a feature request for this. Could you elaborate a little more about the specific controls you were hoping for so I can better communicate with the engineering team all of the controls you were hoping to have?<\/p>\n<p>Also, is the issue with Plotly specificly related to 3D objects or Plotly in general? We do support adding Plotly charts but if there is an issue with uploading 3D Plotly objects I can see if this is intended behavior or a bug.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-05T03:44:58.715Z",
                "Answer_body":"<blockquote>\n<p>Also, is the issue with Plotly specificly related to 3D objects or Plotly in general?<\/p>\n<\/blockquote>\n<p>I\u2019m not sure about other Plotly function is function properly or not, but at least I\u2019m sure about Plotly 3D object isn\u2019t function properly in wandb\u2019s table. Here\u2019s  the realted  <a href=\"https:\/\/github.com\/wandb\/client\/issues\/2191#issuecomment-841452278\" rel=\"noopener nofollow ugc\">github issue<\/a> I found.<\/p>\n<blockquote>\n<p>Could you elaborate a little more about the specific controls you were hoping for so I can better communicate with the engineering team all of the controls you were hoping to have?<\/p>\n<\/blockquote>\n<ol>\n<li>point size control<\/li>\n<li>background color control<\/li>\n<li>relative coordinates on the side of the charts to get an idea of the 3d object\u2019s size<\/li>\n<li>customizable default view angle (in the thumbnail)<\/li>\n<li>mesh support<\/li>\n<li>voxel support<\/li>\n<li>separate color arrays, rather then tied with point sets defaultly. (Optional, since users can still do some array operations to change that in current wandb.Object3D.<\/li>\n<\/ol>\n<p>Thanks in advanced!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-05T03:56:08.729Z",
                "Answer_body":"<p>Also, I think it would be great if wandb can support a time lapse video\/gif demonstrating how the prediction deformed\/changed along with epochs without doing a lot of coding.<\/p>\n<p>I know users can do it in various ways, but it would be a nice  feature to have if wandb can support it natively and effortlessly, no matter it\u2019s an image or a 3D object.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-09T15:59:23.756Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/justice\">@justice<\/a> thank you for the details! I\u2019ll capture this and pass it on to our engineering team and will follow up with you here if we are able implement any of these features.<\/p>\n<p>Does the workaround of converting the Plotly figure to html and wrapping it in a <code>wandb.Html()<\/code> object work for some of these features in the meantime?<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-09T16:20:23.526Z",
                "Answer_body":"<p>I\u2019m not sure about if <code>wandb.Html()<\/code> will work or not.<br>\nI\u2019m not so familiar with html, but I don\u2019t think html can do something like rotating the 3d object as in <strong>Plotly<\/strong>? I didn\u2019t really give it a try through.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-13T23:54:55.254Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/justice\">@justice<\/a>, I wanted to check in and see if you had gotten the chance to try out the HTML method. I\u2019ve went ahead and put in a feature request but wanted to see if this was working as a temporary workaround?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-12T23:55:50.417Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"custom set objectd wonder team add custom seet objectd tri us plotli achiev custom point cloud visual saw team sai plotli support github issu exampl point size backgorund color nice task",
        "Question_preprocessed_content":"custom set object wonder add custom seet object tri us plotli achiev custom point cloud visual saw team sai plotli support github issu exampl point size backgorund color nice task",
        "Question_gpt_summary_original":"The user is facing challenges in customizing settings for wandb.Object3D, specifically in achieving custom 3D point cloud visualization. The user attempted to use Plotly but found out that it is not supported. The user is requesting for custom settings such as point size and background color to be added for better 3D task visualization.",
        "Question_gpt_summary":"user face challeng custom set objectd specif achiev custom point cloud visual user attempt us plotli support user request custom set point size background color ad better task visual",
        "Answer_original_content":"justic featur request elabor littl specif control hope better commun engin team control hope issu plotli specificli relat object plotli gener support ad plotli chart issu upload plotli object intend behavior bug thank nate issu plotli specificli relat object plotli gener sure plotli function function properli sure plotli object isnt function properli tabl here realt github issu elabor littl specif control hope better commun engin team control hope point size control background color control rel coordin chart idea object size customiz default view angl thumbnail mesh support voxel support separ color arrai ti point set defaultli option user arrai oper chang current objectd thank advanc think great support time laps video gif demonstr predict deform chang epoch lot code know user wai nice featur support nativ effortlessli matter imag object justic thank detail ill captur pass engin team follow abl implement featur workaround convert plotli figur html wrap html object work featur meantim sure html work familiar html dont think html like rotat object plotli didnt try justic want check gotten chanc try html method iv went ahead featur request want work temporari workaround topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"featur request elabor littl specif control hope better commun engin team control hope issu plotli specificli relat object plotli gener support ad plotli chart issu upload plotli object intend behavior bug thank nate issu plotli specificli relat object plotli gener sure plotli function function properli sure plotli object isnt function properli tabl here realt github issu elabor littl specif control hope better commun engin team control hope point size control background color control rel coordin chart idea object size customiz default view angl mesh support voxel support separ color arrai ti point set defaultli option user arrai oper chang current object thank advanc think great support time laps demonstr predict epoch lot code know user wai nice featur support nativ effortlessli matter imag object thank detail ill captur pass engin team follow abl implement featur workaround convert plotli figur html wrap object work featur meantim sure work familiar html dont think html like rotat object plotli didnt try want check gotten chanc try html method iv went ahead featur request want work temporari workaround topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user is looking for custom settings for .object3d, such as point size and background color, but has encountered difficulty in finding a supported solution. the answer suggests putting in a feature request for this and asks for more details about the specific controls the user is hoping for. the answer also mentions a workaround of converting the plotly figure to html and wrapping it in a .html() object. the engineering team will be informed of the user's request and will follow up if any of the features can be implemented. additionally, the user suggests a feature of time-lapse video\/gif demonstrating how the prediction deformed\/changed along with epochs without doing a lot of coding.",
        "Answer_gpt_summary":"user look custom set objectd point size background color encount difficulti find support solut answer suggest put featur request ask detail specif control user hope answer mention workaround convert plotli figur html wrap html object engin team inform user request follow featur implement addition user suggest featur time laps video gif demonstr predict deform chang epoch lot code"
    },
    {
        "Question_id":null,
        "Question_title":"Best Approach to Clientside Machine Learning for Text Classification",
        "Question_body":"I have approximately 100k rows of text data (initially PDF documents that have been OCR). Most are rows of less than 5000 characters. Each of the source documents are addressed to some department. These are typically in the form of the below examples where the target department would 'Urology' (there are several departments).\n\nUrologly Department\n\n\nUrologly Clinic\n\n\nUrology Out Patients\n\n\nUrology\n\n\nDear urology team\n\nI have read a bit on ML Text Analysis and it seems I should be able to make a pretty good model by reviewing several hundred documents for each department (I have built an App to help me do this) and manually Classifying those documents. Some documents may mention urology but are actually addressed to another department. Typically the addressed department text is at the top third (first 3-7 lines) of the text body.\n\nI cannot use any online tools, i.e. I can't upload any of the Document text to servers to process I need a client side library. I have read and completed several tutorials using the ML.net but these are pretty basic (sentiment, entity detection without any initial training), and read an excellent blog at MonkeyLearn: which seems to acknowledge that can do what I imagine I should be able to do.\n\nSo can anybody point me in the right direction, can I use some offline Microsoft client library to complete my task? Is there some other Open Source client library i should look at. Will I have to learn Go, or python to complete the task (currently a C# dev).\n\nNote: I could get fairly good matches simply using SQL Text search and a bit of C# with plenty of hard coded rules, but I thought I'd try ML -- however its a nest of complications at the moment and i am going around in circles.\n\nMany Thanks\nMike.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637112667940,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@MikeShapleski-3383 I see two possible solutions for your scenario.\n\nExtracting text from your documents using the computer vision API and passing the required text as input to Azure Text Analytics for Health API\n\n\nUsing Azure cognitive search to upload the documents and creating a search service and enabling specific skills on the service to extract PII data or entities\n\nThe first solution can help you achieve this and ensure everything is offline or using docker containers without uploading any of your data to any storage externally. For billing purposes the containers need to connect to a metering endpoint on Azure to bill your usage of both these services(Computer Vision API & Azure text analytics containers). Also, you can use C# client library to call the local endpoint of these containers. The setup could take time to configure docker containers and passing the PDF documents to the computer vision read API to extract text. The extracted text can then be directly used or stored, to call the text analytics for health API.\n\nThe second solution can be used to index all the documents by using the search service by having your data in the cloud or behind a firewall to index the documents and make them searchable. There are some skills that can be enabled on the search service to extract entities and other PII information but this may not extract the same data as text analytics for health. This solution can be faster to setup because you can directly query your data after uploading the documents.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/629917\/best-approach-to-clientside-machine-learning-for-t.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-17T10:19:58.317Z",
                "Answer_score":0,
                "Answer_body":"@MikeShapleski-3383 I see two possible solutions for your scenario.\n\nExtracting text from your documents using the computer vision API and passing the required text as input to Azure Text Analytics for Health API\n\n\nUsing Azure cognitive search to upload the documents and creating a search service and enabling specific skills on the service to extract PII data or entities\n\nThe first solution can help you achieve this and ensure everything is offline or using docker containers without uploading any of your data to any storage externally. For billing purposes the containers need to connect to a metering endpoint on Azure to bill your usage of both these services(Computer Vision API & Azure text analytics containers). Also, you can use C# client library to call the local endpoint of these containers. The setup could take time to configure docker containers and passing the PDF documents to the computer vision read API to extract text. The extracted text can then be directly used or stored, to call the text analytics for health API.\n\nThe second solution can be used to index all the documents by using the search service by having your data in the cloud or behind a firewall to index the documents and make them searchable. There are some skills that can be enabled on the search service to extract entities and other PII information but this may not extract the same data as text analytics for health. This solution can be faster to setup because you can directly query your data after uploading the documents.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":3,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1637144398316,
        "Question_original_content":"best approach clientsid machin learn text classif approxim row text data initi pdf document ocr row charact sourc document address depart typic form exampl target depart urolog depart urologli depart urologli clinic urolog patient urolog dear urolog team read bit text analysi abl pretti good model review document depart built app help manual classifi document document mention urolog actual address depart typic address depart text line text bodi us onlin tool upload document text server process need client librari read complet tutori net pretti basic sentiment entiti detect initi train read excel blog monkeylearn acknowledg imagin abl anybodi point right direct us offlin microsoft client librari complet task open sourc client librari look learn python complet task current dev note fairli good match simpli sql text search bit plenti hard code rule thought try nest complic moment go circl thank mike",
        "Question_preprocessed_content":"best approach clientsid machin learn text classif approxim row text data row charact sourc document address depart typic form exampl target depart urolog urologli depart urologli clinic urolog patient urolog dear urolog team read bit text analysi abl pretti good model review document depart manual classifi document document mention urolog actual address depart typic address depart text text bodi us onlin tool upload document text server process need client librari read complet tutori pretti basic read excel blog monkeylearn acknowledg imagin abl anybodi point right direct us offlin microsoft client librari complet task open sourc client librari look learn python complet task note fairli good match simpli sql text search bit plenti hard code rule thought try nest complic moment go circl thank mike",
        "Question_gpt_summary_original":"The user has encountered challenges in using machine learning for text classification due to the large amount of data they have and the need for a client-side library. They have attempted to use ML.net but found it too basic and are seeking advice on whether they can use an offline Microsoft client library or an open-source client library. They also mention the difficulty of manually classifying documents and the possibility of documents mentioning a department but being addressed to another department.",
        "Question_gpt_summary":"user encount challeng machin learn text classif larg data need client librari attempt us net basic seek advic us offlin microsoft client librari open sourc client librari mention difficulti manual classifi document possibl document mention depart address depart",
        "Answer_original_content":"mikeshapleski possibl solut scenario extract text document vision api pass requir text input azur text analyt health api azur cognit search upload document creat search servic enabl specif skill servic extract pii data entiti solut help achiev ensur offlin docker contain upload data storag extern bill purpos contain need connect meter endpoint azur usag servic vision api azur text analyt contain us client librari local endpoint contain setup time configur docker contain pass pdf document vision read api extract text extract text directli store text analyt health api second solut index document search servic have data cloud firewal index document searchabl skill enabl search servic extract entiti pii inform extract data text analyt health solut faster setup directli queri data upload document answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"possibl solut scenario extract text document vision api pass requir text input azur text analyt health api azur cognit search upload document creat search servic enabl specif skill servic extract pii data entiti solut help achiev ensur offlin docker contain upload data storag extern bill purpos contain need connect meter endpoint azur usag servic us client librari local endpoint contain setup time configur docker contain pass pdf document vision read api extract text extract text directli store text analyt health api second solut index document search servic have data cloud firewal index document searchabl skill enabl search servic extract entiti pii inform extract data text analyt health solut faster setup directli queri data upload document answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions for the user's scenario of classifying 100k rows of text data. the first solution involves using the computer vision api to extract text from documents, passing it to azure text analytics for health api, and using azure cognitive search to upload the documents and create a search service. the second solution involves indexing all the documents using the search service and enabling specific skills to extract entities and other pii information. the first solution ensures offline processing and can be time-consuming to set up, while the second solution can be faster to set up but may not extract the same data as text analytics for health.",
        "Answer_gpt_summary":"answer suggest possibl solut user scenario classifi row text data solut involv vision api extract text document pass azur text analyt health api azur cognit search upload document creat search servic second solut involv index document search servic enabl specif skill extract entiti pii inform solut ensur offlin process time consum set second solut faster set extract data text analyt health"
    },
    {
        "Question_id":null,
        "Question_title":"Machine Learning",
        "Question_body":"i have problem with detecting the objects using the ML.NET and draw a boundary boxes\nand i cannot find an example or module that i can learn.",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1594208627597,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/44135\/machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-14T19:13:54.527Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nObject detection is one of the classic problems in computer vision: Recognize what objects are inside a given image and also where they are in the image. For these cases, you can either use pre-trained models or train your own model to classify images specific to your custom domain. This sample uses a pre-trained model by default, but you can also add your own model exported from Custom Vision.\n\nI have a sample here for you:\n\nhttps:\/\/github.com\/dotnet\/machinelearning-samples\/tree\/master\/samples\/csharp\/end-to-end-apps\/ObjectDetection-Onnx\n\nThis sample consists of two separate apps:\n\nA WPF Core desktop app that renders a live-stream of the device's web cam, runs the video frames through an object detection model using ML.NET, and paints bounding boxes with labels indicating the objects detected in real-time.\nAn ASP.NET Core Web app that allows the user to upload or select an image. The Web app then runs the image through an object detection model using ML.NET, and paints bounding boxes with labels indicating the objects detected.\nThe Web app shows the images listed on the right, and each image may be selected to process. Once the image is processed, it is drawn in the middle of the screen with labeled bounding boxes around each detected object as shown below.\n\nLet me know if you have any question.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":32.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"machin learn problem detect object net draw boundari box exampl modul learn",
        "Question_preprocessed_content":"machin learn problem detect object draw boundari box exampl modul learn",
        "Question_gpt_summary_original":"The user is facing challenges in detecting objects using ML.NET and drawing boundary boxes. They are unable to find any relevant examples or modules to learn from.",
        "Question_gpt_summary":"user face challeng detect object net draw boundari box unabl relev exampl modul learn",
        "Answer_original_content":"object detect classic problem vision recogn object insid given imag imag case us pre train model train model classifi imag specif custom domain sampl us pre train model default add model export custom vision sampl http github com dotnet machinelearn sampl tree master sampl csharp end end app objectdetect onnx sampl consist separ app wpf core desktop app render live stream devic web cam run video frame object detect model net paint bound box label indic object detect real time asp net core web app allow user upload select imag web app run imag object detect model net paint bound box label indic object detect web app show imag list right imag select process imag process drawn middl screen label bound box detect object shown let know question",
        "Answer_preprocessed_content":"object detect classic problem vision recogn object insid given imag imag case us model train model classifi imag specif custom domain sampl us model default add model export custom vision sampl sampl consist separ app wpf core desktop app render devic web cam run video frame object detect model paint bound box label indic object detect core web app allow user upload select imag web app run imag object detect model paint bound box label indic object detect web app show imag list right imag select process imag process drawn middl screen label bound box detect object shown let know question",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer for the user's question about ingesting data from customers and deploying models for real-time and batch inference with minimal technical overheads when transitioning from aws and on-prem to azure. the answer provides a sample code for object detection using pre-trained models or custom models in computer vision. the sample code consists of two separate apps: a desktop app that runs a live-stream of the device's web cam and an asp.net web app that allows the user to upload or select an image. the apps use ml.net to run the image through an object detection model and paint bounding boxes with labels indicating the objects detected.",
        "Answer_gpt_summary":"solut provid answer user question ingest data custom deploi model real time batch infer minim technic overhead transit aw prem azur answer provid sampl code object detect pre train model custom model vision sampl code consist separ app desktop app run live stream devic web cam asp net web app allow user upload select imag app us net run imag object detect model paint bound box label indic object detect"
    },
    {
        "Question_id":null,
        "Question_title":"Cannot use artifact when in offline mode",
        "Question_body":"<p>Hi,<\/p>\n<p>How can I use artifacts without actually enabling wandb syncing? Sometimes I want just to play around my notebook without logging anything, but using data\/models logged as artifacts in my project. I think I can do it via cli but I would like to know if there\u2019s something I\u2019m missing in the API.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1632313502310,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":287.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cannot-use-artifact-when-in-offline-mode\/739",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-23T20:10:20.762Z",
                "Answer_body":"<p>Hello!<\/p>\n<p>Here\u2019s a copy pasta from the <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\">docs-faq<\/a>, please lmk if this doesn\u2019t answer your Q:<\/p>\n<h2><a name=\"heading-1\" class=\"anchor\" href=\"#heading-1\"><\/a><\/h2>\n<p>Can I run wandb offline?<\/p>\n<p>If you\u2019re training on an offline machine and want to upload your results to our servers afterwards, we have a feature for you!<\/p>\n<ol>\n<li>\n<p>Set the environment variable <code>WANDB_MODE=offline<\/code> to save the metrics locally, no internet required.<\/p>\n<\/li>\n<li>\n<p>When you\u2019re ready, run <code>wandb init<\/code> in your directory to set the project name.<\/p>\n<\/li>\n<li>\n<p>Run <code>wandb sync YOUR_RUN_DIRECTORY<\/code> to push the metrics to our cloud service and see your results in our hosted web app.<\/p>\n<\/li>\n<\/ol>",
                "Answer_score":3.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T03:44:17.983Z",
                "Answer_body":"<p>You can do something like this:<\/p>\n<pre><code class=\"lang-auto\">artifact =  wandb.Artifact(name=\"folds\", type=\"dataset\")\nartifact.add_file('.\/df_train.csv')\nartifact.add_file('.\/df_valid.csv')\n<\/code><\/pre>",
                "Answer_score":47.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T07:54:15.442Z",
                "Answer_body":"<p>The thing is that I want to be online, but just to use\/download artifacts, but not for logging anything. In other words, I want to be able to download from my project but not to upload.<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T09:38:30.540Z",
                "Answer_body":"<p>You could try the following pseudocode:<\/p>\n<ol>\n<li>wandb.login()<\/li>\n<li>wandb.init()<\/li>\n<li>Download from Artifacts<\/li>\n<li>wandb.finish()<\/li>\n<li>set WANDB_MODE=offline<\/li>\n<li>do training<\/li>\n<\/ol>\n<p>An alternative to 4 + 5 would be to  turn on <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/anon?q=disable\">anonymous mode<\/a> for your training, everything will  be tracked and synced to a temporary anon account and not linked to your account<\/p>\n<p><code>wandb.init(anonymous=\"allow\")<\/code><\/p>",
                "Answer_score":72.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T10:52:16.926Z",
                "Answer_body":"<p>Thanks! The anonymus mode could do the trick for me\u2026however, I can\u2019t udnerstand how it exactly works. I\u2019m trying the <a href=\"http:\/\/bit.ly\/anon-mode\" rel=\"noopener nofollow ugc\">colab notebook they provide<\/a>,  setting <code>anonymous=must<\/code> but  it still creates a run linked to my account<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T11:43:51.750Z",
                "Answer_body":"<p>hmmm ok, I guess maybe you have to log out\u2026will forward that feedback. Maybe instead of anonymous you could use set the <code>mode<\/code> parameter to <code>offline<\/code>, or <code>disabled<\/code> then<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/docs.wandb.ai\/ref\/python\/init\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/41cd209363aac9340aa990ad198a67c63ad5a47b.png\" class=\"site-icon\" width=\"256\" height=\"256\">\n\n      <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/init\" target=\"_blank\" rel=\"noopener\">docs.wandb.ai<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a_2_690x362.png\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/75ad225cd4a0f29cec0e6dd7859f017ee0df3c7a_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/docs.wandb.ai\/ref\/python\/init\" target=\"_blank\" rel=\"noopener\">wandb.init<\/a><\/h3>\n\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T12:24:22.844Z",
                "Answer_body":"<p>Yes that\u2019s what I used to do (<code>mode=disabled<\/code>) when my run does not use artifacts as input datasets. The problem is that with mode=disabled I cannot use artifacts<\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:07.422Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us artifact offlin mode us artifact actual enabl sync want plai notebook log data model log artifact project think cli like know there miss api thank",
        "Question_preprocessed_content":"us artifact offlin mode us artifact actual enabl sync want plai notebook log log artifact project think cli like know there miss api thank",
        "Question_gpt_summary_original":"The user is facing a challenge of not being able to use artifacts without enabling wandb syncing. They want to use data\/models logged as artifacts in their project without logging anything and are looking for a way to do it via the API.",
        "Question_gpt_summary":"user face challeng abl us artifact enabl sync want us data model log artifact project log look wai api",
        "Answer_original_content":"hello here copi pasta doc faq lmk doesnt answer run offlin your train offlin machin want upload result server featur set environ variabl mode offlin save metric local internet requir your readi run init directori set project run sync run directori push metric cloud servic result host web app like artifact artifact fold type dataset artifact add file train csv artifact add file valid csv thing want onlin us download artifact log word want abl download project upload try follow pseudocod login init download artifact finish set mode offlin train altern turn anonym mode train track sync temporari anon account link account init anonym allow thank anonymu mode trick mehowev udnerstand exactli work try colab notebook provid set anonym creat run link account guess mayb log outwil forward feedback mayb instead anonym us set mode paramet offlin disabl doc init ye that mode disabl run us artifact input dataset problem mode disabl us artifact topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hello here copi pasta lmk doesnt answer run offlin your train offlin machin want upload result server featur set environ variabl save metric local internet requir your readi run directori set project run push metric cloud servic result host web app like thing want onlin artifact log word want abl download project upload try follow pseudocod login init download artifact finish set train altern turn anonym mode train track sync temporari anon account link account thank anonymu mode trick mehowev udnerstand exactli work try colab notebook provid set creat run link account guess mayb log outwil forward feedback mayb instead anonym us set paramet init ye that run us artifact input dataset problem mode disabl us artifact topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user wants to use artifacts in offline mode without enabling syncing. one solution is to set the environment variable _mode=offline to save metrics locally and upload them later. another solution is to use anonymous mode for training, which tracks and syncs everything to a temporary anonymous account. alternatively, the user can set the mode parameter to offline or disabled in the .init function, but this will not allow the use of artifacts.",
        "Answer_gpt_summary":"user want us artifact offlin mode enabl sync solut set environ variabl mode offlin save metric local upload later solut us anonym mode train track sync temporari anonym account altern user set mode paramet offlin disabl init function allow us artifact"
    },
    {
        "Question_id":null,
        "Question_title":"enhanced speech feature",
        "Question_body":"Hi I have a queryIn Dailogflow if we enable enhanced speech feature, specifically, credit card info (i.e. number), if that is spoken by user, is that stored by Google. Please help",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629336840000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":414.0,
        "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/enhanced-speech-feature\/td-p\/167747\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2021-08-20T06:59:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI understand you are looking to use the enhanced model in Dialogflow and you are looking to understand the Data Security. Please let me know if my understanding is wrong.\n\nIf that is what you are looking for, then I think you should read this section of this article[0] which addresses this concern. As explained in the doc[0], Google uses the data sent to Dialogflow on the project with data logging enabled. Google uses this data solely to train and improve Google products and services. So, while you'll maintain full ownership of all data that you upload to a project with data logging enabled, there are some terms[1] which I think you should be aware of.\n\n[0]https:\/\/cloud.google.com\/dialogflow\/es\/docs\/speech-enhanced-models#data-security\n[1]https:\/\/cloud.google.com\/dialogflow\/docs\/data-logging-terms"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"enhanc speech featur queryin dailogflow enabl enhanc speech featur specif credit card info number spoken user store googl help",
        "Question_preprocessed_content":"enhanc speech featur queryin dailogflow enabl enhanc speech featur specif credit card info spoken user store googl help",
        "Question_gpt_summary_original":"The user is seeking clarification on whether enabling the enhanced speech feature in Dialogflow will result in Google storing any credit card information spoken by the user.",
        "Question_gpt_summary":"user seek clarif enabl enhanc speech featur dialogflow result googl store credit card inform spoken user",
        "Answer_original_content":"hello understand look us enhanc model dialogflow look understand data secur let know understand wrong look think read section articl address concern explain doc googl us data sent dialogflow project data log enabl googl us data sole train improv googl product servic maintain ownership data upload project data log enabl term think awar http cloud googl com dialogflow doc speech enhanc model data secur http cloud googl com dialogflow doc data log term view solut origin post",
        "Answer_preprocessed_content":"hello understand look us enhanc model dialogflow look understand data secur let know understand wrong look think read section articl address concern explain doc googl us data sent dialogflow project data log enabl googl us data sole train improv googl product servic maintain ownership data upload project data log enabl term think awar view solut origin post",
        "Answer_gpt_summary_original":"the answer suggests that the user should read a specific section of an article that addresses their concern about data security when using the enhanced speech feature in dialogflow. the article explains that google uses the data sent to dialogflow on the project with data logging enabled solely to train and improve google products and services. the user will maintain full ownership of all data uploaded to a project with data logging enabled, but there are some terms they should be aware of.",
        "Answer_gpt_summary":"answer suggest user read specif section articl address concern data secur enhanc speech featur dialogflow articl explain googl us data sent dialogflow project data log enabl sole train improv googl product servic user maintain ownership data upload project data log enabl term awar"
    },
    {
        "Question_id":54825390.0,
        "Question_title":"create aws sagemker endpoint with lambda function",
        "Question_body":"<p>I created an endpoint in <code>aws sagemaker<\/code> and it works well, I created a <code>lambda<\/code> function(<code>python3.6<\/code>) that takes files from <code>S3<\/code>, invoke the endpoint and then put the output in a file in <code>S3<\/code>. <\/p>\n\n<p>I wonder if I can create the endpoint at every event(a file uploaded in an <code>s3 bucket)<\/code> and then delete the endpoint <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1550832424633,
        "Question_favorite_count":null,
        "Question_last_edit_time":1550840459832,
        "Question_score":0.0,
        "Question_view_count":180.0,
        "Answer_body":"<p>Yes you can Using <code>S3<\/code> event notification for object-created and call a <code>lambda<\/code> for creating endpoint for <code>sagemaker<\/code>.<\/p>\n\n<p>This example shows how to make <code>object-created event trigger lambda<\/code><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/with-s3.html<\/a><\/p>\n\n<p>You can use <code>python sdk<\/code> to create endpoint for <code>sagemaker<\/code><\/p>\n\n<p><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_endpoint<\/a><\/p>\n\n<p>But it might be slow for creating endpoint so you may be need to wait.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1550837083230,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54825390",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1550836836772,
        "Question_original_content":"creat aw sagemk endpoint lambda function creat endpoint work creat lambda function python take file invok endpoint output file wonder creat endpoint event file upload bucket delet endpoint",
        "Question_preprocessed_content":"creat aw sagemk endpoint lambda function creat endpoint work creat function take file invok endpoint output file wonder creat endpoint event file upload delet endpoint",
        "Question_gpt_summary_original":"The user has successfully created an endpoint in AWS Sagemaker and a Lambda function in Python 3.6 that takes files from S3, invokes the endpoint, and puts the output in a file in S3. However, the user is facing a challenge of creating the endpoint at every event (a file uploaded in an S3 bucket) and then deleting the endpoint.",
        "Question_gpt_summary":"user successfulli creat endpoint lambda function python take file invok endpoint put output file user face challeng creat endpoint event file upload bucket delet endpoint",
        "Answer_original_content":"ye event notif object creat lambda creat endpoint exampl show object creat event trigger lambda http doc aw amazon com lambda latest html us python sdk creat endpoint http boto amazonaw com document api latest refer servic html client creat endpoint slow creat endpoint need wait",
        "Answer_preprocessed_content":"ye event notif creat endpoint exampl show us creat endpoint slow creat endpoint need wait",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- use s3 event notification for object-created to trigger a lambda function for creating the endpoint.\n- use the python sdk (boto3) to create the endpoint.\n- be aware that creating the endpoint using the sdk might be slow, so you may need to wait.",
        "Answer_gpt_summary":"possibl solut answer us event notif object creat trigger lambda function creat endpoint us python sdk boto creat endpoint awar creat endpoint sdk slow need wait"
    },
    {
        "Question_id":null,
        "Question_title":"Set Github as Artifacts location",
        "Question_body":"Im testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1561645149000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":20.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/QKRO33wr3hM",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-27T15:34:12",
                "Answer_body":"No, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia....@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-06-27T15:36:20",
                "Answer_body":"Hi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/F25604B5-40AA-4907-960B-C588609BC72A%40databricks.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-06-27T16:03:55",
                "Answer_body":"Thank you, Matei and Zahir.\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\nHi, I think you should use the right tool for the right job. GitHub is for sharing code.\nThx\n\n\n\nOn 27 Jun 2019, at 21:34, Matei Zaharia <ma...@databricks.com> wrote:\n\n\nNo, you can\u2019t use GitHub by default, but it may be possible to write an artifact store plugin that does that. Our idea was that artifacts can be very large, however (multi-gigabyte models or multi-terabyte datasets), so it might not make sense to use GitHub.\n\n\nMatei\n\n\n\nOn Jun 27, 2019, at 11:19 AM, SoniaK <sofia...@8451.com> wrote:\n\n\nIm testing MLFLOW 1.0.0. I run MLFLOW in docker container, myArifacts location is Minio bucket (Minio is running in its own docker container), backend store is Postgres ( running in its own docker container); I train my model in Jupiter notebook ( also in its own docker container). Is it possible to use Github as an artifact location?\n\n\nThank you.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/59f7455b-4b7b-41f5-8058-6bb231bb0807%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout.\n\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-07-01T18:14:36",
                "Answer_body":"Hi Zahir.\u00a0 If GitHub is not the right tool for the job (I don't disagree with that), what would you say IS the right tool for persisting artifacts?\n\nThanks in advance!\n\n\nOn Thursday, June 27, 2019 at 2:36:20 PM UTC-5, zahir hamroune wrote:\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-07-02T06:37:31",
                "Answer_body":"You can use a shared filesystem or blob store, such as AWS S3, Azure Blob Storage, HDFS, or a POSIX file system set up through NFS.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/9bd4f3f5-4574-473e-8fd9-81361945f52f%40googlegroups.com.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"set github artifact locat test run docker contain myarifact locat minio bucket minio run docker contain backend store postgr run docker contain train model jupit notebook docker contain possibl us github artifact locat thank",
        "Question_preprocessed_content":"set github artifact locat test run docker contain myarifact locat minio bucket backend store postgr train model jupit notebook possibl us github artifact locat thank",
        "Question_gpt_summary_original":"The user is facing a challenge of whether it is possible to use Github as an artifact location while testing MLFLOW 1.0.0. They are currently running MLFLOW, Minio, Postgres, and Jupiter notebook in separate docker containers, with Minio serving as the current artifact location.",
        "Question_gpt_summary":"user face challeng possibl us github artifact locat test current run minio postgr jupit notebook separ docker contain minio serv current artifact locat",
        "Answer_original_content":"us github default possibl write artifact store plugin idea artifact larg multi gigabyt model multi terabyt dataset sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket minio run docker contain backend store postgr run docker contain train model jupit notebook docker contain possibl us github artifact locat thank receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bbbb googlegroup com option visit http group googl com optout think us right tool right job github share code thx view discuss web visit http group googl com msgid user cbca databrick com thank matei zahir thursdai june utc zahir hamroun wrote think us right tool right job github share code thx jun matei zaharia wrote us github default possibl write artifact store plugin idea artifact larg multi gigabyt model multi terabyt dataset sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket minio run docker contain backend store postgr run docker contain train model jupit notebook docker contain possibl us github artifact locat thank receiv messag subscrib googl group user group unsubscrib group stop receiv email send email googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bbbb googlegroup com option visit http group googl com optout receiv messag subscrib googl group user group unsubscrib group stop receiv email send email googlegroup com zahir github right tool job disagre right tool persist artifact thank advanc thursdai june utc zahir hamroun wrote us share filesystem blob store aw azur blob storag hdf posix file set nf receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user bdff googlegroup com",
        "Answer_preprocessed_content":"us github default possibl write artifact store plugin idea artifact larg sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket backend store postgr train model jupit notebook possibl us github artifact locat thank receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit think us right tool right job github share code thx view discuss web visit thank matei zahir thursdai june zahir hamroun wrote think us right tool right job github share code thx jun matei zaharia wrote us github default possibl write artifact store plugin idea artifact larg sens us github matei jun soniak wrote test run docker contain myarifact locat minio bucket backend store postgr train model jupit notebook possibl us github artifact locat thank receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit receiv messag subscrib googl group group unsubscrib group stop receiv email send email zahir github right tool job right tool persist artifact thank advanc thursdai june zahir hamroun wrote us share filesystem blob store aw azur blob storag hdf posix file set nf receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- GitHub cannot be used by default as an artifact location.\n- It may be possible to write an artifact store plugin that uses GitHub, but it might not make sense to use GitHub for large artifacts.\n- Shared filesystem or blob store such as AWS S3, Azure Blob Storage, HDFS, or a POSIX file system set up through NFS can be used as an alternative to GitHub.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss github default artifact locat possibl write artifact store plugin us github sens us github larg artifact share filesystem blob store aw azur blob storag hdf posix file set nf altern github person opinion bias includ summari"
    },
    {
        "Question_id":65609804.0,
        "Question_title":"How to append stepfunction execution id to SageMaker job names?",
        "Question_body":"<p>I have a step function statemachine which creates SageMaker batch transform job, the definition is written in Terraform, I wanted to add the stepfunction execution id to the batch transform job names:<\/p>\n<p>in stepfunction terraform file:<\/p>\n<pre><code>  definition = templatefile(&quot;stepfuntion.json&quot;,\n    {\n      xxxx\n)\n<\/code><\/pre>\n<p>in the &quot;stepfuntion.json&quot;:<\/p>\n<pre><code>{...\n          &quot;TransformJobName&quot;: &quot;jobname-$$.Execution.Id&quot;,\n  \n          }\n      },\n        &quot;End&quot;: true\n      }\n    }\n  }\n<\/code><\/pre>\n<p>But after terraform apply, it didn't generate the actual id, it gave me <code>jobname-$$.Execution.Id<\/code>, can anyone help with this please?<\/p>\n<p>Resources: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/input-output-contextobject.html<\/a>\n&quot;To access the context object, first specify the parameter name by appending .$ to the end, as you do when selecting state input with a path. Then, to access context object data instead of the input, prepend the path with $$.. This tells AWS Step Functions to use the path to select a node in the context object.&quot;<\/p>\n<p>Can someone tell me what I'm missing please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610012100150,
        "Question_favorite_count":null,
        "Question_last_edit_time":1610014278923,
        "Question_score":0.0,
        "Question_view_count":488.0,
        "Answer_body":"<p>The var you are trying to use terraform doesn't know about it<\/p>\n<blockquote>\n<p>jobname-$$.Execution.Id.<\/p>\n<\/blockquote>\n<p>That's something specific to the Step function and available within state machine not available for terraform.<\/p>",
        "Answer_comment_count":17.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65609804",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1610017101707,
        "Question_original_content":"append stepfunct execut job name step function statemachin creat batch transform job definit written terraform want add stepfunct execut batch transform job name stepfunct terraform file definit templatefil stepfunt json stepfunt json transformjobnam jobnam execut end true terraform appli gener actual gave jobnam execut help resourc http doc aw amazon com step function latest input output contextobject html access context object specifi paramet append end select state input path access context object data instead input prepend path tell aw step function us path select node context object tell miss",
        "Question_preprocessed_content":"append stepfunct execut job name step function statemachin creat batch transform job definit written terraform want add stepfunct execut batch transform job name stepfunct terraform file terraform appli gener actual gave help resourc access context object specifi paramet append end select state input path access context object data instead input prepend path tell aw step function us path select node context tell miss",
        "Question_gpt_summary_original":"The user is trying to add the step function execution id to the batch transform job names in a Terraform file, but after applying the changes, it did not generate the actual id and instead gave the user \"jobname-$$.Execution.Id\". The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user try add step function execut batch transform job name terraform file appli chang gener actual instead gave user jobnam execut user seek help resolv issu",
        "Answer_original_content":"var try us terraform know jobnam execut specif step function avail state machin avail terraform",
        "Answer_preprocessed_content":"var try us terraform know specif step function avail state machin avail terraform",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the answer explains that the variable the user is trying to use is specific to stepfunction and not available in terraform.",
        "Answer_gpt_summary":"solut provid answer answer explain variabl user try us specif stepfunct avail terraform"
    },
    {
        "Question_id":73388133.0,
        "Question_title":"Is there a way for Optuna `suggest_categorical`to return multiple choices from list?",
        "Question_body":"<p>I am using Optuna for hyperparametrization of my model. And i have a field where I want to test multiple combinations from a list. For example: I have <code>[&quot;lisa&quot;,&quot;adam&quot;,&quot;test&quot;]<\/code> and i want <code>suggest_categorical<\/code> to return not just one, but a random combination: maybe <code>[&quot;lisa&quot;, &quot;adam&quot;]<\/code>, maybe <code>[&quot;adam&quot;]<\/code>, maybe <code>[&quot;lisa&quot;, &quot;adam&quot;, &quot;test&quot;]<\/code>. Is there a way to get this with built in Optuna function?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660737627797,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Answer_body":"<p>You could use <code>itertools.combinations<\/code> to generate all possible combinations of list items and then pass them to optuna's <code>suggest_categorical<\/code> as choices:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport itertools\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# generate the combinations\niterable = ['lisa', 'adam', 'test']\ncombinations = []\nfor r in range(1, len(iterable) + 1):\n    combinations.extend([list(x) for x in itertools.combinations(iterable=iterable, r=r)])\nprint(combinations)\n# [['lisa'], ['adam'], ['test'], ['lisa', 'adam'], ['lisa', 'test'], ['adam', 'test'], ['lisa', 'adam', 'test']]\n\n# sample the combinations\ndef objective(trial):\n    combination = trial.suggest_categorical(name='combination', choices=combinations)\n    return round(random.random(), 2)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=3)\n# [I 2022-08-18 08:03:51,658] A new study created in memory with name: no-name-3874ce95-2394-4526-bb19-0d9822d7e45c\n# [I 2022-08-18 08:03:51,659] Trial 0 finished with value: 0.94 and parameters: {'combination': ['adam']}. Best is trial 0 with value: 0.94.\n# [I 2022-08-18 08:03:51,660] Trial 1 finished with value: 0.87 and parameters: {'combination': ['lisa', 'test']}. Best is trial 1 with value: 0.87.\n# [I 2022-08-18 08:03:51,660] Trial 2 finished with value: 0.29 and parameters: {'combination': ['lisa', 'adam']}. Best is trial 2 with value: 0.29.\n<\/code><\/pre>\n<p>Using lists as choices in optuna's <code>suggest_categorical<\/code> throws a warning message, but apparently this is mostly inconsequential (see <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2341\" rel=\"nofollow noreferrer\">this issue<\/a> in optuna's GitHub repository).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1660803027380,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73388133",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660802705056,
        "Question_original_content":"wai suggest categor return multipl choic list hyperparametr model field want test multipl combin list exampl lisa adam test want suggest categor return random combin mayb lisa adam mayb adam mayb lisa adam test wai built function",
        "Question_preprocessed_content":"wai return multipl choic list hyperparametr model field want test multipl combin list exampl want return random combin mayb mayb mayb wai built function",
        "Question_gpt_summary_original":"The user is facing a challenge with using Optuna for hyperparameter optimization of their model. They want to test multiple combinations from a list using the `suggest_categorical` function, but the function only returns one choice at a time. The user is looking for a way to get a random combination of choices from the list using the built-in Optuna function.",
        "Question_gpt_summary":"user face challeng hyperparamet optim model want test multipl combin list suggest categor function function return choic time user look wai random combin choic list built function",
        "Answer_original_content":"us itertool combin gener possibl combin list item pass suggest categor choic import import itertool import random import warn warn filterwarn ignor gener combin iter lisa adam test combin rang len iter combin extend list itertool combin iter iter print combin lisa adam test lisa adam lisa test adam test lisa adam test sampl combin def object trial combin trial suggest categor combin choic combin return round random random studi creat studi studi optim object trial new studi creat memori ddec trial finish valu paramet combin adam best trial valu trial finish valu paramet combin lisa test best trial valu trial finish valu paramet combin lisa adam best trial valu list choic suggest categor throw warn messag appar inconsequenti issu github repositori",
        "Answer_preprocessed_content":"us gener possibl combin list item pass choic list choic throw warn messag appar inconsequenti",
        "Answer_gpt_summary_original":"Possible solutions extracted from the answer are:\n\n- Use itertools.combinations to generate all possible combinations of list items.\n- Pass the generated combinations to optuna's suggest_categorical as choices.\n- Sample the combinations using optuna's create_study and optimize functions.\n\nThe answer provides a code snippet that demonstrates how to implement these solutions. Additionally, it mentions a warning message that may be thrown when using lists as choices in optuna's suggest_categorical, but notes that this warning is mostly inconsequential.",
        "Answer_gpt_summary":"possibl solut extract answer us itertool combin gener possibl combin list item pass gener combin suggest categor choic sampl combin creat studi optim function answer provid code snippet demonstr implement solut addition mention warn messag thrown list choic suggest categor note warn inconsequenti"
    },
    {
        "Question_id":null,
        "Question_title":"Train and deploy H2O model using MLFlow spark",
        "Question_body":"Hi,\n\n\nI want to train and deploy a h2o model using mlfow spark as mentioned in the diagram:\u00a0https:\/\/res.infoq.com\/presentations\/mlflow-databricks\/en\/slides\/sl21-1566324281761.jpg\nI am training the model using below link:\nhttps:\/\/docs.databricks.com\/_static\/notebooks\/h2o-sparkling-water-python.html\nThen after training when I try to deploy the model using mlflow spark, it throws an error \"MLFlow can only save descendants of pyspark.ml.Model which implement MLReadable and MLWritable\".\nCan anyone help and let me know what I am doing wrong.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1572311154000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":9.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/RJBzJjxPl0w",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-11-28T22:22:06",
                "Answer_body":"If you are unable to save due to custom transformer maybe you can check this\u00a0saving spark custom transformer\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"train deploi model spark want train deploi model mlfow spark mention diagram http re infoq com present databrick slide jpg train model link http doc databrick com static notebook sparkl water python html train try deploi model spark throw error save descend pyspark model implement mlreadabl mlwritabl help let know wrong",
        "Question_preprocessed_content":"train deploi model spark want train deploi model mlfow spark mention train model link train try deploi model spark throw error save descend implement mlreadabl mlwritabl help let know wrong",
        "Question_gpt_summary_original":"The user is facing a challenge in deploying a H2O model using MLFlow spark. They have successfully trained the model using a provided link, but when attempting to deploy it using MLFlow spark, they receive an error message stating that MLFlow can only save descendants of pyspark.ml.Model which implement MLReadable and MLWritable. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user face challeng deploi model spark successfulli train model provid link attempt deploi spark receiv error messag state save descend pyspark model implement mlreadabl mlwritabl user seek assist resolv issu",
        "Answer_original_content":"unabl save custom transform mayb check thissav spark custom transform",
        "Answer_preprocessed_content":"unabl save custom transform mayb check thissav spark custom transform",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a solution for saving a custom transformer in Spark. However, no other solutions are mentioned.",
        "Answer_gpt_summary":"solut discuss provid link solut save custom transform spark solut mention"
    },
    {
        "Question_id":60180314.0,
        "Question_title":"Azure machine learning failing on sample for training",
        "Question_body":"<p>I am trying to run below sample code in my notebook, Running on python 3.6 kernel.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-models-with-aml<\/a>\nDownload the MNIST dataset<\/p>\n\n<p>The following code failed with the attribute error, on line of the following code from azureml.opendatasets import MNIST <\/p>\n\n<pre><code>from azureml.core import Dataset\nfrom azureml.opendatasets import MNIST\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1581475311887,
        "Question_favorite_count":null,
        "Question_last_edit_time":1581477772576,
        "Question_score":0.0,
        "Question_view_count":555.0,
        "Answer_body":"<p>Prerequisites:\nThe tutorial and accompanying utils.py file is also available on <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/tree\/master\/tutorials\" rel=\"nofollow noreferrer\">GitHub<\/a> if you wish to use it on your own <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-environment#local\" rel=\"nofollow noreferrer\">local environment<\/a>. Run pip install azureml-sdk[notebooks] azureml-opendatasets matplotlib to install dependencies for this tutorial.<\/p>\n\n<p>If you are using older version then upgrade to the latest Azure ML SDK Version 1.0.85.<\/p>\n\n<p>!pip install --upgrade azureml-sdk<\/p>\n\n<pre><code># check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)\n<\/code><\/pre>\n\n<p>Also <\/p>\n\n<p>!pip install --upgrade azureml-opendataset <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1581482106848,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60180314",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581481802847,
        "Question_original_content":"fail sampl train try run sampl code notebook run python kernel http doc microsoft com azur machin learn tutori train model aml download mnist dataset follow code fail attribut error line follow code opendataset import mnist core import dataset opendataset import mnist",
        "Question_preprocessed_content":"fail sampl train try run sampl code notebook run python kernel download mnist dataset follow code fail attribut error line follow code opendataset import mnist",
        "Question_gpt_summary_original":"The user encountered an attribute error while trying to run a sample code in their notebook for training a model using Azure machine learning. The error occurred on the line of code importing the MNIST dataset from azureml.opendatasets.",
        "Question_gpt_summary":"user encount attribut error try run sampl code notebook train model error occur line code import mnist dataset opendataset",
        "Answer_original_content":"prerequisit tutori accompani util file avail github wish us local environ run pip instal sdk notebook opendataset matplotlib instal depend tutori older version upgrad latest sdk version pip instal upgrad sdk check core sdk version number print sdk version core version pip instal upgrad opendataset",
        "Answer_preprocessed_content":"prerequisit tutori accompani file avail github wish us local environ run pip instal sdk opendataset matplotlib instal depend tutori older version upgrad latest sdk version pip instal sdk pip instal opendataset",
        "Answer_gpt_summary_original":"The answer provides possible solutions to an attribute error encountered while trying to run sample code for training. The solutions include checking and upgrading the Azure ML SDK version, installing dependencies using pip, and installing\/upgrading the azureml-opendataset package.",
        "Answer_gpt_summary":"answer provid possibl solut attribut error encount try run sampl code train solut includ check upgrad sdk version instal depend pip instal upgrad opendataset packag"
    },
    {
        "Question_id":null,
        "Question_title":"Tensorflow and Azure machine learning",
        "Question_body":"Is azure working well with Tensorflow framework? I don\u2019t see any document about it. Any help is good.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653989511207,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @Chungsun-1776\n\nWelcome to the Microsoft Q&A Platform,\n\nTensorFlow is supported on Azure Machine Learning:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-tensorflow\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-machine-learning-tensorflow?tabs=bash\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-keras\n\nI hope this helps!\n\nPlease don\u2019t forget to \"Accept the answer\" and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/871068\/tensorflow-and-azure-machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-31T10:26:09.017Z",
                "Answer_score":1,
                "Answer_body":"Hello @Chungsun-1776\n\nWelcome to the Microsoft Q&A Platform,\n\nTensorFlow is supported on Azure Machine Learning:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-tensorflow\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-machine-learning-tensorflow?tabs=bash\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-keras\n\nI hope this helps!\n\nPlease don\u2019t forget to \"Accept the answer\" and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1653992769016,
        "Question_original_content":"tensorflow azur work tensorflow framework dont document help good",
        "Question_preprocessed_content":"tensorflow azur work tensorflow framework dont document help good",
        "Question_gpt_summary_original":"The user is facing a challenge in finding documentation on whether Azure works well with the Tensorflow framework and is seeking assistance.",
        "Question_gpt_summary":"user face challeng find document azur work tensorflow framework seek assist",
        "Answer_original_content":"hello chungsun welcom microsoft platform tensorflow support http doc microsoft com azur machin learn train tensorflow http doc microsoft com azur azur function function machin learn tensorflow tab bash http doc microsoft com azur machin learn train kera hope help dont forget accept answer vote inform provid help benefici commun member",
        "Answer_preprocessed_content":"hello welcom microsoft platform tensorflow support hope help dont forget accept answer inform provid help benefici commun member",
        "Answer_gpt_summary_original":"possible solutions to determine if azure is compatible with the tensorflow framework are provided in the answer. the answer includes links to microsoft documentation that show tensorflow is supported on azure for machine learning and azure functions. the answer also includes a link to documentation on how to train keras on azure.",
        "Answer_gpt_summary":"possibl solut determin azur compat tensorflow framework provid answer answer includ link microsoft document tensorflow support azur machin learn azur function answer includ link document train kera azur"
    },
    {
        "Question_id":60197897.0,
        "Question_title":"Does Sagemaker pass any data other than the model itself between training and prediction steps?",
        "Question_body":"<p>I'm building a Scikit-learn model on Sagemaker.<\/p>\n\n<p>I'd like to reference the data used in training in my <code>predict_fn<\/code>. (Instead of the indices returned from NNS, I'd like to return the names and data of each neighbor.)<\/p>\n\n<p>I know this can be done by writing\/reading from S3, as in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a> , but was wondering if there were more elegant solutions.<\/p>\n\n<p>Are there other ways to make the data used in the training job available to the prediction function?<\/p>\n\n<p>Edit: Using the advice from the accepted solution I was able to pass data as a dict.<\/p>\n\n<pre><code>model = nn.fit(train_data)\n\nmodel_dict = {\n   \"model\": model,\n   \"reference\": train_data\n}\n\njoblib.dump(model_dict, path)\n<\/code><\/pre>\n\n<p>predict_fn:<\/p>\n\n<pre><code>def predict_fn(input_data, model_dict):\n   model = model_dict[\"model\"]\n   reference = model_dict[\"reference\"]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581546836597,
        "Question_favorite_count":null,
        "Question_last_edit_time":1582837925156,
        "Question_score":0.0,
        "Question_view_count":140.0,
        "Answer_body":"<p>you can bring to the endpoint instance (either in the <code>model.tar.gz<\/code> or via later download) a file storing the mapping between indexes and record names; this way you can translate from neighbor IDs to record names on the fly in the <code>predict_fn<\/code> or in the <code>output_fn<\/code>. For giant indexes this mapping (along with other metadata) can be in an external database too (eg dynamoDB, redis)<\/p>\n\n<p>the link you attach (SageMaker Batch Transform) is quite a different concept; it's for instantiating ephemeral fleet of machine(s) to run a one-time prediction task with input data in S3 and results written to s3. You question seem to refer to the alternative, permanent, real-time endpoint deployment mode.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60197897",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581723595003,
        "Question_original_content":"pass data model train predict step build scikit learn model like refer data train predict instead indic return nn like return name data neighbor know write read http aw amazon com blog machin learn associ predict result input data amazon batch transform wonder eleg solut wai data train job avail predict function edit advic accept solut abl pass data dict model fit train data model dict model model refer train data joblib dump model dict path predict def predict input data model dict model model dict model refer model dict refer",
        "Question_preprocessed_content":"pass data model train predict step build model like refer data train know wonder eleg solut wai data train job avail predict function edit advic accept solut abl pass data dict",
        "Question_gpt_summary_original":"The user is building a Scikit-learn model on Sagemaker and wants to reference the data used in training in their predict function. They are looking for ways to make the data used in the training job available to the prediction function other than writing\/reading from S3. The user was able to pass data as a dictionary using the advice from the accepted solution.",
        "Question_gpt_summary":"user build scikit learn model want refer data train predict function look wai data train job avail predict function write read user abl pass data dictionari advic accept solut",
        "Answer_original_content":"bring endpoint instanc model tar later download file store map index record name wai translat neighbor id record name fly predict output giant index map metadata extern databas dynamodb redi link attach batch transform differ concept instanti ephemer fleet machin run time predict task input data result written question refer altern perman real time endpoint deploy mode",
        "Answer_preprocessed_content":"bring endpoint instanc file store map index record name wai translat neighbor id record name fly giant index map extern databas link attach differ concept instanti ephemer fleet machin run predict task input data result written question refer altern perman endpoint deploy mode",
        "Answer_gpt_summary_original":"the answer suggests that a possible solution to pass data used in the training job to the prediction function is to bring a file storing the mapping between indexes and record names to the endpoint instance. this way, it is possible to translate from neighbor ids to record names on the fly in the predict_fn or in the output_fn. for large indexes, this mapping can be stored in an external database such as dynamodb or redis. the answer also clarifies that the link provided in the question refers to a different concept, which is for a one-time prediction task with input data in s3 and results written to s3, while the question seems to refer to the alternative, permanent, real-time endpoint deployment mode.",
        "Answer_gpt_summary":"answer suggest possibl solut pass data train job predict function bring file store map index record name endpoint instanc wai possibl translat neighbor id record name fly predict output larg index map store extern databas dynamodb redi answer clarifi link provid question refer differ concept time predict task input data result written question refer altern perman real time endpoint deploy mode"
    },
    {
        "Question_id":null,
        "Question_title":"Does Ground Truth Support Circles?",
        "Question_body":"Customer has circular objects in their data. Does Ground Truth support drawing circles rather than boxes out of the box (no pun intended)? I know that it supports semantic segmentation, but that is overkill in this case.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595625000000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":27.0,
        "Answer_body":"As of July 2020, we currently have Crowd HTML Element support for bounding box and polygons.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqVY0A3PIQsuJpcce1tjJcA\/does-ground-truth-support-circles",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-24T21:37:59.000Z",
                "Answer_score":0,
                "Answer_body":"As of July 2020, we currently have Crowd HTML Element support for bounding box and polygons.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1595626679000,
        "Question_original_content":"ground truth support circl custom circular object data ground truth support draw circl box box pun intend know support semant segment overkil case",
        "Question_preprocessed_content":"ground truth support circl custom circular object data ground truth support draw circl box box know support semant segment overkil case",
        "Question_gpt_summary_original":"The user is facing a challenge in using Ground Truth for their circular objects as they are unsure if it supports drawing circles instead of boxes. They are aware of semantic segmentation but feel it is unnecessary for their needs.",
        "Question_gpt_summary":"user face challeng ground truth circular object unsur support draw circl instead box awar semant segment feel unnecessari need",
        "Answer_original_content":"juli current crowd html element support bound box polygon",
        "Answer_preprocessed_content":"juli current crowd html element support bound box polygon",
        "Answer_gpt_summary_original":"possible solutions to the user's question are that they can use polygons instead of boxes to draw around circular objects in their data, and that they can utilize crowd html element support for bounding box and polygons. the answer does not mention anything about semantic segmentation being an overkill solution.",
        "Answer_gpt_summary":"possibl solut user question us polygon instead box draw circular object data util crowd html element support bound box polygon answer mention semant segment overkil solut"
    },
    {
        "Question_id":62725467.0,
        "Question_title":"AWS Sagemaker, how to connect existing local jupyter notebook with ML algorithms to AWS",
        "Question_body":"<p>I have written a code for ML in my local machine in jupyter notebook. I notice that AWS Sagemaker has its own notebook instances too. '<\/p>\n<ol>\n<li>Can I upload my existing local jupyter notebook in AWS Sagemaker directly?\nor<\/li>\n<li>Do I need to type the entire code again in instance of AWS Sagemaker jupyter notebook?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593838237637,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":155.0,
        "Answer_body":"<ol>\n<li><p>Can I upload my existing local jupyter notebook in AWS Sagemaker directly? or\nYes you can upload from local disk or computer<\/p>\n<\/li>\n<li><p>Do I need to type the entire code again in instance of AWS Sagemaker jupyter notebook?\nNo<\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62725467",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1598173972672,
        "Question_original_content":"connect exist local jupyt notebook algorithm aw written code local machin jupyt notebook notic notebook instanc upload exist local jupyt notebook directli need type entir code instanc jupyt notebook",
        "Question_preprocessed_content":"connect exist local jupyt notebook algorithm aw written code local machin jupyt notebook notic notebook instanc upload exist local jupyt notebook directli need type entir code instanc jupyt notebook",
        "Question_gpt_summary_original":"The user is facing challenges in connecting their existing local Jupyter notebook with ML algorithms to AWS Sagemaker. They are unsure whether they can directly upload their local notebook to Sagemaker or if they need to retype the entire code in the Sagemaker instance.",
        "Question_gpt_summary":"user face challeng connect exist local jupyt notebook algorithm unsur directli upload local notebook need retyp entir code instanc",
        "Answer_original_content":"upload exist local jupyt notebook directli ye upload local disk need type entir code instanc jupyt notebook",
        "Answer_preprocessed_content":"upload exist local jupyt notebook directli ye upload local disk need type entir code instanc jupyt notebook",
        "Answer_gpt_summary_original":"possible solutions: \n- the user can upload their existing local jupyter notebook directly to aws. \n- they can upload it from their local disk or computer. \n- they do not need to type the entire code again in the instance of jupyter notebook.",
        "Answer_gpt_summary":"possibl solut user upload exist local jupyt notebook directli aw upload local disk need type entir code instanc jupyt notebook"
    },
    {
        "Question_id":null,
        "Question_title":"Why is Designer so slow to execute?",
        "Question_body":"I'm running the simple tutorials, preparing for DP 100. I was wondering why the execution of Designer Pipelines is so slow, even when running very simple operations and minuscule DataFrames such as Automobile price data (Raw). I also may start working for a company that has been using Azure for Machine Learning and the interviewer commented something along the lines of being it very very slow.\n\nIf a very simple model training pipeline on a 200 records DataFrame took almost 20 minutes, I keep wondering how long it would take to compute a real world data pipeline.\n\nAny insights? Thank you.",
        "Question_answer_count":5,
        "Question_comment_count":1,
        "Question_creation_time":1601819361423,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/116085\/why-is-designer-so-slow-to-execute.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-05T10:10:32.187Z",
                "Answer_score":0,
                "Answer_body":"@ivangvi can you please check if your compute has spinned up or is a cold compute in this case? If you are running on a cold compute, it may take several minutes to spin up. Also Azure Machine Learning is running on the backend of Azure Machine Learning pipeline, so if your input data hasn't changed, next time pipeline is automatically using the cached result of that module so it should be fast compared with first time running time.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-13T21:28:42.713Z",
                "Answer_score":0,
                "Answer_body":"Designer is pretty interesting, but I agree: IT IS TOO SLOW. Unusably slow, unfortunately.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-12T19:30:38.833Z",
                "Answer_score":0,
                "Answer_body":"I can't believe it has been over 7 months since the first post in this thread and this fundamental issue still persists! As @ivangvi indicating, simply following Microsoft's own ML tutorial (here: https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/create-regression-model-azure-machine-learning-designer\/explore-data), takes 20 min just to apply a few simple transformation to a 205 row x 36 column dataset. Just removing one column took 5 min! The whole thing would take milliseconds in a local Jupiter notebook. Why would anyone ever use this? I'm baffled.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-17T12:16:01.32Z",
                "Answer_score":0,
                "Answer_body":"Same for me, I am experimenting with some Udacity courses that were created by MS, and the pipelines are so slow to the point of unbearabe. For on example the course says that the pipeline should take around 10 mins and the actual time was 50 mins. There was another case were my lab free time of 1hr elapsed and the pipeline didnt complete the execution....",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T17:40:02.17Z",
                "Answer_score":0,
                "Answer_body":"I used to teach Azure Classic and it was great. Now, I need to switch to Azure Designer but it is too slow.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"design slow execut run simpl tutori prepar wonder execut design pipelin slow run simpl oper minuscul datafram automobil price data raw start work compani azur machin learn interview comment line slow simpl model train pipelin record datafram took minut wonder long comput real world data pipelin insight thank",
        "Question_preprocessed_content":"design slow execut run simpl tutori prepar wonder execut design pipelin slow run simpl oper minuscul datafram automobil price data start work compani azur machin learn interview comment line slow simpl model train pipelin record datafram took minut wonder long comput real world data pipelin insight thank",
        "Question_gpt_summary_original":"The user is experiencing slow execution of Designer Pipelines even when running simple operations and small DataFrames. They are concerned about the potential impact on real-world data pipelines and are seeking insights to address the issue.",
        "Question_gpt_summary":"user experienc slow execut design pipelin run simpl oper small datafram concern potenti impact real world data pipelin seek insight address issu",
        "Answer_original_content":"ivangvi check comput spin cold comput case run cold comput minut spin run backend pipelin input data hasn chang time pipelin automat cach result modul fast compar time run time design pretti interest agre slow unus slow unfortun believ month post thread fundament issu persist ivangvi indic simpli follow microsoft tutori http doc microsoft com learn modul creat regress model azur machin learn design explor data take min appli simpl transform row column dataset remov column took min thing millisecond local jupit notebook us baffl experi udac cours creat pipelin slow point unbearab exampl cours sai pipelin min actual time min case lab free time elaps pipelin didnt complet execut teach azur classic great need switch azur design slow",
        "Answer_preprocessed_content":"check comput spin cold comput case run cold comput minut spin run backend pipelin input data hasn chang time pipelin automat cach result modul fast compar time run time design pretti interest agre slow unus slow unfortun believ month post thread fundament issu persist indic simpli follow microsoft tutori take min appli simpl transform row column dataset remov column took min thing millisecond local jupit notebook us baffl experi udac cours creat pipelin slow point unbearab exampl cours sai pipelin min actual time min case lab free time elaps pipelin didnt complet teach azur classic great need switch azur design slow",
        "Answer_gpt_summary_original":"possible solutions to the slow execution times in azure designer include checking if the compute has spinned up or is a cold compute, using the cached result of the module if the input data hasn't changed, and being patient with the slow performance of the designer. however, the answer also suggests that the designer is too slow and unusable, and that using a local jupiter notebook would be faster.",
        "Answer_gpt_summary":"possibl solut slow execut time azur design includ check comput spin cold comput cach result modul input data hasn chang patient slow perform design answer suggest design slow unus local jupit notebook faster"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker Batch Transform local mode?",
        "Question_body":"Hi,\n\nA customer is experimenting with SageMaker batch transform with parquet and is interested is some form of local development to speedup iteration. Does SageMaker Batch Transform support local mode?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1571055107000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":151.0,
        "Answer_body":"You can do local testing by running the container in serve mode as a docker. Then using Curl\/Postman to send an HTTP request and inspecting the response.\n\nThe request can be CSV\/JSON or binary (a parquet file in your case).\n\nIf you're able to run the Pytorch model in serve mode locally, then this local testing provides a lot of coverage before running in Batch Transform itself.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtNtH0LyFSLCXc0xCV4hYkw\/sage-maker-batch-transform-local-mode",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-10-17T09:18:46.000Z",
                "Answer_score":0,
                "Answer_body":"You can do local testing by running the container in serve mode as a docker. Then using Curl\/Postman to send an HTTP request and inspecting the response.\n\nThe request can be CSV\/JSON or binary (a parquet file in your case).\n\nIf you're able to run the Pytorch model in serve mode locally, then this local testing provides a lot of coverage before running in Batch Transform itself.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1571303926000,
        "Question_original_content":"batch transform local mode custom experi batch transform parquet interest form local develop speedup iter batch transform support local mode",
        "Question_preprocessed_content":"batch transform local mode custom experi batch transform parquet interest form local develop speedup iter batch transform support local mode",
        "Question_gpt_summary_original":"The user is facing a challenge with SageMaker Batch Transform and is looking for a way to speed up iteration through local development. They are specifically interested in whether SageMaker Batch Transform supports local mode.",
        "Question_gpt_summary":"user face challeng batch transform look wai speed iter local develop specif interest batch transform support local mode",
        "Answer_original_content":"local test run contain serv mode docker curl postman send http request inspect respons request csv json binari parquet file case abl run pytorch model serv mode local local test provid lot coverag run batch transform",
        "Answer_preprocessed_content":"local test run contain serv mode docker send http request inspect respons request binari abl run pytorch model serv mode local local test provid lot coverag run batch transform",
        "Answer_gpt_summary_original":"possible solutions to speed up experimentation with parquet using batch transform in local mode include running the container in serve mode as a docker, sending an http request using curl\/postman, and inspecting the response. the request can be csv\/json or binary (a parquet file in this case). local testing provides a lot of coverage before running in batch transform itself, especially if the pytorch model can be run in serve mode locally.",
        "Answer_gpt_summary":"possibl solut speed experiment parquet batch transform local mode includ run contain serv mode docker send http request curl postman inspect respons request csv json binari parquet file case local test provid lot coverag run batch transform especi pytorch model run serv mode local"
    },
    {
        "Question_id":36126897.0,
        "Question_title":"How to restart VM in Azure ML Notebook?",
        "Question_body":"<p>I am writing code in Jupyter Notebook in Azure ML Studio.<\/p>\n\n<p>At current moment every command causes kernel death. Even in new clear notebook I could not execute even <code>print 'hello'<\/code> - kernel died immediately.<\/p>\n\n<p>Also I could not use bash commands like <code>!ls<\/code> - It crashes kernel too.<\/p>\n\n<p>How could I restart my VM or restart session in Azure ML Studio with killing all running VM?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1458551625417,
        "Question_favorite_count":null,
        "Question_last_edit_time":1458567186852,
        "Question_score":0.0,
        "Question_view_count":488.0,
        "Answer_body":"<p>I have found that if I go from notebook menu to File->Open, then I see all my notebooks, their statuses and I could shutdown them.<\/p>\n\n<p>Also I have found that some of my closed notebooks were still alive and have shut them down.<\/p>\n\n<p>After this my working notebook came back to life.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36126897",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1458573398360,
        "Question_original_content":"restart notebook write code jupyt notebook studio current moment command caus kernel death new clear notebook execut print hello kernel di immedi us bash command like crash kernel restart restart session studio kill run",
        "Question_preprocessed_content":"restart notebook write code jupyt notebook studio current moment command caus kernel death new clear notebook execut kernel di immedi us bash command like crash kernel restart restart session studio kill run",
        "Question_gpt_summary_original":"The user is facing challenges with their Jupyter Notebook in Azure ML Studio, as every command causes kernel death, including simple commands like print and bash commands like ls. The user is seeking a solution to restart their VM or session in Azure ML Studio without killing all running VMs.",
        "Question_gpt_summary":"user face challeng jupyt notebook studio command caus kernel death includ simpl command like print bash command like user seek solut restart session studio kill run vm",
        "Answer_original_content":"notebook menu file open notebook status shutdown close notebook aliv shut work notebook came life",
        "Answer_preprocessed_content":"notebook menu notebook status shutdown close notebook aliv shut work notebook came life",
        "Answer_gpt_summary_original":"the possible solutions to the challenge of restarting a vm in jupyter notebook in studio are to go to the notebook menu and select file->open to see all the notebooks and their statuses, shut down any closed notebooks that are still alive, and then try restarting the working notebook.",
        "Answer_gpt_summary":"possibl solut challeng restart jupyt notebook studio notebook menu select file open notebook status shut close notebook aliv try restart work notebook"
    },
    {
        "Question_id":72133111.0,
        "Question_title":"Azure ML: What means reconnecting terminal?",
        "Question_body":"<p>I am a newbie in this, and I am facing some problems with the Azure ML workspace. I ran a python code from the terminal, and then I opened another terminal to check the process. I got the following message in the terminal that checked the process:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9XLPw.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<p>I appreciate any tips.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651781550743,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":48.0,
        "Answer_body":"<blockquote>\n<p>What does this mean? It keeps running, but I don't know if it is a bad message. It takes soo long, and I don't want to lose the processing time.<\/p>\n<\/blockquote>\n<ul>\n<li><code>Reconnecting terminal<\/code> message can appear for multiple reasons like intermittent connectivity issues, unused active terminal sessions, processing of different size\/format of data.<\/li>\n<li>Make sure you close any unused terminal sessions to preserve your compute instance's resources. Idle terminals may impact the performance of compute instances.<\/li>\n<\/ul>\n<p>You can refer to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#manage-terminal-sessions\" rel=\"nofollow noreferrer\">Access a compute instance terminal in your workspace<\/a>, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-optimize-data-processing\" rel=\"nofollow noreferrer\">Optimize data processing with Azure Machine Learning<\/a> and <a href=\"https:\/\/www.youtube.com\/watch?v=kiScfw9i4FM\" rel=\"nofollow noreferrer\">Azure ML: Speed up processing time<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72133111",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651825622423,
        "Question_original_content":"mean reconnect termin newbi face problem workspac ran python code termin open termin check process got follow messag termin check process mean keep run know bad messag take soo long want lose process time appreci tip",
        "Question_preprocessed_content":"mean reconnect termin newbi face problem workspac ran python code termin open termin check process got follow messag termin check process mean keep run know bad messag take soo long want lose process time appreci tip",
        "Question_gpt_summary_original":"The user is encountering a challenge with the Azure ML workspace while running a python code from the terminal. They opened another terminal to check the process and received a message about reconnecting the terminal. The user is unsure about the meaning of the message and is concerned about losing processing time.",
        "Question_gpt_summary":"user encount challeng workspac run python code termin open termin check process receiv messag reconnect termin user unsur mean messag concern lose process time",
        "Answer_original_content":"mean keep run know bad messag take soo long want lose process time reconnect termin messag appear multipl reason like intermitt connect issu unus activ termin session process differ size format data sure close unus termin session preserv comput instanc resourc idl termin impact perform comput instanc refer access comput instanc termin workspac optim data process speed process time",
        "Answer_preprocessed_content":"mean keep run know bad messag take soo long want lose process time messag appear multipl reason like intermitt connect issu unus activ termin session process differ data sure close unus termin session preserv comput instanc resourc idl termin impact perform comput instanc refer access comput instanc termin workspac optim data process speed process time",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- the \"reconnecting terminal\" message can appear for multiple reasons like intermittent connectivity issues, unused active terminal sessions, processing of different size\/format of data.\n- make sure to close any unused terminal sessions to preserve your compute instance's resources.\n- idle terminals may impact the performance of compute instances.\n- refer to access a compute instance terminal in your workspace to troubleshoot the issue.\n- optimize data processing with certain techniques to speed up processing time. \n\nin summary, the user can try troubleshooting the issue by closing unused terminal sessions, optimizing data processing, and accessing a compute instance terminal in their workspace.",
        "Answer_gpt_summary":"possibl solut answer reconnect termin messag appear multipl reason like intermitt connect issu unus activ termin session process differ size format data sure close unus termin session preserv comput instanc resourc idl termin impact perform comput instanc refer access comput instanc termin workspac troubleshoot issu optim data process certain techniqu speed process time summari user try troubleshoot issu close unus termin session optim data process access comput instanc termin workspac"
    },
    {
        "Question_id":null,
        "Question_title":"DocAI - Response in a single json file",
        "Question_body":"Hello Experts,\nI'm doing BatchProcessDocument. I have 18 pages of a PDF file and tried to process this using DocumentProcessorServiceClient API. After the process, Im getting response in json file. This is perfect.\nBut the json output file is created only for the 5 pages of the source PDF file. Each 5 pages of the content are converted into a separate json file.My question here is, is it possible to have a single output json file for a PDF source file? ",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1668727800000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":59.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocAI-Response-in-a-single-json-file\/td-p\/490702\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-18T15:57:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nJust to confirm, when you said \"Each 5 pages of the content are converted into a separate json file.\" does it mean that 1 json per page? or 1 json per 5 pages? Also can you provide the code and sample file that you are using? Please make sure there are no PIIs (Personal Identifiable Information) in your file when providing it here."
            },
            {
                "Answer_creation_time":"2022-11-20T21:41:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello,\n\nThanks for you response. Actually I have two points.\n\nA pdf file should be processed and the response for this file in a single json file\nI have a file with a table of around 1000 rows. This table data can not be displayed in a single page. I just want a json object for this whole table. But currently the code is working for objects in a single page.\n\nBelow is my sample source code. I could not attach my sample file to this discussion.\n\n\u00a0\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileReader;\nimport java.io.IOException;\nimport java.util.List;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\n\nimport com.google.api.gax.core.FixedCredentialsProvider;\n\n\/\/ [START documentai_batch_process_document]\n\nimport com.google.api.gax.longrunning.OperationFuture;\nimport com.google.api.gax.paging.Page;\nimport com.google.auth.oauth2.GoogleCredentials;\nimport com.google.cloud.documentai.v1.BatchDocumentsInputConfig;\nimport com.google.cloud.documentai.v1.BatchProcessMetadata;\nimport com.google.cloud.documentai.v1.BatchProcessRequest;\nimport com.google.cloud.documentai.v1.BatchProcessResponse;\nimport com.google.cloud.documentai.v1.Document;\nimport com.google.cloud.documentai.v1.DocumentOutputConfig;\nimport com.google.cloud.documentai.v1.DocumentOutputConfig.GcsOutputConfig;\nimport com.google.cloud.documentai.v1.DocumentProcessorServiceClient;\nimport com.google.cloud.documentai.v1.DocumentProcessorServiceSettings;\nimport com.google.cloud.documentai.v1.GcsDocument;\nimport com.google.cloud.documentai.v1.GcsDocuments;\nimport com.google.cloud.storage.Blob;\nimport com.google.cloud.storage.BlobId;\nimport com.google.cloud.storage.Bucket;\nimport com.google.cloud.storage.Storage;\nimport com.google.cloud.storage.StorageOptions;\nimport com.google.common.collect.Lists;\nimport com.google.protobuf.util.JsonFormat;\n\npublic class CustomProcessDocument {\n\t\n\tpublic static void main(String a[]) {\n\t\t\n\t\ttry {\n\t\t\tCustomProcess();\n\t\t} catch (IOException | InterruptedException | ExecutionException | TimeoutException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\t\n\tpublic static void CustomProcess() \n\t\t\tthrows IOException, InterruptedException, ExecutionException, TimeoutException {\n\t\t\n        String projectId = \"my-project-id\";\n        String location = \"us\"; \/\/ Format is \"us\" or \"eu\".\n        String processerId = \"my-processor-id\";\n        String outputGcsBucketName = \"my-storage-bucket-name\";\n        String outputGcsPrefix = \"my-output-path\";\n        String inputGcsUri = \"gs:\/\/my-storage-bucket-name\/sample-pdf-file.pdf\";\n        String tokenPath = \"credentials-json-file-path\";\n        CustomProcessDoc(projectId, location, processerId, inputGcsUri, outputGcsBucketName, outputGcsPrefix, tokenPath);\n\t}\n\n    public static void CustomProcessDoc(String projectId, String location, String processorId, String gcsInputUri, \n    \t\tString gcsOutputBucketName, String gcsOutputUriPrefix, String tokenPath) {\n    \t\n        \/\/ Initialize client that will be used to send requests. This client only needs to be created\n        \/\/ once, and can be reused for multiple requests. After completing all of your requests, call\n        \/\/ the \"close\" method on the client to safely clean up any remaining background resources.\n        try {\n        \t\n        \tGoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(tokenPath)).createScoped(Lists.newArrayList(\"https:\/\/www.googleapis.com\/auth\/cloud-platform\"));\n        \tDocumentProcessorServiceSettings setting = DocumentProcessorServiceSettings.newBuilder().setCredentialsProvider(FixedCredentialsProvider.create(credentials)).build();\n        \tDocumentProcessorServiceClient client = DocumentProcessorServiceClient.create(setting);\n        \t\n            \/\/ The full resource name of the processor, e.g.:\n            \/\/ projects\/project-id\/locations\/location\/processor\/processor-id\n            \/\/ You must create new processors in the Cloud Console first\n            String name = String.format(\"projects\/%s\/locations\/%s\/processors\/%s\", projectId, location, processorId);\n\n            GcsDocument gcsDocument = GcsDocument.newBuilder().setGcsUri(gcsInputUri).setMimeType(\"application\/pdf\").build();\n\n            GcsDocuments gcsDocuments = GcsDocuments.newBuilder().addDocuments(gcsDocument).build();\n\n            BatchDocumentsInputConfig inputConfig = BatchDocumentsInputConfig.newBuilder().setGcsDocuments(gcsDocuments).build();\n\n            String fullGcsPath = String.format(\"gs:\/\/%s\/%s\/\", gcsOutputBucketName, gcsOutputUriPrefix);\n            GcsOutputConfig gcsOutputConfig = GcsOutputConfig.newBuilder().setGcsUri(fullGcsPath).build();\n\n            DocumentOutputConfig documentOutputConfig = DocumentOutputConfig.newBuilder().setGcsOutputConfig(gcsOutputConfig).build();\n\n            \/\/ Configure the batch process request.\n            BatchProcessRequest request = BatchProcessRequest.newBuilder().setName(name).setInputDocuments(inputConfig).setDocumentOutputConfig(documentOutputConfig).build();\n\n            OperationFuture<BatchProcessResponse, BatchProcessMetadata> future = client.batchProcessDocumentsAsync(request);\n\n            \/\/ Batch process document using a long-running operation.\n            \/\/ You can wait for now, or get results later.\n            \/\/ Note: first request to the service takes longer than subsequent\n            \/\/ requests.\n            System.out.println(\"Waiting for operation to complete...\");\n            future.get(240, TimeUnit.SECONDS);\n\n            System.out.println(\"Document processing complete.\");\n\n\/\/            Storage storage = StorageOptions.newBuilder().setProjectId(projectId).build().getService();\n            Storage storage = StorageOptions.newBuilder().setCredentials(credentials).setProjectId(projectId).build().getService();\n            Bucket bucket = storage.get(gcsOutputBucketName);\n\n            \/\/ List all of the files in the Storage bucket.\n            Page<Blob> blobs = bucket.list(Storage.BlobListOption.prefix(gcsOutputUriPrefix + \"\/\"));\n            System.out.println(\"blobs : \"+blobs);\n            int idx = 0;\n            for (Blob blob : blobs.iterateAll()) {\n                if (!blob.isDirectory()) {\n                    System.out.printf(\"Fetched file #%d\\n\", ++idx);\n                    \/\/ Read the results\n\n                    \/\/ Download and store json data in a temp file.\n                    File tempFile = File.createTempFile(\"file\", \".json\");\n                    Blob fileInfo = storage.get(BlobId.of(gcsOutputBucketName, blob.getName()));\n                    fileInfo.downloadTo(tempFile.toPath());\n\n                    \/\/ Parse json file into Document.\n                    FileReader reader = new FileReader(tempFile);\n                    Document.Builder builder = Document.newBuilder();\n                    JsonFormat.parser().merge(reader, builder);\n\n                    Document document = builder.build();\n\n                    \/\/ Get all of the document text as one big string.\n                    String text = document.getText();\n\n                    \/\/ Read the text recognition output from the processor\n                    System.out.println(\"The document contains the following paragraphs:\");\n                    Document.Page page1 = document.getPages(0);\n                    List<Document.Page.Paragraph> paragraphList = page1.getParagraphsList();\n                    for (Document.Page.Paragraph paragraph : paragraphList) {\n                        String paragraphText = getText(paragraph.getLayout().getTextAnchor(), text);\n                        System.out.printf(\"Paragraph text:%s\\n\", paragraphText);\n                    }\n\n                    \/\/ Form parsing provides additional output about\n                    \/\/ form-formatted PDFs. You must create a form\n                    \/\/ processor in the Cloud Console to see full field details.\n                    System.out.println(\"The following form key\/value pairs were detected:\");\n\n                    for (Document.Page.FormField field : page1.getFormFieldsList()) {\n                        String fieldName = getText(field.getFieldName().getTextAnchor(), text);\n                        String fieldValue = getText(field.getFieldValue().getTextAnchor(), text);\n\n                        System.out.println(\"Extracted form fields pair:\");\n                        System.out.printf(\"\\t(%s, %s))\", fieldName, fieldValue);\n                    }\n\n                    \/\/ Clean up temp file.\n                    tempFile.deleteOnExit();\n                }\n            }\n        } catch (IOException | InterruptedException | TimeoutException | ExecutionException e) {\n        \te.printStackTrace();\n        }\n    }\n\n    \/\/ Extract shards from the text field\n    private static String getText(Document.TextAnchor textAnchor, String text) {\n    \t\n        if (textAnchor.getTextSegmentsList().size() > 0) {\n        \t\n            int startIdx = (int) textAnchor.getTextSegments(0).getStartIndex();\n            int endIdx = (int) textAnchor.getTextSegments(0).getEndIndex();\n            return text.substring(startIdx, endIdx);\n        }\n        return \"[NO TEXT]\";\n    }\n}"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"docai respons singl json file hello expert batchprocessdocu page pdf file tri process documentprocessorservicecli api process get respons json file perfect json output file creat page sourc pdf file page content convert separ json file question possibl singl output json file pdf sourc file",
        "Question_preprocessed_content":"docai respons singl json file hello expert batchprocessdocu page pdf file tri process documentprocessorservicecli api process get respons json file perfect json output file creat page sourc pdf file page content convert separ json question possibl singl output json file pdf sourc file",
        "Question_gpt_summary_original":"The user is facing a challenge with the DocumentProcessorServiceClient API while using BatchProcessDocument to process an 18-page PDF file. Although the response is in a json file, it is only created for 5 pages of the source PDF file, with each 5 pages of content converted into a separate json file. The user is seeking a solution to have a single output json file for the entire PDF source file.",
        "Question_gpt_summary":"user face challeng documentprocessorservicecli api batchprocessdocu process page pdf file respons json file creat page sourc pdf file page content convert separ json file user seek solut singl output json file entir pdf sourc file",
        "Answer_original_content":"confirm said page content convert separ json file mean json page json page provid code sampl file sure pii person identifi inform file provid hello thank respons actual point pdf file process respons file singl json file file tabl row tabl data displai singl page want json object tabl current code work object singl page sampl sourc code attach sampl file discuss import java file import java fileinputstream import java fileread import java ioexcept import java util list import java util concurr executionexcept import java util concurr timeunit import java util concurr timeoutexcept import com googl api gax core fixedcredentialsprovid start documentai batch process document import com googl api gax longrun operationfutur import com googl api gax page page import com googl auth oauth googlecredenti import com googl cloud documentai batchdocumentsinputconfig import com googl cloud documentai batchprocessmetadata import com googl cloud documentai batchprocessrequest import com googl cloud documentai batchprocessrespons import com googl cloud documentai document import com googl cloud documentai documentoutputconfig import com googl cloud documentai documentoutputconfig gcsoutputconfig import com googl cloud documentai documentprocessorservicecli import com googl cloud documentai documentprocessorserviceset import com googl cloud documentai gcsdocument import com googl cloud documentai gcsdocument import com googl cloud storag blob import com googl cloud storag blobid import com googl cloud storag bucket import com googl cloud storag storag import com googl cloud storag storageopt import com googl common collect list import com googl protobuf util jsonformat public class customprocessdocu public static void main string try customprocess catch ioexcept interruptedexcept executionexcept timeoutexcept printstacktrac public static void customprocess throw ioexcept interruptedexcept executionexcept timeoutexcept string projectid project string locat format string processerid processor string outputgcsbucketnam storag bucket string outputgcsprefix output path string inputgcsuri storag bucket sampl pdf file pdf string tokenpath credenti json file path customprocessdoc projectid locat processerid inputgcsuri outputgcsbucketnam outputgcsprefix tokenpath public static void customprocessdoc string projectid string locat string processorid string gcsinputuri string gcsoutputbucketnam string gcsoutputuriprefix string tokenpath initi client send request client need creat reus multipl request complet request close method client safe clean remain background resourc try googlecredenti credenti googlecredenti fromstream new fileinputstream tokenpath createscop list newarraylist http googleapi com auth cloud platform documentprocessorserviceset set documentprocessorserviceset newbuild setcredentialsprovid fixedcredentialsprovid creat credenti build documentprocessorservicecli client documentprocessorservicecli creat set resourc processor project project locat locat processor processor creat new processor cloud consol string string format project locat processor projectid locat processorid gcsdocument gcsdocument gcsdocument newbuild setgcsuri gcsinputuri setmimetyp applic pdf build gcsdocument gcsdocument gcsdocument newbuild aocument gcsdocument build batchdocumentsinputconfig inputconfig batchdocumentsinputconfig newbuild setgcsdocu gcsdocument build string fullgcspath string format gcsoutputbucketnam gcsoutputuriprefix gcsoutputconfig gcsoutputconfig gcsoutputconfig newbuild setgcsuri fullgcspath build documentoutputconfig documentoutputconfig documentoutputconfig newbuild setgcsoutputconfig gcsoutputconfig build configur batch process request batchprocessrequest request batchprocessrequest newbuild setnam setinputdocu inputconfig setdocumentoutputconfig documentoutputconfig build operationfutur futur client batchprocessdocumentsasync request batch process document long run oper wait result later note request servic take longer subsequ request println wait oper complet futur timeunit second println document process complet storag storag storageopt newbuild setprojectid projectid build getservic storag storag storageopt newbuild setcredenti credenti setprojectid projectid build getservic bucket bucket storag gcsoutputbucketnam list file storag bucket page blob bucket list storag bloblistopt prefix gcsoutputuriprefix println blob blob int idx blob blob blob iterateal blob isdirectori printf fetch file idx read result download store json data temp file file tempfil file createtempfil file json blob fileinfo storag blobid gcsoutputbucketnam blob getnam fileinfo downloadto tempfil topath pars json file document fileread reader new fileread tempfil document builder builder document newbuild jsonformat parser merg reader builder document document builder build document text big string string text document gettext read text recognit output processor println document contain follow paragraph document page page document getpag list paragraphlist page getparagraphslist document page paragraph paragraph paragraphlist string paragraphtext gettext paragraph getlayout gettextanchor text printf paragraph text paragraphtext form pars provid addit output form format pdf creat form processor cloud consol field detail println follow form kei valu pair detect document page formfield field page getformfieldslist string fieldnam gettext field getfieldnam gettextanchor text string fieldvalu gettext field getfieldvalu gettextanchor text println extract form field pair printf fieldnam fieldvalu clean temp file tempfil deleteonexit catch ioexcept interruptedexcept timeoutexcept executionexcept printstacktrac extract shard text field privat static string gettext document textanchor textanchor string text textanchor gettextsegmentslist size int startidx int textanchor gettextseg getstartindex int endidx int textanchor gettextseg getendindex return text substr startidx endidx return text",
        "Answer_preprocessed_content":"confirm said page content convert separ json mean json page json page provid code sampl file sure pii file provid hello thank respons actual point pdf file process respons file singl json file file tabl row tabl data displai singl page want json object tabl current code work object singl page sampl sourc code attach sampl file discuss import import import import import import import import import import import import import import import import import import import import import import import import import import import import import import public class customprocessdocu catch public static void customprocess throw ioexcept interruptedexcept executionexcept timeoutexcept public static void customprocessdoc form pars provid addit output pdf creat form processor cloud consol field detail follow form pair detect clean temp file catch extract shard text field privat static string textanchor string text return text",
        "Answer_gpt_summary_original":"the user is trying to process a pdf file with 18 pages and is receiving a json output file for only 5 pages, and is wondering if it is possible to have a single output json file for the entire pdf source file. the answer provides a sample source code that processes a pdf file and generates a json output file for each page. however, the user wants a single json object for the whole table. the answer does not provide a solution for generating a single json object for the whole table.",
        "Answer_gpt_summary":"user try process pdf file page receiv json output file page wonder possibl singl output json file entir pdf sourc file answer provid sampl sourc code process pdf file gener json output file page user want singl json object tabl answer provid solut gener singl json object tabl"
    },
    {
        "Question_id":null,
        "Question_title":"Remotes config doesn't work for Gist",
        "Question_body":"<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">train:\n  description: train\n  main: tests\/train\n  sourcecode:\n    - exclude: '*.json'\n\nremotes:\n  remotename:\n    type: gist\n    user: username\n    gist-name: results.md\n<\/code><\/pre>\n<p>This remote doesn\u2019t show up when I run <code>guild remotes<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647929797879,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":146.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/remotes-config-doesnt-work-for-gist\/840",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-22T19:49:49.335Z",
                "Answer_body":"<p>You need to define remotes in <a href=\"https:\/\/my.guild.ai\/t\/user-config-reference\/173\">User Config<\/a>. Guild is going to interpret your example as having a <code>remotes<\/code> operation (it\u2019s parallel to <code>train<\/code>, which is treated as an operation).<\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"remot config work gist train descript train main test train sourcecod exclud json remot remotenam type gist user usernam gist result remot doesnt run remot",
        "Question_preprocessed_content":"remot config work gist remot doesnt run",
        "Question_gpt_summary_original":"The user is facing a challenge where the remote configuration for Gist is not working and the remote does not appear when running \"guild remotes\".",
        "Question_gpt_summary":"user face challeng remot configur gist work remot appear run remot",
        "Answer_original_content":"need defin remot user config go interpret exampl have remot oper parallel train treat oper",
        "Answer_preprocessed_content":"need defin remot user config go interpret exampl have oper",
        "Answer_gpt_summary_original":"the solution to the challenge of remotes config for a gist not working properly is to define remotes in the user config. the user should note that guild interprets the example as having a remotes operation, which is parallel to the train operation and is treated as an operation.",
        "Answer_gpt_summary":"solut challeng remot config gist work properli defin remot user config user note interpret exampl have remot oper parallel train oper treat oper"
    },
    {
        "Question_id":null,
        "Question_title":"Access sweep_id and run_id within train() function for local weight storage",
        "Question_body":"<p>Hello,<\/p>\n<p>As I cannot simply upload infinitely many weights using artifacts, I also want to store some locally.<br>\nFor naming, I would like to use the sweep id and\/or the run id.<\/p>\n<p>Can I access that somehow in the train function I hand over to the agent?<\/p>\n<p>Thanks<\/p>\n<p>Markus<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660719705149,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":113.0,
        "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/access-sweep-id-and-run-id-within-train-function-for-local-weight-storage\/2948",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-18T22:04:29.034Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":31.4,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-10-17T22:05:26.069Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660860269032,
        "Question_original_content":"access sweep run train function local weight storag hello simpli upload infinit weight artifact want store local name like us sweep run access train function hand agent thank marku",
        "Question_preprocessed_content":"access train function local weight storag hello simpli upload infinit weight artifact want store local name like us sweep run access train function hand agent thank marku",
        "Question_gpt_summary_original":"The user is facing a challenge in storing weights locally and wants to use the sweep id and\/or the run id for naming. They are seeking advice on how to access this information within the train function they are using for the agent.",
        "Question_gpt_summary":"user face challeng store weight local want us sweep run name seek advic access inform train function agent",
        "Answer_original_content":"hei markuskarn run object return init contain inform properti abl access run run sweep train function call run init thank ramit",
        "Answer_preprocessed_content":"hei object return contain inform properti abl access train function call thank ramit",
        "Answer_gpt_summary_original":"possible solutions: \n- access the run object returned from .init() function.\n- use run.id and run.sweep_id properties to access sweep_id and run_id respectively.\n- call .init(...) function before the train() function to ensure the run object is available. \n\nsummary: the run object returned from .init() function contains the sweep_id and run_id properties that can be accessed within the train() function.",
        "Answer_gpt_summary":"possibl solut access run object return init function us run run sweep properti access sweep run respect init function train function ensur run object avail summari run object return init function contain sweep run properti access train function"
    },
    {
        "Question_id":38189399.0,
        "Question_title":"azure ml experiment return different results than webservice",
        "Question_body":"<p>same input is used in two cases, but different result is returned from python module<\/p>\n\n<p>here is the python script that return the result to the webservice:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\n\n\n  def get_segments(dataframe):\n     dataframe['segment']=dataframe['segment'].astype('str')\n     segments = dataframe.loc[~dataframe['segment'].duplicated()]['segment']\n     return segments\n\n\n  def azureml_main(dataframe1 = None, dataframe2 = None):\n\n   df = dataframe1\n   segments = get_segments(df)\n   segmentCount =segments.size\n\n   if (segmentCount &gt; 0) :\n      res = pd.DataFrame(columns=['segmentId','recommendation'],index=[range(segmentCount)])\n    i=0    \n    for seg in segments:\n        d= df.query('segment ==[\"{}\"]'.format(seg)).sort(['count'],ascending=[0])\n\n        res['segmentId'][i]=seg\n        recommendation='['\n        for index, x in d.iterrows():\n            item=str(x['ItemId'])\n            recommendation = recommendation + item + ','\n        recommendation = recommendation[:-1] + ']'\n        res['recommendation'][i]= recommendation\n        i=i+1\n   else:\n\n      res = pd.DataFrame(columns=[seg,pdver],index=[range(segmentCount)])\n\nreturn res,\n<\/code><\/pre>\n\n<p>when in experiment it returnd the actual itemIds, when in webservice it returns some numbers<\/p>\n\n<p>the purpose of this code is to pivot some table by segment column for recommendation<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1467651436410,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1468140771380,
        "Question_score":3.0,
        "Question_view_count":264.0,
        "Answer_body":"<p>After discussion with the product team from Microsoft. the issue was resolved.\nthe product team rolled out an update to the web service first, and only later to the ML-Studio, which fixed an issue with categorical attributes in \"Execute python script\".\nthe issue was in a earlier stage of the flow and has nothing to do with the python code above.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38189399",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1468139423743,
        "Question_original_content":"experi return differ result webservic input case differ result return python modul python script return result webservic import panda import sy def segment datafram datafram segment datafram segment astyp str segment datafram loc datafram segment duplic segment return segment def main datafram datafram datafram segment segment segmentcount segment size segmentcount re datafram column segmentid recommend index rang segmentcount seg segment queri segment format seg sort count ascend re segmentid seg recommend index iterrow item str itemid recommend recommend item recommend recommend re recommend recommend re datafram column seg pdver index rang segmentcount return re experi returnd actual itemid webservic return number purpos code pivot tabl segment column recommend",
        "Question_preprocessed_content":"experi return differ result webservic input case differ result return python modul python script return result webservic experi returnd actual itemid webservic return number purpos code pivot tabl segment column recommend",
        "Question_gpt_summary_original":"The user is facing a challenge where the same input is used in two cases, but different results are returned from the Python module. The Python script is designed to pivot some table by segment column for recommendation, but when used in the experiment, it returns the actual itemIds, while in the webservice, it returns some numbers.",
        "Question_gpt_summary":"user face challeng input case differ result return python modul python script design pivot tabl segment column recommend experi return actual itemid webservic return number",
        "Answer_original_content":"discuss product team microsoft issu resolv product team roll updat web servic later studio fix issu categor attribut execut python script issu earlier stage flow python code",
        "Answer_preprocessed_content":"discuss product team microsoft issu resolv product team roll updat web servic later fix issu categor attribut execut python script issu earlier stage flow python code",
        "Answer_gpt_summary_original":"possible solutions to the challenge of getting different results from the same input in an experiment and a webservice are to contact the product team and report the issue, wait for them to roll out an update to the web service and ml-studio, and check if the issue is related to categorical attributes in \"execute python script\".",
        "Answer_gpt_summary":"possibl solut challeng get differ result input experi webservic contact product team report issu wait roll updat web servic studio check issu relat categor attribut execut python script"
    },
    {
        "Question_id":null,
        "Question_title":"Call last Sagemaker Model in Batch Transform Jobs",
        "Question_body":"Hi Dears,\n\nHope this message finds you well\n\nI have a sagemaker model, buit by on demand notebook. I have been used batch transform jobs using lambda function, It take input inference json from s3 to create batch transform job and have finally predictions.\n\nThe question how can I make lambda to use last trained model automaticity ? model_name = 'forecasting-deepar-2022-05-20-22-23-20-225',\n\nLambda code :\n\nif 'input_data_4' in file:\n\n            def batch_transform():\n                transformer = Transformer(\n                    model_name = 'forecasting-deepar-2022-05-20-22-23-20-225',\n                    instance_count = 2,\n                    instance_type = 'ml.m5.xlarge',\n                    assemble_with = 'Line',\n                    output_path = output_data_path,\n                    base_transform_job_name ='daily-output-predictions-to-s3',\n                    #sagemaker_session = sagemaker.session.Session,\n                    accept = 'application\/jsonlines')\n                transformer.transform(data = input_data_path, \n                                    content_type = 'application\/jsonlines', \n                                    split_type = 'Line',\n                                    wait=False, \n                                    logs=True)\n                    #Waits for the Pipeline Transform Job to finish.\n                print('Batch Transform Job Created successfully!')\n            batch_transform()\n\n\nThanks Basem",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655467756412,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":93.0,
        "Answer_body":"Hi Basem,\n\nIf I understood correctly you'd like your Lambda function to automatically choose the latest SageMaker model when it runs, instead of hard-coding the model name.\n\nAlthough you could do this simply with boto3.client(\"sagemaker\").list_models(...) (which can sort by creation time), I would not recommend it. The reason is that in general this lists all models present in SageMaker - which might include some for different use cases in future, even if you only have the one DeepAR forecasting use-case today. You'd have to manually filter after the API call.\n\nA better approach would probably be to register your forecasting models in SageMaker Model Registry - which will allow you to register different versions and track extra metadata like metrics and approval status for each version if you need.\n\nFirst (e.g. from your notebook) you can create a model package group to track your forecasting models.\nThen (when you create your SageMaker Model) you can register it as a new version in the group - via Model.register().\nAt the point you want to look up which model to use, you can then list_model_packages which can filter to your specific group of models, and also by approval status if you like.\n\nSo for example you could set the model package group name as a configuration environment variable for your Lambda function, and have the function dynamically look up the latest version to use from the group when needed.\n\nOf course there are also many more custom ways to do this such as creating an SSM Parameter to track the name of your current accepted model, or creating your own model registry using a data store like DynamoDB... But SageMaker Model Registry seems like the most purpose-built tool for the job here to me.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUF0u2FxOyTqK8-9hQG40i7g\/call-last-sagemaker-model-in-batch-transform-jobs",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-23T08:04:25.907Z",
                "Answer_score":0,
                "Answer_body":"Hi Basem,\n\nIf I understood correctly you'd like your Lambda function to automatically choose the latest SageMaker model when it runs, instead of hard-coding the model name.\n\nAlthough you could do this simply with boto3.client(\"sagemaker\").list_models(...) (which can sort by creation time), I would not recommend it. The reason is that in general this lists all models present in SageMaker - which might include some for different use cases in future, even if you only have the one DeepAR forecasting use-case today. You'd have to manually filter after the API call.\n\nA better approach would probably be to register your forecasting models in SageMaker Model Registry - which will allow you to register different versions and track extra metadata like metrics and approval status for each version if you need.\n\nFirst (e.g. from your notebook) you can create a model package group to track your forecasting models.\nThen (when you create your SageMaker Model) you can register it as a new version in the group - via Model.register().\nAt the point you want to look up which model to use, you can then list_model_packages which can filter to your specific group of models, and also by approval status if you like.\n\nSo for example you could set the model package group name as a configuration environment variable for your Lambda function, and have the function dynamically look up the latest version to use from the group when needed.\n\nOf course there are also many more custom ways to do this such as creating an SSM Parameter to track the name of your current accepted model, or creating your own model registry using a data store like DynamoDB... But SageMaker Model Registry seems like the most purpose-built tool for the job here to me.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1655971465907,
        "Question_original_content":"model batch transform job dear hope messag find model buit demand notebook batch transform job lambda function input infer json creat batch transform job final predict question lambda us train model automat model forecast deepar lambda code input data file def batch transform transform transform model forecast deepar instanc count instanc type xlarg assembl line output path output data path base transform job daili output predict session session session accept applic jsonlin transform transform data input data path content type applic jsonlin split type line wait fals log true wait pipelin transform job finish print batch transform job creat successfulli batch transform thank basem",
        "Question_preprocessed_content":"model batch transform job dear hope messag find model buit demand notebook batch transform job lambda function input infer json creat batch transform job final predict question lambda us train model automat lambda code file def transform transform line wait fals log true wait pipelin transform job finish print thank basem",
        "Question_gpt_summary_original":"The user is facing a challenge in making their lambda function use the last trained Sagemaker model automatically for batch transform jobs. They have provided their current lambda code and are seeking a solution to this issue.",
        "Question_gpt_summary":"user face challeng make lambda function us train model automat batch transform job provid current lambda code seek solut issu",
        "Answer_original_content":"basem understood correctli like lambda function automat choos latest model run instead hard code model simpli boto client list model sort creation time recommend reason gener list model present includ differ us case futur deepar forecast us case todai manual filter api better approach probabl regist forecast model model registri allow regist differ version track extra metadata like metric approv statu version need notebook creat model packag group track forecast model creat model regist new version group model regist point want look model us list model packag filter specif group model approv statu like exampl set model packag group configur environ variabl lambda function function dynam look latest version us group need cours custom wai creat ssm paramet track current accept model creat model registri data store like dynamodb model registri like purpos built tool job",
        "Answer_preprocessed_content":"basem understood correctli like lambda function automat choos latest model run instead model simpli recommend reason gener list model present includ differ us case futur deepar forecast todai manual filter api better approach probabl regist forecast model model registri allow regist differ version track extra metadata like metric approv statu version need creat model packag group track forecast model regist new version group point want look model us filter specif group model approv statu like exampl set model packag group configur environ variabl lambda function function dynam look latest version us group need cours custom wai creat ssm paramet track current accept model creat model registri data store like model registri like tool job",
        "Answer_gpt_summary_original":"possible solutions to automatically call the last trained model in batch transform jobs using lambda function are:\n\n1. use boto3.client(\"\").list_models(...) to sort by creation time, but manually filter after the api call.\n2. register forecasting models in model registry to track different versions and metadata like metrics and approval status for each version.\n3. create a model package group to track forecasting models and register them as new versions in the group via model.register().\n4. list_model_packages to filter to specific group of models and approval status.\n5. set the model package group name as a configuration environment variable for the lambda function and have the function dynamically look up the latest version to use from the group when needed.",
        "Answer_gpt_summary":"possibl solut automat train model batch transform job lambda function us boto client list model sort creation time manual filter api regist forecast model model registri track differ version metadata like metric approv statu version creat model packag group track forecast model regist new version group model regist list model packag filter specif group model approv statu set model packag group configur environ variabl lambda function function dynam look latest version us group need"
    },
    {
        "Question_id":null,
        "Question_title":"Copying a dvc repository",
        "Question_body":"<p>I have realized that a dvc repository cannot be backed up as I would any working directory, since links cannot be backed up (except by archiving).<\/p>\n<p>I am using symlinks with <code>cache.protected true<\/code>.<\/p>\n<p>Apparently, the way to backup a dvc repository is to set up another dvc repository for this purpose and transfer all changes there.<\/p>\n<p>I have set up a backup dvc repository on a flash drive, as follows:<\/p>\n<pre><code>cd \/path\/to\/mount\/point\/\nmkdir backup\ncd backup\ngit init\ndvc init\ndvc remote add -d mainremote \/path\/to\/my\/dvc\/repository\ngit add -A\ngit commit\ngit clone --no-hardlinks \/path\/to\/my\/dvc\/repository\ndvc pull\n<\/code><\/pre>\n<p>What happened, was that git clone did not copy any symlinks and <code>dvc pull<\/code> reported:<\/p>\n<blockquote>\n<p>WARNING: Some of the cache files do not exist neither locally nor on remote. Missing cache files:<\/p>\n<\/blockquote>\n<p>And here it listed all my data files and directories, some of them twice.<\/p>\n<p>After that a few more error messages followed, saying that the cache files could not be downloaded since they do not exist on the flash repository.<\/p>\n<p>Please help.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1567023444597,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":2304.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/copying-a-dvc-repository\/213",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-08-28T20:48:36.614Z",
                "Answer_body":"<p>I have realized that the file system on my flash drive is msdos and it does not support symlinks at all.<\/p>\n<p>Is there any good way of backing up my dvc repository to a removable drive?<\/p>\n<p>I can set up incremental tar backup, although I do not like this option.<\/p>\n<p>If I get a removable drive that supports symlinks, is there any good way to backup my dvc repository to it?<\/p>",
                "Answer_score":27.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-28T22:40:51.139Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/byoussin\">@byoussin<\/a> !<\/p>\n<p>Indeed, if you try to simply copy your full dvc repo that is set up to use symlinks to FAT partition, you will indeed run into errors, as FAT doesn\u2019t support symlinks. It would work when copying to a fs that supports symlinks though <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\">  But I think there is a better way to back it up anyway. To backup your dvc repo, you need to backup two components: git repository(don\u2019t have to worry about it if you have it on github already) and dvc cache(usually in .dvc\/cache, unless you\u2019ve changed it to point to another dir). Git repository tracks .dvc files, that have metadata that tells dvc which cache files to link where in your workspace. So something like <code>git clone<\/code> + <code>cp path\/to\/repo\/.dvc\/cache .dvc\/cache<\/code> would be an alternative to copying the whole repo when working with FAT. Also, another way would be to set up your FAT flash as a dvc remote with <code>dvc remote add -d mybackup \/path\/to\/my\/fat\/flash<\/code> and run <code>git push &amp;&amp; dvc push<\/code> to backup data for the current workspace(i.e. current commit).<\/p>",
                "Answer_score":47.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-29T07:56:40.153Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> Thanks!<br>\nI have found another removable that supports symlinks and am backing up my repo by copying by rsync; this is the simplest solution.<\/p>\n<p>As for your other solution,<\/p>\n<blockquote>\n<p>So something like <code>git clone<\/code> + <code>cp path\/to\/repo\/.dvc\/cache .dvc\/cache<\/code> would be an alternative to copying the whole repo when working with FAT.<\/p>\n<\/blockquote>\n<p>I would not know how to restore from such backup.  I suggest that for benefit of other users you post the directions somewhere.<\/p>\n<p>As for your third solution,<\/p>\n<blockquote>\n<p>Also, another way would be to set up your FAT flash as a dvc remote with <code>dvc remote add -d mybackup \/path\/to\/my\/fat\/flash<\/code> and run <code>git push &amp;&amp; dvc push<\/code> to backup data for the current workspace(i.e. current commit).<\/p>\n<\/blockquote>\n<p>I think I tried to do something very close: instead of <code>dvc push<\/code> from my home repository I tried <code>dvc pull<\/code> from the backup repositiory (see my script above), and it did not work.<\/p>",
                "Answer_score":52.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-29T08:58:51.547Z",
                "Answer_body":"<blockquote>\n<p>I have found another removable that supports symlinks and am backing up my repo by copying by rsync; this is the simplest solution.<\/p>\n<\/blockquote>\n<p>Glad to hear it works for you <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<blockquote>\n<p>I would not know how to restore from such backup. I suggest that for benefit of other users you post the directions somewhere.<\/p>\n<\/blockquote>\n<p>Just <code>git clone<\/code> and then copy .dvc\/cache in place and run <code>dvc checkout<\/code>, as simple as that.<\/p>\n<p>Backup consists of backing up git repo and dvc remote and doesn\u2019t differ that much from the way you would\u2019ve shared your dvc project with other people, which is described in <a href=\"https:\/\/dvc.org\/doc\/use-cases\/share-data-and-model-files\" rel=\"nofollow noopener\">https:\/\/dvc.org\/doc\/use-cases\/share-data-and-model-files<\/a> . Your experience was not very good because you\u2019ve tried to copy repo as is into FAT flash drive, which doesn\u2019t support symlinks. Please feel free to create an issue on <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc.org\/issues<\/a> and describing your experince and your thoughts about the backup guide, maybe even consider contributing an small article about it <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<blockquote>\n<p>I think I tried to do something very close: instead of  <code>dvc push<\/code>  from my home repository I tried  <code>dvc pull<\/code>  from the backup repositiory (see my script above), and it did not work.<\/p>\n<\/blockquote>\n<p>Your script above does some very weird things like creating a new wrapper git\/dvc repostiory and then cloning original one into it. The approach that I\u2019ve suggested should work.<\/p>\n<p>Let us know if you have any questions <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":107.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"copi repositori realiz repositori back work directori link back archiv symlink cach protect true appar wai backup repositori set repositori purpos transfer chang set backup repositori flash drive follow path mount point mkdir backup backup git init init remot add mainremot path repositori git add git commit git clone hardlink path repositori pull happen git clone copi symlink pull report warn cach file exist local remot miss cach file list data file directori twice error messag follow sai cach file download exist flash repositori help",
        "Question_preprocessed_content":"copi repositori realiz repositori back work directori link back symlink appar wai backup repositori set repositori purpos transfer chang set backup repositori flash drive follow happen git clone copi symlink report warn cach file exist local remot miss cach file list data file directori twice error messag follow sai cach file download exist flash repositori help",
        "Question_gpt_summary_original":"The user is facing challenges in backing up a dvc repository due to the inability to back up links. They have attempted to set up a backup dvc repository on a flash drive but encountered issues with git clone not copying symlinks and dvc pull reporting missing cache files. The user is seeking assistance in resolving these issues.",
        "Question_gpt_summary":"user face challeng back repositori inabl link attempt set backup repositori flash drive encount issu git clone copi symlink pull report miss cach file user seek assist resolv issu",
        "Answer_original_content":"realiz file flash drive msdo support symlink good wai back repositori remov drive set increment tar backup like option remov drive support symlink good wai backup repositori byoussin try simpli copi repo set us symlink fat partit run error fat doesnt support symlink work copi support symlink think better wai backup repo need backup compon git repositori dont worri github cach usual cach youv chang point dir git repositori track file metadata tell cach file link workspac like git clone path repo cach cach altern copi repo work fat wai set fat flash remot remot add mybackup path fat flash run git push push backup data current workspac current commit kuprus thank remov support symlink back repo copi rsync simplest solut solut like git clone path repo cach cach altern copi repo work fat know restor backup suggest benefit user post direct solut wai set fat flash remot remot add mybackup path fat flash run git push push backup data current workspac current commit think tri close instead push home repositori tri pull backup repositiori script work remov support symlink back repo copi rsync simplest solut glad hear work know restor backup suggest benefit user post direct git clone copi cach place run checkout simpl backup consist back git repo remot doesnt differ wai wouldv share project peopl describ http org doc us case share data model file experi good youv tri copi repo fat flash drive doesnt support symlink feel free creat issu http github com iter org issu describ experinc thought backup guid mayb consid contribut small articl think tri close instead push home repositori tri pull backup repositiori script work script weird thing like creat new wrapper git repostiori clone origin approach iv suggest work let know question",
        "Answer_preprocessed_content":"realiz file flash drive msdo support symlink good wai back repositori remov drive set increment tar backup like option remov drive support symlink good wai backup repositori try simpli copi repo set us symlink fat partit run error fat doesnt support symlink work copi support symlink think better wai backup repo need backup compon git repositori cach git repositori track file metadata tell cach file link workspac like altern copi repo work fat wai set fat flash remot run backup data current current commit thank remov support symlink back repo copi rsync simplest solut solut like altern copi repo work fat know restor backup suggest benefit user post direct solut wai set fat flash remot run backup data current current commit think tri close instead home repositori tri backup repositiori work remov support symlink back repo copi rsync simplest solut glad hear work know restor backup suggest benefit user post direct copi place run simpl backup consist back git repo remot doesnt differ wai wouldv share project peopl describ experi good youv tri copi repo fat flash drive doesnt support symlink feel free creat issu describ experinc thought backup guid mayb consid contribut small articl think tri close instead home repositori tri backup repositiori work script weird thing like creat new wrapper git repostiori clone origin approach iv suggest work let know question",
        "Answer_gpt_summary_original":"possible solutions to backing up a repository to a removable drive include setting up incremental tar backup, copying the git repository and cache separately, setting up the removable drive as a remote and pushing data to it, or copying the repository using rsync. it is important to note that the file system on the removable drive must support symlinks. if encountering issues, it is recommended to seek help from the community or create an issue on the relevant platform.",
        "Answer_gpt_summary":"possibl solut back repositori remov drive includ set increment tar backup copi git repositori cach separ set remov drive remot push data copi repositori rsync import note file remov drive support symlink encount issu recommend seek help commun creat issu relev platform"
    },
    {
        "Question_id":69721067.0,
        "Question_title":"GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set",
        "Question_body":"<p>I'm new to Google Cloud Platform and I'm trying to create a Feature Store to fill with values from a csv file from Google Cloud Storage. The aim is to do that from a local notebook in Python.\nI'm basically following the code <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/feature_store\/gapic-feature-store.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, making the appropriate changes since I'm working with the credit card public dataset.\nThe error that raises when I run the code is the following:<\/p>\n<pre><code>GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set.\n<\/code><\/pre>\n<p>and it happens during the ingestion of the data from the csv file.<\/p>\n<p>Here it is the code I'm working on:<\/p>\n<pre><code>import os\nfrom datetime import datetime\nfrom google.cloud import bigquery\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform_v1.types import feature as feature_pb2\nfrom google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\nfrom google.cloud.aiplatform_v1.types import \\\n    featurestore_service as featurestore_service_pb2\nfrom google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\nfrom google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n\ncredential_path = r&quot;C:\\Users\\...\\.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n\n## Constants\nPROJECT_ID = &quot;my-project-ID&quot;\nREGION = &quot;us-central1&quot;\nAPI_ENDPOINT = &quot;us-central1-aiplatform.googleapis.com&quot;\nINPUT_CSV_FILE = &quot;my-input-file.csv&quot;\nFEATURESTORE_ID = &quot;fraud_detection&quot;\n\n## Output dataset\nDESTINATION_DATA_SET = &quot;fraud_predictions&quot;\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\nDESTINATION_DATA_SET = &quot;{prefix}_{timestamp}&quot;.format(\n    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n)\n\n## Output table. Make sure that the table does NOT already exist; \n## the BatchReadFeatureValues API cannot overwrite an existing table\nDESTINATION_TABLE_NAME = &quot;training_data&quot;\n\nDESTINATION_PATTERN = &quot;bq:\/\/{project}.{dataset}.{table}&quot;\nDESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, \n    table=DESTINATION_TABLE_NAME\n)\n\n## Create dataset\nclient = bigquery.Client(project=PROJECT_ID)\ndataset_id = &quot;{}.{}&quot;.format(client.project, DESTINATION_DATA_SET)\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = REGION\ndataset = client.create_dataset(dataset)\nprint(&quot;Created dataset {}.{}&quot;.format(client.project, dataset.dataset_id))\n\n## Create client for CRUD and data_client for reading feature values.\nclient = aiplatform.gapic.FeaturestoreServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\ndata_client = aiplatform.gapic.FeaturestoreOnlineServingServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\nBASE_RESOURCE_PATH = client.common_location_path(PROJECT_ID, REGION)\n\n## Create featurestore (only the first time)\ncreate_lro = client.create_featurestore(\n    featurestore_service_pb2.CreateFeaturestoreRequest(\n        parent=BASE_RESOURCE_PATH,\n        featurestore_id=FEATURESTORE_ID,\n        featurestore=featurestore_pb2.Featurestore(\n            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n                fixed_node_count=1\n            ),\n        ),\n    )\n)\n\n## Wait for LRO to finish and get the LRO result.\nprint(create_lro.result())\n\nclient.get_featurestore(\n    name=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n)\n\n## Create credit card entity type (only the first time)\ncc_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;creditcards&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Credit card entity&quot;,\n        ),\n    )\n)\n\n## Create fraud entity type (only the first time)\nfraud_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;frauds&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Fraud entity&quot;,\n        ),\n    )\n)\n\n## Create features for credit card type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v1&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v2&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v3&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v4&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v5&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v6&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v7&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v8&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v9&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v10&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v11&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v12&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v13&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v14&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v15&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v16&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v17&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v18&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v19&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v20&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v21&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v22&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v23&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v24&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v25&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v26&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v27&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v28&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;amount&quot;,\n        ),\n    ],\n).result()\n\n## Create features for fraud type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;class&quot;,\n        ),\n    ],\n).result()\n\n## Import features values for credit cards\nimport_cc_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/cc_details_train.csv&quot;])),\n    entity_id_field=&quot;cc_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v1&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v2&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v3&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v4&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v5&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v6&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v7&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v8&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v9&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v10&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v11&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v12&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v13&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v14&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v15&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v16&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v17&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v18&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v19&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v20&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v21&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v22&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v23&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v24&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v25&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v26&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v27&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v28&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;amount&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_cc_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n\n## Import features values for frauds\nimport_fraud_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/data_fraud_train.csv&quot;])),\n    entity_id_field=&quot;fraud_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;class&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_fraud_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n<\/code><\/pre>\n<p>When I check the <code>Ingestion Jobs<\/code> from the <code>Feature<\/code> section of Google Cloud Console I see that the job has finished but no values are added to my features.<\/p>\n<p>Any advice it is really precious.<\/p>\n<p>Thank you all.<\/p>\n<p><strong>EDIT 1<\/strong>\nIn the image below there is an example of the first row of the csv file I used as input (<code>cc_details_train.csv<\/code>). All the unseen features  are similar, the feature <code>class<\/code> can assume 0 or 1 values.\nThe injection job lasts about 5 minutes to import (ideally) 3000 rows, but it ends without error and without importing any value.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" alt=\"Rows of my csv file\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_creation_time":1635242363740,
        "Question_favorite_count":null,
        "Question_last_edit_time":1636446864247,
        "Question_score":1.0,
        "Question_view_count":357.0,
        "Answer_body":"<p><strong>VERTEX AI recomendations when using CSV to ImportValues \/ using ImportFeatureValuesRequest<\/strong><\/p>\n<p>Its possible that when using this feature you might end not able to import any data at all. You must pay attention to the time field you are using as it must be in compliance with google time formats.<\/p>\n<ol>\n<li>feature_time_field, must follow the time constraint rule set by google which is RFC3339, ie: '2021-04-15T08:28:14Z'. You can check details about the field <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.featurestores.entityTypes\/importFeatureValues#request-body\" rel=\"nofollow noreferrer\">here<\/a> and details about timestamp format can be found <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/google.protobuf#timestamp\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Other columns, fields must match is designed value. One exception is field entity_id_field, As it can be any value.<\/li>\n<\/ol>\n<p>Note: I my test i found that if i do not properly set up the time field as google recommended date format it will just not upload any feature value at all.<\/p>\n<p><em>test.csv<\/em><\/p>\n<pre><code>cc_id,time,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,amount\n100,2021-04-15T08:28:14Z,-1.359807,-0.072781,2.534897,1.872351,2.596267,0.465238,0.923123,0.347986,0.987354,1.234657,2.128645,1.958237,0.876123,-1.712984,-0.876436,1.74699,-1.645877,-0.936121,1.456327,0.087623,1.900872,2.876234,1.874123,0.923451,0.123432,0.000012,1.212121,0.010203,1000\n<\/code><\/pre>\n<p><em>output:<\/em><\/p>\n<pre><code>imported_entity_count: 1\nimported_feature_value_count: 29\n<\/code><\/pre>\n<p><strong>About optimization and working with features<\/strong><\/p>\n<p>You can check the official documentation <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#single-label-classification\" rel=\"nofollow noreferrer\">here<\/a> to see the min and max amount of records recommended for processing. As a piece of advice you should only use the actual working features to run and the recommended amount of values for it.<\/p>\n<p><strong>See your running ingested job<\/strong><\/p>\n<p>Either if you use VertexUI or code to generated the ingested job. You can track its run by going into the UI to this path:<\/p>\n<pre><code>VertexAI &gt; Features &gt; View Ingested Jobs \n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1638471768683,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69721067",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1636364178672,
        "Question_original_content":"googleapicallerror unexpect state long run oper respons error set new googl cloud platform try creat featur store valu csv file googl cloud storag aim local notebook python basic follow code make appropri chang work credit card public dataset error rais run code follow googleapicallerror unexpect state long run oper respons error set happen ingest data csv file code work import datetim import datetim googl cloud import bigqueri googl cloud import aiplatform googl cloud aiplatform type import featur featur googl cloud aiplatform type import featurestor featurestor googl cloud aiplatform type import featurestor servic featurestor servic googl cloud aiplatform type import entiti type entiti type googl cloud aiplatform type import featureselector idmatch credenti path user json environ googl applic credenti credenti path constant project project region central api endpoint central aiplatform googleapi com input csv file input file csv featurestor fraud detect output dataset destin data set fraud predict timestamp datetim strftime destin data set prefix timestamp format prefix destin data set timestamp timestamp output tabl sure tabl exist batchreadfeaturevalu api overwrit exist tabl destin tabl train data destin pattern project dataset tabl destin tabl uri destin pattern format project project dataset destin data set tabl destin tabl creat dataset client bigqueri client project project dataset format client project destin data set dataset bigqueri dataset dataset dataset locat region dataset client creat dataset dataset print creat dataset format client project dataset dataset creat client crud data client read featur valu client aiplatform gapic featurestoreservicecli client option api endpoint api endpoint data client aiplatform gapic featurestoreonlineservingservicecli client option api endpoint api endpoint base resourc path client common locat path project region creat featurestor time creat lro client creat featurestor featurestor servic createfeaturestorerequest parent base resourc path featurestor featurestor featurestor featurestor featurestor onlin serv config featurestor featurestor onlineservingconfig fix node count wait lro finish lro result print creat lro result client featurestor client featurestor path project region featurestor creat credit card entiti type time entiti type lro client creat entiti type featurestor servic createentitytyperequest parent client featurestor path project region featurestor entiti type creditcard entiti type entiti type entitytyp descript credit card entiti creat fraud entiti type time fraud entiti type lro client creat entiti type featurestor servic createentitytyperequest parent client featurestor path project region featurestor entiti type fraud entiti type entiti type entitytyp descript fraud entiti creat featur credit card type time client batch creat featur parent client entiti type path project region featurestor creditcard request featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur result creat featur fraud type time client batch creat featur parent client entiti type path project region featurestor fraud request featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur class result import featur valu credit card import request aiplatform gapic importfeaturevaluesrequest entiti type client entiti type path project region featurestor creditcard csv sourc aiplatform gapic csvsourc gc sourc aiplatform gapic gcssourc uri fraud detect dataset detail train csv entiti field featur spec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec featur time field time worker count start import ingest lro client import featur valu import request poll lro statu print lro complet ingest lro result import featur valu fraud import fraud request aiplatform gapic importfeaturevaluesrequest entiti type client entiti type path project region featurestor fraud csv sourc aiplatform gapic csvsourc gc sourc aiplatform gapic gcssourc uri fraud detect dataset data fraud train csv entiti field fraud featur spec aiplatform gapic importfeaturevaluesrequest featurespec class featur time field time worker count start import ingest lro client import featur valu import fraud request poll lro statu print lro complet ingest lro result check ingest job featur section googl cloud consol job finish valu ad featur advic preciou thank edit imag exampl row csv file input detail train csv unseen featur similar featur class assum valu inject job last minut import ideal row end error import valu",
        "Question_preprocessed_content":"googleapicallerror unexpect state oper respons error set new googl cloud platform try creat featur store valu csv file googl cloud storag aim local notebook python basic follow code make appropri chang work credit card public dataset error rais run code follow happen ingest data csv file code work check section googl cloud consol job finish valu ad featur advic preciou thank edit imag exampl row csv file input unseen featur similar featur assum valu inject job last minut import row end error import valu",
        "Question_gpt_summary_original":"the user is encountering an error while attempting to create a feature store from a local notebook in python and ingest data from a csv file on google cloud storage.",
        "Question_gpt_summary":"user encount error attempt creat featur store local notebook python ingest data csv file googl cloud storag",
        "Answer_original_content":"recomend csv importvalu importfeaturevaluesrequest possibl featur end abl import data pai attent time field complianc googl time format featur time field follow time constraint rule set googl rfc check detail field detail timestamp format column field match design valu except field entiti field valu note test properli set time field googl recommend date format upload featur valu test csv time output import entiti count import featur valu count optim work featur check offici document min max record recommend process piec advic us actual work featur run recommend valu run ingest job us vertexui code gener ingest job track run go path vertexai featur view ingest job",
        "Answer_preprocessed_content":"recomend csv importvalu importfeaturevaluesrequest possibl featur end abl import data pai attent time field complianc googl time format follow time constraint rule set googl rfc check detail field detail timestamp format column field match design valu except field valu note test properli set time field googl recommend date format upload featur valu output optim work featur check offici document min max record recommend process piec advic us actual work featur run recommend valu run ingest job us vertexui code gener ingest job track run go path",
        "Answer_gpt_summary_original":"the answer provides recommendations for importing csv data into a feature store from a local notebook in python. it suggests paying attention to the time field and ensuring it follows the google time format. other columns must match the designed value, except for the entity_id_field. the answer also recommends checking the official documentation for optimization and working with features and using the recommended amount of values for processing. the user can track the ingested job's run by going into the ui to the path: vertexai > features > view ingested jobs.",
        "Answer_gpt_summary":"answer provid recommend import csv data featur store local notebook python suggest pai attent time field ensur follow googl time format column match design valu entiti field answer recommend check offici document optim work featur recommend valu process user track ingest job run go path vertexai featur view ingest job"
    },
    {
        "Question_id":null,
        "Question_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models",
        "Question_body":"Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.\n\n    create_endpoint_config_response = client.create_endpoint_config(\r\n        EndpointConfigName=endpoint_config_name,\r\n        ProductionVariants=[\r\n            {\r\n                \"InstanceType\": \"ml.m5.large\",\r\n                \"InitialVariantWeight\": 0.5,\r\n                \"InitialInstanceCount\": 1,\r\n                \"ModelName\": model_name1,\r\n                \"VariantName\": model_name1,\r\n            },\r\n             {\r\n                \"InstanceType\": \"ml.m5.large\",\r\n                \"InitialVariantWeight\": 0.5,\r\n                \"InitialInstanceCount\": 1,\r\n                \"ModelName\": model_name2,\r\n                \"VariantName\": model_name2,\r\n            }\r\n        ]\r\n    )\n\n\nI confirm in the GUI that it in fact has multiple models. I invoke it like this:\n\n    response = client.invoke_endpoint(\r\n        EndpointName=endpoint_name, \r\n        TargetModel=model_name1,\r\n        ContentType=\"text\/x-libsvm\", \r\n        Body=payload\r\n    )\n\n\nand get this error:\n\nValidationError: An error occurred (ValidationError) when calling the\nInvokeEndpoint operation: Endpoint\nmy-endpoint1 is not a multi-model endpoint\nand does not support target model header.\n\nThe same problem was discussed at https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026 with no resolution.\n\nHow can I invoke a multimodel endpoint?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629115147000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":302.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA_yXPjW3TUOinot9BJe5GA\/why-do-i-get-an-error-that-sagemaker-endpoint-does-not-have-multiple-models",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-18T10:50:59.000Z",
                "Answer_score":0,
                "Answer_body":"The answer is discussed here https:\/\/stackoverflow.com\/questions\/68802388\/\n\nTwo recommended improvements:\n\nThe error message is factually wrong. It should be amended.\nThe details should be in the documentation https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html and it would be good to further clarify whether ModelDataUrl is meant to be a prefix onto which model_filename.tar.gz is appended or whether it is a full path to a file (as appears to be the case in some Notebooks).\n\nEdited by: JoshuaFox on Aug 18, 2021 3:52 AM",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error endpoint multipl model invok multimodel endpoint error multimodel creat like creat endpoint config respons client creat endpoint config endpointconfignam endpoint config productionvari instancetyp larg initialvariantweight initialinstancecount modelnam model variantnam model instancetyp larg initialvariantweight initialinstancecount modelnam model variantnam model confirm gui fact multipl model invok like respons client invok endpoint endpointnam endpoint targetmodel model contenttyp text libsvm bodi payload error validationerror error occur validationerror call invokeendpoint oper endpoint endpoint multi model endpoint support target model header problem discuss http github com aw amazon exampl issu resolut invok multimodel endpoint",
        "Question_preprocessed_content":"error endpoint multipl model invok multimodel endpoint error multimodel creat like productionvari confirm gui fact multipl model invok like respons bodi payload error validationerror error occur call invokeendpoint oper endpoint endpoint support target model header problem discuss resolut invok multimodel endpoint",
        "Question_gpt_summary_original":"The user is encountering an error when trying to invoke a multimodel Sagemaker Endpoint, despite having created it with multiple models and confirming it in the GUI. The error message states that the endpoint is not multimodel and does not support target model header. The user is seeking a solution to invoke a multimodel endpoint.",
        "Question_gpt_summary":"user encount error try invok multimodel endpoint despit have creat multipl model confirm gui error messag state endpoint multimodel support target model header user seek solut invok multimodel endpoint",
        "Answer_original_content":"answer discuss http stackoverflow com question recommend improv error messag factual wrong amend detail document http doc aw amazon com latest invok multi model endpoint html good clarifi modeldataurl meant prefix model filenam tar append path file appear case notebook edit joshuafox aug",
        "Answer_preprocessed_content":"answer discuss recommend improv error messag factual wrong amend detail document good clarifi modeldataurl meant prefix append path file edit joshuafox aug",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions to the error encountered when invoking a multi-model endpoint. firstly, the error message should be amended as it is factually wrong. secondly, the documentation should be consulted to clarify whether the modeldataurl is a prefix or a full path to a file.",
        "Answer_gpt_summary":"answer suggest possibl solut error encount invok multi model endpoint firstli error messag amend factual wrong secondli document consult clarifi modeldataurl prefix path file"
    },
    {
        "Question_id":null,
        "Question_title":"Automated machine learning (AutoML)",
        "Question_body":"Hi everyone,\n\nI want to find the right model for my code, that's why I would like to use Azure AutoML.\nI'm at the point where I split the data:\nX_train, X_test, y_train, y_test (...)\n\nA vector matrix is created from X_train with TF\/IDF:\ntfidf_vectorizer_matrix = tfidf_vectorizer.fit_transform (X_train).toarray ()\n\nNormally I would now run through my models one by one, which also works.\n\nMy problem is how must automl_config = AutoMLConfig (...) be filled so that I can use TF\/IDF ?\n\nA little sample code would be very helpful.\nPlease need help on this topic.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606236131380,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/174091\/automated-machine-learning-automl.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-25T13:25:23.43Z",
                "Answer_score":0,
                "Answer_body":"@WeinhandlVolkerNMMECM-6517 Thanks for the question. AutoMLConfig has a 'FeaturizationConfig' setting https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py . Using this, I believe you can specify TfIdf as a supported transformer for a particular column https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-automl-core\/azureml.automl.core.constants.supportedtransformers?view=azure-ml-py\n\nThe following are the supported models for AutoMLconfig. The three different task parameter values determine the list of algorithms, or models, to apply. Use the allowed_models or blocked_models parameters to further modify iterations with the available models to include or exclude.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-01T08:40:40.047Z",
                "Answer_score":0,
                "Answer_body":"Hello ramr-msft,\n\nThanks for your quick reply.\nI am familiar with the 'FeaturizationConfig'.\nI created these as far as possible.\n\nfeaturization_config = FeaturizationConfig ()\nfeaturization_config.add_transformer_params ('TfIdf', ['cleaned'], ???)\n\nIt is just unclear to me what to add to 'params'\n1. CUSTOMIZABLE_TRANSFORMERS = {'Imputer', 'HashOneHotEncoder', 'TfIdf'}\n2. Enter columns for the specified transformer.\n3. ???\nadd_transformer_params (transformer: str, cols: typing.List [str], params: typing.Dict [str, typing.Any])\nThe examples shown are unclear for TfIdf.\n\n\n\n\n\n\nWhich entries I have to do in the AutoMLConfig?\n\nautoml_config = AutoMLConfig (name = 'Automated ML Experiment',\ntask = 'classification',\ncompute_target = training_cluster,\ntraining_data = X_train,\nlabel_column_name = y_train,\n#training_data = train_ds,\n#validation_data = test_ds,\n# label_column_name = 'destination_folder',\nprimary_metric = 'AUC_weighted',\nfeaturization = featurization_config,\nblocked_models = ['XGBoostClassifier'],\n)\n\nfeaturization = featurization_config\n\n\ntraining_data = Column or entire dataset?\n\n\nlabel_column_name = Result column?\n\nRegards, Volker",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"autom machin learn automl want right model code like us azur automl point split data train test train test vector matrix creat train idf tfidf vector matrix tfidf vector fit transform train toarrai normal run model work problem automl config automlconfig fill us idf littl sampl code help need help topic",
        "Question_preprocessed_content":"autom machin learn want right model code like us azur automl point split data vector matrix creat toarrai normal run model work problem automlconfig fill us littl sampl code help need help topic",
        "Question_gpt_summary_original":"The user is facing a challenge in configuring Azure AutoML to use TF\/IDF for their data. They are unsure of how to fill the automl_config parameter and are seeking a sample code to help them.",
        "Question_gpt_summary":"user face challeng configur azur automl us idf data unsur automl config paramet seek sampl code help",
        "Answer_original_content":"weinhandlvolkernmmecm thank question automlconfig featurizationconfig set http doc microsoft com python api train automl client train automl automlconfig automlconfig view azur believ specifi tfidf support transform particular column http doc microsoft com python api automl core automl core constant supportedtransform view azur follow support model automlconfig differ task paramet valu determin list algorithm model appli us allow model block model paramet modifi iter avail model includ exclud hello ramr msft thank quick repli familiar featurizationconfig creat far possibl featur config featurizationconfig featur config add transform param tfidf clean unclear add param customiz transform imput hashonehotencod tfidf enter column specifi transform add transform param transform str col type list str param type dict str type exampl shown unclear tfidf entri automlconfig automl config automlconfig autom experi task classif comput target train cluster train data train label column train train data train valid data test label column destin folder primari metric auc weight featur featur config block model xgboostclassifi featur featur config train data column entir dataset label column result column regard volker",
        "Answer_preprocessed_content":"thank question automlconfig featurizationconfig set believ specifi tfidf support transform particular column follow support model automlconfig differ task paramet valu determin list algorithm model appli us paramet modifi iter avail model includ exclud hello thank quick repli familiar featurizationconfig creat far possibl featurizationconfig unclear add param enter column specifi transform exampl shown unclear tfidf entri automlconfig automlconfig featur column entir dataset result column regard volker",
        "Answer_gpt_summary_original":"the answer suggests using the 'featurizationconfig' setting in azure automl to specify tf\/idf as a supported transformer for a particular column. the answer also provides a link to the supported transformers documentation and mentions the different task parameter values that determine the list of algorithms or models to apply. additionally, the answer provides an example of how to use the 'featurizationconfig' and 'automlconfig' settings in code.",
        "Answer_gpt_summary":"answer suggest featurizationconfig set azur automl specifi idf support transform particular column answer provid link support transform document mention differ task paramet valu determin list algorithm model appli addition answer provid exampl us featurizationconfig automlconfig set code"
    },
    {
        "Question_id":null,
        "Question_title":"(Windows 11) `wandb.sweep()` gives ConnectionResetError: [WinError 10054]",
        "Question_body":"<p>Hello, sort of new to wandb. I\u2019m trying sweeps for the first time - I had no problem creating and running a sweep from the web UI, then copy-pasting the command to start an agent from bash. However, starting it using <code>wandb.agent()<\/code> keeps giving me problems. It starts training, but I keep running into two problems:<\/p>\n<ol>\n<li>I keep getting the error below each time a new run starts - it seems to be originating from another thread created by wandb, so I\u2019m not sure what to do about it. Also, the runs get logged in the sweep page, but none of them have the data I\u2019ve logged (and each run has the \u201cactive\u201d dot even once the script ends).<\/li>\n<li>I can\u2019t figure out what wandb calls to make, and in what order, to get the agent to populate its randomized values into <code>wandb.config<\/code>. I would like to set some default config values (which are not specified by my <code>sweep_config<\/code> dict), but have the sweep agent update <code>wandb.config<\/code> with the randomized values created from the <code>sweep_config<\/code> dict (it seems like this is what happens when I run from bash). In the traceback below, you can see I print <code>wandb.config<\/code> right before the model is trained, and it simply uses the <code>config<\/code> dict I specified when calling <code>wandb.init<\/code> (it is not updated\/overwritten by the agent).<\/li>\n<\/ol>\n<p>Below is the full traceback. Thanks in advance for any ideas.<\/p>\n<pre><code class=\"lang-auto\">C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\python.exe C:\\Users\\jacks\\ml-project\\training_script.py \nUsing device: cuda\nwandb: Currently logged in as: jacksth22. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.13.3\nwandb: Run data is saved locally in C:\\Users\\jacks\\ml-project\\wandb\\run-20221004_162225-1aj4c6jt\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run restful-dew-107\nwandb:  View project at https:\/\/wandb.ai\/jacksth22\/&lt;project&gt;\nwandb:  View run at https:\/\/wandb.ai\/jacksth22\/&lt;project&gt;\/runs\/1aj4c6jt\nLoading data...done (elapsed=1.49s).\nConverting data to tensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500\/500 [00:03&lt;00:00, 164.20it\/s]\ndone (elapsed=3.05s).\nCreate sweep with ID: plq6uobc\nSweep URL: https:\/\/wandb.ai\/jacksth22\/uncategorized\/sweeps\/plq6uobc\n=== Starting sweep agent ===\nwandb: WARNING Calling wandb.login() after wandb.init() has no effect.\nwandb: Waiting for W&amp;B process to finish... (success).\nwandb:                                                                                \nwandb: Synced restful-dew-107: https:\/\/wandb.ai\/jacksth22\/&lt;project&gt;\/runs\/1aj4c6jt\nwandb: Synced 6 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: .\\wandb\\run-20221004_162225-1aj4c6jt\\logs\nwandb: Agent Starting Run: 1bqu30r5 with config:\nwandb: \tbatch_size: 16\nwandb: \tdense_depth: 2\nwandb: \tdense_width: 64\nwandb: \tdepth: 6\nwandb: \tdropout: 0.25778082906860794\nwandb: \tepochs: 15\nwandb: \tgradient_clipping: 1\nwandb: \theads: 4\nwandb: \tlr: 0.0008418633888555167\nwandb: \tlr_warmup: 1960\nwandb: \tmax_seq_len: 64\nwandb: \toptimizer: AdamW\nwandb: \tuse_max_pool: False\nModel created with 90,714 parameters.\ntest: wandb.config: {'epochs': 3, 'batch_size': 16, 'test_size': 0.3, 'lr': 5, 'lr_warmup': 10000, 'optimizer': 'SGD', 'use_max_pool': True, 'embedding_dimension': 24, 'max_sequence_length': 512, 'heads': 8, 'depth': 10, 'rng_seed': 1, 'gradient_clipping': 1.0, 'dense_width': 64, 'dense_depth': 3, 'dropout': 0.2, 'log_dir': 'ml\/logs\/2022-10-04_16-22-22', 'using_small_dataset': True}\nTraining epoch 1\/3:   0%|          | 0\/22 [00:00&lt;?, ?it\/s]Exception in thread ChkStopThr:\nTraceback (most recent call last):\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 980, in _bootstrap_inner\n    self.run()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 917, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 190, in check_status\n    status_response = self._interface.communicate_stop_status()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 128, in communicate_stop_status\n    resp = self._communicate_stop_status(status)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 69, in _communicate_stop_status\n    data = super()._communicate_stop_status(status)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 399, in _communicate_stop_status\n    resp = self._communicate(req, local=True)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 230, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 58, in _communicate_async\n    future = self._router.send_and_receive(rec, local=local)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\router.py\", line 94, in send_and_receive\n    self._send_message(rec)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\router_sock.py\", line 35, in _send_message\n    self._sock_client.send_record_communicate(record)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 216, in send_record_communicate\n    self.send_server_request(server_req)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\nException in thread NetStatThr:\nTraceback (most recent call last):\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 980, in _bootstrap_inner\n    self.run()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 917, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 172, in check_network_status\n    status_response = self._interface.communicate_network_status()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 139, in communicate_network_status\n    resp = self._communicate_network_status(status)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 82, in _communicate_network_status\n    data = super()._communicate_network_status(status)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 409, in _communicate_network_status\n    resp = self._communicate(req, local=True)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 230, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 58, in _communicate_async\n    future = self._router.send_and_receive(rec, local=local)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\router.py\", line 94, in send_and_receive\n    self._send_message(rec)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\router_sock.py\", line 35, in _send_message\n    self._sock_client.send_record_communicate(record)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 216, in send_record_communicate\n    self.send_server_request(server_req)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\nTraining epoch 1\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22\/22 [00:06&lt;00:00,  3.32it\/s]\nTesting epoch 1\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10\/10 [00:00&lt;00:00, 20.42it\/s]\nTraining epoch 2\/3:   0%|          | 0\/22 [00:00&lt;?, ?it\/s]Train: acc =    9.43% | loss = 260.72%\nTest:  acc =   16.67% | loss = 225.52%\nTraining epoch 2\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22\/22 [00:03&lt;00:00,  5.70it\/s]\nTesting epoch 2\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10\/10 [00:00&lt;00:00, 21.73it\/s]\nTraining epoch 3\/3:   0%|          | 0\/22 [00:00&lt;?, ?it\/s]Train: acc =   11.60% | loss = 221.38%\nTest:  acc =   13.33% | loss = 218.46%\nTraining epoch 3\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22\/22 [00:03&lt;00:00,  5.84it\/s]\nTesting epoch 3\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10\/10 [00:00&lt;00:00, 21.05it\/s]\nTrain: acc =   12.40% | loss = 218.99%\nTest:  acc =   13.33% | loss = 235.97%\nException in thread Thread-14:\nTraceback (most recent call last):\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 299, in _run_job\n    wandb.finish()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3565, in finish\n    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 282, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 245, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1713, in finish\n    return self._finish(exit_code, quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1719, in _finish\n    tel.feature.finish = True\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\telemetry.py\", line 40, in __exit__\n    self._run._telemetry_callback(self._obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 593, in _telemetry_callback\n    self._telemetry_flush()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 604, in _telemetry_flush\n    self._backend.interface._publish_telemetry(self._telemetry_obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 78, in _publish_telemetry\n    self._publish(rec)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 980, in _bootstrap_inner\n    self.run()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 917, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 303, in _run_job\n    wandb.finish(exit_code=1)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3565, in finish\n    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 282, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 245, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1713, in finish\n    return self._finish(exit_code, quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1719, in _finish\n    tel.feature.finish = True\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\telemetry.py\", line 40, in __exit__\n    self._run._telemetry_callback(self._obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 593, in _telemetry_callback\n    self._telemetry_flush()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 604, in _telemetry_flush\n    self._backend.interface._publish_telemetry(self._telemetry_obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 78, in _publish_telemetry\n    self._publish(rec)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\nwandb: Agent Starting Run: ly4ox06s with config:\nwandb: \tbatch_size: 16\nwandb: \tdense_depth: 4\nwandb: \tdense_width: 128\nwandb: \tdepth: 10\nwandb: \tdropout: 0.3449039365016265\nwandb: \tepochs: 15\nwandb: \tgradient_clipping: 1\nwandb: \theads: 6\nwandb: \tlr: 0.0007649760799562746\nwandb: \tlr_warmup: 1057\nwandb: \tmax_seq_len: 64\nwandb: \toptimizer: AdamW\nwandb: \tuse_max_pool: False\nModel created with 90,714 parameters.\ntest: wandb.config: {'epochs': 3, 'batch_size': 16, 'test_size': 0.3, 'lr': 5, 'lr_warmup': 10000, 'optimizer': 'SGD', 'use_max_pool': True, 'embedding_dimension': 24, 'max_sequence_length': 512, 'heads': 8, 'depth': 10, 'rng_seed': 1, 'gradient_clipping': 1.0, 'dense_width': 64, 'dense_depth': 3, 'dropout': 0.2, 'log_dir': 'ml\/logs\/2022-10-04_16-22-22', 'using_small_dataset': True}\nTraining epoch 1\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22\/22 [00:03&lt;00:00,  5.86it\/s]\nTesting epoch 1\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10\/10 [00:00&lt;00:00, 21.62it\/s]\nTrain: acc =   10.57% | loss = 247.77%\nTest:  acc =   10.00% | loss = 227.83%\nTraining epoch 2\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22\/22 [00:03&lt;00:00,  5.84it\/s]\nTesting epoch 2\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10\/10 [00:00&lt;00:00, 21.09it\/s]\nTraining epoch 3\/3:   0%|          | 0\/22 [00:00&lt;?, ?it\/s]Train: acc =   10.60% | loss = 236.37%\nTest:  acc =   10.00% | loss = 231.93%\nTraining epoch 3\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22\/22 [00:03&lt;00:00,  5.83it\/s]\nTesting epoch 3\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10\/10 [00:00&lt;00:00, 21.15it\/s]\nException in thread Thread-15:\nTraceback (most recent call last):\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 299, in _run_job\n    wandb.finish()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3565, in finish\nTrain: acc =   10.40% | loss = 233.15%\nTest:  acc =   13.33% | loss = 230.50%\n    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 282, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 245, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1713, in finish\n    return self._finish(exit_code, quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1719, in _finish\n    tel.feature.finish = True\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\telemetry.py\", line 40, in __exit__\n    self._run._telemetry_callback(self._obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 593, in _telemetry_callback\n    self._telemetry_flush()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 604, in _telemetry_flush\n    self._backend.interface._publish_telemetry(self._telemetry_obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 78, in _publish_telemetry\n    self._publish(rec)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 980, in _bootstrap_inner\n    self.run()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 917, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 303, in _run_job\n    wandb.finish(exit_code=1)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3565, in finish\n    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 282, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 245, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1713, in finish\n    return self._finish(exit_code, quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1719, in _finish\n    tel.feature.finish = True\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\telemetry.py\", line 40, in __exit__\n    self._run._telemetry_callback(self._obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 593, in _telemetry_callback\n    self._telemetry_flush()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 604, in _telemetry_flush\n    self._backend.interface._publish_telemetry(self._telemetry_obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 78, in _publish_telemetry\n    self._publish(rec)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\nwandb: Agent Starting Run: viajyjrk with config:\nwandb: \tbatch_size: 16\nwandb: \tdense_depth: 4\nwandb: \tdense_width: 128\nwandb: \tdepth: 8\nwandb: \tdropout: 0.06200113164824134\nwandb: \tepochs: 15\nwandb: \tgradient_clipping: 1\nwandb: \theads: 6\nwandb: \tlr: 0.0006929869499125819\nwandb: \tlr_warmup: 2917\nwandb: \tmax_seq_len: 512\nwandb: \toptimizer: SGD\nwandb: \tuse_max_pool: False\nTraining epoch 1\/3:   0%|          | 0\/22 [00:00&lt;?, ?it\/s]Model created with 90,714 parameters.\ntest: wandb.config: {'epochs': 3, 'batch_size': 16, 'test_size': 0.3, 'lr': 5, 'lr_warmup': 10000, 'optimizer': 'SGD', 'use_max_pool': True, 'embedding_dimension': 24, 'max_sequence_length': 512, 'heads': 8, 'depth': 10, 'rng_seed': 1, 'gradient_clipping': 1.0, 'dense_width': 64, 'dense_depth': 3, 'dropout': 0.2, 'log_dir': 'ml\/logs\/2022-10-04_16-22-22', 'using_small_dataset': True}\nTraining epoch 1\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22\/22 [00:03&lt;00:00,  5.65it\/s]\nTesting epoch 1\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10\/10 [00:00&lt;00:00, 21.07it\/s]\nTrain: acc =    9.43% | loss = 262.65%\nTest:  acc =   10.00% | loss = 221.72%\nTraining epoch 2\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22\/22 [00:03&lt;00:00,  5.86it\/s]\nTesting epoch 2\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10\/10 [00:00&lt;00:00, 21.02it\/s]\nTrain: acc =   11.60% | loss = 236.80%\nTest:  acc =   10.00% | loss = 234.05%\nTraining epoch 3\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22\/22 [00:03&lt;00:00,  5.81it\/s]\nTesting epoch 3\/3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10\/10 [00:00&lt;00:00, 20.80it\/s]\nException in thread Thread-16:\nTraceback (most recent call last):\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 299, in _run_job\n    wandb.finish()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3565, in finish\n    wandb.run.finish(exit_code=exit_code, quiet=quiet)\nTrain: acc =   11.00% | loss = 226.62%\nTest:  acc =    9.33% | loss = 221.71%\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 282, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 245, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1713, in finish\n    return self._finish(exit_code, quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1719, in _finish\n    tel.feature.finish = True\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\telemetry.py\", line 40, in __exit__\n    self._run._telemetry_callback(self._obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 593, in _telemetry_callback\n    self._telemetry_flush()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 604, in _telemetry_flush\n    self._backend.interface._publish_telemetry(self._telemetry_obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 78, in _publish_telemetry\n    self._publish(rec)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 980, in _bootstrap_inner\n    self.run()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\threading.py\", line 917, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\agents\\pyagent.py\", line 303, in _run_job\n    wandb.finish(exit_code=1)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 3565, in finish\n    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 282, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 245, in wrapper\n    return func(self, *args, **kwargs)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1713, in finish\n    return self._finish(exit_code, quiet)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 1719, in _finish\n    tel.feature.finish = True\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\telemetry.py\", line 40, in __exit__\n    self._run._telemetry_callback(self._obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 593, in _telemetry_callback\n    self._telemetry_flush()\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 604, in _telemetry_flush\n    self._backend.interface._publish_telemetry(self._telemetry_obj)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 78, in _publish_telemetry\n    self._publish(rec)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n    self._sock_client.send_record_publish(record)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n    self._send_message(msg)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"C:\\Users\\jacks\\anaconda3\\envs\\ml-project\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n\nProcess finished with exit code 0\n<\/code><\/pre>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1664915209749,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":392.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/windows-11-wandb-sweep-gives-connectionreseterror-winerror-10054\/3217",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-06T22:39:41.846Z",
                "Answer_body":"<p>Hi Jackson, this might also be a proxy configuration issue if you are trying this from inside a fire-walled corporate network or are you connected to a VPN?<\/p>",
                "Answer_score":21.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-12T12:35:51.902Z",
                "Answer_body":"<p>This also happened to me now and I do not have VPN\/firewall. It happened when I changed the project name, used wandb.init(project_name = \u201cnew_project_name\u201d) and since then every time I want to run wandb() I get the same error like above.<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-12T16:03:00.848Z",
                "Answer_body":"<p>I see, can you give me the debug logs that are found in your wandb run directory when this occurs?<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-17T19:08:23.678Z",
                "Answer_body":"<p>Hi Jackson, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-18T11:34:50.418Z",
                "Answer_body":"<p>I\u2019m receiving the same error repeatedly when I start a sweep.<\/p>\n<p>The log files: <a href=\"https:\/\/mega.nz\/file\/kfFH2Q7Y#I35dATozdctEoH_J4-DuLvCWSWVH1Tzy_iph6F5WW68\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">4.1 KB file on MEGA<\/a><\/p>\n<pre><code class=\"lang-auto\">Create sweep with ID: qs048o8w\nSweep URL: https:\/\/wandb.ai\/maxw\/lit-mnist\/sweeps\/qs048o8w\nwandb: WARNING Calling wandb.login() after wandb.init() has no effect.\nwandb: Waiting for W&amp;B process to finish... (success).\nwandb: Synced solar-yogurt-63: https:\/\/wandb.ai\/maxw\/lit-mnist\/runs\/vla9pdno\nwandb: Synced 6 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nwandb: Find logs at: .\\wandb\\run-20221118_121240-vla9pdno\\logs\nwandb: Agent Starting Run: 618an4iq with config:\nwandb: \tbatch_size: 16\nwandb: \tdropout: 0.5\nwandb: \tepochs: 5\nwandb: \tlr: 0.06530954613403434\nC:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n  rank_zero_warn(\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n  | Name | Type       | Params\n------------------------------------\n0 | net  | Sequential | 1.4 M \n------------------------------------\n1.4 M     Trainable params\n0         Non-trainable params\n1.4 M     Total params\n5.518     Total estimated model params size (MB)\nSanity Checking: 0it [00:00, ?it\/s]C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n  rank_zero_warn(\nSanity Checking DataLoader 0:   0%|          | 0\/2 [00:00&lt;?, ?it\/s]Exception in thread ChkStopThr:\nTraceback (most recent call last):\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\threading.py\", line 980, in _bootstrap_inner\n    self.run()\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\threading.py\", line 917, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 200, in check_status\n    status_response = self._interface.communicate_stop_status()\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 128, in communicate_stop_status\n    resp = self._communicate_stop_status(status)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 69, in _communicate_stop_status\n    data = super()._communicate_stop_status(status)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 424, in _communicate_stop_status\n    resp = self._communicate(req, local=True)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 255, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 58, in _communicate_async\n    future = self._router.send_and_receive(rec, local=local)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\interface\\router.py\", line 94, in send_and_receive\n    self._send_message(rec)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\interface\\router_sock.py\", line 36, in _send_message\n    self._sock_client.send_record_communicate(record)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 216, in send_record_communicate\nException in thread NetStatThr:\nTraceback (most recent call last):\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\threading.py\", line 980, in _bootstrap_inner\n    self.send_server_request(server_req)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n    self.run()\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\threading.py\", line 917, in run\n    self._send_message(msg)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 182, in check_network_status\n    self._sendall_with_error_handle(header + data)\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n    status_response = self._interface.communicate_network_status()\n  File \"C:\\Users\\perry\\miniconda3\\envs\\PaperReplicas\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 139, in communicate_network_status\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n<\/code><\/pre>",
                "Answer_score":10.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-17T11:35:16.621Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"window sweep give connectionreseterror winerror hello sort new try sweep time problem creat run sweep web copi past command start agent bash start agent keep give problem start train run problem get error time new run start origin thread creat sure run log sweep page data iv log run activ dot script end figur call order agent popul random valu config like set default config valu specifi sweep config dict sweep agent updat config random valu creat sweep config dict like happen run bash traceback print config right model train simpli us config dict specifi call init updat overwritten agent traceback thank advanc idea user jack anaconda env project python ex user jack project train script devic cuda current log jacksth us login relogin forc relogin track run version run data save local user jack project run ajcjt run offlin turn sync sync run rest dew view project http jacksth view run http jacksth run ajcjt load data elaps convert data tensor run ajcjt sync file media file artifact file file log run ajcjt log agent start run bqur config batch size dens depth dens width depth dropout epoch gradient clip head warmup max seq len optim adamw us max pool fals model creat paramet test config epoch batch size test size warmup optim sgd us max pool true embed dimens max sequenc length head depth rng seed gradient clip dens width dens depth dropout log dir log small dataset true train epoch except thread chkstopthr traceback recent file user jack anaconda env project lib thread line bootstrap inner self run file user jack anaconda env project lib thread line run self target self arg self kwarg file user jack anaconda env project lib site packag sdk run line check statu statu respons self interfac commun stop statu file user jack anaconda env project lib site packag sdk interfac interfac line commun stop statu resp self commun stop statu statu file user jack anaconda env project lib site packag sdk interfac interfac sock line commun stop statu data super commun stop statu statu file user jack anaconda env project lib site packag sdk interfac interfac share line commun stop statu resp self commun req local true file user jack anaconda env project lib site packag sdk interfac interfac share line commun return self commun async rec local local timeout timeout file user jack anaconda env project lib site packag sdk interfac interfac sock line commun async futur self router send receiv rec local local file user jack anaconda env project lib site packag sdk interfac router line send receiv self send messag rec file user jack anaconda env project lib site packag sdk interfac router sock line send messag self sock client send record commun record file user jack anaconda env project lib site packag sdk lib sock client line send record commun self send server request server req file user jack anaconda env project lib site packag sdk lib sock client line send server request self send messag msg file user jack anaconda env project lib site packag sdk lib sock client line send messag self sendal error handl header data file user jack anaconda env project lib site packag sdk lib sock client line sendal error handl sent self sock send data connectionreseterror winerror exist connect forcibl close remot host except thread netstatthr traceback recent file user jack anaconda env project lib thread line bootstrap inner self run file user jack anaconda env project lib thread line run self target self arg self kwarg file user jack anaconda env project lib site packag sdk run line check network statu statu respons self interfac commun network statu file user jack anaconda env project lib site packag sdk interfac interfac line commun network statu resp self commun network statu statu file user jack anaconda env project lib site packag sdk interfac interfac sock line commun network statu data super commun network statu statu file user jack anaconda env project lib site packag sdk interfac interfac share line commun network statu resp self commun req local true file user jack anaconda env project lib site packag sdk interfac interfac share line commun return self commun async rec local local timeout timeout file user jack anaconda env project lib site packag sdk interfac interfac sock line commun async futur self router send receiv rec local local file user jack anaconda env project lib site packag sdk interfac router line send receiv self send messag rec file user jack anaconda env project lib site packag sdk interfac router sock line send messag self sock client send record commun record file user jack anaconda env project lib site packag sdk lib sock client line send record commun self send server request server req file user jack anaconda env project lib site packag sdk lib sock client line send server request self send messag msg file user jack anaconda env project lib site packag sdk lib sock client line send messag self sendal error handl header data file user jack anaconda env project lib site packag sdk lib sock client line sendal error handl sent self sock send data connectionreseterror winerror exist connect forcibl close remot host train epoch test epoch train epoch train acc loss test acc loss train epoch test epoch train epoch train acc loss test acc loss train epoch test epoch train acc loss test acc loss except thread thread traceback recent file user jack anaconda env project lib site packag agent pyagent line run job finish file user jack anaconda env project lib site packag sdk run line finish run finish exit code exit code quiet quiet file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line finish return self finish exit code quiet file user jack anaconda env project lib site packag sdk run line finish tel featur finish true file user jack anaconda env project lib site packag sdk lib telemetri line exit self run telemetri callback self obj file user jack anaconda env project lib site packag sdk run line telemetri callback self telemetri flush file user jack anaconda env project lib site packag sdk run line telemetri flush self backend interfac publish telemetri self telemetri obj file user jack anaconda env project lib site packag sdk interfac interfac share line publish telemetri self publish rec file user jack anaconda env project lib site packag sdk interfac interfac sock line publish self sock client send record publish record file user jack anaconda env project lib site packag sdk lib sock client line send record publish self send server request server req file user jack anaconda env project lib site packag sdk lib sock client line send server request self send messag msg file user jack anaconda env project lib site packag sdk lib sock client line send messag self sendal error handl header data file user jack anaconda env project lib site packag sdk lib sock client line sendal error handl sent self sock send data connectionreseterror winerror exist connect forcibl close remot host handl except except occur traceback recent file user jack anaconda env project lib thread line bootstrap inner self run file user jack anaconda env project lib thread line run self target self arg self kwarg file user jack anaconda env project lib site packag agent pyagent line run job finish exit code file user jack anaconda env project lib site packag sdk run line finish run finish exit code exit code quiet quiet file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line finish return self finish exit code quiet file user jack anaconda env project lib site packag sdk run line finish tel featur finish true file user jack anaconda env project lib site packag sdk lib telemetri line exit self run telemetri callback self obj file user jack anaconda env project lib site packag sdk run line telemetri callback self telemetri flush file user jack anaconda env project lib site packag sdk run line telemetri flush self backend interfac publish telemetri self telemetri obj file user jack anaconda env project lib site packag sdk interfac interfac share line publish telemetri self publish rec file user jack anaconda env project lib site packag sdk interfac interfac sock line publish self sock client send record publish record file user jack anaconda env project lib site packag sdk lib sock client line send record publish self send server request server req file user jack anaconda env project lib site packag sdk lib sock client line send server request self send messag msg file user jack anaconda env project lib site packag sdk lib sock client line send messag self sendal error handl header data file user jack anaconda env project lib site packag sdk lib sock client line sendal error handl sent self sock send data connectionreseterror winerror exist connect forcibl close remot host agent start run lyox config batch size dens depth dens width depth dropout epoch gradient clip head warmup max seq len optim adamw us max pool fals model creat paramet test config epoch batch size test size warmup optim sgd us max pool true embed dimens max sequenc length head depth rng seed gradient clip dens width dens depth dropout log dir log small dataset true train epoch test epoch train acc loss test acc loss train epoch test epoch train epoch train acc loss test acc loss train epoch test epoch except thread thread traceback recent file user jack anaconda env project lib site packag agent pyagent line run job finish file user jack anaconda env project lib site packag sdk run line finish train acc loss test acc loss run finish exit code exit code quiet quiet file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line finish return self finish exit code quiet file user jack anaconda env project lib site packag sdk run line finish tel featur finish true file user jack anaconda env project lib site packag sdk lib telemetri line exit self run telemetri callback self obj file user jack anaconda env project lib site packag sdk run line telemetri callback self telemetri flush file user jack anaconda env project lib site packag sdk run line telemetri flush self backend interfac publish telemetri self telemetri obj file user jack anaconda env project lib site packag sdk interfac interfac share line publish telemetri self publish rec file user jack anaconda env project lib site packag sdk interfac interfac sock line publish self sock client send record publish record file user jack anaconda env project lib site packag sdk lib sock client line send record publish self send server request server req file user jack anaconda env project lib site packag sdk lib sock client line send server request self send messag msg file user jack anaconda env project lib site packag sdk lib sock client line send messag self sendal error handl header data file user jack anaconda env project lib site packag sdk lib sock client line sendal error handl sent self sock send data connectionreseterror winerror exist connect forcibl close remot host handl except except occur traceback recent file user jack anaconda env project lib thread line bootstrap inner self run file user jack anaconda env project lib thread line run self target self arg self kwarg file user jack anaconda env project lib site packag agent pyagent line run job finish exit code file user jack anaconda env project lib site packag sdk run line finish run finish exit code exit code quiet quiet file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line finish return self finish exit code quiet file user jack anaconda env project lib site packag sdk run line finish tel featur finish true file user jack anaconda env project lib site packag sdk lib telemetri line exit self run telemetri callback self obj file user jack anaconda env project lib site packag sdk run line telemetri callback self telemetri flush file user jack anaconda env project lib site packag sdk run line telemetri flush self backend interfac publish telemetri self telemetri obj file user jack anaconda env project lib site packag sdk interfac interfac share line publish telemetri self publish rec file user jack anaconda env project lib site packag sdk interfac interfac sock line publish self sock client send record publish record file user jack anaconda env project lib site packag sdk lib sock client line send record publish self send server request server req file user jack anaconda env project lib site packag sdk lib sock client line send server request self send messag msg file user jack anaconda env project lib site packag sdk lib sock client line send messag self sendal error handl header data file user jack anaconda env project lib site packag sdk lib sock client line sendal error handl sent self sock send data connectionreseterror winerror exist connect forcibl close remot host agent start run viajyjrk config batch size dens depth dens width depth dropout epoch gradient clip head warmup max seq len optim sgd us max pool fals train epoch model creat paramet test config epoch batch size test size warmup optim sgd us max pool true embed dimens max sequenc length head depth rng seed gradient clip dens width dens depth dropout log dir log small dataset true train epoch test epoch train acc loss test acc loss train epoch test epoch train acc loss test acc loss train epoch test epoch except thread thread traceback recent file user jack anaconda env project lib site packag agent pyagent line run job finish file user jack anaconda env project lib site packag sdk run line finish run finish exit code exit code quiet quiet train acc loss test acc loss file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line finish return self finish exit code quiet file user jack anaconda env project lib site packag sdk run line finish tel featur finish true file user jack anaconda env project lib site packag sdk lib telemetri line exit self run telemetri callback self obj file user jack anaconda env project lib site packag sdk run line telemetri callback self telemetri flush file user jack anaconda env project lib site packag sdk run line telemetri flush self backend interfac publish telemetri self telemetri obj file user jack anaconda env project lib site packag sdk interfac interfac share line publish telemetri self publish rec file user jack anaconda env project lib site packag sdk interfac interfac sock line publish self sock client send record publish record file user jack anaconda env project lib site packag sdk lib sock client line send record publish self send server request server req file user jack anaconda env project lib site packag sdk lib sock client line send server request self send messag msg file user jack anaconda env project lib site packag sdk lib sock client line send messag self sendal error handl header data file user jack anaconda env project lib site packag sdk lib sock client line sendal error handl sent self sock send data connectionreseterror winerror exist connect forcibl close remot host handl except except occur traceback recent file user jack anaconda env project lib thread line bootstrap inner self run file user jack anaconda env project lib thread line run self target self arg self kwarg file user jack anaconda env project lib site packag agent pyagent line run job finish exit code file user jack anaconda env project lib site packag sdk run line finish run finish exit code exit code quiet quiet file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line wrapper return func self arg kwarg file user jack anaconda env project lib site packag sdk run line finish return self finish exit code quiet file user jack anaconda env project lib site packag sdk run line finish tel featur finish true file user jack anaconda env project lib site packag sdk lib telemetri line exit self run telemetri callback self obj file user jack anaconda env project lib site packag sdk run line telemetri callback self telemetri flush file user jack anaconda env project lib site packag sdk run line telemetri flush self backend interfac publish telemetri self telemetri obj file user jack anaconda env project lib site packag sdk interfac interfac share line publish telemetri self publish rec file user jack anaconda env project lib site packag sdk interfac interfac sock line publish self sock client send record publish record file user jack anaconda env project lib site packag sdk lib sock client line send record publish self send server request server req file user jack anaconda env project lib site packag sdk lib sock client line send server request self send messag msg file user jack anaconda env project lib site packag sdk lib sock client line send messag self sendal error handl header data file user jack anaconda env project lib site packag sdk lib sock client line sendal error handl sent self sock send data connectionreseterror winerror exist connect forcibl close remot host process finish exit code",
        "Question_preprocessed_content":"give connectionreseterror hello sort new try sweep time problem creat run sweep web command start agent bash start keep give problem start train run problem get error time new run start origin thread creat sure run log sweep page data iv log figur call order agent popul random valu like set default config valu sweep agent updat random valu creat dict traceback print right model train simpli us dict specifi call traceback thank advanc idea",
        "Question_gpt_summary_original":"the user is encountering challenges with using the `.sweep()` function, resulting in connectionreseterror: [winerror 10054] and difficulty in getting the agent to populate its randomized values into .config.",
        "Question_gpt_summary":"user encount challeng sweep function result connectionreseterror winerror difficulti get agent popul random valu config",
        "Answer_original_content":"jackson proxi configur issu try insid wall corpor network connect vpn happen vpn firewal happen chang project init project new project time want run error like debug log run directori occur jackson heard go close request like open convers let know receiv error repeatedli start sweep log file file mega creat sweep qsow sweep url http maxw lit mnist sweep qsow warn call login init effect wait process finish success sync solar yogurt http maxw lit mnist run vlapdno sync file media file artifact file file log run vlapdno log agent start run aniq config batch size dropout epoch user perri miniconda env paperreplica lib site packag pytorch lightn loop util possibleuserwarn max epoch set set epoch train epoch limit set max epoch rank zero warn local rank cuda visibl devic type param net sequenti trainabl param non trainabl param total param total estim model param size saniti check user perri miniconda env paperreplica lib site packag pytorch lightn trainer connector data connector possibleuserwarn dataload val dataload worker bottleneck consid increas valu num worker argument try number cpu machin dataload init improv perform rank zero warn saniti check dataload except thread chkstopthr traceback recent file user perri miniconda env paperreplica lib thread line bootstrap inner self run file user perri miniconda env paperreplica lib thread line run self target self arg self kwarg file user perri miniconda env paperreplica lib site packag sdk run line check statu statu respons self interfac commun stop statu file user perri miniconda env paperreplica lib site packag sdk interfac interfac line commun stop statu resp self commun stop statu statu file user perri miniconda env paperreplica lib site packag sdk interfac interfac sock line commun stop statu data super commun stop statu statu file user perri miniconda env paperreplica lib site packag sdk interfac interfac share line commun stop statu resp self commun req local true file user perri miniconda env paperreplica lib site packag sdk interfac interfac share line commun return self commun async rec local local timeout timeout file user perri miniconda env paperreplica lib site packag sdk interfac interfac sock line commun async futur self router send receiv rec local local file user perri miniconda env paperreplica lib site packag sdk interfac router line send receiv self send messag rec file user perri miniconda env paperreplica lib site packag sdk interfac router sock line send messag self sock client send record commun record file user perri miniconda env paperreplica lib site packag sdk lib sock client line send record commun except thread netstatthr traceback recent file user perri miniconda env paperreplica lib thread line bootstrap inner self send server request server req file user perri miniconda env paperreplica lib site packag sdk lib sock client line send server request self run file user perri miniconda env paperreplica lib thread line run self send messag msg file user perri miniconda env paperreplica lib site packag sdk lib sock client line send messag self target self arg self kwarg file user perri miniconda env paperreplica lib site packag sdk run line check network statu self sendal error handl header data file user perri miniconda env paperreplica lib site packag sdk lib sock client line sendal error handl statu respons self interfac commun network statu file user perri miniconda env paperreplica lib site packag sdk interfac interfac line commun network statu sent self sock send data connectionreseterror winerror exist connect forcibl close remot host topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"jackson proxi configur issu try insid corpor network connect vpn happen happen chang project time want run error like debug log run directori occur jackson heard go close request like convers let know receiv error repeatedli start sweep log file file mega topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no clear solutions provided in the answer. the answerer asks for debug logs and suggests that the issue may be related to a proxy configuration issue if the user is trying to run the function from inside a fire-walled corporate network or while connected to a vpn. the answerer also mentions that they encountered a similar error when they changed the project name and used `.init(project_name = new_project_name)`.",
        "Answer_gpt_summary":"clear solut provid answer answer ask debug log suggest issu relat proxi configur issu user try run function insid wall corpor network connect vpn answer mention encount similar error chang project init project new project"
    },
    {
        "Question_id":70379395.0,
        "Question_title":"Vertex AI - Viewing Pipeline Output",
        "Question_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639659363547,
        "Question_favorite_count":null,
        "Question_last_edit_time":1639726542407,
        "Question_score":2.0,
        "Question_view_count":346.0,
        "Answer_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1639750456896,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639667359983,
        "Question_original_content":"view pipelin output follow tutori creat schedul pipelin run minut thing print hello return string run run time updat run result success time question simpl string print output pipelin",
        "Question_preprocessed_content":"view pipelin output follow tutori creat schedul pipelin run minut thing print return string run run time updat run result success time question simpl string print output pipelin",
        "Question_gpt_summary_original":"The user has successfully created a scheduled Vertex AI Pipeline that prints a greeting string and returns the same string. However, the user is facing a challenge in locating the output of the pipeline and the printed string.",
        "Question_gpt_summary":"user successfulli creat schedul pipelin print greet string return string user face challeng locat output pipelin print string",
        "Answer_original_content":"output print statement pipelin cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon refer screenshot ran sampl pipelin codelab intro vertex pipelin output print statement pipelin updat compon pipelin run deploi individu custom job correspond sampl pipelin consist compon entri custom job section shown view log run level need queri log entri respect job id pipelin compon job cloud schedul job queri look like resourc label job job job job sever default simultan pipelin run simpler queri like resourc type cloud schedul job job sever default",
        "Answer_preprocessed_content":"output statement pipelin cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon refer screenshot ran sampl pipelin codelab intro vertex pipelin output statement pipelin updat compon pipelin run deploi individu custom job correspond sampl pipelin consist compon entri custom job section shown view log run level need queri log entri respect pipelin compon cloud schedul job queri look like simultan pipelin run simpler queri like",
        "Answer_gpt_summary_original":"possible solutions to view the output of a scheduled pipeline that prints and returns a string are: \n1. check the cloud logging with appropriate filters to find the output of the print() statements in the pipeline. \n2. click on the respective component in the console and click \"view logs\" in the right pane to check logs for each component in the pipeline. \n3. query the log entries with the respective job_ids of the pipeline components and the job_id of the cloud scheduler job to view the logs on the run level. \n4. use a simpler query like resource.type=(\"cloud_scheduler_job\" or \"ml_job\") severity>=default if there are no simultaneous pipeline runs.",
        "Answer_gpt_summary":"possibl solut view output schedul pipelin print return string check cloud log appropri filter output print statement pipelin click respect compon consol click view log right pane check log compon pipelin queri log entri respect job id pipelin compon job cloud schedul job view log run level us simpler queri like resourc type cloud schedul job job sever default simultan pipelin run"
    },
    {
        "Question_id":null,
        "Question_title":"Generating scoring.py for azure ml model deployment",
        "Question_body":"Hi,\nI have registered a pretrained model to azure ml and i wish to deploy the model.\nThere is a compulsory file attachment called scoring file that i need to attach in order to deploy my model.\nMay i know how do i generate this scoring.py file?\nThanks in advance",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1642499410857,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/699403\/generating-scoringpy-for-azure-ml-model-deployment.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-18T17:38:53.153Z",
                "Answer_score":0,
                "Answer_body":"@Yuzu-9670\n\nHello,\n\nThanks for reaching out to us. Please follow below documentation to define your entry script after you registered your model.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli#define-a-dummy-entry-script\n\nPlease start with define a dummy entry and do the steps after.\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-26T04:53:39.51Z",
                "Answer_score":0,
                "Answer_body":"See this link:\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-managed-online-endpoints?tabs=azure-cli#understand-the-scoring-script\n\nSee this other thread as well:\nhttps:\/\/learn.microsoft.com\/en-us\/answers\/questions\/713048\/azure-ml-how-should-my-score-script-look-like-to-d.html\n\nIn my case, using Automated ML, I was able to download the scoring script by clicking on the model (yolov5 in my case) then clicking download\n@IamBeginner-6521",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":17.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"gener score model deploy regist pretrain model wish deploi model compulsori file attach call score file need attach order deploi model know gener score file thank advanc",
        "Question_preprocessed_content":"gener model deploy regist pretrain model wish deploi model compulsori file attach call score file need attach order deploi model know gener file thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge in generating a scoring.py file for deploying a pretrained model on Azure ML. They are seeking guidance on how to create this compulsory file attachment.",
        "Question_gpt_summary":"user face challeng gener score file deploi pretrain model seek guidanc creat compulsori file attach",
        "Answer_original_content":"yuzu hello thank reach follow document defin entri script regist model http doc microsoft com azur machin learn deploi tab azcli defin dummi entri script start defin dummi entri step hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder link http learn microsoft com azur machin learn deploi manag onlin endpoint tab azur cli understand score script thread http learn microsoft com answer question azur score script look like html case autom abl download score script click model yolov case click download iambeginn",
        "Answer_preprocessed_content":"hello thank reach follow document defin entri script regist model start defin dummi entri step hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder link thread case autom abl download score script click model click download",
        "Answer_gpt_summary_original":"the answer provides a link to documentation on how to define an entry script for a pretrained model in order to generate a scoring.py file. the user is advised to start with defining a dummy entry and follow the steps provided in the documentation. additionally, the answer provides links to other resources that may be helpful.",
        "Answer_gpt_summary":"answer provid link document defin entri script pretrain model order gener score file user advis start defin dummi entri follow step provid document addition answer provid link resourc help"
    },
    {
        "Question_id":56255154.0,
        "Question_title":"How to use a pretrained model from s3 to predict some data?",
        "Question_body":"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. <\/p>\n\n<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.<\/p>\n\n<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:<\/p>\n\n<p><a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker\" rel=\"noreferrer\">https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker<\/a>\n<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb<\/a><\/p>\n\n<p>My code is this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pipeline import PipelineModel\nfrom sagemaker.model import Model\n\ns3_model_bucket = 'bucket'\ns3_model_key_prefix = 'prefix'\ndata = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\nmodels = ss_model.create_model() # ss_model is my sagemaker.estimator\n\nmodel = PipelineModel(name=data, role=role, models= [models])\nss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558522248223,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1558524094856,
        "Question_score":6.0,
        "Question_view_count":7404.0,
        "Answer_body":"<p>You can actually instantiate a Python SDK <code>model<\/code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:<\/p>\n\n<pre><code>trainedmodel = sagemaker.model.Model(\n    model_data='s3:\/\/...model path here..\/model.tar.gz',\n    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1\n    role=role)  # your role here; could be different name\n\ntrainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>\n\n<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:<\/p>\n\n<pre><code>predictor = sagemaker.predictor.RealTimePredictor(\n    endpoint='endpoint name here',\n    content_type='image\/jpeg',\n    accept='image\/png')\n<\/code><\/pre>\n\n<p>More on those abstractions:<\/p>\n\n<ul>\n<li><code>Model<\/code>: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html<\/a><\/li>\n<li><code>Predictor<\/code>:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56255154",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1558621559712,
        "Question_original_content":"us pretrain model predict data train semant segment model save bucket want load model predict imag know predict leav notebook instanc run train easi deploi help want us older model look sourc abl come work http cours fast deploy amzn html deploi http aw amazon com get start tutori build train deploi machin learn model http readthedoc stabl pipelin html http github com awslab amazon exampl blob master advanc function infer pipelin sparkml xgboost abalon infer pipelin sparkml xgboost abalon ipynb code pipelin import pipelinemodel model import model model bucket bucket model kei prefix prefix data format model bucket model kei prefix model tar model model creat model model estim model pipelinemodel data role role model model predictor model deploi initi instanc count instanc type xlarg",
        "Question_preprocessed_content":"us pretrain model predict data train semant segment model save bucket want load model predict imag know predict leav notebook instanc run train easi deploi help want us older model look sourc abl come work code",
        "Question_gpt_summary_original":"The user wants to load a semantic segmentation model from an S3 bucket to predict some images in Sagemaker. They have tried to use sources such as Fast.ai, AWS tutorials, and Sagemaker documentation to come up with a solution, but their code is not working. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user want load semant segment model bucket predict imag tri us sourc fast aw tutori document come solut code work user seek help resolv issu",
        "Answer_original_content":"actual instanti python sdk model object exist artifact deploi endpoint allow deploi model train artifact have retrain notebook exampl semant segment model trainedmodel model model model data model path model tar imag dkr ecr west amazonaw com semant segment latest exampl path semant segment west role role role differ trainedmodel deploi initi instanc count instanc type xlarg similarli instanti predictor object deploi endpoint authent client support sdk follow command predictor predictor realtimepredictor endpoint endpoint content type imag jpeg accept imag png abstract model http readthedoc stabl model html predictor http readthedoc stabl predictor html",
        "Answer_preprocessed_content":"actual instanti python sdk object exist artifact deploi endpoint allow deploi model train artifact have retrain notebook exampl semant segment model similarli instanti predictor object deploi endpoint authent client support sdk follow command abstract",
        "Answer_gpt_summary_original":"the solution to the challenge of using a pretrained model from an s3 bucket to predict some data is to instantiate a python sdk model object from existing artifacts and deploy it to an endpoint. this allows you to deploy a model from trained artifacts without having to retrain in the notebook. you can also instantiate a predictor object on a deployed endpoint from any authenticated client supporting the sdk.",
        "Answer_gpt_summary":"solut challeng pretrain model bucket predict data instanti python sdk model object exist artifact deploi endpoint allow deploi model train artifact have retrain notebook instanti predictor object deploi endpoint authent client support sdk"
    },
    {
        "Question_id":55479366.0,
        "Question_title":"Errors running Sagemaker Batch Transformation with LDA model",
        "Question_body":"<p>I've successfully trained a LDA model with sagemaker, I've been able to set up an Inference API but it has a limit of how many records I can query at a time. <\/p>\n\n<p>I need to get predictions for a large file and have been trying to use Batch Transformation however am running against roadblock.<\/p>\n\n<p>My input date is in application\/x-recordio-protobuf content type, code is as follows:<\/p>\n\n<pre><code># Initialize the transformer object\ntransformer =sagemaker.transformer.Transformer(\n    base_transform_job_name='Batch-Transform',\n    model_name=model_name,\n    instance_count=1,\n    instance_type='ml.c4.xlarge',\n    output_path=output_location,\n    max_payload=20,\n    strategy='MultiRecord'\n    )\n# Start a transform job\ntransformer.transform(input_location, content_type='application\/x-recordio-protobuf',split_type=\"RecordIO\")\n# Then wait until the transform job has completed\ntransformer.wait()\n\n# Fetch validation result \ns3_client.download_file(bucket, 'topic_model_batch_transform\/output\/batch_tansform_part0.pbr.out', 'batch_tansform-result')\nwith open('batch_tansform-result') as f:\n    results = f.readlines()   \nprint(\"Sample transform result: {}\".format(results[0]))\n<\/code><\/pre>\n\n<p>I have chunked by input file into 10 files each around 19MB in size. I am attempting at first to run on a single chunk, therefore 19MB in total. I have tried changing strategy, trying SingleRecord. I have also tried different split_types, also trying None and \"Line\". <\/p>\n\n<p>I've read the documentation but its not clear what else I should try, also the error messages are very unclear.<\/p>\n\n<pre><code>2019-04-02T15:49:47.617:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=20, BatchStrategy=MULTI_RECORD\n#011at java.lang.Thread.run(Thread.java:748)2019-04-02T15:49:48.035:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: Bad HTTP status returned from invoke: 413\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr:\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: Message:\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\"&gt;\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;title&gt;413 Request Entity Too Large&lt;\/title&gt;\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;h1&gt;Request Entity Too Large&lt;\/h1&gt;\n2019-04-02T15:49:48.036:[sagemaker logs]: du-sagemaker\/data\/batch_transform\/batch_tansform_part0.pbr: &lt;p&gt;The data value transmitted exceeds the capacity limit.&lt;\/p&gt;\n<\/code><\/pre>\n\n<p>The above is the last one I got with the above configuration, before that I was also getting a 400 HTTP error code.<\/p>\n\n<p>Any help or pointers would be greatly appreciated! Thank you<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1554221924910,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":3622.0,
        "Answer_body":"<p>I managed to resolve the issue, it seemed the maxpayload I was using was too high. I set  <code>MaxPayloadInMB=1<\/code> and it now runs like a dream<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55479366",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1556626383163,
        "Question_original_content":"error run batch transform lda model successfulli train lda model abl set infer api limit record queri time need predict larg file try us batch transform run roadblock input date applic recordio protobuf content type code follow initi transform object transform transform transform base transform job batch transform model model instanc count instanc type xlarg output path output locat max payload strategi multirecord start transform job transform transform input locat content type applic recordio protobuf split type recordio wait transform job complet transform wait fetch valid result client download file bucket topic model batch transform output batch tansform pbr batch tansform result open batch tansform result result readlin print sampl transform result format result chunk input file file size attempt run singl chunk total tri chang strategi try singlerecord tri differ split type try line read document clear try error messag unclear log maxconcurrenttransform maxpayloadinmb batchstrategi multi record java lang thread run thread java log data batch transform batch tansform pbr bad http statu return invok log data batch transform batch tansform pbr log data batch transform batch tansform pbr messag log data batch transform batch tansform pbr log data batch transform batch tansform pbr request entiti larg log data batch transform batch tansform pbr request entiti larg log data batch transform batch tansform pbr data valu transmit exce capac limit got configur get http error code help pointer greatli appreci thank",
        "Question_preprocessed_content":"error run batch transform lda model successfulli train lda model abl set infer api limit record queri time need predict larg file try us batch transform run roadblock input date content type code follow chunk input file file size attempt run singl chunk total tri chang strategi try singlerecord tri differ try line read document clear try error messag unclear got configur get http error code help pointer greatli appreci thank",
        "Question_gpt_summary_original":"The user has encountered challenges while trying to use Sagemaker Batch Transformation with an LDA model. They have successfully trained the model and set up an Inference API, but are facing limitations on the number of records they can query at a time. The user has attempted to use Batch Transformation to get predictions for a large file, but is encountering errors with unclear error messages. They have tried different strategies and split types, but are still unable to run the transformation successfully. The user has chunked their input file into 10 files, each around 19MB in size, and is attempting to run on a single chunk. The error message they are receiving is \"413 Request Entity Too Large\".",
        "Question_gpt_summary":"user encount challeng try us batch transform lda model successfulli train model set infer api face limit number record queri time user attempt us batch transform predict larg file encount error unclear error messag tri differ strategi split type unabl run transform successfulli user chunk input file file size attempt run singl chunk error messag receiv request entiti larg",
        "Answer_original_content":"manag resolv issu maxpayload high set maxpayloadinmb run like dream",
        "Answer_preprocessed_content":"manag resolv issu maxpayload high set run like dream",
        "Answer_gpt_summary_original":"the solution to the challenge of receiving unclear error messages such as 413 and 400 http status codes while running batch transformation with an lda model is to reduce the maxpayload to 1 mb.",
        "Answer_gpt_summary":"solut challeng receiv unclear error messag http statu code run batch transform lda model reduc maxpayload"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ml notebooks sharing and compute selection.",
        "Question_body":"What kind of collaboration do we need among the data scientists or developers who need to share these notebooks? What kind of compute does these notebooks require? Is it all single node?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591956129183,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@azureml056-5112 Please follow the below for managing compute instances. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#managing-a-compute-instance All data scientists or developers need is access to the AzureML Workspace and they will have access to a shared file share where everyone\u2019s notebooks can be accessed.\n\n\n\n\nAll notebook require a Compute Instance(CI). CI is a managed VM that exists in AzureML.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/35432\/azure-ml-notebooks-sharing-and-compute-selection.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-12T11:43:28.857Z",
                "Answer_score":0,
                "Answer_body":"@azureml056-5112 Please follow the below for managing compute instances. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-compute-instance#managing-a-compute-instance All data scientists or developers need is access to the AzureML Workspace and they will have access to a shared file share where everyone\u2019s notebooks can be accessed.\n\n\n\n\nAll notebook require a Compute Instance(CI). CI is a managed VM that exists in AzureML.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1591962208856,
        "Question_original_content":"notebook share comput select kind collabor need data scientist develop need share notebook kind comput notebook requir singl node",
        "Question_preprocessed_content":"notebook share comput select kind collabor need data scientist develop need share notebook kind comput notebook requir singl node",
        "Question_gpt_summary_original":"The user is facing challenges related to collaboration and compute selection for sharing Azure ML notebooks among data scientists and developers. They are unsure about the type of collaboration needed and the compute requirements for these notebooks, specifically whether it is all single node.",
        "Question_gpt_summary":"user face challeng relat collabor comput select share notebook data scientist develop unsur type collabor need comput requir notebook specif singl node",
        "Answer_original_content":"follow manag comput instanc http doc microsoft com azur machin learn concept comput instanc manag comput instanc data scientist develop need access workspac access share file share everyon notebook access notebook requir comput instanc manag exist",
        "Answer_preprocessed_content":"follow manag comput instanc data scientist develop need access workspac access share file share everyon notebook access notebook requir comput instanc manag exist",
        "Answer_gpt_summary_original":"the answer suggests that for sharing notebooks among data scientists and developers, all they need is access to the workspace and a shared file share where everyone's notebooks can be accessed. additionally, all notebooks require a compute instance (ci), which is a managed virtual machine that exists in azure. the link provided in the answer can be followed for managing compute instances.",
        "Answer_gpt_summary":"answer suggest share notebook data scientist develop need access workspac share file share notebook access addition notebook requir comput instanc manag virtual machin exist azur link provid answer follow manag comput instanc"
    },
    {
        "Question_id":null,
        "Question_title":"a request to change the personal account (MSA) associated with my Certification profile",
        "Question_body":"I want to connect my MSA with my certification profile.\n\nMy MSA is julia.loef@hotmail.com and my work account is julia.loef-bleiksch@nl.abnamro.com\n\nCan you please help me with this?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661434194677,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi @JuliaLoefBleiksch-4662\n\nPlease post this on Microsoft Certifications forum found at https:\/\/trainingsupport.microsoft.com\/en-us\/mcp and someone will gladly assist.\n\nUnfortunately MS Certifications is not supported on this forum.\n\nIf this was helpful please accept answer.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/981835\/a-request-to-change-the-personal-account-msa-assoc.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-25T13:50:43.443Z",
                "Answer_score":0,
                "Answer_body":"Hi @JuliaLoefBleiksch-4662\n\nPlease post this on Microsoft Certifications forum found at https:\/\/trainingsupport.microsoft.com\/en-us\/mcp and someone will gladly assist.\n\nUnfortunately MS Certifications is not supported on this forum.\n\nIf this was helpful please accept answer.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1661435443443,
        "Question_original_content":"request chang person account msa associ certif profil want connect msa certif profil msa julia loef hotmail com work account julia loef bleiksch abnamro com help",
        "Question_preprocessed_content":"request chang person account associ certif profil want connect msa certif profil msa work account help",
        "Question_gpt_summary_original":"The user is requesting assistance in changing the personal account associated with their certification profile to their Microsoft account (MSA) from their work account.",
        "Question_gpt_summary":"user request assist chang person account associ certif profil microsoft account msa work account",
        "Answer_original_content":"julialoefbleiksch post microsoft certif forum http trainingsupport microsoft com mcp gladli assist unfortun certif support forum help accept answer",
        "Answer_preprocessed_content":"post microsoft certif forum gladli assist unfortun certif support forum help accept answer",
        "Answer_gpt_summary_original":"the solution to the user's request to change their personal account associated with their certification profile is to post the request on the microsoft certifications forum found at https:\/\/trainingsupport.microsoft.com\/en-us\/mcp. someone will assist them there as ms certifications are not supported on the current forum.",
        "Answer_gpt_summary":"solut user request chang person account associ certif profil post request microsoft certif forum http trainingsupport microsoft com mcp assist certif support current forum"
    },
    {
        "Question_id":null,
        "Question_title":"Two Class Decision Forest - Evaluate Results with Own Test Dataset?",
        "Question_body":"Hi - can you train the Azure Two Class Decision Tree forest with a training data set you upload, then test it with a different data set that you upload rather than letting Azure do the random splitting into train and test?\n\nThank you!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659565398167,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/954252\/two-class-decision-forest-evaluate-results-with-ow.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-04T16:23:52.697Z",
                "Answer_score":0,
                "Answer_body":"@DunleavyLisa-8881 Thanks for the question. Yes, you can do the two-class decision forest using the azure ML SDK.\n\nHere is the document to upload and train data.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"class decis forest evalu result test dataset train azur class decis tree forest train data set upload test differ data set upload let azur random split train test thank",
        "Question_preprocessed_content":"class decis forest evalu result test dataset train azur class decis tree forest train data set upload test differ data set upload let azur random split train test thank",
        "Question_gpt_summary_original":"The user is facing a challenge in evaluating the results of the Azure Two Class Decision Tree forest with their own test dataset. They want to know if it is possible to train the model with their own uploaded training dataset and test it with a different uploaded dataset instead of letting Azure randomly split the data into train and test sets.",
        "Question_gpt_summary":"user face challeng evalu result azur class decis tree forest test dataset want know possibl train model upload train dataset test differ upload dataset instead let azur randomli split data train test set",
        "Answer_original_content":"dunleavylisa thank question ye class decis forest sdk document upload train data",
        "Answer_preprocessed_content":"thank question ye decis forest sdk document upload train data",
        "Answer_gpt_summary_original":"possible solutions: \n- the user can train an azure two class decision tree forest with a training dataset they upload using the sdk.\n- the user can test the trained model with a different dataset they upload.",
        "Answer_gpt_summary":"possibl solut user train azur class decis tree forest train dataset upload sdk user test train model differ dataset upload"
    },
    {
        "Question_id":null,
        "Question_title":"request for help in loading the latest version of model from mlflow",
        "Question_body":"Hello everyone , can someone help me how can i load the latest version of my model automatically in databricks ?",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1622224745000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":14.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/n3WYjzCufuY",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-28T18:47:07",
                "Answer_body":"You can load it via model URI, e.g.\u00a0\n\n\nmy_model = mlflow.pyfunc.load_model(\"models:\/mymodel\/Production\")\n\n\nTomas\n\n\n\ue5d3"
            },
            {
                "Answer_creation_time":"2021-05-28T19:03:30",
                "Answer_body":"how can i specify the latest version ?\n\ue5d3"
            },
            {
                "Answer_creation_time":"2021-05-28T19:15:07",
                "Answer_body":"mlflow.pyfunc.load_model(\"models:\/mymodel\/versionX\n\n\nTo get that latest version, use API\u00a0\n\n\nhttps:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.get_latest_versions\n\n\n\n\nCheers\u00a0\nJules\u00a0\n\u2014\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn May 28, 2021, at 4:03 PM, nadine ben harrath <nadinebe...@gmail.com> wrote:\n\n\n\ufeffhow can i specify the latest version ?\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/57be45e8-d140-4f08-b979-6ab995a977f1n%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"request help load latest version model hello help load latest version model automat databrick",
        "Question_preprocessed_content":"request help load latest version model hello help load latest version model automat databrick",
        "Question_gpt_summary_original":"The user is seeking assistance in automatically loading the latest version of their model from mlflow in Databricks.",
        "Question_gpt_summary":"user seek assist automat load latest version model databrick",
        "Answer_original_content":"load model uri model pyfunc load model model mymodel product toma specifi latest version pyfunc load model model mymodel versionx latest version us api http org doc latest python api track html track client latest version cheer jule sent iphon pardon dumb thumb typo nadin ben harrath wrote specifi latest version receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user bee abafn googlegroup com",
        "Answer_preprocessed_content":"load model uri toma specifi latest version latest version us api cheer jule sent iphon pardon dumb thumb typo nadin ben harrath wrote specifi latest version receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Solution: The user can load the latest version of the model by using the API \"mlflow.tracking.MlflowClient.get_latest_versions\". They can specify the latest version using the model URI, e.g., \"mlflow.pyfunc.load_model(\"models:\/mymodel\/versionX\")\".",
        "Answer_gpt_summary":"solut user load latest version model api track client latest version specifi latest version model uri pyfunc load model model mymodel versionx"
    },
    {
        "Question_id":null,
        "Question_title":"Use a trained model for use cases (a new data set)",
        "Question_body":"Dear all\n\nI conducted some experiments with Microsoft Azure automated ML and DESIGNER.\n\nAs far as I understand the given results, the trained model shows e.g. the accuracy? How well or in how many cases can the trained model predict the value (e.g. TRUE or FALSE) correctly?\n\nNow, I want to use the \"trained\" model(s) for use cases. My goal is to use the trained model(s) and provide predictions for new samples (a new data set). E.g. I want to predict the value \"TRUE\" or \"FALSE\" for the values of the new data set.\n\nIn my case, there is no value in the column (TRUE or FALSE). I want the model to provide me with the answers.\n\nNext steps: As far as I see, I need to deploy the model so that I can conduct the same experiments with new samples?\n\nOr how can I apply my trained model for the new use cases? (please see my description above)\n\nThank you for your feedback\n\nBest regards\n\nLukas",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1665310129110,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1040790\/use-a-trained-model-for-use-cases-a-new-data-set.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-10T14:14:26.413Z",
                "Answer_score":0,
                "Answer_body":"@LukasBusers-1078 After you create a deployment, you can score it as described in Test the endpoint with sample data.\nAre you facing any error, if yes please add the error details.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-26T14:46:01.66Z",
                "Answer_score":0,
                "Answer_body":"Tutorial Overview\nThis tutorial is divided into three parts; they are:\n\nPrepare a Training Dataset\nHow to Fit a Model on the Training Dataset\nHow to Connect Predictions With Inputs to the Model\n2. Prepare a Training Dataset\nLet\u2019s start off by defining a dataset that we can use with our model.\n\nYou may have your own dataset in a CSV file or in a NumPy array in memory.\n\nIn this case, we will use a simple two-class or binary classification problem with two numerical input variables.\n\nInputs: Two numerical input variables:\nOutputs: A class label as either a 0 or 1.\nWe can use the make_blobs() scikit-learn function to create this dataset with 1,000 examples.\n\nThe example below creates the dataset with separate arrays for the input (X) and outputs (y).",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us train model us case new data set dear conduct experi microsoft azur autom design far understand given result train model show accuraci case train model predict valu true fals correctli want us train model us case goal us train model provid predict new sampl new data set want predict valu true fals valu new data set case valu column true fals want model provid answer step far need deploi model conduct experi new sampl appli train model new us case descript thank feedback best regard luka",
        "Question_preprocessed_content":"us train model us case dear conduct experi microsoft azur autom design far understand given result train model show accuraci case train model predict valu correctli want us train model us case goal us train model provid predict new sampl want predict valu true fals valu new data set case valu column want model provid answer step far need deploi model conduct experi new sampl appli train model new us case thank feedback best regard luka",
        "Question_gpt_summary_original":"The user has conducted experiments with Microsoft Azure automated ML and DESIGNER and has trained a model to predict values accurately. The user now wants to use the trained model for new use cases and provide predictions for a new data set. However, the user is unsure how to apply the trained model for the new use cases and is seeking feedback on how to deploy the model to conduct experiments with new samples.",
        "Question_gpt_summary":"user conduct experi microsoft azur autom design train model predict valu accur user want us train model new us case provid predict new data set user unsur appli train model new us case seek feedback deploi model conduct experi new sampl",
        "Answer_original_content":"lukasbus creat deploy score describ test endpoint sampl data face error ye add error detail tutori overview tutori divid part prepar train dataset fit model train dataset connect predict input model prepar train dataset let start defin dataset us model dataset csv file numpi arrai memori case us simpl class binari classif problem numer input variabl input numer input variabl output class label us blob scikit learn function creat dataset exampl exampl creat dataset separ arrai input output",
        "Answer_preprocessed_content":"creat deploy score describ test endpoint sampl data face error ye add error detail tutori overview tutori divid part prepar train dataset fit model train dataset connect predict input model prepar train dataset let start defin dataset us model dataset csv file numpi arrai memori case us simpl binari classif problem numer input variabl input numer input variabl output class label us function creat dataset exampl exampl creat dataset separ arrai input output",
        "Answer_gpt_summary_original":"possible solutions to use a trained model to provide predictions for a new data set are to create a deployment and score it as described in the tutorial. the tutorial is divided into three parts: preparing a training dataset, fitting a model on the training dataset, and connecting predictions with inputs to the model. the example dataset used in the tutorial is a simple two-class or binary classification problem with two numerical input variables. the make_blobs() scikit-learn function can be used to create this dataset with 1,000 examples.",
        "Answer_gpt_summary":"possibl solut us train model provid predict new data set creat deploy score describ tutori tutori divid part prepar train dataset fit model train dataset connect predict input model exampl dataset tutori simpl class binari classif problem numer input variabl blob scikit learn function creat dataset exampl"
    },
    {
        "Question_id":null,
        "Question_title":"Select the metrics according to different step thresholds in the Runs Table",
        "Question_body":"<p>Take the following as an example. We train a model with different configurations  for 1000 steps. Then, the Runs Table only shows the final values of the logged metrics. In some cases, we might want to compare different runs within 500 steps. Can we add support for this functionality?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1668068789469,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/select-the-metrics-according-to-different-step-thresholds-in-the-runs-table\/3403",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-14T21:56:18.409Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lanlin\">@lanlin<\/a> thank you for your question! This is indeed not feasible today, and I can make a feature request if you\u2019d like. You could get the intermediate values for all Runs using <code>Weave<\/code> in your Workspace instead. An alternative option would be to download the metrics you\u2019re interested at and log a <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#update-metrics-for-a-run-after-the-run-has-finished\">new column using the API<\/a> for the specific step, then you could display this in the Runs Table section. Would any of these work for you? Let me know if you would like some example of these workarounds.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-15T02:13:10.866Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/thanos-wandb\">@thanos-wandb<\/a> thanks for your reply. For now, I use the API to upload a new column to the Table. But it would be better to allow some operations directly in the web UI.<\/p>",
                "Answer_score":10.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-21T22:02:49.855Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lanlin\">@lanlin<\/a> just an update on this, I have created a feature request for the engineering team as this would be indeed a very useful addition. Thanks for this suggestion, we will reach out to you here if there are any updates on this.<\/p>\n<p>In the meantime, another alternative would be to query these results from a Weave panel that you can add in your project\u2019s Workspace and provide an expression such as:<br>\n<code>runs.history.concat.filter((row) =&gt; row[\"epochs\"]==500)<\/code><br>\nI hope this helps, please let me know if you have any further questions.<\/p>",
                "Answer_score":10.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-20T22:03:15.380Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"select metric accord differ step threshold run tabl follow exampl train model differ configur step run tabl show final valu log metric case want compar differ run step add support function",
        "Question_preprocessed_content":"select metric accord differ step threshold run tabl follow exampl train model differ configur step run tabl show final valu log metric case want compar differ run step add support function",
        "Question_gpt_summary_original":"The user is facing a challenge with the Runs Table, which only displays the final values of logged metrics after training a model with different configurations for 1000 steps. The user wants to compare different runs within 500 steps and is seeking support for this functionality.",
        "Question_gpt_summary":"user face challeng run tabl displai final valu log metric train model differ configur step user want compar differ run step seek support function",
        "Answer_original_content":"lanlin thank question feasibl todai featur request youd like intermedi valu run weav workspac instead altern option download metric your interest log new column api specif step displai run tabl section work let know like exampl workaround thano thank repli us api upload new column tabl better allow oper directli web lanlin updat creat featur request engin team us addit thank suggest reach updat meantim altern queri result weav panel add project workspac provid express run histori concat filter row row epoch hope help let know question topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank question feasibl todai featur request youd like intermedi valu run workspac instead altern option download metric your interest log new column api specif step displai run tabl section work let know like exampl workaround thank repli us api upload new column tabl better allow oper directli web updat creat featur request engin team us addit thank suggest reach updat meantim altern queri result weav panel add project workspac provid express hope help let know question topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions to compare different runs within 500 steps in the runs table are to use weave in the workspace to get intermediate values for all runs or to download the metrics and log a new column using the api for the specific step. another alternative is to query the results from a weave panel that can be added in the project's workspace. the user has also requested a feature to allow these operations directly in the web ui, and a feature request has been created for the engineering team.",
        "Answer_gpt_summary":"possibl solut compar differ run step run tabl us weav workspac intermedi valu run download metric log new column api specif step altern queri result weav panel ad project workspac user request featur allow oper directli web featur request creat engin team"
    },
    {
        "Question_id":47460981.0,
        "Question_title":"Error when creating cluster environment for azureML: \"Failed to get scoring front-end info\"",
        "Question_body":"<p>I just started with using the Azure Machine Learning Services and ran into this problem. Creating a local environment and deploying my model to localhost works perfectly fine. \nCan anyone identify what could have caused this error, because i do not know where to start..<\/p>\n\n<p>I tried to create a cluster for Location \"eastus2\" aswell, which caused the same error.\nThank you very much in advance!<\/p>\n\n<p>Btw, the ressource group and ressources are being created into my azure account.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wYwJD.png\" rel=\"nofollow noreferrer\">Image of error<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1511458774783,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":381.0,
        "Answer_body":"<p>Ashvin [MSFT]<\/p>\n\n<p>Sorry to hear that you were facing issues. We checked logs on our side using the info you provided in the screenshot. The cluster setup failed because there weren't enough cores to fit AzureML and system components in the cluster. You specified agent-vm-size of D1v2 which has 1 CPU core. By default we create 2 agents so total cores were 2. To resolve, can you please try creating a new cluster without specifying agent size? Then AzureML will create 2 agents of D3v2 which is 8 cores total. This should fit the AzureML and system components and leave some room for you to deploy your services. <\/p>\n\n<p>If you wish a bigger cluster you could specify agent-count along with agent-vm-size to appropriately size your cluster but please have minimum total of 8 cores with each individual VM >= 2 cores to ensure cluster works smoothly. Hope this helps.<\/p>\n\n<p>We are working on our side to add error handling to ensure request fails with clear error message. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1511812588163,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47460981",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1511812251067,
        "Question_original_content":"error creat cluster environ fail score end info start servic ran problem creat local environ deploi model localhost work perfectli fine identifi caus error know start tri creat cluster locat eastu aswel caus error thank advanc btw ressourc group ressourc creat azur account imag error",
        "Question_preprocessed_content":"error creat cluster environ fail score info start servic ran problem creat local environ deploi model localhost work perfectli fine identifi caus error know tri creat cluster locat eastu aswel caus error thank advanc btw ressourc group ressourc creat azur account imag error",
        "Question_gpt_summary_original":"The user encountered an error while creating a cluster environment for Azure Machine Learning Services, specifically \"Failed to get scoring front-end info\". The user is unsure of the cause of the error and has attempted to create a cluster in a different location with the same result. The user confirms that the resource group and resources are being created in their Azure account.",
        "Question_gpt_summary":"user encount error creat cluster environ servic specif fail score end info user unsur caus error attempt creat cluster differ locat result user confirm resourc group resourc creat azur account",
        "Answer_original_content":"ashvin msft sorri hear face issu check log info provid screenshot cluster setup fail weren core fit compon cluster specifi agent size cpu core default creat agent total core resolv try creat new cluster specifi agent size creat agent core total fit compon leav room deploi servic wish bigger cluster specifi agent count agent size appropri size cluster minimum total core individu core ensur cluster work smoothli hope help work add error handl ensur request fail clear error messag",
        "Answer_preprocessed_content":"ashvin sorri hear face issu check log info provid screenshot cluster setup fail weren core fit compon cluster specifi cpu core default creat agent total core resolv try creat new cluster specifi agent size creat agent core total fit compon leav room deploi servic wish bigger cluster specifi appropri size cluster minimum total core individu core ensur cluster work smoothli hope help work add error handl ensur request fail clear error messag",
        "Answer_gpt_summary_original":"possible solutions to the error encountered when creating a cluster environment for a model include creating a new cluster without specifying agent size, creating 2 agents of d3v2 which is 8 cores total, and specifying agent-count along with agent-vm-size to appropriately size the cluster with a minimum total of 8 cores and each individual vm >= 2 cores to ensure the cluster works smoothly. the answer also mentions that the team is working on adding error handling to ensure requests fail with clear error messages.",
        "Answer_gpt_summary":"possibl solut error encount creat cluster environ model includ creat new cluster specifi agent size creat agent core total specifi agent count agent size appropri size cluster minimum total core individu core ensur cluster work smoothli answer mention team work ad error handl ensur request fail clear error messag"
    },
    {
        "Question_id":40798184.0,
        "Question_title":"Azure ML Batch Run - Single Output",
        "Question_body":"<p>I create an forecasting experiment using R engine. My data source is pivoted, hence I need to pass row by row.\nThe output works great with single row prediction. But when I try to populate multiple lines, it still gives single row output - for the first record only.<\/p>\n\n<p>I'm trying to loop my result as follows :<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\nlibrary(forecast)\nlibrary(reshape)\nlibrary(dplyr)\nlibrary(zoo)\n#exclude non required columns\nmy.ds &lt;- dataset1[, -c(4,5,6)]\n# set the CIs we want to use here, so we can reuse this vector\ncis &lt;- c(80, 95)\n\nfor (i in 1:nrow(my.ds)) {\nmy.start &lt;- my.ds[i,c(3)]\nmy.product &lt;- my.ds[i, \"Product\"]\nmy.location &lt;- my.ds[i, \"Location\"]\nmy.result &lt;- melt(my.ds[i,], id = c(\"Product\",\"Location\"))\nmy.ts &lt;- ts(my.result$value, frequency=52, start=c(my.start,1))\n# generate the forecast using those ci levels\nf &lt;- forecast(na.interp(my.ts), h=52, level=cis)\n# make a data frame containing the forecast information, including the index\nz &lt;- as.data.frame(cbind(seq(1:52),\n                       f$mean,\n                       Reduce(cbind, lapply(seq_along(cis), function(i) cbind(f$lower[,i], f$upper[,i])))))\n# give the columns better names\nnames(z) &lt;- c(\"index\", \"mean\", paste(rep(c(\"lower\", \"upper\"), times = length(cis)), rep(cis, each = 2), sep = \".\"))\n# manipulate the results as you describe\nzw &lt;- z %&gt;%\n# keep only the variable you want and its index\nmutate(sssf = upper.95 - mean) %&gt;%\nselect(index, mean, sssf) %&gt;%\n# add product and location info\nmutate(product = my.product,\n       location = my.location) %&gt;%\n# rearrange columns so it's easier to read\nselect(product, location, index, mean, sssf)\nzw &lt;- melt(zw, id.vars = c(\"product\", \"location\", \"index\"), measure.vars = c(\"mean\",\"sssf\"))\ndata.set &lt;- cast(zw, product + location ~ index + variable, value = \"value\")\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data.set\");\n}\n<\/code><\/pre>\n\n<p>This is design of my experiment :\n<a href=\"https:\/\/i.stack.imgur.com\/6lYd1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6lYd1.png\" alt=\"experiment\"><\/a><\/p>\n\n<p>And this is how sample <a href=\"https:\/\/www.dropbox.com\/s\/xgfc7pnyy29frid\/dhf-00009E850%20-%20Copy.csv?dl=0\" rel=\"nofollow noreferrer\" title=\"input file\">input<\/a> looks like :<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/QlRiE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/QlRiE.png\" alt=\"Sample input\"><\/a><\/p>\n\n<p>I'm testing using the Excel test workbook downloaded from experiment site.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1480051198437,
        "Question_favorite_count":null,
        "Question_last_edit_time":1480086026020,
        "Question_score":2.0,
        "Question_view_count":195.0,
        "Answer_body":"<p>I figured out the problem :<\/p>\n\n<pre><code>{\n...\nds &lt;- cast(zw, product + location ~ index + variable, value = \"value\")\ndata.set &lt;- rbind(data.set, ds)\n}\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data.set\");\n<\/code><\/pre>\n\n<p>I should be merging the rows and then output outside of the loop.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40798184",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1480086125487,
        "Question_original_content":"batch run singl output creat forecast experi engin data sourc pivot need pass row row output work great singl row predict try popul multipl line give singl row output record try loop result follow map base option input port variabl dataset variabl want index mutat upper mean select index mean add product locat info mutat product product locat locat rearrang column easier read select product locat index mean melt var product locat index measur var mean data set cast product locat index variabl valu valu select data frame sent output dataset port maml mapoutputport data set design experi sampl input look like test excel test workbook download experi site",
        "Question_preprocessed_content":"batch run singl output creat forecast experi engin data sourc pivot need pass row row output work great singl row predict try popul multipl line give singl row output record try loop result follow design experi sampl input look like test excel test workbook download experi site",
        "Question_gpt_summary_original":"The user is facing a challenge with Azure ML Batch Run - Single Output. They have created a forecasting experiment using R engine with a pivoted data source that needs to be passed row by row. The output works well with single row prediction, but when the user tries to populate multiple lines, it still gives a single row output for the first record only. The user is trying to loop their result, but it is not working as expected. They have shared the design of their experiment and a sample input file for reference.",
        "Question_gpt_summary":"user face challeng batch run singl output creat forecast experi engin pivot data sourc need pass row row output work singl row predict user tri popul multipl line give singl row output record user try loop result work expect share design experi sampl input file refer",
        "Answer_original_content":"figur problem cast product locat index variabl valu valu data set rbind data set select data frame sent output dataset port maml mapoutputport data set merg row output outsid loop",
        "Answer_preprocessed_content":"figur problem merg row output outsid loop",
        "Answer_gpt_summary_original":"the solution to the challenge with the forecasting experiment using the r engine is to merge the rows and then output outside of the loop. the code provided in the answer shows how to do this by using the \"rbind\" function to combine the rows and then sending the resulting data frame to the output dataset port.",
        "Answer_gpt_summary":"solut challeng forecast experi engin merg row output outsid loop code provid answer show rbind function combin row send result data frame output dataset port"
    },
    {
        "Question_id":null,
        "Question_title":"YOLOv5 sweeps?",
        "Question_body":"<p>Does anyone know if it\u2019s possible to use YOLOv5 train.py with sweeps?<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1641696247744,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":227.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/yolov5-sweeps\/1735",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-11T11:20:00.087Z",
                "Answer_body":"<p>Hey David, checking on this.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-11T13:03:22.331Z",
                "Answer_body":"<p>Thanks Arman!<\/p>\n<p>I\u2019ve been using YOLOv5 evolve, which is similar to HP sweeps.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-12T11:48:11.179Z",
                "Answer_body":"<p>Hey David, you can find instructions on how to use Sweeps with YOLOv5 <a href=\"https:\/\/github.com\/ultralytics\/yolov5\/issues\/607\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-12T13:56:35.985Z",
                "Answer_body":"<p>Thank you again Arman!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-13T13:57:34.280Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"yolov sweep know possibl us yolov train sweep thank",
        "Question_preprocessed_content":"yolov sweep know possibl us yolov sweep thank",
        "Question_gpt_summary_original":"The user is seeking information on whether it is possible to use YOLOv5 train.py with sweeps.",
        "Question_gpt_summary":"user seek inform possibl us yolov train sweep",
        "Answer_original_content":"hei david check thank arman iv yolov evolv similar sweep hei david instruct us sweep yolov thank arman topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei david check thank arman iv yolov evolv similar sweep hei david instruct us sweep yolov thank arman topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solution: the answer suggests that the user can use sweeps with yolov5 by following the instructions provided in a link. the answer also mentions that the user can use yolov5 evolve, which is similar to hp sweeps.",
        "Answer_gpt_summary":"possibl solut answer suggest user us sweep yolov follow instruct provid link answer mention user us yolov evolv similar sweep"
    },
    {
        "Question_id":70975320.0,
        "Question_title":"EventBridge trigger: Sagemaker Processing Job finished",
        "Question_body":"<p>I'm currently developing some ETL for my ML model with AWS. The thing is that I want to <strong>trigger<\/strong> a Lambda when some Sagemaker Processing Job is finished. And the <strong>event<\/strong> passed to the Lambda, should be the configuration info (job name, arguments, etc..) of the Sagemaker Processing Job.<\/p>\n<p><strong>Q1<\/strong>: How can I do to <em>trigger the event<\/em> when the Processing Job is finished?<\/p>\n<p><strong>Q2<\/strong>: How can I do to pass the <em>Processing Job configurations as an event<\/em> for the Lambda?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643907548550,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1643931586140,
        "Question_score":3.0,
        "Question_view_count":504.0,
        "Answer_body":"<p>You can use the following EventBridge rule pattern:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [&quot;aws.sagemaker&quot;],\n  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;]\n  }\n}\n<\/code><\/pre>\n<p>The ProcessingJobStatus list can be modified based on which statuses you want to handle.<\/p>\n<p>You can set a Lambda function as the target of your EventBridge rule.<\/p>\n<p>Here is a sample event which will be passed to your Lambda, taken from AWS console:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;version&quot;: &quot;0&quot;,\n  &quot;id&quot;: &quot;0a15f67d-aa23-0123-0123-01a23w89r01t&quot;,\n  &quot;detail-type&quot;: &quot;SageMaker Processing Job State Change&quot;,\n  &quot;source&quot;: &quot;aws.sagemaker&quot;,\n  &quot;account&quot;: &quot;123456789012&quot;,\n  &quot;time&quot;: &quot;2019-05-31T21:49:54Z&quot;,\n  &quot;region&quot;: &quot;us-east-1&quot;,\n  &quot;resources&quot;: [&quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job\/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingInputs&quot;: [{\n      &quot;InputName&quot;: &quot;InputName&quot;,\n      &quot;S3Input&quot;: {\n        &quot;S3Uri&quot;: &quot;s3:\/\/input\/s3\/uri&quot;,\n        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/local\/path&quot;,\n        &quot;S3DataType&quot;: &quot;MANIFEST_FILE&quot;,\n        &quot;S3InputMode&quot;: &quot;PIPE&quot;,\n        &quot;S3DataDistributionType&quot;: &quot;FULLYREPLICATED&quot;\n      }\n    }],\n    &quot;ProcessingOutputConfig&quot;: {\n      &quot;Outputs&quot;: [{\n        &quot;OutputName&quot;: &quot;OutputName&quot;,\n        &quot;S3Output&quot;: {\n          &quot;S3Uri&quot;: &quot;s3:\/\/output\/s3\/uri&quot;,\n          &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/output\/local\/path&quot;,\n          &quot;S3UploadMode&quot;: &quot;CONTINUOUS&quot;\n        }\n      }],\n      &quot;KmsKeyId&quot;: &quot;KmsKeyId&quot;\n    },\n    &quot;ProcessingJobName&quot;: &quot;integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,\n    &quot;ProcessingResources&quot;: {\n      &quot;ClusterConfig&quot;: {\n        &quot;InstanceCount&quot;: 3,\n        &quot;InstanceType&quot;: &quot;ml.c5.xlarge&quot;,\n        &quot;VolumeSizeInGB&quot;: 5,\n        &quot;VolumeKmsKeyId&quot;: &quot;VolumeKmsKeyId&quot;\n      }\n    },\n    &quot;StoppingCondition&quot;: {\n      &quot;MaxRuntimeInSeconds&quot;: 2000\n    },\n    &quot;AppSpecification&quot;: {\n      &quot;ImageUri&quot;: &quot;012345678901.dkr.ecr.us-west-2.amazonaws.com\/processing-uri:latest&quot;\n    },\n    &quot;NetworkConfig&quot;: {\n      &quot;EnableInterContainerTrafficEncryption&quot;: true,\n      &quot;EnableNetworkIsolation&quot;: false,\n      &quot;VpcConfig&quot;: {\n        &quot;SecurityGroupIds&quot;: [&quot;SecurityGroupId1&quot;, &quot;SecurityGroupId2&quot;, &quot;SecurityGroupId3&quot;],\n        &quot;Subnets&quot;: [&quot;Subnet1&quot;, &quot;Subnet2&quot;]\n      }\n    },\n    &quot;RoleArn&quot;: &quot;arn:aws:iam::012345678987:role\/SageMakerPowerUser&quot;,\n    &quot;ExperimentConfig&quot;: {},\n    &quot;ProcessingJobArn&quot;: &quot;arn:aws:sagemaker:us-west-2:012345678987:processing-job\/integ-test-analytics-algo-54ee3282-5899-4aa3-afc2-7ce1d02&quot;,\n    &quot;ProcessingJobStatus&quot;: &quot;Completed&quot;,\n    &quot;LastModifiedTime&quot;: 1589879735000,\n    &quot;CreationTime&quot;: 1589879735000\n  }\n}\n<\/code><\/pre>\n<p><strong>Edit:<\/strong><\/p>\n<p>If you want to match a ProcessingJobName with specific prefix:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [&quot;aws.sagemaker&quot;],\n  &quot;detail-type&quot;: [&quot;SageMaker Processing Job State Change&quot;],\n  &quot;detail&quot;: {\n    &quot;ProcessingJobStatus&quot;: [&quot;Failed&quot;, &quot;Completed&quot;, &quot;Stopped&quot;],\n    &quot;ProcessingJobName&quot;: [{\n      &quot;prefix&quot;: &quot;standarize-data&quot;\n    }]\n  }\n}\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1643920466910,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70975320",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643913781760,
        "Question_original_content":"eventbridg trigger process job finish current develop etl model aw thing want trigger lambda process job finish event pass lambda configur info job argument process job trigger event process job finish pass process job configur event lambda",
        "Question_preprocessed_content":"eventbridg trigger process job finish current develop etl model aw thing want trigger lambda process job finish event pass lambda configur info process job trigger event process job finish pass process job configur event lambda",
        "Question_gpt_summary_original":"The user is facing two challenges related to AWS ETL for their ML model. Firstly, they want to trigger a Lambda function when a Sagemaker Processing Job is finished. Secondly, they want to pass the configuration information of the Sagemaker Processing Job as an event to the Lambda function.",
        "Question_gpt_summary":"user face challeng relat aw etl model firstli want trigger lambda function process job finish secondli want pass configur inform process job event lambda function",
        "Answer_original_content":"us follow eventbridg rule pattern sourc aw type process job state chang processingjobstatu fail complet stop processingjobstatu list modifi base status want handl set lambda function target eventbridg rule sampl event pass lambda taken aw consol version afd awrt type process job state chang sourc aw account time region east resourc arn aw west process job integ test analyt algo afc ced processinginput inputnam inputnam sinput suri input uri localpath opt process input local path sdatatyp manifest file sinputmod pipe sdatadistributiontyp fullyrepl processingoutputconfig output outputnam outputnam soutput suri output uri localpath opt process output local path suploadmod continu kmskeyid kmskeyid processingjobnam integ test analyt algo afc ced processingresourc clusterconfig instancecount instancetyp xlarg volumesizeingb volumekmskeyid volumekmskeyid stoppingcondit maxruntimeinsecond appspecif imageuri dkr ecr west amazonaw com process uri latest networkconfig enableintercontainertrafficencrypt true enablenetworkisol fals vpcconfig securitygroupid securitygroupid securitygroupid securitygroupid subnet subnet subnet rolearn arn aw iam role powerus experimentconfig processingjobarn arn aw west process job integ test analyt algo afc ced processingjobstatu complet lastmodifiedtim creationtim edit want match processingjobnam specif prefix sourc aw type process job state chang processingjobstatu fail complet stop processingjobnam prefix standar data",
        "Answer_preprocessed_content":"us follow eventbridg rule pattern processingjobstatu list modifi base status want handl set lambda function target eventbridg rule sampl event pass lambda taken aw consol edit want match processingjobnam specif prefix",
        "Answer_gpt_summary_original":"the solution to triggering an event when a processing job is finished and passing the processing job configurations as an event for the lambda is to use an eventbridge rule pattern. the processingjobstatus list can be modified based on which statuses you want to handle. you can set a lambda function as the target of your eventbridge rule. a sample event will be passed to your lambda, taken from aws console. if you want to match a processingjobname with a specific prefix, you can modify the eventbridge rule pattern.",
        "Answer_gpt_summary":"solut trigger event process job finish pass process job configur event lambda us eventbridg rule pattern processingjobstatu list modifi base status want handl set lambda function target eventbridg rule sampl event pass lambda taken aw consol want match processingjobnam specif prefix modifi eventbridg rule pattern"
    },
    {
        "Question_id":69685819.0,
        "Question_title":"Taking Json type as parameter for cloudformation template",
        "Question_body":"<p>I am trying to take the environment variables as parameters for the template:\n<a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html<\/a><\/p>\n<p>The type seems to be Json in the template and I dont understand how to populate it.<\/p>\n<p>It seems like I can define this if i hardcode environment variables as below:<\/p>\n<pre><code>Resources:\n  SageMakerModel:\n    Type: 'AWS::SageMaker::Model'\n    Properties:\n      ExecutionRoleArn: \n        Ref: ExecutionRoleArn\n      EnableNetworkIsolation: false\n      PrimaryContainer:\n        Environment:\n          REQUEST_KEEP_ALIVE_TIME_SEC: '90'\n        Image: \n          Ref: ImageURI\n<\/code><\/pre>\n<p>However, there doesnt seem to be a way pass this in ? Anyone figured this out or any recommended way to do this ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1634971849693,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":415.0,
        "Answer_body":"<p>I was able to get this to work by using AWS:Include and Fn:Transform and storing my environment variables as json in passed s3 file.<\/p>\n<p>My cfn template looks like:<\/p>\n<pre><code>Resources:\n  SageMakerModel:\n    Type: 'AWS::SageMaker::Model'\n    Properties:\n      ExecutionRoleArn: \n        Ref: ExecutionRoleArn\n      EnableNetworkIsolation: false\n      PrimaryContainer:\n        Environment:\n          Fn::Transform:\n            Name: AWS::Include\n            Parameters:\n              Location: &lt;your S3 file&gt;\n        Image: \n          Ref: ImageURI\n<\/code><\/pre>\n<p>My s3 file looks like:<\/p>\n<pre><code>{\n  &quot;REQUEST_KEEP_ALIVE_TIME_SEC&quot;: &quot;90&quot;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69685819",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1635277709907,
        "Question_original_content":"take json type paramet cloudform templat try environ variabl paramet templat http doc aw amazon com awscloudform latest userguid aw properti model containerdefinit html type json templat dont understand popul like defin hardcod environ variabl resourc model type aw model properti executionrolearn ref executionrolearn enablenetworkisol fals primarycontain environ request aliv time sec imag ref imageuri doesnt wai pass figur recommend wai",
        "Question_preprocessed_content":"take json type paramet cloudform templat try environ variabl paramet templat type json templat dont understand popul like defin hardcod environ variabl doesnt wai pass figur recommend wai",
        "Question_gpt_summary_original":"The user is facing challenges in taking environment variables as parameters for a cloudformation template. The type of the parameter is Json, and the user is unsure of how to populate it. The user has tried hardcoding the environment variables, but is looking for a recommended way to pass them in.",
        "Question_gpt_summary":"user face challeng take environ variabl paramet cloudform templat type paramet json user unsur popul user tri hardcod environ variabl look recommend wai pass",
        "Answer_original_content":"abl work aw includ transform store environ variabl json pass file cfn templat look like resourc model type aw model properti executionrolearn ref executionrolearn enablenetworkisol fals primarycontain environ transform aw includ paramet locat imag ref imageuri file look like request aliv time sec",
        "Answer_preprocessed_content":"abl work aw includ transform store environ variabl json pass file cfn templat look like file look like",
        "Answer_gpt_summary_original":"the solution to the challenge of taking a json type as a parameter for a cloudformation template is to use aws:include and fn:transform. the environment variables should be stored as json in a passed s3 file. the cloudformation template should include resources with properties that reference the execution role arn, image uri, and the json file location. the json file should contain the environment variables in the required format.",
        "Answer_gpt_summary":"solut challeng take json type paramet cloudform templat us aw includ transform environ variabl store json pass file cloudform templat includ resourc properti refer execut role arn imag uri json file locat json file contain environ variabl requir format"
    },
    {
        "Question_id":null,
        "Question_title":"MLflow 0.9.0 Released!",
        "Question_body":"MLflow 0.9.0 has been released!\n\n\nMLflow 0.9.0 introduces several major features:\nSupport for running MLflow Projects in Docker containers.\nDatabase stores for the MLflow Tracking Server.\nSimplified custom Python model packaging.\nPlugin systems allowing third party libraries to extend MLflow functionality.\nSupport for HTTP authentication to the Tracking Server in the R client.\nAnd a few breaking changes:\n[Scoring] The pyfunc scoring server now expects requests with the application\/json content type to contain json-serialized pandas dataframes in the split format, rather than the records format. Also, when reading the pandas dataframes from JSON, the scoring server no longer automatically infers data types as it can result in unintentional conversion of data types.\n[API] Removed GetMetric & GetParam from the REST API as they are subsumed by GetRun.\nFor a comprehensive list of features, see the release change log , and check out the latest documentation on mlflow.org .",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1553788560000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":21.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/nmStGjJT720",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-03-28T16:24:33",
                "Answer_body":"And the MLflow v0.9.0 release blog is published too.\n\n\nCheers\nJules\n\n\n\n\n--\u00a0\n\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nApache Spark Developer & Community Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\n\n\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/d713c0ec-8383-4e86-883d-7759b80ef64f%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-03-28T16:37:14",
                "Answer_body":"Gooooood new guys...\u00a0\n\nEnvoy\u00e9 depuis mon t\u00e9l\u00e9phone Orange\n\n\n-------- Message original --------\nObjet\u00a0: Re: MLflow 0.9.0 Released!\nDe\u00a0: Jules Damji\n\u00c0\u00a0: Sue Ann Hong\nCc\u00a0: mlflow-users\n\n\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAG0FyUbYV0W0LJ-qw5AZA0oY3%3DkeC9EGKbgkwE5st25GRXo0kg%40mail.gmail.com.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"releas releas introduc major featur support run project docker contain databas store track server simplifi custom python model packag plugin system allow parti librari extend function support http authent track server client break chang score pyfunc score server expect request applic json content type contain json serial panda datafram split format record format read panda datafram json score server longer automat infer data type result unintent convers data type api remov getmetr getparam rest api subsum getrun comprehens list featur releas chang log check latest document org",
        "Question_preprocessed_content":"releas releas introduc major featur support run project docker contain databas store track server simplifi custom python model packag plugin system allow parti librari extend function support http authent track server client break chang score pyfunc score server expect request content type contain panda datafram split format record format read panda datafram json score server longer automat infer data type result unintent convers data type api remov getmetr getparam rest api subsum getrun comprehens list featur releas chang log check latest document org",
        "Question_gpt_summary_original":"The user is facing challenges related to the changes introduced in MLflow 0.9.0, which include support for running MLflow Projects in Docker containers, database stores for the MLflow Tracking Server, simplified custom Python model packaging, plugin systems allowing third party libraries to extend MLflow functionality, and support for HTTP authentication to the Tracking Server in the R client. Additionally, there are some breaking changes related to the pyfunc scoring server and the REST API, which may require the user to update their code accordingly.",
        "Question_gpt_summary":"user face challeng relat chang introduc includ support run project docker contain databas store track server simplifi custom python model packag plugin system allow parti librari extend function support http authent track server client addition break chang relat pyfunc score server rest api requir user updat code accordingli",
        "Answer_original_content":"releas blog publish cheer jule best idea simpl jule damji apach spark develop commun advoc databrick databrick com receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user dcec beff googlegroup com option visit http group googl com optout new gui envoi depui mon tlphone orang messag origin objet releas jule damji sue ann hong user view discuss web visit http group googl com msgid user cagfyubyvwlj qwazaoi dkecegkbgkwestgrxokg mail gmail com",
        "Answer_preprocessed_content":"releas blog publish cheer jule best idea simpl jule damji apach spark develop commun advoc databrick receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit new envoi depui mon tlphone orang messag origin objet releas jule damji sue ann hong user view discuss web visit",
        "Answer_gpt_summary_original":"No solutions are mentioned in the discussion. The message is a notification about the release of MLflow v0.9.0.",
        "Answer_gpt_summary":"solut mention discuss messag notif releas"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to continue training with additional epochs? Also where can I find logs in local?",
        "Question_body":"<p>Dear wandb team,<\/p>\n<p>I currently start using pytorch-lightning combining with wandb, so I will use WandbLogger. (Due to link limits, I didn\u2019t put url for the documentation.)<\/p>\n<p>Suppose I had trained a model, let\u2019s say for 10 epochs. The <em>project<\/em> is <strong>wandb_toy<\/strong>,  and the <em>name or ID<\/em> is <strong>toy<\/strong>. After training, it will automatically create two folders under <code>.\/<\/code>, i.e., <code>.\/wandb<\/code> and <code>.\/wandb_toy<\/code>. I know that the checkpoints will be saved in <code>wandb_toy\/toy\/checkpoints<\/code>. Also there will be a new run folder in <code>.\/wandb<\/code>, let\u2019s say <code>.\/wandb\/run-20230101_102000<\/code>.<\/p>\n<p>Now if I want to continue the training from <em>epochs=10<\/em> to <em>epochs=20<\/em>, I know I can load the model state with adding <em>ckpt_path<\/em> during <em>trainer.fit()<\/em>, also update the config with <em>allow_val_change<\/em> setting to <em>True<\/em>. However, there is still a new additional run folder created, e.g., <code>.\/wandb\/run-20230202_104000<\/code>.<\/p>\n<hr>\n<p><strong>My questions are:<\/strong><\/p>\n<ul>\n<li>I want to keep saving or update things in the original run folder <code>.\/wandb\/run-20230101_102000<\/code>.<\/li>\n<li>Also is there any way to name the run folder, for example, change  <code>.\/wandb\/run-20230101_102000<\/code> to  <code>.\/wandb\/my_toy_run<\/code>. I have tried keywords like <em>dir<\/em> or <em>save_dir<\/em> already, but seems not right.<\/li>\n<li>If I delete the project in wandb ai (without delete folders in <code>.\/wandb<\/code>), then continue training from <em>epoch=10<\/em> to <em>epoch=20<\/em>, It will log only from epoch=10~20. Is there any way to still get the previous log from epoch=0~10? I have tried to look up <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/save-restore#examples-of-wandb.restore\"> Save &amp; Restore Files<\/a> or <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming\">Resume Runs<\/a>, but unfortunately I couldn\u2019t figure it out.<\/li>\n<li>If I want to see each epochs log (e.g., accuracy and loss) in  my local, which file should I look for?<\/li>\n<\/ul>\n<p>Thanks for reading and I apologize if I couldn\u2019t make things clear.<\/p>\n<p>Best wishes,<br>\nYian<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1673356093482,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-continue-training-with-additional-epochs-also-where-can-i-find-logs-in-local\/3667",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-11T14:23:51.279Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ntuyianchen\">@ntuyianchen<\/a>, thanks for writing in and for sharing this detailed explanation of your use-case! Answering your questions:<\/p>\n<ul>\n<li>This is not possible at the moment as every time you resume a run a new folder is created <code>run-date_time<\/code> (this name is not mutable either other than manually) although the run is the same. This is useful to be able to track easily the different processes.<\/li>\n<li>You cannot resume a run at a previous step, it will only be resumed from the last step. The intention of this is to avoid overwritting previous logged data.<\/li>\n<li>Log metrics are not saved locally (other than the last epoch in <code>files\/wandb-metadata.json<\/code>), but they are accesible through our public API by accesing <code>run.history()<\/code>. <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/run#history\">Here<\/a> you can have a look at the documentation on how to do this.<\/li>\n<\/ul>\n<p>Please let me know if these answers would be useful. Also, if you would like to have any of these features available, feel free to explain which ones and I will create a new request!<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-11T17:18:22.250Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/luis_bergua1\">@luis_bergua1<\/a> , thanks for the concrete reply! It\u2019s really helpful and now I can understand why and how wandb designed this way. The <code>run.history()<\/code> is really the thing I was looking for, thank you very much!<\/p>\n<p>Thanks to wandb team for making this awesome package and documentations. I have one thing that\u2019s irrelevant to our discussion. I found that <strong>the text format seemed to be wrong<\/strong> at <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/run?_gl=1*1culllr*_ga*MTkwNzMzMTI4LjE2NzI0NzI2OTM.*_ga_JH1SJHJQXJ*MTY3MzQ1NTQ2OS40OC4xLjE2NzM0NTY1MTYuNDkuMC4w#history\">here<\/a> (the table for <code>Arguments<\/code> and <code>Returns<\/code> don\u2019t break new lines correctly in my Mac Safari, which makes it a little hard to read.) This  is not really an immediate big deal and I think it can be fixed in the future some day <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> .<\/p>\n<p>Cheers!<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f.png\" data-download-href=\"\/uploads\/short-url\/ckbG7FMxBS7IwaZs55NAnQ9Y8PB.png?dl=1\" title=\"\u622a\u5716 2023-01-12 \u4e0a\u53481.15.42\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f_2_581x499.png\" alt=\"\u622a\u5716 2023-01-12 \u4e0a\u53481.15.42\" data-base62-sha1=\"ckbG7FMxBS7IwaZs55NAnQ9Y8PB\" width=\"581\" height=\"499\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f_2_581x499.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f_2_871x748.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/5\/56624e0424bcaa43a1d179393b9d14a71de4878f_2_1162x998.png 2x\" data-dominant-color=\"F7F8F9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">\u622a\u5716 2023-01-12 \u4e0a\u53481.15.42<\/span><span class=\"informations\">1550\u00d71332 208 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-13T15:16:55.454Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ntuyianchen\">@ntuyianchen<\/a>, great to see that <code>run.history()<\/code> works for you! Thank you very much for the kind feedback, we really appreciate it! Regarding the table in the docs, I am not fully understanding you, would you like to have a different align foe the text\/have it justified?<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-15T15:16:44.781Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/luis_bergua1\">@luis_bergua1<\/a> , sorry for my ambiguous feedback. I was expecting that the argument should be something like<\/p>\n<blockquote>\n<p>samples (int, optional): The number \u2026<br>\npandas (bool, optional): Return \u2026<br>\nkeys (list optional): Only return \u2026<\/p>\n<\/blockquote>\n<p>Since without line-breaking, it\u2019s quite hard to find the argument at first sight.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-19T12:02:57.186Z",
                "Answer_body":"<p>Thanks a lot for clarifying <a class=\"mention\" href=\"\/u\/ntuyianchen\">@ntuyianchen<\/a>! I see the issue now, I\u2019ll report it!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"possibl continu train addit epoch log local dear team current start pytorch lightn combin us logger link limit didnt url document suppos train model let epoch project toi toi train automat creat folder toi know checkpoint save toi toi checkpoint new run folder let run want continu train epoch epoch know load model state ad ckpt path trainer fit updat config allow val chang set true new addit run folder creat run question want save updat thing origin run folder run wai run folder exampl chang run toi run tri keyword like dir save dir right delet project delet folder continu train epoch epoch log epoch wai previou log epoch tri look save restor file resum run unfortun figur want epoch log accuraci loss local file look thank read apolog thing clear best wish yian",
        "Question_preprocessed_content":"possibl continu train addit epoch log local dear team current start combin us logger suppos train model let epoch project toi train automat creat folder know checkpoint save new run folder let want continu train epoch epoch know load model state ad updat config set true new addit run folder creat question want save updat thing origin run folder wai run folder exampl chang tri keyword like dir right delet project continu train epoch epoch log epoch wai previou log epoch tri look save restor file resum run unfortun figur want epoch log local file look thank read apolog thing clear best wish yian",
        "Question_gpt_summary_original":"The user is facing several challenges related to using pytorch-lightning with WandbLogger. They want to continue training a model from epoch 10 to epoch 20 while saving or updating things in the original run folder, and also want to name the run folder. Additionally, they want to know if there is a way to retrieve previous logs from epoch 0 to 10 after deleting the project in Wandb AI. Finally, they are looking for the file that contains each epoch's log (e.g., accuracy and loss) in their local system.",
        "Question_gpt_summary":"user face challeng relat pytorch lightn logger want continu train model epoch epoch save updat thing origin run folder want run folder addition want know wai retriev previou log epoch delet project final look file contain epoch log accuraci loss local",
        "Answer_original_content":"ntuyianchen thank write share detail explan us case answer question possibl moment time resum run new folder creat run date time mutabl manual run us abl track easili differ process resum run previou step resum step intent avoid overwrit previou log data log metric save local epoch file metadata json acces public api acces run histori look document let know answer us like featur avail feel free explain on creat new request lui bergua thank concret repli help understand design wai run histori thing look thank thank team make awesom packag document thing that irrelev discuss text format wrong tabl argument return dont break new line correctli mac safari make littl hard read immedi big deal think fix futur dai cheer ntuyianchen great run histori work thank kind feedback appreci tabl doc fulli understand like differ align foe text justifi lui bergua sorri ambigu feedback expect argument like sampl int option number panda bool option return kei list option return line break hard argument sight thank lot clarifi ntuyianchen issu ill report",
        "Answer_preprocessed_content":"thank write share detail explan answer question possibl moment time resum run new folder creat run us abl track easili differ process resum run previou step resum step intent avoid overwrit previou log data log metric save local acces public api acces look document let know answer us like featur avail feel free explain on creat new request thank concret repli help understand design wai thing look thank thank team make awesom packag document thing that irrelev discuss text format wrong immedi big deal think fix futur dai cheer great work thank kind feedback appreci tabl doc fulli understand like differ align foe justifi sorri ambigu feedback expect argument like sampl number panda return kei return hard argument sight thank lot clarifi issu ill report",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- it is not possible to resume a run at a previous step, it will only be resumed from the last step.\n- log metrics are not saved locally, but they are accessible through the public api by accessing run.history().\n- if the user would like to have any of these features available, they can explain which ones and the team will create a new request.\n- the user can use run.history() to access logs.\n- the team will report the issue with the table in the documentation.",
        "Answer_gpt_summary":"possibl solut extract answer possibl resum run previou step resum step log metric save local access public api access run histori user like featur avail explain on team creat new request user us run histori access log team report issu tabl document"
    },
    {
        "Question_id":null,
        "Question_title":"Missing principal component analysis module in Azure ML Designer",
        "Question_body":"Hi, I cannot find the Principal Component module in Azure ml designer. For the classic ML studio version it used to be under the data transformation group of transformations but seems to be no longer there. Am I missing something? Thanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1614943157500,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/300776\/missing-principal-component-analysis-module-in-azu.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-05T19:36:26.153Z",
                "Answer_score":0,
                "Answer_body":"Hi, it is under Anomaly Detection drop down menu.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"miss princip compon analysi modul design princip compon modul design classic studio version data transform group transform longer miss thank",
        "Question_preprocessed_content":"miss princip compon analysi modul design princip compon modul design classic studio version data transform group transform longer miss thank",
        "Question_gpt_summary_original":"The user is facing a challenge in finding the Principal Component module in Azure ML Designer, which used to be under the data transformation group of transformations in the classic ML studio version. The user is seeking assistance in locating the module.",
        "Question_gpt_summary":"user face challeng find princip compon modul design data transform group transform classic studio version user seek assist locat modul",
        "Answer_original_content":"anomali detect drop menu",
        "Answer_preprocessed_content":"anomali detect drop menu",
        "Answer_gpt_summary_original":"the solution to locate the principal component analysis module in the designer version of ml studio is to look for it under the anomaly detection drop-down menu.",
        "Answer_gpt_summary":"solut locat princip compon analysi modul design version studio look anomali detect drop menu"
    },
    {
        "Question_id":null,
        "Question_title":"Loading a saved table to pandas dataframe",
        "Question_body":"<p>Hi, I have been recently using wandb a lot in my projects and it is really helpful.<\/p>\n<p>my issue is that I an trying access the logged tables as a pandas dataframe in a program. I check the documentation and tried the solution mentioned in the <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#access-tables-programmatically\">documentation here<\/a>.  Once I run this instead of getting the table it returns a dictionary like this<\/p>\n<pre><code class=\"lang-auto\">{'artifact_path': 'wandb-client-artifact:\/\/sx4urflmwtczzq7zf71hsfxkill8hqrnd8o4uirjbcswuuc29f0xxrq6nra7uo2kzsp8jmu4s2g53e7xl3xuyu4lfjiowz9v63r9fbn7d3r8ckmlz5lrkhncuyhr0e46:latest\/metrics.table.json', '_latest_artifact_path': 'wandb-client-artifact:\/\/sx4urflmwtczzq7zf71hsfxkill8hqrnd8o4uirjbcswuuc29f0xxrq6nra7uo2kzsp8jmu4s2g53e7xl3xuyu4lfjiowz9v63r9fbn7d3r8ckmlz5lrkhncuyhr0e46:latest\/metrics.table.json', 'path': 'media\/table\/metrics_2_491a3e34c6fcf4271cb2.table.json', 'size': 413, '_type': 'table-file', 'ncols': 9, 'nrows': 3, 'sha256': '491a3e34c6fcf4271cb2378f9a33ff5dc8c9cdb8268299b4f96b88151730ecad'}\n<\/code><\/pre>\n<p>It would be great if someone can help me to convert this to a table so that I can perform aggregations on the results.<\/p>\n<p>Thanks<br>\nPrateek<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1647037796200,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":383.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/loading-a-saved-table-to-pandas-dataframe\/2063",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-13T15:00:33.126Z",
                "Answer_body":"<p>Hey Prateek,<\/p>\n<p>you can use this function (its a modified version of a function from the wandb repo).<\/p>\n<pre><code class=\"lang-auto\">def get_table_data_from_url(source_url: str, api_key: Optional[str] = None) -&gt; None:\n    response = requests.get(source_url, auth=(\"api\", api_key), stream=True, timeout=5)\n    response.raise_for_status()\n    bytes_list = []\n    for data in response.iter_content(chunk_size=1024):\n        bytes_list.append(data)\n    final_byte_data = b\"\".join(bytes_list)\n    data_dict = json.loads(final_byte_data.decode(\"utf-8\"))\n    table_df = pd.DataFrame(data=data_dict[\"data\"], columns=data_dict[\"columns\"])\n    return table_df\n<\/code><\/pre>\n<p>To get the specific url of the artifact, you can just iterate over run.files() and save the url attribute of the returned files. By checking out the file names you should be able to see which files are relevant for you.<\/p>\n<p>Best<br>\nDarius<\/p>",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-12T15:01:00.509Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"load save tabl panda datafram recent lot project help issu try access log tabl panda datafram program check document tri solut mention document run instead get tabl return dictionari like artifact path client artifact sxurflmwtczzqzfhsfxkillhqrndouirjbcswuucfxxrqnrauokzspjmusgexlxuyulfjiowzvrfbndrckmlzlrkhncuyhr latest metric tabl json latest artifact path client artifact sxurflmwtczzqzfhsfxkillhqrndouirjbcswuucfxxrqnrauokzspjmusgexlxuyulfjiowzvrfbndrckmlzlrkhncuyhr latest metric tabl json path media tabl metric aecfcfcb tabl json size type tabl file ncol nrow sha aecfcfcbfaffdcccdbbfbecad great help convert tabl perform aggreg result thank prateek",
        "Question_preprocessed_content":"load save tabl panda datafram recent lot project help issu try access log tabl panda datafram program check document tri solut mention document run instead get tabl return dictionari like great help convert tabl perform aggreg result thank prateek",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing logged tables as a pandas dataframe in their program using wandb. Instead of getting the table, they receive a dictionary and are seeking help to convert it to a table to perform aggregations on the results.",
        "Question_gpt_summary":"user face challeng access log tabl panda datafram program instead get tabl receiv dictionari seek help convert tabl perform aggreg result",
        "Answer_original_content":"hei prateek us function modifi version function repo def tabl data url sourc url str api kei option str respons request sourc url auth api api kei stream true timeout respons rais statu byte list data respons iter content chunk size byte list append data final byte data join byte list data dict json load final byte data decod utf tabl datafram data data dict data column data dict column return tabl specif url artifact iter run file save url attribut return file check file name abl file relev best dariu topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei prateek us function specif url artifact iter save url attribut return file check file name abl file relev best dariu topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the answer suggests using a modified version of a function from a repository to load a saved table to a pandas dataframe. the function takes a source url and an optional api key as input, and returns a dataframe. the answer also suggests iterating over run.files() to get the specific url of the artifact and checking the file names to identify relevant files.",
        "Answer_gpt_summary":"answer suggest modifi version function repositori load save tabl panda datafram function take sourc url option api kei input return datafram answer suggest iter run file specif url artifact check file name identifi relev file"
    },
    {
        "Question_id":null,
        "Question_title":"vertex AI Workbench is hanging with error \"Opening notebook with JupyterLab\" for more than a day",
        "Question_body":"I am trying to follow instructions in https:\/\/cloud.google.com\/vertex-ai\/docs\/tutorials\/jupyter-notebooks (vertex AI Jupyter Notebooks tutorials). Steps done1. For the first notebook \"Text Classification model\" I have clicked on \"Vertex AI Workbench\". It takes me to GCP console & workbench.2. I am supposed to click on the \"Create\" button, which I did.3. THen the message \"Opening notebook with JupyterLab\" will come. But it is there for past 1 day, and still it hasn't finished creating. So I canceled the same. I tried once more the same thing happens. Not sure why?I have screen shots, but can't see anywhere to attach.Have anyone tried this tutorial, especially in workbench? Thanks,",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1662624720000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":112.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/vertex-AI-Workbench-is-hanging-with-error-quot-Opening-notebook\/td-p\/464300\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-08T22:12:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello,\u00a0\n\nAnybody active on these forums?\n\nIdeally some GCP reps should be there. Especially with newer offering like vertexAI - fundamental issues should be easy to solve!!"
            },
            {
                "Answer_creation_time":"2022-09-09T00:01:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Today I have retried the same. It worked at least creation of notebook.\n\nBut when executing step\n\nInstall additional packages\n\nInstall the following packages for executing this notebook.\n\nI am getting error:\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-recommendations-ai 0.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.8.1 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\napache-beam 2.40.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 9.0.0 which is incompatible.\n\n\u00a0\n\nAny help?"
            },
            {
                "Answer_creation_time":"2022-09-20T14:32:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"After searching for a solution for your case, it seems to be more an issue of the package version.\n\nI found a GitHub repository dealing with a similar problem to yours; there, you will likely find solutions to resolve it."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"workbench hang error open notebook jupyterlab dai try follow instruct http cloud googl com vertex doc tutori jupyt notebook jupyt notebook tutori step notebook text classif model click workbench take gcp consol workbench suppos click creat button messag open notebook jupyterlab come past dai hasn finish creat cancel tri thing happen sure screen shot attach tri tutori especi workbench thank",
        "Question_preprocessed_content":"workbench hang error open notebook jupyterlab dai try follow instruct step notebook text classif model click workbench take gcp consol suppos click creat button messag open notebook jupyterlab come past dai hasn finish creat cancel tri thing happen sure screen shot tri tutori especi workbench thank",
        "Question_gpt_summary_original":"The user is facing challenges while trying to follow the instructions in the Vertex AI Jupyter Notebooks tutorials. Specifically, the Vertex AI Workbench is hanging with an error message \"Opening notebook with JupyterLab\" for more than a day, preventing the user from creating the notebook. The user has tried to create the notebook multiple times, but the same issue persists. The user is seeking help from anyone who has tried this tutorial, especially in workbench.",
        "Question_gpt_summary":"user face challeng try follow instruct jupyt notebook tutori specif workbench hang error messag open notebook jupyterlab dai prevent user creat notebook user tri creat notebook multipl time issu persist user seek help tri tutori especi workbench",
        "Answer_original_content":"hello anybodi activ forum ideal gcp rep especi newer offer like vertexai fundament issu easi solv todai retri work creation notebook execut step instal addit packag instal follow packag execut notebook get error error pip depend resolv current account packag instal behaviour sourc follow depend conflict googl cloud recommend requir googl api core grpc googl api core incompat apach beam requir dill dill incompat apach beam requir pyarrow pyarrow incompat help search solut case issu packag version github repositori deal similar problem like solut resolv",
        "Answer_preprocessed_content":"hello anybodi activ forum ideal gcp rep especi newer offer like vertexai fundament issu easi solv todai retri work creation notebook execut step instal addit packag instal follow packag execut notebook get error error pip depend resolv current account packag instal behaviour sourc follow depend conflict requir incompat requir dill incompat requir pyarrow incompat help search solut case issu packag version github repositori deal similar problem like solut resolv",
        "Answer_gpt_summary_original":"possible solutions to the challenge of creating a notebook in google cloud platform's workbench include seeking help from gcp representatives, retrying the creation process, and resolving package version conflicts by referring to a similar problem on a github repository.",
        "Answer_gpt_summary":"possibl solut challeng creat notebook googl cloud platform workbench includ seek help gcp repres retri creation process resolv packag version conflict refer similar problem github repositori"
    },
    {
        "Question_id":null,
        "Question_title":"Convert to tabular from file dataset",
        "Question_body":"Yesterday I ran the \u2018consume\u2019 code to pull the data onto the cluster and it seemed to work but I still can\u2019t use it from ML designer as it\u2019s a file dataset rather than a tabular dataset. I\u2019ve tried a couple of ways of converting it without success, my next avenue of exploration is to use the SDK rather than trying to do it through the console",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617956083027,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/350780\/convert-to-tabular-from-file-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-09T20:14:35.61Z",
                "Answer_score":0,
                "Answer_body":"Hi, Designer only supports tabular dataset, hence you'd need to use SDK to work with file dataset. Based on the documentation, there's no method for converting file dataset to tabular dataset. Is your file in csv format? Perhaps you can create a datastore and connect to your blob storage. Then, use the import data module in designer to connect to your datastore(data path in blob storage). You can also create tabular dataset from datastore. Let me know if you have further questions, thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"convert tabular file dataset yesterdai ran consum code pull data cluster work us design file dataset tabular dataset iv tri coupl wai convert success avenu explor us sdk try consol",
        "Question_preprocessed_content":"convert tabular file dataset yesterdai ran consum code pull data cluster work us design file dataset tabular dataset iv tri coupl wai convert success avenu explor us sdk try consol",
        "Question_gpt_summary_original":"The user is facing challenges in converting a file dataset to a tabular dataset, which is preventing them from using it in ML designer. They have attempted to convert it without success and are now considering using the SDK as an alternative method.",
        "Question_gpt_summary":"user face challeng convert file dataset tabular dataset prevent design attempt convert success consid sdk altern method",
        "Answer_original_content":"design support tabular dataset need us sdk work file dataset base document method convert file dataset tabular dataset file csv format creat datastor connect blob storag us import data modul design connect datastor data path blob storag creat tabular dataset datastor let know question thank",
        "Answer_preprocessed_content":"design support tabular dataset need us sdk work file dataset base document method convert file dataset tabular dataset file csv format creat datastor connect blob storag us import data modul design connect datastor creat tabular dataset datastor let know question thank",
        "Answer_gpt_summary_original":"possible solutions from the answer include using the sdk to work with file datasets, creating a datastore and connecting to blob storage, using the import data module in designer to connect to the datastore, and creating a tabular dataset from the datastore. however, there is no method for directly converting a file dataset to a tabular dataset in designer.",
        "Answer_gpt_summary":"possibl solut answer includ sdk work file dataset creat datastor connect blob storag import data modul design connect datastor creat tabular dataset datastor method directli convert file dataset tabular dataset design"
    },
    {
        "Question_id":73663585.0,
        "Question_title":"Add Security groups in Amazon SageMaker for distributed training jobs",
        "Question_body":"<p>We would like to enforce specific security groups to be set on the SageMaker training jobs (XGBoost in script mode).\nHowever, distributed training, in this case, won\u2019t work out of the box, since the containers need to communicate with each other. What are the minimum inbound\/outbound rules (ports) that we need to specify for training jobs so that they can communicate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662733216593,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":19.0,
        "Answer_body":"<p>setting up training in VPC including specifying security groups is documented here:\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups<\/a><\/p>\n<p>Normally you would allow all communication between the training nodes. To do this you specify the security group source and destination to the name of the security group itself, and allow all IPv4 traffic. If you want to figure out what ports are used, you could: 1\/ define the permissive security group. 2\/ Turn on VPC flow logs 3\/ run training. 4\/ examine VPC Flow logs 5\/ update the security group only to the required ports.<\/p>\n<p>I must say restricting communication between the training nodes might be an extreme, so I would challenge the customer why it's really needed, as all nodes carry the same job, have the same IAM role, and are transiate by nature.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73663585",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662835019252,
        "Question_original_content":"add secur group distribut train job like enforc specif secur group set train job xgboost script mode distribut train case wont work box contain need commun minimum inbound outbound rule port need specifi train job commun",
        "Question_preprocessed_content":"add secur group distribut train job like enforc specif secur group set train job distribut train case wont work box contain need commun minimum rule need specifi train job commun",
        "Question_gpt_summary_original":"The user is facing challenges in adding security groups to Amazon SageMaker for distributed training jobs, specifically for XGBoost in script mode. The issue is that distributed training won't work out of the box because the containers need to communicate with each other, and the user needs to specify the minimum inbound\/outbound rules (ports) for training jobs to enable communication.",
        "Question_gpt_summary":"user face challeng ad secur group distribut train job specif xgboost script mode issu distribut train won work box contain need commun user need specifi minimum inbound outbound rule port train job enabl commun",
        "Answer_original_content":"set train vpc includ specifi secur group document http doc aw amazon com latest train vpc html train vpc group normal allow commun train node specifi secur group sourc destin secur group allow ipv traffic want figur port defin permiss secur group turn vpc flow log run train examin vpc flow log updat secur group requir port restrict commun train node extrem challeng custom need node carri job iam role transiat natur",
        "Answer_preprocessed_content":"set train vpc includ specifi secur group document normal allow commun train node specifi secur group sourc destin secur group allow ipv traffic want figur port defin permiss secur group turn vpc flow log run train examin vpc flow log updat secur group requir port restrict commun train node extrem challeng custom need node carri job iam role transiat natur",
        "Answer_gpt_summary_original":"the answer suggests that for distributed training jobs, it is recommended to set up training in vpc and specify security groups to allow all communication between the training nodes. if the user wants to figure out what ports are used, they can define a permissive security group, turn on vpc flow logs, run training, examine vpc flow logs, and update the security group only to the required ports. the answer also challenges the need to restrict communication between the training nodes as all nodes carry the same job, have the same iam role, and are transient by nature.",
        "Answer_gpt_summary":"answer suggest distribut train job recommend set train vpc specifi secur group allow commun train node user want figur port defin permiss secur group turn vpc flow log run train examin vpc flow log updat secur group requir port answer challeng need restrict commun train node node carri job iam role transient natur"
    },
    {
        "Question_id":69678393.0,
        "Question_title":"Creating docker file which installs python with sklearn and pandas that can be used on sagemaker",
        "Question_body":"<p>I am quite new to docker. My problem in short is to create a docker file that contains python with sklearn and pandas which can be used on aws sagemaker.<\/p>\n<p>My current docker file looks like the following:<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>However when i try to create this image I get an error at line <code>pip3 install sagemaker-training<\/code>. The error is the following:<\/p>\n<pre><code>error: command 'gcc' failed: No such file or directory\n\nERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python3.9 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-fj0cb373\/sagemaker-training_66ca9935ed134c95ac11a32e118e4568\/setup.py'&quot;'&quot;';f = getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__) if os.path.exists(__file__) else io.StringIO('&quot;'&quot;'from setuptools import setup; setup()'&quot;'&quot;');code = f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' install --record \/tmp\/pip-record-o5rzjscd\/install-record.txt --single-version-externally-managed --compile --install-headers \/opt\/conda\/include\/python3.9\/sagemaker-training Check the logs for full command output.\n\nThe command '\/bin\/bash -o pipefail -c pip3 install sagemaker-training' returned a non-zero code: 1\n<\/code><\/pre>\n<p>If there is a more suitable base image can someone point that out to me? I am generally trying to follow this page <a href=\"https:\/\/github.com\/aws\/sagemaker-training-toolkit\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-training-toolkit<\/a>.<\/p>\n<p>Note: I realise I can use some sagemaker pre-built containers without using my own docker file. However I am trying to do this for my own learning so I know what to do for projects that can't utilise them.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1634912008467,
        "Question_favorite_count":null,
        "Question_last_edit_time":1634913542568,
        "Question_score":0.0,
        "Question_view_count":161.0,
        "Answer_body":"<p>I adjusted your Dockerfile and it builds successfully for me.<\/p>\n<pre><code>FROM jupyter\/scipy-notebook\nARG defaultuser=jovyan\nUSER root\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update &amp;&amp; \\\n    apt-get -y install gcc mono-mcs &amp;&amp; \\\n    rm -rf \/var\/lib\/apt\/lists\/*\nUSER $defaultuser\n\nRUN pip3 install sagemaker-training\n\nCOPY train.py \/opt\/ml\/code\/train.py\n\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>(I had to adjust for the fact that the default user from the base container isn't root, when installing GCC)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69678393",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1635439092896,
        "Question_original_content":"creat docker file instal python sklearn panda new docker problem short creat docker file contain python sklearn panda current docker file look like follow jupyt scipi notebook run pip instal train copi train opt code train env program train try creat imag error line pip instal train error follow error command gcc fail file directori error command error exit statu opt conda bin python import sy setuptool token sy argv tmp pip instal fjcb train caedcacae setup file tmp pip instal fjcb train caedcacae setup getattr token open open file path exist file stringio setuptool import setup setup code read replac close exec compil code file exec instal record tmp pip record orzjscd instal record txt singl version extern manag compil instal header opt conda includ python train check log command output command bin bash pipefail pip instal train return non zero code suitabl base imag point gener try follow page http github com aw train toolkit note realis us pre built contain docker file try learn know project utilis",
        "Question_preprocessed_content":"creat docker file instal python sklearn panda new docker problem short creat docker file contain python sklearn panda current docker file look like follow try creat imag error line error follow suitabl base imag point gener try follow page note realis us contain docker file try learn know project utilis",
        "Question_gpt_summary_original":"The user is facing challenges in creating a docker file that contains Python with sklearn and pandas which can be used on AWS Sagemaker. The user's current docker file is encountering an error at the line \"pip3 install sagemaker-training\" due to the absence of the \"gcc\" command. The user is seeking advice on a more suitable base image and is trying to create the docker file for their own learning purposes.",
        "Question_gpt_summary":"user face challeng creat docker file contain python sklearn panda user current docker file encount error line pip instal train absenc gcc command user seek advic suitabl base imag try creat docker file learn purpos",
        "Answer_original_content":"adjust dockerfil build successfulli jupyt scipi notebook arg defaultus jovyan user root env debian frontend noninteract run apt updat apt instal gcc mono mc var lib apt list user defaultus run pip instal train copi train opt code train env program train adjust fact default user base contain isn root instal gcc",
        "Answer_preprocessed_content":"adjust dockerfil build successfulli adjust fact default user base contain isn root instal gcc",
        "Answer_gpt_summary_original":"the answer suggests adjusting the dockerfile by installing gcc and mono-mcs, removing apt-get lists, and copying the train.py file to the code directory. additionally, the answer notes that the default user from the base container isn't root, so adjustments need to be made for that.",
        "Answer_gpt_summary":"answer suggest adjust dockerfil instal gcc mono mc remov apt list copi train file code directori addition answer note default user base contain isn root adjust need"
    },
    {
        "Question_id":null,
        "Question_title":"Running concurrent sessions from SageMaker notebooks on Glue Dev Endpoints.",
        "Question_body":"Customer who has created a AWS glue dev endpoint and want to run two Sagemaker notebooks in parallel on same single Dev endpoint but its not working .\n\nThe one which is invoked first is only able to run the job, while another one fails. what could be possible reasons and fix for it?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591020062000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"SageMaker notebooks are Jupyter notebooks that uses the SparkMagic module to connect to a local Livy setup. The local Livy does an SSH tunnel to Livy service on the Glue Spark server. Apache Livy binds to post 8998 and is a RESTful service that can relay multiple Spark session commands at the same time so multiple port binding conflicts cannot happen. So yes, you can have multiple sessions as long as the backend cluster has resources to serve that many sessions.\n\nYou can run the following command in a notebook to check the defaults for Spark sessions:\n\nspark.sparkContext.getConf().getAll()\n\n\nI see the following defaults in my Spark session. You can easily override them from the config file at ~\/.sparkmagic\/config.json or by using the %%configure magic from within the notebook.\n\nspark.executor.cores 4\nspark.executor.memory 5g\nspark.driver.memory 5g\n\n\nNote that spark.executor.instances is not set and spark.dynamicAllocation.enabled is not overridden which means that it is true, so if you have a demanding Spark job in one notebook, it can take over all resources in the cluster and prevent other Spark sessions from starting. The recommendation when sharing a single Glue Dev endpoint is to limit each session to a few executors so that multiple sessions can acquire resources from the cluster e.g.:\n\n%%configure -f\n{\"executorMemory\": \"5G\", \"executorCores\":4,\"numExecutors\":2}\n\n\n(Note: Tested on multiple SageMaker PySpark notebooks in single SageMaker notebook instances as well as multiple SageMaker notebook instances.)",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIDitlJMgTlGai61w_Zvqdg\/running-concurrent-sessions-from-sage-maker-notebooks-on-glue-dev-endpoints",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-01T16:52:06.000Z",
                "Answer_score":0,
                "Answer_body":"SageMaker notebooks are Jupyter notebooks that uses the SparkMagic module to connect to a local Livy setup. The local Livy does an SSH tunnel to Livy service on the Glue Spark server. Apache Livy binds to post 8998 and is a RESTful service that can relay multiple Spark session commands at the same time so multiple port binding conflicts cannot happen. So yes, you can have multiple sessions as long as the backend cluster has resources to serve that many sessions.\n\nYou can run the following command in a notebook to check the defaults for Spark sessions:\n\nspark.sparkContext.getConf().getAll()\n\n\nI see the following defaults in my Spark session. You can easily override them from the config file at ~\/.sparkmagic\/config.json or by using the %%configure magic from within the notebook.\n\nspark.executor.cores 4\nspark.executor.memory 5g\nspark.driver.memory 5g\n\n\nNote that spark.executor.instances is not set and spark.dynamicAllocation.enabled is not overridden which means that it is true, so if you have a demanding Spark job in one notebook, it can take over all resources in the cluster and prevent other Spark sessions from starting. The recommendation when sharing a single Glue Dev endpoint is to limit each session to a few executors so that multiple sessions can acquire resources from the cluster e.g.:\n\n%%configure -f\n{\"executorMemory\": \"5G\", \"executorCores\":4,\"numExecutors\":2}\n\n\n(Note: Tested on multiple SageMaker PySpark notebooks in single SageMaker notebook instances as well as multiple SageMaker notebook instances.)",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1591030326000,
        "Question_original_content":"run concurr session notebook glue dev endpoint custom creat aw glue dev endpoint want run notebook parallel singl dev endpoint work invok abl run job fail possibl reason fix",
        "Question_preprocessed_content":"run concurr session notebook glue dev endpoint custom creat aw glue dev endpoint want run notebook parallel singl dev endpoint work invok abl run job fail possibl reason fix",
        "Question_gpt_summary_original":"The user is facing challenges in running two Sagemaker notebooks in parallel on a single AWS Glue Dev Endpoint. The first notebook is able to run successfully, but the second one fails. The user is seeking possible reasons and solutions for this issue.",
        "Question_gpt_summary":"user face challeng run notebook parallel singl aw glue dev endpoint notebook abl run successfulli second fail user seek possibl reason solut issu",
        "Answer_original_content":"notebook jupyt notebook us sparkmag modul connect local livi setup local livi ssh tunnel livi servic glue spark server apach livi bind post rest servic relai multipl spark session command time multipl port bind conflict happen ye multipl session long backend cluster resourc serv session run follow command notebook check default spark session spark sparkcontext getconf getal follow default spark session easili overrid config file sparkmag config json configur magic notebook spark executor core spark executor memori spark driver memori note spark executor instanc set spark dynamicalloc enabl overridden mean true demand spark job notebook resourc cluster prevent spark session start recommend share singl glue dev endpoint limit session executor multipl session acquir resourc cluster configur executormemori executorcor numexecutor note test multipl pyspark notebook singl notebook instanc multipl notebook instanc",
        "Answer_preprocessed_content":"notebook jupyt notebook us sparkmag modul connect local livi setup local livi ssh tunnel livi servic glue spark server apach livi bind post rest servic relai multipl spark session command time multipl port bind conflict happen ye multipl session long backend cluster resourc serv session run follow command notebook check default spark session follow default spark session easili overrid config file configur magic notebook note set overridden mean true demand spark job notebook resourc cluster prevent spark session start recommend share singl glue dev endpoint limit session executor multipl session acquir resourc cluster configur executormemori executorcor numexecutor note test multipl pyspark notebook singl notebook instanc multipl notebook",
        "Answer_gpt_summary_original":"possible solutions to the issue of running concurrent sessions from notebooks on aws glue dev endpoints are:\n\n- check the defaults for spark sessions by running the command \"spark.sparkcontext.getconf().getall()\" in a notebook and ensure that spark.executor.instances is not set and spark.dynamicallocation.enabled is not overridden.\n- limit each session to a few executors so that multiple sessions can acquire resources from the cluster by using the %%configure magic from within the notebook, for example: \"%%configure -f {\"executormemory\": \"5g\", \"executorcores\":4,\"numexecutors\":2}\".",
        "Answer_gpt_summary":"possibl solut issu run concurr session notebook aw glue dev endpoint check default spark session run command spark sparkcontext getconf getal notebook ensur spark executor instanc set spark dynamicalloc enabl overridden limit session executor multipl session acquir resourc cluster configur magic notebook exampl configur executormemori executorcor numexecutor"
    },
    {
        "Question_id":61547494.0,
        "Question_title":"How to snap a python package with plugin packages?",
        "Question_body":"<p>I'd like to bundle the Python package <code>kedro<\/code> which provides a command line interface (<code>kedro<\/code>). In addition I'd like to put the Python package <code>kedro-docker<\/code> into the snap as well. This second package extends the first package's command line interface (<code>kedro docker<\/code>). But when I create a snap with the <code>snapcraft.yaml<\/code> below I get only the command line interface of the first package:<\/p>\n\n<pre class=\"lang-yaml prettyprint-override\"><code>name: kedro\nbase: core18\nversion: latest\ndescription: |\n    Kedro is a development workflow framework that implements software\n    engineering best-practice for data pipelines with an eye towards\n    productionising machine learning models.\n\ngrade: devel\nconfinement: devmode\n\narchitectures:\n  - build-on: [amd64]\n\napps:\n  kedro:\n    command: kedro\n    plugs:\n      - home\n      - network\n      - network-bind\n      - docker\n    environment: {\n      LANG: C.UTF-8,\n      LC_ALL: C.UTF-8\n    }\n\nparts:\n  kedro:\n    plugin: python\n    python-version: python3\n    python-packages:\n      - kedro==0.15.9\n      - kedro-docker==0.1.1\n<\/code><\/pre>\n\n<p>How can I get the extended command line interface (<code>kedro docker<\/code>) into the snap?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1588354211093,
        "Question_favorite_count":null,
        "Question_last_edit_time":1588354692552,
        "Question_score":0.0,
        "Question_view_count":246.0,
        "Answer_body":"<p>I'm no expert and never used <code>snapcraft<\/code>, therefore just a hypothesis here. Kedro-Docker exposes only project-specific commands that won't show up unless you are in the root of the project. So if you run <code>kedro new<\/code> and then <code>cd &lt;project-dir&gt; &amp;&amp; kedro<\/code>, you should (ideally) see a <code>docker<\/code> group of commands:<\/p>\n\n<pre><code>Global commands from Kedro\nCommands:\n  docs  See the kedro API docs and introductory tutorial.\n  info  Get more information about kedro.\n  new   Create a new kedro project.\n\nProject specific commands from Docker\nCommands:\n  docker  Dockerize your Kedro project.\n\nProject specific commands from &lt;project-dir&gt;\/kedro_cli.py\nCommands:\n  activate-nbstripout  Install the nbstripout git hook to automatically...\n  build-docs           Build the project documentation.\n  build-reqs           Build the project dependency requirements.\n  install              Install project dependencies from both...\n  ipython              Open IPython with project specific variables loaded.\n  jupyter              Open Jupyter Notebook \/ Lab with project specific...\n  lint                 Run flake8, isort and (on Python &gt;=3.6) black.\n  package              Package the project as a Python egg and wheel.\n  run                  Run the pipeline.\n  test                 Run the test suite.\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61547494",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1588362730510,
        "Question_original_content":"snap python packag plugin packag like bundl python packag provid command line interfac addit like python packag docker snap second packag extend packag command line interfac docker creat snap snapcraft yaml command line interfac packag base core version latest descript develop workflow framework implement softwar engin best practic data pipelin ey productionis machin learn model grade devel confin devmod architectur build amd app command plug home network network bind docker environ lang utf utf part plugin python python version python python packag docker extend command line interfac docker snap",
        "Question_preprocessed_content":"snap python packag plugin packag like bundl python packag provid command line interfac addit like python packag snap second packag extend packag command line interfac creat snap command line interfac packag extend command line interfac snap",
        "Question_gpt_summary_original":"The user is facing a challenge in bundling two Python packages, 'kedro' and 'kedro-docker', into a snap. While the 'kedro' package provides a command line interface, the 'kedro-docker' package extends it with the 'kedro docker' command. However, the snap created using the provided 'snapcraft.yaml' file only includes the command line interface of the 'kedro' package, and the user is seeking a solution to include the extended command line interface of the 'kedro-docker' package in the snap.",
        "Question_gpt_summary":"user face challeng bundl python packag docker snap packag provid command line interfac docker packag extend docker command snap creat provid snapcraft yaml file includ command line interfac packag user seek solut includ extend command line interfac docker packag snap",
        "Answer_original_content":"expert snapcraft hypothesi docker expos project specif command won root project run new ideal docker group command global command command doc api doc introductori tutori info inform new creat new project project specif command docker command docker docker project project specif command cli command activ nbstripout instal nbstripout git hook automat build doc build project document build req build project depend requir instal instal project depend ipython open ipython project specif variabl load jupyt open jupyt notebook lab project specif lint run flake isort python black packag packag project python egg wheel run run pipelin test run test suit",
        "Answer_preprocessed_content":"expert hypothesi docker expos command won root project run group command",
        "Answer_gpt_summary_original":"there are no possible solutions provided in the answer. the answer is a hypothesis that is not related to the question.",
        "Answer_gpt_summary":"possibl solut provid answer answer hypothesi relat question"
    },
    {
        "Question_id":34948242.0,
        "Question_title":"Azure: plot without labels",
        "Question_body":"<p>Suppose I have a dataframe (myDataframe) with two column of values (third and fourth). I want to plot them in a bi-dimensional graph. If I do it in R it works, but it returns me a graph without labels when I run the script from Azure Machine Learning. Someone with ideas?<\/p>\n\n<pre><code>...\nplot(myDataframe[,3],myDataframe[,4], \n       main=\"my title\",\n       xlab= \"x\"\n       ylab= \"y\",\n       col= \"blue\", pch = 19, cex = 0.1, lty = \"solid\", lwd = 2)\n\n# lines(x,y=x, col=\"yellow\")\n\n# add LABELS\ntext(DF_relativo[,A], DF_relativo[,B], \n       labels=DF_relativo$names, cex= 0.7, pos=2)\n...\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1453470518060,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":91.0,
        "Answer_body":"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/b1c26728eb6c4e4d80dddceae992d653\" rel=\"nofollow\">Cortana Intelligence gallery example<\/a> for the particular task.  <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":-1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34948242",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1466402236047,
        "Question_original_content":"azur plot label suppos datafram mydatafram column valu fourth want plot dimension graph work return graph label run script idea plot mydatafram mydatafram main titl xlab ylab col blue pch cex lty solid lwd line col yellow add label text relativo relativo label relativo name cex po",
        "Question_preprocessed_content":"azur plot label suppos datafram column valu want plot graph work return graph label run script idea",
        "Question_gpt_summary_original":"The user is facing a challenge in plotting a bi-dimensional graph in Azure Machine Learning without labels, even though it works in R. They are seeking ideas on how to add labels to the graph.",
        "Question_gpt_summary":"user face challeng plot dimension graph label work seek idea add label graph",
        "Answer_original_content":"ggplot bit differ us plot label cortana intellig galleri exampl particular task",
        "Answer_preprocessed_content":"ggplot bit differ us plot label cortana intellig galleri exampl particular task",
        "Answer_gpt_summary_original":"possible solution: the user can use ggplot2() to plot two columns of values from a dataframe in a bi-dimensional graph with labels. they can refer to the cortana intelligence gallery example for guidance.",
        "Answer_gpt_summary":"possibl solut user us ggplot plot column valu datafram dimension graph label refer cortana intellig galleri exampl guidanc"
    },
    {
        "Question_id":null,
        "Question_title":"Problems submittimg azure machine learning pipelines",
        "Question_body":"I have a problem submitting my azure machine learning pipeline. I used the Import Data model to import data from a ULR via HTTP. When I clicked on submit, it submitted successfully with no green vertical line indication usually on the left side of the rectangular Import Data model. However, I got a green tick on the left pain that the job is completed. To continue building the pipeline, I added Select Columns in Dataset model to select the required columns. But as you can see from the screen shots, the select columns by name option is disabled and there are no names to select from.\nWhen I click the job details on the left pain on the Authoring page, it opens a read-only pipeline which cannot be edited. When I clone it, I am able to edit it but I am unable to add Select Columns in Dataset model to select the required columns as previously.\nI never experienced this problem with the Azure machine learning (Classic) or earlier versions of the Microsoft Azure Machine Learning Studio. I started experiencing this as soon as Microsoft changed the interface recently. Can someone please help me?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653665897827,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/867306\/problems-submitimg-azure-machine-learning-pipeline.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-30T08:18:39.733Z",
                "Answer_score":0,
                "Answer_body":"@EbenezerDansoAmoako-4939 Yes, with the change in UI the functionality to load the data from the downstream module is not being displayed, in this case it is select columns from dataset.\nYou can however enter the column names during authoring by first previewing the data from the import data module and note down the column names that needs to be added in downstream module.\n\nWe will pass the feedback of this change to the product team for review. Thanks!!\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"problem submittimg pipelin problem submit pipelin import data model import data ulr http click submit submit successfulli green vertic line indic usual left rectangular import data model got green tick left pain job complet continu build pipelin ad select column dataset model select requir column screen shot select column option disabl name select click job detail left pain author page open read pipelin edit clone abl edit unabl add select column dataset model select requir column previous experienc problem classic earlier version studio start experienc soon microsoft chang interfac recent help",
        "Question_preprocessed_content":"problem submittimg pipelin problem submit pipelin import data model import data ulr http click submit submit successfulli green vertic line indic usual left rectangular import data model got green tick left pain job complet continu build pipelin ad select column dataset model select requir column screen shot select column option disabl name select click job detail left pain author page open pipelin edit clone abl edit unabl add select column dataset model select requir column previous experienc problem earlier version studio start experienc soon microsoft chang interfac recent help",
        "Question_gpt_summary_original":"The user is facing challenges in submitting an Azure machine learning pipeline. The Import Data model was used to import data from a URL via HTTP, but upon submission, there was no green vertical line indication. The Select Columns in Dataset model was added to select required columns, but the select columns by name option was disabled and there were no names to select from. The job details on the left pane opened a read-only pipeline that cannot be edited, and cloning it did not solve the problem. The user suspects that the problem started when Microsoft changed the interface recently.",
        "Question_gpt_summary":"user face challeng submit pipelin import data model import data url http submiss green vertic line indic select column dataset model ad select requir column select column option disabl name select job detail left pane open read pipelin edit clone solv problem user suspect problem start microsoft chang interfac recent",
        "Answer_original_content":"ebenezerdansoamoako ye chang function load data downstream modul displai case select column dataset enter column name author preview data import data modul note column name need ad downstream modul pass feedback chang product team review thank answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"ye chang function load data downstream modul displai case select column dataset enter column name author preview data import data modul note column name need ad downstream modul pass feedback chang product team review thank answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"possible solutions from the answer include manually entering column names during authoring by previewing data from the import data module and noting down the column names that need to be added in the downstream module. the feedback regarding the change in ui will also be passed to the product team for review.",
        "Answer_gpt_summary":"possibl solut answer includ manual enter column name author preview data import data modul note column name need ad downstream modul feedback chang pass product team review"
    },
    {
        "Question_id":null,
        "Question_title":"While registering a dataframe in AzureML pipeline, getting error: 'DataFrame' object has no attribute 'register. How do we actually store dataframe into Azure Blob Storage?",
        "Question_body":"While registering a dataframe in AzureML pipeline, getting error: 'DataFrame' object has no attribute 'register. How do we actually store dataframe into Azure Blob Storage?\n\nCode snippet-\n\n<DataFrame>.register(workspace=ws, name='<abc>', description='<abc>', tags = {'format':'CSV'}, create_new_version=True)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624431973167,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/448224\/while-registering-a-dataframe-in-azureml-pipeline.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-23T10:48:47.457Z",
                "Answer_score":0,
                "Answer_body":"@JitenderKumarChandel-0663 I think this is a valid error since the dataset cannot be registered with the above command. You should try the steps mentioned in this notebook.\n\nThese steps should help to register your CSV data as dataframe.\n\n datastore = ws.get_default_datastore()\n datastore.upload_files(files = ['.\/train-dataset\/iris.csv'],\n                        target_path = 'train-dataset\/tabular\/',\n                        overwrite = True,\n                        show_progress = True)\n    \n from azureml.core import Dataset\n dataset = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-dataset\/tabular\/iris.csv')])\n    \n # preview the first 3 rows of the dataset\n dataset.take(3).to_pandas_dataframe()\n\n\n\nPlease feel free to accept the above response as answer if it helped. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"regist datafram pipelin get error datafram object attribut regist actual store datafram azur blob storag regist datafram pipelin get error datafram object attribut regist actual store datafram azur blob storag code snippet regist workspac descript tag format csv creat new version true",
        "Question_preprocessed_content":"regist datafram pipelin get error datafram object attribut regist actual store datafram azur blob storag regist datafram pipelin get error datafram object attribut regist actual store datafram azur blob storag code snippet regist",
        "Question_gpt_summary_original":"The user encountered an error while trying to register a dataframe in an AzureML pipeline, receiving the message \"'DataFrame' object has no attribute 'register.\" The user is seeking guidance on how to properly store a dataframe into Azure Blob Storage.",
        "Question_gpt_summary":"user encount error try regist datafram pipelin receiv messag datafram object attribut regist user seek guidanc properli store datafram azur blob storag",
        "Answer_original_content":"jitenderkumarchandel think valid error dataset regist command try step mention notebook step help regist csv data datafram datastor default datastor datastor upload file file train dataset iri csv target path train dataset tabular overwrit true progress true core import dataset dataset dataset tabular delimit file path datastor train dataset tabular iri csv preview row dataset dataset panda datafram feel free accept respons answer help thank",
        "Answer_preprocessed_content":"think valid error dataset regist command try step mention notebook step help regist csv data datafram datastor overwrit true true core import dataset dataset preview row dataset feel free accept respons answer help thank",
        "Answer_gpt_summary_original":"the answer suggests trying the steps mentioned in a notebook to register a csv data as a dataframe. the steps involve uploading files to a default datastore and using a core dataset to preview the first three rows of the dataset.",
        "Answer_gpt_summary":"answer suggest try step mention notebook regist csv data datafram step involv upload file default datastor core dataset preview row dataset"
    },
    {
        "Question_id":63740792.0,
        "Question_title":"Provide additional input to docker container running inference model",
        "Question_body":"<p>We are using AWS Sagemaker feature, bring your own docker, where we have inference model written in R. As I understood, batch transform job runs container in a following way:<\/p>\n<pre><code>docker run image serve\n<\/code><\/pre>\n<p>Also, on docker we have a logic to determine which function to invoke:<\/p>\n<pre><code>args &lt;- commandArgs()\nif (any(grepl('train', args))) {\n    train()}\nif (any(grepl('serve', args))) {\n    serve()}\n<\/code><\/pre>\n<p>Is there a way, to override default container invocation so we can pass some additional parameters?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1599220820873,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":702.0,
        "Answer_body":"<p>As you said, and is indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, Sagemaker will run your container with the following command:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>docker run image serve\n<\/code><\/pre>\n<p>By issuing this command Sagemaker will overwrite any <code>CMD<\/code> that you provide in your container Dockerfile, so you cannot use <code>CMD<\/code> to provide dynamic arguments to your program.<\/p>\n<p>We can think in use the Dockerfile <code>ENTRYPOINT<\/code> to consume some environment variables, but the documentation of AWS dictates that it is preferable use the <code>exec<\/code> form of the <code>ENTRYPOINT<\/code>. Somethink like:<\/p>\n<pre><code>ENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;]\n<\/code><\/pre>\n<p>I think that, for analogy with <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">model training<\/a>, they need this kind of container execution to enable the container to receive termination signals:<\/p>\n<blockquote>\n<p>The exec form of the <code>ENTRYPOINT<\/code> instruction starts the executable directly, not as a child of <code>\/bin\/sh<\/code>. This enables it to receive signals like <code>SIGTERM<\/code> and <code>SIGKILL<\/code> from SageMaker APIs.<\/p>\n<\/blockquote>\n<p>To allow variable expansion, we need to use the <code>ENTRYPOINT<\/code> <code>shell<\/code> form. Imagine:<\/p>\n<pre><code>ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;\/usr\/bin\/Rscript&quot;, &quot;\/opt\/ml\/mars.R&quot;, &quot;--no-save&quot;, &quot;$ENV_VAR1&quot;]\n<\/code><\/pre>\n<p>If you try to do the same with the <code>exec<\/code> form the variables provided will be treated as a literal and will not be sustituited for their actual values.<\/p>\n<p>Please, see the approved answer of <a href=\"https:\/\/stackoverflow.com\/questions\/37904682\/how-do-i-use-docker-environment-variable-in-entrypoint-array\">this<\/a> stackoverflow question for a great explanation of this subject.<\/p>\n<p>But, one thing you can do is obtain the value of these variables in your R code, similar as when you process <code>commandArgs<\/code>:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>ENV_VAR1 &lt;- Sys.getenv(&quot;ENV_VAR1&quot;)\n<\/code><\/pre>\n<p>To pass environment variables to the container, as indicated in the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html\" rel=\"nofollow noreferrer\">AWS documentation<\/a>, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\"><code>CreateModel<\/code><\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\"><code>CreateTransformJob<\/code><\/a> requests on your container.<\/p>\n<p>You probably will need to include in your Dockerfile <code>ENV<\/code> definitions for every required environment variable on your container, and provide for these definitions default values with <code>ARG<\/code>:<\/p>\n<pre><code>ARG ENV_VAR1_DEFAULT_VALUE=VAL1\nENV_VAR1=$ENV_VAR1_DEFAULT_VALUE\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1599720910636,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63740792",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1599691026140,
        "Question_original_content":"provid addit input docker contain run infer model featur bring docker infer model written understood batch transform job run contain follow wai docker run imag serv docker logic determin function invok arg commandarg grepl train arg train grepl serv arg serv wai overrid default contain invoc pass addit paramet",
        "Question_preprocessed_content":"provid addit input docker contain run infer model featur bring docker infer model written understood batch transform job run contain follow wai docker logic determin function invok wai overrid default contain invoc pass addit paramet",
        "Question_gpt_summary_original":"The user is facing a challenge with AWS Sagemaker's \"bring your own docker\" feature, specifically with passing additional parameters to the container running the inference model written in R. The current setup uses batch transform job to run the container with a default invocation, and the user is looking for a way to override this default and pass additional parameters.",
        "Question_gpt_summary":"user face challeng bring docker featur specif pass addit paramet contain run infer model written current setup us batch transform job run contain default invoc user look wai overrid default pass addit paramet",
        "Answer_original_content":"said indic aw document run contain follow command docker run imag serv issu command overwrit cmd provid contain dockerfil us cmd provid dynam argument program think us dockerfil entrypoint consum environ variabl document aw dictat prefer us exec form entrypoint somethink like entrypoint usr bin rscript opt mar save think analog model train need kind contain execut enabl contain receiv termin signal exec form entrypoint instruct start execut directli child bin enabl receiv signal like sigterm sigkil api allow variabl expans need us entrypoint shell form imagin entrypoint usr bin rscript opt mar save env var try exec form variabl provid treat liter sustituit actual valu approv answer stackoverflow question great explan subject thing obtain valu variabl code similar process commandarg env var sy getenv env var pass environ variabl contain indic aw document us createmodel createtransformjob request contain probabl need includ dockerfil env definit requir environ variabl contain provid definit default valu arg arg env var default valu val env var env var default valu",
        "Answer_preprocessed_content":"said indic aw document run contain follow command issu command overwrit provid contain dockerfil us provid dynam argument program think us dockerfil consum environ variabl document aw dictat prefer us form somethink like think analog model train need kind contain execut enabl contain receiv termin signal exec form instruct start execut directli child enabl receiv signal like api allow variabl expans need us form imagin try form variabl provid treat liter sustituit actual valu approv answer stackoverflow question great explan subject thing obtain valu variabl code similar process pass environ variabl contain indic aw document us request contain probabl need includ dockerfil definit requir environ variabl contain provid definit default valu",
        "Answer_gpt_summary_original":"possible solutions to the challenge of providing additional input to a docker container running an inference model written in r include using the entrypoint shell form to allow variable expansion, obtaining the value of variables in the r code, and passing environment variables to the container using createmodel and createtransformjob requests. it is also recommended to include env definitions for every required environment variable on the container and provide default values with arg.",
        "Answer_gpt_summary":"possibl solut challeng provid addit input docker contain run infer model written includ entrypoint shell form allow variabl expans obtain valu variabl code pass environ variabl contain createmodel createtransformjob request recommend includ env definit requir environ variabl contain provid default valu arg"
    },
    {
        "Question_id":null,
        "Question_title":"Staged pipeline steps not given labels",
        "Question_body":"<p>I\u2019ve noticed that guild applies that labels I give to pipelines to each pipeline step when the pipeline stage is ran directly.  However, if I instead stage the pipeline operation and then run the staged pipeline operation this doesn\u2019t happen.  Instead only the pipeline operation receives label, not the steps of the pipeline.   For example, lets say my <code>guild.yml<\/code> is given as follows.  The contents of <code>train.py<\/code> and <code>test.py<\/code> aren\u2019t really important.<\/p>\n<pre><code>- operations:\n    mypipeline:\n        steps:\n            - train\n            - test\n\n    train:\n        sourcecode:\n            dest: .\n            select: train.py\n        exec: \"python train.py\"\n\n    test:\n        sourcecode:\n            dest: .\n            select: test.py\n        exec: \"python test.py\"\n<\/code><\/pre>\n<p>If I run <code>guild run mypipeline -y --label debug<\/code> I get<\/p>\n<pre><code>[1:b21131b7]   test                                      2022-01-03 17:27:55  completed  debug\n[2:8dd06463]   train                                     2022-01-03 17:27:55  completed  debug\n[3:092f43f2]   mypipeline                                2022-01-03 17:27:54  completed  debug\n<\/code><\/pre>\n<p>But if I run <code>guild run mypipeline -y --label debug --stage &amp;&amp; guild run queue -y<\/code> I get the following.  Note that only the <code>mypipeline<\/code> operation receives the label <code>debug<\/code> while the step operations receive no label.<\/p>\n<pre><code>[1:5b2c8b79]   test                                      2022-01-03 17:28:48  completed\n[2:85ae374c]   train                                     2022-01-03 17:28:47  completed\n[3:a0dc7ef1]   mypipeline                                2022-01-03 17:28:46  completed   debug\n<\/code><\/pre>\n<p>Note that similar commands like <code>guild run mypipeline -y --label debug --stage &amp;&amp; guild run --start $(guild select 1)<\/code> have the same effect.<\/p>\n<p>Is this intended behavior? If so how can I make it so that the steps of the staged pipeline operation receive the same label as the pipeline operation that started them?  Thanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1641249403629,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":220.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/staged-pipeline-steps-not-given-labels\/791",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-03T22:40:22.107Z",
                "Answer_body":"<p>This looks like a bug. Guild stores run params in staged runs and my guess is that it\u2019s either not storing this particular param or not using it when starting the staged run. I\u2019ll take a look.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-07T18:24:30.642Z",
                "Answer_body":"<p>This is fixed in master and will be available in the next release (a pre-release will go out later today with this - look for 0.7.5.dev3).<\/p>\n<p>Thanks for the detailed report!<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-07T20:08:14.313Z",
                "Answer_body":"<p>Good to hear, thanks.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stage pipelin step given label iv notic appli label pipelin pipelin step pipelin stage ran directli instead stage pipelin oper run stage pipelin oper doesnt happen instead pipelin oper receiv label step pipelin exampl let yml given follow content train test arent import oper mypipelin step train test train sourcecod dest select train exec python train test sourcecod dest select test exec python test run run mypipelin label debug test complet debug train complet debug mypipelin complet debug run run mypipelin label debug stage run queue follow note mypipelin oper receiv label debug step oper receiv label bcb test complet aec train complet adcef mypipelin complet debug note similar command like run mypipelin label debug stage run start select effect intend behavior step stage pipelin oper receiv label pipelin oper start thank",
        "Question_preprocessed_content":"stage pipelin step given label iv notic appli label pipelin pipelin step pipelin stage ran directli instead stage pipelin oper run stage pipelin oper doesnt happen instead pipelin oper receiv label step pipelin exampl let given follow content arent import run run follow note oper receiv label step oper receiv label note similar command like effect intend behavior step stage pipelin oper receiv label pipelin oper start thank",
        "Question_gpt_summary_original":"The user is facing a challenge where the labels given to pipeline steps are not applied when the pipeline is staged and run. Only the pipeline operation receives the label, while the steps do not. The user is seeking clarification on whether this is intended behavior and how to make the steps of the staged pipeline operation receive the same label as the pipeline operation that started them.",
        "Question_gpt_summary":"user face challeng label given pipelin step appli pipelin stage run pipelin oper receiv label step user seek clarif intend behavior step stage pipelin oper receiv label pipelin oper start",
        "Answer_original_content":"look like bug store run param stage run guess store particular param start stage run ill look fix master avail releas pre releas later todai look dev thank detail report good hear thank",
        "Answer_preprocessed_content":"look like bug store run param stage run guess store particular param start stage run ill look fix master avail releas thank detail report good hear thank",
        "Answer_gpt_summary_original":"possible solutions: \n- the issue is a bug and the person answering the question will investigate and fix it. \n- the fix for this bug is available in the next release (0.7.5.dev3).",
        "Answer_gpt_summary":"possibl solut issu bug person answer question investig fix fix bug avail releas dev"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Designer: Export Code",
        "Question_body":"Is there an option to export the Azure ML Designer to code so we can copy between workspaces?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1646150939773,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi, this feature is currently not supported as mentioned on this thread. However, it's on the roadmap.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/755142\/azure-ml-designer-export-code.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-02T06:20:05.287Z",
                "Answer_score":1,
                "Answer_body":"Hi, this feature is currently not supported as mentioned on this thread. However, it's on the roadmap.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-11-10T18:23:47.713Z",
                "Answer_score":0,
                "Answer_body":"Is the roadmap public? When is this feature planning on being released?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1646202005287,
        "Question_original_content":"design export code option export design code copi workspac",
        "Question_preprocessed_content":"design export code option export design code copi workspac",
        "Question_gpt_summary_original":"The user is facing a challenge with the Azure ML Designer as they are unable to export it to code for copying between workspaces.",
        "Question_gpt_summary":"user face challeng design unabl export code copi workspac",
        "Answer_original_content":"featur current support mention thread roadmap",
        "Answer_preprocessed_content":"featur current support mention thread roadmap",
        "Answer_gpt_summary_original":"possible solutions: none.\n\nsummary: the user is looking for an option to export the designer to code, but currently, there is no such feature available. however, it is on the roadmap for future development.",
        "Answer_gpt_summary":"possibl solut summari user look option export design code current featur avail roadmap futur develop"
    },
    {
        "Question_id":70976273.0,
        "Question_title":"`Model.deploy()` failing for AutoML with `400 'automatic_resources' is not supported for Model`",
        "Question_body":"<p>Trying to use the python SDK to deploy an AutoML model, but am recieving the error<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 'automatic_resources' is not supported for Model\n<\/code><\/pre>\n<p>Here's what I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.init(project=&quot;MY_PROJECT&quot;)\n\n# model is an AutoML tabular model\nmodel = aiplatform.Model(&quot;MY_MODEL_ID&quot;)\n\n# this fails\nmodel.deploy()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643912020147,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":149.0,
        "Answer_body":"<p>You encounter the <code>automatic_resources<\/code> because deploying models for AutoML Tables requires the user to define the machine type at model deployment. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#specify\" rel=\"nofollow noreferrer\">configure compute resources for prediction docs<\/a> for more information.<\/p>\n<blockquote>\n<p>If you want to use a custom-trained model or an AutoML tabular model\nto serve online predictions, you must specify a machine type when you\ndeploy the Model resource as a DeployedModel to an Endpoint.<\/p>\n<\/blockquote>\n<p>You should add defining of resources to your code for you to continue the deployment. Adding <code>machine_type<\/code> parameter should suffice. See <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/configure-compute#machine-types\" rel=\"nofollow noreferrer\">supported machine types docs<\/a> for complete list.<\/p>\n<p>See code below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nproject = &quot;your-project-id&quot;\nlocation = &quot;us-central1&quot;\nmodel_id = &quot;99999999&quot;\n\naiplatform.init(project=project, location=location)\nmodel_name = (f&quot;projects\/{project}\/locations\/{location}\/models\/{model_id}&quot;)\n\nmodel = aiplatform.Model(model_name=model_name)\n\nmodel.deploy(machine_type=&quot;n1-standard-4&quot;)\n\nmodel.wait()\nprint(model.display_name)\n<\/code><\/pre>\n<p>Output:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NHpJF.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70976273",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643942120972,
        "Question_original_content":"model deploi fail automl automat resourc support model try us python sdk deploi automl model reciev error googl api core except invalidargu automat resourc support model run googl cloud import aiplatform aiplatform init project project model automl tabular model model aiplatform model model fail model deploi",
        "Question_preprocessed_content":"fail automl try us python sdk deploi automl model reciev error run",
        "Question_gpt_summary_original":"The user is encountering an error while trying to deploy an AutoML model using the python SDK. The error message states that \"automatic_resources\" is not supported for the model, resulting in a 400 InvalidArgument error. The user has provided the code they are running, which includes initializing the project and model, but the deployment fails.",
        "Question_gpt_summary":"user encount error try deploi automl model python sdk error messag state automat resourc support model result invalidargu error user provid code run includ initi project model deploy fail",
        "Answer_original_content":"encount automat resourc deploi model automl tabl requir user defin machin type model deploy configur comput resourc predict doc inform want us custom train model automl tabular model serv onlin predict specifi machin type deploi model resourc deployedmodel endpoint add defin resourc code continu deploy ad machin type paramet suffic support machin type doc complet list code googl cloud import aiplatform project project locat central model aiplatform init project project locat locat model project project locat locat model model model aiplatform model model model model deploi machin type standard model wait print model displai output",
        "Answer_preprocessed_content":"encount deploi model automl tabl requir user defin machin type model deploy configur comput resourc predict doc inform want us model automl tabular model serv onlin predict specifi machin type deploi model resourc deployedmodel endpoint add defin resourc code continu deploy ad paramet suffic support machin type doc complet list code output",
        "Answer_gpt_summary_original":"possible solutions to the error encountered when deploying an automl tabular model using the python sdk include defining the machine type at model deployment and adding the machine_type parameter to the code. the user should refer to the \"configure compute resources for prediction\" and \"supported machine types\" documentation for more information. the provided code snippet demonstrates how to add the machine_type parameter to the deployment code.",
        "Answer_gpt_summary":"possibl solut error encount deploi automl tabular model python sdk includ defin machin type model deploy ad machin type paramet code user refer configur comput resourc predict support machin type document inform provid code snippet demonstr add machin type paramet deploy code"
    },
    {
        "Question_id":null,
        "Question_title":"MLflow feature roadmap",
        "Question_body":"Hi there,\u00a0\n\n\nIs there a roadmap of features that are planned for MLflow and that you maybe would like help with?\u00a0\n\n\n-- Bruno",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1531377865000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":314.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/FG3X_OaEgtw",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-07-12T20:17:58",
                "Answer_body":"Hi Bruno,\n\nWe posted some slides with Databricks\u2019 short term roadmap at https:\/\/www.slideshare.net\/databricks\/introduction-fo-mlflow\/20. IT would be great to get help on many of these areas. Are there specific ones you\u2019re interested in? Depending on the complexity, it might be good to create a separate thread about them on GitHub to discuss the requirements and design.\n\nMatei\n\n\ue5d3\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/5ee3c2b6-fac4-4f02-aeb7-a29f89590bdc%40googlegroups.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"featur roadmap roadmap featur plan mayb like help bruno",
        "Question_preprocessed_content":"featur roadmap roadmap featur plan mayb like help bruno",
        "Question_gpt_summary_original":"The user is inquiring about a roadmap of features planned for MLflow and is offering to provide assistance. No specific challenges are mentioned in the given text.",
        "Question_gpt_summary":"user inquir roadmap featur plan offer provid assist specif challeng mention given text",
        "Answer_original_content":"bruno post slide databrick short term roadmap http slideshar net databrick introduct great help area specif on your interest depend complex good creat separ thread github discuss requir design matei receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user eecb fac aeb afbdc googlegroup com option visit http group googl com optout",
        "Answer_preprocessed_content":"bruno post slide databrick short term roadmap great help area specif on your interest depend complex good creat separ thread github discuss requir design matei receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"No solutions are mentioned in the given text.",
        "Answer_gpt_summary":"solut mention given text"
    },
    {
        "Question_id":null,
        "Question_title":"How can I use a already trained model of AutoML in Designer pipelines, for retraining and deploy?",
        "Question_body":"I have a model that is already been trained with Auto ML (Voting Ensemble algorithm). This model is already deployed and in production.\nI would like to know how can I retrain this model using the Designer pipelines? More specificaly, how can I import this model, builded on Auto ML, with some of the components in the designer for retraining?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663853333980,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1019338\/how-can-i-use-a-already-trained-model-of-automl-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-23T08:00:28.367Z",
                "Answer_score":0,
                "Answer_body":"Hello @RafaelNakamura-5502\n\nThanks for using Microsoft Q&A platform. I understand you have already trained your model with AutoML. Luckily in V2 preview, you can deploy an AutoML-trained machine learning model to an online (real-time inference) endpoint.\n\nThe guidance is here - https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-automl-endpoint?tabs=Studio\n\nBut please be aware that SDK v2 is currently in public preview. The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see Supplemental Terms of Use for Microsoft Azure Previews- https:\/\/azure.microsoft.com\/support\/legal\/preview-supplemental-terms\/\n\nIf you are aimming to a stable environment and want to use SDK v1, you can import your model to designer as this guidance - https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-models?tabs=use-local\n\nTo create a model in Machine Learning, from the UI, open the Models page. Select Register model, and select where your model is located. Fill out the required fields, and then select Register. Then you can find the model in your conponent and use it directly in designer.\n\nI hope above information helps. Please let us know if you have more questions.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us train model automl design pipelin retrain deploi model train auto vote ensembl algorithm model deploi product like know retrain model design pipelin specificali import model build auto compon design retrain",
        "Question_preprocessed_content":"us train model automl design pipelin retrain deploi model train auto model deploi product like know retrain model design pipelin specificali import model build auto compon design retrain",
        "Question_gpt_summary_original":"The user is facing a challenge of retraining an already deployed and in-production model that was trained using AutoML's Voting Ensemble algorithm. They are seeking guidance on how to import the model into Designer pipelines and use its components for retraining.",
        "Question_gpt_summary":"user face challeng retrain deploi product model train automl vote ensembl algorithm seek guidanc import model design pipelin us compon retrain",
        "Answer_original_content":"hello rafaelnakamura thank microsoft platform understand train model automl luckili preview deploi automl train machin learn model onlin real time infer endpoint guidanc http learn microsoft com azur machin learn deploi automl endpoint tab studio awar sdk current public preview preview version provid servic level agreement recommend product workload certain featur support constrain capabl inform supplement term us microsoft azur preview http azur microsoft com support legal preview supplement term aim stabl environ want us sdk import model design guidanc http learn microsoft com azur machin learn manag model tab us local creat model machin learn open model page select regist model select model locat requir field select regist model conpon us directli design hope inform help let know question regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_preprocessed_content":"hello thank microsoft platform understand train model automl luckili preview deploi machin learn model onlin endpoint guidanc awar sdk current public preview preview version provid servic level agreement recommend product workload certain featur support constrain capabl inform supplement term us microsoft azur preview aim stabl environ want us sdk import model design guidanc creat model machin learn open model page select regist model select model locat requir field select regist model conpon us directli design hope inform help let know question regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. deploy an automl-trained machine learning model to an online endpoint using v2 preview sdk.\n2. import the model to designer using sdk v1 and use it directly in designer.",
        "Answer_gpt_summary":"possibl solut answer deploi automl train machin learn model onlin endpoint preview sdk import model design sdk us directli design"
    },
    {
        "Question_id":null,
        "Question_title":"Find the \"run history at log step\" for a particular artifact using the API?",
        "Question_body":"<p>In the web UI, for a particular dashboard, under metadata I am interested in the \u201cRun history at log step\u201d information. I\u2019d like to retrieve this using the API.<\/p>\n<p>If I use run.scan_history, I get rows with model scores and the epoch and step, but not the artifact ID.<\/p>\n<p>If I have a public API Artifact, I have the artifact ID and version but not the epoch or step. Artifact<\/p>\n<p>How do I find the run history at log step for a particular artifact? I\u2019ve checked the API code and docs but still can\u2019t figure it out.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1675496600113,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":33.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/find-the-run-history-at-log-step-for-a-particular-artifact-using-the-api\/3819",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-07T18:29:14.634Z",
                "Answer_body":"<p>Hello Joseph!<\/p>\n<p>Just to get a better understanding of your question:<\/p>\n<ul>\n<li>Could you share your workspace that you are working on?<\/li>\n<li>What do you mean by \u201cRun history at log step\u201d?<\/li>\n<li>Are you trying to pull an artifact generated by a run or an artifact that you have logged?<\/li>\n<\/ul>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T00:03:04.176Z",
                "Answer_body":"<p>Hi Joseph, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run histori log step particular artifact api web particular dashboard metadata interest run histori log step inform like retriev api us run scan histori row model score epoch step artifact public api artifact artifact version epoch step artifact run histori log step particular artifact iv check api code doc figur",
        "Question_preprocessed_content":"run histori log step particular artifact api web particular dashboard metadata interest run histori log step inform like retriev api us row model score epoch step artifact public api artifact artifact version epoch step artifact run histori log step particular artifact iv check api code doc figur",
        "Question_gpt_summary_original":"The user is facing challenges in retrieving the \"Run history at log step\" information for a particular artifact using the API. They have tried using the run.scan_history method but it does not provide the artifact ID. They also have the artifact ID and version but not the epoch or step. The user is seeking guidance on how to find the run history at log step for a specific artifact.",
        "Question_gpt_summary":"user face challeng retriev run histori log step inform particular artifact api tri run scan histori method provid artifact artifact version epoch step user seek guidanc run histori log step specif artifact",
        "Answer_original_content":"hello joseph better understand question share workspac work mean run histori log step try pull artifact gener run artifact log joseph heard go close request like open convers let know",
        "Answer_preprocessed_content":"hello joseph better understand question share workspac work mean run histori log step try pull artifact gener run artifact log joseph heard go close request like convers let know",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer as the responder is seeking clarification on the user's question.",
        "Answer_gpt_summary":"solut provid answer respond seek clarif user question"
    },
    {
        "Question_id":null,
        "Question_title":"Getting ConnectTimeout in offline mode when trying to log an image",
        "Question_body":"<p>I am running wandb in offline mode since I don\u2019t have an internet connection on the compute nodes that I use for my experiments.<br>\nThis works fine when I\u2019m logging training loss and other things.<br>\nWhen I try to log images, however, I get the following warning <code>wandb: Network error (ConnectTimeout), entering retry loop.<\/code>  and the run waits forever.<\/p>\n<p>The logging happens through:<\/p>\n<pre><code class=\"lang-python\">images = []\nfor i in range(10):\n    images.append(wandb.Image(image[i], caption=f\"{caption}.{i}\"))\nwandb.log({category: images})\n<\/code><\/pre>\n<p>I can even see that the offline mode is active since I get the following output when I stop the run:<\/p>\n<pre><code class=\"lang-auto\">wandb: You can sync this run to the cloud by running:\nwandb: wandb sync \/scratch_emmy\/outputs\/wandb\/offline-run-20230208_142336-9685bcf5ea8d5d35ccc9d93b2d035832\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1675863242911,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/getting-connecttimeout-in-offline-mode-when-trying-to-log-an-image\/3844",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-10T19:43:42.600Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/arnenix\">@arnenix<\/a> , happy to look into this for you. I ran a test on my end and did not run into the same issue you face. I was successful in logging i<a href=\"https:\/\/wandb.ai\/mohammadbakir\/jira-offline-imgs\/runs\/1fae84wm\/overview?workspace=user-mohammadbakir\">mages offline then syncing the run to wandb<\/a> using the same code example you provided.<\/p>\n<p>Which version of wandb are you using? If not <a href=\"https:\/\/github.com\/wandb\/wandb\/releases\" rel=\"noopener nofollow ugc\">our latest<\/a>,  upgrade and try again.  If the problem persists, provide me a copy of your <code>debug.log<\/code> and <code>debug-internal.log<\/code> files of the runs that are failing. These are located in the run\/logs folder under the wandb folder of your working directory. Thanks<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-15T23:38:10.521Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/arnenix\">@arnenix<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-21T07:37:56.683Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> ,<\/p>\n<p>sorry for not responding earlier.<br>\nI also opened an issue on github and they were able to reproduce my issue and are working on a fix.<br>\nFor further info see: <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4946\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: Getting ConnectTimeout in offline mode when trying to log an image \u00b7 Issue #4946 \u00b7 wandb\/wandb \u00b7 GitHub<\/a><\/p>\n<p>I think this thread can be closed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get connecttimeout offlin mode try log imag run offlin mode dont internet connect comput node us experi work fine log train loss thing try log imag follow warn network error connecttimeout enter retri loop run wait forev log happen imag rang imag append imag imag caption caption log categori imag offlin mode activ follow output stop run sync run cloud run sync scratch emmi output offlin run bcfeadddbd",
        "Question_preprocessed_content":"get connecttimeout offlin mode try log imag run offlin mode dont internet connect comput node us experi work fine log train loss thing try log imag follow warn run wait forev log happen offlin mode activ follow output stop run",
        "Question_gpt_summary_original":"The user is encountering a challenge when trying to log images in offline mode using wandb. They receive a ConnectTimeout warning and the run waits indefinitely. The user is able to log other data successfully in offline mode. The logging is done using a loop to append images to a list and then logging the list using wandb.log(). The user is able to confirm that offline mode is active.",
        "Question_gpt_summary":"user encount challeng try log imag offlin mode receiv connecttimeout warn run wait indefinit user abl log data successfulli offlin mode log loop append imag list log list log user abl confirm offlin mode activ",
        "Answer_original_content":"arnenix happi look ran test end run issu face success log imag offlin sync run code exampl provid version latest upgrad try problem persist provid copi debug log debug intern log file run fail locat run log folder folder work directori thank arnenix heard go close request like open convers let know mohammadbakir sorri respond earlier open issu github abl reproduc issu work fix info cli get connecttimeout offlin mode try log imag issu github think thread close",
        "Answer_preprocessed_content":"happi look ran test end run issu face success log imag offlin sync run code exampl provid version latest upgrad try problem persist provid copi file run fail locat folder folder work directori thank heard go close request like convers let know sorri respond earlier open issu github abl reproduc issu work fix info get connecttimeout offlin mode try log imag issu github think thread close",
        "Answer_gpt_summary_original":"possible solutions to the connecttimeout error when attempting to log images while running in offline mode are: upgrading to the latest version, providing debug.log and debug-internal.log files of the failing runs, and checking the issue on github for further information.",
        "Answer_gpt_summary":"possibl solut connecttimeout error attempt log imag run offlin mode upgrad latest version provid debug log debug intern log file fail run check issu github inform"
    },
    {
        "Question_id":72663991.0,
        "Question_title":"Using external libraries for model training in aws sagemaker",
        "Question_body":"<p>I am just getting started with aws sagemaker and realized it doesn't have a random forest classifier. I found this github tutorial on creating your own and deploying it in sagemaker: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a>.<\/p>\n<p>I am using the python sdk and was more or less curious to see if anyone actually uses this or any external libraries for training with sagemaker. It seems that if you aren't using the built in algorithms then it is very involved to create your own and the functionality of the model and ability to interpret it is very limited once you do get it trained.<\/p>\n<p>For example after deploying the model to an aws endpoint and pulling down the artifacts I could only call the <code>predict<\/code> method (no <code>predict_probab<\/code> as is possible in the actual <code>sklearn randomforestclassifier<\/code>). I also haven't been able to find anything like what you get in <code>sklearn.metrics<\/code> such as <code>accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix<\/code> etc so I'm assuming one would need to build equivalents to these from scratch to be able to interpret their model.<\/p>\n<p>I have spent a week or so researching this and trying to get this rigged up and it just seems that importing external libraries for model training in sagemaker is not very popular or well-documented online. Interested to know if I'm just unaware of more functionality or if there are alternatives that people prefer or if I should just stick with the built in xgboost classifier if I am looking for a tree-based option. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655494198873,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":50.0,
        "Answer_body":"<p>It is possible to add extra packages for training when using any of the Framework containers such as SKLearn. Kindly see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#using-third-party-libraries\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>\n<p>From a hosting perspective, you do have the ability to provide a custom entry_point \/ inference.py script that you can use to control model loading, pre and post processing. Please see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">link<\/a> for more information<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72663991",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1657063869132,
        "Question_original_content":"extern librari model train get start realiz random forest classifi github tutori creat deploi http github com aw amazon exampl blob main python sdk scikit learn randomforest sklearn endend ipynb python sdk curiou actual us extern librari train aren built algorithm involv creat function model abil interpret limit train exampl deploi model aw endpoint pull artifact predict method predict probab possibl actual sklearn randomforestclassifi haven abl like sklearn metric accuraci score roc auc score precis score recal score score confus matrix assum need build equival scratch abl interpret model spent week research try rig import extern librari model train popular document onlin interest know unawar function altern peopl prefer stick built xgboost classifi look tree base option thank",
        "Question_preprocessed_content":"extern librari model train get start realiz random forest classifi github tutori creat deploi python sdk curiou actual us extern librari train aren built algorithm involv creat function model abil interpret limit train exampl deploi model aw endpoint pull artifact method haven abl like assum need build equival scratch abl interpret model spent week research try rig import extern librari model train popular onlin interest know unawar function altern peopl prefer stick built xgboost classifi look option thank",
        "Question_gpt_summary_original":"The user is facing challenges while using external libraries for model training in AWS Sagemaker. They found a Github tutorial on creating their own random forest classifier and deploying it in Sagemaker, but they realized that it is very involved to create their own model and the functionality of the model and ability to interpret it is very limited once it is trained. The user is unable to find anything like what they get in sklearn.metrics, such as accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, etc. The user has spent a week researching this and trying to get it rigged up, but it seems that importing external libraries for model training in Sagemaker is not very popular or well-documented online.",
        "Question_gpt_summary":"user face challeng extern librari model train github tutori creat random forest classifi deploi realiz involv creat model function model abil interpret limit train user unabl like sklearn metric accuraci score roc auc score precis score recal score score confus matrix user spent week research try rig import extern librari model train popular document onlin",
        "Answer_original_content":"possibl add extra packag train framework contain sklearn kindli link inform host perspect abil provid custom entri point infer script us control model load pre post process link inform",
        "Answer_preprocessed_content":"possibl add extra packag train framework contain sklearn kindli link inform host perspect abil provid custom script us control model load pre post process link inform",
        "Answer_gpt_summary_original":"possible solutions to the user's challenges with using external libraries for model training in the given context are: \n1. adding extra packages for training when using any of the framework containers such as sklearn. \n2. providing a custom entry_point \/ inference.py script from a hosting perspective to control model loading, pre and post processing.",
        "Answer_gpt_summary":"possibl solut user challeng extern librari model train given context ad extra packag train framework contain sklearn provid custom entri point infer script host perspect control model load pre post process"
    },
    {
        "Question_id":36297520.0,
        "Question_title":"Using Sacred Module with iPython",
        "Question_body":"<p>I am trying to set up <a href=\"https:\/\/pypi.python.org\/pypi\/sacred\" rel=\"nofollow noreferrer\"><code>sacred<\/code><\/a> for Python and I am going through the <a href=\"http:\/\/sacred.readthedocs.org\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">tutorial<\/a>. I was able to set up sacred using <code>pip install sacred<\/code> with no issues. I am having trouble running the basic code:<\/p>\n\n<pre><code>from sacred import Experiment\n\nex = Experiment(\"hello_world\")\n<\/code><\/pre>\n\n<p>Running this code returns the a <code>ValueError<\/code>:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-25-66f549cfb192&gt; in &lt;module&gt;()\n      1 from sacred import Experiment\n      2 \n----&gt; 3 ex = Experiment(\"hello_world\")\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/experiment.pyc in __init__(self, name, ingredients)\n     42         super(Experiment, self).__init__(path=name,\n     43                                          ingredients=ingredients,\n---&gt; 44                                          _caller_globals=caller_globals)\n     45         self.default_command = \"\"\n     46         self.command(print_config, unobserved=True)\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/ingredient.pyc in __init__(self, path, ingredients, _caller_globals)\n     48         self.doc = _caller_globals.get('__doc__', \"\")\n     49         self.sources, self.dependencies = \\\n---&gt; 50             gather_sources_and_dependencies(_caller_globals)\n     51 \n     52     # =========================== Decorators ==================================\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/dependencies.pyc in gather_sources_and_dependencies(globs)\n    204 def gather_sources_and_dependencies(globs):\n    205     dependencies = set()\n--&gt; 206     main = Source.create(globs.get('__file__'))\n    207     sources = {main}\n    208     experiment_path = os.path.dirname(main.filename)\n\n\/Users\/ryandevera\/anaconda\/lib\/python2.7\/site-packages\/sacred\/dependencies.pyc in create(filename)\n     61         if not filename or not os.path.exists(filename):\n     62             raise ValueError('invalid filename or file not found \"{}\"'\n---&gt; 63                              .format(filename))\n     64 \n     65         mainfile = get_py_file_if_possible(os.path.abspath(filename))\n\nValueError: invalid filename or file not found \"None\"\n<\/code><\/pre>\n\n<p>I am not sure why this error is returning. The documentation does not say anything about setting up an Experiment file prior to running the code. Any help would be greatly appreciated!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":5,
        "Question_creation_time":1459297642757,
        "Question_favorite_count":null,
        "Question_last_edit_time":1505634379180,
        "Question_score":4.0,
        "Question_view_count":1818.0,
        "Answer_body":"<p>The traceback given indicates that the constructor for <code>Experiment<\/code> searches its namespace to find the file in which its defined.<\/p>\n\n<p>Thus, to make the example work, place the example code into a file and run that file directly.<\/p>\n\n<p>If you are using <code>ipython<\/code>, then you could always try using the <code>%%python<\/code> command, which will effectively capture the code you give it into a file before running it (in a separate python process).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36297520",
        "Tool":"Sacred",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1459298810363,
        "Question_original_content":"modul ipython try set python go tutori abl set pip instal issu have troubl run basic code import experi experi hello world run code return valueerror valueerror traceback recent import experi experi hello world user ryandevera anaconda lib python site packag experi pyc init self ingredi super experi self init path ingredi ingredi caller global caller global self default command self command print config unobserv true user ryandevera anaconda lib python site packag ingredi pyc init self path ingredi caller global self doc caller global doc self sourc self depend gather sourc depend caller global decor user ryandevera anaconda lib python site packag depend pyc gather sourc depend glob def gather sourc depend glob depend set main sourc creat glob file sourc main experi path path dirnam main filenam user ryandevera anaconda lib python site packag depend pyc creat filenam filenam path exist filenam rais valueerror invalid filenam file format filenam mainfil file possibl path abspath filenam valueerror invalid filenam file sure error return document set experi file prior run code help greatli appreci",
        "Question_preprocessed_content":"modul ipython try set python go tutori abl set issu have troubl run basic code run code return sure error return document set experi file prior run code help greatli appreci",
        "Question_gpt_summary_original":"The user is encountering a ValueError when trying to run basic code using the Sacred module in Python. The error message states that the filename is invalid or not found. The user is unsure why this error is occurring and is seeking assistance.",
        "Question_gpt_summary":"user encount valueerror try run basic code modul python error messag state filenam invalid user unsur error occur seek assist",
        "Answer_original_content":"traceback given indic constructor experi search namespac file defin exampl work place exampl code file run file directli ipython try python command effect captur code file run separ python process",
        "Answer_preprocessed_content":"traceback given indic constructor search namespac file defin exampl work place exampl code file run file directli try command effect captur code file run",
        "Answer_gpt_summary_original":"possible solutions to the valueerror encountered by the user when setting up a module for python and running basic code are: placing the example code into a file and running that file directly, or using the %%python command if using ipython, which captures the code into a file before running it in a separate python process. the traceback indicates that the constructor for experiment searches its namespace to find the file in which it's defined.",
        "Answer_gpt_summary":"possibl solut valueerror encount user set modul python run basic code place exampl code file run file directli python command ipython captur code file run separ python process traceback indic constructor experi search namespac file defin"
    },
    {
        "Question_id":null,
        "Question_title":"[problem at MMS predict] At MMS(sagemaker), error code(500), type(InternalServerException)",
        "Question_body":"I make pytorch model with sagemaker, MMS. This is my mms code.\n\n%%time\ninstance_type = 'c5.large'\n# accelerator_type = 'eia2.medium'\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=f\"ml.{instance_type}\"\n)\n\nmme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\")\nlist(mme.list_models())\n#> [ 'model.tar.gz']\n\nI try to predict with this code.\n\nstart_time = time.time()\npredicted_value = predictor.predict(requests, target_model=\"LV1\")\nduration = time.time() - start_time\nprint(\"${:,.2f}, took {:,d} ms\\n\".format(predicted_value[0], int(duration * 1000)))\n\nAnd, return error message.\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers\"\n}\n\n\nMMS with pytorch is 'little' difficult. X)\n\nhelp me, please.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660209790025,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":80.0,
        "Answer_body":"Hi , I think your target model on the prediction needs to have the name of the model you have deployed - for example , when you are adding the model with mme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\") the model_data_path contains the name of the model . From the sagemaker-examples: (https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb) **model_data_path is the relative path to the S3 prefix we specified above (i.e. model_data_prefix) where our endpoint will source models for inference requests.Since this is a relative path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. Chicago_IL.tar.gz). In your case \"model.tar.gz\". However, when predicting you call the model ,target_model=\"LV1\"?",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBCxtcfyrTymZ7isHG3X5Qg\/problem-at-mms-predict-at-mms-sagemaker-error-code-500-type-internal-server-exception",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-12T14:02:03.533Z",
                "Answer_score":1,
                "Answer_body":"Hi , I think your target model on the prediction needs to have the name of the model you have deployed - for example , when you are adding the model with mme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\") the model_data_path contains the name of the model . From the sagemaker-examples: (https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb) **model_data_path is the relative path to the S3 prefix we specified above (i.e. model_data_prefix) where our endpoint will source models for inference requests.Since this is a relative path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. Chicago_IL.tar.gz). In your case \"model.tar.gz\". However, when predicting you call the model ,target_model=\"LV1\"?",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-08-16T02:33:28.778Z",
                "Answer_score":0,
                "Answer_body":"Accoding to your comment, I modify code and excution. I try 2 solution.\n\n#1 predictor.predict\n\npredicted_value = predictor.predict(data=requests, target_model=\"modal.tar.gz\")\n\nreturn\n\nValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Failed to download model data(bucket: sagemaker-ap-northeast-2-344487737937, key: LouisVuiotton-cpu-2022-08-16-02-02-04-408-c6i-large\/model\/modal.tar.gz). Please ensure that there is an object located at the URL and that the role passed to CreateModel has permissions to download the model.\n\n\n#2 With boto3, invoke_endpoint()\n\nimport boto3\n\nclient = boto3.client('sagemaker-runtime')\nendpoint_name = predictor.endpoint_name\nresponse = client.invoke_endpoint(\n    EndpointName=endpoint_name,\n    Body=requests,\n    ContentType='application\/x-image',\n#     Accept='string',\n#     CustomAttributes='string',\n    TargetModel='model.tar.gz',\n#     TargetVariant='string',\n#     TargetContainerHostname='string',\n#     InferenceId='string'\n)\n\nreturn\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers\"\n}\n\". See https:\/\/ap-northeast-2.console.aws.amazon.com\/cloudwatch\/home?region=ap-northeast-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/LV-multi-2022-08-16-02-11-15 in account 344487737937 for more information.\n\n\nI assume sol 2, boto3.invoke_endpoint's result [ \"message\": \"Failed to start workers\" ] come from sol 1, [that the role passed to CreateModel has permissions to download the model.].\n\nI already use excution role [''arn:aws:iam::344487737937:role\/service-role\/AmazonSageMaker-ExecutionRole-20220713T151818\"]. How to I get additional role (that the role passed to CreateModel has permissions to download the model.)?",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660312923532,
        "Question_original_content":"problem mm predict mm error code type internalserverexcept pytorch model mm mm code time instanc type larg acceler type eia medium predictor mme deploi initi instanc count instanc type instanc type mme add model model data sourc model path model data path model tar list mme list model model tar try predict code start time time time predict valu predictor predict request target model durat time time start time print took format predict valu int durat return error messag modelerror error occur modelerror call invokeendpoint oper receiv server error model messag code type internalserverexcept messag fail start worker mm pytorch littl difficult help",
        "Question_preprocessed_content":"mm error code type pytorch model mm mm code time predictor try predict code durat took int return error messag modelerror error occur call invokeendpoint oper receiv server error model messag code type internalserverexcept messag fail start worker mm pytorch littl difficult help",
        "Question_gpt_summary_original":"The user encountered an error code 500 with type InternalServerException while trying to predict with a pytorch model using MMS on Sagemaker. The error message stated that the workers failed to start, and the user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error code type internalserverexcept try predict pytorch model mm error messag state worker fail start user seek help resolv issu",
        "Answer_original_content":"think target model predict need model deploi exampl ad model mme add model model data sourc model path model data path model tar model data path contain model exampl http github com aw amazon exampl blob main advanc function multi model xgboost home valu xgboost multi model endpoint home valu ipynb model data path rel path prefix specifi model data prefix endpoint sourc model infer request rel path simpli pass wish model artifact infer time chicago tar case model tar predict model target model",
        "Answer_preprocessed_content":"think target model predict need model deploi exampl ad model contain model exampl rel path prefix specifi endpoint sourc model infer rel path simpli pass wish model artifact infer time case predict model",
        "Answer_gpt_summary_original":"the solution to the error code (500) and type (internalserverexception) encountered when using the mms() function to deploy a pytorch model is to ensure that the target model on the prediction has the same name as the model deployed. the model name should be specified in the model_data_path parameter when adding the model using mme.add_model(). the name of the model artifact should also be passed at inference time.",
        "Answer_gpt_summary":"solut error code type internalserverexcept encount mm function deploi pytorch model ensur target model predict model deploi model specifi model data path paramet ad model mme add model model artifact pass infer time"
    },
    {
        "Question_id":48310237.0,
        "Question_title":"Sagemaker \"Could not find model data\" when trying to deploy my model",
        "Question_body":"<p>I have a training script in Sagemaker like,<\/p>\n\n<pre><code>def train(current_host, hosts, num_cpus, num_gpus, channel_input_dirs, model_dir, hyperparameters, **kwargs):\n    ... Train a network ...\n    return net\n\ndef save(net, model_dir):\n    # save the model\n    logging.info('Saving model')\n    y = net(mx.sym.var('data'))\n    y.save('%s\/model.json' % model_dir)\n    net.collect_params().save('%s\/model.params' % model_dir)\n\ndef model_fn(model_dir):\n    symbol = mx.sym.load('%s\/model.json' % model_dir)\n    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\n    inputs = mx.sym.var('data')\n    param_dict = gluon.ParameterDict('model_')\n    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n    net.load_params('%s\/model.params' % model_dir, ctx=mx.cpu())\n    return net\n<\/code><\/pre>\n\n<p>Most of which I stole from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">MNIST Example<\/a>.<\/p>\n\n<p>When I train, everything goes fine, but when trying to deploy like,<\/p>\n\n<pre><code>m = MXNet(\"lstm_trainer.py\", \n          role=role, \n          train_instance_count=1, \n          train_instance_type=\"ml.c4.xlarge\",\n          hyperparameters={'batch_size': 100, \n                         'epochs': 20, \n                         'learning_rate': 0.1, \n                         'momentum': 0.9, \n                         'log_interval': 100})\nm.fit(inputs) # No errors\npredictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>I get, (<a href=\"https:\/\/gist.github.com\/aidan-plenert-macdonald\/7eb7ba7402790b61596938b5cbf605b6\" rel=\"nofollow noreferrer\">full output<\/a>)<\/p>\n\n<pre><code>INFO:sagemaker:Creating model with name: sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\n---------------------------------------------------------------------------\n  ... Stack dump ...\nClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz.\n<\/code><\/pre>\n\n<p>Looking in my S3 bucket <code>s3:\/\/sagemaker-us-west-2-01234567890\/sagemaker-mxnet-py2-cpu-2018-01-17-20-52-52-599\/output\/model.tar.gz<\/code>, I in fact don't see the model.<\/p>\n\n<p>What am I missing?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1516224347000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":2950.0,
        "Answer_body":"<p>When you are calling the training job you should specify the output directory:<\/p>\n\n<pre><code>#Bucket location where results of model training are saved.\nmodel_artifacts_location = 's3:\/\/&lt;bucket-name&gt;\/artifacts'\n\nm = MXNet(entry_point='lstm_trainer.py',\n          role=role,\n          output_path=model_artifacts_location,\n          ...)\n<\/code><\/pre>\n\n<p>If you don't specify the output directory the function will use a default location, that it might not have the permissions to create or write to.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48310237",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1516602490380,
        "Question_original_content":"model data try deploi model train script like def train current host host num cpu num gpu channel input dir model dir hyperparamet kwarg train network return net def save net model dir save model log info save model net sym var data save model json model dir net collect param save model param model dir def model model dir symbol sym load model json model dir output symbol softmax data symbol softmax label input sym var data param dict gluon parameterdict model net gluon symbolblock output input param dict net load param model param model dir ctx cpu return net stole mnist exampl train goe fine try deploi like mxnet lstm trainer role role train instanc count train instanc type xlarg hyperparamet batch size epoch learn rate momentum log interv fit input error predictor deploi initi instanc count instanc type xlarg output info creat model mxnet cpu stack dump clienterror error occur validationexcept call createmodel oper model data west mxnet cpu output model tar look bucket west mxnet cpu output model tar fact model miss",
        "Question_preprocessed_content":"model data try deploi model train script like stole mnist exampl train goe fine try deploi like look bucket fact model miss",
        "Question_gpt_summary_original":"The user is encountering a challenge with Sagemaker where they are unable to deploy their model due to an error message stating that the model data could not be found at the specified S3 bucket location. The user has checked the bucket and confirmed that the model is not present.",
        "Question_gpt_summary":"user encount challeng unabl deploi model error messag state model data specifi bucket locat user check bucket confirm model present",
        "Answer_original_content":"call train job specifi output directori bucket locat result model train save model artifact locat artifact mxnet entri point lstm trainer role role output path model artifact locat specifi output directori function us default locat permiss creat write",
        "Answer_preprocessed_content":"call train job specifi output directori specifi output directori function us default locat permiss creat write",
        "Answer_gpt_summary_original":"the solution to the challenge of not finding model data during deployment is to specify the output directory when calling the training job. the user should provide the bucket location where the results of model training are saved. if the output directory is not specified, the function will use a default location that may not have the necessary permissions to create or write to.",
        "Answer_gpt_summary":"solut challeng find model data deploy specifi output directori call train job user provid bucket locat result model train save output directori specifi function us default locat necessari permiss creat write"
    },
    {
        "Question_id":65992776.0,
        "Question_title":"MLflow change experiment id",
        "Question_body":"<p>I am unable to change the experiment id of a MLflow experiment.<\/p>\n<p>Currently, I am running the following code to create an experiment before logging:<\/p>\n<pre><code>mlflow.set_experiment(experiment_name=&quot;my_model&quot;)\n\nwith mlflow.start_run():\n   #train model\n<\/code><\/pre>\n<p>Doing so allows me to create a new experiment, but the experiment id will always be 1.<\/p>\n<p>The yaml file created looks like this:<\/p>\n<pre><code>artifact_location: file:\/\/\/project\/src\/mlruns\/1\nexperiment_id: '1'\nlifecycle_stage: active\nname: my_model\n<\/code><\/pre>\n<p>I have tried to look at the MLflow documentation, but I cannot find examples or functions where the experiment id is altered.<\/p>\n<p>I would greatly appreciate any help or tips with this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1612182622783,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":2003.0,
        "Answer_body":"<p>You should call you <code>experiment_id<\/code> in the <code>start_run()<\/code>:<\/p>\n<pre><code>mlflow.set_experiment(&quot;experiment name&quot;)\nexperiment = mlflow.get_experiment_by_name(&quot;experiment name&quot;)\n\nwith mlflow.start_run(experiment_id=experiment.experiment_id):\n     # train model\n<\/code><\/pre>\n<p><strong>Note<\/strong>: If you use <code>set_tracking_uri()<\/code>, you should <code>set_experiment()<\/code> after that.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65992776",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1625475911003,
        "Question_original_content":"chang experi unabl chang experi experi current run follow code creat experi log set experi experi model start run train model allow creat new experi experi yaml file creat look like artifact locat file project src mlrun experi lifecycl stage activ model tri look document exampl function experi alter greatli appreci help tip",
        "Question_preprocessed_content":"chang experi unabl chang experi experi current run follow code creat experi log allow creat new experi experi yaml file creat look like tri look document exampl function experi alter greatli appreci help tip",
        "Question_gpt_summary_original":"The user is facing a challenge in changing the experiment id of a MLflow experiment. They have tried creating a new experiment using code, but the experiment id remains 1. The user has searched for solutions in the MLflow documentation but has not found any examples or functions to alter the experiment id.",
        "Question_gpt_summary":"user face challeng chang experi experi tri creat new experi code experi remain user search solut document exampl function alter experi",
        "Answer_original_content":"experi start run set experi experi experi experi experi start run experi experi experi train model note us set track uri set experi",
        "Answer_preprocessed_content":"note us",
        "Answer_gpt_summary_original":"to change the experiment id of an experiment when running code to create a new experiment, the user should call their experiment id in the start_run() and set the experiment name using .set_experiment(). they can then get the experiment by name using .get_experiment_by_name() and start the run using .start_run() with the experiment id. if set_tracking_uri() is used, set_experiment() should be called after that.",
        "Answer_gpt_summary":"chang experi experi run code creat new experi user experi start run set experi set experi experi experi start run start run experi set track uri set experi call"
    },
    {
        "Question_id":null,
        "Question_title":"Get best model from artifacts",
        "Question_body":"<p>Hi,<\/p>\n<p>I guess this use-case is common but I cannot figure it out\u2026<\/p>\n<p>I would like to log one model per run, and in the end, be able to load the best overall model for production.<\/p>\n<p>So like :<br>\nRun 1,2,3,4,5\u2026<\/p>\n<pre><code class=\"lang-auto\">run.log_artifact(my_model_artifact)\n<\/code><\/pre>\n<p>Production:<\/p>\n<pre><code class=\"lang-auto\">artifact = api.artifact.get_best_of_all_my_runs()\n<\/code><\/pre>\n<p>For now my solution is :<\/p>\n<pre><code class=\"lang-auto\">Runs : \nartifact.save() # with the same name so only one artifact for all runs\n\nProduction\nartifact = api.artifact(\"entity\/project\/artifact:alias\") # Get the only model (which also should be the best)\n<\/code><\/pre>\n<p>Thanks in advance for any help.<br>\nhave a great day<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1634323325709,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":322.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ierezell\">@ierezell<\/a>,<\/p>\n<p>I think I understand your question a little better. We are currently working on a feature that will make exactly what you\u2019re asking for super clean and easy <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>In the meantime the most straightforward way of going about this would be to save the performance of all of the metrics you care about to the <code>run.summary<\/code> of the run that produced that model. Then from the <code>api<\/code> you can query all the runs in the project and select the best run using the metrics you set in the summary. Then you can get the download the best model and use it as an input to your production code.<\/p>\n<p>Here is a <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#download-the-best-model-file-from-a-sweep\">related example<\/a> querying the best model from a sweep but the process will be slightly different for you use case.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/get-best-model-from-artifacts\/992",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-18T23:37:22.999Z",
                "Answer_body":"<p>Hi there! You can do this in a couple of ways but you don\u2019t have to limit yourself to logging just one artifact. A common flow would be to log a model checkpoint every but then to also log a \u201cbest model\u201d artifact. Since artifacts are versioned you don\u2019t have to worry about renaming the new \u201cbest model\u201d artifact. Then at the end of your run you not only have an artifact history of your model at each of the checkpoints but also a versioned history of all the best models.<\/p>",
                "Answer_score":97.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-19T17:22:20.423Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a>,<\/p>\n<p>Thanks a lot for the reply !<\/p>\n<p>I indeed do this and have my <code>best model<\/code> for each run and it works great.<\/p>\n<p>Then I can see all my runs in the <code>Table<\/code> Tab, with their hyperparameters, and a column for the score\/accuracy.<\/p>\n<p>However my questions is : Could I get the best model among all my runs so I can use it in production ?<\/p>\n<p>Let say I have one parameter P :<br>\nI run Run1 with P1, get Accuracy A1 \/\/ Run 2 with P2 get Accuracy A2 \/\/ Run3 with P3 get Accurary A3<\/p>\n<p>Then in the table I can see Run1, Run2 and Run3 each with logs \/ metrics \/ models \/ Parameters.<\/p>\n<p>I would like to be able to do in my production\/inference code :<br>\n\u201cSelect best Accuracy model from Run1, Run2, Run3 so I can use the best model in inference mode in production\u201d<\/p>\n<p>I hope my explanation was clear\u2026<br>\nThanks a lot for your time.,<\/p>\n<p>Have a great day<\/p>",
                "Answer_score":2.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-22T19:28:10.719Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ierezell\">@ierezell<\/a>,<\/p>\n<p>I think I understand your question a little better. We are currently working on a feature that will make exactly what you\u2019re asking for super clean and easy <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>In the meantime the most straightforward way of going about this would be to save the performance of all of the metrics you care about to the <code>run.summary<\/code> of the run that produced that model. Then from the <code>api<\/code> you can query all the runs in the project and select the best run using the metrics you set in the summary. Then you can get the download the best model and use it as an input to your production code.<\/p>\n<p>Here is a <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#download-the-best-model-file-from-a-sweep\">related example<\/a> querying the best model from a sweep but the process will be slightly different for you use case.<\/p>",
                "Answer_score":22.4,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-10-25T14:07:41.400Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a>,<\/p>\n<p>Thanks for the reply, this is indeed what I was looking for ! <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>I will be glad to test the new feature (even in beta) but I\u2019m sure you will communicate when it will be available.<br>\nElse I was thinking about using the API and you just confirmed it so I will be headed for that.<\/p>\n<p>Have a great day.<\/p>",
                "Answer_score":47.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1634930890719,
        "Question_original_content":"best model artifact guess us case common figur like log model run end abl load best overal model product like run run log artifact model artifact product artifact api artifact best run solut run artifact save artifact run product artifact api artifact entiti project artifact alia model best thank advanc help great dai",
        "Question_preprocessed_content":"best model artifact guess common figur like log model run end abl load best overal model product like run product solut thank advanc help great dai",
        "Question_gpt_summary_original":"The user is facing a challenge in logging one model per run and being able to load the best overall model for production. They have tried saving the artifact with the same name for all runs and getting the only model for production, but they are seeking a better solution.",
        "Question_gpt_summary":"user face challeng log model run abl load best overal model product tri save artifact run get model product seek better solut",
        "Answer_original_content":"ierezel think understand question littl better current work featur exactli your ask super clean easi meantim straightforward wai go save perform metric care run summari run produc model api queri run project select best run metric set summari download best model us input product code relat exampl queri best model sweep process slightli differ us case",
        "Answer_preprocessed_content":"think understand question littl better current work featur exactli your ask super clean easi meantim straightforward wai go save perform metric care run produc model queri run project select best run metric set summari download best model us input product code relat exampl queri best model sweep process slightli differ us case",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. wait for a feature that will make logging one model per run and loading the best overall model for production super clean and easy.\n2. save the performance of all the metrics that matter to the run.summary of the run that produced that model.\n3. query all the runs in the project and select the best run using the metrics set in the summary.\n4. download the best model and use it as an input to the production code.",
        "Answer_gpt_summary":"possibl solut answer wait featur log model run load best overal model product super clean easi save perform metric matter run summari run produc model queri run project select best run metric set summari download best model us input product code"
    },
    {
        "Question_id":60355240.0,
        "Question_title":"Pipeline can't find nodes in kedro",
        "Question_body":"<p>I was following <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html\" rel=\"nofollow noreferrer\">pipelines tutorial<\/a>, create all needed files, started the kedro with <code>kedro run --node=preprocessing_data<\/code> but got stuck with such error message:<\/p>\n\n<pre><code>ValueError: Pipeline does not contain nodes named ['preprocessing_data'].\n<\/code><\/pre>\n\n<p>If I run kedro without <code>node<\/code> parameter, I receive<\/p>\n\n<pre><code>kedro.context.context.KedroContextError: Pipeline contains no nodes\n<\/code><\/pre>\n\n<p>Contents of the files:<\/p>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/nodes.py\ndef preprocess_data(data: SparkDataSet) -&gt; None:\n    print(data)\n    return\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/pipeline.py\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=preprocess_data,\n                inputs=\"data\",\n                outputs=\"preprocessed_data\",\n                name=\"preprocessing_data\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipeline.py\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": de_pipeline,\n        \"__default__\": Pipeline([])\n    }\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1582395101757,
        "Question_favorite_count":null,
        "Question_last_edit_time":1583175186660,
        "Question_score":4.0,
        "Question_view_count":2155.0,
        "Answer_body":"<p>I think it looks like you need to have the pipeline in <code>__default__<\/code>.\ne.g.<\/p>\n\n<pre><code>def create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": data_engineering_pipeline,\n        \"__default__\": data_engineering_pipeline\n    }\n<\/code><\/pre>\n\n<p>Then <code>kedro run --node=preprocessing_data<\/code> works for me.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60355240",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1582427680590,
        "Question_original_content":"pipelin node follow pipelin tutori creat need file start run node preprocess data got stuck error messag valueerror pipelin contain node name preprocess data run node paramet receiv context context contexterror pipelin contain node content file src project pipelin data engin node def preprocess data data sparkdataset print data return src project pipelin data engin pipelin def creat pipelin kwarg return pipelin node func preprocess data input data output preprocess data preprocess data src project pipelin def creat pipelin kwarg dict str pipelin pipelin creat pipelin return pipelin default pipelin",
        "Question_preprocessed_content":"pipelin node follow pipelin tutori creat need file start got stuck error messag run paramet receiv content file",
        "Question_gpt_summary_original":"The user encountered an error while following the pipelines tutorial in Kedro. They created all the necessary files and started Kedro with the node parameter, but received an error message stating that the pipeline does not contain nodes named 'preprocessing_data'. Running Kedro without the node parameter resulted in an error message stating that the pipeline contains no nodes. The user provided the contents of the relevant files, including the function to preprocess data, the pipeline creation function, and the function to create pipelines.",
        "Question_gpt_summary":"user encount error follow pipelin tutori creat necessari file start node paramet receiv error messag state pipelin contain node name preprocess data run node paramet result error messag state pipelin contain node user provid content relev file includ function preprocess data pipelin creation function function creat pipelin",
        "Answer_original_content":"think look like need pipelin default def creat pipelin kwarg dict str pipelin pipelin creat pipelin return data engin pipelin default data engin pipelin run node preprocess data work",
        "Answer_preprocessed_content":"think look like need pipelin work",
        "Answer_gpt_summary_original":"the solution to the error encountered when running a pipeline with the node parameter is to have the pipeline in \"__default__\". the user needs to modify the code to include the pipeline in \"__default__\" and then run the command with the appropriate node parameter.",
        "Answer_gpt_summary":"solut error encount run pipelin node paramet pipelin default user need modifi code includ pipelin default run command appropri node paramet"
    },
    {
        "Question_id":null,
        "Question_title":"Account deletion",
        "Question_body":"<p>Hi,<br>\nI created two accounts by accident. Would you please delete this account (kaminski)?<br>\nIs there a cool-down to use the email address bound to this account again? Because I would like to add it, my academic address, as the primary address to my main account (jkaminski).<\/p>\n<p>Cheers<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652167902800,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":183.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/account-deletion\/2386",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-10T08:11:10.638Z",
                "Answer_body":"<p>Hey Johannes, I\u2019ve deleted your account. You should be able to add it as the primary address for the other account<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-09T08:11:51.319Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"account delet creat account accid delet account kaminski cool us email address bound account like add academ address primari address main account jkaminski cheer",
        "Question_preprocessed_content":"account delet creat account accid delet account us email address bound account like add academ address primari address main account cheer",
        "Question_gpt_summary_original":"The user accidentally created two accounts and is requesting to delete one of them (kaminski). They also want to know if there is a cool-down period to use the email address associated with the deleted account again, as they want to add it as the primary address to their main account (jkaminski).",
        "Question_gpt_summary":"user accident creat account request delet kaminski want know cool period us email address associ delet account want add primari address main account jkaminski",
        "Answer_original_content":"hei johann iv delet account abl add primari address account topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei johann iv delet account abl add primari address account topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions: \n- the user's account has been deleted.\n- the email address bound to the deleted account can be added as the primary address for the other account.\n- there is a 60-day cool-down period before new replies are no longer allowed. \n\nsummary: the user's account has been deleted, and they can add the email address to their other account. there is a 60-day cool-down period for new replies.",
        "Answer_gpt_summary":"possibl solut user account delet email address bound delet account ad primari address account dai cool period new repli longer allow summari user account delet add email address account dai cool period new repli"
    },
    {
        "Question_id":50508217.0,
        "Question_title":"Can sagemaker's linear learner be used for multiclass classification?",
        "Question_body":"<p>I am building a multiclass classifier on aws Sagemaker, and would love to use the predefined linearlearner algorithm for classification. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1527161288830,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":410.0,
        "Answer_body":"<p>Yes, it is possible now.<\/p>\n\n<p>You can set the predictor_type hyper-parameter to <code>multiclass_classifier<\/code>.<\/p>\n\n<p>See the documentation here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50508217",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1531423415907,
        "Question_original_content":"linear learner multiclass classif build multiclass classifi love us predefin linearlearn algorithm classif",
        "Question_preprocessed_content":"linear learner multiclass classif build multiclass classifi love us predefin linearlearn algorithm classif",
        "Question_gpt_summary_original":"The user is facing a challenge in determining whether Sagemaker's linear learner algorithm can be used for multiclass classification.",
        "Question_gpt_summary":"user face challeng determin linear learner algorithm multiclass classif",
        "Answer_original_content":"ye possibl set predictor type hyper paramet multiclass classifi document http doc aw amazon com latest hyperparamet html",
        "Answer_preprocessed_content":"ye possibl set document",
        "Answer_gpt_summary_original":"the solution to using amazon's linear learner algorithm for multiclass classification is to set the predictor_type hyper-parameter to \"multiclass_classifier\". the documentation for this can be found at the provided link.",
        "Answer_gpt_summary":"solut amazon linear learner algorithm multiclass classif set predictor type hyper paramet multiclass classifi document provid link"
    },
    {
        "Question_id":69498670.0,
        "Question_title":"Which IAM roles and policies should I delete to not being charged by AWS?",
        "Question_body":"<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsLaunchRole<\/li>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>AWSServiceRoleForAmazonSageMakerNotebooks<\/li>\n<\/ol>\n<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>Plus some execution policies<\/li>\n<\/ol>\n<p>Is Jupyter server within sagemaker studio also be stopped for not being charged?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633708664963,
        "Question_favorite_count":null,
        "Question_last_edit_time":1634116759467,
        "Question_score":0.0,
        "Question_view_count":105.0,
        "Answer_body":"<p><strong>AWS IAM is a free service<\/strong> - you do not get charged for roles, policies or any other aspect of IAM.<\/p>\n<p>From <a href=\"https:\/\/aws.amazon.com\/iam\/#:%7E:text=IAM%20is%20a%20feature%20of,AWS%20services%20by%20your%20users.\" rel=\"nofollow noreferrer\">the documentation<\/a>:<\/p>\n<blockquote>\n<p>IAM is a feature of your AWS account offered at no additional charge. You will be charged only for use of other AWS services by your users.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69498670",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1633709785048,
        "Question_original_content":"iam role polici delet charg aw role delet amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol awsserviceroleforamazonnotebook role delet amazonservicecatalogproductsuserol plu execut polici jupyt server studio stop charg",
        "Question_preprocessed_content":"iam role polici delet charg aw role delet amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol awsserviceroleforamazonnotebook role delet amazonservicecatalogproductsuserol plu execut polici jupyt server studio stop charg",
        "Question_gpt_summary_original":"The user is seeking guidance on which IAM roles and policies to delete in order to avoid being charged by AWS. They are specifically asking if certain roles, such as AmazonSageMakerServiceCatalogProductsLaunchRole and AmazonSageMakerServiceCatalogProductsUseRole, should be deleted, as well as some execution policies. The user also inquires if stopping the Jupyter server within SageMaker Studio will prevent charges.",
        "Question_gpt_summary":"user seek guidanc iam role polici delet order avoid charg aw specif ask certain role amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol delet execut polici user inquir stop jupyt server studio prevent charg",
        "Answer_original_content":"aw iam free servic charg role polici aspect iam document iam featur aw account offer addit charg charg us aw servic user",
        "Answer_preprocessed_content":"aw iam free servic charg role polici aspect iam document iam featur aw account offer addit charg charg us aw servic user",
        "Answer_gpt_summary_original":"summary: there are no iam roles or policies that need to be deleted to avoid being charged by aws as iam is a free service and users will only be charged for the use of other aws services.",
        "Answer_gpt_summary":"summari iam role polici need delet avoid charg aw iam free servic user charg us aw servic"
    },
    {
        "Question_id":68505595.0,
        "Question_title":"failing to create image in azure ml workspace",
        "Question_body":"<p>I am able to create image and run azure ml service in one env but when I am moving to another env its not able to create image and failing with this error -<\/p>\n<p>Message: Received bad response from Model Management Service:\nResponse Code: 500\n{&quot;code&quot;:&quot;InternalServerError&quot;,&quot;statusCode&quot;:500,&quot;message&quot;:&quot;An internal server error occurred. Please try again. If the problem persists, contact support.&quot;,&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;8667981d-ef71-4e7c-a735-c43ef07b51b8&quot;}}'<\/p>\n<p>these logs are not helpful to find issue<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1627079442010,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":110.0,
        "Answer_body":"<p>As the error message said, this issue is an internal issue, please raise a support ticket to assign a support engineer to investigate it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1629745090063,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68505595",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1628037949483,
        "Question_original_content":"fail creat imag workspac abl creat imag run servic env move env abl creat imag fail error messag receiv bad respons model manag servic respons code code internalservererror statuscod messag intern server error occur try problem persist contact support correl requestid cefbb log help issu",
        "Question_preprocessed_content":"fail creat imag workspac abl creat imag run servic env move env abl creat imag fail error messag receiv bad respons model manag servic respons code code internalservererror statuscod messag intern server error occur try problem persist contact log help issu",
        "Question_gpt_summary_original":"The user is facing challenges in creating an image in Azure ML workspace. They are able to create an image and run Azure ML service in one environment, but when moving to another environment, it fails with an internal server error message. The logs provided are not helpful in identifying the issue.",
        "Question_gpt_summary":"user face challeng creat imag workspac abl creat imag run servic environ move environ fail intern server error messag log provid help identifi issu",
        "Answer_original_content":"error messag said issu intern issu rais support ticket assign support engin investig",
        "Answer_preprocessed_content":"error messag said issu intern issu rais support ticket assign support engin investig",
        "Answer_gpt_summary_original":"the solution to the user's challenge of encountering an internal server error with no helpful logs while creating an image and running a service in a different environment is to raise a support ticket to assign a support engineer to investigate the issue.",
        "Answer_gpt_summary":"solut user challeng encount intern server error help log creat imag run servic differ environ rais support ticket assign support engin investig issu"
    },
    {
        "Question_id":null,
        "Question_title":"Dialogflow should have its own official facebook app for integration",
        "Question_body":"Current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. While most of the page's owner are not developer and just want to link some of their page to dialogflowI want to propose that dialogflow should have facebook app with `manage_pages` permission. Have button for oauth with facebook for integration. And just allow user to choose some of their pages to link with dialogflow project. Then all the process in the guideline can be automated. Dialogflow could also config the settings for Webhooks channels it needI want to comment that this was a very roadblock that I have faced when I try to start integrate facebook. The message was not get to dialogflow properly and I don't know I also need `messaging_postbacks` channel, not only `messages`. If Dialogflow app will manage these for us it will be the far much better integration experienceps. Please also add label `Dialogflow` and `Dialogflow ES` to the available label",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1665696420000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-should-have-its-own-official-facebook-app-for\/td-p\/477998\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-13T21:27:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. While most of the page's owner are not developer and just want to link some of their page to dialogflow\n\nI want to propose that dialogflow should have facebook app with `manage_pages` permission. Have button for oauth with facebook for integration. And just allow user to choose some of their pages to link with dialogflow project. Then all the process in the guideline can be automated. Dialogflow could also config the\u00a0settings for\u00a0Webhooks channels it need\n\nI want to comment that this was a very roadblock that I have faced when I try to start integrate facebook. The message was not get to dialogflow properly and I don't know I also need `messaging_postbacks` channel, not only `messages`. If Dialogflow app will manage these for us it will be the far much better integration experience\n\nps. Please also add label `Dialogflow` and `Dialogflow ES` to the available label"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dialogflow offici facebook app integr current dialogflow integr sensibl tediou facebook develop creat facebook app page owner develop want link page dialogflowi want propos dialogflow facebook app manag page permiss button oauth facebook integr allow user choos page link dialogflow project process guidelin autom dialogflow config set webhook channel needi want comment roadblock face try start integr facebook messag dialogflow properli know need messag postback channel messag dialogflow app manag far better integr experiencep add label dialogflow dialogflow avail label",
        "Question_preprocessed_content":"dialogflow offici facebook app integr current dialogflow integr sensibl tediou facebook develop creat facebook app page owner develop want link page dialogflowi want propos dialogflow facebook app permiss button oauth facebook integr allow user choos page link dialogflow project process guidelin autom dialogflow config set webhook channel needi want comment roadblock face try start integr facebook messag dialogflow properli know need channel dialogflow app manag far better integr experiencep add label avail label",
        "Question_gpt_summary_original":"The user is proposing that Dialogflow should have its own official Facebook app with \"manage_pages\" permission to make integration easier for non-developers. The current integration process is tedious and requires users to become Facebook developers and create their own app. The user faced roadblocks in integrating Facebook with Dialogflow, including messages not getting to Dialogflow properly and not knowing that they needed the \"messaging_postbacks\" channel. The user suggests that if Dialogflow manages these issues, it would provide a better integration experience.",
        "Question_gpt_summary":"user propos dialogflow offici facebook app manag page permiss integr easier non develop current integr process tediou requir user facebook develop creat app user face roadblock integr facebook dialogflow includ messag get dialogflow properli know need messag postback channel user suggest dialogflow manag issu provid better integr experi",
        "Answer_original_content":"current dialogflow integr sensibl tediou facebook develop creat facebook app page owner develop want link page dialogflow want propos dialogflow facebook app manag page permiss button oauth facebook integr allow user choos page link dialogflow project process guidelin autom dialogflow config theset forwebhook channel need want comment roadblock face try start integr facebook messag dialogflow properli know need messag postback channel messag dialogflow app manag far better integr experi add label dialogflow dialogflow avail label",
        "Answer_preprocessed_content":"current dialogflow integr sensibl tediou facebook develop creat facebook app page owner develop want link page dialogflow want propos dialogflow facebook app permiss button oauth facebook integr allow user choos page link dialogflow project process guidelin autom dialogflow config theset forwebhook channel need want comment roadblock face try start integr facebook messag dialogflow properli know need channel dialogflow app manag far better integr experi add label avail label",
        "Answer_gpt_summary_original":"possible solutions to the roadblock of integrating facebook with dialogflow include creating a facebook app with `manage_pages` permission, automating the process of linking pages to dialogflow, and allowing dialogflow to configure the necessary webhook channels. the answer suggests that this roadblock was faced due to the tedious process of becoming a facebook developer and creating an app, and that an improved integration experience could be achieved if dialogflow managed the necessary channels.",
        "Answer_gpt_summary":"possibl solut roadblock integr facebook dialogflow includ creat facebook app manag page permiss autom process link page dialogflow allow dialogflow configur necessari webhook channel answer suggest roadblock face tediou process facebook develop creat app improv integr experi achiev dialogflow manag necessari channel"
    },
    {
        "Question_id":52807787.0,
        "Question_title":"Azure machine learning studio get access to the file in upload zip file",
        "Question_body":"<p>I am trying to execute Python script from Azure machine learning studio. I had a script bundle(zip file) connect to the Python script as input. There are python files, txt files and other type of files in this zip file. My question is how do I get the file path from this zip file. For example, if I have language model in this  zip file, named lm.pcl, what's the file path of this language model? \nThanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539557794623,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":679.0,
        "Answer_body":"<p>They're available under the <code>.\/Script Bundle<\/code> directory. For example, if you were to load a pickled model from the zip file, you'd write something along these lines:<\/p>\n\n<pre><code>import pandas as pd\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n\n    model = pickle.load(open(\".\/Script Bundle\/model.pkl\", \"rb\"))\n    ...\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52807787",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1539582490283,
        "Question_original_content":"studio access file upload zip file try execut python script studio script bundl zip file connect python script input python file txt file type file zip file question file path zip file exampl languag model zip file name pcl file path languag model thank",
        "Question_preprocessed_content":"studio access file upload zip file try execut python script studio script bundl connect python script input python file txt file type file zip file question file path zip file exampl languag model zip file name file path languag model thank",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing files within a zip file that they have uploaded to Azure machine learning studio. They are specifically looking for the file path of a language model within the zip file.",
        "Question_gpt_summary":"user face challeng access file zip file upload studio specif look file path languag model zip file",
        "Answer_original_content":"avail script bundl directori exampl load pickl model zip file write line import panda import pickl def main datafram datafram model pickl load open script bundl model pkl",
        "Answer_preprocessed_content":"avail directori exampl load pickl model zip file write line",
        "Answer_gpt_summary_original":"possible solution: the files within the zip file can be accessed from the \".\/script bundle\" directory. for example, to load a pickled model from the zip file, one can use the \"pickle.load\" function and specify the path to the model file within the \".\/script bundle\" directory.",
        "Answer_gpt_summary":"possibl solut file zip file access script bundl directori exampl load pickl model zip file us pickl load function specifi path model file script bundl directori"
    },
    {
        "Question_id":null,
        "Question_title":"Google ML kit",
        "Question_body":"I know Google provides an ML kit supported by android that we can integrate into an app. The ML Kit provides many Vision and NLP APIs that can help us make our own Google-like Lens.Anyone can give me more information on how to get the ML kit?I am the CEO and I am looking for a CTO to my company, must be good in Python, A.I., Machine Learning, IoT and Robotics.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660119120000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":106.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-ML-kit\/td-p\/452579\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-15T10:45:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"ML Kit is a mobile SDK that brings Google's on-device machine learning expertise to Android and iOS apps. To use ML Kit on Android you\u2019ll need to add the libraries to your module's app-level gradle file. To use on Ios you need to include the ML Kit pods in your Podfile.\n\nYou can use this document\u00a0to see the whole product's quickstarts.\n\nAdditionally see the left menu guides\u00a0for each product and how to use it on Ios or Android."
            },
            {
                "Answer_creation_time":"2022-08-16T20:54:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello\n\nThank you for the information."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"googl kit know googl provid kit support android integr app kit provid vision nlp api help googl like len inform kit ceo look cto compani good python machin learn iot robot",
        "Question_preprocessed_content":"googl kit know googl provid kit support android integr app kit provid vision nlp api help inform kit ceo look cto compani good python machin learn iot robot",
        "Question_gpt_summary_original":"The user is seeking more information on how to obtain Google's ML kit, which offers Vision and NLP APIs for integration into an app. Additionally, the user is looking for a CTO for their company who is skilled in Python, A.I., Machine Learning, IoT, and Robotics.",
        "Question_gpt_summary":"user seek inform obtain googl kit offer vision nlp api integr app addition user look cto compani skill python machin learn iot robot",
        "Answer_original_content":"kit mobil sdk bring googl devic machin learn expertis android io app us kit android youll need add librari modul app level gradl file us io need includ kit pod podfil us documentto product quickstart addition left menu guidesfor product us io android hello thank inform",
        "Answer_preprocessed_content":"kit mobil sdk bring googl machin learn expertis android io app us kit android youll need add librari modul gradl file us io need includ kit pod podfil us documentto product quickstart addition left menu guidesfor product us io android hello thank inform",
        "Answer_gpt_summary_original":"there are two possible solutions in the answer. the first solution is to use google's ml kit, which is a mobile sdk that brings on-device machine learning expertise to android and ios apps. to use ml kit on android, the user needs to add the libraries to their module's app-level gradle file. to use it on ios, they need to include the ml kit pods in their podfile. the second solution is to check out the product's quickstarts and guides for each product and how to use it on ios or android.",
        "Answer_gpt_summary":"possibl solut answer solut us googl kit mobil sdk bring devic machin learn expertis android io app us kit android user need add librari modul app level gradl file us io need includ kit pod podfil second solut check product quickstart guid product us io android"
    },
    {
        "Question_id":null,
        "Question_title":"Models not being registered when pipeline is triggered using REST endpoint",
        "Question_body":"I have a pipeline with two python script steps. The first script performs some data cleaning, and the second one trains a model, saves it in the 'outputs' folder, and finally calls the Model.register() function to register the updated model file. Just to be clear, the registration is being done in the script that trains the model (which runs on the cloud), not the script that starts the experiment (which runs on my laptop).\n\nIt works as it should when I run the experiment using the Experiment.submit() function call, but when I run the published pipeline using the REST endpoint, the model doesn't get registered. I can see the REST call recorded as a new experiment, and the 'outputs' folder of the second step has a model file too. But the new model doesn't get registered for some reason.\n\nDoes anyone know what's going wrong here?",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1641571746503,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/687290\/models-not-being-registered-when-pipeline-is-trigg.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-27T08:01:04.413Z",
                "Answer_score":0,
                "Answer_body":"@DaveJ-4580 Thanks for the details and apologies for the late response. I think the problem is that you rely on AML background process to automatically upload content under .\/outputs to AML workspace.\nBut when the upload is not complete and we immediately call run.register_model which takes the content from AML workspace then the error will happen.\nTo avoid that situation, you can do it like this:\n- Persist model (joblib.dump) to a custom folder other than outputs\n- Manually run upload_file to upload the model AML workspace. Name the destination same name with your model file.\n- Then run run.register_model.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"model regist pipelin trigger rest endpoint pipelin python script step script perform data clean second train model save output folder final call model regist function regist updat model file clear registr script train model run cloud script start experi run laptop work run experi experi submit function run publish pipelin rest endpoint model regist rest record new experi output folder second step model file new model regist reason know go wrong",
        "Question_preprocessed_content":"model regist pipelin trigger rest endpoint pipelin python script step script perform data clean second train model save output folder final call function regist updat model file clear registr script train model script start experi work run experi function run publish pipelin rest endpoint model regist rest record new experi output folder second step model file new model regist reason know go wrong",
        "Question_gpt_summary_original":"The user is facing a challenge where the model is not being registered when the pipeline is triggered using a REST endpoint, even though it works fine when using the Experiment.submit() function call. The model is being trained and saved in the 'outputs' folder, but the Model.register() function is not registering the updated model file.",
        "Question_gpt_summary":"user face challeng model regist pipelin trigger rest endpoint work fine experi submit function model train save output folder model regist function regist updat model file",
        "Answer_original_content":"davej thank detail apolog late respons think problem reli aml background process automat upload content output aml workspac upload complet immedi run regist model take content aml workspac error happen avoid situat like persist model joblib dump custom folder output manual run upload file upload model aml workspac destin model file run run regist model",
        "Answer_preprocessed_content":"thank detail apolog late respons think problem reli aml background process automat upload content aml workspac upload complet immedi take content aml workspac error happen avoid situat like persist model custom folder output manual run upload model aml workspac destin model file run",
        "Answer_gpt_summary_original":"possible solutions to the issue of models not being registered when the pipeline is triggered using a rest endpoint are: persisting the model to a custom folder other than outputs, manually uploading the model to the aml workspace using the upload_file function, and then registering the model using the run.register_model function. this can avoid errors that occur when the upload is not complete and run.register_model is called immediately.",
        "Answer_gpt_summary":"possibl solut issu model regist pipelin trigger rest endpoint persist model custom folder output manual upload model aml workspac upload file function regist model run regist model function avoid error occur upload complet run regist model call immedi"
    },
    {
        "Question_id":null,
        "Question_title":"Pytorch cannot detect GPU when using an AML Compute Cluster with a GPU",
        "Question_body":"Hi,\n\nI've been trying to train a pytorch model on the Azure ML compute clusters (STANDARD_NV6) but I cannot get the code to detect and use the GPU device, torch.cuda.is_available() always returns False.\n\nI'm using a custom environment and have tried using a few different dockerfiles as base images from the Microsoft container repository. For example, I've tried the \"mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04\" base image.\n\nIn the build log, I can see that the correct dependencies are installed each time but the code still doesn't detect a GPU. I tried forcing docker to use the GPU with docker_arguments = [\"--gpus\", \"all\"] but this causes the build to fail with this error:\n\n AzureMLCompute job failed.\n FailedStartingContainer: Unable to start docker container\n     FailedContainerStart: Unable to start docker container\n     err: warning: your kernel does not support swap limit capabilities or the cgroup is not mounted. memory limited without swap.\n docker: error response from daemon: oci runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:459: container init caused: running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: mount error: file creation failed: \/mnt\/docker\/overlay2\/66b78fe178db5d08ca4db26528f1a6de00aba65b528a6568649b1abcbea22348\/merged\/run\/nvidia-persistenced\/socket: no such device or address: unknown.\n    \n     Reason: warning: your kernel does not support swap limit capabilities or the cgroup is not mounted. memory limited without swap.\n docker: error response from daemon: oci runtime create failed: container_linux.go:370: starting container process caused: process_linux.go:459: container init caused: running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: mount error: file creation failed: \/mnt\/docker\/overlay2\/66b78fe178db5d08ca4db26528f1a6de00aba65b528a6568649b1abcbea22348\/merged\/run\/nvidia-persistenced\/socket: no such device or address: unknown.\n    \n     Info: Failed to prepare an environment for the job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.\n\n\n\nIt feels like I've missed some obvious step somewhere...\n\nThanks for any help!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617721297227,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/346224\/pytorch-cannot-detect-gpu-when-using-an-aml-comput.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-07T10:11:49.583Z",
                "Answer_score":0,
                "Answer_body":"@ClaudiaVanea-8710 Thanks for the question. which means driver issues, Can you please add more details about the Pytorch version that you using. Especially with pytorch where somehow the pytorch doesn\u2019t install correctly with the latest CUDA drivers. Can you please try installing the latest nvdia drivers.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pytorch detect gpu aml comput cluster gpu try train pytorch model comput cluster standard code detect us gpu devic torch cuda avail return fals custom environ tri differ dockerfil base imag microsoft contain repositori exampl tri mcr microsoft com openmpi cuda cudnn ubuntu base imag build log correct depend instal time code detect gpu tri forc docker us gpu docker argument gpu caus build fail error comput job fail failedstartingcontain unabl start docker contain failedcontainerstart unabl start docker contain err warn kernel support swap limit capabl cgroup mount memori limit swap docker error respons daemon oci runtim creat fail contain linux start contain process caus process linux contain init caus run hook error run hook exit statu stdout stderr nvidia contain cli mount error file creation fail mnt docker overlai bfedbdcadbfadeababababcbea merg run nvidia persistenc socket devic address unknown reason warn kernel support swap limit capabl cgroup mount memori limit swap docker error respons daemon oci runtim creat fail contain linux start contain process caus process linux contain init caus run hook error run hook exit statu stdout stderr nvidia contain cli mount error file creation fail mnt docker overlai bfedbdcadbfadeababababcbea merg run nvidia persistenc socket devic address unknown info fail prepar environ job execut job environ prepar fail err exit statu feel like miss obviou step thank help",
        "Question_preprocessed_content":"pytorch detect gpu aml comput cluster gpu try train pytorch model comput cluster code detect us gpu devic return fals custom environ tri differ dockerfil base imag microsoft contain repositori exampl tri base imag build log correct depend instal time code detect gpu tri forc docker us gpu caus build fail error comput job fail failedstartingcontain unabl start docker contain failedcontainerstart unabl start docker contain err warn kernel support swap limit capabl cgroup mount memori limit swap docker error respons daemon oci runtim creat fail start contain process caus contain init caus run hook error run hook exit statu stdout stderr mount error file creation fail devic address unknown reason warn kernel support swap limit capabl cgroup mount memori limit swap docker error respons daemon oci runtim creat fail start contain process caus contain init caus run hook error run hook exit statu stdout stderr mount error file creation fail devic address unknown info fail prepar environ job execut job environ prepar fail err exit statu feel like miss obviou step thank help",
        "Question_gpt_summary_original":"The user is facing challenges in training a Pytorch model on Azure ML compute clusters with GPU. The code is unable to detect and use the GPU device, and torch.cuda.is_available() always returns False. The user has tried using a custom environment and different dockerfiles as base images, but the code still doesn't detect a GPU. The user also tried forcing docker to use the GPU with docker_arguments = [\"--gpus\", \"all\"], but this caused the build to fail with an error.",
        "Question_gpt_summary":"user face challeng train pytorch model comput cluster gpu code unabl detect us gpu devic torch cuda avail return fals user tri custom environ differ dockerfil base imag code detect gpu user tri forc docker us gpu docker argument gpu caus build fail error",
        "Answer_original_content":"claudiavanea thank question mean driver issu add detail pytorch version especi pytorch pytorch doesnt instal correctli latest cuda driver try instal latest nvdia driver",
        "Answer_preprocessed_content":"thank question mean driver issu add detail pytorch version especi pytorch pytorch doesnt instal correctli latest cuda driver try instal latest nvdia driver",
        "Answer_gpt_summary_original":"the possible solution to the challenge of pytorch not detecting the gpu when using an aml compute cluster with a gpu is to install the latest nvidia drivers as there may be driver issues. the user is advised to provide more details about the pytorch version being used as sometimes pytorch does not install correctly with the latest cuda drivers.",
        "Answer_gpt_summary":"possibl solut challeng pytorch detect gpu aml comput cluster gpu instal latest nvidia driver driver issu user advis provid detail pytorch version pytorch instal correctli latest cuda driver"
    },
    {
        "Question_id":59773167.0,
        "Question_title":"When will experiment be deleted with lifecycle_stage is set as deleted",
        "Question_body":"<p>I can see experiment 2 is in deleted, but when it will be deleted actually?<\/p>\n\n<pre><code>2   test    hdfs:\/\/\/1234\/mlflow deleted\n<\/code><\/pre>\n\n<p>If the experiment is not deleted automatically, how can I delete it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1579189296547,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":412.0,
        "Answer_body":"<p>I am assuming you use sql store?<\/p>\n\n<p>Currently there is no way to tell mlflow to hard-delete experiments. We are working with open source contributors to add a cli command that would perform garbage-collection of deleted experiments. This should be added soon in one of the upcoming mlflow releases. In the meantime, you can connect to your sql store and delete the experiments manually.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59773167",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1579736829916,
        "Question_original_content":"experi delet lifecycl stage set delet experi delet delet actual test hdf delet experi delet automat delet",
        "Question_preprocessed_content":"experi delet set delet experi delet delet actual experi delet automat delet",
        "Question_gpt_summary_original":"The user is facing challenges in determining when an experiment will be deleted when its lifecycle_stage is set as deleted. They are also unsure of how to manually delete the experiment if it is not deleted automatically.",
        "Question_gpt_summary":"user face challeng determin experi delet lifecycl stage set delet unsur manual delet experi delet automat",
        "Answer_original_content":"assum us sql store current wai tell hard delet experi work open sourc contributor add cli command perform garbag collect delet experi ad soon upcom releas meantim connect sql store delet experi manual",
        "Answer_preprocessed_content":"assum us sql store current wai tell experi work open sourc contributor add cli command perform delet experi ad soon upcom releas meantim connect sql store delet experi manual",
        "Answer_gpt_summary_original":"possible solutions: \n- currently, there is no way to hard-delete experiments with a lifecycle_stage set as deleted. \n- the team is working on adding a cli command that would perform garbage-collection of deleted experiments in upcoming releases. \n- in the meantime, the user can connect to their sql store and delete the experiments manually.",
        "Answer_gpt_summary":"possibl solut current wai hard delet experi lifecycl stage set delet team work ad cli command perform garbag collect delet experi upcom releas meantim user connect sql store delet experi manual"
    },
    {
        "Question_id":69878915.0,
        "Question_title":"Deploying multiple models to same endpoint in Vertex AI",
        "Question_body":"<p>Our use case is as follows:\nWe have multiple custom trained models (in the hundreds, and the number increases as we allow the user of our application to create models through the UI, which we then train and deploy on the fly) and so deploying each model to a separate endpoint is expensive as Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/deploy-model-console#custom-trained\" rel=\"nofollow noreferrer\">charges per node used<\/a>. Based on the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> it seems that we can deploy models of different types to the same endpoint but I am not sure how that would work. Let's say I have 2 different custom trained models deployed using custom containers for prediction to the same endpoint. Also, say I specify the traffic split to be 50% for the two models. Now, how do I send a request to a specific model? Using the python SDK, we make calls to the endpoint, like so:<\/p>\n<pre><code>from google.cloud import aiplatform\nendpoint = aiplatform.Endpoint(endpoint_id)\nprediction = endpoint.predict(instances=instances)\n\n# where endpoint_id is the id of the endpoint and instances are the observations for which a prediction is required\n<\/code><\/pre>\n<p>My understanding is that in this scenario, vertex AI will route some calls to one model and some to the other based on the traffic split. I could use the parameters field, as specified in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">docs<\/a>, to specify the model and then process the request accordingly in the custom prediction container, but still some calls will end up going to a model which it will not be able to process (because Vertex AI is not going to be sending all requests to all models, otherwise the traffic split wouldn't make sense). How do I then deploy multiple models to the same endpoint and make sure that every prediction request is guaranteed to be served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1636348555970,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1271.0,
        "Answer_body":"<p>This <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> talks about a use case where 2 models are trained on the same feature set and are sharing the ingress prediction traffic. As you have understood correctly, this does not apply to models that have been trained on different feature sets, that is, different models.<\/p>\n<p>Unfortunately, deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI at the moment. There is an ongoing feature request that is being worked on. However, we cannot provide an exact ETA on when that feature will be available.<\/p>\n<p>I reproduced the multi-model setup and noticed the below points.<\/p>\n<p><strong>Traffic Splitting<\/strong><\/p>\n<blockquote>\n<p>I deployed 2 different models to the same endpoint and sent predictions to it. I set a 50-50 traffic splitting rule and saw errors that implied requests being sent to the wrong model.<\/p>\n<\/blockquote>\n<p><strong>Cost Optimization<\/strong><\/p>\n<blockquote>\n<p>When multiple models are deployed to the same endpoint, they are deployed to separate, independent nodes. So, you will still be charged for each node used. Also, node autoscaling happens at the model level, not at the endpoint level.<\/p>\n<\/blockquote>\n<p>A plausible workaround would be to pack all your models into a single container and use a custom HTTP server logic to send prediction requests to the appropriate model. This could be achieved using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\"><code>parameters<\/code><\/a> field of the prediction request body. The custom logic would look something like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    parameters = body[&quot;parameters&quot;]\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    if(parameters[&quot;model_name&quot;]==&quot;random_forest&quot;):\n        print(parameters[&quot;model_name&quot;])\n        outputs = _random_forest_model.predict(preprocessed_inputs)\n    else:\n        print(parameters[&quot;model_name&quot;])\n        outputs = _decision_tree_model.predict(inputs)\n\n    return {&quot;predictions&quot;: [_class_names[class_num] for class_num in outputs]}\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1636405249843,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69878915",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1636404890008,
        "Question_original_content":"deploi multipl model endpoint us case follow multipl custom train model hundr number increas allow user applic creat model train deploi fly deploi model separ endpoint expens charg node base document deploi model differ type endpoint sure work let differ custom train model deploi custom contain predict endpoint specifi traffic split model send request specif model python sdk call endpoint like googl cloud import aiplatform endpoint aiplatform endpoint endpoint predict endpoint predict instanc instanc endpoint endpoint instanc observ predict requir understand scenario rout call model base traffic split us paramet field specifi doc specifi model process request accordingli custom predict contain call end go model abl process go send request model traffic split wouldn sens deploi multipl model endpoint sure predict request guarante serv",
        "Question_preprocessed_content":"deploi multipl model endpoint us case follow multipl custom train model deploi model separ endpoint expens charg node base document deploi model differ type endpoint sure work let differ custom train model deploi custom contain predict endpoint specifi traffic split model send request specif model python sdk call endpoint like understand scenario rout call model base traffic split us paramet field specifi doc specifi model process request accordingli custom predict contain call end go model abl process deploi multipl model endpoint sure predict request guarante serv",
        "Question_gpt_summary_original":"The user is facing the challenge of deploying multiple custom trained models to the same endpoint in Vertex AI. Deploying each model to a separate endpoint is expensive, and the user is unsure how to send a request to a specific model when multiple models are deployed to the same endpoint. The user is concerned that some calls will end up going to a model that cannot process the request, and is seeking a solution to ensure that every prediction request is guaranteed to be served.",
        "Question_gpt_summary":"user face challeng deploi multipl custom train model endpoint deploi model separ endpoint expens user unsur send request specif model multipl model deploi endpoint user concern call end go model process request seek solut ensur predict request guarante serv",
        "Answer_original_content":"document talk us case model train featur set share ingress predict traffic understood correctli appli model train differ featur set differ model unfortun deploi differ model endpoint util node possibl moment ongo featur request work provid exact eta featur avail reproduc multi model setup notic point traffic split deploi differ model endpoint sent predict set traffic split rule saw error impli request sent wrong model cost optim multipl model deploi endpoint deploi separ independ node charg node node autosc happen model level endpoint level plausibl workaround pack model singl contain us custom http server logic send predict request appropri model achiev paramet field predict request bodi custom logic look like app post environ aip predict rout async def predict request request bodi await request json paramet bodi paramet instanc bodi instanc input asarrai instanc preprocess input preprocessor preprocess input paramet model random forest print paramet model output random forest model predict preprocess input print paramet model output decis tree model predict input return predict class name class num class num output",
        "Answer_preprocessed_content":"document talk us case model train featur set share ingress predict traffic understood correctli appli model train differ featur set differ model unfortun deploi differ model endpoint util node possibl moment ongo featur request work provid exact eta featur avail reproduc setup notic point traffic split deploi differ model endpoint sent predict set traffic split rule saw error impli request sent wrong model cost optim multipl model deploi endpoint deploi separ independ node charg node node autosc happen model level endpoint level plausibl workaround pack model singl contain us custom http server logic send predict request appropri model achiev field predict request bodi custom logic look like",
        "Answer_gpt_summary_original":"possible solutions to the challenge of deploying multiple custom trained models to the same endpoint while ensuring that each prediction request is served correctly are not available at the moment. deploying different models to the same endpoint utilizing only one node is not possible, but there is an ongoing feature request that is being worked on. a plausible workaround would be to pack all the models into a single container and use a custom http server logic to send prediction requests to the appropriate model using the parameters field of the prediction request body. however, cost optimization may not be possible as multiple models deployed to the same endpoint are deployed to separate, independent nodes, and node autoscaling happens at the model level, not at the endpoint level.",
        "Answer_gpt_summary":"possibl solut challeng deploi multipl custom train model endpoint ensur predict request serv correctli avail moment deploi differ model endpoint util node possibl ongo featur request work plausibl workaround pack model singl contain us custom http server logic send predict request appropri model paramet field predict request bodi cost optim possibl multipl model deploi endpoint deploi separ independ node node autosc happen model level endpoint level"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to do machine learning in Azure IoT Central?",
        "Question_body":"I was wondering if it is possible to do machine learning on Azure IoT central. I read in some places that it is possible to do so in Azure IoT Edge. I saw a template for Video Analytics but cannot seem to find a way to implement my own models. If Edge is the only way to perform machine learning in Azure IoT, is there some way to use IoT Edge with IoT Central? Or, is it possible to train your own Tensorflow Lite Models with Raspberry Pi and just host the Pi in IoT Hub? If both are possible, which of the two would be the easiest?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618360709993,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/356183\/is-it-possible-to-do-machine-learning-in-azure-iot.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-14T16:12:08.863Z",
                "Answer_score":1,
                "Answer_body":"@KC-6678 it looks like you are having great challenges ahead :)!\n\nTo your main question, the direct answer is yes you can use Machine Learning in conjunction with an Azure IoT Central Solution which ingests data from an Azure IoT Edge device previously trained by your own Tensorflow Lite Models.\n\nThe bigger question is how do you want to do it and what are the current constraints you have + when do you want the actionable decisions coming from your connected device sensors to be made?\n\nMy advice is that you follow one of our Learning paths or Modules, for example:\n\nAI edge engineer\n\n\nIdentify anomalies by routing data via IoT Hub to a built-in ML model in Azure Stream Analytics\n\nYou will learn how to use a trained model and clearly distinguish when you need an ML model running in the Edge vs in the Cloud.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"possibl machin learn azur iot central wonder possibl machin learn azur iot central read place possibl azur iot edg saw templat video analyt wai implement model edg wai perform machin learn azur iot wai us iot edg iot central possibl train tensorflow lite model raspberri host iot hub possibl easiest",
        "Question_preprocessed_content":"possibl machin learn azur iot central wonder possibl machin learn azur iot central read place possibl azur iot edg saw templat video analyt wai implement model edg wai perform machin learn azur iot wai us iot edg iot central possibl train tensorflow lite model raspberri host iot hub possibl easiest",
        "Question_gpt_summary_original":"The user is facing challenges in implementing machine learning on Azure IoT Central. They have read that it is possible to do so on Azure IoT Edge, but are unsure if it can be used with IoT Central. They are also unsure if they can train their own Tensorflow Lite models with Raspberry Pi and host it in IoT Hub. The user is seeking clarification on which option would be the easiest.",
        "Question_gpt_summary":"user face challeng implement machin learn azur iot central read possibl azur iot edg unsur iot central unsur train tensorflow lite model raspberri host iot hub user seek clarif option easiest",
        "Answer_original_content":"look like have great challeng ahead main question direct answer ye us machin learn conjunct azur iot central solut ingest data azur iot edg devic previous train tensorflow lite model bigger question want current constraint want action decis come connect devic sensor advic follow learn path modul exampl edg engin identifi anomali rout data iot hub built model azur stream analyt learn us train model clearli distinguish need model run edg cloud",
        "Answer_preprocessed_content":"look like have great challeng ahead main question direct answer ye us machin learn conjunct azur iot central solut ingest data azur iot edg devic previous train tensorflow lite model bigger question want current constraint want action decis come connect devic sensor advic follow learn path modul exampl edg engin identifi anomali rout data iot hub model azur stream analyt learn us train model clearli distinguish need model run edg cloud",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- yes, it is possible to use machine learning with azure iot central by ingesting data from an azure iot edge device previously trained by your own tensorflow lite models.\n- follow one of the learning paths or modules provided by azure, such as the ai edge engineer module, to learn how to use a trained model and distinguish when you need an ml model running in the edge vs in the cloud.",
        "Answer_gpt_summary":"possibl solut answer ye possibl us machin learn azur iot central ingest data azur iot edg devic previous train tensorflow lite model follow learn path modul provid azur edg engin modul learn us train model distinguish need model run edg cloud"
    },
    {
        "Question_id":null,
        "Question_title":"Training Personalizer",
        "Question_body":"I am considering using Personalizer for project and have found limited third party metrics for this service. This one article indicates needing TENS of thousands of hits to get good results.\n\nhttps:\/\/medium.com\/@EnefitIT\/we-tested-azure-personalizer-heres-what-you-can-expect-8c5ec074a28e\n\nCan any one provide any other data?\n\nObviously, over time it will get better, but does it have to get to 10K+ to get good?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606863751290,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@GregorioRojas-6472 The minimum requirements to have an effective recommendation is to have a minimum of ~1k\/day content-related events. Higher rate of events do help you to provide faster and better recommendations. All the requirements are documented in the official documentation page of the service. The samples repo provides some data along with the quickstart's from the documentation to get started. The service now provides an E0 tier or apprentice mode that helps you test the service and gain confidence to move to a higher tier with production level recommendations.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/182318\/training-personalizer.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-02T07:18:51.207Z",
                "Answer_score":0,
                "Answer_body":"@GregorioRojas-6472 The minimum requirements to have an effective recommendation is to have a minimum of ~1k\/day content-related events. Higher rate of events do help you to provide faster and better recommendations. All the requirements are documented in the official documentation page of the service. The samples repo provides some data along with the quickstart's from the documentation to get started. The service now provides an E0 tier or apprentice mode that helps you test the service and gain confidence to move to a higher tier with production level recommendations.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1606893531207,
        "Question_original_content":"train person consid person project limit parti metric servic articl indic need ten thousand hit good result http medium com enefitit test azur person here expect ceca provid data obvious time better good",
        "Question_preprocessed_content":"train person consid person project limit parti metric servic articl indic need ten thousand hit good result provid data obvious time better good",
        "Question_gpt_summary_original":"The user is facing challenges in finding third-party metrics for Personalizer, a service they are considering for their project. They have come across an article that suggests needing tens of thousands of hits to get good results, and are seeking additional data to confirm this. The user is questioning whether it is necessary to reach 10K+ hits to achieve good results.",
        "Question_gpt_summary":"user face challeng find parti metric person servic consid project come articl suggest need ten thousand hit good result seek addit data confirm user question necessari reach hit achiev good result",
        "Answer_original_content":"gregorioroja minimum requir effect recommend minimum dai content relat event higher rate event help provid faster better recommend requir document offici document page servic sampl repo provid data quickstart document start servic provid tier apprentic mode help test servic gain confid higher tier product level recommend",
        "Answer_preprocessed_content":"minimum requir effect recommend minimum event higher rate event help provid faster better recommend requir document offici document page servic sampl repo provid data quickstart document start servic provid tier apprentic mode help test servic gain confid higher tier product level recommend",
        "Answer_gpt_summary_original":"the answer suggests that for effective recommendations using azure personalizer, a minimum of ~1k\/day content-related events is required. higher rates of events can provide faster and better recommendations. the official documentation page of the service provides all the requirements, and the samples repo provides some data to get started. the service now provides an e0 tier or apprentice mode to test the service and gain confidence before moving to a higher tier with production level recommendations.",
        "Answer_gpt_summary":"answer suggest effect recommend azur person minimum dai content relat event requir higher rate event provid faster better recommend offici document page servic provid requir sampl repo provid data start servic provid tier apprentic mode test servic gain confid move higher tier product level recommend"
    },
    {
        "Question_id":64173739.0,
        "Question_title":"AWS Sagemaker + AWS Lambda",
        "Question_body":"<p>I try to use AWS SageMaker following documentation. I successfully loaded data, trained and deployed the model.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4Mjew.png\" rel=\"nofollow noreferrer\">deployed-model<\/a><\/p>\n<p>My next step have to be using AWS Lambda, connect it to this SageMaker endpoint.\nI saw, that I need to give Lambda IAM execution role permission to invoke a model endpoint.\nI add some data to IAM policy JSON and now it has this view<\/p>\n<pre><code>{\n&quot;Version&quot;: &quot;2012-10-17&quot;,\n&quot;Statement&quot;: [\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;logs:CreateLogGroup&quot;,\n        &quot;Resource&quot;: &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:*&quot;\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: [\n            &quot;logs:CreateLogStream&quot;,\n            &quot;logs:PutLogEvents&quot;\n        ],\n        &quot;Resource&quot;: [\n            &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:log-group:\/aws\/lambda\/test-sagemaker:*&quot;\n        ]\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n        &quot;Resource&quot;: &quot;*&quot;\n    }\n]\n<\/code><\/pre>\n<p>}<\/p>\n<p>Problem that even with role that have permission for invoking SageMaker endpoint my Lambda function didn't see it<\/p>\n<pre><code>An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint xgboost-2020-10-02-12-15-36-097 of account &lt;my-account&gt; not found.: ValidationError\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601650547150,
        "Question_favorite_count":null,
        "Question_last_edit_time":1601906646463,
        "Question_score":0.0,
        "Question_view_count":623.0,
        "Answer_body":"<p>I found an error by myself. Problem was in different regions. For training and deploying model I used us-east-2 and for lambda I used us-east-1. Just creating all in same region fixed this issue!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64173739",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1601906782043,
        "Question_original_content":"aw lambda try us follow document successfulli load data train deploi model deploi model step aw lambda connect endpoint saw need lambda iam execut role permiss invok model endpoint add data iam polici json view version statement effect allow action log createloggroup resourc arn aw log east effect allow action log createlogstream log putlogev resourc arn aw log east log group aw lambda test effect allow action invokeendpoint resourc problem role permiss invok endpoint lambda function error occur validationerror call invokeendpoint oper endpoint xgboost account validationerror",
        "Question_preprocessed_content":"aw lambda try us follow document successfulli load data train deploi model step aw lambda connect endpoint saw need lambda iam execut role permiss invok model endpoint add data iam polici json view problem role permiss invok endpoint lambda function",
        "Question_gpt_summary_original":"The user successfully loaded data, trained and deployed a model using AWS SageMaker, but encountered challenges when trying to connect AWS Lambda to the SageMaker endpoint. Despite adding data to the IAM policy JSON to give Lambda IAM execution role permission to invoke the model endpoint, the Lambda function was unable to see the endpoint, resulting in a validation error.",
        "Question_gpt_summary":"user successfulli load data train deploi model encount challeng try connect aw lambda endpoint despit ad data iam polici json lambda iam execut role permiss invok model endpoint lambda function unabl endpoint result valid error",
        "Answer_original_content":"error problem differ region train deploi model east lambda east creat region fix issu",
        "Answer_preprocessed_content":"error problem differ region train deploi model lambda creat region fix issu",
        "Answer_gpt_summary_original":"solution: the issue of connecting an aws lambda to an endpoint despite having the necessary iam execution role permission was resolved by creating all resources in the same region. the user had used different regions for training and deploying the model and for the lambda, which caused the error.",
        "Answer_gpt_summary":"solut issu connect aw lambda endpoint despit have necessari iam execut role permiss resolv creat resourc region user differ region train deploi model lambda caus error"
    },
    {
        "Question_id":null,
        "Question_title":"Torch.multiprocessing.spawn fails",
        "Question_body":"<p>I have some code that works standalone, but fails when run from guild.  The offending line is:<\/p>\n<pre><code>torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args))\n<\/code><\/pre>\n<p>and the complaint is:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">  [...] \n  File \"\/usr\/lib\/python3.10\/multiprocessing\/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/context.py\", line 288, in _Popen\n    return Popen(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/spawn.py\", line 183, in get_preparation_data\n    main_mod_name = getattr(main_module.__spec__, \"name\", None)\nAttributeError: 'dict' object has no attribute '__spec__'\n<\/code><\/pre>\n<p>Does anyone have any tips?  I\u2019m not sure I really understand what\u2019s failing in the spawn call\u2026  Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1665950939224,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":159.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/torch-multiprocessing-spawn-fails\/929",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-17T17:38:14.680Z",
                "Answer_body":"<p>I\u2019m sorry you\u2019re running into this! I created an <a href=\"https:\/\/github.com\/guildai\/issue-resolution\/tree\/master\/my.guild.ai-929-torch-multiprocessing-spawn-fails\">issue resolution doc<\/a> that easily reproduces this. I\u2019ll spend some time looking into it.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-17T17:58:48.903Z",
                "Answer_body":"<p>Still investigating but there is a work-around - if you your script using Python using Guild\u2019s <code>exec<\/code> spec this way:<\/p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https:\/\/github.com\/guildai\/issue-resolution\/blob\/93de41a7741148e838e8c65e9b74b6854d11ad49\/my.guild.ai-929-torch-multiprocessing-spawn-fails\/guild.yml#L1-L2\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/guildai\/issue-resolution\/blob\/93de41a7741148e838e8c65e9b74b6854d11ad49\/my.guild.ai-929-torch-multiprocessing-spawn-fails\/guild.yml#L1-L2\" target=\"_blank\" rel=\"noopener\">github.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https:\/\/github.com\/guildai\/issue-resolution\/blob\/93de41a7741148e838e8c65e9b74b6854d11ad49\/my.guild.ai-929-torch-multiprocessing-spawn-fails\/guild.yml#L1-L2\" target=\"_blank\" rel=\"noopener\">guildai\/issue-resolution\/blob\/93de41a7741148e838e8c65e9b74b6854d11ad49\/my.guild.ai-929-torch-multiprocessing-spawn-fails\/guild.yml#L1-L2<\/a><\/h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-yml\">\n      <ol class=\"start lines\" start=\"1\" style=\"counter-reset: li-counter 0 ;\">\n          <li>test-exec:<\/li>\n          <li>  exec: python .guild\/sourcecode\/test.py<\/li>\n      <\/ol>\n    <\/code><\/pre>\n\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Note that this form doesn\u2019t support Python global variable based flags - you\u2019d need to either pass command line arguments along or use config files.<\/p>\n<p>Still looking into the underlying issue but I wanted to get you a workaround sooner than later.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"torch multiprocess spawn fail code work standalon fail run offend line torch multiprocess spawn main worker nproc gpu arg gpu arg complaint file usr lib python multiprocess process line start self popen self popen self file usr lib python multiprocess context line popen return popen process obj file usr lib python multiprocess popen spawn posix line init super init process obj file usr lib python multiprocess popen fork line init self launch process obj file usr lib python multiprocess popen spawn posix line launch prep data spawn prepar data process obj file usr lib python multiprocess spawn line prepar data main mod getattr main modul spec attributeerror dict object attribut spec tip sure understand what fail spawn thank",
        "Question_preprocessed_content":"fail code work standalon fail run offend line complaint tip sure understand what fail spawn thank",
        "Question_gpt_summary_original":"The user is encountering an error when running their code using torch.multiprocessing.spawn, specifically when running it from guild. The error message indicates that the issue is with the preparation data and that the main module name is not being properly retrieved. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user encount error run code torch multiprocess spawn specif run error messag indic issu prepar data main modul properli retriev user seek advic resolv issu",
        "Answer_original_content":"sorri your run creat issu resolut doc easili reproduc ill spend time look investig work script python exec spec wai github com issu resolut blob deaeecebbdad torch multiprocess spawn fail yml test exec exec python sourcecod test note form doesnt support python global variabl base flag youd need pass command line argument us config file look underli issu want workaround sooner later",
        "Answer_preprocessed_content":"sorri your run creat issu resolut doc easili reproduc ill spend time look investig script python spec wai note form doesnt support python global variabl base flag youd need pass command line argument us config file look underli issu want workaround sooner later",
        "Answer_gpt_summary_original":"the answer suggests that the user can use a work-around by running their script using python using guilds exec spec. the provided link contains a yaml file that can be used to execute the script. however, it is noted that this form does not support python global variable-based flags, and the user would need to pass command-line arguments or use config files. the underlying issue is still being investigated.",
        "Answer_gpt_summary":"answer suggest user us work run script python exec spec provid link contain yaml file execut script note form support python global variabl base flag user need pass command line argument us config file underli issu investig"
    },
    {
        "Question_id":72058686.0,
        "Question_title":"How do I deploy a pre trained sklearn model on AWS sagemaker? (Endpoint stuck on creating)",
        "Question_body":"<p>To start with, I understand that this question has been asked multiple times but I haven't found the solution to my problem.<\/p>\n<p>So, to start with I have used joblib.dump to save a locally trained sklearn RandomForest. I then uploaded this to s3, made a folder called code and put in an inference script there, called inference.py.<\/p>\n<pre><code>import joblib\nimport json\nimport numpy\nimport scipy\nimport sklearn\nimport os\n\n&quot;&quot;&quot;\nDeserialize fitted model\n&quot;&quot;&quot;\ndef model_fn(model_dir):\n    model_path = os.path.join(model_dir, 'test_custom_model')\n    model = joblib.load(model_path)\n    return model\n\n&quot;&quot;&quot;\ninput_fn\n    request_body: The body of the request sent to the model.\n    request_content_type: (string) specifies the format\/variable type of the request\n&quot;&quot;&quot;\ndef input_fn(request_body, request_content_type):\n    if request_content_type == 'application\/json':\n        request_body = json.loads(request_body)\n        inpVar = request_body['Input']\n        return inpVar\n    else:\n        raise ValueError(&quot;This model only supports application\/json input&quot;)\n\n&quot;&quot;&quot;\npredict_fn\n    input_data: returned array from input_fn above\n    model (sklearn model) returned model loaded from model_fn above\n&quot;&quot;&quot;\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n&quot;&quot;&quot;\noutput_fn\n    prediction: the returned value from predict_fn above\n    content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n&quot;&quot;&quot;\n\ndef output_fn(prediction, content_type):\n    res = int(prediction[0])\n    respJSON = {'Output': res}\n    return respJSON\n<\/code><\/pre>\n<p>Very simple so far.<\/p>\n<p>I also put this into the local jupyter sagemaker session<\/p>\n<p>all_files (folder)\ncode (folder)\ninference.py (python file)\ntest_custom_model (joblib dump of model)<\/p>\n<p>The script turns this folder all_files into a tar.gz file<\/p>\n<p>Then comes the main script that I ran on sagemaker:<\/p>\n<pre><code>import boto3\nimport json\nimport os\nimport joblib\nimport pickle\nimport tarfile\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\nimport subprocess\nfrom sagemaker import get_execution_role\n\n#Setup\nclient = boto3.client(service_name=&quot;sagemaker&quot;)\nruntime = boto3.client(service_name=&quot;sagemaker-runtime&quot;)\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nprint(region)\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\n#Bucket for model artifacts\ndefault_bucket = 'pretrained-model-deploy'\nmodel_artifacts = f&quot;s3:\/\/{default_bucket}\/test_custom_model.tar.gz&quot;\n\n#Build tar file with model data + inference code\nbashCommand = &quot;tar -cvpzf test_custom_model.tar.gz all_files&quot;\nprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\noutput, error = process.communicate()\n\n#Upload tar.gz to bucket\nresponse = s3.meta.client.upload_file('test_custom_model.tar.gz', default_bucket, 'test_custom_model.tar.gz')\n\n# retrieve sklearn image\nimage_uri = sagemaker.image_uris.retrieve(\n    framework=&quot;sklearn&quot;,\n    region=region,\n    version=&quot;0.23-1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n)\n\n#Step 1: Model Creation\nmodel_name = &quot;sklearn-test&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(&quot;Model name: &quot; + model_name)\ncreate_model_response = client.create_model(\n    ModelName=model_name,\n    Containers=[\n        {\n            &quot;Image&quot;: image_uri,\n            &quot;ModelDataUrl&quot;: model_artifacts,\n        }\n    ],\n    ExecutionRoleArn=role,\n)\nprint(&quot;Model Arn: &quot; + create_model_response[&quot;ModelArn&quot;])\n\n#Step 2: EPC Creation - Serverless\nsklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nresponse = client.create_endpoint_config(\n   EndpointConfigName=sklearn_epc_name,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 2048,\n                &quot;MaxConcurrency&quot;: 20\n            }\n        } \n    ]\n)\n\n# #Step 2: EPC Creation - Synchronous\n# sklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n# endpoint_config_response = client.create_endpoint_config(\n#     EndpointConfigName=sklearn_epc_name,\n#     ProductionVariants=[\n#         {\n#             &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n#             &quot;ModelName&quot;: model_name,\n#             &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n#             &quot;InitialInstanceCount&quot;: 1\n#         },\n#     ],\n# )\n# print(&quot;Endpoint Configuration Arn: &quot; + endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n#Step 3: EP Creation\nendpoint_name = &quot;sklearn-local-ep&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=sklearn_epc_name,\n)\nprint(&quot;Endpoint Arn: &quot; + create_endpoint_response[&quot;EndpointArn&quot;])\n\n\n#Monitor creation\ndescribe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\nwhile describe_endpoint_response[&quot;EndpointStatus&quot;] == &quot;Creating&quot;:\n    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n    print(describe_endpoint_response)\n    time.sleep(15)\nprint(describe_endpoint_response)\n<\/code><\/pre>\n<p>Now, I mainly just want the serverless deployment but that fails after a while with this error message:<\/p>\n<pre><code>{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 16, 11, 52000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '305', 'date': 'Fri, 29 Apr 2022 12:21:59 GMT'}, 'RetryAttempts': 0}}\n{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Failed', 'FailureReason': 'Unable to successfully stand up your model within the allotted 180 second timeout. Please ensure that downloading your model artifacts, starting your model container and passing the ping health checks can be completed within 180 seconds.', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 22, 2, 68000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '559', 'date': 'Fri, 29 Apr 2022 12:22:15 GMT'}, 'RetryAttempts': 0}}\n<\/code><\/pre>\n<p>The real time deployment is just permanently stuck at creating.<\/p>\n<p>Cloudwatch has the following errors:\nError handling request \/ping<\/p>\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n<p>with traceback:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base_async.py&quot;, line 55, in handle\n    self.handle_request(listener_name, req, client, addr)\n<\/code><\/pre>\n<p>Copy paste has stopped working so I have attached an image of it instead.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hw80j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hw80j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the error message I get:\nEndpoint Arn: arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09\n{'EndpointName': 'sklearn-local-ep2022-04-29-13-18-09', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09', 'EndpointConfigName': 'sklearn-epc2022-04-29-13-18-07', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 13, 18, 9, 548000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 13, 18, 13, 119000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '306', 'date': 'Fri, 29 Apr 2022 13:18:24 GMT'}, 'RetryAttempts': 0}}<\/p>\n<p>These are the permissions I have associated with that role:<\/p>\n<pre><code>AmazonSageMaker-ExecutionPolicy\nSecretsManagerReadWrite\nAmazonS3FullAccess\nAmazonSageMakerFullAccess\nEC2InstanceProfileForImageBuilderECRContainerBuilds\nAWSAppRunnerServicePolicyForECRAccess\n<\/code><\/pre>\n<p>What am I doing wrong? I've tried different folder structures for the zip file, different accounts, all to no avail. I don't really want to use the model.deploy() method as I don't know how to use serverless with that, and it's also inconcistent between different model types (I'm trying to make a flexible deployment pipeline where different (xgb \/ sklearn) models can be deployed with minimal changes.<\/p>\n<p>Please send help, I'm very close to smashing my hair and tearing out my laptop, been struggling with this for a whole 4 days now.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1651238636737,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1651255570927,
        "Question_score":1.0,
        "Question_view_count":397.0,
        "Answer_body":"<p>I've solved this problem - I used sagemaker.model.model to load in the model data I already had and I called the deploy method on the aforementioned model object to deploy it. Further, I had the inference script and the model file in the same place as the notebook and directly called them, as this gave me an error earlier as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72058686",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651509693110,
        "Question_original_content":"deploi pre train sklearn model endpoint stuck creat start understand question ask multipl time haven solut problem start joblib dump save local train sklearn randomforest upload folder call code infer script call infer import joblib import json import numpi import scipi import sklearn import deseri fit model def model model dir model path path join model dir test custom model model joblib load model path return model input request bodi bodi request sent model request content type string specifi format variabl type request def input request bodi request content type request content type applic json request bodi json load request bodi inpvar request bodi input return inpvar rais valueerror model support applic json input predict input data return arrai input model sklearn model return model load model def predict input data model return model predict input data output predict return valu predict content type content type endpoint expect return json string def output predict content type re int predict respjson output re return respjson simpl far local jupyt session file folder code folder infer python file test custom model joblib dump model script turn folder file tar file come main script ran import boto import json import import joblib import pickl import tarfil import import time time import gmtime strftime import subprocess import execut role setup client boto client servic runtim boto client servic runtim boto session boto session session boto session resourc region boto session region print region session session role execut role bucket model artifact default bucket pretrain model deploi model artifact default bucket test custom model tar build tar file model data infer code bashcommand tar cvpzf test custom model tar file process subprocess popen bashcommand split stdout subprocess pipe output error process commun upload tar bucket respons meta client upload file test custom model tar default bucket test custom model tar retriev sklearn imag imag uri imag uri retriev framework sklearn region region version version instanc type xlarg step model creation model sklearn test strftime gmtime print model model creat model respons client creat model modelnam model contain imag imag uri modeldataurl model artifact executionrolearn role print model arn creat model respons modelarn step epc creation serverless sklearn epc sklearn epc strftime gmtime respons client creat endpoint config endpointconfignam sklearn epc productionvari modelnam model variantnam sklearnvari serverlessconfig memorysizeinmb maxconcurr step epc creation synchron sklearn epc sklearn epc strftime gmtime endpoint config respons client creat endpoint config endpointconfignam sklearn epc productionvari variantnam sklearnvari modelnam model instancetyp xlarg initialinstancecount print endpoint configur arn endpoint config respons endpointconfigarn step creation endpoint sklearn local strftime gmtime creat endpoint respons client creat endpoint endpointnam endpoint endpointconfignam sklearn epc print endpoint arn creat endpoint respons endpointarn monitor creation endpoint respons client endpoint endpointnam endpoint endpoint respons endpointstatu creat endpoint respons client endpoint endpointnam endpoint print endpoint respons time sleep print endpoint respons mainli want serverless deploy fail error messag endpointnam sklearn local endpointarn arn aw east endpoint sklearn local endpointconfignam sklearn epc endpointstatu creat creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal responsemetadata requestid ddb cbe httpstatuscod httpheader amzn requestid ddb cbe content type applic amz json content length date fri apr gmt retryattempt endpointnam sklearn local endpointarn arn aw east endpoint sklearn local endpointconfignam sklearn epc endpointstatu fail failurereason unabl successfulli stand model allot second timeout ensur download model artifact start model contain pass ping health check complet second creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal responsemetadata requestid abaffb httpstatuscod httpheader amzn requestid abaffb content type applic amz json content length date fri apr gmt retryattempt real time deploy perman stuck creat cloudwatch follow error error handl request ping attributeerror nonetyp object attribut startswith traceback traceback recent file miniconda lib python site packag gunicorn worker base async line handl self handl request listen req client addr copi past stop work attach imag instead error messag endpoint arn arn aw east endpoint sklearn local endpointnam sklearn local endpointarn arn aw east endpoint sklearn local endpointconfignam sklearn epc endpointstatu creat creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal responsemetadata requestid efee httpstatuscod httpheader amzn requestid efee content type applic amz json content length date fri apr gmt retryattempt permiss associ role amazon executionpolici secretsmanagerreadwrit amazonsfullaccess amazonfullaccess ecinstanceprofileforimagebuilderecrcontainerbuild awsapprunnerservicepolicyforecraccess wrong tri differ folder structur zip file differ account avail want us model deploi method know us serverless inconcist differ model type try flexibl deploy pipelin differ xgb sklearn model deploi minim chang send help close smash hair tear laptop struggl dai",
        "Question_preprocessed_content":"deploi pre train sklearn model start understand question ask multipl time haven solut problem start save local train sklearn randomforest upload folder call code infer script call simpl far local jupyt session code script turn folder file come main script ran mainli want serverless deploy fail error messag real time deploy perman stuck creat cloudwatch follow error error handl request attributeerror nonetyp object attribut startswith traceback copi past stop work attach imag instead error messag endpoint arn endpointnam endpointarn endpointconfignam endpointstatu creat creationtim tzinfo tzlocal lastmodifiedtim tzinfo tzlocal responsemetadata retryattempt permiss associ role wrong tri differ folder structur zip file differ account avail want us method know us serverless inconcist differ model type model deploi minim chang send help close smash hair tear laptop struggl dai",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a pre-trained sklearn model on AWS Sagemaker. They have saved the model using joblib.dump and uploaded it to S3, along with an inference script. They have also created a tar.gz file with the model data and inference code and uploaded it to a bucket. The user is attempting to create a serverless endpoint configuration, but it fails with an error message indicating that the model cannot be started within the allotted 180-second timeout. The user has checked Cloudwatch logs and found an error handling request \/ping with an AttributeError. The user has tried different folder structures for the zip file and different accounts, but the issue persists. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user face challeng deploi pre train sklearn model save model joblib dump upload infer script creat tar file model data infer code upload bucket user attempt creat serverless endpoint configur fail error messag indic model start allot second timeout user check cloudwatch log error handl request ping attributeerror user tri differ folder structur zip file differ account issu persist user seek help resolv issu",
        "Answer_original_content":"solv problem model model load model data call deploi method aforement model object deploi infer script model file place notebook directli call gave error earlier",
        "Answer_preprocessed_content":"solv problem load model data call deploi method aforement model object deploi infer script model file place notebook directli call gave error earlier",
        "Answer_gpt_summary_original":"the solution to the problem of a pre-trained sklearn model getting stuck on creating while deploying on an endpoint is to use .model.model to load in the model data and call the deploy method on the model object. additionally, keeping the inference script and model file in the same place as the notebook can also help avoid errors.",
        "Answer_gpt_summary":"solut problem pre train sklearn model get stuck creat deploi endpoint us model model load model data deploi method model object addition keep infer script model file place notebook help avoid error"
    },
    {
        "Question_id":60190365.0,
        "Question_title":"Sagemaker with tensorflow 2 not saving model",
        "Question_body":"<p>I am working with Keras and I am trying to train a model using Sagemaker. I have the following issue:\nWhen I train my model using TensorFlow 1.12 everything works fine:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                            base_job_name='mlearning-test',\n                         role=role,\n                         train_instance_count=1,\n                         input_mode='Pipe',\n                         train_instance_type='ml.p2.xlarge',\n                         framework_version='1.12.0')\n<\/code><\/pre>\n\n<p>My model is trained and the model is saved in S3. Not problems.<\/p>\n\n<p>However, if I changed the framework version to be 2.0.0<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                                base_job_name='mlearning-test',\n                             role=role,\n                             train_instance_count=1,\n                             input_mode='Pipe',\n                             train_instance_type='ml.p2.xlarge',\n                             framework_version='2.0.0')\n<\/code><\/pre>\n\n<p>I get the following error: <\/p>\n\n<pre><code>2020-02-12 13:54:36,601 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path \/opt\/ml\/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#adapting-your-local-tensorflow-script\n<\/code><\/pre>\n\n<p>The training job is marked as successful but there is nothing in the S3 bucket and indeed there was not training.<\/p>\n\n<p>As an alternative, I tried putting the py_version='py3' but this keeps happening. is there a major difference that I am not aware of when using TF2 on sagemaker? <\/p>\n\n<p>I don't think the entry point is needed since it works fine with version 1.12 but in case you are curious or can spot something here it is:<\/p>\n\n<pre><code>import tensorflow as tf\nfrom sagemaker_tensorflow import PipeModeDataset\n#from tensorflow.contrib.data import map_and_batch\n\nINPUT_TENSOR_NAME = 'inputs_input'  \nBATCH_SIZE = 64\nNUM_CLASSES = 5\nBUFFER_SIZE = 50\nPREFETCH_SIZE = 1\nLENGHT = 512\nSEED = 26\nEPOCHS = 1\nWIDTH = 512\n\ndef keras_model_fn(hyperparameters):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(WIDTH, 'relu', input_shape=(None, WIDTH), name = 'inputs'),\n        #tf.keras.layers.InputLayer(input_shape=(None, WIDTH), name=INPUT_TENSOR_NAME),\n        tf.keras.layers.Dense(256, 'relu'),\n        tf.keras.layers.Dense(128, 'relu'),\n        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    opt = tf.keras.optimizers.RMSprop()\n\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n    return model\n\ndef serving_input_fn(hyperparameters):\n    # Notice that the input placeholder has the same input shape as the Keras model input\n    tensor = tf.placeholder(tf.float32, shape=[None, WIDTH])\n\n    # The inputs key INPUT_TENSOR_NAME matches the Keras InputLayer name\n    inputs = {INPUT_TENSOR_NAME: tensor}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\ndef train_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during training\"\"\"\n    return _input_fn('train')\n\ndef eval_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during evaluation\"\"\"\n    return _input_fn('eval')\n\ndef _input_fn(channel):\n    \"\"\"Returns a Dataset for reading from a SageMaker PipeMode channel.\"\"\"\n    print(\"DATA \"+channel)\n    features={\n        'question': tf.FixedLenFeature([WIDTH], tf.float32),\n        'label': tf.FixedLenFeature([1], tf.int64)\n    }\n\n    def parse(record):\n        parsed = tf.parse_single_example(record, features)\n        #print(\"--------&gt;\"+str(tf.cast(parsed['question'], tf.float32))\n        return {\n            INPUT_TENSOR_NAME: tf.cast(parsed['question'], tf.float32)\n        }, parsed['label']\n\n    ds = PipeModeDataset(channel)\n    if EPOCHS &gt; 1:\n        ds = ds.repeat(EPOCHS)\n    ds = ds.prefetch(PREFETCH_SIZE)\n    #ds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n    #                            num_parallel_batches=BUFFER_SIZE))\n    ds = ds.map(parse, num_parallel_calls=NUM_PARALLEL_BATCHES)\n    ds = ds.batch(BATCH_SIZE)\n\n    return ds\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1581516927893,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":913.0,
        "Answer_body":"<p>you're correct, <strong>there has been a major, beneficial change last year in the SageMaker TensorFlow experience named the <em>Script Mode<\/em> formalism<\/strong>. As you can see in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">SDK Documentation<\/a>:<\/p>\n\n<p><em>\"Warning.\nWe have added a new format of your TensorFlow training script with TensorFlow version 1.11. This new way gives the user script more flexibility. This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions. In addition we are adding Python 3 support with Script Mode. The last supported version of Legacy Mode will be TensorFlow 1.12. Script Mode is available with TensorFlow version 1.11 and newer. Make sure you refer to the correct version of this README when you prepare your script. You can find the Legacy Mode README here.\"<\/em><\/p>\n\n<p>with TensorFlow 2, you need to follow that <em>Script Mode<\/em> formalism and save your model in the <code>opt\/ml\/model<\/code> path, otherwise nothing will be sent to S3. <em>Script Mode<\/em> is quite straightforward to implement and gives better flexibility and portability, and this spec is shared with SageMaker Sklearn container, SageMaker Pytorch container and SageMaker MXNet container so definitely worth adopting<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60190365",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581724456863,
        "Question_original_content":"tensorflow save model work kera try train model follow issu train model tensorflow work fine estim tensorflow entri point entrypoint base job mlearn test role role train instanc count input mode pipe train instanc type xlarg framework version model train model save problem chang framework version estim tensorflow entri point entrypoint base job mlearn test role role train instanc count input mode pipe train instanc type xlarg framework version follow error tensorflow contain train warn model artifact save path opt model train job save model file detail construct train script http readthedoc stabl html adapt local tensorflow script train job mark success bucket train altern tri put version keep happen major differ awar think entri point need work fine version case curiou spot import tensorflow tensorflow import pipemodedataset tensorflow contrib data import map batch input tensor input input batch size num class buffer size prefetch size lenght seed epoch width def kera model hyperparamet model kera sequenti kera layer dens width relu input shape width input kera layer inputlay input shape width input tensor kera layer dens relu kera layer dens relu kera layer dens num class activ softmax opt kera optim rmsprop model compil loss categor crossentropi optim opt metric accuraci return model def serv input hyperparamet notic input placehold input shape kera model input tensor placehold float shape width input kei input tensor match kera inputlay input input tensor tensor return estim export servinginputreceiv input input def train input train dir param return input function feed model train return input train def eval input train dir param return input function feed model evalu return input eval def input channel return dataset read pipemod channel print data channel featur question fixedlenfeatur width float label fixedlenfeatur int def pars record pars pars singl exampl record featur print str cast pars question float return input tensor cast pars question float pars label pipemodedataset channel epoch repeat epoch prefetch prefetch size appli map batch pars batch size batch size num parallel batch buffer size map pars num parallel call num parallel batch batch batch size return",
        "Question_preprocessed_content":"tensorflow save model work kera try train model follow issu train model tensorflow work fine model train model save problem chang framework version follow error train job mark success bucket train altern tri put keep happen major differ awar think entri point need work fine version case curiou spot",
        "Question_gpt_summary_original":"The user is facing challenges while trying to save a model in Sagemaker using TensorFlow 2.0.0. The training job is marked as successful but the model is not saved in the S3 bucket. The user has tried changing the py_version to 'py3' but the issue persists. The user has shared the entry point code for reference.",
        "Question_gpt_summary":"user face challeng try save model tensorflow train job mark success model save bucket user tri chang version issu persist user share entri point code refer",
        "Answer_original_content":"correct major benefici chang year tensorflow experi name script mode formal sdk document warn ad new format tensorflow train script tensorflow version new wai give user script flexibl new format call script mode oppos legaci mode support tensorflow older version addit ad python support script mode support version legaci mode tensorflow script mode avail tensorflow version newer sure refer correct version readm prepar script legaci mode readm tensorflow need follow script mode formal save model opt model path sent script mode straightforward implement give better flexibl portabl spec share sklearn contain pytorch contain mxnet contain definit worth adopt",
        "Answer_preprocessed_content":"correct major benefici chang year tensorflow experi name script mode formal sdk document warn ad new format tensorflow train script tensorflow version new wai give user script flexibl new format call script mode oppos legaci mode support tensorflow older version addit ad python support script mode support version legaci mode tensorflow script mode avail tensorflow version newer sure refer correct version readm prepar script legaci mode readm tensorflow need follow script mode formal save model path sent script mode straightforward implement give better flexibl portabl spec share sklearn contain pytorch contain mxnet contain definit worth adopt",
        "Answer_gpt_summary_original":"possible solutions to the issue of saving a model when training with tensorflow 2.0.0 include following the script mode formalism and saving the model in the opt\/ml\/model path. script mode is a more flexible and portable way of implementing tensorflow and is shared with other containers such as sklearn, pytorch, and mxnet.",
        "Answer_gpt_summary":"possibl solut issu save model train tensorflow includ follow script mode formal save model opt model path script mode flexibl portabl wai implement tensorflow share contain sklearn pytorch mxnet"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to import a tensorflow model to spark",
        "Question_body":"I have a requirement where I am doing a training and exporting a keras\/tensorflow model using mlflow. Is it possile to import that model to spark mlflow and do scoring there? I could not find such an option in documentation. When I tried the same I am getting the expection MlflowException: Model does not have the \"spark\" flavor\nml flow and then import that ml flow model into spark and do scoring there",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548755952000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":24.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Bx6GadlNgbE",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-02-04T17:58:35",
                "Answer_body":"It should be possible to import it using mlflow.pyfunc.spark_udf (https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.pyfunc.html#mlflow.pyfunc.spark_udf). Have you tried that?\n\nMatei\n\n\n> On Jan 29, 2019, at 6:59 AM, Kailas JC <kail...@gmail.com> wrote:\n>\n> I have a requirement where I am doing a training and exporting a keras\/tensorflow model using mlflow. Is it possile to import that model to spark mlflow and do scoring there? I could not find such an option in documentation. When I tried the same I am getting the expection MlflowException: Model does not have the \"spark\" flavor\n> ml flow and then import that ml flow model into spark and do scoring there\n>\n>\n\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/374ed4bd-513c-42d0-9881-db62ac82066c%40googlegroups.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"possibl import tensorflow model spark requir train export kera tensorflow model possil import model spark score option document tri get expect except model spark flavor flow import flow model spark score",
        "Question_preprocessed_content":"possibl import tensorflow model spark requir train export model possil import model spark score option document tri get expect except model spark flavor flow import flow model spark score",
        "Question_gpt_summary_original":"The user is facing a challenge of importing a keras\/tensorflow model exported using mlflow to spark mlflow for scoring. The user could not find an option in the documentation and encountered an exception stating that the model does not have the \"spark\" flavor.",
        "Question_gpt_summary":"user face challeng import kera tensorflow model export spark score user option document encount except state model spark flavor",
        "Answer_original_content":"possibl import pyfunc spark udf http org doc latest python api pyfunc html pyfunc spark udf tri matei jan kaila wrote requir train export kera tensorflow model possil import model spark score option document tri get expect except model spark flavor flow import flow model spark score receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user edbd dbacc googlegroup com option visit http group googl com optout",
        "Answer_preprocessed_content":"possibl import tri matei jan kaila wrote requir train export model possil import model spark score option document tri get expect except model spark flavor flow import flow model spark score receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution: It is suggested to use mlflow.pyfunc.spark_udf to import the model into Spark and do scoring there.",
        "Answer_gpt_summary":"solut suggest us pyfunc spark udf import model spark score"
    },
    {
        "Question_id":57403281.0,
        "Question_title":"Installing textshape package for Microsoft R Open 3.4.4 on Azure ML Studio",
        "Question_body":"<p>I'm trying to use the R <code>sentimentr<\/code> package on Azure ML Studio. As this package is not supported, I'm trying to install it and its dependencies as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-r-script#bkmk_AddingANewPackage\" rel=\"nofollow noreferrer\">in the documentation<\/a>.<\/p>\n\n<p>The steps that I have performed are:<\/p>\n\n<ul>\n<li><p>downloaded Windows binaries from the R Open 3.4.4 snapshot at <a href=\"https:\/\/mran.microsoft.com\/timemachine\" rel=\"nofollow noreferrer\">CRAN time machine<\/a><\/p>\n\n<ul>\n<li><code>sentimentr_2.2.3.zip<\/code><\/li>\n<li><code>syuzhet_1.0.4.zip<\/code><\/li>\n<li><code>textclean_0.6.3.zip<\/code><\/li>\n<li><code>lexicon_0.7.4.zip<\/code><\/li>\n<li><code>textshape_1.5.0.zip<\/code> <\/li>\n<\/ul><\/li>\n<li><p>zipped those zip files into a zipped folder <code>packages.zip<\/code><\/p><\/li>\n<li>uploaded <code>packages.zip<\/code> as a dataset to Microsoft Azure ML Studio<\/li>\n<\/ul>\n\n<p>In my ML experiment I connect the <code>packages.zip<\/code> dataset to the \"Script Bundle (Zip)\" input port on \"Execute R Script\" and include this code:<\/p>\n\n<pre><code># install R package contained in src  \ninstall.packages(\"src\/lexicon_0.7.4.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/textclean_0.6.3.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/textshape_1.5.0.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/syuzhet_1.0.4.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\ninstall.packages(\"src\/sentimentr_2.2.3.zip\", \n                 lib = \".\", \n                 repos = NULL, \n                 verbose = TRUE)  \n\n# load libraries\nlibrary(sentimentr, lib.loc = \".\", verbose = TRUE)\n<\/code><\/pre>\n\n<p>The experiment runs successfully, until I include a function from <code>sentimentr<\/code>:<\/p>\n\n<pre><code>mydata &lt;- mydata %&gt;%\n  get_sentences() %&gt;%\n  sentiment()\n<\/code><\/pre>\n\n<p>This gives the error:<\/p>\n\n<blockquote>\n  <p>there is no package called 'textshape'<\/p>\n<\/blockquote>\n\n<p>Which is difficult to understand given that the output log does not indicate an issue with the packages:<\/p>\n\n<pre><code>[Information]         The following files have been unzipped for sourcing in path=[\"src\"]:\n[Information]                           Name  Length                Date\n[Information]         1 sentimentr_2.2.3.zip 3366245 2019-08-07 14:57:00\n[Information]         2    syuzhet_1.0.4.zip 2918474 2019-08-07 15:05:00\n[Information]         3  textclean_0.6.3.zip 1154814 2019-08-07 15:13:00\n[Information]         4    lexicon_0.7.4.zip 4551995 2019-08-07 15:17:00\n[Information]         5  textshape_1.5.0.zip  463095 2019-08-07 15:42:00\n[Information]         Loading objects:\n[Information]           port1\n[Information]         [1] \"Loading variable port1...\"\n[Information]         package 'lexicon' successfully unpacked and MD5 sums checked   \n[Information]         package 'textclean' successfully unpacked and MD5 sums checked\n[Information]         package 'textshape' successfully unpacked and MD5 sums checked\n[Information]         package 'syuzhet' successfully unpacked and MD5 sums checked\n[Information]         package 'sentimentr' successfully unpacked and MD5 sums checked\n<\/code><\/pre>\n\n<p>Has anyone seen this, or similar issues? Is it possible that \"successfully unpacked\" is not the same as successfully installed and usable?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1565219530070,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>I can now answer my own question thanks to <a href=\"https:\/\/twitter.com\/bryan_hepworth\/status\/1159432174225055749\" rel=\"nofollow noreferrer\">a hint on Twitter<\/a> from @bryan_hepworth.<\/p>\n\n<p>The R packages were installed correctly, but not in the standard library location. So when a function from <code>sentimentr<\/code> runs, R tries to load the dependency package <code>textshape<\/code>:<\/p>\n\n<pre><code>library(textshape)\n<\/code><\/pre>\n\n<p>Which of course does not exist <em>in the standard location<\/em> as Azure ML does not support it.<\/p>\n\n<p>The solution is to load <code>textshape<\/code> explicitly from its installed location:<\/p>\n\n<pre><code>library(textshape, lib.loc = \".\")\n<\/code><\/pre>\n\n<p>So the solution is: explicitly load packages that you installed at the start of your R code, rather than letting R try to load them as dependencies, which will fail.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57403281",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1565302474167,
        "Question_original_content":"instal textshap packag microsoft open studio try us sentimentr packag studio packag support try instal depend describ document step perform download window binari open snapshot cran time machin sentimentr zip syuzhet zip textclean zip lexicon zip textshap zip zip zip file zip folder packag zip upload packag zip dataset studio experi connect packag zip dataset script bundl zip input port execut script includ code instal packag contain src instal packag src lexicon zip lib repo null verbos true instal packag src textclean zip lib repo null verbos true instal packag src textshap zip lib repo null verbos true instal packag src syuzhet zip lib repo null verbos true instal packag src sentimentr zip lib repo null verbos true load librari librari sentimentr lib loc verbos true experi run successfulli includ function sentimentr mydata sentenc sentiment give error packag call textshap difficult understand given output log indic issu packag inform follow file unzip sourc path src inform length date inform sentimentr zip inform syuzhet zip inform textclean zip inform lexicon zip inform textshap zip inform load object inform port inform load variabl port inform packag lexicon successfulli unpack sum check inform packag textclean successfulli unpack sum check inform packag textshap successfulli unpack sum check inform packag syuzhet successfulli unpack sum check inform packag sentimentr successfulli unpack sum check seen similar issu possibl successfulli unpack successfulli instal usabl",
        "Question_preprocessed_content":"instal textshap packag microsoft open studio try us packag studio packag support try instal depend describ document step perform download window binari open snapshot cran time machin zip zip file zip folder upload dataset studio experi connect dataset script bundl input port execut script includ code experi run successfulli includ function give error packag call textshap difficult understand given output log indic issu packag seen similar issu possibl successfulli unpack successfulli instal usabl",
        "Question_gpt_summary_original":"The user is trying to install the R sentimentr package on Azure ML Studio, which is not supported. They have downloaded the required packages and uploaded them as a dataset to Azure ML Studio. The experiment runs successfully until they include a function from sentimentr, which gives an error indicating that the textshape package is missing, even though the output log shows that all packages have been successfully unpacked. The user is seeking help to understand the issue and whether successfully unpacked means successfully installed and usable.",
        "Question_gpt_summary":"user try instal sentimentr packag studio support download requir packag upload dataset studio experi run successfulli includ function sentimentr give error indic textshap packag miss output log show packag successfulli unpack user seek help understand issu successfulli unpack mean successfulli instal usabl",
        "Answer_original_content":"answer question thank hint twitter bryan hepworth packag instal correctli standard librari locat function sentimentr run tri load depend packag textshap librari textshap cours exist standard locat support solut load textshap explicitli instal locat librari textshap lib loc solut explicitli load packag instal start code let try load depend fail",
        "Answer_preprocessed_content":"answer question thank hint twitter packag instal correctli standard librari locat function run tri load depend packag cours exist standard locat support solut load explicitli instal locat solut explicitli load packag instal start code let try load depend fail",
        "Answer_gpt_summary_original":"the solution to the challenge of installing the textshape package for microsoft r open 3.4.4 on studio is to explicitly load packages that were installed at the start of the r code, rather than letting r try to load them as dependencies, which will fail. specifically, the user should load textshape explicitly from its installed location using the command \"library(textshape, lib.loc = \".\")\".",
        "Answer_gpt_summary":"solut challeng instal textshap packag microsoft open studio explicitli load packag instal start code let try load depend fail specif user load textshap explicitli instal locat command librari textshap lib loc"
    },
    {
        "Question_id":null,
        "Question_title":"Sweep, how is the optimisation metric selected in bayesian optimisation",
        "Question_body":"<p>Hi every one,<\/p>\n<p>When using a sweep, the selection of a metric that need to beoptimized is required when using bayesian optimisation.<\/p>\n<p>I wanted to know if for the selection of the next critierion for the next runs, the bayesian optimisation is based on the value of that metric at the end of the run (last epoch) or on the highest value reached by the metric during the run ?<\/p>\n<p>Because in the first case, if I choose a metric calculated over my validation dataset, it performance may decrease during the training because of overfitting, then the value of my metric at the end would not reflect the best performance of my model.<\/p>\n<p>Thanks for your help<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1663336094344,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":78.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-how-is-the-optimisation-metric-selected-in-bayesian-optimisation\/3126",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-19T22:16:04.437Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/felix_quinton\">@felix_quinton<\/a> ,  please visit <a href=\"https:\/\/wandb.ai\/site\/articles\/bayesian-hyperparameter-optimization-a-primer\">this detailed article<\/a> on the specifics of how Bayesian optimization works. If you still have any questions please let us know.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-21T15:30:31.835Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>Thanks for your time, i read this article but it doesn\u2019t seems to answer my question, I might have been unclear.<\/p>\n<p>Lets take an example, :<\/p>\n<p>I want to found the best hyperparameter configuration for my model over 10 runs with bayesian search.<br>\nI choose the accuracy over my validation dataset as a metric to maximise.  I train all my runs over 1000 epochs.<\/p>\n<p>For the run 1:<\/p>\n<ul>\n<li>The  run achieve it\u2019s best value of accuracy over validation dataset at the epoch 700, with a value  of 0.60<\/li>\n<li>After that the run start to overfit and the value of accuracy over validation decrease to 0.40 at epoch 1000<\/li>\n<\/ul>\n<p>For the run 2:<\/p>\n<ul>\n<li>The  run achieve it\u2019s best value of accuracy over validation dataset at the epoch 700, with a value  of 0.50<\/li>\n<li>After that the run start to overfit and the value of accuracy over validation decrease to 0.45 at epoch 1000<\/li>\n<\/ul>\n<p>In my eyes, I would considered the run 1 as a better run since this configuration as reached the highest results with a maximum score of 0.60 compare to 0.50 for the run 2.<\/p>\n<p>But the process seems to only care of the value reached at the end of the training to consider the quality of the run. Meaning that, in this case, the run 2 would appears as a better run with a score of 0.45 compared to 0.40 for the run 1. And so the hyperparameter combination of run 2 would be  have more influence than the one of run 1 in the bayesian optimisation process.<\/p>\n<p>Am I right ?<\/p>\n<p>Thanks.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-20T15:30:36.454Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"sweep optimis metric select bayesian optimis sweep select metric need beoptim requir bayesian optimis want know select critierion run bayesian optimis base valu metric end run epoch highest valu reach metric run case choos metric calcul valid dataset perform decreas train overfit valu metric end reflect best perform model thank help",
        "Question_preprocessed_content":"sweep optimis metric select bayesian optimis sweep select metric need beoptim requir bayesian optimis want know select critierion run bayesian optimis base valu metric end run highest valu reach metric run case choos metric calcul valid dataset perform decreas train overfit valu metric end reflect best perform model thank help",
        "Question_gpt_summary_original":"The user is seeking clarification on how the metric for optimization is selected in Bayesian optimization when using a sweep. They are unsure if the selection is based on the value of the metric at the end of the run or the highest value reached during the run. The user is concerned that if they choose a metric calculated over the validation dataset, the performance may decrease during training due to overfitting, and the final metric value may not reflect the best performance of the model.",
        "Question_gpt_summary":"user seek clarif metric optim select bayesian optim sweep unsur select base valu metric end run highest valu reach run user concern choos metric calcul valid dataset perform decreas train overfit final metric valu reflect best perform model",
        "Answer_original_content":"felix quinton visit detail articl specif bayesian optim work question let know thank time read articl doesnt answer question unclear let exampl want best hyperparamet configur model run bayesian search choos accuraci valid dataset metric maximis train run epoch run run achiev best valu accuraci valid dataset epoch valu run start overfit valu accuraci valid decreas epoch run run achiev best valu accuraci valid dataset epoch valu run start overfit valu accuraci valid decreas epoch ey consid run better run configur reach highest result maximum score compar run process care valu reach end train consid qualiti run mean case run appear better run score compar run hyperparamet combin run influenc run bayesian optimis process right thank topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"visit detail articl specif bayesian optim work question let know thank time read articl doesnt answer question unclear let exampl want best hyperparamet configur model run bayesian search choos accuraci valid dataset metric maximis train run epoch run run achiev best valu accuraci valid dataset epoch valu run start overfit valu accuraci valid decreas epoch run run achiev best valu accuraci valid dataset epoch valu run start overfit valu accuraci valid decreas epoch ey consid run better run configur reach highest result maximum score compar run process care valu reach end train consid qualiti run mean case run appear better run score compar run hyperparamet combin run influenc run bayesian optimis process right thank topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user is seeking clarification on how the optimization metric is selected when using bayesian optimization with a sweep, and whether the metric is based on the value at the end of the run or the highest value reached during the run. the answer suggests visiting a detailed article on the specifics of how bayesian optimization works. however, the user provides an example of two runs with different hyperparameter configurations and asks whether the process only cares about the value reached at the end of the training to consider the quality of the run. the answer does not provide any solutions to the user's question.",
        "Answer_gpt_summary":"user seek clarif optim metric select bayesian optim sweep metric base valu end run highest valu reach run answer suggest visit detail articl specif bayesian optim work user provid exampl run differ hyperparamet configur ask process care valu reach end train consid qualiti run answer provid solut user question"
    },
    {
        "Question_id":72490682.0,
        "Question_title":"How to create an aws sagemaker project using terraform?",
        "Question_body":"<p>This is the terraform shown in the docs:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>I created a service catalog product with id: &quot;prod-xxxxxxxxxxxxx&quot;.\nWhen I substitute the service catalog product id into the above template,\nto get the following:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.prod-xxxxxxxxxxxxx\n  }\n}\n<\/code><\/pre>\n<p>I run terraform plan, but the following error occurs:<\/p>\n<pre><code>A managed resource &quot;aws_servicecatalog_product&quot; &quot;prod-xxxxxxxxxxxxx&quot; has not been declared in the root module.\n\n<\/code><\/pre>\n<p>What do I need to do to fix this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1654264489180,
        "Question_favorite_count":null,
        "Question_last_edit_time":1654266014543,
        "Question_score":1.0,
        "Question_view_count":298.0,
        "Answer_body":"<p>Since the documentation is lacking a bit of clarity, in order to have this work as in the example, you would first have to create the Service Catalog product in Terraform as well, e.g.:<\/p>\n<pre><code>resource &quot;aws_servicecatalog_product&quot; &quot;example&quot; {\n  name  = &quot;example&quot;\n  owner = [aws_security_group.example.id] # &lt;---- This would need to be created first\n  type  = aws_subnet.main.id # &lt;---- This would need to be created first\n\n  provisioning_artifact_parameters {\n    template_url = &quot;https:\/\/s3.amazonaws.com\/cf-templates-ozkq9d3hgiq2-us-east-1\/temp1.json&quot;\n  }\n\n  tags = {\n    foo = &quot;bar&quot;\n  }\n}\n<\/code><\/pre>\n<p>You can reference it then in the SageMaker project the same way as in the example:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = aws_servicecatalog_product.example.id\n  }\n}\n<\/code><\/pre>\n<p>Each of the resources that gets created has a set of attributes that can be accessed as needed by other resources, data sources or outputs. In order to understand how this works, I strongly suggest reading the documentation about referencing values [1]. Since you already created the Service Catalog product, the only thing you need to do is provide the string value for the product ID:<\/p>\n<pre><code>resource &quot;aws_sagemaker_project&quot; &quot;example&quot; {\n  project_name = &quot;example&quot;\n\n  service_catalog_provisioning_details {\n    product_id = &quot;prod-xxxxxxxxxxxxx&quot;\n  }\n}\n<\/code><\/pre>\n<p>When I can't understand what value is expected by an argument (e.g., <code>product_id<\/code> in this case), I usually read the docs and look for examples like in [2]. Note: That example is CloudFormation, but it can help you understand what type of a value is expected (e.g., string, number, bool).<\/p>\n<p>You could also import the created Service Catalog product into Terraform so you can manage it with IaC [3]. You should understand all the implications of <code>terraform import<\/code> though before trying it [4].<\/p>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/references\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/references<\/a><\/p>\n<p>[2] <a href=\"https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example\" rel=\"nofollow noreferrer\">https:\/\/docs.amazonaws.cn\/en_us\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-project.html#aws-resource-sagemaker-project--examples--SageMaker_Project_Example<\/a><\/p>\n<p>[3] <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import\" rel=\"nofollow noreferrer\">https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/servicecatalog_product#import<\/a><\/p>\n<p>[4] <a href=\"https:\/\/www.terraform.io\/cli\/commands\/import\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/cli\/commands\/import<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72490682",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1654269294487,
        "Question_original_content":"creat project terraform terraform shown doc resourc aw project exampl project exampl servic catalog provis detail product aw servicecatalog product exampl creat servic catalog product prod substitut servic catalog product templat follow resourc aw project exampl project exampl servic catalog provis detail product aw servicecatalog product prod run terraform plan follow error occur manag resourc aw servicecatalog product prod declar root modul need fix error",
        "Question_preprocessed_content":"creat project terraform terraform shown doc creat servic catalog product substitut servic catalog product templat follow run terraform plan follow error occur need fix error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to create an AWS Sagemaker project using Terraform. The error message indicates that a managed resource has not been declared in the root module. The user is seeking guidance on how to fix this error.",
        "Question_gpt_summary":"user encount error try creat project terraform error messag indic manag resourc declar root modul user seek guidanc fix error",
        "Answer_original_content":"document lack bit clariti order work exampl creat servic catalog product terraform resourc aw servicecatalog product exampl exampl owner aw secur group exampl need creat type aw subnet main need creat provis artifact paramet templat url http amazonaw com templat ozkqdhgiq east temp json tag foo bar refer project wai exampl resourc aw project exampl project exampl servic catalog provis detail product aw servicecatalog product exampl resourc get creat set attribut access need resourc data sourc output order understand work strongli suggest read document referenc valu creat servic catalog product thing need provid string valu product resourc aw project exampl project exampl servic catalog provis detail product prod understand valu expect argument product case usual read doc look exampl like note exampl cloudform help understand type valu expect string number bool import creat servic catalog product terraform manag iac understand implic terraform import try http terraform languag express refer http doc amazonaw awscloudform latest userguid aw resourc project html aw resourc project exampl project exampl http registri terraform provid hashicorp aw latest doc resourc servicecatalog product import http terraform cli command import",
        "Answer_preprocessed_content":"document lack bit clariti order work exampl creat servic catalog product terraform refer project wai exampl resourc get creat set attribut access need resourc data sourc output order understand work strongli suggest read document referenc valu creat servic catalog product thing need provid string valu product understand valu expect argument usual read doc look exampl like note exampl cloudform help understand type valu expect import creat servic catalog product terraform manag iac understand implic try",
        "Answer_gpt_summary_original":"the solution to the error encountered when creating an aws project using terraform is to create the service catalog product in terraform first, and then reference it in the project. the resources created have attributes that can be accessed by other resources, data sources, or outputs. the user can provide the string value for the product id or import the created service catalog product into terraform to manage it with iac. the user is advised to read the documentation and understand the implications of terraform import before trying it.",
        "Answer_gpt_summary":"solut error encount creat aw project terraform creat servic catalog product terraform refer project resourc creat attribut access resourc data sourc output user provid string valu product import creat servic catalog product terraform manag iac user advis read document understand implic terraform import try"
    },
    {
        "Question_id":null,
        "Question_title":"Logging graph plot in pyspark form into mlflow",
        "Question_body":"I am trying to log K elbow result in pyspark form into mlflow, any thought on how this could be done?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573769729000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/lrZwJNTVH6M",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-11-15T03:00:52",
                "Answer_body":"Sure, just save it as an image file, and then log the image in the run with log_artifact(). It will render if you drill into it in the run in the tracking server.\n\n\nOn Thu, Nov 14, 2019 at 9:15 PM gs <seahg...@gmail.com> wrote:\n\nI am trying to log K elbow result in pyspark form into mlflow, any thought on how this could be done?\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/b3f5b5a9-8e2f-4527-87d9-0909c102542e%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"log graph plot pyspark form try log elbow result pyspark form thought",
        "Question_preprocessed_content":"log graph plot pyspark form try log elbow result pyspark form thought",
        "Question_gpt_summary_original":"The user is facing a challenge of logging a K elbow result in pyspark form into mlflow and is seeking suggestions on how to accomplish this task.",
        "Question_gpt_summary":"user face challeng log elbow result pyspark form seek suggest accomplish task",
        "Answer_original_content":"sure save imag file log imag run log artifact render drill run track server thu nov wrote try log elbow result pyspark form thought receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user bfba googlegroup com",
        "Answer_preprocessed_content":"sure save imag file log imag run render drill run track server thu nov wrote try log elbow result pyspark form thought receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Solution: The suggested solution is to save the K elbow result as an image file and then log it in the run with log_artifact(). It will render if you drill into it in the run in the tracking server.",
        "Answer_gpt_summary":"solut suggest solut save elbow result imag file log run log artifact render drill run track server"
    },
    {
        "Question_id":null,
        "Question_title":"How to package custom prediction code and serve it using an Endpoint in Vertex AI ?",
        "Question_body":"Goal: serve prediction request from a Vertex AI Endpoint by executing custom prediction logic.Expected Workflow:1. Upload a pretrained image_quality.pb model (developed in a non vertex-ai pythonic environment) in a gcs bucket2. Port existing image inference logic into a container and serve the prediction functionality through a vertex AI endpoint. 3. Use Vertex AI api for logging and capturing metrics inside the  custom inference logic.4. Finally we want to pass a list of images (stored in another gcs bucket) to that endpoint.5. We also want to see the logs and metrics in tensorboard.Existing Vertex AI code samples provide examples for custom training , invoking model.batch_predict \/ endpoint.predict , but don't mention how to execute custom prediction code.It would be great if someone can provide guidelines and links to documents\/code in order to implement the above steps.Thanks  ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635161760000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":204.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-package-custom-prediction-code-and-serve-it-using-an\/td-p\/173876\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-19T07:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Please find the following guides respective of the points\n\n1.\n[1] Import model:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/import-model\n[2] What cannot be migrated:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/start\/migrating-to-vertex-ai#migration-exceptions\n2.\n[3] Custom containers:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/containers-overview\n[4] https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/create-custom-container\n3.\n[5] About metrics:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/monitoring-metrics\n4.\n[6] Passing list of images:\u00a0https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/create-dataset-api\n5.\n[7] Metrics in Tensorboard:\u00a0https:\/\/cloud.google.com\/architecture\/ml-on-gcp-best-practices?hl=en#use-vertex-tensorboard-to-visua...\n\n\nAs there is no existing unifying guide for these operations, I created a documentation feature request to have one, and asked the documentation team to post updates here."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"packag custom predict code serv endpoint goal serv predict request endpoint execut custom predict logic expect workflow upload pretrain imag qualiti model develop non vertex python environ gc bucket port exist imag infer logic contain serv predict function endpoint us api log captur metric insid custom infer logic final want pass list imag store gc bucket endpoint want log metric tensorboard exist code sampl provid exampl custom train invok model batch predict endpoint predict mention execut custom predict code great provid guidelin link document code order implement step thank",
        "Question_preprocessed_content":"packag custom predict code serv endpoint goal serv predict request endpoint execut custom predict workflow upload pretrain model gc bucket port exist imag infer logic contain serv predict function endpoint us api log captur metric insid custom infer final want pass list imag want log metric code sampl provid exampl custom train invok mention execut custom predict great provid guidelin link order implement",
        "Question_gpt_summary_original":"The user is facing challenges in packaging custom prediction code and serving it using an Endpoint in Vertex AI. They need to upload a pretrained model in a GCS bucket, port existing inference logic into a container, use Vertex AI API for logging and capturing metrics, and pass a list of images to the endpoint. Existing Vertex AI code samples provide examples for custom training but don't mention how to execute custom prediction code. The user is seeking guidelines and links to documents\/code to implement the required steps.",
        "Question_gpt_summary":"user face challeng packag custom predict code serv endpoint need upload pretrain model gc bucket port exist infer logic contain us api log captur metric pass list imag endpoint exist code sampl provid exampl custom train mention execut custom predict code user seek guidelin link document code implement requir step",
        "Answer_original_content":"follow guid respect point import model http cloud googl com vertex doc gener import model migrat http cloud googl com vertex doc start migrat vertex migrat except custom contain http cloud googl com vertex doc train contain overview http cloud googl com vertex doc train creat custom contain metric http cloud googl com vertex doc gener monitor metric pass list imag http cloud googl com vertex doc dataset creat dataset api metric tensorboard http cloud googl com architectur gcp best practic us vertex tensorboard visua exist unifi guid oper creat document featur request ask document team post updat",
        "Answer_preprocessed_content":"follow guid respect point import custom pass list metric exist unifi guid oper creat document featur request ask document team post updat",
        "Answer_gpt_summary_original":"the answer provides several guides for packaging custom prediction code and serving it using an endpoint. these include importing models, creating custom containers, monitoring metrics, and passing lists of images. the answer also mentions a documentation feature request for a unifying guide on these operations.",
        "Answer_gpt_summary":"answer provid guid packag custom predict code serv endpoint includ import model creat custom contain monitor metric pass list imag answer mention document featur request unifi guid oper"
    },
    {
        "Question_id":null,
        "Question_title":"Using R model in SageMaker ML pipelines",
        "Question_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples here. And it doesn't look that R is fully supported currently by ML Pipelines. Any examples and success stories are very welcome.\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643230196748,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":97.0,
        "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sage-maker-ml-pipelines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-28T21:07:43.709Z",
                "Answer_score":1,
                "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643404063708,
        "Question_original_content":"model pipelin possibl us model train serv pipelin look exampl look fulli support current pipelin exampl success stori welcom thank",
        "Question_preprocessed_content":"model pipelin possibl us model train serv pipelin look exampl look fulli support current pipelin exampl success stori welcom thank",
        "Question_gpt_summary_original":"The user is facing challenges in using R model training and serving in SageMaker ML Pipelines as it does not seem to be fully supported currently. They are seeking examples and success stories to overcome this challenge.",
        "Question_gpt_summary":"user face challeng model train serv pipelin fulli support current seek exampl success stori overcom challeng",
        "Answer_original_content":"gener possibl us python sdk boto reticul packag direct exampl pipelin possibl orchestr product pipelin contain train serv set dag reticul python sdk achiev aw step function refer follow exampl refer http github com aw sampl reinvent aim production amazon http youtub com watch zppnfvqdca",
        "Answer_preprocessed_content":"gener possibl us python sdk boto reticul packag direct exampl pipelin possibl orchestr product pipelin contain train serv set dag reticul python sdk achiev aw step function refer follow exampl refer",
        "Answer_gpt_summary_original":"possible solutions for using r models in ml pipelines include using the python sdk and boto3 with the reticulate package in r, orchestrating the production pipeline using r containers for training and serving, and setting up the dag using reticulate and the python sdk. aws step functions can also be used to achieve this. the provided github example and youtube video can be used as references.",
        "Answer_gpt_summary":"possibl solut model pipelin includ python sdk boto reticul packag orchestr product pipelin contain train serv set dag reticul python sdk aw step function achiev provid github exampl youtub video refer"
    },
    {
        "Question_id":null,
        "Question_title":"Sweeps with multiple seeds for the same config values",
        "Question_body":"<p>Hi,<\/p>\n<p>Sometimes (for example in RL) agents are very unstable and you only know how a config behaves if you tested it on 5-10 seeds. So I was wondering if there is a feature in wandb sweeps that allows the aggregation of a metric over multiple seeds (but the same config values)?<\/p>\n<p>I know one solution is to define a for loop in my own training script that repeats the same config, but I would like these runs to be executed in parallel, and possibly even on different machines.<\/p>\n<p>Thanks,<br>\nTom<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1635085861671,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":954.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweeps-with-multiple-seeds-for-the-same-config-values\/1077",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-25T12:49:35.818Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tomjur\">@tomjur<\/a>, this is an interesting question!<br>\nThis is possible if you use the seed as a parameter in your sweep config and then group based on the parameters you care about.<br>\nThere are a bunch of ways to group your runs and aggregate your metrics. You can click the group button above your runs and choose the metrics you care about, or you can group within each plot by editing a plot and clicking the Group tab and choosing the parameter you want to group on.<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping#grouping-dynamically-in-the-ui\">\n  <header class=\"source\">\n      \n\n      <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping#grouping-dynamically-in-the-ui\" target=\"_blank\" rel=\"noopener\">docs.wandb.ai<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_690x362.png\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping#grouping-dynamically-in-the-ui\" target=\"_blank\" rel=\"noopener\">Group Runs<\/a><\/h3>\n\n  <p>Group training and evaluation runs into larger experiments<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":29.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-25T13:05:01.637Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/_scott\">@_scott<\/a> thanks for the replay!<\/p>\n<p>Sorry, but I still don\u2019t understand the solution. Maybe I can try to be more explicit in my description of the problem (in the following I assume bayes \\ random search since I do not have the budget to do a grid search):<\/p>\n<p>Let\u2019s say I defined a distribution over all parameters, and specifically, I defined 3 uniform values for seeds. Now if an agent samples a configuration with seed 1, what forces the hyper-parameter optimization process (in the wandb controller) to select the same configuration again but with seed no. 2?<br>\nIf the optimization is random, it is possible but not likely (especially with a continuous random variable), if the optimization is bayes it is unlikely but might also have the bad side-effect of preferring easy seeds.<\/p>\n<p>I think this might be a common pain point in RL (and possibly GAN) sweeps.<\/p>\n<p>Thanks again!<br>\nTom<\/p>",
                "Answer_score":89.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-25T13:49:16.270Z",
                "Answer_body":"<p>Thanks for sharing more detail about your question. I understand now. In essence, you want a grid search over random seeds while also doing a bayes \/ random search for your other configs, and make sure the configs are the same for each Sweep.<\/p>\n<p>It isn\u2019t currently possible to do this, but this is definitely a feature we will try to support in the future because it\u2019s a common workflow for people like you trying to deterministically run Sweeps. Thank you for adding another +1 to this feature request.<\/p>\n<p>Unfortunately, my only suggestion for now is doing a grid search for each of the configurations you want to test, including the random seed in that search.<\/p>",
                "Answer_score":34.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-25T14:16:33.610Z",
                "Answer_body":"<p>Thank you for adding this as a feature request!<br>\nBTW I am a new user, how can I track the progress on this feature?<\/p>",
                "Answer_score":69.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-25T15:36:30.825Z",
                "Answer_body":"<p>Our CLI is Open Source so you can track the release notes here: <a href=\"https:\/\/github.com\/wandb\/client\/releases\" class=\"inline-onebox\">Releases \u00b7 wandb\/client \u00b7 GitHub<\/a><br>\nAnd here\u2019s the code for Sweeps: <a href=\"https:\/\/github.com\/wandb\/sweeps\" class=\"inline-onebox\">GitHub - wandb\/sweeps: W&amp;B Hyperparameter Sweep Engine. File sweeps related issues at the W&amp;B client: https:\/\/github.com\/wandb\/client<\/a><\/p>",
                "Answer_score":28.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-25T15:37:43.123Z",
                "Answer_body":"<blockquote>\n<p>BTW I am a new user<\/p>\n<\/blockquote>\n<p>Welcome! Hope you\u2019re enjoying using W&amp;B, here to help if you have anymore questions or issues.<br>\n<img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji only-emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":8.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-26T14:13:20.651Z",
                "Answer_body":"<p>Yes W&amp;B is great (: thanks for all the help so far<\/p>",
                "Answer_score":52.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:07.673Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":17.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"sweep multipl seed config valu exampl agent unstabl know config behav test seed wonder featur sweep allow aggreg metric multipl seed config valu know solut defin loop train script repeat config like run execut parallel possibl differ machin thank tom",
        "Question_preprocessed_content":"sweep multipl seed config valu agent unstabl know config behav test seed wonder featur sweep allow aggreg metric multipl seed know solut defin loop train script repeat config like run execut parallel possibl differ machin thank tom",
        "Question_gpt_summary_original":"The user is facing challenges in testing unstable agents in RL and wants to know if there is a feature in wandb sweeps that allows the aggregation of a metric over multiple seeds for the same config values. The user is aware of a solution involving a for loop in their own training script, but wants the runs to be executed in parallel and possibly on different machines.",
        "Question_gpt_summary":"user face challeng test unstabl agent want know featur sweep allow aggreg metric multipl seed config valu user awar solut involv loop train script want run execut parallel possibl differ machin",
        "Answer_original_content":"tomjur interest question possibl us seed paramet sweep config group base paramet care bunch wai group run aggreg metric click group button run choos metric care group plot edit plot click group tab choos paramet want group doc group run group train evalu run larger experi scott thank replai sorri dont understand solut mayb try explicit descript problem follow assum bay random search budget grid search let defin distribut paramet specif defin uniform valu seed agent sampl configur seed forc hyper paramet optim process control select configur seed optim random possibl like especi continu random variabl optim bay unlik bad effect prefer easi seed think common pain point possibl gan sweep thank tom thank share question understand essenc want grid search random seed bay random search config sure config sweep isnt current possibl definit featur try support futur common workflow peopl like try determinist run sweep thank ad featur request unfortun suggest grid search configur want test includ random seed search thank ad featur request btw new user track progress featur cli open sourc track releas note releas client github here code sweep github sweep hyperparamet sweep engin file sweep relat issu client http github com client btw new user welcom hope your enjoi help anymor question issu ye great thank help far topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"interest question possibl us seed paramet sweep config group base paramet care bunch wai group run aggreg metric click group button run choos metric care group plot edit plot click group tab choos paramet want group group run group train evalu run larger experi thank replai sorri dont understand solut mayb try explicit descript problem let defin distribut paramet specif defin uniform valu seed agent sampl configur seed forc optim process select configur seed optim random possibl like optim bay unlik bad prefer easi seed think common pain point sweep thank tom thank share question understand essenc want grid search random seed bay random search config sure config sweep isnt current possibl definit featur try support futur common workflow peopl like try determinist run sweep thank ad featur request unfortun suggest grid search configur want test includ random seed search thank ad featur request btw new user track progress featur cli open sourc track releas note releas github here code sweep github hyperparamet sweep engin file sweep relat issu client btw new user welcom hope your enjoi help anymor question issu ye great thank help far topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user is looking for a feature in sweeps that allows the aggregation of a metric over multiple seeds with the same config values, while also allowing for parallel execution of the runs on different machines. the answer suggests using the seed as a parameter in the sweep config and then grouping based on the parameters. however, the user has a specific problem with the optimization process and the solution is not currently possible. the answer suggests doing a grid search for each of the configurations the user wants to test, including the random seed in that search. the feature is a common workflow for people like the user trying to deterministically run sweeps, and it is a feature that will be supported in the future. the user can track the progress on this feature by checking the release notes on github.",
        "Answer_gpt_summary":"user look featur sweep allow aggreg metric multipl seed config valu allow parallel execut run differ machin answer suggest seed paramet sweep config group base paramet user specif problem optim process solut current possibl answer suggest grid search configur user want test includ random seed search featur common workflow peopl like user try determinist run sweep featur support futur user track progress featur check releas note github"
    },
    {
        "Question_id":null,
        "Question_title":"Adding tfRecords files to artifacts doesn't work?",
        "Question_body":"<p>Hello,<\/p>\n<p>I am trying logging my tfRecords files to artefact, but it seems to not be working (I get an error: \u201cwandb: Network error (TransientError), entering retry loop.\u201d).<\/p>\n<p>I am providing the code I use below. I am pretty sure it is something regarding the tfRecords file since I tried changing the contents of my folders to contain only .csv and .paqruet and it worked nicely. Do you have any ideas what could be happening here?<\/p>\n<pre><code class=\"lang-auto\">with wandb.init(project=\"----\", entity='----', job_type='saving_processed_files') as run:\n    train_data_art = wandb.Artifact(\n        name='train_data',\n        type='train_data'  \n    )\n\n    files_train = os.listdir(final_path_train)\n    files_train=[x  for x in files_train if x[0]!='.']\n\n    for file in files_train:\n        file_path = os.path.join(final_path_train, file)\n        train_data_art.add_file(file_path, name=file)\n\n    run.log_artifact(train_data_art)\n<\/code><\/pre>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_creation_time":1645200359797,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":201.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/adding-tfrecords-files-to-artifacts-doesnt-work\/1948",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-18T23:21:12.490Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drlje\">@drlje<\/a>,<\/p>\n<p>Is this error still popping up with TF Record files? Usually TransientErrors are minor network issues that automatically get resolved after a while.<\/p>\n<p>Please let me know if this error still persists and I will dig in further into what might be happening here in that case.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-22T18:02:59.790Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drlje\">@drlje<\/a>,<\/p>\n<p>We wanted to follow up with you regarding this issue as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-25T20:23:30.807Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drlje\">@drlje<\/a>,<\/p>\n<p>Since we have not heard back from you, I am closing out this request. If you would like to re-open this conversation, please let us know!<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-26T14:09:58.492Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a> ,<\/p>\n<p>Sorry for not being prompt. I tried again and I got the same error. However, I also tries doing this on a small fraction of data (also saved as a TFRecords) and it went through. So I am guessing this has something to do with the size - the total size of my files is around 10gb. Do you think that could be the issue?<\/p>\n<p>Thanks a lot!<\/p>\n<p>Marin<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-22T09:00:11.736Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a>,<\/p>\n<p>Do you have a feedback regarding the limit size of the files being uploaded? We are thinking to upgrade our account and this issue is really important for us.<\/p>\n<p>Thanks!<\/p>\n<p>Marin<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-13T14:29:02.000Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>Could you please re-look at this issue - I have left additional comment a while ago, and in a couple of days we will face this issue again, so I would love to get to the bottom of it.<\/p>\n<p><a href=\"https:\/\/community.wandb.ai\/t\/adding-tfrecords-files-to-artifacts-doesnt-work\/1948\/6\">https:\/\/community.wandb.ai\/t\/adding-tfrecords-files-to-artifacts-doesnt-work\/1948\/6<\/a><\/p>\n<p>Thanks!<\/p>\n<p>Marin<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-14T17:33:53.146Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drlje\">@drlje<\/a>,<\/p>\n<p>I\u2019m sorry about not responding here sooner. I\u2019m sorry to hear you are still facing this issue, and I will definitely assist you here. You said you are seeing an error with a lot of data : Could you share a little bit more  information about the structure of this data and the behavior you see? More specifically:<\/p>\n<ul>\n<li>How many files do you have?<\/li>\n<li>Are you seeing a lot of time delay before these errors show up?<\/li>\n<li>Could you try uploading this same scale of information but using some other file format? (like a set of <code>.txt<\/code> files)<\/li>\n<\/ul>\n<p>Additionally, the <code>debug.log<\/code> and <code>debug-internal.log<\/code> files associated with the run where you are facing this issue would be highly appreciated since it would give us some more visibility into this issue.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-22T06:54:30.577Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a>,<\/p>\n<p>In reproducing the issue today, the artifact was saved without any problems; so I guess the issue can be closed. If we experience the same problematic again, I will follow the steps above and supply you with the log files.<\/p>\n<p>Many thanks!<\/p>\n<p>Best,<\/p>\n<p>Marin<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-21T06:55:17.151Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"ad tfrecord file artifact work hello try log tfrecord file artefact work error network error transienterror enter retri loop provid code us pretti sure tfrecord file tri chang content folder contain csv paqruet work nice idea happen init project entiti job type save process file run train data art artifact train data type train data file train listdir final path train file train file train file file train file path path join final path train file train data art add file file path file run log artifact train data art",
        "Question_preprocessed_content":"ad tfrecord file artifact work hello try log tfrecord file artefact work enter retri provid code us pretti sure tfrecord file tri chang content folder contain csv paqruet work nice idea happen",
        "Question_gpt_summary_original":"The user is encountering an error while trying to log their tfRecords files to an artifact using WandB. They suspect that the issue is related to the tfRecords file format since they were able to successfully log other file formats. The user has provided their code and is seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error try log tfrecord file artifact suspect issu relat tfrecord file format abl successfulli log file format user provid code seek assist resolv issu",
        "Answer_original_content":"drlje error pop record file usual transienterror minor network issu automat resolv let know error persist dig happen case thank ramit drlje want follow issu heard let know assist issu resolv thank ramit drlje heard close request like open convers let know thank ramit hello ramit goolri sorri prompt tri got error tri small fraction data save tfrecord went guess size total size file think issu thank lot marin ramit goolri feedback limit size file upload think upgrad account issu import thank marin look issu left addit comment ago coupl dai face issu love http commun ad tfrecord file artifact doesnt work thank marin drlje sorri respond sooner sorri hear face issu definit assist said see error lot data share littl bit inform structur data behavior specif file see lot time delai error try upload scale inform file format like set txt file addition debug log debug intern log file associ run face issu highli appreci visibl issu thank ramit ramit goolri reproduc issu todai artifact save problem guess issu close experi problemat follow step suppli log file thank best marin topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"error pop record file usual transienterror minor network issu automat resolv let know error persist dig happen case thank ramit want follow issu heard let know assist issu resolv thank ramit heard close request like convers let know thank ramit hello sorri prompt tri got error tri small fraction data went guess size total size file think issu thank lot marin feedback limit size file upload think upgrad account issu import thank marin issu left addit comment ago coupl dai face issu love thank marin sorri respond sooner sorri hear face issu definit assist said see error lot data share littl bit inform structur data behavior specif file see lot time delai error try upload scale inform file format addition file associ run face issu highli appreci visibl issu thank ramit reproduc issu todai artifact save problem guess issu close experi problemat follow step suppli log file thank best marin topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user was encountering an error when attempting to log their tfrecords files to an artifact, despite successfully logging other file types. the error was related to the size of the files, which were around 10gb. the possible solutions suggested were to try uploading the same scale of information but using some other file format, like a set of .txt files, and to provide the debug.log and debug-internal.log files associated with the run where the issue occurred. the issue was eventually resolved, and the user was advised to follow the same steps if they experience the same problem again.",
        "Answer_gpt_summary":"user encount error attempt log tfrecord file artifact despit successfulli log file type error relat size file possibl solut suggest try upload scale inform file format like set txt file provid debug log debug intern log file associ run issu occur issu eventu resolv user advis follow step experi problem"
    },
    {
        "Question_id":null,
        "Question_title":"How to use Azure files for model endpoints created in AML hosted on AKS?",
        "Question_body":"Hello,\nwe have a complex data processing pipeline, including multiple different Models.\nOne of those models is a GPU-Model, which will be hosted as an API via AML.\nTherefore, we are also using an AKS for the hosting\/compute. This is working fine.\nHowever, the model itself is supposed to use data generated by prior steps.\nDownloading the data from a storage account into the container, is not a good solution.\nWe would like to \"mount\" the storage account directly and use the data therein (similar to databricks).\n\nI also know, that for persistent volumes in AKS, you can use Azure Files.\nHowever, normally you set this is up in the AKS and in the deployment-yaml.\nWith AML I did not find a way, to modify the deployments to use persistent volumes.\nIs there really no way? Or did I just not find the documentation for that?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1656329447103,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/904580\/how-to-use-azure-files-for-model-endpoints-created.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-28T03:25:38.337Z",
                "Answer_score":1,
                "Answer_body":"Hello @83226169 ,\n\nThank you for posting query in Microsoft Q&A Platform. Is this for inference?\n\nBelow content might be helpful.\nhttps:\/\/www.youtube.com\/watch?v=SmVHNpKVs3Y\nAre you looking for something like below?\n\n\nhttps:\/\/www.youtube.com\/watch?v=D0qsjJYj5Ow\n\nAzure Machine Learning team has a dedicated ARM template within Azure QuickStart templates that deploys secure configurations.\nhttps:\/\/microsoft.github.io\/azureml-ops-accelerator\/3-Deploy\/ARMTemplates\/\n\n\n\n\n\n\n\nRegards,\nPritee",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-29T08:21:12.187Z",
                "Answer_score":0,
                "Answer_body":"Thanks for the replies.\nI was initially looking for a way to use azure files with AML. And actually I found something in the depth of the documentation: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-kubernetes (at the bottom \u201cAzureML jobs connect with custom data storage\u201d).\n\nHowever, we re-evaluated our design and concluded, that the connection to the blob storage is actually enough and preferred to Azure Files. Therefore, your hints and links are helpful to create the more secure setup with the Storage Account, than just using the account key.\n\n\n\n\nKind regards,\nTobias",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":23.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us azur file model endpoint creat aml host ak hello complex data process pipelin includ multipl differ model model gpu model host api aml ak host comput work fine model suppos us data gener prior step download data storag account contain good solut like mount storag account directli us data similar databrick know persist volum ak us azur file normal set ak deploy yaml aml wai modifi deploy us persist volum wai document",
        "Question_preprocessed_content":"us azur file model endpoint creat aml host ak hello complex data process pipelin includ multipl differ model model host api aml ak work fine model suppos us data gener prior step download data storag account contain good solut like mount storag account directli us data know persist volum ak us azur file normal set ak aml wai modifi deploy us persist volum wai document",
        "Question_gpt_summary_original":"The user is facing challenges in using Azure Files for model endpoints created in AML hosted on AKS. They are unable to modify the deployments to use persistent volumes and are looking for a way to \"mount\" the storage account directly to use the data therein.",
        "Question_gpt_summary":"user face challeng azur file model endpoint creat aml host ak unabl modifi deploy us persist volum look wai mount storag account directli us data",
        "Answer_original_content":"hello thank post queri microsoft platform infer content help http youtub com watch smvhnpkvsy look like http youtub com watch dqsjjyjow team dedic arm templat azur quickstart templat deploi secur configur http microsoft github op acceler deploi armtempl regard prite thank repli initi look wai us azur file aml actual depth document http doc microsoft com azur machin learn refer kubernet job connect custom data storag evalu design conclud connect blob storag actual prefer azur file hint link help creat secur setup storag account account kei kind regard tobia",
        "Answer_preprocessed_content":"hello thank post queri microsoft platform infer content help look like team dedic arm templat azur quickstart templat deploi secur configur regard prite thank repli initi look wai us azur file aml actual depth document design conclud connect blob storag actual prefer azur file hint link help creat secur setup storag account account kei kind regard tobia",
        "Answer_gpt_summary_original":"the answer suggests several possible solutions to the user's challenge of accessing data stored in a storage account from a container hosted on aks for a gpu model api created in aml. these include using an azure quickstart template to deploy secure configurations, connecting to custom data storage using kubernetes, and connecting to blob storage instead of azure files. the answer also mentions the importance of creating a more secure setup with the storage account, rather than just using the account key.",
        "Answer_gpt_summary":"answer suggest possibl solut user challeng access data store storag account contain host ak gpu model api creat aml includ azur quickstart templat deploi secur configur connect custom data storag kubernet connect blob storag instead azur file answer mention import creat secur setup storag account account kei"
    },
    {
        "Question_id":35411741.0,
        "Question_title":"Azure ML: Getting Error 503: NoMoreResources to any web service API even when I only make 1 request",
        "Question_body":"<p>Getting the following response even when I make one request (concurrency set to 200) to a web service. <\/p>\n\n<p>{ status: 503, headers: '{\"content-length\":\"174\",\"content-type\":\"application\/json; charset=utf-8\",\"etag\":\"\\\"8ce068bf420a485c8096065ea3e4f436\\\"\",\"server\":\"Microsoft-HTTPAPI\/2.0\",\"x-ms-request-id\":\"d5c56cdd-644f-48ba-ba2b-6eb444975e4c\",\"date\":\"Mon, 15 Feb 2016 04:54:01 GMT\",\"connection\":\"close\"}',  body: '{\"error\":{\"code\":\"ServiceUnavailable\",\"message\":\"Service is temporarily unavailable.\",\"details\":[{\"code\":\"NoMoreResources\",\"message\":\"No resources available for request.\"}]}}' }<\/p>\n\n<p>The request-response web service is a recommender retraining web service with the training set containing close to 200k records. The training set is already present in my ML studio dataset, only 10-15 extra records are passed in the request. The same experiment was working flawlessly till 13th Feb 2016. I have already tried increasing the concurrency but still the same issue. I even reduced the size of the training set to 20 records, still didn't work.<\/p>\n\n<p>I have two web service both doing something similar and both aren't working since 13th Feb 2016. <\/p>\n\n<p>Finally, I created a really small experiment ( skill.csv --> split row ---> web output )   which doesn't take any input. It just has to return some part of the dataset. Did not work, response code 503.<\/p>\n\n<p>The logs I got are as follows<\/p>\n\n<p>{\n  \"version\": \"2014-10-01\",\n  \"diagnostics\": [{\n    .....\n    {\n      \"type\": \"GetResourceEndEvent\",\n      \"timestamp\": 13.1362,\n      \"resourceId\": \"5e2d653c2b214e4dad2927210af4a436.865467b9e7c5410e9ebe829abd0050cd.v1-default-111\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    },\n    {\n      \"type\": \"InitializationSummary\",\n      \"time\": \"2016-02-15T04:46:18.3651714Z\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    }\n  ]\n}<\/p>\n\n<p>What am I missing? Or am I doing it completely wrong?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>PS: Data is stored in mongoDB and then imported as CSV<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1455545889500,
        "Question_favorite_count":null,
        "Question_last_edit_time":1456850010663,
        "Question_score":1.0,
        "Question_view_count":283.0,
        "Answer_body":"<p>This was an Azure problem. I quote the Microsoft guy, <\/p>\n\n<blockquote>\n  <p>We believe we have isolated the issue impacting tour service and we are currently working on a fix. We will be able to deploy this in the next couple of days. The problem is impacting only the ASIA AzureML region at this time, so if this is an option for you, might I suggest using a workspace in either the US or EU region until the fix gets rolled out here.<\/p>\n<\/blockquote>\n\n<p>To view the complete discussion, click <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/985e253e-5e54-45a5-a359-5c501152c445\/getting-error-503-nomoreresources-to-any-web-service-api-even-when-i-only-make-1-request?forum=MachineLearning&amp;prof=required\" rel=\"nofollow\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35411741",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1455597422632,
        "Question_original_content":"get error nomoreresourc web servic api request get follow respons request concurr set web servic statu header content length content type applic json charset utf etag cebfaceaef server microsoft httpapi request dccdd bab ebec date mon feb gmt connect close bodi error code serviceunavail messag servic temporarili unavail detail code nomoreresourc messag resourc avail request request respons web servic recommend retrain web servic train set contain close record train set present studio dataset extra record pass request experi work flawlessli till feb tri increas concurr issu reduc size train set record work web servic similar aren work feb final creat small experi skill csv split row web output input return dataset work respons code log got follow version diagnost type getresourceendev timestamp resourceid edcbedadafa beceebeabdcd default statu failur error uri target storag locat specifi consid chang request locat mode type initializationsummari time statu failur error uri target storag locat specifi consid chang request locat mode miss complet wrong thank advanc data store mongodb import csv",
        "Question_preprocessed_content":"get error nomoreresourc web servic api request get follow respons request web servic statu header feb gmt connect close bodi web servic recommend retrain web servic train set contain close record train set present studio dataset extra record pass request experi work flawlessli till feb tri increas concurr issu reduc size train set record work web servic similar aren work feb final creat small experi input return dataset work respons code log got follow version diagnost miss complet wrong thank advanc data store mongodb import csv",
        "Question_gpt_summary_original":"The user is encountering an error 503 when making requests to a web service API, even when only making one request with a concurrency set to 200. The web service is a recommender retraining web service with a training set containing close to 200k records. The training set is already present in the user's ML studio dataset, and only 10-15 extra records are passed in the request. The same experiment was working flawlessly until February 13th, 2016. The user has tried increasing the concurrency and reducing the size of the training set, but the issue persists. The user has also created a small experiment that doesn't take any input, but it still didn't work. The logs show an error related to the target storage location.",
        "Question_gpt_summary":"user encount error make request web servic api make request concurr set web servic recommend retrain web servic train set contain close record train set present user studio dataset extra record pass request experi work flawlessli februari user tri increas concurr reduc size train set issu persist user creat small experi input work log error relat target storag locat",
        "Answer_original_content":"azur problem quot microsoft gui believ isol issu impact tour servic current work fix abl deploi coupl dai problem impact asia region time option suggest workspac region fix get roll view complet discuss click",
        "Answer_preprocessed_content":"azur problem quot microsoft gui believ isol issu impact tour servic current work fix abl deploi coupl dai problem impact asia region time option suggest workspac region fix get roll view complet discuss click",
        "Answer_gpt_summary_original":"possible solutions: \n- wait for the fix to be deployed in the next couple of days.\n- use a workspace in either the us or eu region until the fix gets rolled out in the asia region. \n\nsummary: the user's error 503: nomoreresources issue was caused by an azure problem that is currently being worked on by microsoft. the fix will be deployed in the next couple of days. in the meantime, the user can use a workspace in either the us or eu region as an alternative solution.",
        "Answer_gpt_summary":"possibl solut wait fix deploi coupl dai us workspac region fix get roll asia region summari user error nomoreresourc issu caus azur problem current work microsoft fix deploi coupl dai meantim user us workspac region altern solut"
    },
    {
        "Question_id":50035628.0,
        "Question_title":"Include additional scripts when deploying a Azure ML experimentation service",
        "Question_body":"<p>When training my model the data I start with consist of rows of json data and the expected values I would like to predict from that json data. The json data follows the schema I my deployed service will receive the input as. Before training I run a number of python functions to transform the data and extract features calculated from the raw json data. It is that transformed data which my model is trained on.<\/p>\n\n<p>I have extracted the code to transform the json data into the input my model expects into a separate python file. Now I would like to have my scoring script use that python script to prepare the input sent to the service before feeding it into my trained model.<\/p>\n\n<p>Is there a way to include the data transformation script with the scoring script when deploying my service using the cli command:<\/p>\n\n<pre><code>az ml service create realtime \n    -f &lt;scoring-script&gt;.py \n    --model-file model.pkl \n    -s service_schema.json \n    -n &lt;some-name&gt; \n    -r python \n    --collect-model-data true \n    -c aml_config\\conda_dependencies.yml\n<\/code><\/pre>\n\n<p><em>(the new lines in the above command added for clarity)<\/em><\/p>\n\n<p>The two ways I've come up with is to either:<\/p>\n\n<ul>\n<li>Create my own base docker image that contains the transformation script and use that image as the base for my service. Seems a bit cumbersome to do if I need similar (but different) data transformations for later models.<\/li>\n<li>Concatenate the transformation script with my scoring script into a single file. Seems a bit hacky.<\/li>\n<\/ul>\n\n<p><strong>Is there another way to achive my goal of having a separate data transformation script used both in training and in scoring?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1524721574190,
        "Question_favorite_count":null,
        "Question_last_edit_time":1524806425600,
        "Question_score":1.0,
        "Question_view_count":252.0,
        "Answer_body":"<p>So running <code>az ml service create realtime -h<\/code> provides information about the <code>-d<\/code> flag.<\/p>\n\n<p><code>-d : Files and directories required by the service. Multiple dependencies can be specified with additional -d arguments.<\/code><\/p>\n\n<p>Please try using this flag and provide the additional python file that you would like to call too from your <code>score.py<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50035628",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1524806224528,
        "Question_original_content":"includ addit script deploi experiment servic train model data start consist row json data expect valu like predict json data json data follow schema deploi servic receiv input train run number python function transform data extract featur calcul raw json data transform data model train extract code transform json data input model expect separ python file like score script us python script prepar input sent servic feed train model wai includ data transform script score script deploi servic cli command servic creat realtim model file model pkl servic schema json python collect model data true aml config conda depend yml new line command ad clariti wai come creat base docker imag contain transform script us imag base servic bit cumbersom need similar differ data transform later model concaten transform script score script singl file bit hacki wai achiv goal have separ data transform script train score",
        "Question_preprocessed_content":"includ addit script deploi experiment servic train model data start consist row json data expect valu like predict json data json data follow schema deploi servic receiv input train run number python function transform data extract featur calcul raw json data transform data model train extract code transform json data input model expect separ python file like score script us python script prepar input sent servic feed train model wai includ data transform script score script deploi servic cli command new line command ad clariti wai come creat base docker imag contain transform script us imag base servic bit cumbersom need similar data transform later model concaten transform script score script singl file bit hacki wai achiv goal have separ data transform script train score",
        "Question_gpt_summary_original":"The user is facing a challenge of including a data transformation script with the scoring script when deploying a service using Azure ML experimentation service. The user has considered creating a base docker image that contains the transformation script or concatenating the transformation script with the scoring script into a single file, but is looking for another way to achieve the goal of having a separate data transformation script used both in training and in scoring.",
        "Question_gpt_summary":"user face challeng includ data transform script score script deploi servic experiment servic user consid creat base docker imag contain transform script concaten transform script score script singl file look wai achiev goal have separ data transform script train score",
        "Answer_original_content":"run servic creat realtim provid inform flag file directori requir servic multipl depend specifi addit argument try flag provid addit python file like score",
        "Answer_preprocessed_content":"run provid inform flag try flag provid addit python file like",
        "Answer_gpt_summary_original":"the solution to including additional scripts when deploying an experimentation service is to use the -d flag provided by the command \"az ml service create realtime -h\". this flag allows for multiple dependencies to be specified with additional -d arguments. the user can provide the additional python file that they would like to call from their score.py file.",
        "Answer_gpt_summary":"solut includ addit script deploi experiment servic us flag provid command servic creat realtim flag allow multipl depend specifi addit argument user provid addit python file like score file"
    },
    {
        "Question_id":null,
        "Question_title":"How does one save a plot in wandb with wandb.log?",
        "Question_body":"<p>I\u2019m trying to save a plot with wandb.log. Their <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots\">docs<\/a> say to do:<\/p>\n<pre><code class=\"lang-auto\">    wandb.log({\"chart\": plt})\n<\/code><\/pre>\n<p>but this fails for me.<\/p>\n<p>I get two errors, 1st error (when I do NOT do <code>plt.show()<\/code> before trying to do wand.log):<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/Applications\/PyCharm.app\/Contents\/plugins\/python\/helpers\/pydev\/_pydevd_bundle\/pydevd_exec2.py\", line 3, in Exec\n    exec(exp, global_vars, local_vars)\n  File \"&lt;input&gt;\", line 1, in &lt;module&gt;\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 256, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 222, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1548, in log\n    self._log(data=data, step=step, commit=commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1339, in _log\n    self._partial_history_callback(data, step, commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1228, in _partial_history_callback\n    self._backend.interface.publish_partial_history(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 541, in publish_partial_history\n    data = history_dict_to_json(run, data, step=user_step, ignore_copy_err=True)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 54, in history_dict_to_json\n    payload[key] = val_to_json(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 82, in val_to_json\n    val = Plotly.make_plot_media(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/plotly.py\", line 48, in make_plot_media\n    val = util.matplotlib_to_plotly(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/util.py\", line 560, in matplotlib_to_plotly\n    return tools.mpl_to_plotly(obj)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/tools.py\", line 112, in mpl_to_plotly\n    matplotlylib.Exporter(renderer).run(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 53, in run\n    self.crawl_fig(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 124, in crawl_fig\n    self.crawl_ax(ax)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 146, in crawl_ax\n    self.draw_collection(ax, collection)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 289, in draw_collection\n    offset_order = offset_dict[collection.get_offset_position()]\nAttributeError: 'LineCollection' object has no attribute 'get_offset_position'\n<\/code><\/pre>\n<p>I get two errors, 2nd error (when I DO <code>plt.show()<\/code> before trying to do wand.log):<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/Applications\/PyCharm.app\/Contents\/plugins\/python\/helpers\/pydev\/_pydevd_bundle\/pydevd_exec2.py\", line 3, in Exec\n    exec(exp, global_vars, local_vars)\n  File \"&lt;input&gt;\", line 1, in &lt;module&gt;\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 256, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 222, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1548, in log\n    self._log(data=data, step=step, commit=commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1339, in _log\n    self._partial_history_callback(data, step, commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1228, in _partial_history_callback\n    self._backend.interface.publish_partial_history(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 541, in publish_partial_history\n    data = history_dict_to_json(run, data, step=user_step, ignore_copy_err=True)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 54, in history_dict_to_json\n    payload[key] = val_to_json(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 82, in val_to_json\n    val = Plotly.make_plot_media(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/plotly.py\", line 48, in make_plot_media\n    val = util.matplotlib_to_plotly(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/util.py\", line 560, in matplotlib_to_plotly\n    return tools.mpl_to_plotly(obj)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/tools.py\", line 112, in mpl_to_plotly\n    matplotlylib.Exporter(renderer).run(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 53, in run\n    self.crawl_fig(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 122, in crawl_fig\n    with self.renderer.draw_figure(fig=fig, props=utils.get_figure_properties(fig)):\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/contextlib.py\", line 119, in __enter__\n    return next(self.gen)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/renderers\/base.py\", line 45, in draw_figure\n    self.open_figure(fig=fig, props=props)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/renderer.py\", line 90, in open_figure\n    self.mpl_x_bounds, self.mpl_y_bounds = mpltools.get_axes_bounds(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mpltools.py\", line 265, in get_axes_bounds\n    x_min, y_min, x_max, y_max = min(x_min), min(y_min), max(x_max), max(y_max)\nValueError: min() arg is an empty sequence\n<\/code><\/pre>\n<p>Note that their trivial example DOES work:<\/p>\n<pre><code class=\"lang-auto\">import matplotlib.pyplot as plt\n\nplt.plot([1, 2, 3, 4])\nplt.ylabel(\"some interesting numbers\")\nwandb.log({\"chart\": plt})\n<\/code><\/pre>\n<p>for me.<\/p>\n<hr>\n<p>cross posted: <a href=\"https:\/\/stackoverflow.com\/questions\/72134168\/how-does-one-save-a-plot-in-wandb-with-wandb-log\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">python - How does one save a plot in wandb with wandb.log? - Stack Overflow<\/a><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1651788927254,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":188.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-does-one-save-a-plot-in-wandb-with-wandb-log\/2373",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-09T21:30:52.813Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/brando\">@brando<\/a>,<\/p>\n<p>I\u2019m sorry you are facing this. Could you share the code snippet you were using to generate this chart? I\u2019ll test this on my end and see how this can be resolved.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-12T22:18:46.528Z",
                "Answer_body":"<p>Hi Brando,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-08T21:31:40.625Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save plot log try save plot log doc log chart plt fail error error plt try wand log traceback recent file applic pycharm app content plugin python helper pydev pydevd bundl pydevd exec line exec exec exp global var local var file line file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self log data data step step commit commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self partial histori callback data step commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line partial histori callback self backend interfac publish partial histori file user brandomiranda opt anaconda env meta learn lib python site packag sdk interfac interfac line publish partial histori data histori dict json run data step user step ignor copi err true file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line histori dict json payload kei val json file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line val json val plotli plot media val file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type plotli line plot media val util matplotlib plotli val file user brandomiranda opt anaconda env meta learn lib python site packag util line matplotlib plotli return tool mpl plotli obj file user brandomiranda opt anaconda env meta learn lib python site packag plotli tool line mpl plotli matplotlylib export render run fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line run self crawl fig fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line crawl fig self crawl file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line crawl self draw collect collect file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line draw collect offset order offset dict collect offset posit attributeerror linecollect object attribut offset posit error error plt try wand log traceback recent file applic pycharm app content plugin python helper pydev pydevd bundl pydevd exec line exec exec exp global var local var file line file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self log data data step step commit commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self partial histori callback data step commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line partial histori callback self backend interfac publish partial histori file user brandomiranda opt anaconda env meta learn lib python site packag sdk interfac interfac line publish partial histori data histori dict json run data step user step ignor copi err true file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line histori dict json payload kei val json file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line val json val plotli plot media val file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type plotli line plot media val util matplotlib plotli val file user brandomiranda opt anaconda env meta learn lib python site packag util line matplotlib plotli return tool mpl plotli obj file user brandomiranda opt anaconda env meta learn lib python site packag plotli tool line mpl plotli matplotlylib export render run fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line run self crawl fig fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line crawl fig self render draw figur fig fig prop util figur properti fig file user brandomiranda opt anaconda env meta learn lib python contextlib line enter return self gen file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport render base line draw figur self open figur fig fig prop prop file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib render line open figur self mpl bound self mpl bound mpltool ax bound fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mpltool line ax bound min min max max min min min min max max max max valueerror min arg sequenc note trivial exampl work import matplotlib pyplot plt plt plot plt ylabel interest number log chart plt cross post python save plot log stack overflow",
        "Question_preprocessed_content":"save plot log try save plot log doc fail error error error error note trivial exampl work cross post python save plot log stack overflow",
        "Question_gpt_summary_original":"The user is trying to save a plot with wandb.log but is encountering errors. The first error occurs when plt.show() is not done before trying to do wand.log, and the second error occurs when plt.show() is done before trying to do wand.log. The trivial example provided by wandb works for the user.",
        "Question_gpt_summary":"user try save plot log encount error error occur plt try wand log second error occur plt try wand log trivial exampl provid work user",
        "Answer_original_content":"brando sorri face share code snippet gener chart ill test end resolv thank ramit brando want follow support request heard let know assist issu resolv best topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"sorri face share code snippet gener chart ill test end resolv thank ramit brando want follow support request heard let know assist issu resolv best topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the responder is asking for more information to help resolve the issue and is following up with the user to see if they still need assistance.",
        "Answer_gpt_summary":"solut provid answer respond ask inform help resolv issu follow user need assist"
    },
    {
        "Question_id":null,
        "Question_title":"Greengrass for data processing and ML model training",
        "Question_body":"Is it possible to train and deploy ML models in Greengrass? Or is Greengrass limited to inference while training is done using SageMaker in cloud?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1556295399000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":"As a native service offering, Greengrass has support for deploying models to the edge and running inference code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2FEvRboNRL2Ipn1yE7IBvg\/greengrass-for-data-processing-and-ml-model-training",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-04-26T16:21:21.000Z",
                "Answer_score":0,
                "Answer_body":"As a native service offering, Greengrass has support for deploying models to the edge and running inference code against those models. Nothing prevents you from deploying your own code to the edge that would train a model, but I suspect you wouldn't be able to store it as a Greengrass local resource for later inferences without doing a round trip to the cloud and redeploying to GG.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1556295681000,
        "Question_original_content":"greengrass data process model train possibl train deploi model greengrass greengrass limit infer train cloud",
        "Question_preprocessed_content":"greengrass data process model train possibl train deploi model greengrass greengrass limit infer train cloud",
        "Question_gpt_summary_original":"The user is questioning whether it is possible to train and deploy ML models in Greengrass or if it is limited to inference, with training being done using SageMaker in the cloud.",
        "Question_gpt_summary":"user question possibl train deploi model greengrass limit infer train cloud",
        "Answer_original_content":"nativ servic offer greengrass support deploi model edg run infer code model prevent deploi code edg train model suspect wouldn abl store greengrass local resourc later infer round trip cloud redeploi",
        "Answer_preprocessed_content":"nativ servic offer greengrass support deploi model edg run infer code model prevent deploi code edg train model suspect wouldn abl store greengrass local resourc later infer round trip cloud redeploi",
        "Answer_gpt_summary_original":"the answer suggests that aws greengrass has native support for deploying ml models to the edge and running inference code against those models. however, it is possible to deploy custom code to the edge for training models, but it may not be possible to store them as a local resource for later inferences without a round trip to the cloud and redeploying to greengrass.",
        "Answer_gpt_summary":"answer suggest aw greengrass nativ support deploi model edg run infer code model possibl deploi custom code edg train model possibl store local resourc later infer round trip cloud redeploi greengrass"
    },
    {
        "Question_id":72633246.0,
        "Question_title":"Error in pytorch data loader batch cycles",
        "Question_body":"<p>I keep getting this error in sagemaker when iterating through pytorch dataloader batch cycles:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self._process_data(data), w_id)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1299, in _process_data\n    data.reraise()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/_utils.py&quot;, line 429, in reraise\n    raise self.exc_type(msg)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 84, in __init__\n    super(HTTPClientError, self).__init__(**kwargs)\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/botocore\/exceptions.py&quot;, line 40, in __init__\n    msg = self.fmt.format(**kwargs)\nKeyError: 'error'\n\n---------------------------------------------------------------------------\nUnexpectedStatusException                 Traceback (most recent call last)\n&lt;ipython-input-1-81655136a841&gt; in &lt;module&gt;\n     58                             py_version='py3')\n     59 \n---&gt; 60 pytorch_estimator.fit({'train': Runtime.dataset_path}, job_name=Runtime.job_name)\n     61 \n     62 #print(pytorch_estimator.latest_job_tensorboard_artifacts_path())\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n    955         self.jobs.append(self.latest_training_job)\n    956         if wait:\n--&gt; 957             self.latest_training_job.wait(logs=logs)\n    958 \n    959     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n   1954         # If logs are requested, call logs_for_jobs.\n   1955         if logs != &quot;None&quot;:\n-&gt; 1956             self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n   1957         else:\n   1958             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll, log_type)\n   3751 \n   3752         if wait:\n-&gt; 3753             self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n   3754             if dot:\n   3755                 print()\n\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n   3304                 ),\n   3305                 allowed_statuses=[&quot;Completed&quot;, &quot;Stopped&quot;],\n-&gt; 3306                 actual_status=status,\n   3307             )\n   3308 \n\nUnexpectedStatusException: Error for Training job 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/opt\/conda\/bin\/python3.6 main.py --runtime_var dataset_name=U12239-2022-05-09-14-39-18,job_name=2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training,model_name=pix2pix&quot;\n\n  0%|          | 0\/248 [00:00&lt;?, ?it\/s]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\n  0%|          | 1\/248 [00:30&lt;2:07:28, 30.97s\/it]\nTraceback (most recent call last):\n  File &quot;main.py&quot;, line 371, in &lt;module&gt;\n    g_scaler=g_scaler, d_scaler=d_scaler, runtime_log_folder=runtime_log_folder, runtime_log_file_name=runtime_log_file_name)\n  File &quot;main.py&quot;, line 78, in train_fn\n    for idx, (x, y) in enumerate(loop):\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/tqdm\/std.py&quot;, line 1171, in __iter__\n    for obj in iterable:\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 525, in __next__\n    (data, worker_id) = self._next_data()\n  File &quot;\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py&quot;, line 1252, in _next_data\n    return (self\n<\/code><\/pre>\n<p>Here is the code which results in the error:<\/p>\n<pre><code>def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce, g_scaler, d_scaler,runtime_log_folder,runtime_log_file_name):\n\n    total_output=''\n    \n    loop = tqdm(loader, leave=True)\n    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;\n\n    print(&quot;Loop&quot;)\n    print(loop)\n    print(&quot;Length loop&quot;)\n    print(len(loop))\n    for idx, (x, y) in enumerate(loop): #&lt;--error happens here\n        print(&quot;Loop index&quot;)\n        print(idx)\n        print(&quot;Loop item&quot;)\n        print(x,y)\n        x = x.to(device)\n        y = y.to(device)\n        \n        # train discriminator\n        with torch.cuda.amp.autocast():\n            y_fake = gen(x)\n\n            D_real = disc(x, y)\n            D_fake = disc(x, y_fake.detach())\n            # use detach so as to avoid breaking computational graph when do optimizer.step on discriminator\n            # can use detach, or when do loss.backward put loss.backward(retain_graph = True)\n\n            D_real_loss = bce(D_real, torch.ones_like(D_real))\n            D_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            D_loss = (D_real_loss + D_fake_loss) \/ 2\n            \n            # log tensorboard\n            \n        disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n        \n        # train generator\n        with torch.cuda.amp.autocast():\n            \n            D_fake = disc(x, y_fake)\n\n            # compute fake loss\n            # trick discriminator to believe these are real, hence send in torch.oneslikedfake\n            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n\n            # compute L1 loss\n            L1 = l1(y_fake, y) * args.l1_lambda\n\n            G_loss = G_fake_loss + L1\n            \n            # log tensorboard\n           \n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n        \n        # print epoch, generator loss, discriminator loss\n        print(f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]')\n        output = f'[Epoch {epoch}\/{args.num_epochs} (b: {idx})] [D loss: {D_loss}, D real loss: {D_real_loss}, D fake loss: {D_fake_loss}] [G loss: ##{G_loss}, G fake loss: {G_fake_loss}, L1 loss: {L1}]\\n'\n        total_output+=output\n\n\n\n    runtime_log = get_json_file_from_s3(runtime_log_folder, runtime_log_file_name)\n    runtime_log += total_output\n    upload_json_file_to_s3(runtime_log_folder,runtime_log_file_name,json.dumps(runtime_log))\n\n\n\ndef __getitem__(self, index):\n    print(&quot;Index &quot;,index)\n    pair_key = self.list_files[index]\n    print(&quot;Pair key &quot;,pair_key)\n    pair = Boto.s3_client.list_objects(Bucket=Boto.bucket_name, Prefix=pair_key, Delimiter='\/')\n\n    input_image_key = pair.get('Contents')[1].get('Key')\n    input_image_path = f's3:\/\/{Boto.bucket_name}\/{input_image_key}'\n    print(&quot;Input image path &quot;,input_image_path)\n    input_image_s3_source = get_file_from_filepath(input_image_path)\n    input_image = np.array(Image.open(input_image_s3_source))\n\n    target_image_key = pair.get('Contents')[0].get('Key')\n    target_image_path = f's3:\/\/{Boto.bucket_name}\/{target_image_key}'\n    print(&quot;Target image path &quot;,target_image_path)\n    target_image_s3_source = get_file_from_filepath(target_image_path)\n    target_image = np.array(Image.open(target_image_s3_source))\n\n    augmentations = config.both_transform(image=input_image, image0=target_image)\n\n    # get input image and target image by doing augmentations of images\n    input_image, target_image = augmentations['image'], augmentations['image0']\n\n    input_image = config.transform_only_input(image=input_image)['image']\n    target_image = config.transform_only_mask(image=target_image)['image']\n    \n    print(&quot;Input image size &quot;,input_image.size())\n    print(&quot;Target image size &quot;,target_image.size())\n    \n    return input_image, target_image\n\n<\/code><\/pre>\n<p>I did multiple runs and here are the traces of the failure points<\/p>\n<pre><code>i) 2022-06-03-05-00-04-pix2pix-U12239-2022-05-09-14-39-18-training\nNo index shown\n[Epoch 0\/100 (b: 0)]\n\nii) 2022-06-03-05-16-49-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niii) 2022-06-03-05-44-46-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 0\/100 (b: 0)]\n\niv) 2022-06-03-06-08-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\n[Epoch 1\/100 (b: 0)]\n\nv) 2022-06-15-02-49-20-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  160\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P423712\/Pair_71\/\n[Epoch 0\/100 (b: 0)\n\nvi) 2022-06-15-02-59-43-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P425642\/Pair_27\/\n[Epoch 0\/100 (b: 247)]\n\nvii) 2022-06-15-04-49-33-pix2pix-U12239-2022-05-09-14-39-18-training\nIndex  64\nPair key  datasets\/training-data\/testing\/2022-05-09-14-39-18\/match-raws-finals\/U12239\/P415414\/Pair_124\/\nNo specific epoch \n<\/code><\/pre>\n<p>My batch size is 248, so as you can see it seems to fail either at the start of the batch (0) or at the end (247). Also there are some common Indexes in the get item which seems to cause it to fail, namely Index 64 and Index 160. However there doesn't seem to be a common data point in the dataset that causes it to fail, as can be seen from the pair key all 3 data points in the datasets are different.<\/p>\n<p>Does anyone have any idea why this error happens please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655303532370,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":55.0,
        "Answer_body":"<p>Try to run the same training script outside of a SageMaker training job and see what happens.<br \/>\nIf the error doesn't happen on a standalone script, try to run it as a <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"nofollow noreferrer\">Local SageMaker training job<\/a>, so you can reproduce it in seconds instead of minutes, and potentially use a debugger to figure out what is the problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72633246",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1655379897027,
        "Question_original_content":"error pytorch data loader batch cycl get error iter pytorch dataload batch cycl traceback recent file main line scaler scaler scaler scaler runtim log folder runtim log folder runtim log file runtim log file file main line train idx enumer loop file opt conda lib python site packag tqdm std line iter obj iter file opt conda lib python site packag torch util data dataload line data worker self data file opt conda lib python site packag torch util data dataload line data return self process data data file opt conda lib python site packag torch util data dataload line process data data rerais file opt conda lib python site packag torch util line rerais rais self exc type msg file opt conda lib python site packag botocor except line init super httpclienterror self init kwarg file opt conda lib python site packag botocor except line init msg self fmt format kwarg keyerror error unexpectedstatusexcept traceback recent version pytorch estim fit train runtim dataset path job runtim job print pytorch estim latest job tensorboard artifact path anaconda env pytorch latest lib python site packag estim fit self input wait log job experi config self job append self latest train job wait self latest train job wait log log def compil job self anaconda env pytorch latest lib python site packag estim wait self log log request log job log self session log job self job wait true log type log self session wait job self job anaconda env pytorch latest lib python site packag session log job self job wait poll log type wait self check job statu job descript trainingjobstatu dot print anaconda env pytorch latest lib python site packag session check job statu self job desc statu kei allow status complet stop actual statu statu unexpectedstatusexcept error train job pixpix train fail reason algorithmerror executeuserscripterror command opt conda bin python main runtim var dataset job pixpix train model pixpix scaler scaler scaler scaler runtim log folder runtim log folder runtim log file runtim log file file main line train idx enumer loop file opt conda lib python site packag tqdm std line iter obj iter file opt conda lib python site packag torch util data dataload line data worker self data file opt conda lib python site packag torch util data dataload line data return self code result error def train disc gen loader opt disc opt gen bce scaler scaler runtim log folder runtim log file total output loop tqdm loader leav true devic cuda torch cuda avail cpu print loop print loop print length loop print len loop idx enumer loop error happen print loop index print idx print loop item print devic devic train discrimin torch cuda amp autocast fake gen real disc fake disc fake detach us detach avoid break comput graph optim step discrimin us detach loss backward loss backward retain graph true real loss bce real torch on like real fake loss bce fake torch on like fake loss real loss fake loss log tensorboard disc zero grad scaler scale loss backward scaler step opt disc scaler updat train gener torch cuda amp autocast fake disc fake comput fake loss trick discrimin believ real send torch oneslikedfak fake loss bce fake torch on like fake comput loss fake arg lambda loss fake loss log tensorboard opt gen zero grad scaler scale loss backward scaler step opt gen scaler updat print epoch gener loss discrimin loss print epoch epoch arg num epoch idx loss loss real loss real loss fake loss fake loss loss loss fake loss fake loss loss output epoch epoch arg num epoch idx loss loss real loss real loss fake loss fake loss loss loss fake loss fake loss loss total output output runtim log json file runtim log folder runtim log file runtim log total output upload json file runtim log folder runtim log file json dump runtim log def getitem self index print index index pair kei self list file index print pair kei pair kei pair boto client list object bucket boto bucket prefix pair kei delimit input imag kei pair content kei input imag path boto bucket input imag kei print input imag path input imag path input imag sourc file filepath input imag path input imag arrai imag open input imag sourc target imag kei pair content kei target imag path boto bucket target imag kei print target imag path target imag path target imag sourc file filepath target imag path target imag arrai imag open target imag sourc augment config transform imag input imag imag target imag input imag target imag augment imag input imag target imag augment imag augment imag input imag config transform input imag input imag imag target imag config transform mask imag target imag imag print input imag size input imag size print target imag size target imag size return input imag target imag multipl run trace failur point pixpix train index shown epoch pixpix train index epoch pixpix train index epoch pixpix train index epoch pixpix train index pair kei dataset train data test match raw final pair epoch pixpix train index pair kei dataset train data test match raw final pair epoch vii pixpix train index pair kei dataset train data test match raw final pair specif epoch batch size fail start batch end common index item caus fail index index common data point dataset caus fail seen pair kei data point dataset differ idea error happen",
        "Question_preprocessed_content":"error pytorch data loader batch cycl get error iter pytorch dataload batch cycl code result error multipl run trace failur point batch size fail start batch end common index item caus fail index index common data point dataset caus fail seen pair kei data point dataset differ idea error happen",
        "Question_gpt_summary_original":"The user is encountering an error in Sagemaker while iterating through PyTorch dataloader batch cycles. The error occurs at either the start or end of the batch and at specific indexes (64 and 160). There doesn't seem to be a common data point causing the error. The error message includes a KeyError and an UnexpectedStatusException. The user has provided code snippets and traces of the failure points.",
        "Question_gpt_summary":"user encount error iter pytorch dataload batch cycl error occur start end batch specif index common data point caus error error messag includ keyerror unexpectedstatusexcept user provid code snippet trace failur point",
        "Answer_original_content":"try run train script outsid train job happen error happen standalon script try run local train job reproduc second instead minut potenti us debugg figur problem",
        "Answer_preprocessed_content":"try run train script outsid train job happen error happen standalon script try run local train job reproduc second instead minut potenti us debugg figur problem",
        "Answer_gpt_summary_original":"the possible solutions to the error in pytorch data loader batch cycles are to run the training script outside of a training job and see if the error persists. if the error doesn't happen on a standalone script, try to run it as a local training job to reproduce it quickly and use a debugger to identify the problem.",
        "Answer_gpt_summary":"possibl solut error pytorch data loader batch cycl run train script outsid train job error persist error happen standalon script try run local train job reproduc quickli us debugg identifi problem"
    },
    {
        "Question_id":null,
        "Question_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Question_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603454458000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":426.0,
        "Answer_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\nNote: This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceInstanceType\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:CreateTrainingJob\",\n                \"sagemaker:CreateHyperParameterTuningJob\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"ForAllValues:StringLike\": {\n                    \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                }\n            }\n        }\n\n     ]\n}",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-23T12:15:48.000Z",
                "Answer_score":0,
                "Answer_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\nNote: This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceInstanceType\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:CreateTrainingJob\",\n                \"sagemaker:CreateHyperParameterTuningJob\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"ForAllValues:StringLike\": {\n                    \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                }\n            }\n        }\n\n     ]\n}",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603455348000,
        "Question_original_content":"limit type instanc data scientist launch train job want limit type instanc data scientist launch run train job hyperparamet tune job possibl limit instanc size option avail iam polici method exampl remov abil launch xlarg instanc",
        "Question_preprocessed_content":"limit type instanc data scientist launch train job want limit type instanc data scientist launch run train job hyperparamet tune job possibl limit instanc size option avail iam polici method exampl remov abil launch instanc",
        "Question_gpt_summary_original":"The user wants to limit the types of instances that data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. They are seeking a way to restrict the instance size options available through SageMaker, such as removing the ability to launch ml.p3.16xlarge instances, using IAM policies or another method.",
        "Question_gpt_summary":"user want limit type instanc data scientist launch run train job hyperparamet tune job seek wai restrict instanc size option avail remov abil launch xlarg instanc iam polici method",
        "Answer_original_content":"ye limit type instanc avail data scientist launch iam polici similar follow note exampl iam polici allow user launch comput optim type train job hyperparamet tune job version statement sid enforceinstancetyp effect allow action createtrainingjob createhyperparametertuningjob resourc condit forallvalu stringlik instancetyp",
        "Answer_preprocessed_content":"ye limit type instanc avail data scientist launch iam polici similar follow note exampl iam polici allow user launch comput optim type train job hyperparamet tune job version statement resourc condit",
        "Answer_gpt_summary_original":"the answer suggests that it is possible to limit the types of instances that data scientists can launch for training and hyperparameter tuning jobs by using an iam policy. the example iam policy provided in the answer allows users to launch only compute optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.",
        "Answer_gpt_summary":"answer suggest possibl limit type instanc data scientist launch train hyperparamet tune job iam polici exampl iam polici provid answer allow user launch comput optim type train job hyperparamet tune job"
    },
    {
        "Question_id":70888883.0,
        "Question_title":"Sagemaker batch transform job failure for 'batchStrategy: MultiRecord' along with data processing",
        "Question_body":"<p>We are using SageMaker Batch Transform job and to fit as many records in a mini-batch as can fit within the <code>MaxPayloadInMB<\/code> limit, we are setting <code>BatchStrategy<\/code> to <code>MultiRecord<\/code> and <code>SplitType<\/code> to <code>Line<\/code>.<\/p>\n<p>Input to the SageMaker batch transform job is:<\/p>\n<pre><code>{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 90, &quot;Experience&quot;: 26, &quot;Income&quot;: 30, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 55, &quot;Experience&quot;: 26, &quot;Income&quot;: 450, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-69e22778-594916685f4ceca66c08bfbc&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:46:32.386Z&quot;}\n<\/code><\/pre>\n<p>This is the SageMaker batch transform job config:<\/p>\n<pre><code>apiVersion: sagemaker.aws.amazon.com\/v1\nkind: BatchTransformJob\nmetadata:\n        generateName: '...-batchtransform'\nspec:\n        batchStrategy: MultiRecord\n        dataProcessing:\n                JoinSource: Input\n                OutputFilter: $\n                inputFilter: $.requestBody\n        modelClientConfig:\n                invocationsMaxRetries: 0\n                invocationsTimeoutInSeconds: 3\n        mName: '..'\n        region: us-west-2\n        transformInput:\n                contentType: application\/json\n                dataSource:\n                        s3DataSource:\n                                s3DataType: S3Prefix\n                                s3Uri: s3:\/\/......\/part-\n                splitType: Line\n        transformOutput:\n                accept: application\/json\n                assembleWith: Line\n                kmsKeyId: '....'\n                s3OutputPath: s3:\/\/....\/batch_output\n        transformResources:\n                instanceCount: ..\n                instanceType: '..'\n<\/code><\/pre>\n<p>The SageMaker batch transform job fails with:<\/p>\n<p>Error in batch transform data-log -<\/p>\n<blockquote>\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n400 Bad Request 2022-01-27T00:55:39.781:[sagemaker\nlogs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:\n<p>Failed to decode JSON object: Extra data: line 2 column 1 (char\n163)<\/p>\n<\/blockquote>\n<p><strong>Observation:<\/strong>\nThis issue occurs when we provide <code>batchStrategy: MultiRecord<\/code> in the manifest along with these data processing configs:<\/p>\n<pre><code>dataProcessing:\n        JoinSource: Input\n        OutputFilter: $\n        inputFilter: $.requestBody\n<\/code><\/pre>\n<p><strong>NOTE:<\/strong> If we put <code>batchStrategy: SingleRecord<\/code> along with the aforementioned data processing configs, it just works fine (job succeeds)!<\/p>\n<p><strong>Question:<\/strong> How can we achieve successful run with <code>batchStrategy: MultiRecord<\/code> along with the aforementioned data processing config?<\/p>\n<p>A successful output with <code>batchStrategy: SingleRecord<\/code> looks like this:<\/p>\n<blockquote>\n<p>{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:90,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:30}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-69e22778-594916685f4ceca66c08bfbc&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:55,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:450}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:46:32.386Z&quot;}\nRegion name \u2013 optional: Relevant resource ARN \u2013 optional:\narn:aws:sagemaker:us-west-2:435945521637:transform-job\/my-pipeline-9v28r-bat-e548fbfb125946528957e0f123456789<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1643344870580,
        "Question_favorite_count":null,
        "Question_last_edit_time":1643607224392,
        "Question_score":0.0,
        "Question_view_count":659.0,
        "Answer_body":"<p>When your input data is in JSON line format and you choose a SingleRecord BatchStrategy, your container will receive a single JSON payload body like below<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>However, if you use MultiRecord, Batch transform will split your JSON line input (which might contain 100 lines for example) into multiple records (say 10 records) all sent at once to your container as shown below:<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n.\n.\n.\n{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>Therefore your container should be able to handle such input for it to work. However, from the error message, I can see it is complaining about invalid JSON format as it reads the second row of the request.<\/p>\n<p>I also noticed that you have supplied <code>ContentType<\/code> and <code>AcceptType<\/code> as <code>application\/json<\/code> but instead should be <code>application\/jsonlines<\/code><\/p>\n<p>Could you please test your container to see if it can handle multiple JSON line records per single invocation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70888883",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643938529208,
        "Question_original_content":"batch transform job failur batchstrategi multirecord data process batch transform job fit record mini batch fit maxpayloadinmb limit set batchstrategi multirecord splittyp line input batch transform job requestbodi data ag experi incom famili ccavg mname loanpredict mversion testflag fals environ dev transactionid sdf bcecbdbf timestamp requestbodi data ag experi incom famili ccavg mname loanpredict mversion testflag fals environ dev transactionid fcecacbfbc timestamp batch transform job config apivers aw amazon com kind batchtransformjob metadata generatenam batchtransform spec batchstrategi multirecord dataprocess joinsourc input outputfilt inputfilt requestbodi modelclientconfig invocationsmaxretri invocationstimeoutinsecond mname region west transforminput contenttyp applic json datasourc sdatasourc sdatatyp sprefix suri splittyp line transformoutput accept applic json assemblewith line kmskeyid soutputpath batch output transformresourc instancecount instancetyp batch transform job fail error batch transform data log log ephemer dev loanpredict usw dev loanpredict pipelin fbb bcaaff txt log ephemer dev loanpredict usw dev loanpredict pipelin fbb bcaaff txt bad request log ephemer dev loanpredict usw dev loanpredict pipelin fbb bcaaff txt fail decod json object extra data line column char observ issu occur provid batchstrategi multirecord manifest data process config dataprocess joinsourc input outputfilt inputfilt requestbodi note batchstrategi singlerecord aforement data process config work fine job succe question achiev success run batchstrategi multirecord aforement data process config success output batchstrategi singlerecord look like output predict environ dev transactionid sdf bcecbdbf mname loanpredict mversion requestbodi data ag ccavg experi famili incom testflag fals timestamp output predict environ dev transactionid fcecacbfbc mname loanpredict mversion requestbodi data ag ccavg experi famili incom testflag fals timestamp region option relev resourc arn option arn aw west transform job pipelin bat efbfbef",
        "Question_preprocessed_content":"batch transform job failur batchstrategi multirecord data process batch transform job fit record fit limit set input batch transform job batch transform job config batch transform job fail error batch transform log log bad request log fail decod json object extra data line column observ issu occur provid manifest data process config note aforement data process config work fine question achiev success run aforement data process config success output look like region option relev resourc arn option",
        "Question_gpt_summary_original":"The user is encountering a failure in their SageMaker batch transform job when using the <code>batchStrategy: MultiRecord<\/code> configuration along with certain data processing configurations. The error message indicates a failure to decode a JSON object due to extra data. The user is seeking a solution to successfully run the job with the <code>batchStrategy: MultiRecord<\/code> configuration and the aforementioned data processing configurations. The job runs successfully when using the <code>batchStrategy: SingleRecord<\/code> configuration.",
        "Question_gpt_summary":"user encount failur batch transform job batchstrategi multirecord configur certain data process configur error messag indic failur decod json object extra data user seek solut successfulli run job batchstrategi multirecord configur aforement data process configur job run successfulli batchstrategi singlerecord configur",
        "Answer_original_content":"input data json line format choos singlerecord batchstrategi contain receiv singl json payload bodi like us multirecord batch transform split json line input contain line exampl multipl record record sent contain shown contain abl handl input work error messag complain invalid json format read second row request notic suppli contenttyp accepttyp applic json instead applic jsonlin test contain handl multipl json line record singl invoc",
        "Answer_preprocessed_content":"input data json line format choos singlerecord batchstrategi contain receiv singl json payload bodi like us multirecord batch transform split json line input multipl record sent contain shown contain abl handl input work error messag complain invalid json format read second row request notic suppli instead test contain handl multipl json line record singl invoc",
        "Answer_gpt_summary_original":"the answer suggests that the error in the data-log may be due to the use of 'batchstrategy: multirecord' with data processing configs. it advises the user to ensure that their container can handle multiple json line records per single invocation. additionally, the answer notes that the contenttype and accepttype should be set to application\/jsonlines instead of application\/json.",
        "Answer_gpt_summary":"answer suggest error data log us batchstrategi multirecord data process config advis user ensur contain handl multipl json line record singl invoc addition answer note contenttyp accepttyp set applic jsonlin instead applic json"
    },
    {
        "Question_id":null,
        "Question_title":"Custom Argument pass to Docker Container Azure ML inference",
        "Question_body":"Hello Team,\n\nI'm trying to pass the arguments to Azure ML docker. I have created an environment like this.\n\n env = Environment.from_conda_specification(name='pytorch-1.6-gpu', file_path='curated_env\/conda_dependencies.yml' )\n\n\n\nAm I passing the arguments correct?\n\n DOCKER_ARGUMENTS = [\"--shm-size\",\"32G\"]  # increase shared memory\n env.docker.arguments = DOCKER_ARGUMENTS\n\n\n\n\nThe main goal of this project is to deploy a model on the AKS inference cluster. I have successfully deployed the model. When I try to get predictions from the model I got this error\n\nIt is possible that data loaders workers are out of shared memory. Please try to raise your shared memory limit\n\nHow can I do that if that's not the correct way to pass arguments?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1632856020243,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/569816\/custom-argument-pass-to-docker-container-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-29T06:31:54.6Z",
                "Answer_score":0,
                "Answer_body":"@khubaibRaza-8970 To pass the argument for increasing the default \"shm_size\" you would have to use the DockerConfiguration object. Here is a sample to achieve this:\n\n from azureml.core import Environment\n from azureml.core import ScriptRunConfig\n from azureml.core.runconfig import DockerConfiguration\n    \n    \n # Specify VM and Python environment:\n my_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)\n my_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'\n    \n docker_config = DockerConfiguration(use_docker=True,shm_size='32g')\n    \n # Finally, use the environment in the ScriptRunConfig:\n src = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,\n                       script=SCRIPT_FILE_TO_EXECUTE,\n                       arguments=EXECUTE_ARGUMENTS,\n                       compute_target=compute_target,\n                       environment=my_env,\n                       docker_runtime_config=docker_config)\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"custom argument pass docker contain infer hello team try pass argument docker creat environ like env environ conda specif pytorch gpu file path curat env conda depend yml pass argument correct docker argument shm size increas share memori env docker argument docker argument main goal project deploi model ak infer cluster successfulli deploi model try predict model got error possibl data loader worker share memori try rais share memori limit correct wai pass argument",
        "Question_preprocessed_content":"custom argument pass docker contain infer hello team try pass argument docker creat environ like env pass argument correct increas share memori main goal project deploi model ak infer cluster successfulli deploi model try predict model got error possibl data loader worker share memori try rais share memori limit correct wai pass argument",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to pass arguments to Azure ML docker. They have created an environment and passed the arguments, but when trying to get predictions from the model, they received an error indicating that the data loaders workers are out of shared memory. The user is seeking guidance on how to increase the shared memory limit.",
        "Question_gpt_summary":"user encount challeng try pass argument docker creat environ pass argument try predict model receiv error indic data loader worker share memori user seek guidanc increas share memori limit",
        "Answer_original_content":"khubaibraza pass argument increas default shm size us dockerconfigur object sampl achiev core import environ core import scriptrunconfig core runconfig import dockerconfigur specifi python environ env environ conda specif test env file path path yaml file env docker base imag mcr microsoft com openmpi cuda cudnn ubuntu docker config dockerconfigur us docker true shm size final us environ scriptrunconfig src scriptrunconfig sourc directori deploi contain folder path script script file execut argument execut argument comput target comput target environ env docker runtim config docker config answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"pass argument increas default us dockerconfigur object sampl achiev core import environ core import scriptrunconfig import dockerconfigur specifi python environ final us environ scriptrunconfig src answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"the solution to passing custom arguments to a docker container for inference on an aks cluster and resolving the error related to shared memory is to use the dockerconfiguration object and specify the desired \"shm_size\" value. the answer provides a sample code snippet to achieve this.",
        "Answer_gpt_summary":"solut pass custom argument docker contain infer ak cluster resolv error relat share memori us dockerconfigur object specifi desir shm size valu answer provid sampl code snippet achiev"
    },
    {
        "Question_id":null,
        "Question_title":"What's the best way to preserve Azure ML workspace so that it can be restored",
        "Question_body":"What's the best way to preserve Azure ML workspace so that it can be restored at a later point? I was hoping to find some automatic way to take a snapshot of artifacts & code and dump it into Azure storage, but haven't been able to find anything relevant in the online documentation.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1669442139003,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@VaraPrasad-1740 Thanks for the question. I would recommend you can have a git repository that backs your project. For some details about this approach you can check https:\/\/santiagof.medium.com\/structure-your-machine-learning-project-source-code-like-a-pro-44815cac8652",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1105130\/what39s-the-best-way-to-preserve-azure-ml-workspac.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-28T00:46:02.44Z",
                "Answer_score":0,
                "Answer_body":"@VaraPrasad-1740 Thanks for the question. I would recommend you can have a git repository that backs your project. For some details about this approach you can check https:\/\/santiagof.medium.com\/structure-your-machine-learning-project-source-code-like-a-pro-44815cac8652",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1669596362440,
        "Question_original_content":"best wai preserv workspac restor best wai preserv workspac restor later point hope automat wai snapshot artifact code dump azur storag haven abl relev onlin document",
        "Question_preprocessed_content":"best wai preserv workspac restor best wai preserv workspac restor later point hope automat wai snapshot artifact code dump azur storag haven abl relev onlin document",
        "Question_gpt_summary_original":"The user is facing challenges in finding an automatic way to preserve their Azure ML workspace so that it can be restored at a later point. They have searched for relevant information in the online documentation but have not found anything useful.",
        "Question_gpt_summary":"user face challeng find automat wai preserv workspac restor later point search relev inform onlin document us",
        "Answer_original_content":"varaprasad thank question recommend git repositori back project detail approach check http santiagof medium com structur machin learn project sourc code like pro cac",
        "Answer_preprocessed_content":"thank question recommend git repositori back project detail approach check",
        "Answer_gpt_summary_original":"possible solution: the answer suggests using a git repository to back up the project and preserve the workspace for future restoration. the user can refer to the provided link for more details on this approach.",
        "Answer_gpt_summary":"possibl solut answer suggest git repositori project preserv workspac futur restor user refer provid link detail approach"
    },
    {
        "Question_id":null,
        "Question_title":"Vertex AI dataset permissions",
        "Question_body":"Is there a way to assign IAM roles to datasets in Vertex AI so only certain people have access to certain datasets?",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1632122100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":432.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-dataset-permissions\/td-p\/170536\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-20T07:15:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Is there a way to assign IAM roles to datasets in Vertex AI so only certain people have access to certain datasets?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dataset permiss wai assign iam role dataset certain peopl access certain dataset",
        "Question_preprocessed_content":"dataset permiss wai assign iam role dataset certain peopl access certain dataset",
        "Question_gpt_summary_original":"The user is facing a challenge in assigning IAM roles to datasets in Vertex AI to restrict access to certain datasets for specific individuals.",
        "Question_gpt_summary":"user face challeng assign iam role dataset restrict access certain dataset specif individu",
        "Answer_original_content":"wai assign iam role dataset certain peopl access certain dataset",
        "Answer_preprocessed_content":"wai assign iam role dataset certain peopl access certain dataset",
        "Answer_gpt_summary_original":"the solution to limit access to certain datasets is to assign iam roles to those datasets. this will ensure that only certain people with the assigned roles will have access to those datasets.",
        "Answer_gpt_summary":"solut limit access certain dataset assign iam role dataset ensur certain peopl assign role access dataset"
    },
    {
        "Question_id":72478572.0,
        "Question_title":"Amazon Sagemaker JupiterLab Notebook - No matching distribution found for Pandas",
        "Question_body":"<p>If I attempt to Upgrade <strong>Pandas<\/strong> above version <strong>1.1.5<\/strong> on my <strong>AWS Sagemaker<\/strong> provided <strong>JupyterLab<\/strong> notebook I receive the error <strong>No Matching Distribution Found<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\n!{sys.executable} -m pip install --pre --upgrade pandas==1.3.5\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>ERROR: Could not find a version that satisfies the requirement pandas==1.3.5 (from versions: 0.1, 0.2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.25.2, 0.25.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5)\nERROR: No matching distribution found for pandas==1.3.5\n<\/code><\/pre>\n<h2>Background<\/h2>\n<p>I created a Notebook instance from the AWS Console via <strong>AWS Sagemaker -&gt; Notebook instances -&gt; Create Notebook instance<\/strong>.<\/p>\n<p>I then selected the Kernel <strong>conda_Python3<\/strong>.<\/p>\n<p>I use <strong>sys.executable<\/strong> to show the Kernel's Python, Pip and Pandas version.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -version\nPython 3.6.13\n\n!{sys.executable} -m pip show pip\nName: pip\nVersion: 21.3.1\nSummary: The PyPA recommended tool for installing Python packages.\nHome-page: https:\/\/pip.pypa.io\/\nAuthor: The pip developers\nAuthor-email: distutils-sig@python.org\nLicense: MIT\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: \nRequired-by: \n\n!{sys.executable} -m pip show pandas\nName: pandas\nVersion: 1.1.5\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: https:\/\/pandas.pydata.org\nAuthor: \nAuthor-email: \nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: numpy, python-dateutil, pytz\nRequired-by: autovizwidget, awswrangler, hdijupyterutils, odo, sagemaker, seaborn, shap, smclarify, sparkmagic, statsmodels\n<\/code><\/pre>\n<p>I cannot upgrade <strong>Pandas<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -m pip install --pre --upgrade pandas\nRequirement already satisfied: pandas in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (1.1.5)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy&gt;=1.15.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (1.18.5)\nRequirement already satisfied: six&gt;=1.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654183154907,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":184.0,
        "Answer_body":"<p>see this - <a href=\"https:\/\/stackoverflow.com\/questions\/68750375\/no-matching-distribution-found-for-pandas-1-3-1\">No matching distribution found for pandas==1.3.1<\/a><\/p>\n<p>The latest version to support python 3.6 is 1.1.5.<\/p>\n<p>You can create a new conda environment with python version &gt;= 3.7 in your existing notebook, or move to notebooks with Amazon Linux 2 (see <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebook-instance-now-supports-amazon-linux-2\/\" rel=\"nofollow noreferrer\">blog post<\/a>). In the AL2 notebooks, <code>conda_python3<\/code> kernels come with Python 3.8.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72478572",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1654275686200,
        "Question_original_content":"jupiterlab notebook match distribut panda attempt upgrad panda version provid jupyterlab notebook receiv error match distribut import sy sy execut pip instal pre upgrad panda error version satisfi requir panda version error match distribut panda background creat notebook instanc aw consol notebook instanc creat notebook instanc select kernel conda python us sy execut kernel python pip panda version sy execut version python sy execut pip pip pip version summari pypa recommend tool instal python packag home page http pip pypa author pip develop author email distutil sig python org licens mit locat home user anaconda env python lib python site packag requir requir sy execut pip panda panda version summari power data structur data analysi time seri statist home page http panda pydata org author author email licens bsd locat home user anaconda env python lib python site packag requir numpi python dateutil pytz requir autovizwidget awswrangl hdijupyterutil odo seaborn shap smclarifi sparkmag statsmodel upgrad panda sy execut pip instal pre upgrad panda requir satisfi panda home user anaconda env python lib python site packag requir satisfi python dateutil home user anaconda env python lib python site packag panda requir satisfi pytz home user anaconda env python lib python site packag panda requir satisfi numpi home user anaconda env python lib python site packag panda requir satisfi home user anaconda env python lib python site packag python dateutil panda",
        "Question_preprocessed_content":"jupiterlab notebook match distribut panda attempt upgrad panda version provid jupyterlab notebook receiv error match distribut background creat notebook instanc aw consol notebook instanc creat notebook instanc select kernel us kernel python pip panda version upgrad panda",
        "Question_gpt_summary_original":"The user is encountering an error when attempting to upgrade Pandas above version 1.1.5 on their AWS Sagemaker provided JupyterLab notebook. The error message states that there is no matching distribution found for Pandas. The user has created a Notebook instance from the AWS Console via AWS Sagemaker and selected the Kernel conda_Python3. The user is unable to upgrade Pandas and has attempted to upgrade it using pip install, but it did not work.",
        "Question_gpt_summary":"user encount error attempt upgrad panda version provid jupyterlab notebook error messag state match distribut panda user creat notebook instanc aw consol select kernel conda python user unabl upgrad panda attempt upgrad pip instal work",
        "Answer_original_content":"match distribut panda latest version support python creat new conda environ python version exist notebook notebook amazon linux blog post notebook conda python kernel come python",
        "Answer_preprocessed_content":"match distribut latest version support python creat new conda environ python version exist notebook notebook amazon linux notebook kernel come python",
        "Answer_gpt_summary_original":"possible solutions to the challenge of upgrading pandas version on a provided jupyterlab notebook and encountering the error \"no matching distribution found\" when attempting to upgrade above version 1.1.5 are: \n1. create a new conda environment with python version >= 3.7 in the existing notebook.\n2. move to notebooks with amazon linux 2, where conda_python3 kernels come with python 3.8.",
        "Answer_gpt_summary":"possibl solut challeng upgrad panda version provid jupyterlab notebook encount error match distribut attempt upgrad version creat new conda environ python version exist notebook notebook amazon linux conda python kernel come python"
    },
    {
        "Question_id":null,
        "Question_title":"LightGBM on SageMaker",
        "Question_body":"I have a customer who wants to install LightGBM on SageMaker notebooks, as they are currently using it outside of SageMaker.\n\nRight now, they are interested in the ability to SSH into the instance, but it would be great if we could provide them a way to install LightGBM right now.\n\nCheers",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1516632842000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":388.0,
        "Answer_body":"It's possible to do, I have used it myself for gradient boosting, from within Jupyter you can simply run:\n\n!conda install -y -c conda-forge lightgbm\n\n\nWithin a selected conda environment. No terminal access is needed, however it must be done, On the top right of the Jupyter notebook you can choose a terminal environment which will give you a shell to the backend instance and you can install there.\n\nHowever if you want the notebook to be immutable\/transferable you can do the install within the notebook .\n\nThanks",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPwkZcylKQR6-u0pghgrseA\/light-gbm-on-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-01-22T14:58:17.000Z",
                "Answer_score":0,
                "Answer_body":"It's possible to do, I have used it myself for gradient boosting, from within Jupyter you can simply run:\n\n!conda install -y -c conda-forge lightgbm\n\n\nWithin a selected conda environment. No terminal access is needed, however it must be done, On the top right of the Jupyter notebook you can choose a terminal environment which will give you a shell to the backend instance and you can install there.\n\nHowever if you want the notebook to be immutable\/transferable you can do the install within the notebook .\n\nThanks",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1516633097000,
        "Question_original_content":"lightgbm custom want instal lightgbm notebook current outsid right interest abil ssh instanc great provid wai instal lightgbm right cheer",
        "Question_preprocessed_content":"lightgbm custom want instal lightgbm notebook current outsid right interest abil ssh instanc great provid wai instal lightgbm right cheer",
        "Question_gpt_summary_original":"The user is facing a challenge of installing LightGBM on SageMaker notebooks for a customer who is currently using it outside of SageMaker. They are also interested in the ability to SSH into the instance.",
        "Question_gpt_summary":"user face challeng instal lightgbm notebook custom current outsid interest abil ssh instanc",
        "Answer_original_content":"possibl gradient boost jupyt simpli run conda instal conda forg lightgbm select conda environ termin access need right jupyt notebook choos termin environ shell backend instanc instal want notebook immut transfer instal notebook thank",
        "Answer_preprocessed_content":"possibl gradient boost jupyt simpli run conda instal lightgbm select conda environ termin access need right jupyt notebook choos termin environ shell backend instanc instal want notebook instal notebook thank",
        "Answer_gpt_summary_original":"possible solutions to install lightgbm on notebooks are:\n\n1. run the command \"!conda install -y -c conda-forge lightgbm\" within a selected conda environment from within jupyter.\n2. choose a terminal environment on the top right of the jupyter notebook to get a shell to the backend instance and install lightgbm there.\n3. install lightgbm within the notebook if you want it to be immutable\/transferable.",
        "Answer_gpt_summary":"possibl solut instal lightgbm notebook run command conda instal conda forg lightgbm select conda environ jupyt choos termin environ right jupyt notebook shell backend instanc instal lightgbm instal lightgbm notebook want immut transfer"
    },
    {
        "Question_id":null,
        "Question_title":"Semi-transparent smoothing stopped working",
        "Question_body":"<p>Smoothing suddenly stopped making the original graph semi-transparent.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png\" data-download-href=\"\/uploads\/short-url\/6HJfxA7ERNAiH7FwWeL8MzvFDIi.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_512x500.png\" alt=\"image\" data-base62-sha1=\"6HJfxA7ERNAiH7FwWeL8MzvFDIi\" width=\"512\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_512x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">517\u00d7504 48.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Any way your could rollback to the previous behavior? Some ETA for fixing this would be much appreciated, so our team could plan accordingly.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1639019539043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":378.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/semi-transparent-smoothing-stopped-working\/1492",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-09T03:25:43.450Z",
                "Answer_body":"<p>PS. For anyone facing this issue - a colleague of mine found a work-around in the \u201cEdit panel\u201d to turn off the original chart:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/de2a9528f51e0238785436562d02036c199af134.png\" alt=\"image\" data-base62-sha1=\"vHnellx3bs9OhnqiUZ1lqfZ0unW\" width=\"227\" height=\"463\"><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-09T12:06:18.979Z",
                "Answer_body":"<p>Hey Sebastian,<br>\nthank you for reporting this. I\u2019ll share this with the team so we can fix this as soon as possible.<br>\nWill let you know once the issue is resolved.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-13T08:19:32.556Z",
                "Answer_body":"<p>Hey there, the issue has been fixed. Thank you for your patience!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-11T08:19:42.541Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"semi transpar smooth stop work smooth suddenli stop make origin graph semi transpar imag wai rollback previou behavior eta fix appreci team plan accordingli",
        "Question_preprocessed_content":"smooth stop work smooth suddenli stop make origin graph imag wai rollback previou behavior eta fix appreci team plan accordingli",
        "Question_gpt_summary_original":"The user is facing a challenge where the semi-transparent smoothing feature has suddenly stopped working, resulting in the original graph not being semi-transparent anymore. The user is requesting a rollback to the previous behavior and an ETA for fixing the issue.",
        "Question_gpt_summary":"user face challeng semi transpar smooth featur suddenli stop work result origin graph semi transpar anymor user request rollback previou behavior eta fix issu",
        "Answer_original_content":"face issu colleagu work edit panel turn origin chart hei sebastian thank report ill share team fix soon possibl let know issu resolv hei issu fix thank patienc topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"face issu colleagu edit panel turn origin chart hei sebastian thank report ill share team fix soon possibl let know issu resolv hei issu fix thank patienc topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"a colleague found a work-around in the edit panel to turn off the original chart. the team is aware of the issue and will fix it as soon as possible. the issue has been fixed and the user is thanked for their patience.",
        "Answer_gpt_summary":"colleagu work edit panel turn origin chart team awar issu fix soon possibl issu fix user thank patienc"
    },
    {
        "Question_id":null,
        "Question_title":"Trying to do multiple voice files with speech-to-text",
        "Question_body":"Hello.I'm someone who's trying to make speech-to-text work without being a coder in any way whatsoever. I have let's say hundreds of individual audio files and they go from 30 seconds to a minute and a half. The problem is that uploading them to the bucket makes it so there's hundreds of individual ones. And I need to create a transcriptions individually. what do I do? can I not just transcribe everything in one folder?",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1652292780000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Trying-to-do-multiple-voice-files-with-speech-to-text\/td-p\/422295\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-13T22:37:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"It requires coding - you load files in a bucket (say in 'input' folder) and run a background job to produce a \"txt\" file for each using speech-to-text API (say in 'output' folder).\u00a0 If you have files formats such as mp4 then use transcoding. This is the step roughly."
            },
            {
                "Answer_creation_time":"2022-05-15T21:36:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Understood. But do you have any idea how I'd code that? Of course I am\u00a0 unfamiliar with how to code... but anyway no problem at all if you cannot help there."
            },
            {
                "Answer_creation_time":"2022-05-15T22:10:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Coding it will not be easy if you are not a programmer. It requires a bunch of technology and tools. Roughly steps would be:\n\n1. Using gsutil tool of the GCP, upload files to a bucket.\n\n2. Write a program to read file from the bucket and invoke\u00a0 Speech-to-Text API. It will require you to acquire an access-token (OAuth2).\u00a0\n\n3. If files are small then you could do 2 without uploading files to the bucket.\u00a0\n\nYou can find the example programs here:\u00a0https:\/\/cloud.google.com\/speech-to-text\/docs\/samples"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"try multipl voic file speech text hello try speech text work coder wai whatsoev let hundr individu audio file second minut half problem upload bucket make hundr individu on need creat transcript individu transcrib folder",
        "Question_preprocessed_content":"try multipl voic file try work coder wai whatsoev let hundr individu audio file second minut half problem upload bucket make hundr individu on need creat transcript individu transcrib folder",
        "Question_gpt_summary_original":"The user has encountered a challenge in transcribing multiple voice files using speech-to-text technology. They have hundreds of individual audio files ranging from 30 seconds to a minute and a half, which they need to transcribe individually after uploading them to the bucket. The user is looking for a solution to transcribe everything in one folder instead of creating transcriptions individually.",
        "Question_gpt_summary":"user encount challeng transcrib multipl voic file speech text technolog hundr individu audio file rang second minut half need transcrib individu upload bucket user look solut transcrib folder instead creat transcript individu",
        "Answer_original_content":"requir code load file bucket input folder run background job produc txt file speech text api output folder file format us transcod step roughli understood idea code cours unfamiliar code problem help code easi programm requir bunch technolog tool roughli step gsutil tool gcp upload file bucket write program read file bucket invok speech text api requir acquir access token oauth file small upload file bucket exampl program http cloud googl com speech text doc sampl",
        "Answer_preprocessed_content":"requir code load file bucket run background job produc txt file api file format us transcod step roughli understood idea code cours unfamiliar problem help code easi programm requir bunch technolog tool roughli step gsutil tool gcp upload file bucket write program read file bucket invok api requir acquir file small upload file bucket exampl program",
        "Answer_gpt_summary_original":"possible solutions to transcribe multiple audio files without coding knowledge are not provided in the answer. however, the answer suggests that the task requires coding and provides a rough outline of the steps involved in coding the solution. the steps include uploading files to a bucket using gsutil tool, writing a program to read files from the bucket and invoke speech-to-text api, and acquiring an access-token (oauth2). the answer also provides a link to example programs that can be used as a reference.",
        "Answer_gpt_summary":"possibl solut transcrib multipl audio file code knowledg provid answer answer suggest task requir code provid rough outlin step involv code solut step includ upload file bucket gsutil tool write program read file bucket invok speech text api acquir access token oauth answer provid link exampl program refer"
    },
    {
        "Question_id":null,
        "Question_title":"Cant see the team after accepting invite",
        "Question_body":"<p>Hi! I can\u2019t see my team after accepting the invite, it says \u201cReceived an invite but still can\u2019t see the team? Make sure you are logged in with the email where you received the invite.\u201d but it does not tell you what to do when this is already checked <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/stuck_out_tongue.png?v=12\" title=\":stuck_out_tongue:\" class=\"emoji\" alt=\":stuck_out_tongue:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>regards<br>\nR<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1659088178684,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":582.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cant-see-the-team-after-accepting-invite\/2823",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-30T00:10:33.264Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/molen\">@molen<\/a> , it shows on our end you are already a member of the team <strong>miselbo<\/strong>. Do let me know if you can\u2019t access the team from your wandb profile and we will take a closer look.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-30T06:58:02.916Z",
                "Answer_body":"<p>hi <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> , thanks for getting back to me. the team im trying to join is <strong>calejo-dev<\/strong><br>\nregards<br>\nR<\/p>",
                "Answer_score":16.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-04T23:36:37.689Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/molen\">@molen<\/a> , I checked the <strong>calejo-dev<\/strong> team and you are member of it. It\u2019s listed under your teams on your profile page, <a href=\"https:\/\/wandb.ai\/ricky_molen\" class=\"inline-onebox\">Weights &amp; Biases<\/a>. Please confirm if you can see the team as well and if you need further assistance. Thank-you.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-03T23:37:27.316Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"team accept invit team accept invit sai receiv invit team sure log email receiv invit tell check regard",
        "Question_preprocessed_content":"team accept invit team accept invit sai receiv invit team sure log email receiv invit tell check regard",
        "Question_gpt_summary_original":"The user is unable to see their team after accepting an invite and is receiving a message to ensure they are logged in with the correct email, but is unsure of what to do next.",
        "Question_gpt_summary":"user unabl team accept invit receiv messag ensur log correct email unsur",
        "Answer_original_content":"molen show end member team miselbo let know access team profil closer look mohammadbakir thank get team try join calejo dev regard molen check calejo dev team member list team profil page confirm team need assist thank topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"show end member team miselbo let know access team profil closer look thank get team try join regard check team member list team profil page confirm team need assist topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions: \n- check if the user is logged in with the correct email address.\n- check if the user is a member of the correct team.\n- check if the user can access the team from their profile.\n- if the user still can't access the team, take a closer look and provide further assistance.",
        "Answer_gpt_summary":"possibl solut check user log correct email address check user member correct team check user access team profil user access team closer look provid assist"
    },
    {
        "Question_id":71340893.0,
        "Question_title":"When I get a prediction from sagemaker endpoint, what does the endpoint do?",
        "Question_body":"<p>In sagemaker, the docs talk about inference scripts requiring to have 4 specific functions. When we get a prediction, the python SDK sends a request to the endpoint.<\/p>\n<p>Then the inference script runs. But I cannot find where in the SDK the inference script is run.<\/p>\n<p>When I navigate through the sdk code the <code>Predictor.predict()<\/code> method calls the sagemaker session to post a request to the endpoint and get a response. That is the final step in the sdk. Sagemaker is obviously doing something when it receives that request.<\/p>\n<p>What is the code that it runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646326739290,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":405.0,
        "Answer_body":"<p>The endpoint is essentially a Flask web server running in a Docker container<\/p>\n<p>If it's a scikit-learn image, when you invoke the endpoint, it loads your script from S3, then...<\/p>\n<p>It calls <code>input_fn(request_body: bytearray, content_type) -&gt; np.ndarray<\/code> to parse the <code>request_body<\/code> into a numpy array<\/p>\n<p>Then it calls your <code>model_fn(model_dir: str) -&gt; object<\/code> function to load the model from <code>model_dir<\/code> and return the model<\/p>\n<p>Then it calls <code>predict_fn(input_object: np.ndarray, model: object) -&gt; np.array<\/code>, which calls your <code>model.predict()<\/code> function and returns the prediction<\/p>\n<p>Then it calls <code>output_fn(prediction: np.array, accept: str)<\/code> to take the result from <code>predict_fn<\/code> and encode it to the <code>accept<\/code> type<\/p>\n<p>You don't need to implement all of these functions yourself, as there are defaults<\/p>\n<p>You <strong>do<\/strong> need to implement <code>model_fn<\/code><\/p>\n<p>You only need to implement <code>input_fn<\/code> if you have non numeric data<\/p>\n<p>You only need to implement <code>predict_fn<\/code> if your model uses something other than <code>.predict()<\/code><\/p>\n<p>You can see how the default function implementations work <a href=\"https:\/\/github.com\/aws\/sagemaker-scikit-learn-container\/blob\/master\/src\/sagemaker_sklearn_container\/serving.py\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1646761223947,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71340893",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1646329084320,
        "Question_original_content":"predict endpoint endpoint doc talk infer script requir specif function predict python sdk send request endpoint infer script run sdk infer script run navig sdk code predictor predict method call session post request endpoint respons final step sdk obvious receiv request code run",
        "Question_preprocessed_content":"predict endpoint endpoint doc talk infer script requir specif function predict python sdk send request endpoint infer script run sdk infer script run navig sdk code method call session post request endpoint respons final step sdk obvious receiv request code run",
        "Question_gpt_summary_original":"The user is facing a challenge in understanding the process of getting a prediction from a Sagemaker endpoint. They are unable to locate where the inference script is run in the Python SDK and are curious about the code that Sagemaker runs when it receives a request for prediction.",
        "Question_gpt_summary":"user face challeng understand process get predict endpoint unabl locat infer script run python sdk curiou code run receiv request predict",
        "Answer_original_content":"endpoint essenti flask web server run docker contain scikit learn imag invok endpoint load script call input request bodi bytearrai content type ndarrai pars request bodi numpi arrai call model model dir str object function load model model dir return model call predict input object ndarrai model object arrai call model predict function return predict call output predict arrai accept str result predict encod accept type need implement function default need implement model need implement input non numer data need implement predict model us predict default function implement work",
        "Answer_preprocessed_content":"endpoint essenti flask web server run docker contain imag invok endpoint load script call pars numpi arrai call function load model return model call call function return predict call result encod type need implement function default need implement need implement non numer data need implement model us default function implement work",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n\n- understanding that the endpoint is essentially a flask web server running in a docker container if it's a scikit-learn image.\n- knowing that when the endpoint is invoked, it loads the script from s3.\n- implementing the necessary functions (input_fn, model_fn, predict_fn, and output_fn) to parse the request body, load the model, make predictions, and encode the results.\n- not needing to implement all of these functions yourself, as there are defaults available.\n- only needing to implement input_fn if you have non-numeric data, and predict_fn if your model uses something other than .predict().",
        "Answer_gpt_summary":"possibl solut answer includ understand endpoint essenti flask web server run docker contain scikit learn imag know endpoint invok load script implement necessari function input model predict output pars request bodi load model predict encod result need implement function default avail need implement input non numer data predict model us predict"
    },
    {
        "Question_id":null,
        "Question_title":"Setup DVC to work with shared data on NAS server",
        "Question_body":"<p>Hi DVC team!<\/p>\n<p>Thanks for providing these wonderful tools. Our scenario is that we have a server with NAS storage and many projects need to use data stored in NAS. The storage will be updated so the size of data will grow every day<\/p>\n<pre><code class=\"lang-auto\">\/mnt\/dataset\/project1_data\/\n<\/code><\/pre>\n<p>And the model code is listed in<\/p>\n<pre><code class=\"lang-auto\">\/home\/user\/project1\/\n<\/code><\/pre>\n<p>The project1_data takes about 100 GB, so when I used <strong>dvc add \/mnt\/dataset\/project1_data\/<\/strong>, it takes so long that I shut down it before it runs to end.<\/p>\n<p>My question is how do we use DVC to do data version control and code control based on NAS storage? I have a look into <a href=\"https:\/\/dvc.org\/doc\/use-cases\/multiple-data-scientists-on-a-single-machine\" rel=\"nofollow noopener\">Shared Development Server<\/a> but not clear for adding data from NAS.<\/p>\n<p>Thanks again!<\/p>",
        "Question_answer_count":10,
        "Question_comment_count":0,
        "Question_creation_time":1557992716645,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":12056.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/setup-dvc-to-work-with-shared-data-on-nas-server\/180",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-05-16T13:43:49.791Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jimmy15923\">@jimmy15923<\/a> !<\/p>\n<p>That happens because when you are running <code>dvc add \/mnt\/dataset\/project1_data<\/code>, dvc is trying to cache your data, by storing it <code>.dvc\/cache<\/code> and creating links back to <code>\/mnt\/dataset\/project1_data<\/code>. Did you mean to cache it and version it with dvc or do you just want to specify it as a dependency for your next stage without caching it? If it is the case, then you could simply specify it as an external dependency in your <code>dvc run<\/code> command like so <code>dvc run -d \/mnt\/dataset\/project1_data ...<\/code>.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":35.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-05-17T03:05:35.217Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>!<\/p>\n<p>Thanks for your reply!<br>\nSince the \/mnt\/dataset\/project1_data will be updated every day, I want to version it (be able to checkout to old version).<br>\nSorry about that I am not clear for \u201cspecify it as a dependency for your next stage without caching it?\u201d, so I don\u2019t know which case to use.<\/p>",
                "Answer_score":165.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-05-18T21:08:50.876Z",
                "Answer_body":"<p>Make sure to read this thread as well: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/4303\" class=\"inline-onebox\">guide: Using a shared cache in networked filesystems (NAS\/NFS, etc) \u00b7 Issue #4303 \u00b7 iterative\/dvc.org \u00b7 GitHub<\/a> . It makes things faster on NAS\/NFS.<\/p>\n<p>Hi <a class=\"mention\" href=\"\/u\/jimmy15923\">@jimmy15923<\/a>!<\/p>\n<p>Could you give a little bit more context, please:<\/p>\n<ol>\n<li>How long was it taking before you interrupted the process?<\/li>\n<li>How do you mount your NAS storage? NFS, something else?<\/li>\n<li>How is your code written now that processes this data? Does it address it via the <code>\/mnt\/dataset\/project1_data\/<\/code> path? Or do you use symlinks to the projects directory?<\/li>\n<\/ol>\n<p>It looks like in your case it would be beneficial to keep DVC cache (a place where all versions of all datasets, models, etc are stored) on your NAS. Thus you would avoid copying large files from you NAS to the machine (let me know if you actually want to have a copy of your data in your project\u2019s workspace).<\/p>\n<pre><code class=\"lang-bash\">cd \/home\/user\/project1\/\ndvc init\ndvc cache dir \/mnt\/dataset\/storage\ndvc config cache.type \"reflink,symlink,hardlink,copy\" # to enable symlinks to avoid copying\ndvc config cache.protected true # to make links RO so that we you don't corrupt them accidentally\ngit add .dvc .gitignore\ngit commit . -m \"initialize DVC\"\n<\/code><\/pre>\n<p>Now, to add a first version of the dataset into the DVC cache (this is done once for a dataset), I would do the following:<\/p>\n<pre><code class=\"lang-auto\">cd \/mnt\/dataset\ncp -r \/home\/user\/project1\/\ncd project1\nmv \/mnt\/dataset\/project1_data\/ data\/\ndvc add data # it should be way faster now\ngit add data.dvc .gitignore\ngit commit . -m \"add first version of the dataset\"\ngit tag -a \"v1.0\" -m \"dataset v1.0\"\ngit push origin HEAD\ngit push origin v1.0\n<\/code><\/pre>\n<p>Next, in your project, you can do something like this:<\/p>\n<pre><code class=\"lang-auto\">cd \/home\/user\/project1\/\ngit pull\n# you should see data.dvc file now\ndvc checkout\n# you should see the data directory now that should be symlink to the NAS storage\n<\/code><\/pre>",
                "Answer_score":899.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-05-23T09:36:50.709Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a>!<\/p>\n<p>Really appreciate your reply and sorry I just notice it until today!<\/p>\n<p>About your questions:<\/p>\n<ol>\n<li>\n<p>How long was it taking before you interrupted the process?<br>\nFor about 10 mins because it looks like needs lots of time to finish<\/p>\n<\/li>\n<li>\n<p>How do you mount your NAS storage? NFS, something else?<br>\nIt\u2019s NFS so everyone in the server can access the NAS<\/p>\n<\/li>\n<li>\n<p>How is your code written now that processes this data? Does it address it via the  <code>\/mnt\/dataset\/project1_data\/<\/code>  path? Or do you use symlinks to the projects directory?<br>\nI will load the data directly from NAS, codes below<\/p>\n<\/li>\n<\/ol>\n<pre><code class=\"lang-python\">cv2.imread(\"\/mnt\/dataset\/project1_data\/imag.png\")\n<\/code><\/pre>\n<p>I agree that it\u2019s beneficial to keep DVC cache in NAS, but I can\u2019t find an example in documentation.<\/p>\n<p>Thanks again for your sample code, I will try it <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/grinning.png?v=9\" title=\":grinning:\" class=\"emoji\" alt=\":grinning:\">!<\/p>",
                "Answer_score":124.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-06-10T11:03:15.960Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a>!<\/p>\n<p>Thanks for your code and it works perfectly in my case.<\/p>\n<p>Just two questions<\/p>\n<ol>\n<li>I have a parse_data.py, which will parse new data and save into <strong>\/mnt\/dataset\/project1_data\/<\/strong> daily. So I will get new images in project_data1 everyday.<br>\nBase on your code, you move all images into the copy project1_data<\/li>\n<\/ol>\n<pre><code class=\"lang-bash\">cd \/mnt\/dataset\/project1\nmv \/mnt\/dataset\/project1_data\/*.png  data\/\n<\/code><\/pre>\n<p>Can I simply revise the code in parse_data.py and save data directly into <strong>\/mnt\/dataset\/project1\/data\/<\/strong>?<\/p>\n<ol start=\"2\">\n<li>When adding new data, dvc will unprotect all data which cost lots of time (about 20 mins), is this inevitable?<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/fc5b556773d3f98121b7034cf376c2d3047c6b6d.png\" alt=\"image\" width=\"592\" height=\"147\">\n<\/li>\n<\/ol>",
                "Answer_score":113.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-06-11T04:45:39.208Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jimmy15923\">@jimmy15923<\/a> !<\/p>\n<aside class=\"quote no-group\" data-username=\"jimmy15923\" data-post=\"6\" data-topic=\"180\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/j\/8baadc\/40.png\" class=\"avatar\"> jimmy15923:<\/div>\n<blockquote>\n<p>When adding new data, dvc will unprotect all data which cost lots of time (about 20 mins), is this inevitable?<\/p>\n<\/blockquote>\n<\/aside>\n<p>That is strange. What dvc version are you using?e<\/p>",
                "Answer_score":18.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-06-11T05:00:02.456Z",
                "Answer_body":"<p>Hi!<br>\nI think it\u2019s the latest?<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/1cebe7f32817fdb6a22dad29a66ce9146d0f22d7.png\" alt=\"image\" width=\"463\" height=\"63\"><\/p>",
                "Answer_score":53.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-06-11T21:37:30.627Z",
                "Answer_body":"<p>Hi, <a class=\"mention\" href=\"\/u\/jimmy15923\">@jimmy15923<\/a>!<\/p>\n<ol>\n<li>Yes, absolutely! You can change your code and write directly to the data folder.<\/li>\n<\/ol>\n<p>(the concern is that we have seen that DVC hangs when you try to run it directly from NFS, let us know if you hit this issue - we\u2019ll figure out a workaround)<\/p>\n<ol start=\"2\">\n<li>Could you share a little bit more details please? Your config file (<code>.dvc\/config<\/code>) for the repo in which it is happening. And the sequence of commands\/steps to reproduce?<\/li>\n<\/ol>",
                "Answer_score":8.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-06-12T03:05:36.013Z",
                "Answer_body":"<p>Hi, <a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a>!<\/p>\n<p>Thanks for your reply. I had change the cache.protected = False, but still not works. Here is my config<\/p>\n<blockquote>\n<p>root@be9f4fa8f4b2:\/home\/jimmy15923\/dvc_project# cat .dvc\/config<br>\n[cache]<br>\ndir = \/mnt\/dataset\/dvc_exp\/<br>\ntype = symlink<br>\nprotected = false<\/p>\n<\/blockquote>\n<p>I follow your suggestion and save the new data directly into<\/p>\n<blockquote>\n<p>\/mnt\/dataset\/project1\/data  (which is copy from \/home\/user\/project1).<\/p>\n<\/blockquote>\n<p>Then I used<\/p>\n<blockquote>\n<p>dvc add data<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/4761ede07707cb9461224d031c8b99c73502b521.png\" alt=\"image\" data-base62-sha1=\"abtIbQn09i22xrID2nHiF4Yc4gh\" width=\"589\" height=\"419\"><\/p>\n<\/blockquote>\n<p>You can see that it is unprotecting all data! I also have two large files and it seems that it always re-computing md5 (not only done once). Did I set anything wrong? Many thanks for your support!<\/p>",
                "Answer_score":52.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-06-12T19:33:52.229Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jimmy15923\">@jimmy15923<\/a> !<\/p>\n<p>Indeed, when re-adding your data, dvc first unprotects it to be able to work on it. We should definitely act smarter about it. Could you please create an issue for that on out github?<\/p>\n<p>The checksum re-computation happens for that same reason. Since you are using symlinks, after <code>unprotect<\/code> that file has a different inode, so dvc is not able to find checksum for it in our state db and so has to re-compute it again. We could fix that by saving an entry to the state db when we are unprotecting something that we already know the checksum for. Please be sure to link our conversation here in the created github issue.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":227.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"setup work share data na server team thank provid wonder tool scenario server na storag project need us data store na storag updat size data grow dai mnt dataset project data model code list home user project project data take add mnt dataset project data take long shut run end question us data version control code control base na storag look share develop server clear ad data na thank",
        "Question_preprocessed_content":"setup work share data na server team thank provid wonder tool scenario server na storag project need us data store na storag updat size data grow dai model code list take add take long shut run end question us data version control code control base na storag look share develop server clear ad data na thank",
        "Question_gpt_summary_original":"The user is facing challenges in setting up DVC to work with shared data on a NAS server. The data stored on the NAS server is updated regularly, and the size of the data is increasing every day. When the user tried to add the project1_data to DVC, it took a long time, and the user had to shut it down before it completed. The user is seeking guidance on how to use DVC for data version control and code control based on NAS storage. The user has looked into the Shared Development Server but is unclear about adding data from NAS.",
        "Question_gpt_summary":"user face challeng set work share data na server data store na server updat regularli size data increas dai user tri add project data took long time user shut complet user seek guidanc us data version control code control base na storag user look share develop server unclear ad data na",
        "Answer_original_content":"jimmi happen run add mnt dataset project data try cach data store cach creat link mnt dataset project data mean cach version want specifi depend stage cach case simpli specifi extern depend run command like run mnt dataset project data thank ruslan kuprus thank repli mnt dataset project data updat dai want version abl checkout old version sorri clear specifi depend stage cach dont know case us sure read thread guid share cach network filesystem na nf issu iter org github make thing faster na nf jimmi littl bit context long take interrupt process mount na storag nf code written process data address mnt dataset project data path us symlink project directori look like case benefici cach place version dataset model store na avoid copi larg file na machin let know actual want copi data project workspac home user project init cach dir mnt dataset storag config cach type reflink symlink hardlink copi enabl symlink avoid copi config cach protect true link corrupt accident git add gitignor git commit initi add version dataset cach dataset follow mnt dataset home user project project mnt dataset project data data add data wai faster git add data gitignor git commit add version dataset git tag dataset git push origin head git push origin project like home user project git pull data file checkout data directori symlink na storag shcheklein appreci repli sorri notic todai question long take interrupt process min look like need lot time finish mount na storag nf nf server access na code written process data address mnt dataset project data path us symlink project directori load data directli na code imread mnt dataset project data imag png agre benefici cach na exampl document thank sampl code try shcheklein thank code work perfectli case question pars data pars new data save mnt dataset project data daili new imag project data everydai base code imag copi project data mnt dataset project mnt dataset project data png data simpli revis code pars data save data directli mnt dataset project data ad new data unprotect data cost lot time min inevit jimmi jimmi ad new data unprotect data cost lot time min inevit strang version think latest jimmi ye absolut chang code write directli data folder concern seen hang try run directli nf let know hit issu figur workaround share littl bit detail config file config repo happen sequenc command step reproduc shcheklein thank repli chang cach protect fals work config root beffafb home jimmi project cat config cach dir mnt dataset exp type symlink protect fals follow suggest save new data directli mnt dataset project data copi home user project add data unprotect data larg file comput set wrong thank support jimmi ad data unprotect abl work definit act smarter creat issu github checksum comput happen reason symlink unprotect file differ inod abl checksum state comput fix save entri state unprotect know checksum sure link convers creat github issu thank ruslan",
        "Answer_preprocessed_content":"happen run try cach data store creat link mean cach version want specifi depend stage cach case simpli specifi extern depend command like thank ruslan thank repli updat dai want version sorri clear specifi depend stage cach dont know case us sure read thread guid share cach network filesystem issu github make thing faster littl bit context long take interrupt process mount na storag nf code written process data address path us symlink project directori look like case benefici cach na avoid copi larg file na machin add version dataset cach follow project like appreci repli sorri notic todai question long take interrupt process min look like need lot time finish mount na storag nf nf server access na code written process data address path us symlink project directori load data directli na code agre benefici cach na exampl document thank sampl code try thank code work perfectli case question pars new data save daili new imag everydai base code imag copi simpli revis code save data directli ad new data unprotect data cost lot time inevit jimmi ad new data unprotect data cost lot time inevit strang version think latest ye absolut chang code write directli data folder concern seen hang try run directli nf let know hit issu figur workaround share littl bit detail config file repo happen sequenc reproduc thank repli chang fals work config cat cach dir type symlink protect fals follow suggest save new data directli add data unprotect data larg file set wrong thank support data unprotect abl work definit act smarter creat issu github checksum happen reason symlink file differ inod abl checksum state fix save entri state unprotect know checksum sure link convers creat github issu thank ruslan",
        "Answer_gpt_summary_original":"the answer provides a possible solution to the user's challenge of setting up a workflow to work with shared data on a nas server, including data version control and code control, while dealing with a large dataset that is constantly growing. the solution involves caching and versioning the data, specifying it as an external dependency in the run command, and keeping a cache on the nas to avoid copying large files from the nas to the machine. the answer also suggests moving all images into the copy project1_data and revising the code in parse_data.py to save data directly into \/mnt\/dataset\/project1\/data. additionally, the answer acknowledges the issue of unprotecting all data when adding new data and suggests creating an issue for that on github.",
        "Answer_gpt_summary":"answer provid possibl solut user challeng set workflow work share data na server includ data version control code control deal larg dataset constantli grow solut involv cach version data specifi extern depend run command keep cach na avoid copi larg file na machin answer suggest move imag copi project data revis code pars data save data directli mnt dataset project data addition answer acknowledg issu unprotect data ad new data suggest creat issu github"
    },
    {
        "Question_id":null,
        "Question_title":"Vertex pipeline model training component stuck running forever because of metadata issue",
        "Question_body":"'m attempting to run a Vertex pipeline (custom model training) which I was able to run successfully in a different project. As far as I'm aware, all the pieces of infrastructure (service accounts, buckets, etc.) are identical.The error appears in a gray box in the pipeline UI when I click on the model training component and reads the following:I've looked into the log explorer and found that the error logs are audit logs have the following associated tags with them:protoPayload.methodName=\"google.cloud.aiplatform.internal.MetadataService.RefreshLineageSubgraph\"protoPayload.resourceName=\"projects\/724306335858\/locations\/europe-west4\/metadataStores\/defaultLeading me to think that there's an issue with the Vertex Metadatastore or the way my pipeline is using it. The audit logs are automatic though, so I'm not sure.I've tried purging the metadata store as well as deleting it completely. I've also tried running a different model training pipeline that worked before in a different project as well but with no luck.",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1662690300000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":71.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-pipeline-model-training-component-stuck-running-forever\/td-p\/464631\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-09T02:25:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"'m attempting to run a Vertex pipeline (custom model training) which I was able to run successfully in a different project. As far as I'm aware, all the pieces of infrastructure (service accounts, buckets, etc.) are identical.\n\nThe error appears in a gray box in the pipeline UI when I click on the model training component and reads the following:\n\nRetryable error reported. System is retrying.\ncom.google.cloud.ai.platform.common.errors.AiPlatformException: code=ABORTED, message=Specified Execution `etag`: `1662555654045` does not match server `etag`: `1662555533339`, cause=null System is retrying.\n\nI've looked into the log explorer and found that the error logs are audit logs have the following associated tags with them:\n\nprotoPayload.methodName=\"google.cloud.aiplatform.internal.MetadataService.RefreshLineageSubgraph\"\n\nprotoPayload.resourceName=\"projects\/724306335858\/locations\/europe-west4\/metadataStores\/default\n\nLeading me to think that there's an issue with the Vertex Metadatastore or the way my pipeline is using it. The audit logs are automatic though, so I'm not sure.\n\nI've tried purging the metadata store as well as deleting it completely. I've also tried running a different model training pipeline that worked before in a different project as well but with no luck."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"vertex pipelin model train compon stuck run forev metadata issu attempt run vertex pipelin custom model train abl run successfulli differ project far awar piec infrastructur servic account bucket ident error appear grai box pipelin click model train compon read follow look log explor error log audit log follow associ tag protopayload methodnam googl cloud aiplatform intern metadataservic refreshlineagesubgraph protopayload resourcenam project locat europ west metadatastor defaultlead think issu vertex metadatastor wai pipelin audit log automat sure tri purg metadata store delet complet tri run differ model train pipelin work differ project luck",
        "Question_preprocessed_content":"vertex pipelin model train compon stuck run forev metadata issu attempt run vertex pipelin abl run successfulli differ project far awar piec infrastructur error appear grai box pipelin click model train compon read follow look log explor error log audit log follow associ tag think issu vertex metadatastor wai pipelin audit log automat tri purg metadata store delet complet tri run differ model train pipelin work differ project luck",
        "Question_gpt_summary_original":"The user is encountering challenges with a Vertex pipeline model training component that is stuck running forever due to a metadata issue. The error message suggests that there may be an issue with the Vertex Metadatastore or the way the pipeline is using it. The user has attempted to purge and delete the metadata store, as well as run a different model training pipeline, but has not been successful in resolving the issue.",
        "Question_gpt_summary":"user encount challeng vertex pipelin model train compon stuck run forev metadata issu error messag suggest issu vertex metadatastor wai pipelin user attempt purg delet metadata store run differ model train pipelin success resolv issu",
        "Answer_original_content":"attempt run vertex pipelin custom model train abl run successfulli differ project far awar piec infrastructur servic account bucket ident error appear grai box pipelin click model train compon read follow retryabl error report retri com googl cloud platform common error aiplatformexcept code abort messag specifi execut etag match server etag caus null retri look log explor error log audit log follow associ tag protopayload methodnam googl cloud aiplatform intern metadataservic refreshlineagesubgraph protopayload resourcenam project locat europ west metadatastor default lead think issu vertex metadatastor wai pipelin audit log automat sure tri purg metadata store delet complet tri run differ model train pipelin work differ project luck",
        "Answer_preprocessed_content":"attempt run vertex pipelin abl run successfulli differ project far awar piec infrastructur ident error appear grai box pipelin click model train compon read follow retryabl error report retri code abort messag specifi execut match server caus null retri look log explor error log audit log follow associ tag lead think issu vertex metadatastor wai pipelin audit log automat sure tri purg metadata store delet complet tri run differ model train pipelin work differ project luck",
        "Answer_gpt_summary_original":"possible solutions to the issue with the vertex pipeline model training component that is stuck running forever due to a metadata issue include purging or deleting the metadata store, checking the way the pipeline is using the metadata store, and running a different model training pipeline that worked before in a different project. however, it is unclear if any of these solutions have been successful.",
        "Answer_gpt_summary":"possibl solut issu vertex pipelin model train compon stuck run forev metadata issu includ purg delet metadata store check wai pipelin metadata store run differ model train pipelin work differ project unclear solut success"
    },
    {
        "Question_id":null,
        "Question_title":"is there a way to delete azureml runs using the python sdk?",
        "Question_body":"I was wondering if it was possible to delete particular runs using the Python SDK.\nthis would be rather useful to delete old failed runs.\nit already has functions such as cancel(), fail(), submit().",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638386026283,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@AntaraDas-4298 Thanks, Run history documents, which may contain personal user information, are stored in the storage account in blob storage, in subfolders of \/azureml. You can download and delete the data from the portal.\n\nHere is the document to delete workspace data.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data\n\nThere is a Private Preview for deleting an experiment, however such functionality does not delete the intermediate data generated for the run or any child run.\n\u2022 Not deleted:\no Files in azureml-blobstore-GUID\/azureml\/{run_id}\no Code snapshot (zip files)\no Pipeline intermediate data and child runs\no Metric data\n\n\u2022 Deleted\no The output folder content\no Log files",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/648089\/is-there-a-way-to-delete-azureml-runs-using-the-py.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-03T01:16:37.37Z",
                "Answer_score":0,
                "Answer_body":"@AntaraDas-4298 Thanks, Run history documents, which may contain personal user information, are stored in the storage account in blob storage, in subfolders of \/azureml. You can download and delete the data from the portal.\n\nHere is the document to delete workspace data.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-export-delete-data\n\nThere is a Private Preview for deleting an experiment, however such functionality does not delete the intermediate data generated for the run or any child run.\n\u2022 Not deleted:\no Files in azureml-blobstore-GUID\/azureml\/{run_id}\no Code snapshot (zip files)\no Pipeline intermediate data and child runs\no Metric data\n\n\u2022 Deleted\no The output folder content\no Log files",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1638494197368,
        "Question_original_content":"wai delet run python sdk wonder possibl delet particular run python sdk us delet old fail run function cancel fail submit",
        "Question_preprocessed_content":"wai delet run python sdk wonder possibl delet particular run python sdk us delet old fail run function cancel fail submit",
        "Question_gpt_summary_original":"The user is seeking information on whether it is possible to delete specific runs using the Python SDK in AzureML. They believe this would be helpful in removing old failed runs and note that the SDK already has functions for canceling, failing, and submitting runs.",
        "Question_gpt_summary":"user seek inform possibl delet specif run python sdk believ help remov old fail run note sdk function cancel fail submit run",
        "Answer_original_content":"antarada thank run histori document contain person user inform store storag account blob storag subfold download delet data portal document delet workspac data http doc microsoft com azur machin learn export delet data privat preview delet experi function delet intermedi data gener run child run delet file blobstor guid run code snapshot zip file pipelin intermedi data child run metric data delet output folder content log file",
        "Answer_preprocessed_content":"thank run histori document contain person user inform store storag account blob storag subfold download delet data portal document delet workspac data privat preview delet experi function delet intermedi data gener run child run delet file code snapshot pipelin intermedi data child run metric data delet output folder content log file",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. download and delete the run history documents from the storage account in blob storage using the portal.\n2. follow the document provided to delete workspace data.\n3. use the private preview for deleting an experiment, but note that it does not delete the intermediate data generated for the run or any child run.",
        "Answer_gpt_summary":"possibl solut answer download delet run histori document storag account blob storag portal follow document provid delet workspac data us privat preview delet experi note delet intermedi data gener run child run"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning service was configured with a Load Balancer & Premium SSD Managed Disks (Cost Issue)",
        "Question_body":"I create an Azure Machine Learning service and it was configured with a Load Balancer & Premium SSD Managed Disks. Is this configuration correct?\n\nI chose a Public Endpoint for the ML service. Is this the reason for the Load Balancer?\n\nIs there a way for me to change the \"Premium SSD Managed Disks\" to a lower-cost storage?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651101671863,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/829249\/azure-machine-learning-service-was-configured-with.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-28T10:42:07.783Z",
                "Answer_score":0,
                "Answer_body":"@TerrenceRideau-6599 The load balancer is a sub resource that is created when you create the compute instance or cluster. These resources are charged unless the compute instance or cluster is completely deleted. If the compute is in stopped state the resources might still accrue costs. Please check this section of the documentation for details. The endpoints should not be the reason for billing under load balancer. If you have any questions regarding billing you can always raise a azure support case from azure portal to seek clarification.\n\nThe storage that is created with the compute instances chosen from Azure ML portal ml.azure.com cannot be changed. If you would like to use a custom compute type with lower cost storage then you need to use attached compute which is actually created from Azure portal and then added to Azure ML compute(attached) to be used for your experiments. I hope this helps!!\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"servic configur load balanc premium ssd manag disk cost issu creat servic configur load balanc premium ssd manag disk configur correct chose public endpoint servic reason load balanc wai chang premium ssd manag disk lower cost storag",
        "Question_preprocessed_content":"servic configur load balanc premium ssd manag disk creat servic configur load balanc premium ssd manag disk configur correct chose public endpoint servic reason load balanc wai chang premium ssd manag disk storag",
        "Question_gpt_summary_original":"The user has encountered challenges related to the cost of their Azure Machine Learning service, which was configured with a Load Balancer and Premium SSD Managed Disks. They are unsure if this configuration is correct and are questioning if choosing a Public Endpoint for the ML service is the reason for the Load Balancer. Additionally, the user is seeking a way to change the Premium SSD Managed Disks to a lower-cost storage option.",
        "Question_gpt_summary":"user encount challeng relat cost servic configur load balanc premium ssd manag disk unsur configur correct question choos public endpoint servic reason load balanc addition user seek wai chang premium ssd manag disk lower cost storag option",
        "Answer_original_content":"terrencerideau load balanc sub resourc creat creat comput instanc cluster resourc charg comput instanc cluster complet delet comput stop state resourc accru cost check section document detail endpoint reason bill load balanc question bill rais azur support case azur portal seek clarif storag creat comput instanc chosen portal azur com chang like us custom comput type lower cost storag need us attach comput actual creat azur portal ad comput attach experi hope help answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"load balanc sub resourc creat creat comput instanc cluster resourc charg comput instanc cluster complet delet comput stop state resourc accru cost check section document detail endpoint reason bill load balanc question bill rais azur support case azur portal seek clarif storag creat comput instanc chosen portal chang like us custom comput type lower cost storag need us attach comput actual creat azur portal ad comput experi hope help answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- check the documentation to understand the cost implications of load balancers and stopped compute instances or clusters.\n- raise a support case from the azure portal to seek clarification on billing.\n- use attached compute with lower cost storage instead of the default compute instances chosen from portal ml.azure.com.",
        "Answer_gpt_summary":"possibl solut answer check document understand cost implic load balanc stop comput instanc cluster rais support case azur portal seek clarif bill us attach comput lower cost storag instead default comput instanc chosen portal azur com"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning - error during the creation Create a control script",
        "Question_body":"Hello, I am reproducing this tutorial https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world \/ Create a control script. The next observations appear in the console.\n\nI will thank you if some ideas are shared with me to face this issue.\n\nRegards",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643753509637,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@Anth0nyCamp0s I believe the error is because in the config command you are using the script parameter hello.py which is in .\/src directory but because you are already in .\/src directory on the terminal and the source_directory parameter also mentions to use .\/src as the path to the file the following error is indicated in the message.\n\n\n\n\nIf you navigate back to get-started directory in your terminal and run the script run-hello.py your experiment should be created successfully.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/718873\/azure-machine-learning-error-during-the-creation-c.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-02T08:04:16.33Z",
                "Answer_score":0,
                "Answer_body":"@Anth0nyCamp0s I believe the error is because in the config command you are using the script parameter hello.py which is in .\/src directory but because you are already in .\/src directory on the terminal and the source_directory parameter also mentions to use .\/src as the path to the file the following error is indicated in the message.\n\n\n\n\nIf you navigate back to get-started directory in your terminal and run the script run-hello.py your experiment should be created successfully.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643789056328,
        "Question_original_content":"error creation creat control script hello reproduc tutori http doc microsoft com azur machin learn tutori experi hello world creat control script observ appear consol thank idea share face issu regard",
        "Question_preprocessed_content":"error creation creat control script hello reproduc tutori creat control script observ appear consol thank idea share face issu regard",
        "Question_gpt_summary_original":"The user encountered an error while following a tutorial on creating a control script in Azure Machine Learning. They are seeking ideas to resolve the issue.",
        "Question_gpt_summary":"user encount error follow tutori creat control script seek idea resolv issu",
        "Answer_original_content":"anthnycamp believ error config command script paramet hello src directori src directori termin sourc directori paramet mention us src path file follow error indic messag navig start directori termin run script run hello experi creat successfulli answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"believ error config command script paramet directori directori termin paramet mention us path file follow error indic messag navig directori termin run script experi creat successfulli answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"the possible solution to the error encountered while creating a control script is to navigate back to the get-started directory in the terminal and run the script run-hello.py. this should create the experiment successfully.",
        "Answer_gpt_summary":"possibl solut error encount creat control script navig start directori termin run script run hello creat experi successfulli"
    },
    {
        "Question_id":null,
        "Question_title":"Guild doesn't copy module to new source code location",
        "Question_body":"<p>When I use the command <code>guild run train,py<\/code> I get the error <code>Error: can't important module_1<\/code>. When I look in the folder to which guildai copies the source code after I call guild run, I can see that <code>module_1<\/code> was indeed not copied. There is nothing special about that module_1 (normal code files). How can I debug this issue further?<\/p>\n<p>Here is my folder structure:<\/p>\n<p>Folder structure:<\/p>\n<pre><code>guild.yml\ntrain.py\nmodule_1\nmodule_2\nmodule_3\n<\/code><\/pre>\n<p>guild.yml<\/p>\n<pre><code>train:\n  description: Training script\n  main: train\n  # sourcecode:\n  #   - '*.py'\n  flags-dest: global:params\n  flags-import: all\n  flags:\n    ## general\n    gpu:\n      description:\n      default: 0\n    seed:\n      description:\n      default: 0\n...\n<\/code><\/pre>\n<p>Thanks for your help!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1619015850647,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":594.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-doesnt-copy-module-to-new-source-code-location\/690",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-21T17:12:16.681Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jan_r\">@jan_r<\/a>, I\u2019m sorry you\u2019re running into issues here.<\/p>\n<p>Guild should be picking up any Python modules from <code>module_*<\/code> dirs and copying them to the <code>.guild\/sourcecode<\/code> directory in the run. Note that Guild does not attempt to duplicate your project structure. Source code files do not appear in the run directory root, at least by default.<\/p>\n<p>You can view the list of source code files for a run this way:<\/p>\n<pre><code class=\"lang-command\">guild ls --sourcecode  # shows source code files for latest run\n<\/code><\/pre>\n<p>You can test the source code copy logic ahead of time for a run this way:<\/p>\n<pre><code class=\"lang-command\">guild run train --test-sourcecode\n<\/code><\/pre>\n<p>This gives you a detailed list of files that are copied and those that are skipped based on the <code>sourcecode<\/code> config for the operation.<\/p>\n<p>Note that <code>guild run train.py<\/code> runs the the script directly. In this case, because your operation is named <code>train<\/code> (no <code>.py<\/code> extension), Guild does not use the config in <code>guild.yml<\/code>. You need to use <code>guild run train<\/code>, which uses the operation name in <code>guild.yml<\/code>.<\/p>",
                "Answer_score":12.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-21T17:13:56.011Z",
                "Answer_body":"<p>One thought here \u2014 the <code>module_*<\/code> dirs must each have a <code>__init__.py<\/code> file to indicate they\u2019re a package. In my tests, when I removed that file I get the error message you show above.<\/p>",
                "Answer_score":2.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-24T11:32:06.669Z",
                "Answer_body":"<aside class=\"quote group-guildai_staff\" data-username=\"garrett\" data-post=\"2\" data-topic=\"690\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/garrett\/40\/123_2.png\" class=\"avatar\"> garrett:<\/div>\n<blockquote>\n<p><code>guild run train --test-sourcecode<\/code><\/p>\n<\/blockquote>\n<\/aside>\n<p>That\u2019s good advice thanks! Unfortunately, they relevant .py files in my module which are not copied don\u2019t appear in the list of skipped files nor in the list of copied files. The module has a <code>__init__.py<\/code> file so it\u2019s a proper module.<\/p>\n<p>Do you have any other tricks I could use for debugging?<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-26T13:36:40.448Z",
                "Answer_body":"<p>Are the Python source code files stored under a linked directory? What OS is this? Thanks!<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"copi modul new sourc code locat us command run train error error import modul look folder copi sourc code run modul copi special modul normal code file debug issu folder structur folder structur yml train modul modul modul yml train descript train script main train sourcecod flag dest global param flag import flag gener gpu descript default seed descript default thank help",
        "Question_preprocessed_content":"copi modul new sourc code locat us command error look folder copi sourc code run copi special debug issu folder structur folder structur yml thank help",
        "Question_gpt_summary_original":"The user is encountering an error when using the command \"guild run train.py\" and is unable to import \"module_1\". Upon checking the folder where guildai copies the source code, the user found that \"module_1\" was not copied. The user is seeking help to debug this issue further.",
        "Question_gpt_summary":"user encount error command run train unabl import modul check folder copi sourc code user modul copi user seek help debug issu",
        "Answer_original_content":"jan sorri your run issu pick python modul modul dir copi sourcecod directori run note attempt duplic project structur sourc code file appear run directori root default view list sourc code file run wai sourcecod show sourc code file latest run test sourc code copi logic ahead time run wai run train test sourcecod give detail list file copi skip base sourcecod config oper note run train run script directli case oper name train extens us config yml need us run train us oper yml thought modul dir init file indic theyr packag test remov file error messag garrett run train test sourcecod that good advic thank unfortun relev file modul copi dont appear list skip file list copi file modul init file proper modul trick us debug python sourc code file store link directori thank",
        "Answer_preprocessed_content":"sorri your run issu pick python modul dir copi directori run note attempt duplic project structur sourc code file appear run directori root default view list sourc code file run wai test sourc code copi logic ahead time run wai give detail list file copi skip base config oper note run script directli case oper name us config need us us oper thought dir file indic theyr packag test remov file error messag garrett that good advic thank unfortun relev file modul copi dont appear list skip file list copi file modul file proper modul trick us debug python sourc code file store link directori thank",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- check if the module_* directories have a __init__.py file to indicate they are a package.\n- use \"guild run train\" instead of \"guild run train.py\" to use the operation name in guild.yml.\n- test the source code copy logic ahead of time for a run using \"guild run train --test-sourcecode\".\n- view the list of source code files for a run using \"guild ls --sourcecode\".\n- check if the python source code files are stored under a linked directory.\n- debug the issue further by checking the skipped files and copied files list.",
        "Answer_gpt_summary":"possibl solut answer check modul directori init file indic packag us run train instead run train us oper yml test sourc code copi logic ahead time run run train test sourcecod view list sourc code file run sourcecod check python sourc code file store link directori debug issu check skip file copi file list"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Studio notebook - no module named 'tensorflow' when chosen image type \"Tensorflow 2.6 Python 3.8 GPU optimized\"",
        "Question_body":"Hi,\n\nI have created a simple notebook on SageMaker Studio using the Image \"Tensorflow 2.6 Python 3.8 GPU optimized\". But when I try to run simple statement viz. \"import tensorflow\", I am getting the error \"no module named 'tensorflow'\".\n\nI tried to install 'tensorflow' package using pip from the terminal attached to the image. But it shows the message \"requirement already satisfied\".\n\nAm I missing anything here? Please help.\n\nThanks in advance, Praveen",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640585801507,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":100.0,
        "Answer_body":"From the question, I understand that you are trying to use a TensorFlow 2.6 Python 3.8 kernel in SageMaker Studio, but you are unable to import tensorflow.\n\nThe service team are aware of this issue and are actively working on a fix.\n\nMitigation Option\n\nA) If your use case is version flexible, version other than 2.6 should work.\n\nB) If not, you can try the following as workaround\n\nOpen a notebook using a Tensorflow 2.6 Python 3.8 kernel\nExecute the following line in a notebook cell: !sed -i 's|^ *\"python\",| \"\/usr\/local\/bin\/python\",|g' \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json\nStop the kernel\nRe-attach the kernel to your notebook.\n\nHope it helps!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUN63fjMWsT5uVUNn2AIsmhw\/sagemaker-studio-notebook-no-module-named-tensorflow-when-chosen-image-type-tensorflow-2-6-python-3-8-gpu-optimized",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-28T19:55:32.675Z",
                "Answer_score":1,
                "Answer_body":"From the question, I understand that you are trying to use a TensorFlow 2.6 Python 3.8 kernel in SageMaker Studio, but you are unable to import tensorflow.\n\nThe service team are aware of this issue and are actively working on a fix.\n\nMitigation Option\n\nA) If your use case is version flexible, version other than 2.6 should work.\n\nB) If not, you can try the following as workaround\n\nOpen a notebook using a Tensorflow 2.6 Python 3.8 kernel\nExecute the following line in a notebook cell: !sed -i 's|^ *\"python\",| \"\/usr\/local\/bin\/python\",|g' \/usr\/local\/share\/jupyter\/kernels\/python3\/kernel.json\nStop the kernel\nRe-attach the kernel to your notebook.\n\nHope it helps!",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1640721332675,
        "Question_original_content":"studio notebook modul name tensorflow chosen imag type tensorflow python gpu optim creat simpl notebook studio imag tensorflow python gpu optim try run simpl statement viz import tensorflow get error modul name tensorflow tri instal tensorflow packag pip termin attach imag show messag requir satisfi miss help thank advanc praveen",
        "Question_preprocessed_content":"studio notebook modul name tensorflow chosen imag type tensorflow python gpu optim creat simpl notebook studio imag tensorflow python gpu optim try run simpl statement viz import tensorflow get error modul name tensorflow tri instal tensorflow packag pip termin attach imag show messag requir satisfi miss help thank advanc praveen",
        "Question_gpt_summary_original":"The user is encountering an error when trying to import the 'tensorflow' module in a SageMaker Studio notebook created using the \"Tensorflow 2.6 Python 3.8 GPU optimized\" image. The user has attempted to install the package using pip but receives a message stating that the requirement is already satisfied. The user is seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error try import tensorflow modul studio notebook creat tensorflow python gpu optim imag user attempt instal packag pip receiv messag state requir satisfi user seek assist resolv issu",
        "Answer_original_content":"question understand try us tensorflow python kernel studio unabl import tensorflow servic team awar issu activ work fix mitig option us case version flexibl version work try follow workaround open notebook tensorflow python kernel execut follow line notebook cell sed python usr local bin python usr local share jupyt kernel python kernel json stop kernel attach kernel notebook hope help",
        "Answer_preprocessed_content":"question understand try us tensorflow python kernel studio unabl import tensorflow servic team awar issu activ work fix mitig option us case version flexibl version work try follow workaround open notebook tensorflow python kernel execut follow line notebook cell sed python stop kernel kernel notebook hope help",
        "Answer_gpt_summary_original":"possible solutions to the issue of not being able to import tensorflow module in studio despite installation via terminal are: \n1. use a version of tensorflow other than 2.6 if your use case is version flexible.\n2. use a workaround by opening a notebook using a tensorflow 2.6 python 3.8 kernel, executing a specific line of code in a notebook cell, stopping the kernel, and re-attaching it to the notebook. the service team is aware of the issue and working on a fix.",
        "Answer_gpt_summary":"possibl solut issu abl import tensorflow modul studio despit instal termin us version tensorflow us case version flexibl us workaround open notebook tensorflow python kernel execut specif line code notebook cell stop kernel attach notebook servic team awar issu work fix"
    },
    {
        "Question_id":null,
        "Question_title":"Cloud Translation Permission",
        "Question_body":"So I'm pulling my hair out over this and reaching out here for help. I'm trying to set up a service account with Cloud Translation, and Text-to-speech enabled, but we keep getting this response:I have confirmed that the service account has the \"cloudtranslate.generalModels.predict\" permission, and showing the \"Cloud Translation API User\" role. We've also confirmed that it works with a different Service account that my colleague set up in his personal Google console profile. But, we need this setup with an account through our org. I did verify that the service account has the permission from the https:\/\/console.cloud.google.com\/iam-admin\/troubleshooter so and that my organization's admin sees that the service account is granted access through ancestor policies.  So what else can we check? ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1668497880000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Answer_body":"Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied.\u00a0\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Translation-Permission\/td-p\/489632\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-15T08:30:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied.\u00a0\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2022-11-15T08:30:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1668501000000,
        "Question_original_content":"cloud translat permiss pull hair reach help try set servic account cloud translat text speech enabl get respons confirm servic account cloudtransl generalmodel predict permiss show cloud translat api user role confirm work differ servic account colleagu set person googl consol profil need setup account org verifi servic account permiss http consol cloud googl com iam admin troubleshoot organ admin see servic account grant access ancestor polici check",
        "Question_preprocessed_content":"cloud translat permiss pull hair reach help try set servic account cloud translat enabl get respons confirm servic account permiss show cloud translat api user role confirm work differ servic account colleagu set person googl consol profil need setup account org verifi servic account permiss organ admin see servic account grant access ancestor polici check",
        "Question_gpt_summary_original":"The user is facing challenges while setting up a service account with Cloud Translation and Text-to-speech enabled. Despite having the necessary permissions and roles, the user is receiving an error message. The user has confirmed that the service account works with a different account but needs to set it up with an account through their organization. The user has verified that the service account has the required permission and access through ancestor policies. The user is seeking help to identify the issue.",
        "Question_gpt_summary":"user face challeng set servic account cloud translat text speech enabl despit have necessari permiss role user receiv error messag user confirm servic account work differ account need set account organ user verifi servic account requir permiss access ancestor polici user seek help identifi issu",
        "Answer_original_content":"turn hard code valu resourc locat set wrong project cours come permiss deni view solut origin post",
        "Answer_preprocessed_content":"turn valu resourc locat set wrong project cours come permiss deni view solut origin post",
        "Answer_gpt_summary_original":"the solution to the difficulty in setting up a service account with cloud translation and text-to-speech enabled was to check the hard-coded value for resource location and ensure that it is set to the correct project.",
        "Answer_gpt_summary":"solut difficulti set servic account cloud translat text speech enabl check hard code valu resourc locat ensur set correct project"
    },
    {
        "Question_id":66814885.0,
        "Question_title":"Serve online learning models with mlflow",
        "Question_body":"<p>It is not clear to me if one could use mlflow to serve a model that is evolving continuously based on its previous predictions.<\/p>\n<p>I need to be able to query a model in order to make a prediction on a sample of data which is the basic use of mlflow serve. However I also want the model to be updated internaly now that it has seen new data.<\/p>\n<p>Is it possible or does it need a FR ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616753577110,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":360.0,
        "Answer_body":"<p>I think that you should be able to do that by implementing the custom python model or custom flavor, as it's described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">documentation<\/a>.  In this case you need to create a class that is inherited from <code>mlflow.pyfunc.PythonModel<\/code>, and implement the <code>predict<\/code> method, and inside that method you're free to do anything.  Here is just simple example from documentation:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class AddN(mlflow.pyfunc.PythonModel):\n\n    def __init__(self, n):\n        self.n = n\n\n    def predict(self, context, model_input):\n        return model_input.apply(lambda column: column + self.n)\n<\/code><\/pre>\n<p>and this model is then could be saved &amp; loaded again just as normal models:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Construct and save the model\nmodel_path = &quot;add_n_model&quot;\nadd5_model = AddN(n=5)\nmlflow.pyfunc.save_model(path=model_path, python_model=add5_model)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66814885",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618124640630,
        "Question_original_content":"serv onlin learn model clear us serv model evolv continu base previou predict need abl queri model order predict sampl data basic us serv want model updat internali seen new data possibl need",
        "Question_preprocessed_content":"serv onlin learn model clear us serv model evolv continu base previou predict need abl queri model order predict sampl data basic us serv want model updat internali seen new data possibl need",
        "Question_gpt_summary_original":"The user is unsure if mlflow can be used to continuously update a model based on its previous predictions while still being able to query the model for predictions on new data. They are seeking clarification on whether this is possible or if it requires a feature request.",
        "Question_gpt_summary":"user unsur continu updat model base previou predict abl queri model predict new data seek clarif possibl requir featur request",
        "Answer_original_content":"think abl implement custom python model custom flavor describ document case need creat class inherit pyfunc pythonmodel implement predict method insid method free simpl exampl document class addn pyfunc pythonmodel def init self self def predict self context model input return model input appli lambda column column self model save load normal model construct save model model path add model add model addn pyfunc save model path model path python model add model load model python function format load model pyfunc load model model path",
        "Answer_preprocessed_content":"think abl implement custom python model custom flavor describ document case need creat class inherit implement method insid method free simpl exampl document model save load normal model",
        "Answer_gpt_summary_original":"possible solutions to the user's question are to implement a custom python model or custom flavor by creating a class inherited from .pyfunc.pythonmodel and implementing the predict method. inside the predict method, the user is free to do anything. the model can be saved and loaded again just like normal models. the answer provides a simple example from the documentation to illustrate this.",
        "Answer_gpt_summary":"possibl solut user question implement custom python model custom flavor creat class inherit pyfunc pythonmodel implement predict method insid predict method user free model save load like normal model answer provid simpl exampl document illustr"
    },
    {
        "Question_id":null,
        "Question_title":"How to serve a model in sagemaker?",
        "Question_body":"based on documentation provided here , https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure, the model file saved from training is model.pth. I also read that it can be .pt extension or even bin extension. I have seen a example of pytorch_model.bin, but when i tried to serve the model with the pytorch_model.bin, it warns me that .pt or .pth file needs to exist. has anyone run into this?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645625592189,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":102.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUmmN8fILJTqyJz8b0-MqoXw\/how-to-serve-a-model-in-sagemaker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-23T14:42:36.693Z",
                "Answer_score":1,
                "Answer_body":"Depending on the version of the PyTorch framework container that you're using (since they go back quite a long way now), there may be some differences in what formats the default model loader expects & supports.\n\nIn general, if you're not finding that the default loader works for you, you should also be able to provide a model_fn() implementation (usually in code\/inference.py inside your model.tar.gz, although again the exact specifics here may vary if you're using older versions) to override the behaviour: Loading the model how you want and returning it from the function.\n\nIf you have issues getting your model_fn recognised:\n\nI believe there was an issue affecting specific framework images 1.6.0, 1.7.0 and 1.8.0 where a custom model_fn may be ignored. While the major and minor versions of these framework containers correspond to PyTorch versions, the final build number is for tracking AWS DLC changes... So if you're currently pinning to these specific versions e.g. 1.6.0 and finding this issue, you may find upgrading to a later build or just specifying 1.6 would help.\n\nAlso, when deploying via PyTorchModel you can specify a new source_dir (containing your inference.py) and a new model.tar.gz will be created to add in this code... But when deploying directly from estimator.deploy(), you may need to make sure your training job sets everything up by:\n\nCopying\/creating the required code file(s) into ${SM_MODEL_DIR}\/code (to add the inference code to model.tar.gz)\n(In some versions) making sure your original training script name (e.g. train.py) is also a valid entrypoint under ${SM_MODEL_DIR}\/code and exposes your required functions. This is because in some cases the entrypoint seems to get carried over from training, instead of defaulting to inference.py.\n\nMy usual (lazy) solution to this is to make my PyTorch training jobs recursively copy the whole src folder contents from training into ${SM_MODEL_DIR}\/code (as shown here), and to from inference import * in my training entrypoint script (as shown here). I think I've seen this approach work for at least v1.4-1.8, but it's been a while now since I used the older end of that range.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"serv model base document provid http readthedoc stabl framework pytorch pytorch html model directori structur model file save train model pth read extens bin extens seen exampl pytorch model bin tri serv model pytorch model bin warn pth file need exist run",
        "Question_preprocessed_content":"serv model base document provid model file save train read extens bin extens seen exampl tri serv model warn pth file need exist run",
        "Question_gpt_summary_original":"The user is facing challenges while trying to serve a model in Sagemaker. They have encountered an issue where the model file saved from training is in a different format than what is required for serving. Specifically, they have tried to use a pytorch_model.bin file but have been warned that a .pt or .pth file needs to exist. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user face challeng try serv model encount issu model file save train differ format requir serv specif tri us pytorch model bin file warn pth file need exist user seek advic resolv issu",
        "Answer_original_content":"depend version pytorch framework contain long wai differ format default model loader expect support gener find default loader work abl provid model implement usual code infer insid model tar exact specif vari older version overrid behaviour load model want return function issu get model recognis believ issu affect specif framework imag custom model ignor major minor version framework contain correspond pytorch version final build number track aw dlc chang current pin specif version find issu upgrad later build specifi help deploi pytorchmodel specifi new sourc dir contain infer new model tar creat add code deploi directli estim deploi need sure train job set copi creat requir code file model dir code add infer code model tar version make sure origin train script train valid entrypoint model dir code expos requir function case entrypoint carri train instead default infer usual lazi solut pytorch train job recurs copi src folder content train model dir code shown infer import train entrypoint script shown think seen approach work older end rang",
        "Answer_preprocessed_content":"depend version pytorch framework contain differ format default model loader expect support gener find default loader work abl provid implement overrid behaviour load model want return function issu get recognis believ issu affect specif framework imag custom ignor major minor version framework contain correspond pytorch version final build number track aw dlc current pin specif version find issu upgrad later build specifi help deploi pytorchmodel specifi new new creat add deploi directli need sure train job set requir code file version make sure origin train script valid entrypoint expos requir function case entrypoint carri train instead default usual solut pytorch train job recurs copi src folder content train infer import train entrypoint script think seen approach work older end rang",
        "Answer_gpt_summary_original":"possible solutions from the answer include: providing a custom model_fn() implementation to override the default loader behavior, upgrading to a later build or specifying a specific version of the framework container, specifying a new source_dir when deploying via pytorchmodel, copying\/creating the required code file(s) into ${sm_model_dir}\/code, making sure the original training script name is a valid entrypoint under ${sm_model_dir}\/code, and recursively copying the whole src folder contents from training into ${sm_model_dir}\/code.",
        "Answer_gpt_summary":"possibl solut answer includ provid custom model implement overrid default loader behavior upgrad later build specifi specif version framework contain specifi new sourc dir deploi pytorchmodel copi creat requir code file model dir code make sure origin train script valid entrypoint model dir code recurs copi src folder content train model dir code"
    },
    {
        "Question_id":73251212.0,
        "Question_title":"How do I retrieve a model in Vertex AI?",
        "Question_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1659709186783,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":45.0,
        "Answer_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659712728223,
        "Question_original_content":"retriev model defin train job job aiplatform automltexttrainingjob creat model run job model job run work fine dai variabl model jupyt notebook longer exist tri googl cloud import aiplatform vbeta def sampl model client aiplatform vbeta modelservicecli model train pipelin project project locat region model model request aiplatform vbeta getmodelrequest respons client model request request print respons sampl model tri model creat place train pipelin tri pipelin pipelin hpack parser error pars metadata error invalid valu kei content type valu text html charset utf project region set correctli",
        "Question_preprocessed_content":"retriev model defin train job creat model run job work fine dai variabl jupyt notebook longer exist tri tri model creat place tri set correctli",
        "Question_gpt_summary_original":"The user is facing a challenge in retrieving a model in Vertex AI that was created by running a training job in a Jupyter notebook. The user has tried to retrieve the model using the id of the training pipeline and the v1 id of the model, but is encountering an error related to parsing metadata.",
        "Question_gpt_summary":"user face challeng retriev model creat run train job jupyt notebook user tri retriev model train pipelin model encount error relat pars metadata",
        "Answer_original_content":"googl code work",
        "Answer_preprocessed_content":"googl code work",
        "Answer_gpt_summary_original":"possible solution: the answer suggests that the user can try using a google code that has been found to work in retrieving the model created in the jupyter notebook the previous day.",
        "Answer_gpt_summary":"possibl solut answer suggest user try googl code work retriev model creat jupyt notebook previou dai"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb.watch with pytorch not logging anything",
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying to use <code>wandb.watch<\/code> for a pytorch model, unfortunately without success. I checked the documentation and these two threads:<\/p>\n<ul>\n<li>Wandb.watch not logging parameters<\/li>\n<li>When is one supposed to run wandb.watch so that weights and biases tracks params and gradients?<\/li>\n<\/ul>\n<p>But none of the suggested solutions solves my problem. I run in my environment the code from the colab notebook linked in <a href=\"https:\/\/community.wandb.ai\/t\/when-is-one-supposed-to-run-wandb-watch-so-that-weights-and-biases-tracks-params-and-gradients\/518\/3\">this post<\/a> (with <code>N, log_freq = 50, 2<\/code>) and still nothing is logged.<\/p>\n<p>Interestingly, if I set the <code>log_graph=True<\/code> there is a JSON file logged as a file, under <code>root \/ media \/ graph<\/code> in the files section. But I was expecting to get a result similar to <a href=\"https:\/\/wandb.ai\/ayush-thakur\/debug-neural-nets\/runs\/jh061uaf\/model\">this<\/a>.<\/p>\n<p>I am using wandb version 0.12.10.<\/p>\n<p>Kind regards,<br>\nMaciej<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1647450501888,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":1152.0,
        "Answer_body":"<p>Hi,<\/p>\n<p>Eureka! Everything was working correctly, but I always use <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> with project view or run groups view. When I opened the run view both the graph and gradient were there <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>However, there is one problem remaining: <code>parameters<\/code>. When running the colab notebook code with <code>wandb.watch(d, log_freq=log_freq, log=\"all\")<\/code> I still can see only gradients in the run view.<\/p>\n<p><a href=\"https:\/\/wandb.ai\/dmml-heg\/uncategorized\/runs\/2qovzwq9\">Link to run page<\/a>  executed with wandb version 0.12.11 in Google Colab.<\/p>\n<p>EDIT: I found it <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Code in the notebook was using <code>forward()<\/code> instead of <code>__call__()<\/code>. Forward hooks were not executed.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-watch-with-pytorch-not-logging-anything\/2096",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-17T15:45:11.862Z",
                "Answer_body":"<p>Hi Maciej,<\/p>\n<p>Thank you for writing in and for doing as much research on your own as you can for this issue.<\/p>\n<p>In the colab that you had mentioned with the changed parameters that you used, I as able to log parameters properly. Here\u2019s the run that I used with <code>N, log_freq = 50, 2<\/code>. Can you send me a link of the run page where you are experiencing this issue? Also, can you try rerunning this using CLI-0.12.11 and see if that is able to help?<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
                "Answer_score":148.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-18T08:58:43.663Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>Eureka! Everything was working correctly, but I always use <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> with project view or run groups view. When I opened the run view both the graph and gradient were there <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>However, there is one problem remaining: <code>parameters<\/code>. When running the colab notebook code with <code>wandb.watch(d, log_freq=log_freq, log=\"all\")<\/code> I still can see only gradients in the run view.<\/p>\n<p><a href=\"https:\/\/wandb.ai\/dmml-heg\/uncategorized\/runs\/2qovzwq9\">Link to run page<\/a>  executed with wandb version 0.12.11 in Google Colab.<\/p>\n<p>EDIT: I found it <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> Code in the notebook was using <code>forward()<\/code> instead of <code>__call__()<\/code>. Forward hooks were not executed.<\/p>",
                "Answer_score":153.6,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-03-21T15:20:24.555Z",
                "Answer_body":"<p>Yay! That\u2019s great! I\u2019m glad you were able to find out what was wrong with your script <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> I hope you have a beautiful week<\/p>",
                "Answer_score":23.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-17T08:58:44.618Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":13.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1647593923663,
        "Question_original_content":"watch pytorch log try us watch pytorch model unfortun success check document thread watch log paramet suppos run watch track param gradient suggest solut solv problem run environ code colab notebook link post log freq log interestingli set log graph true json file log file root media graph file section expect result similar version kind regard maciej",
        "Question_preprocessed_content":"watch pytorch log try us pytorch model unfortun success check document thread watch log paramet suppos run watch track param gradient suggest solut solv problem run environ code colab notebook link post log interestingli set json file log file file section expect result similar version kind regard maciej",
        "Question_gpt_summary_original":"The user is facing challenges with using wandb.watch for a pytorch model as nothing is being logged. The user has checked the documentation and other threads but none of the suggested solutions have worked. The user has tried running the code from a colab notebook but still nothing is being logged. However, if the user sets log_graph=True, a JSON file is logged as a file, but the user was expecting a result similar to a provided link. The user is using wandb version 0.12.10.",
        "Question_gpt_summary":"user face challeng watch pytorch model log user check document thread suggest solut work user tri run code colab notebook log user set log graph true json file log file user expect result similar provid link user version",
        "Answer_original_content":"eureka work correctli us project view run group view open run view graph gradient problem remain paramet run colab notebook code watch log freq log freq log gradient run view link run page execut version googl colab edit code notebook forward instead forward hook execut",
        "Answer_preprocessed_content":"eureka work correctli us project view run group view open run view graph gradient problem remain run colab notebook code gradient run view link run page execut version googl colab edit code notebook instead forward hook execut",
        "Answer_gpt_summary_original":"the solution to the problem of not being able to log anything using .watch for a pytorch model is to use the run view instead of the project view or run groups view. however, there is still an issue with only being able to see gradients and not parameters. the problem was found to be that the code in the notebook was using forward() instead of __call__(), which caused forward hooks to not be executed.",
        "Answer_gpt_summary":"solut problem abl log watch pytorch model us run view instead project view run group view issu abl gradient paramet problem code notebook forward instead caus forward hook execut"
    },
    {
        "Question_id":73781018.0,
        "Question_title":"How to pass region to the SKLearnProcessor - botocore.exceptions.NoRegionError: You must specify a region",
        "Question_body":"<p>I'm using the following code to do a sklearn transformation job in sagemaker:<\/p>\n<pre><code>region = boto3.session.Session().region_name\nrole = sagemaker.get_execution_role()\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;1.0-1&quot;, role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;, instance_count=1,\n    # sagemaker_session = Session()\n)\nout_path = os.path.join(bucket, prefix, f'test_transform\/data.csv')\nsklearn_processor.run(\n    code=&quot;preprocess.py&quot;,\n    inputs = [\n        ProcessingInput(source = 'my_package\/', destination = '\/opt\/ml\/processing\/input\/code\/my_package\/')\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;test_transform_data&quot;, \n                         source = '\/opt\/ml\/processing\/output\/test_transform',\n                         destination = out_path),\n    ],\n    arguments=[&quot;--time-slot-minutes&quot;, &quot;30min&quot;]\n)\n<\/code><\/pre>\n<p>Within the above code, it's running preprocess.py, and (within) preprocess.py loads the data from snowflake database using the credentials saved in aws secretsmanager:<\/p>\n<pre><code>region = boto3.Session().region_name\nsecrets_client = boto3.client(service_name='secretsmanager', region_name=region)\n<\/code><\/pre>\n<p>So here's where error happen: first line above returns region as None, so the the second line of code raises <code>botocore.exceptions.NoRegionError: You must specify a region<\/code><\/p>\n<p>In this case, how can I pass the region to SKLearnProcessor or is there any other way to make the code working within the processing job instance?<\/p>\n<p>FYI:\nthe source of input <code>'my_package\/'<\/code> is in the structure below to install packages and include py dependencies used in <code>preprocess.py<\/code><\/p>\n<pre><code>\u251c\u2500\u2500 my_package\n\u2502   \u251c\u2500\u2500 file1.py\n\u2502   \u251c\u2500\u2500 file2.py\n\u2502   \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocess.py\n<\/code><\/pre>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663640822130,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":20.0,
        "Answer_body":"<p>set following in the code preprocess.py solved the issue:<\/p>\n<pre><code>os.environ['AWS_DEFAULT_REGION'] = 'us-west-2' \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73781018",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663678383900,
        "Question_original_content":"pass region sklearnprocessor botocor except noregionerror specifi region follow code sklearn transform job region boto session session region role execut role sklearn processor sklearnprocessor framework version role role instanc type xlarg instanc count session session path path join bucket prefix test transform data csv sklearn processor run code preprocess input processinginput sourc packag destin opt process input code packag output processingoutput output test transform data sourc opt process output test transform destin path argument time slot minut min code run preprocess preprocess load data snowflak databas credenti save aw secretsmanag region boto session region secret client boto client servic secretsmanag region region error happen line return region second line code rais botocor except noregionerror specifi region case pass region sklearnprocessor wai code work process job instanc fyi sourc input packag structur instal packag includ depend preprocess packag file file requir txt preprocess thank",
        "Question_preprocessed_content":"pass region sklearnprocessor specifi region follow code sklearn transform job code run load data snowflak databas credenti save aw secretsmanag error happen line return region second line code rais case pass region sklearnprocessor wai code work process job instanc fyi sourc input structur instal packag includ depend thank",
        "Question_gpt_summary_original":"The user is encountering an error while running a sklearn transformation job in Sagemaker. The error is caused by the region being returned as None, which is required to load data from a Snowflake database using AWS Secrets Manager. The user is seeking a solution to pass the region to SKLearnProcessor or find an alternative way to make the code work within the processing job instance.",
        "Question_gpt_summary":"user encount error run sklearn transform job error caus region return requir load data snowflak databas aw secret manag user seek solut pass region sklearnprocessor altern wai code work process job instanc",
        "Answer_original_content":"set follow code preprocess solv issu environ aw default region west",
        "Answer_preprocessed_content":"set follow code solv issu",
        "Answer_gpt_summary_original":"to solve the botocore.exceptions.noregionerror encountered when using the sklearnprocessor to run a preprocess.py script that loads data from a snowflake database using aws secrets manager credentials, the user can set the 'aws_default_region' environment variable to 'us-west-2' in the code of the preprocess.py script.",
        "Answer_gpt_summary":"solv botocor except noregionerror encount sklearnprocessor run preprocess script load data snowflak databas aw secret manag credenti user set aw default region environ variabl west code preprocess script"
    },
    {
        "Question_id":null,
        "Question_title":"How to checkpoint SageMaker model artifact during a training job?",
        "Question_body":"Hi,\n\nIs there a way to regularly checkpoint model artifact in a SageMaker training job for BYO training container?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1586331915000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":182.0,
        "Answer_body":"If you specify a checkpoint configuration (regardless of managed spot training) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n\"CheckpointConfig\": { \n  \"LocalPath\": \"string\",\n  \"S3Uri\": \"string\"\n}\n\n\nThe local path defaults to \/opt\/ml\/checkpoints\/, and then you specify the target path in S3 with S3Uri.\n\nGiven this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, sagemaker-containers implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\nNote: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info here.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrXX2MIygS5igas27GrAhHw\/how-to-checkpoint-sage-maker-model-artifact-during-a-training-job",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-04-08T15:22:36.000Z",
                "Answer_score":0,
                "Answer_body":"If you specify a checkpoint configuration (regardless of managed spot training) when starting a training job, checkpointing will work. You can provide a local path and S3 path as follows (API reference):\n\n\"CheckpointConfig\": { \n  \"LocalPath\": \"string\",\n  \"S3Uri\": \"string\"\n}\n\n\nThe local path defaults to \/opt\/ml\/checkpoints\/, and then you specify the target path in S3 with S3Uri.\n\nGiven this configuration, SageMaker will configure an output channel with Continuous upload mode to Amazon S3. At the time being, this results in running an agent on the hosts that watches the file system and continuously uploads data to Amazon S3. Similar behavior is applied when debugging is enabled, for delivering tensor data to Amazon S3.\n\nAs commented, sagemaker-containers implements its own code to save intermediate outputs and watching files on the file system, but I would rather rely on the functionality offered by the service to avoid dependencies on specific libraries where possible.\n\nNote: when using SageMaker Processing, which in my view can be considered an abstraction over training or, from another perspective, the foundation for training, you can configure an output channel to use continuous upload mode; further info here.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1586359356000,
        "Question_original_content":"checkpoint model artifact train job wai regularli checkpoint model artifact train job byo train contain",
        "Question_preprocessed_content":"checkpoint model artifact train job wai regularli checkpoint model artifact train job byo train contain",
        "Question_gpt_summary_original":"The user is seeking information on how to regularly checkpoint model artifacts during a SageMaker training job for a BYO training container.",
        "Question_gpt_summary":"user seek inform regularli checkpoint model artifact train job byo train contain",
        "Answer_original_content":"specifi checkpoint configur regardless manag spot train start train job checkpoint work provid local path path follow api refer checkpointconfig localpath string suri string local path default opt checkpoint specifi target path suri given configur configur output channel continu upload mode amazon time result run agent host watch file continu upload data amazon similar behavior appli debug enabl deliv tensor data amazon comment contain implement code save intermedi output watch file file reli function offer servic avoid depend specif librari possibl note process view consid abstract train perspect foundat train configur output channel us continu upload mode info",
        "Answer_preprocessed_content":"specifi checkpoint configur start train job checkpoint work provid local path path follow checkpointconfig local path default specifi target path uri given configur configur output channel continu upload mode amazon time result run agent host watch file continu upload data amazon similar behavior appli debug enabl deliv tensor data amazon comment contain implement code save intermedi output watch file file reli function offer servic avoid depend specif librari possibl note process view consid abstract train perspect foundat train configur output channel us continu upload mode info",
        "Answer_gpt_summary_original":"the solution to regularly checkpoint model artifacts during a training job using a byo training container is to specify a checkpoint configuration when starting a training job. this can be done by providing a local path and s3 path. the output channel can be configured with continuous upload mode to amazon s3, which will result in running an agent on the hosts that watches the file system and continuously uploads data to amazon s3. when using processing, an output channel can be configured to use continuous upload mode.",
        "Answer_gpt_summary":"solut regularli checkpoint model artifact train job byo train contain specifi checkpoint configur start train job provid local path path output channel configur continu upload mode amazon result run agent host watch file continu upload data amazon process output channel configur us continu upload mode"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to filter on Date?",
        "Question_body":"There is a Date column in the run's field. Can I do filtering on this field? I haven't find a way to do this.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1563493900000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":5.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/EKDhKa3UrbM",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-07-19T08:15:20",
                "Answer_body":"Hello,\n\n\nUnfortunately, we don\u2019t support filtering on that date right now. The date is actually the start_time time stamp converted into a date in the UI, and I believe the MLflow docs section on Search has a note that start_time isn\u2019t supported in the filter string.\u00a0\n\n\nWould you mind telling us about the use case you have where filtering based on date is really important? If many users have this use case we can probably prioritize adding that feature. Or you can make the change and submit a PR for it!\u00a0\n\n\nBest,\n- Max\n\n\nOn Thu, Jul 18, 2019 at 23:51 Cheng Li <scrapped...@gmail.com> wrote:\n\nThere is a Date column in the run's field. Can I do filtering on this field? I haven't find a way to do this.\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/b1857de5-d645-430d-871f-c6f0172fe52f%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"possibl filter date date column run field filter field haven wai",
        "Question_preprocessed_content":"possibl filter date date column run field filter field haven wai",
        "Question_gpt_summary_original":"The user is facing a challenge in filtering data based on the Date column in the run's field and is unable to find a way to do so.",
        "Question_gpt_summary":"user face challeng filter data base date column run field unabl wai",
        "Answer_original_content":"hello unfortun dont support filter date right date actual start time time stamp convert date believ doc section search note start time isnt support filter string mind tell us case filter base date import user us case probabl priorit ad featur chang submit best max thu jul cheng wrote date column run field filter field haven wai receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user bde cffef googlegroup com",
        "Answer_preprocessed_content":"hello unfortun dont support filter date right date actual time stamp convert date believ doc section search note isnt support filter string mind tell us case filter base date import user us case probabl priorit ad featur chang submit best max thu jul cheng wrote date column run field filter field haven wai receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Solution: No solution provided. The user is unable to filter the database date column based on the run's field. The support team mentions that they do not support filtering on that date and asks about the use case where filtering based on date is really important. They also suggest that the user can make the change and submit a PR for it.",
        "Answer_gpt_summary":"solut solut provid user unabl filter databas date column base run field support team mention support filter date ask us case filter base date import suggest user chang submit"
    },
    {
        "Question_id":null,
        "Question_title":"BUG: Sweep in Jupyter makes new runs impossible to start",
        "Question_body":"<p>Let\u2019s say I\u2019m working on a jupyter notebook and I run a sweep<\/p>\n<pre><code class=\"lang-auto\">sweep_id = wandb.sweep(sweep_config, entity=WANDB_ENTITY, project=WANDB_PROJECT, )\nwandb_agent = wandb.agent(sweep_id, project=WANDB_PROJECT, function=pipeline)\n<\/code><\/pre>\n<p>After an hour I\u2019m happy with the sweep results and I stop execution, not here\u2019s the bug, I CANNOT start a new run manually. Here\u2019s what happens when I run the following code<\/p>\n<blockquote>\n<p>run = wandb.init()<br>\nprint(\u201c------ RUN NAME ------\u201d, run.name)<br>\nrun.finish()<\/p>\n<p>run = wandb.init()<br>\nprint(\u201c------ RUN NAME ------\u201d, run.name)<br>\nrun.finish()<\/p>\n<\/blockquote>\n<p>Output:<\/p>\n<pre><code class=\"lang-auto\">\nChanges to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to the W&amp;B docs.\nFinishing last run (ID:ln732mvm) before initializing another...\nWaiting for W&amp;B process to finish... (success).\nSynced fresh-sweep-32: https:\/\/wandb.ai\/arkareem\/test\/runs\/ln732mvm\nSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nFind logs at: .\/wandb\/run-20220622_111904-ln732mvm\/logs\nSuccessfully finished last run (ID:ln732mvm). Initializing new run:\nTracking run with wandb version 0.12.18\nRun data is saved locally in \/mnt\/m\/MyFiles\/Classes\/wandb\/run-20220622_112014-ln732mvm\nSyncing run fresh-sweep-32 to Weights &amp; Biases (docs)\nSweep page: https:\/\/wandb.ai\/arkareem\/test\/sweeps\/9t3sbtv5\n------ RUN NAME ------ fresh-sweep-32\nWaiting for W&amp;B process to finish... (success).\nSynced fresh-sweep-32: https:\/\/wandb.ai\/arkareem\/test\/runs\/ln732mvm\nSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nFind logs at: .\/wandb\/run-20220622_112014-ln732mvm\/logs\nChanges to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to the W&amp;B docs.\nTracking run with wandb version 0.12.18\nRun data is saved locally in \/mnt\/m\/MyFiles\/Classes\/wandb\/run-20220622_112037-ln732mvm\nSyncing run fresh-sweep-32 to Weights &amp; Biases (docs)\nSweep page: https:\/\/wandb.ai\/arkareem\/test\/sweeps\/9t3sbtv5\n------ RUN NAME ------ fresh-sweep-32\nWaiting for W&amp;B process to finish... (success).\nSynced fresh-sweep-32: https:\/\/wandb.ai\/arkareem\/test\/runs\/ln732mvm\nSynced 5 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\nFind logs at: .\/wandb\/run-20220622_112037-ln732mvm\/logs\n\n<\/code><\/pre>\n<p>How do I start a new run without restarting the notebook and starting everything from scratch???<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1655886774292,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":58.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/bug-sweep-in-jupyter-makes-new-runs-impossible-to-start\/2645",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-23T22:41:23.872Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/arkareem\">@arkareem<\/a><\/p>\n<p>Thank-you for writing in, we will take a look at this for you. Would it be possible for you to share a reproduction of this issue as a <code>Google Colab<\/code> Notebook? It will help me start off my investigation on my end.<\/p>\n<p>Additionally, the <code>debug.log<\/code> and <code>debug-internal.log<\/code> files associated with the affected runs would be very helpful.<\/p>\n<p>Thanks,<br>\nMohammad<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-30T00:20:58.558Z",
                "Answer_body":"<p>Hi Abdulrahman,<\/p>\n<p>Since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-22T22:41:33.201Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"bug sweep jupyt make new run imposs start let work jupyt notebook run sweep sweep sweep sweep config entiti entiti project project agent agent sweep project project function pipelin hour happi sweep result stop execut here bug start new run manual here happen run follow code run init print run run run finish run init print run run run finish output chang environ variabl ignor session start inform modifi set init argument refer doc finish run lnmvm initi wait process finish success sync fresh sweep http arkareem test run lnmvm sync file media file artifact file file log run lnmvm log successfulli finish run lnmvm initi new run track run version run data save local mnt myfil class run lnmvm sync run fresh sweep doc sweep page http arkareem test sweep tsbtv run fresh sweep wait process finish success sync fresh sweep http arkareem test run lnmvm sync file media file artifact file file log run lnmvm log chang environ variabl ignor session start inform modifi set init argument refer doc track run version run data save local mnt myfil class run lnmvm sync run fresh sweep doc sweep page http arkareem test sweep tsbtv run fresh sweep wait process finish success sync fresh sweep http arkareem test run lnmvm sync file media file artifact file file log run lnmvm log start new run restart notebook start scratch",
        "Question_preprocessed_content":"bug sweep jupyt make new run imposs start let work jupyt notebook run sweep hour happi sweep result stop execut here bug start new run manual here happen run follow code run init run run init run output start new run restart notebook start scratch",
        "Question_gpt_summary_original":"The user encountered a bug in Jupyter where running a sweep prevented them from starting a new run manually. The error message indicated that changes to the wandb environment variables would be ignored because the wandb session had already started. The user is seeking a solution to start a new run without restarting the notebook and starting everything from scratch.",
        "Question_gpt_summary":"user encount bug jupyt run sweep prevent start new run manual error messag indic chang environ variabl ignor session start user seek solut start new run restart notebook start scratch",
        "Answer_original_content":"arkareem thank write look possibl share reproduct issu googl colab notebook help start investig end addition debug log debug intern log file associ affect run help thank mohammad abdulrahman heard go close request like open convers let know regard mohammad topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"write look possibl share reproduct issu notebook help start investig end addition file associ affect run help thank mohammad abdulrahman heard go close request like convers let know regard mohammad topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the response is a request for more information and a suggestion to share a reproduction of the issue as a google colab notebook, along with debug.log and debug-internal.log files associated with the affected runs. the conversation was closed due to no response from the user.",
        "Answer_gpt_summary":"solut provid answer respons request inform suggest share reproduct issu googl colab notebook debug log debug intern log file associ affect run convers close respons user"
    },
    {
        "Question_id":63637178.0,
        "Question_title":"Spark is running python 3.7.6 but spark context is showing python 2.7. How to fix using the spark context?",
        "Question_body":"<p>I have installed sagemaker using <code>sc.install_pypi_package(&quot;sagemaker==2.5.1&quot;)<\/code>. However, I get the following error when I try to import sagemaker and it is pointing to python2.7.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yfln3.png\" alt=\"cannot import name git_utils\" \/><\/a><\/p>\n<p>I checked my EMR master node running pyspark and the version there is pyspark 2.4.5 running python 3.7.6.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hyctk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hyctk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So then I tried to upgrade the python version of my spark context but it says<\/p>\n<blockquote>\n<p>&quot;ValueError: Package already installed for current Spark context!&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XBg3E.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>So I thought lemme try uninstalling python2.7 from spark context and that does not let me do it, saying<\/p>\n<blockquote>\n<p>&quot;Not uninstalling python at \/usr\/lib64\/python2.7\/lib-dynload, outside\nenvironment \/tmp\/1598628537004-0&quot;<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ktlfO.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I doing wrong? I believe the sagemaker import is failing due to spark context referring python2.7. How do I fix this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1598630122480,
        "Question_favorite_count":null,
        "Question_last_edit_time":1598631210847,
        "Question_score":0.0,
        "Question_view_count":855.0,
        "Answer_body":"<p>Referred to this <a href=\"https:\/\/aws.amazon.com\/blogs\/big-data\/install-python-libraries-on-a-running-cluster-with-emr-notebooks\/\" rel=\"nofollow noreferrer\">link<\/a> and updated the python version of spark context to python3. This fixes the issue:<\/p>\n<pre><code>%%configure -f\n{ &quot;conf&quot;:{\n          &quot;spark.pyspark.python&quot;: &quot;python3&quot;,\n          &quot;spark.pyspark.virtualenv.enabled&quot;: &quot;true&quot;,\n          &quot;spark.pyspark.virtualenv.type&quot;:&quot;native&quot;,\n          &quot;spark.pyspark.virtualenv.bin.path&quot;:&quot;\/usr\/bin\/virtualenv&quot;\n         }\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63637178",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1598632128583,
        "Question_original_content":"spark run python spark context show python fix spark context instal instal pypi packag follow error try import point python check emr master node run pyspark version pyspark run python tri upgrad python version spark context sai valueerror packag instal current spark context thought lemm try uninstal python spark context let sai uninstal python usr lib python lib dynload outsid environ tmp wrong believ import fail spark context refer python fix",
        "Question_preprocessed_content":"spark run python spark context show python fix spark context instal follow error try import point check emr master node run pyspark version pyspark run python tri upgrad python version spark context sai valueerror packag instal current spark context thought lemm try uninstal spark context let sai uninstal python outsid environ wrong believ import fail spark context refer fix",
        "Question_gpt_summary_original":"The user is facing challenges with their Spark context as it is showing Python 2.7 instead of Python 3.7.6, which is the version they have installed. They have tried to upgrade the Python version of their Spark context but encountered an error message stating that the package is already installed. Additionally, they attempted to uninstall Python 2.7 from the Spark context but received an error message indicating that it is outside the environment. The user believes that the import failure of sagemaker is due to the Spark context referring to Python 2.7.",
        "Question_gpt_summary":"user face challeng spark context show python instead python version instal tri upgrad python version spark context encount error messag state packag instal addition attempt uninstal python spark context receiv error messag indic outsid environ user believ import failur spark context refer python",
        "Answer_original_content":"refer link updat python version spark context python fix issu configur conf spark pyspark python python spark pyspark virtualenv enabl true spark pyspark virtualenv type nativ spark pyspark virtualenv bin path usr bin virtualenv",
        "Answer_preprocessed_content":"refer link updat python version spark context python fix issu",
        "Answer_gpt_summary_original":"the solution to the problem of spark context showing python 2.7 despite having installed python 3.7.6 is to update the python version of the spark context to python 3 by using the configuration command provided in the answer. this will fix the issue.",
        "Answer_gpt_summary":"solut problem spark context show python despit have instal python updat python version spark context python configur command provid answer fix issu"
    },
    {
        "Question_id":null,
        "Question_title":"Voice\/Speech to Text Train Model",
        "Question_body":"Hi.\n\nSo I would like to create a model that 'listens' to audio from movies\/podcasts (with subtitles) then returns the text transcript from it. Problem is, it's in a language not supported by Azure (or most of the big cloud providers). How would I go about and, from scratch, build a model that is trained on the audio from a new language? The input audio all will have subtitles or captions.\n\nI tried Azure ML studio but I couldn't create datasets with audio files. Not sure if I missed something there. Also tried Speech studio but it only supports a select number of languages. Would that be possible at all?\n\nAny suggestions would be appreciated. Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627330507383,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@NathanCarns-0092 Yes, you are correct, to develop a model for speech to text we need a deep learning model here. This is out of the scope of Azure Machine Learning Studio(classic). But I think Azure Machine Learning service should support it, please refer to this: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-deep-learning-vs-machine-learning#machine-translation\n\nI have found one post which may help: https:\/\/towardsdatascience.com\/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706\n\nMoreover, I have forwarded your feedback to see any plan here for Nigerian in Azure.\n\nThanks.\nYutong",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/490113\/voicespeech-to-text-train-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-04T23:39:40.253Z",
                "Answer_score":0,
                "Answer_body":"@NathanCarns-0092 Yes, you are correct, to develop a model for speech to text we need a deep learning model here. This is out of the scope of Azure Machine Learning Studio(classic). But I think Azure Machine Learning service should support it, please refer to this: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-deep-learning-vs-machine-learning#machine-translation\n\nI have found one post which may help: https:\/\/towardsdatascience.com\/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706\n\nMoreover, I have forwarded your feedback to see any plan here for Nigerian in Azure.\n\nThanks.\nYutong",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1628120380252,
        "Question_original_content":"voic speech text train model like creat model listen audio movi podcast subtitl return text transcript problem languag support azur big cloud provid scratch build model train audio new languag input audio subtitl caption tri studio couldn creat dataset audio file sure miss tri speech studio support select number languag possibl suggest appreci thank",
        "Question_preprocessed_content":"text train model like creat model listen audio return text transcript problem languag support azur scratch build model train audio new languag input audio subtitl caption tri studio couldn creat dataset audio file sure miss tri speech studio support select number languag possibl suggest appreci thank",
        "Question_gpt_summary_original":"The user wants to create a model that can transcribe audio from movies\/podcasts with subtitles in a language that is not supported by Azure or other big cloud providers. They have tried using Azure ML studio and Speech studio but have encountered difficulties in creating datasets with audio files and limited language support. The user is seeking suggestions on how to build a model from scratch that can be trained on audio from a new language.",
        "Question_gpt_summary":"user want creat model transcrib audio movi podcast subtitl languag support azur big cloud provid tri studio speech studio encount difficulti creat dataset audio file limit languag support user seek suggest build model scratch train audio new languag",
        "Answer_original_content":"nathancarn ye correct develop model speech text need deep learn model scope studio classic think servic support refer http doc microsoft com azur machin learn concept deep learn machin learn machin translat post help http towardsdatasci com audio deep learn simpl automat speech recognit asr work cfcec forward feedback plan nigerian azur thank yutong",
        "Answer_preprocessed_content":"ye correct develop model speech text need deep learn model scope studio think servic support refer post help forward feedback plan nigerian azur thank yutong",
        "Answer_gpt_summary_original":"possible solutions from the answer include using a deep learning model to develop a speech-to-text model, referring to microsoft's documentation on deep learning versus machine learning, and checking out a post on automatic speech recognition. the answer also mentions forwarding the user's feedback to see if there are any plans for nigerian language support in azure.",
        "Answer_gpt_summary":"possibl solut answer includ deep learn model develop speech text model refer microsoft document deep learn versu machin learn check post automat speech recognit answer mention forward user feedback plan nigerian languag support azur"
    },
    {
        "Question_id":null,
        "Question_title":"Run Time is inaccurate because of including upload time",
        "Question_body":"<p>Some runs will spend minutes because of my terrible network:<\/p>\n<pre><code class=\"lang-shell\">wandb: Waiting for W&amp;B process to finish... (success).\n<\/code><\/pre>\n<p>I find the Run Time column in UI will also contain the uploading time (by comparing with other runs\u2019 Run Time).<\/p>\n<p>My script is organized as follow:<\/p>\n<pre><code class=\"lang-python\">def main(config):\n    ...\n    wandb.init(wandb_config)\n    ...\n\nif __name__ == '__main__':\n    config = blabla\n    for p in [p1, p2, p3]:  # for loop to tune hyperparameters\n        config.param = p\n        main(config)\n<\/code><\/pre>\n<p>To fix this issue, should I use <code>wandb.finish<\/code> in the end of the <code>main()<\/code> function? As the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/finish\">doc<\/a> of <code>wandb.finish<\/code> lists:<\/p>\n<blockquote>\n<p>Marks a run as finished, and finishes uploading all data.<\/p>\n<\/blockquote>\n<p>I worry about whether this func will kill my slow data uploading worker.<\/p>\n<p>Or any other solutions?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1670249098960,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":83.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/run-time-is-inaccurate-because-of-including-upload-time\/3499",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-06T10:04:08.165Z",
                "Answer_body":"<p>I find the Run Time is also inaccurate when sync the offline-run instance. After sync, the Run-Time is about 1 min (seems to be the time spent on the sync command).<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-08T14:34:09.006Z",
                "Answer_body":"<p>Hi Yago!<\/p>\n<p>Looking into this.<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-14T22:12:44.048Z",
                "Answer_body":"<p>Hi Yao!<\/p>\n<p>Sorry for the late response. I have consulted with a few others on my team and this seems to be a bug, considering the fact that you still get the wrong time using the offline mode.<\/p>\n<p>Would you send us some info on how to reproduce this so we can fix it asap?<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-16T10:05:55.002Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"artsiom\" data-post=\"4\" data-topic=\"3499\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sea2.discourse-cdn.com\/business7\/user_avatar\/community.wandb.ai\/artsiom\/40\/1001_2.png\" class=\"avatar\"> artsiom:<\/div>\n<blockquote>\n<p>reproduce<\/p>\n<\/blockquote>\n<\/aside>\n<p>I think reproduce this issue is easy.<\/p>\n<ol>\n<li>Run wandb in offline mode.<\/li>\n<li>After the run is finish, use <code>wandb sync run_path<\/code> command to upload the run.<\/li>\n<li>The RunTime colum is several seconds (time the <code>sync<\/code> command spent) in the UI of uploaded run.<\/li>\n<\/ol>\n<p>My test code is like:<\/p>\n<pre><code class=\"lang-python\">os.environ['WANDB_MODE'] = 'offline'\nrun = wandb.init(...)\n...\nrun.finish()\n<\/code><\/pre>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-21T20:48:23.323Z",
                "Answer_body":"<p>Hi Yao!<\/p>\n<p>Thank you for the directions. I was able to reproduce it and will send it over to the engineers!<br>\nThe only real workaround for this is recording the run-times yourself and then logging them to wandb.<\/p>\n<p>Artsiom<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-27T21:36:19.734Z",
                "Answer_body":"<p>Hi Yao,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-03T15:30:48.984Z",
                "Answer_body":"<p>Hi Yao, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-14T10:06:10.411Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run time inaccur includ upload time run spend minut terribl network wait process finish success run time column contain upload time compar run run time script organ follow def main config init config main config blabla loop tune hyperparamet config param main config fix issu us finish end main function doc finish list mark run finish finish upload data worri func kill slow data upload worker solut",
        "Question_preprocessed_content":"run time inaccur includ upload time run spend minut terribl network run time column contain upload time script organ follow fix issu us end function doc list mark run finish finish upload data worri func kill slow data upload worker solut",
        "Question_gpt_summary_original":"The user is facing challenges with inaccurate run time due to slow network upload time. The Run Time column in the UI includes the uploading time, which affects the accuracy of the data. The user is considering using wandb.finish to fix the issue but is concerned about the impact on the slow data uploading worker. The user is seeking alternative solutions to address the problem.",
        "Question_gpt_summary":"user face challeng inaccur run time slow network upload time run time column includ upload time affect accuraci data user consid finish fix issu concern impact slow data upload worker user seek altern solut address problem",
        "Answer_original_content":"run time inaccur sync offlin run instanc sync run time min time spent sync command yago look cheer artsiom yao sorri late respons consult team bug consid fact wrong time offlin mode send info reproduc fix asap cheer artsiom artsiom reproduc think reproduc issu easi run offlin mode run finish us sync run path command upload run runtim colum second time sync command spent upload run test code like environ mode offlin run init run finish yao thank direct abl reproduc send engin real workaround record run time log artsiom yao want follow support request heard let know assist issu resolv best yao heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"run time inaccur sync instanc sync min yago look cheer artsiom yao sorri late respons consult team bug consid fact wrong time offlin mode send info reproduc fix asap cheer artsiom artsiom reproduc think reproduc issu easi run offlin mode run finish us command upload run runtim colum second upload run test code like yao thank direct abl reproduc send engin real workaround record log artsiom yao want follow support request heard let know assist issu resolv best yao heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user is experiencing inaccuracies in reported run time due to time spent uploading data. the answer suggests that this is a known bug and the only workaround is to record the run times manually and log them. the user is asked to provide information on how to reproduce the issue so that it can be fixed. the conversation ends with a request for the user to confirm if the issue has been resolved or if further assistance is needed.",
        "Answer_gpt_summary":"user experienc inaccuraci report run time time spent upload data answer suggest known bug workaround record run time manual log user ask provid inform reproduc issu fix convers end request user confirm issu resolv assist need"
    },
    {
        "Question_id":66819026.0,
        "Question_title":"Output model metrics to Cloudwatch",
        "Question_body":"<p>I am following the mnist-2 guide from the aws github documentation to implement my own training job <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving<\/a>. I have wrote my code using a similar structure, but I would like to visualise the training and validation metrics from Cloudwatch while the job is running. Do I need to manually specify the metrics I am trying to observe? The AWS guide states &quot;<em>SageMaker automatically parses the logs for metrics that built-in algorithms emit and sends those metrics to CloudWatch.<\/em>&quot; I am only using Tensorflow's training and validation accuracy and loss metrics, which I am not sure if they are built-in, or if I need to call them manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616769637850,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":475.0,
        "Answer_body":"<p>If you are not using a built-in algorithm, like in the example you linked, you have to define your metrics when you create the training job. You have to define regex expressions to grab from the logs the metric values, then cloudwatch will plot for you. The x axis will be the timestamp, you cannot change it.\nBasically just run your traning job and observe how the metrics are outputted, then you can build the appropriate regex. For example, since I am using coco metrics in tensorflow which periodically produce this:<\/p>\n<pre><code>INFO:tensorflow:Saving dict for global step 1109: DetectionBoxes_Precision\/mAP = 0.111895345, DetectionBoxes_Precision\/mAP (large) = 0.12102994, DetectionBoxes_Precision\/mAP (medium) = 0.050807837, DetectionBoxes_Precision\/mAP (small) = -1.0, DetectionBoxes_Precision\/mAP@.50IOU = 0.33130914, DetectionBoxes_Precision\/mAP@.75IOU = 0.03787096, DetectionBoxes_Recall\/AR@1 = 0.18493989, DetectionBoxes_Recall\/AR@10 = 0.36792925, DetectionBoxes_Recall\/AR@100 = 0.48543888, DetectionBoxes_Recall\/AR@100 (large) = 0.5131599, DetectionBoxes_Recall\/AR@100 (medium) = 0.21598063, DetectionBoxes_Recall\/AR@100 (small) = -1.0, Loss\/classification_loss = 0.8041124, Loss\/localization_loss = 0.35313264, Loss\/regularization_loss = 0.15211834, Loss\/total_loss = 1.30936, global_step = 1109, learning_rate = 0.28119853, loss = 1.30936\n<\/code><\/pre>\n<p>I use to grab the total loss for example:<\/p>\n<pre><code>INFO.*Loss\\\/total_loss = ([0-9\\.]+) \n<\/code><\/pre>\n<p>That's it, cloudwatch automatically plot the total_loss in time.<\/p>\n<p>You can define metrics either in the console or in the notebook, like this (just an example from my code):<\/p>\n<pre><code>metrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n<\/code><\/pre>\n<p>In order to test your regex, you can use a tool like <a href=\"https:\/\/regex101.com\/\" rel=\"nofollow noreferrer\">this<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66819026",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1616771138300,
        "Question_original_content":"output model metric cloudwatch follow mnist guid aw github document implement train job http github com aw amazon exampl tree master python sdk tensorflow script mode train serv wrote code similar structur like visualis train valid metric cloudwatch job run need manual specifi metric try observ aw guid state automat pars log metric built algorithm emit send metric cloudwatch tensorflow train valid accuraci loss metric sure built need manual",
        "Question_preprocessed_content":"output model metric cloudwatch follow guid aw github document implement train job wrote code similar structur like visualis train valid metric cloudwatch job run need manual specifi metric try observ aw guid state automat pars log metric algorithm emit send metric tensorflow train valid accuraci loss metric sure need manual",
        "Question_gpt_summary_original":"The user is following a guide to implement their own training job using AWS SageMaker and TensorFlow. They want to visualize the training and validation metrics from Cloudwatch while the job is running, but they are unsure if they need to manually specify the metrics they want to observe. They are only using TensorFlow's built-in training and validation accuracy and loss metrics, but they are not sure if they need to call them manually.",
        "Question_gpt_summary":"user follow guid implement train job tensorflow want visual train valid metric cloudwatch job run unsur need manual specifi metric want observ tensorflow built train valid accuraci loss metric sure need manual",
        "Answer_original_content":"built algorithm like exampl link defin metric creat train job defin regex express grab log metric valu cloudwatch plot axi timestamp chang basic run trane job observ metric output build appropri regex exampl coco metric tensorflow period produc info tensorflow save dict global step detectionbox precis map detectionbox precis map larg detectionbox precis map medium detectionbox precis map small detectionbox precis map iou detectionbox precis map iou detectionbox recal detectionbox recal detectionbox recal detectionbox recal larg detectionbox recal medium detectionbox recal small loss classif loss loss local loss loss regular loss loss total loss global step learn rate loss us grab total loss exampl info loss total loss cloudwatch automat plot total loss time defin metric consol notebook like exampl code metric loss regex loss accuraci regex acc epoch regex epoch valid acc regex val acc valid loss regex val loss estim tensorflow entri point train role execut role train instanc count train instanc type xlarg train max run output path output locat framework version version metric definit metric hyperparamet hyperparamet order test regex us tool like",
        "Answer_preprocessed_content":"algorithm like exampl link defin metric creat train job defin regex express grab log metric valu cloudwatch plot axi timestamp chang basic run trane job observ metric output build appropri regex exampl coco metric tensorflow period produc us grab total loss exampl cloudwatch automat plot time defin metric consol notebook like order test regex us tool like",
        "Answer_gpt_summary_original":"possible solutions to outputting model metrics to cloudwatch while running a training job include defining metrics using regex expressions to grab metric values from logs, observing how metrics are outputted during training and building appropriate regex, and defining metrics either in the console or in the notebook. a tool can be used to test regex expressions.",
        "Answer_gpt_summary":"possibl solut output model metric cloudwatch run train job includ defin metric regex express grab metric valu log observ metric output train build appropri regex defin metric consol notebook tool test regex express"
    },
    {
        "Question_id":56285351.0,
        "Question_title":"Updating tracked dir in DVC",
        "Question_body":"<p>According to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial<\/a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc<\/code> or <code>dvc remove &lt;myfile&gt;.dvc<\/code>) and then add it again via <code>dvc add &lt;mifile&gt;<\/code>. However It's not clear if I should apply the same workflow for the directories.<\/p>\n\n<p>I have the directory under DVC control with the following structure:<\/p>\n\n<pre><code>data\/\n    1.jpg\n    2.jpg\n<\/code><\/pre>\n\n<p>Should I run <code>dvc unprotect data<\/code> every time the directory content is updated?<\/p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data<\/code> in the following use cases:<\/p>\n\n<ul>\n<li><strong>New file is added.<\/strong> For example if I put <code>3.jpg<\/code> image in the data dir<\/li>\n<li><strong>File is deleted.<\/strong> For example if I delete <code>2.jpg<\/code> image in the <code>data<\/code> dir<\/li>\n<li><strong>File is updated.<\/strong> For example if I edit <code>1.jpg<\/code> image via graphic editor.<\/li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558667422490,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1558708772616,
        "Question_score":4.0,
        "Question_view_count":995.0,
        "Answer_body":"<p>Only when file is updated - i.e. edit <code>1.jpg<\/code> with your editor <strong>AND<\/strong> only if hadrlink or symlink cache type is enabled.<\/p>\n\n<p>Please, check this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">link<\/a>:<\/p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or\/and symlink<\/p>\n<\/blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files<\/a> it explains benefits of using hardlinks\/symlinks.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56285351",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1558674266680,
        "Question_original_content":"updat track dir accord tutori updat file remov file control execut unprotect remov add add clear appli workflow directori directori control follow structur data jpg jpg run unprotect data time directori content updat specif interest run unprotect data follow us case new file ad exampl jpg imag data dir file delet exampl delet jpg imag data dir file updat exampl edit jpg imag graphic editor combin previou us case file updat delet new file ad",
        "Question_preprocessed_content":"updat track dir accord tutori updat file remov file control add clear appli workflow directori directori control follow structur run time directori content updat specif interest run follow us case new file ad exampl imag data dir file delet exampl delet imag dir file updat exampl edit imag graphic editor combin previou us case",
        "Question_gpt_summary_original":"The user is seeking clarification on whether they need to run \"dvc unprotect\" every time the content of a directory under DVC control is updated. They are specifically asking if they need to run it when a new file is added, a file is deleted, a file is updated, or a combination of these use cases.",
        "Question_gpt_summary":"user seek clarif need run unprotect time content directori control updat specif ask need run new file ad file delet file updat combin us case",
        "Answer_original_content":"file updat edit jpg editor hadrlink symlink cach type enabl check link updat track file carri caution avoid data corrupt config option cach type set hardlink symlink strongli recommend read document perform optim larg file explain benefit hardlink symlink",
        "Answer_preprocessed_content":"file updat edit editor hadrlink symlink cach type enabl check link updat track file carri caution avoid data corrupt config option set hardlink symlink strongli recommend read document perform optim larg file explain benefit",
        "Answer_gpt_summary_original":"the answer suggests that the user should only run unprotect data when a file is updated, and only if the hadrlink or symlink cache type is enabled. the user is also advised to read a document on performance optimization for large files, which explains the benefits of using hardlinks\/symlinks and warns about data corruption when updating tracked files with caution.",
        "Answer_gpt_summary":"answer suggest user run unprotect data file updat hadrlink symlink cach type enabl user advis read document perform optim larg file explain benefit hardlink symlink warn data corrupt updat track file caution"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker taking an unexpectedly long time to download training data",
        "Question_body":"My customer's 220 Gb of training data took 54 minutes for Sagemaker to download. This is a rate of only 70 MB\/s, which is unexpectedly slow. He is accessing the data in S3 from his p3.8xlarge instance through a private VPC endpoint, so the theoretical maximum bandwidth is 25 Gbps. Is there anything that can be done to speed up the download?\n\nHe started the Sagemaker training with the following function:\n\nestimator = Estimator(image_name, role=role, output_path=output_location, train_instance_count=1, train_instance_type='ml.p3.8xlarge', train_volume_size=300, train_max_run = 52460*60 , security_group_ids='sg-00f1529adc4076841')\n\nThe output was: 2018-10-18 23:27:15 Starting - Starting the training job... Launching requested ML instances...... Preparing the instances for training... 2018-10-18 23:29:15 Downloading - Downloading input data............ .................................................................... .................................................................... .................................................................... 2018-10-19 00:23:50 Training - Downloading the training image..\n\nDataset download took ~54mins",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1540384039000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":654.0,
        "Answer_body":"How are they connect to S3? are they using a VPC endpoint \/ NAT? If they are using a VPC endpoint, My recommendation will be the open a support ticket, it's possible that support will be able to look at the network logs.\n\nAnother option for the customer is to use pipe input, pipe mode is recommended for large datasets, and it'll shorter their startup time because the data is being streamed instead of being downloaded to your training instances.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPpqUS0ckRXCHW0BXgxV5wQ\/sagemaker-taking-an-unexpectedly-long-time-to-download-training-data",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-10-27T06:48:20.000Z",
                "Answer_score":0,
                "Answer_body":"How are they connect to S3? are they using a VPC endpoint \/ NAT? If they are using a VPC endpoint, My recommendation will be the open a support ticket, it's possible that support will be able to look at the network logs.\n\nAnother option for the customer is to use pipe input, pipe mode is recommended for large datasets, and it'll shorter their startup time because the data is being streamed instead of being downloaded to your training instances.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1540622900000,
        "Question_original_content":"take unexpectedli long time download train data custom train data took minut download rate unexpectedli slow access data xlarg instanc privat vpc endpoint theoret maximum bandwidth gbp speed download start train follow function estim estim imag role role output path output locat train instanc count train instanc type xlarg train volum size train max run secur group id fadc output start start train job launch request instanc prepar instanc train download download input data train download train imag dataset download took min",
        "Question_preprocessed_content":"take unexpectedli long time download train data custom train data took minut download rate unexpectedli slow access data instanc privat vpc endpoint theoret maximum bandwidth gbp speed download start train follow function estim role role output start start train launch request prepar instanc download download input train download train dataset download took min",
        "Question_gpt_summary_original":"The user encountered a challenge with Sagemaker taking an unexpectedly long time to download 220 GB of training data, which took 54 minutes at a rate of only 70 MB\/s. Despite accessing the data in S3 from a p3.8xlarge instance through a private VPC endpoint with a theoretical maximum bandwidth of 25 Gbps, the user is seeking ways to speed up the download.",
        "Question_gpt_summary":"user encount challeng take unexpectedli long time download train data took minut rate despit access data xlarg instanc privat vpc endpoint theoret maximum bandwidth gbp user seek wai speed download",
        "Answer_original_content":"connect vpc endpoint nat vpc endpoint recommend open support ticket possibl support abl look network log option custom us pipe input pipe mode recommend larg dataset shorter startup time data stream instead download train instanc",
        "Answer_preprocessed_content":"connect vpc endpoint nat vpc endpoint recommend open support ticket possibl support abl look network log option custom us pipe input pipe mode recommend larg dataset shorter startup time data stream instead download train instanc",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. check if the user is connected to s3 using a vpc endpoint or nat.\n2. if the user is using a vpc endpoint, open a support ticket to have the network logs checked.\n3. use pipe input, as it is recommended for large datasets and will shorten the startup time by streaming the data instead of downloading it to the training instances.",
        "Answer_gpt_summary":"possibl solut answer check user connect vpc endpoint nat user vpc endpoint open support ticket network log check us pipe input recommend larg dataset shorten startup time stream data instead download train instanc"
    },
    {
        "Question_id":null,
        "Question_title":"How to pass values for a \"shap_baseline\" if we have categorical values (string values) as features in classsagemaker.clarify.SHAPConfig method.",
        "Question_body":"using this documentation i passing a single row as to shap_baseline parameter to implement explainability monitoring , a similar implementation of what is done in in this github repo implementation. if I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows. If i ignore the shap_baseline (as it is optional), the schedule is taking forever to run. Help of any kind is really appreciated.\n\nthanks for your time and effort :)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663938111276,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":41.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUVdzi-7h5RAapCYJUehzuZw\/how-to-pass-values-for-a-shap-baseline-if-we-have-categorical-values-string-values-as-features-in-classsagemaker-clarify-shap-config-method",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-04T05:28:35.427Z",
                "Answer_score":0,
                "Answer_body":"Thanks for your question\n\nThere is some guidance and information about selecting a proper baseline from here that may help: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/clarify-feature-attribute-shap-baselines.html\n\nif I am passing a single row as input to shap_baseline parameter, the schedule is failing by concatenating 2 rows\n\nFor this, you may want to open a ticket via AWS Support in the AWS console so we can take a better look at this internally, and include your processing job details if possible (such as your analysis configurations, Job ARN, etc.)\n\nIf i ignore the shap_baseline (as it is optional), the schedule is taking forever to run\n\nThere is a num_clusters parameter in the SHAPConfig which you can explicitly set to reduce the size of the baseline dataset that is generated. A lower number (low single digits) will generally provide faster runtime",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pass valu shap baselin categor valu string valu featur class clarifi shapconfig method document pass singl row shap baselin paramet implement explain monitor similar implement github repo implement pass singl row input shap baselin paramet schedul fail concaten row ignor shap baselin option schedul take forev run help kind appreci thank time effort",
        "Question_preprocessed_content":"pass valu categor valu featur method document pass singl row paramet implement explain monitor similar implement github repo implement pass singl row input paramet schedul fail concaten row ignor schedul take forev run help kind appreci thank time effort",
        "Question_gpt_summary_original":"The user is facing challenges in passing values for a \"shap_baseline\" while implementing explainability monitoring using the classsagemaker.clarify.SHAPConfig method. The user is encountering issues with concatenating two rows and the schedule is taking a long time to run when ignoring the shap_baseline parameter. The user is seeking help to resolve these issues.",
        "Question_gpt_summary":"user face challeng pass valu shap baselin implement explain monitor class clarifi shapconfig method user encount issu concaten row schedul take long time run ignor shap baselin paramet user seek help resolv issu",
        "Answer_original_content":"thank question guidanc inform select proper baselin help http doc aw amazon com latest clarifi featur attribut shap baselin html pass singl row input shap baselin paramet schedul fail concaten row want open ticket aw support aw consol better look intern includ process job detail possibl analysi configur job arn ignor shap baselin option schedul take forev run num cluster paramet shapconfig explicitli set reduc size baselin dataset gener lower number low singl digit gener provid faster runtim",
        "Answer_preprocessed_content":"thank question guidanc inform select proper baselin help pass singl row input paramet schedul fail concaten row want open ticket aw support aw consol better look intern includ process job detail possibl ignor schedul take forev run paramet shapconfig explicitli set reduc size baselin dataset gener lower number gener provid faster runtim",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. check the guidance and information provided in the link https:\/\/docs.aws.amazon.com\/\/latest\/dg\/clarify-feature-attribute-shap-baselines.html to select a proper baseline.\n2. if passing a single row as input to shap_baseline parameter is causing the schedule to fail, open a ticket via aws support in the aws console and include processing job details.\n3. ignore the shap_baseline parameter (as it is optional) if it is causing the schedule to take forever to run.\n4. explicitly set the num_clusters parameter in the shapconfig to reduce the size of the baseline dataset that is generated. a lower number (low single digits) will generally provide faster runtime.",
        "Answer_gpt_summary":"possibl solut answer check guidanc inform provid link http doc aw amazon com latest clarifi featur attribut shap baselin html select proper baselin pass singl row input shap baselin paramet caus schedul fail open ticket aw support aw consol includ process job detail ignor shap baselin paramet option caus schedul forev run explicitli set num cluster paramet shapconfig reduc size baselin dataset gener lower number low singl digit gener provid faster runtim"
    },
    {
        "Question_id":null,
        "Question_title":"How to Register a ML model using MLflow",
        "Question_body":"Hi,\n\nI have a PyTorch model which I have pushed into the dbfs now I want to serve the model using MLflow. I saw that the model needs to be in python_function model.\n\nTo do that I did the following methods\n1. load the model from dbfs using torch load option\n2. Then save the model in python_function model using the pyfunc.save_model function\n3. After this when I register the model I get a decode error\n\nI'm not training any model in the Databricks.\n\nimport mlflow\nimport mlflow.pyfunc\nfrom torch import load as torch_load\npy_model = torch_load( \"\/dbfs\/FileStore\/ml\/ner_model\" , map_location = torch_device(ner_gpu_device))\nmlflow.pytorch.save_model(py_model,path=\"\/dbfs\/FileStore\/pyfunc\/ner_model\")\nmodel = mlflow.pyfunc.load_model(\"\/dbfs\/FileStore\/pyfunc\/ner_model\")\nmlflow.register_model(model,\"ner_model\")\n# loading the python_function model to register\nmodel = mlflow.pyfunc.load_model(\"\/dbfs\/FileStore\/pyfunc\/ner_model\")\nmodel_version = mlflow.register_model(model,\"ner_model\")\n\n\n\n\nthis is the error which I get while running the register model line",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638981630093,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/656507\/how-to-register-a-ml-model-using-mlflow.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-09T06:06:13.953Z",
                "Answer_score":0,
                "Answer_body":"@RajamannarAK-6287 Thanks for the question. Can you please add more details about the trained model and deployment that you are trying.\n\nHere is link to an example of how to deploy and use object detection model in Azure ML with MLFlow option.\n\nthe process to do MLFlow using a custom python flavor then deploy to Azure ML using the Azure version of MLFlow.\nCreated the template here:\n1. Main deployment script to AML: aml_mlflow_deployment\/mlflow_aml_deployment.ipynb at main \u00b7 james-tn\/aml_mlflow_deployment (github.com)\n2. Custom mlflow.pyfunc module: aml_mlflow_deployment\/model_loader.py at main \u00b7 james-tn\/aml_mlflow_deployment (github.com)\n\n\n\n\nThere's also the option of simply using Azure DevOps to pull the model out of the MLFlow model repo in Azure Databricks, containerize it, and put it directly to ACI\/AKS for API scoring outside of Azure Databricks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"regist model pytorch model push dbf want serv model saw model need python function model follow method load model dbf torch load option save model python function model pyfunc save model function regist model decod error train model databrick import import pyfunc torch import load torch load model torch load dbf filestor ner model map locat torch devic ner gpu devic pytorch save model model path dbf filestor pyfunc ner model model pyfunc load model dbf filestor pyfunc ner model regist model model ner model load python function model regist model pyfunc load model dbf filestor pyfunc ner model model version regist model model ner model error run regist model line",
        "Question_preprocessed_content":"regist model pytorch model push dbf want serv model saw model need model follow method load model dbf torch load option save model model function regist model decod error train model databrick import import pyfunc torch import load model load model regist model error run regist model line",
        "Question_gpt_summary_original":"The user is facing challenges in registering a PyTorch model using MLflow. They have loaded the model from dbfs and saved it in python_function model using pyfunc.save_model function. However, when they try to register the model, they encounter a decode error. The user is not training any model in Databricks.",
        "Question_gpt_summary":"user face challeng regist pytorch model load model dbf save python function model pyfunc save model function try regist model encount decod error user train model databrick",
        "Answer_original_content":"rajamannarak thank question add detail train model deploy try link exampl deploi us object detect model option process custom python flavor deploi azur version creat templat main deploy script aml aml deploy aml deploy ipynb main jame aml deploy github com custom pyfunc modul aml deploy model loader main jame aml deploy github com option simpli azur devop pull model model repo azur databrick container directli aci ak api score outsid azur databrick",
        "Answer_preprocessed_content":"thank question add detail train model deploy try link exampl deploi us object detect model option process custom python flavor deploi azur version creat templat main deploy script aml main custom pyfunc modul main option simpli azur devop pull model model repo azur databrick container directli api score outsid azur databrick",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer are: \n- asking for more details about the trained model and deployment.\n- providing an example of how to deploy and use object detection model with  option.\n- creating a template for main deployment script to aml and custom .pyfunc module.\n- using azure devops to pull the model out of the  model repo in azure databricks, containerize it, and put it directly to aci\/aks for api scoring outside of azure databricks.",
        "Answer_gpt_summary":"possibl solut mention answer ask detail train model deploy provid exampl deploi us object detect model option creat templat main deploy script aml custom pyfunc modul azur devop pull model model repo azur databrick container directli aci ak api score outsid azur databrick"
    },
    {
        "Question_id":null,
        "Question_title":"Request to add TVS in Organizations using & contributing to MLflow",
        "Question_body":"Hi Team,\n\nAt TVS Motor we use MLflow extensively for ML model tracking, projects and registry. In the past, I was a contributor to MLflow as well.\n\nSo, can we get the TVS Motor listed in the \u201cOrganizations using and contributing to MLflow\u201d section, please?\n\n\n\n\n\u00a0\n\n\u00a0Thanks,\nNaga",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627294324000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":57.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/ReyGdDjmCeo",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-13T10:20:20",
                "Answer_body":"Dear MLflow team, reinitiating our request to add www.tvsmotor.com listed as\u00a0 \u201cOrganizations using and contributing to MLflow\u201d section. our teams use MLflow extensively for ML model tracking, projects and registry.\u00a0\ncheers.\n\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"request add tv organ contribut team tv motor us extens model track project registri past contributor tv motor list organ contribut section thank naga",
        "Question_preprocessed_content":"request add tv organ contribut team tv motor us extens model track project registri past contributor tv motor list organ contribut section thank naga",
        "Question_gpt_summary_original":"The user is requesting to add TVS Motor to the \"Organizations using and contributing to MLflow\" section, as they use MLflow extensively for ML model tracking, projects, and registry.",
        "Question_gpt_summary":"user request add tv motor organ contribut section us extens model track project registri",
        "Answer_original_content":"dear team reiniti request add tvsmotor com list organ contribut section team us extens model track project registri cheer",
        "Answer_preprocessed_content":"dear team reiniti request add list organ contribut section team us extens model track project registri cheer",
        "Answer_gpt_summary_original":"Solution: No solutions provided. The user is simply making a request to the MLflow team to add their organization to the \"Organizations using and contributing to MLflow\" section on the website.",
        "Answer_gpt_summary":"solut solut provid user simpli make request team add organ organ contribut section websit"
    },
    {
        "Question_id":72073763.0,
        "Question_title":"GCP Vertex Pipeline - Why kfp.v2.dsl.Output as function arguments work without being provided?",
        "Question_body":"<p>Why <code> kfp.v2.dsl.Output<\/code> as function argument works without being provided?<\/p>\n<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/machine_learning\/deepdive2\/machine_learning_in_the_enterprise\/solutions\/create_run_vertex_pipeline.ipynb\" rel=\"nofollow noreferrer\">Create and run ML pipelines with Vertex Pipelines!<\/a> Jupyter notebook example from GCP.<\/p>\n<p>The function <code>classif_model_eval_metrics<\/code> takes <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> which have no default values.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],                # No default value set, hence must be mandatory\n    metricsc: Output[ClassificationMetrics], # No default value set, hence must be mandatory\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]): \n    # Full code at the bottom.\n<\/code><\/pre>\n<p>Hence those arguments should be mandatory, but the function is called without those arguments.<\/p>\n<pre><code>    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p>The entire pipeline code is below.<\/p>\n<pre><code>@kfp.dsl.pipeline(name=&quot;automl-tab-beans-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = &quot;bq:\/\/aju-dev-demos.beans.beans1&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;us-central1&quot;,\n    api_endpoint: str = &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auRoc&quot;: 0.95}',\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project, display_name=display_name, bq_source=bq_source\n    )\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=COLUMNS,\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;Class&quot;,\n    )\n    model_eval_task = classif_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n        # &lt;--- No arguments for ``metrics: Output[Metrics]``` and ```metricsc: Output[ClassificationMetrics]```\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3GpHe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Why does it work and what are <code>metrics: Output[Metrics]<\/code> and <code>metricsc: Output[ClassificationMetrics]<\/code> of type <code>kfp.v2.dsl.Output<\/code>?<\/p>\n<hr \/>\n<h2>classif_model_eval_metrics function code<\/h2>\n<pre><code>from kfp.v2.dsl import (\n    Dataset, Model, Output, Input, \n    OutputPath, ClassificationMetrics, Metrics, component\n)\n\n@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-6:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;, # Optional: you can use this to load the component later\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,      # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n) -&gt; NamedTuple(&quot;Outputs&quot;, [(&quot;dep_decision&quot;, str)]):  # Return parameter.\n    &quot;&quot;&quot;Renders evaluation metrics for an AutoML Tabular classification model.\n    Retrieves the classification model evaluation and render the ROC and confusion matrix\n    for the model. Determine whether the model is sufficiently accurate to deploy.\n    &quot;&quot;&quot;\n    import json\n    import logging\n    from google.cloud import aiplatform\n\n    # Fetch model eval info\n    def get_eval_info(client, model_name):\n        from google.protobuf.json_format import MessageToDict\n        response = client.list_model_evaluations(parent=model_name)\n        metrics_list = []\n        metrics_string_list = []\n        for evaluation in response:\n            metrics = MessageToDict(evaluation._pb.metrics)\n            metrics_str = json.dumps(metrics)\n            metrics_list.append(metrics)\n            metrics_string_list.append(metrics_str)\n        return (\n            evaluation.name,\n            metrics_list,\n            metrics_string_list,\n        )\n\n    def classification_thresholds_check(metrics_dict, thresholds_dict):\n        for k, v in thresholds_dict.items():\n            if k in [&quot;auRoc&quot;, &quot;auPrc&quot;]:  # higher is better\n                if metrics_dict[k] &lt; v:  # if under threshold, don't deploy\n                    return False\n        return True\n\n    def log_metrics(metrics_list, metricsc):\n        test_confusion_matrix = metrics_list[0][&quot;confusionMatrix&quot;]\n        logging.info(&quot;rows: %s&quot;, test_confusion_matrix[&quot;rows&quot;])\n        # log the ROC curve\n        fpr = [], tpr = [], thresholds = []\n        for item in metrics_list[0][&quot;confidenceMetrics&quot;]:\n            fpr.append(item.get(&quot;falsePositiveRate&quot;, 0.0))\n            tpr.append(item.get(&quot;recall&quot;, 0.0))\n            thresholds.append(item.get(&quot;confidenceThreshold&quot;, 0.0))\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        # log the confusion matrix\n        annotations = []\n        for item in test_confusion_matrix[&quot;annotationSpecs&quot;]:\n            annotations.append(item[&quot;displayName&quot;])\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n        # log textual metrics info as well\n        for metric in metrics_list[0].keys():\n            if metric != &quot;confidenceMetrics&quot;:\n                val_string = json.dumps(metrics_list[0][metric])\n                metrics.log_metric(metric, val_string)\n        # metrics.metadata[&quot;model_type&quot;] = &quot;AutoML Tabular classification&quot;\n\n    aiplatform.init(project=project)\n\n    client = aiplatform.gapic.ModelServiceClient(client_options={&quot;api_endpoint&quot;: api_endpoint})\n    eval_name, metrics_list, metrics_str_list = get_eval_info(\n        client, model.uri.replace(&quot;aiplatform:\/\/v1\/&quot;, &quot;&quot;)\n    )\n    log_metrics(metrics_list, metricsc)\n    thresholds_dict = json.loads(thresholds_dict_str)\n\n    return (&quot;true&quot;,) if classification_thresholds_check(metrics_list[0], thresholds_dict) else (&quot;false&quot;, )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1651375335713,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":358.0,
        "Answer_body":"<p>The custom component is defined as a Python function with a <code>@kfp.v2.dsl.component<\/code> decorator.<\/p>\n<p>The <code>@component<\/code> decorator specifies three optional arguments: the base container image to use; any packages to install; and the yaml file to which to write the component specification.<\/p>\n<p>The component function, <code>classif_model_eval_metrics<\/code>, has some input parameters.  The model parameter is an input <code>kfp.v2.dsl.Model artifact<\/code>.<\/p>\n<p>The two function args, <code>metrics<\/code> and <code>metricsc<\/code>, are component Outputs, in this case of types Metrics and ClassificationMetrics. They\u2019re not explicitly passed as inputs to the component step, but rather are automatically instantiated and can be used in the component.<\/p>\n<pre><code>@component(\n    base_image=&quot;gcr.io\/deeplearning-platform-release\/tf2-cpu.2-3:latest&quot;,\n    output_component_file=&quot;tables_eval_component.yaml&quot;,\n    packages_to_install=[&quot;google-cloud-aiplatform&quot;],\n)\ndef classif_model_eval_metrics(\n    project: str,\n    location: str,  # &quot;us-central1&quot;,\n    api_endpoint: str,  # &quot;us-central1-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str,\n    model: Input[Model],\n    metrics: Output[Metrics],\n    metricsc: Output[ClassificationMetrics],\n)\n<\/code><\/pre>\n<p>For example, in the function below, we\u2019re calling <code>metricsc.log_roc_curve()<\/code> and <code>metricsc.log_confusion_matrix()<\/code> to render these visualizations in the Pipelines UI. These Output params become component outputs when the component is compiled, and can be consumed by other pipeline steps.<\/p>\n<pre><code>def log_metrics(metrics_list, metricsc):\n        ...\n        metricsc.log_roc_curve(fpr, tpr, thresholds)\n        ...\n        metricsc.log_confusion_matrix(\n            annotations,\n            test_confusion_matrix[&quot;rows&quot;],\n        )\n<\/code><\/pre>\n<p>For more information you can refer to this <a href=\"https:\/\/cloud.google.com\/blog\/topics\/developers-practitioners\/use-vertex-pipelines-build-automl-classification-end-end-workflow\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72073763",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1652802212843,
        "Question_original_content":"gcp vertex pipelin kfp dsl output function argument work provid kfp dsl output function argument work provid follow creat run pipelin vertex pipelin jupyt notebook exampl gcp function classif model eval metric take metric output metric metricsc output classificationmetr default valu compon base imag gcr deeplearn platform releas cpu latest output compon file tabl eval compon yaml option us load compon later packag instal googl cloud aiplatform def classif model eval metric project str locat str central api endpoint str central aiplatform googleapi com threshold dict str str model input model metric output metric default valu set mandatori metricsc output classificationmetr default valu set mandatori namedtupl output dep decis str code argument mandatori function call argument model eval task classif model eval metric project gcp region api endpoint threshold dict str train output model namedtupl output dep decis str return paramet render evalu metric automl tabular classif model retriev classif model evalu render roc confus matrix model determin model suffici accur deploi import json import log googl cloud import aiplatform fetch model eval info def eval info client model googl protobuf json format import messagetodict respons client list model evalu parent model metric list metric string list evalu respons metric messagetodict evalu metric metric str json dump metric metric list append metric metric string list append metric str return evalu metric list metric string list def classif threshold check metric dict threshold dict threshold dict item auroc auprc higher better metric dict threshold deploi return fals return true def log metric metric list metricsc test confus matrix metric list confusionmatrix log info row test confus matrix row log roc curv fpr tpr threshold item metric list confidencemetr fpr append item falsepositiver tpr append item recal threshold append item confidencethreshold metricsc log roc curv fpr tpr threshold log confus matrix annot item test confus matrix annotationspec annot append item displaynam metricsc log confus matrix annot test confus matrix row log textual metric info metric metric list kei metric confidencemetr val string json dump metric list metric metric log metric metric val string metric metadata model type automl tabular classif aiplatform init project project client aiplatform gapic modelservicecli client option api endpoint api endpoint eval metric list metric str list eval info client model uri replac aiplatform log metric metric list metricsc threshold dict json load threshold dict str return true classif threshold check metric list threshold dict fals",
        "Question_preprocessed_content":"gcp vertex pipelin function argument work provid function argument work provid follow creat run pipelin vertex pipelin jupyt notebook exampl gcp function take default valu argument mandatori function call argument entir pipelin code work type function code",
        "Question_gpt_summary_original":"The user is encountering a challenge with the function `classif_model_eval_metrics` in the GCP Vertex Pipeline. The function takes `metrics: Output[Metrics]` and `metricsc: Output[ClassificationMetrics]` as mandatory arguments, but the function is being called without these arguments. The user is confused about why the function works without these arguments and what `metrics: Output[Metrics]` and `metricsc: Output[ClassificationMetrics]` of type `kfp.v2.dsl.Output` are.",
        "Question_gpt_summary":"user encount challeng function classif model eval metric gcp vertex pipelin function take metric output metric metricsc output classificationmetr mandatori argument function call argument user confus function work argument metric output metric metricsc output classificationmetr type kfp dsl output",
        "Answer_original_content":"custom compon defin python function kfp dsl compon decor compon decor specifi option argument base contain imag us packag instal yaml file write compon specif compon function classif model eval metric input paramet model paramet input kfp dsl model artifact function arg metric metricsc compon output case type metric classificationmetr theyr explicitli pass input compon step automat instanti compon compon base imag gcr deeplearn platform releas cpu latest output compon file tabl eval compon yaml packag instal googl cloud aiplatform def classif model eval metric project str locat str central api endpoint str central aiplatform googleapi com threshold dict str str model input model metric output metric metricsc output classificationmetr exampl function call metricsc log roc curv metricsc log confus matrix render visual pipelin output param compon output compon compil consum pipelin step def log metric metric list metricsc metricsc log roc curv fpr tpr threshold metricsc log confus matrix annot test confus matrix row inform refer document",
        "Answer_preprocessed_content":"custom compon defin python function decor decor specifi option argument base contain imag us packag instal yaml file write compon specif compon function input paramet model paramet input function arg compon output case type metric classificationmetr theyr explicitli pass input compon step automat instanti compon exampl function call render visual pipelin output param compon output compon compil consum pipelin step inform refer document",
        "Answer_gpt_summary_original":"the solution to the challenge with the gcp vertex pipeline is to ensure that the function classif_model_eval_metrics takes arguments of type kfp.v2.dsl.output as function arguments. the custom component is defined as a python function with a @kfp.v2.dsl.component decorator, which specifies three optional arguments: the base container image to use, any packages to install, and the yaml file to which to write the component specification. the component function, classif_model_eval_metrics, has some input parameters, and the two function args, metrics and metricsc, are component outputs, which can be used in the component. these output params become component outputs when the component is compiled and can be consumed by other pipeline steps.",
        "Answer_gpt_summary":"solut challeng gcp vertex pipelin ensur function classif model eval metric take argument type kfp dsl output function argument custom compon defin python function kfp dsl compon decor specifi option argument base contain imag us packag instal yaml file write compon specif compon function classif model eval metric input paramet function arg metric metricsc compon output compon output param compon output compon compil consum pipelin step"
    },
    {
        "Question_id":67361483.0,
        "Question_title":"AWS Sagemaker Studio, cannot load pickle files",
        "Question_body":"<p>I'm a newbie in Sagemaker and i'm trying to load a pickle dataset into sagemaker notebook.\nI'm using the Python 3 (Data Science) kernel and ml.t3.medium instance.\nEither i load the pickle from S3 or I upload it directly from the studio like this:<\/p>\n<pre><code>import pickle5\nwith open('filename', 'rb') as f:\n    x = pickle.load(f)\n<\/code><\/pre>\n<p><strong>I get this Error:<\/strong><\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/IPython\/core\/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--&gt; 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\n..................... more errors here\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\npandas\/_libs\/properties.pyx in pandas._libs.properties.AxisProperty.__get__()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\nAttributeError: 'DataFrame' object has no attribute '_data'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1619992853797,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67361483",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1620025682632,
        "Question_original_content":"studio load pickl file newbi try load pickl dataset notebook python data scienc kernel medium instanc load pickl upload directli studio like import pickl open filenam pickl load error attributeerror traceback recent opt conda lib python site packag ipython core formatt self obj type pprinter self type printer defer pprinter self defer printer printer pretti obj printer flush return stream getvalu error opt conda lib python site packag panda core gener getattr self self accessor return object getattribut self self info axi hold identifi hold panda lib properti pyx panda lib properti axisproperti opt conda lib python site packag panda core gener getattr self self accessor return object getattribut self self info axi hold identifi hold attributeerror datafram object attribut data",
        "Question_preprocessed_content":"studio load pickl file newbi try load pickl dataset notebook python kernel instanc load pickl upload directli studio like error",
        "Question_gpt_summary_original":"The user is facing challenges while trying to load a pickle dataset into Sagemaker notebook using Python 3 (Data Science) kernel and ml.t3.medium instance. The user is encountering an error message that says 'DataFrame' object has no attribute '_data'.",
        "Question_gpt_summary":"user face challeng try load pickl dataset notebook python data scienc kernel medium instanc user encount error messag sai datafram object attribut data",
        "Answer_original_content":"check panda version error typic occur pickl file written old panda version notebook probabl run panda panda datafram pickl probabl",
        "Answer_preprocessed_content":"check panda version error typic occur pickl file written old panda version notebook probabl run panda panda datafram pickl probabl",
        "Answer_gpt_summary_original":"summary: the solution to the attributeerror encountered when loading pickle files into a jupyter notebook is to check the pandas versions. this error usually occurs when the pickled file was written in an old pandas version, while the notebook runs pandas > 1.1.",
        "Answer_gpt_summary":"summari solut attributeerror encount load pickl file jupyt notebook check panda version error usual occur pickl file written old panda version notebook run panda"
    },
    {
        "Question_id":72103557.0,
        "Question_title":"Save the result of a query in a BigQuery Table, in Cloud Storage",
        "Question_body":"<p>I would like to know what is the OPTIMAL way to store the result of a Google BigQuery table query, to Google Cloud storage. My code, which is currently being run in some Jupyter Notebook (in Vertex AI Workbench, same project than both the BigQuery data source, as well as the Cloud Storage destination), looks as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># CELL 1 OF 2\n\nfrom google.cloud import bigquery\nbqclient = bigquery.Client()\n\n# The query string can vary:\nquery_string = &quot;&quot;&quot;\n        SELECT *  \n        FROM `my_project-name.my_db.my_table` \n        LIMIT 2000000\n        &quot;&quot;&quot;\n\ndataframe = (\n    bqclient.query(query_string)\n    .result()\n    .to_dataframe(\n        create_bqstorage_client=True,\n    )\n)\nprint(&quot;Dataframe shape: &quot;, dataframe.shape)\n\n# CELL 2 OF 2:\n\nimport pandas as pd\ndataframe.to_csv('gs:\/\/my_bucket\/test_file.csv', index=False)\n<\/code><\/pre>\n<p>This code takes around 7.5 minutes to successfully complete.<\/p>\n<p><strong>Is there a more OPTIMAL way to achive what was done above?<\/strong> (It would mean <em>faster<\/em>, but maybe something else could be improved).<\/p>\n<p>Some additional notes:<\/p>\n<ol>\n<li>I want to run it &quot;via a Jupyter Notebook&quot; (in Vertex AI Workbench), because sometimes some data preprocessing, or special filtering must be done, which cannot be easily accomplished via SQL queries.<\/li>\n<li>For the first part of the code, I have discarded <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas.read_gbq<\/a>, as it was giving me some weird EOF errors, when (experimentally) &quot;storing as .CSV and reading back&quot;.<\/li>\n<li>Intuitively, I would focus the optimization efforts in the second half of the code (<code>CELL 2 OF 2<\/code>), as the first one was borrowed from <a href=\"https:\/\/cloud.google.com\/bigquery\/docs\/bigquery-storage-python-pandas\" rel=\"nofollow noreferrer\">the official Google documentation<\/a>. I have tried <a href=\"https:\/\/stackoverflow.com\/a\/57404119\/16706763\">this<\/a> but it does not work, however in the same thread <a href=\"https:\/\/stackoverflow.com\/a\/60644694\/16706763\">this<\/a> option worked OK.<\/li>\n<li>It is likley that this code will be included in some Docker image afterwards, so &quot;as little libraries as possible&quot; must be used.<\/li>\n<\/ol>\n<p>Thank you.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1651600671333,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1409.0,
        "Answer_body":"<p>After some experiments, I think I have got to a solution for my original post. First, the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd  # Just one library is imported this time\n\n# This SQL query can vary, modify it to match your needs\nquery_string = &quot;&quot;&quot;\nSELECT *\nFROM `my_project.my_db.my_table`\nLIMIT 2000000\n&quot;&quot;&quot;\n\n# One liner to query BigQuery data.\ndownloaded_dataframe = pd.read_gbq(query_string, dialect='standard', use_bqstorage_api=True)\n\n# Data processing (OPTIONAL, modify it to match your needs)\n# I won't do anything this time, just upload the previously queried data\n\n# Data store in GCS\ndownloaded_dataframe.to_csv('gs:\/\/my_bucket\/uploaded_data.csv', index=False)\n<\/code><\/pre>\n<p>Some final notes:<\/p>\n<ol>\n<li>I have not done an &quot;in-depth research&quot; about the processing speed VS the number of rows existing in a BigQuery table, however I saw that the processing time with the updated code and the original query, now takes ~6 minutes; that's enough for the time being. <em>This answer might have some room for further improvements<\/em> therefore, but it's better than the original situation.<\/li>\n<li>The EOF error I mentioned in  my original post was: <code>ParserError: Error tokenizing data. C error: EOF inside string starting at row 70198<\/code>. In the end I got to realize that it did not have anything to do with <a href=\"https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_gbq.html\" rel=\"nofollow noreferrer\">pandas_gbq<\/a> function, but with &quot;how I was saving the data&quot;. See, <em>I was 'experimentally' storing the .csv file in the Vertex AI Workbench local storage, then downloading it to my local device, and when trying to open that data from my local device, I kept stumbling upon that error, however not getting the same when downloading the .csv data from Cloud Storage<\/em> ... Why? Well, it happens that if you download the .csv data &quot;very quickly&quot; after &quot;it gets generated&quot; (i.e., after few seconds), from Vertex AI Workbench local storage, the data is simply still incomplete, but it does not give any error or warning message: it will simply &quot;let you start with the download&quot;. For this reason, I think it is safer to export your data to Cloud Storage, and then download safely from there. This behaviour is more noticeable on large files (i.e. my own generated file, which had ~3.1GB in size).<\/li>\n<\/ol>\n<p>Hope this helps.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1651696486067,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72103557",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651612810316,
        "Question_original_content":"save result queri bigqueri tabl cloud storag like know optim wai store result googl bigqueri tabl queri googl cloud storag code current run jupyt notebook workbench project bigqueri data sourc cloud storag destin look follow cell googl cloud import bigqueri bqclient bigqueri client queri string vari queri string select project tabl limit datafram bqclient queri queri string result datafram creat bqstorag client true print datafram shape datafram shape cell import panda datafram csv bucket test file csv index fals code take minut successfulli complet optim wai achiv mean faster mayb improv addit note want run jupyt notebook workbench data preprocess special filter easili accomplish sql queri code discard panda read gbq give weird eof error experiment store csv read intuit focu optim effort second half code cell borrow offici googl document tri work thread option work liklei code includ docker imag littl librari possibl thank",
        "Question_preprocessed_content":"save result queri bigqueri tabl cloud storag like know optim wai store result googl bigqueri tabl queri googl cloud storag code current run jupyt notebook look follow code take minut successfulli complet optim wai achiv addit note want run jupyt notebook data preprocess special filter easili accomplish sql queri code discard give weird eof error store csv read intuit focu optim effort second half code borrow offici googl document tri work thread option work liklei code includ docker imag littl librari possibl thank",
        "Question_gpt_summary_original":"The user is seeking advice on the optimal way to store the result of a Google BigQuery table query to Google Cloud storage. They are currently using a Jupyter Notebook in Vertex AI Workbench and have encountered issues with pandas.read_gbq. They believe that the optimization efforts should focus on the second half of the code and want to use as few libraries as possible as the code will be included in a Docker image.",
        "Question_gpt_summary":"user seek advic optim wai store result googl bigqueri tabl queri googl cloud storag current jupyt notebook workbench encount issu panda read gbq believ optim effort focu second half code want us librari possibl code includ docker imag",
        "Answer_original_content":"experi think got solut origin post updat code import panda librari import time sql queri vari modifi match need queri string select project tabl limit liner queri bigqueri data download datafram read gbq queri string dialect standard us bqstorag api true data process option modifi match need won time upload previous queri data data store gc download datafram csv bucket upload data csv index fals final note depth research process speed number row exist bigqueri tabl saw process time updat code origin queri take minut time answer room improv better origin situat eof error mention origin post parsererror error token data error eof insid string start row end got realiz panda gbq function save data experiment store csv file workbench local storag download local devic try open data local devic kept stumbl error get download csv data cloud storag happen download csv data quickli get gener second workbench local storag data simpli incomplet error warn messag simpli let start download reason think safer export data cloud storag download safe behaviour notic larg file gener file size hope help thank",
        "Answer_preprocessed_content":"experi think got solut origin post updat code final note research process speed number row exist bigqueri tabl saw process time updat code origin queri take minut time answer room improv better origin situat eof error mention origin post end got realiz function save data experiment store csv file workbench local storag download local devic try open data local devic kept stumbl error get download csv data cloud storag happen download csv data quickli get gener workbench local storag data simpli incomplet error warn messag simpli let start download reason think safer export data cloud storag download safe behaviour notic larg file hope help thank",
        "Answer_gpt_summary_original":"the answer provides a solution to store the result of a google bigquery table query to google cloud storage using only one library, pandas. the code includes a one-liner to query bigquery data, data processing (optional), and data storage in gcs. the answer also suggests exporting data to cloud storage and then downloading it safely from there to avoid incomplete data downloads. the processing time with the updated code and the original query takes ~6 minutes.",
        "Answer_gpt_summary":"answer provid solut store result googl bigqueri tabl queri googl cloud storag librari panda code includ liner queri bigqueri data data process option data storag gc answer suggest export data cloud storag download safe avoid incomplet data download process time updat code origin queri take minut"
    },
    {
        "Question_id":60529048.0,
        "Question_title":"How to deploy an existing pytorch model previously trained with Amazon Sagemaker and stored in S3 bucket",
        "Question_body":"<p>I have trained a Pytorch model using SageMaker and the model is now stored in an S3 bucket.\nI am trying to retrieve that model and deploying it.<\/p>\n\n<p>This is the code I am using:<\/p>\n\n<pre><code>estimator = sagemaker.model.FrameworkModel(\n    model_data= #link to  model location in s3\n    image=  # image\n    role=role,\n    entry_point='train.py', \n    source_dir='pytorch_source',\n    sagemaker_session = sagemaker_session\n) \n\npredictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")\n<\/code><\/pre>\n\n<p>But after the deployment process (that seems to run smoothly), the predictor is just a NoneType.\nI haven't found any weird message in the logs...<\/p>\n\n<p>I have also made another attempt with the following code:<\/p>\n\n<pre><code>estimator = PyTorchModel(model_data= #link to model location in s3 \n                             role=role,\n                             image= #image\n                             entry_point='pytorch_source\/train.py',\n                            predictor_cls = 'pytorch_source\/train.py',\n                           framework_version = '1.1.0')\n\npredictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")\n<\/code><\/pre>\n\n<p>But it doesn't even complete the deployment.<\/p>\n\n<p>Can anyone help with this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1583334481857,
        "Question_favorite_count":null,
        "Question_last_edit_time":1583398637700,
        "Question_score":0.0,
        "Question_view_count":360.0,
        "Answer_body":"<p>I actually solved using PyTorchModel with the following settings:<\/p>\n\n<pre><code>estimator = PyTorchModel(model_data='#path to model, \n                             role=role,\n                             source_dir='pytorch_source',\n                             entry_point='deploy.py',\n                            predictor_cls = ImgPredictor,\n                           framework_version = '1.1.0')\n<\/code><\/pre>\n\n<p>where ImgPredictor is<\/p>\n\n<pre><code>from sagemaker.predictor import RealTimePredictor, json_deserializer\n\nclass ImgPredictor(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super(ImgPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='application\/x-image', \n                                           deserializer = json_deserializer ,accept='application\/json')\n<\/code><\/pre>\n\n<p>and deploy.py contains the required functions input_fn, output_fn, model_fn and predict_fn.\nAlso, a requirements.txt file was missing in the source directory.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60529048",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1583499635092,
        "Question_original_content":"deploi exist pytorch model previous train store bucket train pytorch model model store bucket try retriev model deploi code estim model frameworkmodel model data link model locat imag imag role role entri point train sourc dir pytorch sourc session session predictor estim deploi initi instanc count instanc type xlarg deploy process run smoothli predictor nonetyp haven weird messag log attempt follow code estim pytorchmodel model data link model locat role role imag imag entri point pytorch sourc train predictor cl pytorch sourc train framework version predictor estim deploi initi instanc count instanc type xlarg complet deploy help",
        "Question_preprocessed_content":"deploi exist pytorch model previous train store bucket train pytorch model model store bucket try retriev model deploi code deploy process predictor nonetyp haven weird messag attempt follow code complet deploy help",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a Pytorch model that was previously trained using Amazon SageMaker and stored in an S3 bucket. The user has tried two different deployment codes, but the predictor is either a NoneType or the deployment process does not complete. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face challeng deploi pytorch model previous train store bucket user tri differ deploy code predictor nonetyp deploy process complet user seek help resolv issu",
        "Answer_original_content":"actual solv pytorchmodel follow set estim pytorchmodel model data path model role role sourc dir pytorch sourc entri point deploi predictor cl imgpredictor framework version imgpredictor predictor import realtimepredictor json deseri class imgpredictor realtimepredictor def init self endpoint session super imgpredictor self init endpoint session content type applic imag deseri json deseri accept applic json deploi contain requir function input output model predict requir txt file miss sourc directori",
        "Answer_preprocessed_content":"actual solv pytorchmodel follow set imgpredictor contain requir function file miss sourc directori",
        "Answer_gpt_summary_original":"the solution to the challenge of deploying an existing pytorch model previously trained with and stored in an s3 bucket is to use pytorchmodel with specific settings. the settings include specifying the path to the model, the role, the source directory, and the entry point. additionally, a predictor class and a requirements.txt file are needed in the source directory. the deploy.py file should contain the required functions input_fn, output_fn, model_fn, and predict_fn.",
        "Answer_gpt_summary":"solut challeng deploi exist pytorch model previous train store bucket us pytorchmodel specif set set includ specifi path model role sourc directori entri point addition predictor class requir txt file need sourc directori deploi file contain requir function input output model predict"
    },
    {
        "Question_id":null,
        "Question_title":"Getting Authenticatin error for python script step in pipeline",
        "Question_body":"I am trying to create a azure pipeline for learning purpose. I followed the steps mentioned in this notebook:\nhttps:\/\/github.com\/MicrosoftLearning\/mslearn-dp100\/blob\/main\/08%20-%20Create%20a%20Pipeline.ipynb\n\n\n\n     #%% connect to a workspace from config\n # use service principal authentication - https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\n    \n azure_svppwd = os.environ.get(\"AZUREML_PASSWD\") # use os.environ[\"AZUREML_PASSWD\"] = ' from azure service client secret\"\n auth_pwd = ServicePrincipalAuthentication(\n     tenant_id=\"7da05296-d70e-4398-8a3d-f113e0dad997\",\n     service_principal_id=\"57bac230-cbb8-409e-be64-5c4516d40771\",\n     service_principal_password=azure_svppwd)\n ws = Workspace.from_config('config',auth=auth_pwd)    \n print(\"Found workspace {} at location {}\".format(ws.name, ws.location))    \n # get the data from datasets\n    \n in_data = ws.datasets.get('testparquet') # dataset is registered through Azure UI which refers to some parquet files in another container\n    \n #%% create environment\n env = Environment.from_conda_specification(name='azenv',file_path='envspec.yml')\n env.register(ws)  # Save the environment for future use and retreival\n #%% create compute target - compute clusters\n try:\n     comp_cluster = ComputeTarget(ws,'amltryoutcluster')\n     print('amltryoutcluster found existing and will be used')\n except 'ComputeTargetException':\n        \n     comp_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3',min_nodes=0,max_nodes=6)\n     # vm_size in above line indicates the type of computation powe we need\n     comp_cluster = ComputeTarget.create(ws,'amltryoutcluster',comp_config)\n     comp_cluster.wait_for_completion(show_output=True)\n #%% Create pipeline data object and steps\n prep_data = PipelineData(name=\"pipedata\",datastore=ws.get_default_datastore())\n #%% Create pipeline steps\n #%% pipeline configuration\n pipe_config = RunConfiguration()\n pipe_config.target = comp_cluster\n pipe_config.environment= Environment.get(ws,\"azenv\")\n prep_step = PythonScriptStep(name=\"prep\",\n                              script_name=\"prep_script.py\",             \n                             source_directory='.\/',                \n                              arguments=['--ipdata',in_data.as_named_input(\"inparquet\"),\n                                         '--oppath',prep_data],\n                              outputs=[prep_data],\n                              compute_target = comp_cluster,\n                              runconfig =pipe_config,\n                              allow_reuse=True)\n train_step = PythonScriptStep(name='train',\n                               script_name='train_script.py',\n                               source_directory='.\/',\n                               arguments=['--prepdata',prep_data],\n                               inputs=[prep_data],\n                               compute_target=comp_cluster,\n                               runconfig=pipe_config,\n                               allow_reuse=True)\n # get experiment and run pipeline\n # Construct the pipeline\n pipeline_steps = [prep_step, train_step]\n pipeline = Pipeline(workspace=ws, steps=pipeline_steps,default_datastore='parquet_ingestion')\n exp =Experiment(ws,'amltryout')\n pipeline_run = exp.submit(pipeline, regenerate_outputs=True)\n\n\n\nIn the prep_script.py, I have the following :\n\n from azureml.core import Experiment,Workspace,Datastore,Dataset\n import pandas as pd\n import argparse\n from azureml.core import Run\n from sklearn import preprocessing\n import os\n    \n parse = argparse.ArgumentParser()\n parse.add_argument('--ipdata')\n parse.add_argument('--oppath')\n args = parse.parse_args()\n save_folder = args.oppath\n    \n run = Run.get_context()\n    \n #https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/work-with-data-in-aml\/5-using-datasets\n pqdata = run.input_datasets['inparquet'].to_pandas_dataframe()\n # ws = run.experiment.workspace\n # pqdata = Dataset.get_by_id(ws, id='testparquet')\n run.log(\"count\",pqdata.count())\n    \n sel_data = pqdata[['title','country','salary']]\n str_enc = preprocessing.LabelEncoder()\n    \n sel_data['title','country'] = str_enc.fit_transform(sel_data['title','country']) \n    \n    \n os.makedirs(save_folder, exist_ok=True)\n save_path = os.path.join(save_folder,'data_prep.csv')\n sel_data.to_csv(save_path)\n    \n run.complete()\n\n\n\n\nAlso some steps in train_script , but when I submit the pipeline it fails at prep step with the following error :\n\nAuthentication failed for Container Registry: 3495c636784040b2ae0f5c8e7ee4133a.azurecr.io.\n\nAm I missing something here?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623207954850,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/427884\/getting-authenticatin-error-for-python-script-step.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-09T13:11:50.927Z",
                "Answer_score":1,
                "Answer_body":"@RaghuvarranVH-5191 Thanks for the question. Can you please add more details about the Docker Container Registry that you are trying and share error log snapshot to check.\nTo Initialize Workspace, Please follow the below doc.\nInitialize a workspace object from persisted configuration.\n\n ws = Workspace.from_config()\n print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n\n\n\nFor Azure machine learning pipelines quickstart please follow the below notebook.\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-getting-started.ipynb",
                "Answer_comment_count":5,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get authenticatin error python script step pipelin try creat azur pipelin learn purpos follow step mention notebook http github com microsoftlearn mslearn blob main creat pipelin ipynb connect workspac config us servic princip authent http github com azur machinelearningnotebook blob master us manag servic authent authent ipynb azur svppwd environ passwd us environ passwd azur servic client secret auth pwd serviceprincipalauthent tenant fedad servic princip bac cbb servic princip password azur svppwd workspac config config auth auth pwd print workspac locat format locat data dataset data dataset testparquet dataset regist azur refer parquet file contain creat environ env environ conda specif azenv file path envspec yml env regist save environ futur us retreiv creat comput target comput cluster try comp cluster computetarget amltryoutclust print amltryoutclust exist computetargetexcept comp config amlcomput provis configur size standard min node max node size line indic type comput pow need comp cluster computetarget creat amltryoutclust comp config comp cluster wait complet output true creat pipelin data object step prep data pipelinedata pipedata datastor default datastor creat pipelin step pipelin configur pipe config runconfigur pipe config target comp cluster pipe config environ environ azenv prep step pythonscriptstep prep script prep script sourc directori argument ipdata data name input inparquet oppath prep data output prep data comput target comp cluster runconfig pipe config allow reus true train step pythonscriptstep train script train script sourc directori argument prepdata prep data input prep data comput target comp cluster runconfig pipe config allow reus true experi run pipelin construct pipelin pipelin step prep step train step pipelin pipelin workspac step pipelin step default datastor parquet ingest exp experi amltryout pipelin run exp submit pipelin regener output true prep script follow core import experi workspac datastor dataset import panda import argpars core import run sklearn import preprocess import pars argpars argumentpars pars add argument ipdata pars add argument oppath arg pars pars arg save folder arg oppath run run context http doc microsoft com learn modul work data aml dataset pqdata run input dataset inparquet panda datafram run experi workspac pqdata dataset testparquet run log count pqdata count sel data pqdata titl countri salari str enc preprocess labelencod sel data titl countri str enc fit transform sel data titl countri makedir save folder exist true save path path join save folder data prep csv sel data csv save path run complet step train script submit pipelin fail prep step follow error authent fail contain registri cbaefceeea azurecr miss",
        "Question_preprocessed_content":"get authenticatin error python script step pipelin try creat azur pipelin learn purpos follow step mention notebook connect workspac config us servic princip authent us azur servic client secret serviceprincipalauthent print data dataset dataset regist azur refer parquet file contain creat environ env save environ futur us retreiv creat comput target comput cluster try computetarget print computetargetexcept line indic type comput pow need creat pipelin data object step creat pipelin step pipelin configur runconfigur pythonscriptstep runconfig pythonscriptstep experi run pipelin construct pipelin pipelin pipelin exp experi follow core import experi workspac datastor dataset import panda import argpars core import run sklearn import preprocess import pars arg run pqdata pqdata testparquet pqdata step submit pipelin fail prep step follow error authent fail contain registri miss",
        "Question_gpt_summary_original":"The user is encountering an authentication error while trying to create an Azure pipeline for learning purposes. The user followed the steps mentioned in a notebook and is using service principal authentication. The pipeline involves creating an environment, compute target, pipeline data object, and steps. The error occurs in the prep_script.py file, where the user is trying to access a dataset and perform some data preprocessing. The error message indicates that authentication failed for the container registry.",
        "Question_gpt_summary":"user encount authent error try creat azur pipelin learn purpos user follow step mention notebook servic princip authent pipelin involv creat environ comput target pipelin data object step error occur prep script file user try access dataset perform data preprocess error messag indic authent fail contain registri",
        "Answer_original_content":"raghuvarranvh thank question add detail docker contain registri try share error log snapshot check initi workspac follow doc initi workspac object persist configur workspac config print resourc group locat subscript sep pipelin quickstart follow notebook http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin get start ipynb",
        "Answer_preprocessed_content":"thank question add detail docker contain registri try share error log snapshot check initi workspac follow doc initi workspac object persist configur sep pipelin quickstart follow notebook",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the authentication error encountered when attempting to create an azure pipeline. the answer requests for more details about the docker container registry and error log snapshot to check. it also provides a link to initialize a workspace object from persisted configuration and a notebook for pipelines quickstart.",
        "Answer_gpt_summary":"solut provid answer authent error encount attempt creat azur pipelin answer request detail docker contain registri error log snapshot check provid link initi workspac object persist configur notebook pipelin quickstart"
    },
    {
        "Question_id":null,
        "Question_title":"WandB not using user PID when updating",
        "Question_body":"<p>Hello,<\/p>\n<p>I used  <code>tempfile.mkdtemp() <\/code> to create a temporary directory for my runs (as I don\u2019t want a persistent folder with tons of runs)<\/p>\n<p>For training everything works fine but when resuming the run to do some validation \/ evaluation updates, and using <code>run.summary.update({\"key\": value})<\/code> I got a<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Path \/tmp\/tmpq5uafy4d\/wandb\/ wasn't writable, using system temp directory\n<\/code><\/pre>\n<p>with obviously<\/p>\n<pre><code class=\"lang-auto\">File \"\/mnt\/Projets\/nlp\/.venv\/lib\/python3.9\/site-packages\/wandb\/sdk\/internal\/sender.py\", line 855, in _update_summary\n    with open(summary_path, \"w\") as f:\nFileNotFoundError: [Errno 2] No such file or directory: '\/tmp\/tmpq5uafy4d\/wandb\/run-20211102_153311-37264m5k\/files\/wandb-summary.json'\n<\/code><\/pre>\n<p>As in the doc of <a href=\"https:\/\/docs.python.org\/3.9\/library\/tempfile.html#tempfile.mkdtemp\" rel=\"noopener nofollow ugc\"><code>mkdtemp<\/code><\/a> :<\/p>\n<pre><code class=\"lang-auto\"> The directory is readable, writable, and searchable only by the creating user ID.\n<\/code><\/pre>\n<p>So I guess WandB is not using the user ID and thus is not able to write in the directory for updating.<br>\nNote that this directory is different from the training one (as it\u2019s random at each init)<\/p>\n<p>Thanks in advance for any help.<br>\nHave a great day.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1635882593147,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":289.0,
        "Answer_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-not-using-user-pid-when-updating\/1204",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-03T15:31:05.874Z",
                "Answer_body":"<p>For the posterity :<\/p>\n<p>After searching for a long time with it, wandb firstly deletes my class object (which call deletion of the temp folder) <strong>and then<\/strong> try to update the run.<\/p>\n<p>To avoid that you need to first call <code>run.finish() or wandb.finish()<\/code> which will first update and then delete your object (or let the garbage collector do it)  thus it will be synced before the object is destroyed and the temp file removed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T16:45:59.689Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ierezell\">@ierezell<\/a>, there are some reasons why you don\u2019t want to automatically delete your run folders, especially if there is an issue with the run. However, I do understand your desire to manage the clutter in your file system. It sounds like having a feature where you could tell wandb to delete the local files it created after a successful run would be the most preferable option for you. Currently, calling <code>wandb sync --clean<\/code> will sync any unsynced runs and then remove those run folders from your computer. The wandb dir will still be there and there but it will only be taking up bytes of space as most of the information will have been deleted. If you still find it annoying to have mostly empty <code>.\/wandb<\/code> folders in your project dirs, you can set the <code>WANDB_DIR<\/code> environment variable to an absolute path where all of your run data will be stored until you call <code>wandb sync --clean<\/code>.<\/p>",
                "Answer_score":20.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T18:08:15.337Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a>,<\/p>\n<p>I didn\u2019t know the <code>wandb sync --clean<\/code> option!<br>\nIt\u2019s quite what I wanted to do, saving space and your solution will only delete finished runs which is nice <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>I don\u2019t mind deleting failed run as I can just relaunch them again (it\u2019s small models on only one machine). This is why I put all my wandb folders in <code>\/tmp<\/code> which means I keep the folder until I reboot.<\/p>\n<p>Is there any option to have the <code>clean<\/code> feature enabled by default? I mean for any run if sync is complete: delete the folder.<\/p>\n<p>Thanks for the response, I guess it solves it but we can continue discussing it.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T18:21:20.198Z",
                "Answer_body":"<p>As of now, there isn\u2019t an option to enable that by default. One issue you should be aware of is that if you are currently logging a run when you call <code>wandb sync --clean<\/code> bad things will happen. We\u2019re working on improving the the robustness of these features and will likely support an automatic clean option in the future.<\/p>",
                "Answer_score":15.4,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-01-02T18:22:08.008Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1635963680198,
        "Question_original_content":"user pid updat hello tempfil mkdtemp creat temporari directori run dont want persist folder ton run train work fine resum run valid evalu updat run summari updat kei valu got warn path tmp tmpquafyd wasn writabl temp directori obvious file mnt projet nlp venv lib python site packag sdk intern sender line updat summari open summari path filenotfounderror errno file directori tmp tmpquafyd run file summari json doc mkdtemp directori readabl writabl searchabl creat user guess user abl write directori updat note directori differ train random init thank advanc help great dai",
        "Question_preprocessed_content":"user pid updat hello creat temporari directori run train work fine resum run valid evalu updat got obvious doc guess user abl write directori updat note directori differ train thank advanc help great dai",
        "Question_gpt_summary_original":"The user encountered a challenge when using WandB to update their runs. They created a temporary directory using tempfile.mkdtemp() for their runs, but when resuming the run to do some validation\/evaluation updates and using run.summary.update({\"key\": value}), they received a warning that the path wasn't writable, and a FileNotFoundError occurred. The user suspects that WandB is not using the user ID and thus is not able to write in the directory for updating.",
        "Question_gpt_summary":"user encount challeng updat run creat temporari directori tempfil mkdtemp run resum run valid evalu updat run summari updat kei valu receiv warn path wasn writabl filenotfounderror occur user suspect user abl write directori updat",
        "Answer_original_content":"isnt option enabl default issu awar current log run sync clean bad thing happen work improv robust featur like support automat clean option futur",
        "Answer_preprocessed_content":"isnt option enabl default issu awar current log run bad thing happen work improv robust featur like support automat clean option futur",
        "Answer_gpt_summary_original":"there is currently no option to enable the temporary directory created by tempfile.mkdtemp() to be writable by the user's id. however, the team is working on improving the robustness of these features and may support an automatic clean option in the future. it is important to note that if the user is currently logging a run when they call sync --clean, bad things will happen.",
        "Answer_gpt_summary":"current option enabl temporari directori creat tempfil mkdtemp writabl user team work improv robust featur support automat clean option futur import note user current log run sync clean bad thing happen"
    },
    {
        "Question_id":51933366.0,
        "Question_title":"Version management in SageMaker",
        "Question_body":"<p>We're currently working with 3 employees in the same notebook-instance, however, since this is a shared workspace this makes version management extra difficult. Is it possible to link aws credentials to your git account from within SageMaker? Or are there any other ways recommended for version management? <\/p>\n\n<p>Right now we're using a single git account for committing the code from within jupyter terminal. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1534776705277,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":836.0,
        "Answer_body":"<p>The situation has changed : Git is now <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/\" rel=\"nofollow noreferrer\">available<\/a> in SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51933366",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1543573965692,
        "Question_original_content":"version manag current work employe notebook instanc share workspac make version manag extra difficult possibl link aw credenti git account wai recommend version manag right singl git account commit code jupyt termin",
        "Question_preprocessed_content":"version manag current work employe share workspac make version manag extra difficult possibl link aw credenti git account wai recommend version manag right singl git account commit code jupyt termin",
        "Question_gpt_summary_original":"The user is facing challenges with version management while working with 3 employees in a shared workspace in SageMaker. They are looking for ways to link AWS credentials to their git account or any other recommended methods for version management. Currently, they are using a single git account for committing the code from within the Jupyter terminal.",
        "Question_gpt_summary":"user face challeng version manag work employe share workspac look wai link aw credenti git account recommend method version manag current singl git account commit code jupyt termin",
        "Answer_original_content":"situat chang git avail",
        "Answer_preprocessed_content":"situat chang git avail",
        "Answer_gpt_summary_original":"possible solution: the user can use the git version control system within jupyter to manage their code and collaborate with their colleagues.",
        "Answer_gpt_summary":"possibl solut user us git version control jupyt manag code collabor colleagu"
    },
    {
        "Question_id":null,
        "Question_title":"Model Serving support on-prem implementation and dockers",
        "Question_body":"Hello All,\n\n\nCould anyone suggest me how to use a model serving in an on-prem setup (outside a databricks environment)?\n\n\nAlso like to understand how to deploy in Kubernetes and Containers \/Pod? My requirement is follows.we have built some models and are selling to multiple customers where each one ask to use a certain setup i.e. their own container \/ pod setup or private cloud etc...\n\n\nThanking anticipation.\n\n\nRegards,\nNagaraj",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1618699040000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":19.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/rA7itpkqVug",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-18T13:07:23",
                "Answer_body":"This article\u2019s author, in his search for a scalable model serving solution, surveys and discusses various aspects of model servings, in general. In particular, Seldon, he concludes is his best choice.\u00a0\n\n\nAnd Seldon core offers strong integration with MLflow.\u00a0\n\n\nAlso, there is a FR request to integrate MLflow with KFserving\u00a0https:\/\/github.com\/mlflow\/mlflow\/issues\/1465\n\n\nCheers\nJules\u00a0\n\n\nhttps:\/\/link.medium.com\/jNHdxGtQyfb\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn Apr 17, 2021, at 7:37 PM, Nagaraj S <snagar...@gmail.com> wrote:\n\n\n\ufeff\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAJgr-o6DK9rsV-WNjKdHiKdJTrwCOqg1GBGT%2Bexg206yUqy_Jw%40mail.gmail.com."
            },
            {
                "Answer_creation_time":"2021-04-18T13:12:47",
                "Answer_body":"Also, note that there are few community deployment pluggins for MLfow model deployemnts:\n\n\nhttps:\/\/mlflow.org\/docs\/latest\/plugins.html#deployment-plugins\n\n\nCheers\u00a0\nJules\u00a0\n\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn Apr 18, 2021, at 10:07 AM, Jules Damji <ju...@databricks.com> wrote:\n\n\n\ufeffThis article\u2019s author, in his search for a scalable model serving solution, surveys and discusses various aspects of model servings, in general. In particular, Seldon, he concludes is his best choice.\u00a0\n\ue5d3"
            },
            {
                "Answer_creation_time":"2021-04-18T16:24:05",
                "Answer_body":"Thank you for sharing this.\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"model serv support prem implement docker hello suggest us model serv prem setup outsid databrick environ like understand deploi kubernet contain pod requir follow built model sell multipl custom ask us certain setup contain pod setup privat cloud thank anticip regard nagaraj",
        "Question_preprocessed_content":"model serv support implement docker hello suggest us model serv setup like understand deploi kubernet contain requir built model sell multipl custom ask us certain setup contain pod setup privat cloud thank anticip regard nagaraj",
        "Question_gpt_summary_original":"The user is seeking advice on how to use model serving in an on-premises setup outside of a Databricks environment. They also want to know how to deploy in Kubernetes and Containers\/Pod, as they have built models that are being sold to multiple customers who require their own container\/pod setup or private cloud.",
        "Question_gpt_summary":"user seek advic us model serv premis setup outsid databrick environ want know deploi kubernet contain pod built model sold multipl custom requir contain pod setup privat cloud",
        "Answer_original_content":"articl author search scalabl model serv solut survei discuss aspect model serv gener particular seldon conclud best choic seldon core offer strong integr request integr kfservinghttp github com issu cheer jule http link medium com jnhdxgtqyfb sent iphon pardon dumb thumb typo apr nagaraj wrote receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user cajgr odkrsv wnjkdhikdjtrwcoqggbgt bexgyuqi mail gmail com note commun deploy pluggin mlfow model deployemnt http org doc latest plugin html deploy plugin cheer jule sent iphon pardon dumb thumb typo apr jule damji wrote articl author search scalabl model serv solut survei discuss aspect model serv gener particular seldon conclud best choic thank share",
        "Answer_preprocessed_content":"articl author search scalabl model serv solut survei discuss aspect model serv gener particular seldon conclud best choic seldon core offer strong integr request integr cheer jule sent iphon pardon dumb thumb typo apr nagaraj wrote receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit note commun deploy pluggin mlfow model deployemnt cheer jule sent iphon pardon dumb thumb typo apr jule damji wrote articl author search scalabl model serv solut survei discuss aspect model serv gener particular seldon conclud best choic thank share",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using Seldon as the best choice for a scalable model serving solution. It also mentions that Seldon core offers strong integration with MLflow. Additionally, there is a feature request to integrate MLflow with KFserving. The discussion also mentions community deployment plugins for MLflow model deployments.",
        "Answer_gpt_summary":"solut discuss suggest seldon best choic scalabl model serv solut mention seldon core offer strong integr addition featur request integr kfserv discuss mention commun deploy plugin model deploy"
    },
    {
        "Question_id":null,
        "Question_title":"Is there a solution for multi-user Notebook on SageMaker?",
        "Question_body":"Is there a solution for multi-user Notebook on Studio Notebook or Notebook Instances? Eg if we want several developers to interact on the same notebook at the same time",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592989945000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":457.0,
        "Answer_body":"Notebook instances are not connected to the user. So if two users has the same access rights they will see and will be able to access the same instance (even in the same time).\n\nThe issue is - Jupyter Notebook is not ready for that, both users will have the same privileges, no tracking who did what, ... And working on the same notebook on the same time - basically they will overwrite each other saves.\n\nI had a need for similar thing (pair programming - data scientist and software engineer) - the only viable solution we were able to find was desktop sharing (like TeamViewer, ...)",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4mxRvXy2QYmkYCvdqNVa2g\/is-there-a-solution-for-multi-user-notebook-on-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-24T14:42:53.000Z",
                "Answer_score":0,
                "Answer_body":"Notebook instances are not connected to the user. So if two users has the same access rights they will see and will be able to access the same instance (even in the same time).\n\nThe issue is - Jupyter Notebook is not ready for that, both users will have the same privileges, no tracking who did what, ... And working on the same notebook on the same time - basically they will overwrite each other saves.\n\nI had a need for similar thing (pair programming - data scientist and software engineer) - the only viable solution we were able to find was desktop sharing (like TeamViewer, ...)",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1593009773000,
        "Question_original_content":"solut multi user notebook solut multi user notebook studio notebook notebook instanc want develop interact notebook time",
        "Question_preprocessed_content":"solut notebook solut notebook studio notebook notebook instanc want develop interact notebook time",
        "Question_gpt_summary_original":"The user is facing a challenge of finding a solution for multi-user Notebook on SageMaker, specifically on Studio Notebook or Notebook Instances, to allow several developers to interact on the same notebook simultaneously.",
        "Question_gpt_summary":"user face challeng find solut multi user notebook specif studio notebook notebook instanc allow develop interact notebook simultan",
        "Answer_original_content":"notebook instanc connect user user access right abl access instanc time issu jupyt notebook readi user privileg track work notebook time basic overwrit save need similar thing pair program data scientist softwar engin viabl solut abl desktop share like teamview",
        "Answer_preprocessed_content":"notebook instanc connect user user access right abl access instanc issu jupyt notebook readi user privileg track work notebook time basic overwrit save need similar thing viabl solut abl desktop share",
        "Answer_gpt_summary_original":"possible solutions to enable multiple users to interact on the same notebook in studio notebook or notebook instances are limited. while multiple users with the same access rights can access the same instance, jupyter notebook is not designed for multiple users to work on the same notebook simultaneously. this can lead to overwriting each other's saves and no tracking of who did what. one possible solution is desktop sharing tools like teamviewer.",
        "Answer_gpt_summary":"possibl solut enabl multipl user interact notebook studio notebook notebook instanc limit multipl user access right access instanc jupyt notebook design multipl user work notebook simultan lead overwrit save track possibl solut desktop share tool like teamview"
    },
    {
        "Question_id":null,
        "Question_title":"AWS SageMaker Notebook Instance is not continuing running the cell when I leave my Laptop to execute the cells over a period of time. Can you tell me how can I solve this?",
        "Question_body":"Hello, When I leave the SageMaker Jupyter notebook to execute the cells for a long period of time, after about 8 hours, it is signing me out of the console and when I log back in, it is not running\/continuing its execution of the cells. There was No error message displayed when this happened. When I left the notebook running, the laptop was ON and the screen was active. Could you tell me how can I leave the notebook to run for a long period of time without worrying about being active on the laptop all the time? Thank you.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660857294691,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":105.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPbfNFX-sQhmUncaRIpCw_A\/aws-sage-maker-notebook-instance-is-not-continuing-running-the-cell-when-i-leave-my-laptop-to-execute-the-cells-over-a-period-of-time-can-you-tell-me-how-can-i-solve-this",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-25T12:02:51.397Z",
                "Answer_score":0,
                "Answer_body":"You may review this blog https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-a-custom-entity-recognizer-for-pdf-documents-using-amazon-comprehend\/ . It has a detailed example now how to build customer recognizer for PDF documents on Amazon Comprehend.\n\nTo create the annotation for the PDF documents, it does it Amazon Ground Truth.\n\nHope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-09T04:56:00.842Z",
                "Answer_score":0,
                "Answer_body":"Hello - if you are being signed out of the AWS console, you may need to set a longer IAM session duration, e.g. to 12 hours.\n\nIf you are not being logged out of AWS console and your SageMaker notebook is still timing out, you may need to run your scripts using nohup command or use SageMaker training so training will continue even if you get disconnected from your Sagemaker notebook.\n\nHope this helps. Thank you!",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"notebook instanc continu run cell leav laptop execut cell period time tell solv hello leav jupyt notebook execut cell long period time hour sign consol log run continu execut cell error messag displai happen left notebook run laptop screen activ tell leav notebook run long period time worri activ laptop time thank",
        "Question_preprocessed_content":"notebook instanc continu run cell leav laptop execut cell period time tell solv hello leav jupyt notebook execut cell long period time hour sign consol log execut cell error messag displai happen left notebook run laptop screen activ tell leav notebook run long period time worri activ laptop time thank",
        "Question_gpt_summary_original":"The user is facing a challenge with AWS SageMaker Notebook Instance as it signs them out of the console after about 8 hours of leaving the notebook to execute cells. When the user logs back in, the notebook does not continue its execution of the cells without displaying any error message. The user is seeking a solution to leave the notebook running for a long period of time without worrying about being active on the laptop all the time.",
        "Question_gpt_summary":"user face challeng notebook instanc sign consol hour leav notebook execut cell user log notebook continu execut cell displai error messag user seek solut leav notebook run long period time worri activ laptop time",
        "Answer_original_content":"review blog http aw amazon com blog machin learn build custom entiti recogn pdf document amazon comprehend detail exampl build custom recogn pdf document amazon comprehend creat annot pdf document amazon ground truth hope help hello sign aw consol need set longer iam session durat hour log aw consol notebook time need run script nohup command us train train continu disconnect notebook hope help thank",
        "Answer_preprocessed_content":"review blog detail exampl build custom recogn pdf document amazon comprehend creat annot pdf document amazon ground truth hope help hello sign aw consol need set longer iam session durat hour log aw consol notebook time need run script nohup command us train train continu disconnect notebook hope help thank",
        "Answer_gpt_summary_original":"the answer does not provide any solutions for the issue of jupyter notebook instance not continuing to run the cells when the user leaves their laptop. however, it suggests setting a longer iam session duration if the user is being signed out of the aws console. additionally, it recommends using the nohup command or training to ensure that the training continues even if the user gets disconnected from the notebook.",
        "Answer_gpt_summary":"answer provid solut issu jupyt notebook instanc continu run cell user leav laptop suggest set longer iam session durat user sign aw consol addition recommend nohup command train ensur train continu user get disconnect notebook"
    },
    {
        "Question_id":50271174.0,
        "Question_title":"How to make a Python Visualization as service | Integrate with website | specially sagemaker",
        "Question_body":"<p>I am from R background where we can use Plumber kind tool which provide visualization\/graph as Image via end points so we can integrate in our Java application.<\/p>\n\n<p>Now I want to integrate my Python\/Juypter visualization graph with my Java application but not sure how to host it and make it as endpoint. Right now I using AWS sagemaker to host Juypter notebook<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1525949293883,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. <\/p>\n\n<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn\u2019t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50271174",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1526073355692,
        "Question_original_content":"python visual servic integr websit special background us plumber kind tool provid visual graph imag end point integr java applic want integr python juypter visual graph java applic sure host endpoint right host juypter notebook",
        "Question_preprocessed_content":"python visual servic integr websit special background us plumber kind tool provid imag end point integr java applic want integr visual graph java applic sure host endpoint right host juypter notebook",
        "Question_gpt_summary_original":"The user is facing a challenge in integrating their Python\/Jupyter visualization graph with their Java application. They are unsure of how to host it and make it an endpoint, and are currently using AWS Sagemaker to host their Jupyter notebook. They are familiar with using a tool called Plumber in R, which provides visualization\/graph as an image via endpoints for integration with Java applications.",
        "Question_gpt_summary":"user face challeng integr python jupyt visual graph java applic unsur host endpoint current host jupyt notebook familiar tool call plumber provid visual graph imag endpoint integr java applic",
        "Answer_original_content":"set differ servic data scientist notebook servic develop model interact wai host servic creat endpoint base train model endpoint invok endpoint api real time infer look differ type host suitabl serv html media rich page doesnt fit host model combin instanc pre built ami instal script congnito authent eb object block storag similar build block scalabl cost effect solut",
        "Answer_preprocessed_content":"set differ servic data scientist notebook servic develop model interact wai host servic creat endpoint base train model endpoint api real time infer look differ type host suitabl serv html media rich page doesnt fit host model combin instanc ami instal script congnito authent eb object block storag similar build block scalabl cost effect solut",
        "Answer_gpt_summary_original":"possible solutions for integrating python\/jupyter visualizations into a java application and hosting it as an endpoint include using a combination of ec2 instances, pre-built ami or installation scripts, cognito for authentication, s3 and ebs for object and block storage, and similar building blocks to create a scalable and cost-effective solution. the hosting service in the answer is not suitable for serving html media-rich pages.",
        "Answer_gpt_summary":"possibl solut integr python jupyt visual java applic host endpoint includ combin instanc pre built ami instal script cognito authent eb object block storag similar build block creat scalabl cost effect solut host servic answer suitabl serv html media rich page"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML AutoML : Data transformation diagram only 1 column",
        "Question_body":"Hello, I am using the AutoML of Azure ML. I don't understand the diagram of Data transformation (that is still in preview). It tells me that I start with 26 columns, which is correct, but then says I'm ending up with 1 column only, after a MeanImputer. ![114216-1column.png][1] If I check the engineered features in the code, I get this table, so 26 columns with the application of MeanImputer for each of them. ![114215-allcolumns.png][2] Could you tell me why the diagram tells me that there is only 1 column at the end? [1]: \/answers\/storage\/attachments\/114216-1column.png [2]: \/answers\/storage\/attachments\/114215-allcolumns.png",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1626184752807,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/474091\/azure-ml-automl-data-transformation-diagram-only-1.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-14T19:36:10.617Z",
                "Answer_score":0,
                "Answer_body":"Thanks for pointing this out. This feature is still in preview, I followed up with the product team, we currently have a work item to improve this feature (we don't have an ETA at the moment). Please disregard the summary workflow for now. Will follow-up with updates accordingly. Hope this helps!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"automl data transform diagram column hello automl understand diagram data transform preview tell start column correct sai end column meanimput column png check engin featur code tabl column applic meanimput allcolumn png tell diagram tell column end answer storag attach column png answer storag attach allcolumn png",
        "Question_preprocessed_content":"automl data transform diagram column hello automl understand diagram data transform tell start column correct sai end column meanimput check engin featur code tabl column applic meanimput tell diagram tell column end",
        "Question_gpt_summary_original":"The user is facing a challenge with the Data transformation diagram in Azure ML AutoML, which shows only 1 column after applying MeanImputer, despite the user having 26 columns in the code. The user is seeking clarification on why the diagram is showing only 1 column.",
        "Question_gpt_summary":"user face challeng data transform diagram automl show column appli meanimput despit user have column code user seek clarif diagram show column",
        "Answer_original_content":"thank point featur preview follow product team current work item improv featur eta moment disregard summari workflow follow updat accordingli hope help",
        "Answer_preprocessed_content":"thank point featur preview follow product team current work item improv featur disregard summari workflow updat accordingli hope help",
        "Answer_gpt_summary_original":"there are currently no solutions to the challenge with the automl data transformation diagram. the product team is aware of the issue and is working on improving the feature, but there is no estimated time of arrival for the fix. the user should disregard the summary workflow for now and wait for updates.",
        "Answer_gpt_summary":"current solut challeng automl data transform diagram product team awar issu work improv featur estim time arriv fix user disregard summari workflow wait updat"
    },
    {
        "Question_id":null,
        "Question_title":"Undelete runs no longer available?",
        "Question_body":"<p>Hi,<br>\nI deleted a run by accident and tried to recover it. However, I noticed there is undelete all runs option available from the dot menu on the overview page of the project.<br>\nIs there a way to recover a deleted run?<br>\nThank you.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1668549759237,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":110.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/summer5e\">@summer5e<\/a> thank you for reporting this. The option to undelete runs should be available when you go to the Project\u2019s overview page, and not in the Runs overview level. I have attached a screenshot of an example, the option can be found when you click on the three vertical dots:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6.png\" data-download-href=\"\/uploads\/short-url\/pKrzn3Dflv0FWE8GGBfCE4JTMhM.png?dl=1\" title=\"Screenshot 2022-11-16 at 12.02.29\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_690x174.png\" alt=\"Screenshot 2022-11-16 at 12.02.29\" data-base62-sha1=\"pKrzn3Dflv0FWE8GGBfCE4JTMhM\" width=\"690\" height=\"174\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_690x174.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_1035x261.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_1380x348.png 2x\" data-dominant-color=\"212122\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-11-16 at 12.02.29<\/span><span class=\"informations\">2845\u00d7719 123 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Could you please let me know the name of your project to look further into this?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/undelete-runs-no-longer-available\/3420",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-16T12:05:29.286Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/summer5e\">@summer5e<\/a> thank you for reporting this. The option to undelete runs should be available when you go to the Project\u2019s overview page, and not in the Runs overview level. I have attached a screenshot of an example, the option can be found when you click on the three vertical dots:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6.png\" data-download-href=\"\/uploads\/short-url\/pKrzn3Dflv0FWE8GGBfCE4JTMhM.png?dl=1\" title=\"Screenshot 2022-11-16 at 12.02.29\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_690x174.png\" alt=\"Screenshot 2022-11-16 at 12.02.29\" data-base62-sha1=\"pKrzn3Dflv0FWE8GGBfCE4JTMhM\" width=\"690\" height=\"174\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_690x174.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_1035x261.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/b\/b4766106065aa247f2d9930f131e3dab402136a6_2_1380x348.png 2x\" data-dominant-color=\"212122\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-11-16 at 12.02.29<\/span><span class=\"informations\">2845\u00d7719 123 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Could you please let me know the name of your project to look further into this?<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-11-16T18:15:55.291Z",
                "Answer_body":"<p>Thank you! I was clicking on the overview in the runs panel not information panel.<br>\nThis solved my problem.<\/p>",
                "Answer_score":10.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-16T19:13:36.556Z",
                "Answer_body":"<p>Glad to hear you got back your deleted run data <a class=\"mention\" href=\"\/u\/summer5e\">@summer5e<\/a>! I will close the ticket for now, and please reach out if you run into any other issue.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-15T19:13:48.578Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1668600329286,
        "Question_original_content":"undelet run longer avail delet run accid tri recov notic undelet run option avail dot menu overview page project wai recov delet run thank",
        "Question_preprocessed_content":"undelet run longer avail delet run accid tri recov notic undelet run option avail dot menu overview page project wai recov delet run thank",
        "Question_gpt_summary_original":"The user accidentally deleted a run and is unable to recover it as the undelete all runs option is no longer available on the overview page of the project. They are seeking assistance in recovering the deleted run.",
        "Question_gpt_summary":"user accident delet run unabl recov undelet run option longer avail overview page project seek assist recov delet run",
        "Answer_original_content":"summer thank report option undelet run avail project overview page run overview level attach screenshot exampl option click vertic dot screenshot let know project look",
        "Answer_preprocessed_content":"thank report option undelet run avail project overview page run overview level attach screenshot exampl option click vertic dot screenshot let know project look",
        "Answer_gpt_summary_original":"the answer suggests that the option to undelete runs is available on the project overview page, not the runs overview level. the solution is to click on the three vertical dots to find the undelete option. the user is also asked to provide the name of their project for further investigation.",
        "Answer_gpt_summary":"answer suggest option undelet run avail project overview page run overview level solut click vertic dot undelet option user ask provid project investig"
    },
    {
        "Question_id":36917948.0,
        "Question_title":"Feature weightage from Azure Machine Learning Deployed Web Service",
        "Question_body":"<p>I am trying to predict from my past data which has around 20 attribute columns and a label. Out of those 20, only 4 are significant for prediction. But i also want to know that if a row falls into one of the classified categories, what other important correlated columns apart from those 4 and what are their weight. I want to get that result from my deployed web service on Azure.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1461854370743,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":303.0,
        "Answer_body":"<p>You can use permutation feature importance module but that will give importance of the features across the sample set. Retrieving the weights on per call basis is not available in Azure ML.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36917948",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1461985174656,
        "Question_original_content":"featur weightag deploi web servic try predict past data attribut column label signific predict want know row fall classifi categori import correl column apart weight want result deploi web servic azur",
        "Question_preprocessed_content":"featur weightag deploi web servic try predict past data attribut column label signific predict want know row fall classifi categori import correl column apart weight want result deploi web servic azur",
        "Question_gpt_summary_original":"The user is facing a challenge in determining the weightage of correlated columns apart from the significant 4 columns for prediction, in order to get accurate results from their deployed web service on Azure.",
        "Question_gpt_summary":"user face challeng determin weightag correl column apart signific column predict order accur result deploi web servic azur",
        "Answer_original_content":"us permut featur import modul import featur sampl set retriev weight basi avail",
        "Answer_preprocessed_content":"us permut featur import modul import featur sampl set retriev weight basi avail",
        "Answer_gpt_summary_original":"possible solution: the user can use the permutation feature importance module to determine the importance of each feature in their deployed web service on azure. however, this module will only give the importance of the features across the sample set, and retrieving the weights on a per-call basis is not available.",
        "Answer_gpt_summary":"possibl solut user us permut featur import modul determin import featur deploi web servic azur modul import featur sampl set retriev weight basi avail"
    },
    {
        "Question_id":35973168.0,
        "Question_title":"How could I save dataset from ipython notebook in Azure ML Studio?",
        "Question_body":"<p>I use next command to save output results:<\/p>\n\n<pre><code>ws.datasets.add_from_dataframe(data, 'GenericCSV', 'output.csv', 'Uotput results')\n<\/code><\/pre>\n\n<p>where <code>ws<\/code> is <code>azureml.Workspace<\/code> object and <code>data<\/code> is <code>pandas.DataFrame<\/code>.<\/p>\n\n<p>It works fine if my dataset size less than 4 mb. Otherwise I got a error:<\/p>\n\n<pre><code>AzureMLHttpError: Maximum request length exceeded.\n<\/code><\/pre>\n\n<p>As I understood this is the error raised by Azure environment limits and the maximum size of the dataset could not be changed. <\/p>\n\n<p>I could split my dataset to 4 mb parts and download them from Azure ML studio, but it is very inconvinient if size of my output dataset is more than 400 mb.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1457889087747,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1457963855452,
        "Question_score":4.0,
        "Question_view_count":3128.0,
        "Answer_body":"<p>I have read the source code in the python package <strong>azureml<\/strong>, and found out that they are using a simple request post when uploading a dataset, which has a limited content length 4194304 bytes.<\/p>\n\n<p>I tried to modify the code inside \"http.py\" within the python package <strong>azureml<\/strong>. I posted the request with a chunked data, and I got the following error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \".\\azuremltest.py\", line 10, in &lt;module&gt;\n    ws.datasets.add_from_dataframe(frame, 'GenericCSV', 'output2.csv', 'Uotput results')\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 507, in add_from_dataframe\n    return self._upload(raw_data, data_type_id, name, description)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 550, in _upload\nraw_data, None)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 135, in upload_dataset\n    upload_result = self._send_post_req(api_path, raw_data)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 197, in _send_post_req\n    raise AzureMLHttpError(response.text, response.status_code)\nazureml.errors.AzureMLHttpError: Chunked transfer encoding is not permitted. Upload size must be indicated in the Content-Length header.\nRequest ID: 7b692d82-845c-4106-b8ec-896a91ecdf2d 2016-03-14 04:32:55Z\n<\/code><\/pre>\n\n<p>The REST API in <strong>azureml<\/strong> package does not support chunked transfer encoding. Hence, I took a look at how the Azure ML studio implements this, and I found out this:<\/p>\n\n<ol>\n<li><p>It post a request with content-length=0 to <code>https:\/\/studioapi.azureml.net\/api\/resourceuploads\/workspaces\/&lt;workspace_id&gt;\/?userStorage=true&amp;dataTypeId=GenericCSV<\/code>, which will return an id in the response body.<\/p><\/li>\n<li><p>Break the .csv file into chunks less than 4194304 bytes, and post them to <code>https:\/\/studioapi.azureml.net\/api\/blobuploads\/workspaces\/&lt;workspace_id&gt;\/?numberOfBlocks=&lt;the number of chunks&gt;&amp;blockId=&lt;index of chunk&gt;&amp;uploadId=&lt;the id you get from previous request&gt;&amp;dataTypeId=GenericCSV<\/code><\/p><\/li>\n<\/ol>\n\n<p>If you really want this functionality, you can implement it with python and the above REST API.<\/p>\n\n<p>If you think it's too complicated, report the issue to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\/issues\" rel=\"nofollow\">this<\/a>. The <strong>azureml<\/strong> python package is still under development, so your suggestion would be very helpful for them.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1457939067983,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35973168",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1457935312407,
        "Question_original_content":"save dataset ipython notebook studio us command save output result dataset add datafram data genericcsv output csv uotput result workspac object data panda datafram work fine dataset size got error httperror maximum request length exceed understood error rais azur environ limit maximum size dataset chang split dataset part download studio inconvini size output dataset",
        "Question_preprocessed_content":"save dataset ipython notebook studio us command save output result object work fine dataset size got error understood error rais azur environ limit maximum size dataset chang split dataset part download studio inconvini size output dataset",
        "Question_gpt_summary_original":"The user is facing a challenge in saving a dataset from ipython notebook in Azure ML Studio. The command used to save the output results works fine for datasets less than 4 mb, but for larger datasets, the user encounters an error message indicating that the maximum request length has been exceeded. The user understands that this is a limitation of the Azure environment and cannot be changed. The user is considering splitting the dataset into 4 mb parts, but this would be inconvenient for datasets larger than 400 mb.",
        "Question_gpt_summary":"user face challeng save dataset ipython notebook studio command save output result work fine dataset larger dataset user encount error messag indic maximum request length exceed user understand limit azur environ chang user consid split dataset part inconveni dataset larger",
        "Answer_original_content":"read sourc code python packag simpl request post upload dataset limit content length byte tri modifi code insid http python packag post request chunk data got follow error traceback recent file test line dataset add datafram frame genericcsv output csv uotput result file python lib site packag init line add datafram return self upload raw data data type descript file python lib site packag init line upload raw data file python lib site packag http line upload dataset upload result self send post req api path raw data file python lib site packag http line send post req rais httperror respons text respons statu code error httperror chunk transfer encod permit upload size indic content length header request bec aecdfd rest api packag support chunk transfer encod took look studio implement post request content length http studioapi net api resourceupload workspac userstorag true datatypeid genericcsv return respons bodi break csv file chunk byte post http studioapi net api blobupload workspac numberofblock blockid uploadid datatypeid genericcsv want function implement python rest api think complic report issu python packag develop suggest help",
        "Answer_preprocessed_content":"read sourc code python packag simpl request post upload dataset limit content length byte tri modifi code insid python packag post request chunk data got follow error rest api packag support chunk transfer encod took look studio implement post request return respons bodi break csv file chunk byte post want function implement python rest api think complic report issu python packag develop suggest help",
        "Answer_gpt_summary_original":"the user can break the dataset into chunks less than 4194304 bytes and post them to a specific url. alternatively, they can report the issue to the developers of the python package.",
        "Answer_gpt_summary":"user break dataset chunk byte post specif url altern report issu develop python packag"
    },
    {
        "Question_id":null,
        "Question_title":"AZURE ML - Web Service",
        "Question_body":"How to configure the input fields with drop down values from the experiment. Eg: if car make is a field, the input field for the car make should start showing options when you start entering.\n\n\nList item\n\nIn the below example, fuel field should show drop down values like, diesel, petrol etc. ![79842-image.png][1] [1]: \/answers\/storage\/attachments\/79842-image.png",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616267169727,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/323764\/azure-ml-web-service.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-22T12:37:47.683Z",
                "Answer_score":0,
                "Answer_body":"@LizRay-1175 Thanks for the question. Can you please add more details about the steps that you performed. Please follow this document to deploy with designer and consume the real time endpoint.: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-model-designer? Basically you can register a trained model in Designer bring it out with SDK\/CLI to deploy it.\n\nSharing a reference notebook from Nicholas Moore: https:\/\/github.com\/nfmoore\/aml-designer-iot-edge\/blob\/main\/00-containerize-designer-model.ipynb.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"web servic configur input field drop valu experi car field input field car start show option start enter list item exampl fuel field drop valu like diesel petrol imag png answer storag attach imag png",
        "Question_preprocessed_content":"web servic configur input field drop valu experi car field input field car start show option start enter list item exampl fuel field drop valu like diesel petrol",
        "Question_gpt_summary_original":"The user is facing a challenge in configuring input fields with drop down values from an experiment in Azure ML. Specifically, they want the input field for a car make to show options when entering and for the fuel field to display drop down values such as diesel and petrol.",
        "Question_gpt_summary":"user face challeng configur input field drop valu experi specif want input field car option enter fuel field displai drop valu diesel petrol",
        "Answer_original_content":"lizrai thank question add detail step perform follow document deploi design consum real time endpoint http doc microsoft com azur machin learn deploi model design basic regist train model design bring sdk cli deploi share refer notebook nichola moor http github com nfmoor aml design iot edg blob main container design model ipynb",
        "Answer_preprocessed_content":"thank question add detail step perform follow document deploi design consum real time basic regist train model design bring deploi share refer notebook nichola moor",
        "Answer_gpt_summary_original":"the answer provides a link to a document that explains how to deploy a model with designer and consume the real-time endpoint. the user can register a trained model in designer and bring it out with sdk\/cli to deploy it. additionally, the answer shares a reference notebook from nicholas moore that may be helpful. however, there is no direct solution provided for configuring input fields with drop-down values from the experiment.",
        "Answer_gpt_summary":"answer provid link document explain deploi model design consum real time endpoint user regist train model design bring sdk cli deploi addition answer share refer notebook nichola moor help direct solut provid configur input field drop valu experi"
    },
    {
        "Question_id":null,
        "Question_title":"Bug report: Apply Math Operation doesn't cache and reuse results from previous run",
        "Question_body":"In Azure ML Designer, I have a complex data preprocessing pipeline. One of the main features of Designer pipelines is that they are supposed to cache the results of each run, so that they don't need to be re-executed every time if nothing that they depend on has changed. This feature works correctly for all of the nodes in my graph except for the Apply Math Operation nodes. This results in the second half of my pipeline needing to be re-run every time I add a new node.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594757991143,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/47002\/bug-report-apply-math-operation-doesnt-cache-and-r.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-14T20:28:46.2Z",
                "Answer_score":0,
                "Answer_body":"Thanks for reaching out to us. We will investigate this deeper and let you know any update.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":38.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"bug report appli math oper cach reus result previou run design complex data preprocess pipelin main featur design pipelin suppos cach result run need execut time depend chang featur work correctli node graph appli math oper node result second half pipelin need run time add new node",
        "Question_preprocessed_content":"bug report appli math oper cach reus result previou run design complex data preprocess pipelin main featur design pipelin suppos cach result run need time depend chang featur work correctli node graph appli math oper node result second half pipelin need time add new node",
        "Question_gpt_summary_original":"The user is facing a challenge with the Apply Math Operation nodes in their complex data preprocessing pipeline in Azure ML Designer. The nodes are not caching and reusing results from previous runs, unlike other nodes in the pipeline. As a result, the second half of the pipeline needs to be re-run every time a new node is added.",
        "Question_gpt_summary":"user face challeng appli math oper node complex data preprocess pipelin design node cach reus result previou run unlik node pipelin result second half pipelin need run time new node ad",
        "Answer_original_content":"thank reach investig deeper let know updat regard yutong",
        "Answer_preprocessed_content":"thank reach investig deeper let know updat regard yutong",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the user has reported an issue with the apply math operation nodes in their designer pipeline not caching and reusing results from previous runs. the support team will investigate the issue and provide an update.",
        "Answer_gpt_summary":"solut provid answer user report issu appli math oper node design pipelin cach reus result previou run support team investig issu provid updat"
    },
    {
        "Question_id":null,
        "Question_title":"How should the input JSONL look for a batch prediction job?",
        "Question_body":"I can't find any examples online of how an input jsonl is supposed to look for a batch training job. When I tried with this:  I got an error email saying  Error Messages: BatchPrediction could not start because no valid instances \nwere found in the input file. Is there some other way this should look for it to work? Maybe like      ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649068920000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":109.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-should-the-input-JSONL-look-for-a-batch-prediction-job\/td-p\/410193\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-08T13:36:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hell sangersteel,\nIt is not possible to use a JSONL file for batch prediction of text classification. Only a CSV file format is accepted for text classification. This is indicated in the [1] [AutoML Natural Language documentation] The CSV file should only contain 1 file (input file) per row. The CSV file and each input file needs to be stored in your Cloud Storage bucket.\n\n[1]\u00a0https:\/\/cloud.google.com\/natural-language\/automl\/docs\/predict#batch_prediction."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"input jsonl look batch predict job exampl onlin input jsonl suppos look batch train job tri got error email sai error messag batchpredict start valid instanc input file wai look work mayb like",
        "Question_preprocessed_content":"input jsonl look batch predict job exampl onlin input jsonl suppos look batch train job tri got error email sai error messag batchpredict start valid instanc input file wai look work mayb like",
        "Question_gpt_summary_original":"The user is facing challenges in understanding how the input JSONL should look for a batch prediction job. They have tried a certain format but received an error message stating that no valid instances were found in the input file. The user is seeking guidance on the correct format for the input JSONL.",
        "Question_gpt_summary":"user face challeng understand input jsonl look batch predict job tri certain format receiv error messag state valid instanc input file user seek guidanc correct format input jsonl",
        "Answer_original_content":"hell sangersteel possibl us jsonl file batch predict text classif csv file format accept text classif indic automl natur languag document csv file contain file input file row csv file input file need store cloud storag bucket http cloud googl com natur languag automl doc predict batch predict",
        "Answer_preprocessed_content":"hell sangersteel possibl us jsonl file batch predict text classif csv file format accept text classif indic csv file contain file row csv file input file need store cloud storag bucket",
        "Answer_gpt_summary_original":"the answer states that a jsonl file cannot be used for batch prediction of text classification and only a csv file format is accepted. the csv file should only contain one file per row and both the csv file and each input file need to be stored in the cloud storage bucket. the solution is to convert the jsonl file to a csv file format and ensure that it follows the required format for batch prediction of text classification.",
        "Answer_gpt_summary":"answer state jsonl file batch predict text classif csv file format accept csv file contain file row csv file input file need store cloud storag bucket solut convert jsonl file csv file format ensur follow requir format batch predict text classif"
    },
    {
        "Question_id":62170192.0,
        "Question_title":"Compute Instance: Best practice for custom Anaconda env",
        "Question_body":"<p>I'd like to use a compute instance as my develop machine.\nAre there any best practices on how to handle custom Anaconda enviroments on these machines?<\/p>\n\n<p>So far, I do it this way:<\/p>\n\n<pre><code>conda create --name testenv python=3\nconda activate testenv\nconda install ipykernel\nipython kernel install --user --name=testenv\nsudo systemctl restart jupyter.service\n<\/code><\/pre>\n\n<p>--> Reload the JupyterHub in your browser.<\/p>\n\n<p>Do you see any drawbacks by doing it this way? I know, some special package combinations in the standard env are lost, but I'd like to know what I've installed in my system.\nOf course, one could combine it with an <code>environment.yml<\/code>.<\/p>\n\n<p>What do you think?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591178661870,
        "Question_favorite_count":null,
        "Question_last_edit_time":1591865477772,
        "Question_score":1.0,
        "Question_view_count":116.0,
        "Answer_body":"<p>Your workaround is the best option as of now. But I know that the Azure ML product group has been working on exactly this problem, but I can't make any promises as to timeline.<\/p>\n\n<p>I share your dream of an easily configurable data science cloud development environment that allows for Git repo cloning and environment creation w\/ a conda yml. We're so close especially given all the press &amp; announcements around Visual Studio Codespaces!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62170192",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1591290256907,
        "Question_original_content":"comput instanc best practic custom anaconda env like us comput instanc develop machin best practic handl custom anaconda enviro machin far wai conda creat testenv python conda activ testenv conda instal ipykernel ipython kernel instal user testenv sudo systemctl restart jupyt servic reload jupyterhub browser drawback wai know special packag combin standard env lost like know instal cours combin environ yml think",
        "Question_preprocessed_content":"comput instanc best practic custom anaconda env like us comput instanc develop machin best practic handl custom anaconda enviro machin far wai reload jupyterhub browser drawback wai know special packag combin standard env lost like know instal cours combin think",
        "Question_gpt_summary_original":"The user is seeking best practices for handling custom Anaconda environments on a compute instance used as a development machine. The user currently creates a new environment using conda, installs necessary packages, and restarts the Jupyter service. The user is concerned about losing special package combinations in the standard environment but wants to know what is installed in the system. The user is considering combining the process with an environment.yml file.",
        "Question_gpt_summary":"user seek best practic handl custom anaconda environ comput instanc develop machin user current creat new environ conda instal necessari packag restart jupyt servic user concern lose special packag combin standard environ want know instal user consid combin process environ yml file",
        "Answer_original_content":"workaround best option know product group work exactli problem promis timelin share dream easili configur data scienc cloud develop environ allow git repo clone environ creation conda yml close especi given press announc visual studio codespac",
        "Answer_preprocessed_content":"workaround best option know product group work exactli problem promis timelin share dream easili configur data scienc cloud develop environ allow git repo clone environ creation conda yml close especi given press announc visual studio codespac",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer, but the user's current method of using conda create, conda activate, conda install, and ipython kernel install is considered the best option for handling custom anaconda environments on compute instances. the product group is working on a solution to this problem, but there is no timeline for its release. the answer also mentions the possibility of an easily configurable data science cloud development environment that allows for git repo cloning and environment creation with a conda yml, which is a promising development.",
        "Answer_gpt_summary":"specif solut provid answer user current method conda creat conda activ conda instal ipython kernel instal consid best option handl custom anaconda environ comput instanc product group work solut problem timelin releas answer mention possibl easili configur data scienc cloud develop environ allow git repo clone environ creation conda yml promis develop"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb: Network error (ConnectionError), entering retry loop",
        "Question_body":"<p>I am using wandb version 0.12.21\u2026 My code was working fine till yesterday, but I am getting this error\u2026<\/p>\n<p>wandb: Currently logged in as: shashi7679. Use <code>wandb login --relogin<\/code> to force relogin<br>\nwandb: Appending key for <a href=\"http:\/\/api.wandb.ai\">api.wandb.ai<\/a> to your netrc file: \/root\/.netrc<br>\nwandb: Network error (ConnectionError), entering retry loop.<\/p>\n<p>Problem at: train.py 333 <br>\nProcess wandb_internal:<br>\nTraceback (most recent call last):<br>\nFile \u201ctrain.py\u201d, line 333, in <br>\nTraceback (most recent call last):<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/multiprocessing\/process.py\u201d, line 315, in _bootstrap<br>\nself.run()<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/multiprocessing\/process.py\u201d, line 108, in run<br>\nself._target(*self._args, **self._kwargs)<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/internal\/internal.py\u201d, line 160, in wandb_internal<br>\nthread.join()<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/threading.py\u201d, line 1011, in join<br>\nself._wait_for_tstate_lock()<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/threading.py\u201d, line 1027, in _wait_for_tstate_lock<br>\nelif lock.acquire(block, timeout):<br>\nKeyboardInterrupt<br>\nwandb.init(project=\u2018Toon-GAN\u2019,config=hyper_parameters)<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\u201d, line 1065, in init<br>\nraise e<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\u201d, line 1043, in init<br>\nrun = wi.init()<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/wandb_init.py\u201d, line 689, in init<br>\nbackend.cleanup()<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/site-packages\/wandb\/sdk\/backend\/backend.py\u201d, line 248, in cleanup<br>\nself.wandb_process.join()<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/multiprocessing\/process.py\u201d, line 149, in join<br>\nres = self._popen.wait(timeout)<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/multiprocessing\/popen_fork.py\u201d, line 47, in wait<br>\nreturn self.poll(os.WNOHANG if timeout == 0.0 else 0)<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/multiprocessing\/popen_fork.py\u201d, line 27, in poll<br>\npid, sts = os.waitpid(self.pid, flag)<br>\nKeyboardInterrupt<br>\n^CTraceback (most recent call last):<br>\nFile \u201c\u201d, line 1, in <br>\nError in atexit._run_exitfuncs:<br>\nTraceback (most recent call last):<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/multiprocessing\/popen_fork.py\u201d, line 27, in poll<br>\npid, sts = os.waitpid(self.pid, flag)<br>\nKeyboardInterrupt<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/multiprocessing\/spawn.py\u201d, line 116, in spawn_main<br>\nexitcode = _main(fd, parent_sentinel)<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/multiprocessing\/spawn.py\u201d, line 129, in _main<br>\nreturn self._bootstrap(parent_sentinel)<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/multiprocessing\/process.py\u201d, line 333, in _bootstrap<br>\nthreading._shutdown()<br>\nFile \u201c\/opt\/conda\/lib\/python3.8\/threading.py\u201d, line 1388, in _shutdown<br>\nlock.acquire()<br>\nKeyboardInterrupt<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1675253882442,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":90.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-network-error-connectionerror-entering-retry-loop\/3789",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-03T17:54:57.278Z",
                "Answer_body":"<p>Hello!<\/p>\n<p>Could you send your debug logs so I can get a closer look into your <code>ConnectionError<\/code>?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-07T23:08:53.273Z",
                "Answer_body":"<p>Hi shashikantprasad1111, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"network error connectionerror enter retri loop version code work fine till yesterdai get error current log shashi us login relogin forc relogin append kei api netrc file root netrc network error connectionerror enter retri loop problem train process intern traceback recent file train line traceback recent file opt conda lib python multiprocess process line bootstrap self run file opt conda lib python multiprocess process line run self target self arg self kwarg file opt conda lib python site packag sdk intern intern line intern thread join file opt conda lib python thread line join self wait tstate lock file opt conda lib python thread line wait tstate lock elif lock acquir block timeout keyboardinterrupt init project toon gan config hyper paramet file opt conda lib python site packag sdk init line init rais file opt conda lib python site packag sdk init line init run init file opt conda lib python site packag sdk init line init backend cleanup file opt conda lib python site packag sdk backend backend line cleanup self process join file opt conda lib python multiprocess process line join re self popen wait timeout file opt conda lib python multiprocess popen fork line wait return self poll wnohang timeout file opt conda lib python multiprocess popen fork line poll pid st waitpid self pid flag keyboardinterrupt ctraceback recent file line error atexit run exitfunc traceback recent file opt conda lib python multiprocess popen fork line poll pid st waitpid self pid flag keyboardinterrupt file opt conda lib python multiprocess spawn line spawn main exitcod main parent sentinel file opt conda lib python multiprocess spawn line main return self bootstrap parent sentinel file opt conda lib python multiprocess process line bootstrap thread shutdown file opt conda lib python thread line shutdown lock acquir keyboardinterrupt",
        "Question_preprocessed_content":"network error enter retri loop version code work fine till yesterdai get error current log shashi us forc relogin append kei netrc file network error enter retri loop problem process traceback file line traceback file line file line run file line file line join file line elif timeout keyboardinterrupt file line init rais file line init run file line init file line cleanup file line join re file line wait return timeout file line poll pid st flag keyboardinterrupt ctraceback file line error traceback file line poll pid st flag keyboardinterrupt file line exitcod file line return file line file line keyboardinterrupt",
        "Question_gpt_summary_original":"The user is encountering a network error (ConnectionError) while using wandb version 0.12.21. The error message suggests that the code was working fine until yesterday, but now the user is unable to connect to the API. The error message also includes a traceback of the error, which shows that the error occurred at line 333 of the train.py file. The traceback also includes a KeyboardInterrupt error, which suggests that the user may have manually interrupted the program.",
        "Question_gpt_summary":"user encount network error connectionerror version error messag suggest code work fine yesterdai user unabl connect api error messag includ traceback error show error occur line train file traceback includ keyboardinterrupt error suggest user manual interrupt program",
        "Answer_original_content":"hello send debug log closer look connectionerror shashikantprasad heard go close request like open convers let know",
        "Answer_preprocessed_content":"hello send debug log closer look shashikantprasad heard go close request like convers let know",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the answer is a request for the user to send debug logs to investigate the connection error. additionally, the answer informs the user that the request will be closed if they do not respond.",
        "Answer_gpt_summary":"solut provid answer answer request user send debug log investig connect error addition answer inform user request close respond"
    },
    {
        "Question_id":null,
        "Question_title":"TensorBoard Hparm Parallel Coordinate View Problem",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m having an issue with the TensorBoard Hparm Parallel Coordinate View. It seems to be duplicated the axes on the lower half.<\/p>\n<p>This happens when I make a new project and just follow the get-started.ipynb file.<\/p>\n<p>I\u2019m using a Mac and Safari browser, and here are the versions info:<\/p>\n<pre><code>guild_version:             0.7.3\ntensorboard_version:       2.6.0\n<\/code><\/pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb.jpeg\" data-download-href=\"\/uploads\/short-url\/oDDDcjhGMAurXRar09EHL2ltw3p.jpeg?dl=1\" title=\"Screen Shot 2021-08-20 at 4.24.55 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_517x336.jpeg\" alt=\"Screen Shot 2021-08-20 at 4.24.55 PM\" data-base62-sha1=\"oDDDcjhGMAurXRar09EHL2ltw3p\" width=\"517\" height=\"336\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_517x336.jpeg, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_775x504.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_1034x672.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/acaf444ae1126cf7cdea1a9aa3bebec91d370cbb_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2021-08-20 at 4.24.55 PM<\/span><span class=\"informations\">1920\u00d71249 181 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Additionally, when I remove one of the variables, the entire plot grows at least 30% in size. Is there a way I can keep this constant.<\/p>\n<p>Lastly, runs in a notebook don\u2019t seem to save an output.index file and guild view throws an error from this.<\/p>\n<p>Thanks for the help! Let me know if I need to share anything else.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1629491848267,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":259.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/tensorboard-hparm-parallel-coordinate-view-problem\/753",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-04T00:50:55.914Z",
                "Answer_body":"<p>Can you share a minimum working example?<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-14T16:06:42.952Z",
                "Answer_body":"<p>In this case you\u2019re running TensorBoard as provided by Google (Guild doesn\u2019t monkey patch TensorBoard or otherwise modify it. I find TB to have a number of strange sizing issues. The underlying framework Google uses is <em>very<\/em> complex and hard to contribute to\/fix so our strategy has been to \u201ctake it as is and be forever grateful\u201d <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=11\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Things I try in these cases:<\/p>\n<ul>\n<li>\n<p>First and foremost, after modifying Guild runs (new runs, deleted runs) restart TensorBoard \u2014 the dynamic behavior works well in most cases but in some cases TB just doesn\u2019t handle new data well. A restart often cleans that up.<\/p>\n<\/li>\n<li>\n<p>Play around with browser zoom - sometimes that balances out strange font and UI sizing.<\/p>\n<\/li>\n<li>\n<p>Play with resizing the browser window.<\/p>\n<\/li>\n<\/ul>\n<p>I wish I could give you a better answer here \u2014 I\u2019m basically blaming TensorBoard for all of this. But in this case, Guild really isn\u2019t involved beyond laying out the TB log dir.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"tensorboard hparm parallel coordin view problem have issu tensorboard hparm parallel coordin view duplic ax lower half happen new project follow start ipynb file mac safari browser version info version tensorboard version screen shot addition remov variabl entir plot grow size wai constant lastli run notebook dont save output index file view throw error thank help let know need share",
        "Question_preprocessed_content":"tensorboard hparm parallel coordin view problem have issu tensorboard hparm parallel coordin view duplic ax lower half happen new project follow file mac safari browser version info screen shot addition remov variabl entir plot grow size wai constant lastli run notebook dont save file view throw error thank help let know need share",
        "Question_gpt_summary_original":"The user is facing challenges with TensorBoard Hparm Parallel Coordinate View, as it is duplicating the axes on the lower half. The issue occurs when a new project is created and the get-started.ipynb file is followed. The user is using a Mac and Safari browser with guild_version 0.7.3 and tensorboard_version 2.6.0. Removing one of the variables causes the entire plot to grow by at least 30%, and runs in a notebook do not save an output.index file, resulting in an error in guild view.",
        "Question_gpt_summary":"user face challeng tensorboard hparm parallel coordin view duplic ax lower half issu occur new project creat start ipynb file follow user mac safari browser version tensorboard version remov variabl caus entir plot grow run notebook save output index file result error view",
        "Answer_original_content":"share minimum work exampl case your run tensorboard provid googl doesnt monkei patch tensorboard modifi number strang size issu underli framework googl us complex hard contribut fix strategi forev grate thing try case foremost modifi run new run delet run restart tensorboard dynam behavior work case case doesnt handl new data restart clean plai browser zoom balanc strang font size plai resiz browser window wish better answer basic blame tensorboard case isnt involv lai log dir",
        "Answer_preprocessed_content":"share minimum work exampl case your run tensorboard provid googl restart tensorboard dynam behavior work case case doesnt handl new data restart clean plai browser zoom balanc strang font size plai resiz browser window wish better answer basic blame tensorboard case isnt involv lai log dir",
        "Answer_gpt_summary_original":"possible solutions to issues with tensorboard hparm parallel coordinate view include restarting tensorboard after modifying guild runs, playing around with browser zoom and resizing the browser window. the answer suggests that tensorboard has complex underlying frameworks that are hard to contribute to or fix, and that the best strategy is to take it as is and be grateful.",
        "Answer_gpt_summary":"possibl solut issu tensorboard hparm parallel coordin view includ restart tensorboard modifi run plai browser zoom resiz browser window answer suggest tensorboard complex underli framework hard contribut fix best strategi grate"
    },
    {
        "Question_id":64164367.0,
        "Question_title":"Nginx authentication issues when building mlflow through docker-compose",
        "Question_body":"<p>I'm trying to dockerize <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">mlflow<\/a> with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP).<\/p>\n<p>Before deploying anything to GCP however, I wanted to get a local deployment working. I found <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">this<\/a> guide that details the process of setting up the environment. Having followed the guide (excluding the SQL part), I can see the  mlflow UI on <code>localhost:80<\/code> as nginx redirects traffic on port 80 to 5000. To add authentication, I found <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-set-up-password-authentication-with-nginx-on-ubuntu-14-04\" rel=\"nofollow noreferrer\">here<\/a> that I can do it using <code>sudo htpasswd -c .htpasswd &lt;username&gt;<\/code> in the <code>etc\/nginx\/<\/code> directory and then adding<\/p>\n<pre><code>location \\ {\n   auth_basic &quot;Private Property&quot;;\n   auth_basic_user_file .htpasswd;\n}\n<\/code><\/pre>\n<p>to the <code>nginx.conf<\/code> (or <code>mlflow.conf<\/code> in this case) to make it appear online. Trouble is, when I go to <code>localhost:80<\/code> <em>now<\/em> and enter in my username\/password, I continue to see<\/p>\n<pre><code>[error] 6#6: *1 open() &quot;\/etc\/nginx\/.htpasswd&quot; failed (2: No such file or directory)\n<\/code><\/pre>\n<p>in the <code>docker-compose up<\/code> logs as they are printed to the terminal, and as such <em>I'm not able to see the mlflow UI<\/em> on <code>localhost:80<\/code> (either a blank screen or nginx 403 error).<\/p>\n<p>Now, I've looked at several other posts (such as <a href=\"https:\/\/stackoverflow.com\/questions\/2010677\/nginx-and-auth-basic\">this one<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/16510374\/403-forbidden-nginx-using-correct-credentials\">this one<\/a>) and it seems to me that nginx doesn't have the right permissions to read the <code>.htpasswd<\/code> in the <code>etc\/nginx\/<\/code> directory file or that the path of the file isn't correct, i.e. the path has to be in reference to the <code>nginx.conf<\/code> file.<\/p>\n<p>Even though I made these corrections to the above towards-data-science files, the problem still persists.  I've been stuck for a while on this. Any particular reasons why this may be happening?<\/p>\n<p>Edit:\nHere is my directory structure in case it may help:<\/p>\n<pre><code>mlflow-docker\/:\n  mlflow\/:\n    Dockerfile\n  nginx\/:\n    Dockerfile\n    mlflow.conf\n    nginx.conf\n  docker-compose.yml\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1601594496277,
        "Question_favorite_count":null,
        "Question_last_edit_time":1601616621328,
        "Question_score":0.0,
        "Question_view_count":841.0,
        "Answer_body":"<p>You need to add the .htpasswd file inside your container's file system.<\/p>\n<p>Generate the password file in your project's nginx folder.<\/p>\n<pre><code>sudo htpasswd -c .htpasswd sammy\n<\/code><\/pre>\n<p>Copy the password file to the nginx container's directory. Add following line in nginx dockerfile.<\/p>\n<pre><code>COPY .htpasswd \/etc\/nginx\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1601619366396,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64164367",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1601618748112,
        "Question_original_content":"nginx authent issu build docker compos try docker postgresql nginx configur googl cloud run gcr googl cloud platform gcp deploi gcp want local deploy work guid detail process set environ have follow guid exclud sql localhost nginx redirect traffic port add authent sudo htpasswd htpasswd nginx directori ad locat auth basic privat properti auth basic user file htpasswd nginx conf conf case appear onlin troubl localhost enter usernam password continu error open nginx htpasswd fail file directori docker compos log print termin abl localhost blank screen nginx error look post nginx right permiss read htpasswd nginx directori file path file isn correct path refer nginx conf file correct data scienc file problem persist stuck particular reason happen edit directori structur case help docker dockerfil nginx dockerfil conf nginx conf docker compos yml",
        "Question_preprocessed_content":"nginx authent issu build try docker postgresql nginx configur googl cloud run googl cloud platform deploi gcp want local deploy work guid detail process set environ have follow guid nginx redirect traffic port add authent directori ad appear onlin troubl enter continu log print termin abl look post nginx right permiss read directori file path file isn correct path refer file correct file problem persist stuck particular reason happen edit directori structur case help",
        "Question_gpt_summary_original":"The user is trying to dockerize mlflow with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP). They followed a guide to set up the environment and added authentication using htpasswd, but they are encountering an error that says \"open() \"\/etc\/nginx\/.htpasswd\" failed (2: No such file or directory)\" when they try to access the mlflow UI on localhost:80. The user suspects that nginx doesn't have the right permissions to read the .htpasswd file or that the path of the file isn't correct. They have tried to correct the problem but it still persists.",
        "Question_gpt_summary":"user try docker postgresql nginx configur googl cloud run gcr googl cloud platform gcp follow guid set environ ad authent htpasswd encount error sai open nginx htpasswd fail file directori try access localhost user suspect nginx right permiss read htpasswd file path file isn correct tri correct problem persist",
        "Answer_original_content":"need add htpasswd file insid contain file gener password file project nginx folder sudo htpasswd htpasswd sammi copi password file nginx contain directori add follow line nginx dockerfil copi htpasswd nginx",
        "Answer_preprocessed_content":"need add htpasswd file insid contain file gener password file project nginx folder copi password file nginx contain directori add follow line nginx dockerfil",
        "Answer_gpt_summary_original":"solution: to resolve authentication issues when building a docker-compose environment with nginx configurations for google cloud run on the google cloud platform, the user needs to generate a .htpasswd file in the project's nginx folder using the command \"sudo htpasswd -c .htpasswd sammy\", add the password file to the nginx container's directory, and add the line \"copy .htpasswd \/etc\/nginx\" in the nginx dockerfile.",
        "Answer_gpt_summary":"solut resolv authent issu build docker compos environ nginx configur googl cloud run googl cloud platform user need gener htpasswd file project nginx folder command sudo htpasswd htpasswd sammi add password file nginx contain directori add line copi htpasswd nginx nginx dockerfil"
    },
    {
        "Question_id":62765658.0,
        "Question_title":"Tensorflow error. TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn",
        "Question_body":"<p>I am trying to run this on Amazon Sagemaker but I am getting this error while when I try to run it on my local machine, it works very fine.<\/p>\n<p>this is my code:<\/p>\n<pre><code>import tensorflow as tf\n\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\n    \ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)\n\ncontent_path = tf.keras.utils.get_file('YellowLabradorLooking_nw4.jpg', 'https:\/\/example.com\/IMG_20200216_163015.jpg')\n\n\nstyle_path = tf.keras.utils.get_file('kandinsky3.jpg','https:\/\/example.com\/download+(2).png')\n\n\ndef load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\ncontent_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\nimport tensorflow_hub as hub\nhub_module = hub.load('https:\/\/tfhub.dev\/google\/magenta\/arbitrary-image-stylization-v1-256\/1')\nstylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)\n\n\nfile_name = 'stylized-image5.png'\ntensor_to_image(stylized_image).save(file_name)\n<\/code><\/pre>\n<p>This is the exact error I get:<\/p>\n<pre><code>---------------------------------------------------------------------------\n<\/code><\/pre>\n<p>TypeError                                 Traceback (most recent call last)<\/p>\n<pre><code>&lt;ipython-input-24-c47a4db4880c&gt; in &lt;module&gt;()\n     53 \n     54 \n---&gt; 55 content_image = load_img(content_path)\n     56 style_image = load_img(style_path)\n     57 \n<\/code><\/pre>\n<p> in load_img(path_to_img)<\/p>\n<pre><code>     34 \n     35     shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n---&gt; 36     long_dim = max(shape)\n     37     scale = max_dim \/ long_dim\n     38 \n<\/code><\/pre>\n<p>~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/tensorflow\/python\/framework\/ops.py in <strong>iter<\/strong>(self)<\/p>\n<pre><code>    475     if not context.executing_eagerly():\n    476       raise TypeError(\n--&gt; 477           &quot;Tensor objects are only iterable when eager execution is &quot;\n    478           &quot;enabled. To iterate over this tensor use tf.map_fn.&quot;)\n    479     shape = self._shape_tuple()\n<\/code><\/pre>\n<p>TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1594076057097,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1530.0,
        "Answer_body":"<p>Your error is being raised in this function <code>load_img<\/code>:<\/p>\n<pre><code>def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n<\/code><\/pre>\n<p>Specifically, this line:<\/p>\n<pre><code>    long_dim = max(shape)\n<\/code><\/pre>\n<p>You are passing a tensor to the <a href=\"https:\/\/docs.python.org\/3\/library\/functions.html#max\" rel=\"nofollow noreferrer\">built-in Python max function<\/a> in graph execution mode. You can only iterate through tensors in eager-execution mode. You probably want to use <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/reduce_max\" rel=\"nofollow noreferrer\">tf.reduce_max<\/a> instead:<\/p>\n<pre><code>    long_dim = tf.reduce_max(shape)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1594159994640,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62765658",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1594129639003,
        "Question_original_content":"tensorflow error typeerror tensor object iter eager execut enabl iter tensor us map try run get error try run local machin work fine code import tensorflow import ipython displai displai import matplotlib pyplot plt import matplotlib mpl mpl rcparam figur figsiz mpl rcparam ax grid fals import numpi import pil imag import time import functool def tensor imag tensor tensor tensor tensor arrai tensor dtype uint ndim tensor assert tensor shape tensor tensor return pil imag fromarrai tensor content path kera util file yellowlabradorlook jpg http exampl com img jpg style path kera util file kandinski jpg http exampl com download png def load img path img max dim img read file path img img imag decod imag img channel img imag convert imag dtype img float shape cast shape img float long dim max shape scale max dim long dim new shape cast shape scale int img imag resiz img new shape img img newaxi return img def imshow imag titl len imag shape imag squeez imag axi plt imshow imag titl plt titl titl content imag load img content path style imag load img style path plt subplot imshow content imag content imag plt subplot imshow style imag style imag import tensorflow hub hub hub modul hub load http tfhub dev googl magenta arbitrari imag styliz styliz imag hub modul constant content imag constant style imag tensor imag styliz imag file styliz imag png tensor imag styliz imag save file exact error typeerror traceback recent content imag load img content path style imag load img style path load img path img shape cast shape img float long dim max shape scale max dim long dim anaconda env amazonei tensorflow lib python site packag tensorflow python framework op iter self context execut eagerli rais typeerror tensor object iter eager execut enabl iter tensor us map shape self shape tupl typeerror tensor object iter eager execut enabl iter tensor us map",
        "Question_preprocessed_content":"tensorflow error typeerror tensor object iter eager execut enabl iter tensor us try run get error try run local machin work fine code exact error typeerror traceback iter typeerror tensor object iter eager execut enabl iter tensor us",
        "Question_gpt_summary_original":"The user is encountering a Tensorflow error while trying to run a code on Amazon Sagemaker. The error message states that Tensor objects are only iterable when eager execution is enabled and suggests using tf.map_fn to iterate over the tensor. The code works fine on the user's local machine.",
        "Question_gpt_summary":"user encount tensorflow error try run code error messag state tensor object iter eager execut enabl suggest map iter tensor code work fine user local machin",
        "Answer_original_content":"error rais function load img def load img path img max dim img read file path img img imag decod imag img channel img imag convert imag dtype img float shape cast shape img float long dim max shape scale max dim long dim new shape cast shape scale int img imag resiz img new shape img img newaxi return img specif line long dim max shape pass tensor built python max function graph execut mode iter tensor eager execut mode probabl want us reduc max instead long dim reduc max shape",
        "Answer_preprocessed_content":"error rais function specif line pass tensor python max function graph execut mode iter tensor mode probabl want us instead",
        "Answer_gpt_summary_original":"the solution to the typeerror encountered when running tensorflow code is to replace the built-in python max function with tf.reduce_max to iterate through tensors in eager-execution mode. specifically, the line causing the error should be changed from \"long_dim = max(shape)\" to \"long_dim = tf.reduce_max(shape)\".",
        "Answer_gpt_summary":"solut typeerror encount run tensorflow code replac built python max function reduc max iter tensor eager execut mode specif line caus error chang long dim max shape long dim reduc max shape"
    },
    {
        "Question_id":null,
        "Question_title":"Dvc push - ERROR: failed to transfer",
        "Question_body":"<p>hi, i just  created a new repo with dvc and now I added 1 data file, but I can\u2019t seem to manage to do <code>dvc push<\/code>, what could be the reason?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d.png\" data-download-href=\"\/uploads\/short-url\/aZ5U8AjYSacHl6fq6MwI4Pk4quN.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d_2_690x113.png\" alt=\"image\" data-base62-sha1=\"aZ5U8AjYSacHl6fq6MwI4Pk4quN\" width=\"690\" height=\"113\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d_2_690x113.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d_2_1035x169.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/4cfdbb5212656025eadc3a4b339a3892fc93d39d.png 2x\" data-dominant-color=\"3F4349\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1333\u00d7220 26.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1676995646296,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":11.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-error-failed-to-transfer\/1533",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-21T16:17:33.139Z",
                "Answer_body":"<p>Hi,<br>\nMay be it\u2019s an authorization issue ?<br>\nuse dvc doctor and dvc push -vv (verbose) , better to use discord chanel<br>\nRgds<\/p>",
                "Answer_score":20.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-21T16:28:05.187Z",
                "Answer_body":"<p>Thanks for the tip with -vv flag. However it doesn\u2019t show the authorization error:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0679ed3eb28b27476c7deebcb119ce52e1cbc25f.png\" data-download-href=\"\/uploads\/short-url\/Vi5GjHH23bUAHpETrTkOUnGOkv.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0679ed3eb28b27476c7deebcb119ce52e1cbc25f.png\" alt=\"image\" data-base62-sha1=\"Vi5GjHH23bUAHpETrTkOUnGOkv\" width=\"690\" height=\"52\" data-dominant-color=\"3F4249\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1353\u00d7103 12.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nWhat could be the reason of too many redirects?<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-21T16:32:16.927Z",
                "Answer_body":"<p>Sorry<br>\nbut have you define the remote repo ?<br>\ndvc remote list \u2026<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-21T16:34:37.340Z",
                "Answer_body":"<p>Yes, I was following commands from DVC docs Get Started. When I do <code>dvc remote list<\/code> it is showing correctly<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"push error fail transfer creat new repo ad data file manag push reason imag",
        "Question_preprocessed_content":"push error fail transfer creat new repo ad data file manag reason imag",
        "Question_gpt_summary_original":"The user is facing an issue with pushing data to a newly created repo using dvc. The error message \"failed to transfer\" is preventing the user from completing the push.",
        "Question_gpt_summary":"user face issu push data newli creat repo error messag fail transfer prevent user complet push",
        "Answer_original_content":"author issu us doctor push verbos better us discord chanel rgd thank tip flag doesnt author error imag reason redirect sorri defin remot repo remot list ye follow command doc start remot list show correctli",
        "Answer_preprocessed_content":"author issu us doctor push better us discord chanel rgd thank tip flag doesnt author error imag reason redirect sorri defin remot repo remot list ye follow command doc start show correctli",
        "Answer_gpt_summary_original":"possible solutions: \n- check for authorization issues using the \"doctor\" command and push with the \"-vv\" (verbose) flag.\n- define the remote repository using the \"remote list\" command.",
        "Answer_gpt_summary":"possibl solut check author issu doctor command push verbos flag defin remot repositori remot list command"
    },
    {
        "Question_id":null,
        "Question_title":"Is There a Way to Visualize the Decision Tree AML Used?",
        "Question_body":"I have used a Two-Class Boosted Decision Tree in Azure ML to make some predictions on data that I am analyzing. Once the model has completed training is there a way for me to visualize the structure of the decision tree that was ultimately used by Azure?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1612898717163,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/265984\/is-there-a-way-to-visualize-the-decision-tree-aml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-10T16:17:06.287Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nThanks for reaching out to us. If you are under Azure Machine Learning Studio(Classic), you can easily do it by Right-click on the output node of the \"Train Model\" module and select \"Visualize\".\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/two-class-boosted-decision-tree#results\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-23T06:33:10.683Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI have confirmed we don't have visualize function for Azure Machine Learning Designer now, but we would like to learn why you are asking about this feature and what you want to do with the visualized graph. We are caring customers' experience and will consider this function if that makes sense.\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"wai visual decis tree aml class boost decis tree predict data analyz model complet train wai visual structur decis tree ultim azur",
        "Question_preprocessed_content":"wai visual decis tree aml boost decis tree predict data analyz model complet train wai visual structur decis tree ultim azur",
        "Question_gpt_summary_original":"The user is facing a challenge in visualizing the structure of the decision tree used by Azure ML after training a Two-Class Boosted Decision Tree to make predictions on analyzed data.",
        "Question_gpt_summary":"user face challeng visual structur decis tree train class boost decis tree predict analyz data",
        "Answer_original_content":"hello thank reach studio classic easili right click output node train model modul select visual http doc microsoft com azur machin learn studio modul refer class boost decis tree result regard yutong hello confirm visual function design like learn ask featur want visual graph care custom experi consid function make sens regard yutong",
        "Answer_preprocessed_content":"hello thank reach studio easili output node train model modul select visual regard yutong hello confirm visual function design like learn ask featur want visual graph care custom experi consid function make sens regard yutong",
        "Answer_gpt_summary_original":"possible solutions: \n- if the user is using azure studio (classic), they can visualize the two-class boosted decision tree by right-clicking on the output node of the \"train model\" module and selecting \"visualize\".\n- however, if the user is using azure designer, there is currently no visualize function available. the support team is interested in learning more about why the user needs this feature and will consider adding it in the future if it makes sense.",
        "Answer_gpt_summary":"possibl solut user azur studio classic visual class boost decis tree right click output node train model modul select visual user azur design current visual function avail support team interest learn user need featur consid ad futur make sens"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb: WARNING Invalid value for property root_dir",
        "Question_body":"<p>Hello,<\/p>\n<p>I defined wandb_log_dir to be another directory than where my code resides and received the following warning: \u201cwandb: WARNING Invalid value for property root_dir: \/home\/xx\/xxx\/experiments. This will raise an error in the future.\u201d<\/p>\n<p>Normally I am not particularly concerned about warnings. But because it said \u201cThis will raise an error in the future\u201d, I would like to know what is considered a valid value for property root_dir (and why)? If defining it the way I did, why will it raise an error done the road?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1670358983881,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":134.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-warning-invalid-value-for-property-root-dir\/3505",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-09T10:57:17.734Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rcheng\">@rcheng<\/a>  this means that unexpected settings will throw an error in next releases, while for now these <code>**kwarg<\/code> will be ignored in order to initialise the run with the default value instead. It\u2019s coming from <a href=\"https:\/\/github.com\/wandb\/wandb\/blob\/dea67a1038a1a539be46554cd83a1430a2267fb8\/wandb\/sdk\/wandb_settings.py#L1069\" rel=\"noopener nofollow ugc\">this<\/a> line of our SDK code.<\/p>\n<p>Could you please provide a bit more information, where you set up this parameter, from the environment variables or from wandb.init() call? Would it be possible to share a code snippet of the line that gives this warning, and also what OS you\u2019re using?<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-09T15:23:46.785Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/thanos-wandb\">@thanos-wandb<\/a> , thanks for the info.<\/p>\n<p>I  am on 20.04.3 LTS and here is how I set up this parameter  :<\/p>\n<p>(from run.py)<\/p>\n<pre><code class=\"lang-auto\">from pytorch_lightning.loggers import WandbLogger\n\nwandb_log_dir = Path(config['logging_params']['save_dir']) \/ args.run_name\nwandb_logger = WandbLogger(save_dir=wandb_log_dir,\n                           name=args.run_name,\n                           project=\"beta-vae\"\n                           )\n<\/code><\/pre>\n<p>config.yaml file:<\/p>\n<pre><code class=\"lang-auto\">logging_params:\n  save_dir: \"logs\/\" # or \"\/home\/xx\/xxx\/experiments\" (the same warning pops up)\n  manual_seed: 1265\n  name: 'BetaVAE'\n\n<\/code><\/pre>\n<p>Actually I realized that the warning pops up, regardless of the save_dir is under where the code resides or in a completely different directory. (I originally thought the warning showed up because it was expecting the log_dir to be at the same directory as the code.)<\/p>\n<p>Thanks in advance for further hints!<\/p>",
                "Answer_score":10.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-22T14:49:44.030Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rcheng\">@rcheng<\/a> thanks for the additional information, I have tried to reproduce the same issue using your code snippet but I didn\u2019t get the same warning. Could you please to explicitly set <code>save_dir='.'<\/code>or <code>save_dir=\/home\/dir1\/dir2\/experiments<\/code>? Would this work for you? also, is <code>Path<\/code> from the <code>pathlib<\/code> package and could you please <code>print(wandb_log_dir)<\/code> and post the output here as well?<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-28T15:19:17.040Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rcheng\">@rcheng<\/a> I wanted to follow up with you on this issue, and see if it still occurs for you? Would any of the above work for you? thanks!<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-29T16:29:13.461Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rcheng\">@rcheng<\/a> since we haven\u2019t heard back from you in a while, I will go ahead and close this ticket for now. If this warning still occurs for you, please let us know and we will be happy to keep investigating!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-29T16:41:56.401Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/thanos-wandb\">@thanos-wandb<\/a> ,<\/p>\n<p>Sorry for the delayed response.  Somehow the warning went away. I will report here if it shows up again in the coming days. Many thanks for the hints.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-29T17:06:23.238Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rcheng\">@rcheng<\/a> glad to hear that, and thanks a lot for the update! Sure, please let us know if that pops up again and we can take it from there.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"warn invalid valu properti root dir hello defin log dir directori code resid receiv follow warn warn invalid valu properti root dir home experi rais error futur normal particularli concern warn said rais error futur like know consid valid valu properti root dir defin wai rais error road thank advanc",
        "Question_preprocessed_content":"warn invalid valu properti hello defin directori code resid receiv follow warn warn invalid valu properti rais error futur normal particularli concern warn said rais error futur like know consid valid valu properti defin wai rais error road thank advanc",
        "Question_gpt_summary_original":"The user encountered a warning message while defining wandb_log_dir to be a directory different from where their code resides. The warning message stated that the value for property root_dir was invalid and would raise an error in the future. The user is seeking clarification on what is considered a valid value for root_dir and why defining it the way they did will raise an error in the future.",
        "Question_gpt_summary":"user encount warn messag defin log dir directori differ code resid warn messag state valu properti root dir invalid rais error futur user seek clarif consid valid valu root dir defin wai rais error futur",
        "Answer_original_content":"rcheng mean unexpect set throw error releas kwarg ignor order initialis run default valu instead come line sdk code provid bit inform set paramet environ variabl init possibl share code snippet line give warn your thano thank info lt set paramet run pytorch lightn logger import logger log dir path config log param save dir arg run logger logger save dir log dir arg run project beta vae config yaml file log param save dir log home experi warn pop manual seed betava actual realiz warn pop regardless save dir code resid complet differ directori origin thought warn show expect log dir directori code thank advanc hint rcheng thank addit inform tri reproduc issu code snippet didnt warn explicitli set save dir save dir home dir dir experi work path pathlib packag print log dir post output rcheng want follow issu occur work thank rcheng havent heard ahead close ticket warn occur let know happi investig thano sorri delai respons warn went awai report show come dai thank hint rcheng glad hear thank lot updat sure let know pop",
        "Answer_preprocessed_content":"mean unexpect set throw error releas ignor order initialis run default valu instead come line sdk code provid bit inform set paramet environ variabl init possibl share code snippet line give warn your thank info lt set paramet file actual realiz warn pop regardless code resid complet differ directori thank advanc hint thank addit inform tri reproduc issu code snippet didnt warn explicitli set work packag post output want follow issu occur work thank havent heard ahead close ticket warn occur let know happi investig sorri delai respons warn went awai report show come dai thank hint glad hear thank lot updat sure let know pop",
        "Answer_gpt_summary_original":"there is no clear solution provided in the answer. the answer is a conversation between the user and the support team trying to gather more information about the issue and suggesting possible solutions to try. the support team asks for more information about how the parameter was set up and suggests explicitly setting the save_dir parameter to a specific directory. the user reports that the warning went away and will report back if it shows up again.",
        "Answer_gpt_summary":"clear solut provid answer answer convers user support team try gather inform issu suggest possibl solut try support team ask inform paramet set suggest explicitli set save dir paramet specif directori user report warn went awai report show"
    },
    {
        "Question_id":null,
        "Question_title":"How to access the data of specific step in the dashboard of wandb.ai?",
        "Question_body":"<p>I am using the dashboard of wandb and want to access the specific step data. However, I can only obtain the metric throughout the whole training step. For example, I wanna get the best mAP in COCO evaluation, and corresponding AP50, AP70 and other evaluation metrics.<\/p>\n<p>Need help. Thanks for your help!<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1646403651386,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":178.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-access-the-data-of-specific-step-in-the-dashboard-of-wandb-ai\/2017",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-09T00:51:13.714Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/djh\">@djh<\/a>,<\/p>\n<p>Could you share the workspace for which you are looking to compute these metrics? The context of the workspace would help me understand your request better.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-09T01:21:03.575Z",
                "Answer_body":"<p>The request is simple, I just cannot find how to access the entire record of one specific step in the dashboard. For example, I have recorded the log metric (loss, acc, .etc.) at each step, but in the dashboard, I can only see the line chart for loss or acc varying from step. My real demand is to access the log information at a certain step. Thanks for your reply!<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-10T21:45:39.535Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/djh\">@djh<\/a>,<\/p>\n<p>This should be possible through our <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/weave#introduction\">Weave Tables<\/a>. The <code>runs.history<\/code> metric holds all the step information and the weave table will let you hold it in a Tabular Format.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-14T21:39:10.199Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/djh\">@djh<\/a>,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<\/p>\n<p>Weights &amp; Biases<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-15T02:44:21.815Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a> , thanks for your instructions, Weave Tables meets my expectation.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-14T02:44:29.536Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"access data specif step dashboard dashboard want access specif step data obtain metric train step exampl wanna best map coco evalu correspond evalu metric need help thank help",
        "Question_preprocessed_content":"access data specif step dashboard dashboard want access specif step data obtain metric train step exampl wanna best map coco evalu correspond evalu metric need help thank help",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing specific step data in the dashboard of wandb.ai. They are only able to obtain metrics throughout the whole training step and are unable to access evaluation metrics such as best mAP in COCO evaluation, and corresponding AP50, AP70.",
        "Question_gpt_summary":"user face challeng access specif step data dashboard abl obtain metric train step unabl access evalu metric best map coco evalu correspond",
        "Answer_original_content":"djh share workspac look comput metric context workspac help understand request better thank ramit request simpl access entir record specif step dashboard exampl record log metric loss acc step dashboard line chart loss acc vari step real demand access log inform certain step thank repli djh possibl weav tabl run histori metric hold step inform weav tabl let hold tabular format thank ramit djh want follow support request heard let know assist issu resolv best ramit goolri thank instruct weav tabl meet expect topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"share workspac look comput metric context workspac help understand request better thank ramit request simpl access entir record specif step dashboard exampl record log metric step dashboard line chart loss acc vari step real demand access log inform certain step thank repli possibl weav tabl metric hold step inform weav tabl let hold tabular format thank ramit want follow support request heard let know assist issu resolv best thank instruct weav tabl meet expect topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user is trying to access the log information of a specific step in the dashboard of .ai, but is only able to obtain the metric throughout the whole training step. the solution to this problem is to use weave tables, which holds all the step information in a tabular format.",
        "Answer_gpt_summary":"user try access log inform specif step dashboard abl obtain metric train step solut problem us weav tabl hold step inform tabular format"
    },
    {
        "Question_id":65577286.0,
        "Question_title":"Dlr model gives notorious result for object detection model",
        "Question_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_neo_compilation_jobs\/gluoncv_ssd_mobilenet\/gluoncv_ssd_mobilenet_neo.ipynb\" rel=\"nofollow noreferrer\">link<\/a> to train an object detection model. I am able to successfully deploy the model on EC2 instance. The accuracy was good. I complied the same model file for m edge Device Jetson Nano. My inference code looks like below,<\/p>\n<pre><code>from dlr import DLRModel\nimport json\nimport cv2\n\nmodel = DLRModel('model', 'gpu')\n\nimg = cv2.imread('test.jpg')\nimg = cv2.resize(img, (512, 512), interpolation=cv2.INTER_AREA)\nimg = np.expand_dims(img, 0)\n\noutputs = model.run(img)\nobjects=outputs[0][0]\nscores=outputs[1][0]\nbounding_boxes=outputs[2][0]\n<\/code><\/pre>\n<p>When I look the result, it's not at all matched with SageMaker Notebook instance result. Boudning boxes' values are sometime in ~70000. I couldn't understand the format of result produced by DLR.<\/p>\n<p>Sample result for an image.<\/p>\n<p>Classes:\n[[10.0], [14.0], [4.0], [10.0], [14.0], [-1.0], [-1.0], [-1.0], [7.0], [-1.0], [6.0], [11.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [2.0], [-1.0], [-1.0], [0.0], [-1.0], [17.0], [-1.0], [-1.0], [6.0], [18.0], [-1.0], [-1.0], [-1.0], [18.0], [-1.0], [12.0], [-1.0], [-1.0], [13.0], [-1.0], [-1.0], [-1.0], [1.0], [-1.0], [-1.0], [5.0], [-1.0], [0.0], [-1.0], [-1.0], [-1.0], [-1.0], [3.0], [8.0], [5.0], [-1.0], [-1.0], [15.0], [-1.0], [9.0], [3.0], [-1.0], [10.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [16.0], [8.0], [-1.0], [16.0], [19.0], [-1.0], [9.0], [-1.0], [4.0], [-1.0], [-1.0], [15.0], [10.0], [-1.0], [-1.0], [4.0], [-1.0], [8.0], [2.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>scores:\n[[0.9527158737182617], [0.910746157169342], [0.28013306856155396], [0.059000786393880844], [0.04898739233613014], [-1.0], [-1.0], [-1.0], [0.04864144325256348], [-1.0], [0.04847110062837601], [0.04843416064977646], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.04821664094924927], [-1.0], [-1.0], [0.04808368161320686], [-1.0], [0.0479729063808918], [-1.0], [-1.0], [0.04782549664378166], [0.04778601974248886], [-1.0], [-1.0], [-1.0], [0.0475776381790638], [-1.0], [0.047538403421640396], [-1.0], [-1.0], [0.047468967735767365], [-1.0], [-1.0], [-1.0], [0.04737424850463867], [-1.0], [-1.0], [0.047330085188150406], [-1.0], [0.04730956256389618], [-1.0], [-1.0], [-1.0], [-1.0], [0.04710235074162483], [0.04710135608911514], [0.047083333134651184], [-1.0], [-1.0], [0.047033149749040604], [-1.0], [0.0469636432826519], [0.046939317137002945], [-1.0], [0.04687687009572983], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [0.046708278357982635], [0.046680934727191925], [-1.0], [0.04660974070429802], [0.046597886830568314], [-1.0], [0.04656397923827171], [-1.0], [0.046513814479112625], [-1.0], [-1.0], [0.04647510126233101], [0.04644943028688431], [-1.0], [-1.0], [0.046245038509368896], [-1.0], [0.01647786796092987], [0.010514501482248306], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0], [-1.0]]<\/p>\n<p>Bouding boxes:\n[[523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [680.2073364257812, 542.9566650390625, 680.2202758789062, 543.36376953125], [425.0546875, -1141093.0, 429.6099853515625, 1142524.5], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [94.88882446289062, -59.820770263671875, 417.0069885253906, 571.7384033203125], [21.017044067382812, 20.89019775390625, 491.50994873046875, 491.19842529296875], [30.993896484375, 30.63165283203125, 480.84320068359375, 481.7999267578125], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [-60.170318603515625, 95.33354187011719, 571.584716796875, 417.34515380859375], [591.7652587890625, -1121890.0, 592.9493408203125, 1123299.75], [523.2319946289062, -777751.875, 523.5722045898438, 778992.625], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0], [-1.0, -1.0, -1.0, -1.0]]<\/p>\n<p>What causes this notorious result?\nIs there any issue while compiling model in Neo?\nAny issue in inference Code?<\/p>\n<p>Any hint would be appreciable.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1609843193240,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":98.0,
        "Answer_body":"<p>The reason behind the abnormal result is due to improper pre-processing method was applied. Here is the complete inference code for Mobilenet-ssd model.<\/p>\n<pre><code>def transform(img):\n    # Normalize\n    mean_vec = np.array([0.485, 0.456, 0.406])\n    stddev_vec = np.array([0.229, 0.224, 0.225])\n    image = (img \/ 255 - mean_vec) \/ stddev_vec\n\n    # Transpose\n    if len(image.shape) == 2:  # for greyscale image\n        image = np.expand_dims(image, axis=2)\n\n    image = np.rollaxis(image, axis=2, start=0)[np.newaxis, :]\n    return image\n\nmodel = DLRModel(model_dir, 'gpu')\n\n\nfor file_name in image_folder:\n    image = PIL.Image.open(file_name)\n    image = np.asarray(image.resize((512, 512)))\n    image = transform(image)\n    # flatten within a input array\n    input_data = {'data': image}\n    outputs = model.run(input_data)\n    objects = outputs[0]\n    scores = outputs[1]\n    bounding_boxes = outputs[2]\n    result = [objects.tolist(), scores.tolist(), bounding_boxes.tolist()]\n    print(json.dumps(result))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65577286",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1609912383447,
        "Question_original_content":"dlr model give notori result object detect model follow link train object detect model abl successfulli deploi model instanc accuraci good compli model file edg devic jetson nano infer code look like dlr import dlrmodel import json import model dlrmodel model gpu img imread test jpg img resiz img interpol inter area img expand dim img output model run img object output score output bound box output look result match notebook instanc result boudn box valu couldn understand format result produc dlr sampl result imag class score boud box caus notori result issu compil model neo issu infer code hint appreci",
        "Question_preprocessed_content":"dlr model give notori result object detect model follow link train object detect model abl successfulli deploi model instanc accuraci good compli model file edg devic jetson nano infer code look like look result match notebook instanc result boudn box valu couldn understand format result produc dlr sampl result imag class score boud box caus notori result issu compil model neo issu infer code hint appreci",
        "Question_gpt_summary_original":"the user is encountering challenges with their object detection model, as the results produced by the dlr model are not matching the results from the notebook instance, with bounding boxes values sometimes reaching ~70000.",
        "Question_gpt_summary":"user encount challeng object detect model result produc dlr model match result notebook instanc bound box valu reach",
        "Answer_original_content":"reason abnorm result improp pre process method appli complet infer code mobilenet ssd model def transform img normal mean vec arrai stddev vec arrai imag img mean vec stddev vec transpos len imag shape greyscal imag imag expand dim imag axi imag rollaxi imag axi start newaxi return imag model dlrmodel model dir gpu file imag folder imag pil imag open file imag asarrai imag resiz imag transform imag flatten input arrai input data data imag output model run input data object output score output bound box output result object tolist score tolist bound box tolist print json dump result",
        "Answer_preprocessed_content":"reason abnorm result improp method appli complet infer code model",
        "Answer_gpt_summary_original":"the solution to the abnormal results in the object detection model is to use the proper pre-processing method. the answer provides the complete inference code for the mobilenet-ssd model, which includes a transform function that normalizes and transposes the image. the code also flattens the input array and runs the model to produce the objects, scores, and bounding boxes.",
        "Answer_gpt_summary":"solut abnorm result object detect model us proper pre process method answer provid complet infer code mobilenet ssd model includ transform function normal transpos imag code flatten input arrai run model produc object score bound box"
    },
    {
        "Question_id":null,
        "Question_title":"Best practice for python package dependency?",
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m figuring out how to structure my project and found your tutorials a very helpful introduction.<\/p>\n<p>In them, however, you only rely on a single script for each stage, for example:<\/p>\n<pre><code class=\"lang-auto\">dvc run -d code\/xml_to_tsv.py -d data\/Posts.xml -o data\/Posts.tsv \\\n          -f prepare.dvc \\\n          python code\/xml_to_tsv.py data\/Posts.xml data\/Posts.tsv\n<\/code><\/pre>\n<p><a href=\"https:\/\/dvc.org\/doc\/tutorials\/pipelines\" rel=\"nofollow noopener\">Source<\/a><\/p>\n<p>Now my understanding is the following:<br>\nIf inside of <code>xml_to_tsv.py<\/code> are imports of some other of my libraries (e.g., a <code>my_special_xml_importer.py<\/code>), and I make changes to <code>my_special_xml_importer.py<\/code>, these changes would not be picked up by dvc, since <code>my_special_xml_importer.py<\/code> is not an explicit dependency of the stage, correct?<\/p>\n<p>What\u2019s the best practice here for bigger projects, where each DVC stage is not just contained in a single script?<\/p>\n<p>Our use case will be the following: For each stage we\u2019ll be having a jupyter notebook, which will import some of our python packages. I\u2019m assuming I should create a stage like this:<\/p>\n<pre><code class=\"lang-auto\">dvc run -d my_notebook.ipynb -d code\/my_lib.py -d data\/Posts.xml -o data\/Posts.tsv\n  -f prepare.dvc\n   papermill my_notebook.ipynb my_notebook_out.ipynb\n<\/code><\/pre>\n<p>Is this a good way, are there other ways, how are people with bigger projects dealing with this issue?<\/p>\n<p>Thanks in advance,<br>\nFabi<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1587641178959,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":648.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-python-package-dependency\/358",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-04-24T00:33:36.522Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rabefabi\">@rabefabi<\/a>!<\/p>\n<p>The reasonable way is to put the code that is very specific to a stage into a separate directory:<\/p>\n<p><code>dvc run -d train ... train\/train.py ...<\/code><\/p>\n<p>In this case, also use <code>.dvcignore<\/code> to exclude <strong>pycache<\/strong>.<\/p>\n<p>I don\u2019t know an easy way for DVC to analyze the actual graph of dependencies build and maintain them. It can be probably done with some custom scripts though.<\/p>",
                "Answer_score":23.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-27T06:52:37.303Z",
                "Answer_body":"<p>Putting the notebooks in each stages subdirectory should work for us, thanks for the hint<\/p>",
                "Answer_score":47.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"best practic python packag depend figur structur project tutori help introduct reli singl script stage exampl run code xml tsv data post xml data post tsv prepar python code xml tsv data post xml data post tsv sourc understand follow insid xml tsv import librari special xml import chang special xml import chang pick special xml import explicit depend stage correct what best practic bigger project stage contain singl script us case follow stage have jupyt notebook import python packag assum creat stage like run notebook ipynb code lib data post xml data post tsv prepar papermil notebook ipynb notebook ipynb good wai wai peopl bigger project deal issu thank advanc fabi",
        "Question_preprocessed_content":"best practic python packag depend figur structur project tutori help introduct reli singl script stage exampl sourc understand follow insid import librari chang chang pick explicit depend stage correct what best practic bigger project stage contain singl script us case follow stage have jupyt notebook import python packag assum creat stage like good wai wai peopl bigger project deal issu thank advanc fabi",
        "Question_gpt_summary_original":"The user is seeking advice on the best practice for managing dependencies in a larger Python project using DVC. They are concerned that changes made to imported libraries will not be picked up by DVC since they are not explicit dependencies of the stage. The user is considering using a Jupyter notebook for each stage and is asking for advice on how to structure their project.",
        "Question_gpt_summary":"user seek advic best practic manag depend larger python project concern chang import librari pick explicit depend stage user consid jupyt notebook stage ask advic structur project",
        "Answer_original_content":"rabefabi reason wai code specif stage separ directori run train train train case us ignor exclud pycach dont know easi wai analyz actual graph depend build maintain probabl custom script put notebook stage subdirectori work thank hint",
        "Answer_preprocessed_content":"reason wai code specif stage separ directori case us exclud pycach dont know easi wai analyz actual graph depend build maintain probabl custom script put notebook stage subdirectori work thank hint",
        "Answer_gpt_summary_original":"the answer suggests putting the code specific to each stage in a separate directory and using .ignore to exclude pycache. however, there is no easy way to analyze the actual graph of dependencies and maintain them, but it can be done with custom scripts. additionally, putting the notebooks in each stage's subdirectory is a helpful hint.",
        "Answer_gpt_summary":"answer suggest put code specif stage separ directori ignor exclud pycach easi wai analyz actual graph depend maintain custom script addition put notebook stage subdirectori help hint"
    },
    {
        "Question_id":70449117.0,
        "Question_title":"Vertex AI Endpoints scales to 0 before increasing number of replicas",
        "Question_body":"<p>I have an endpoint in <code>us-east<\/code> which serves a custom imported model (docker image).<\/p>\n<p>This endpoint uses <code>min replicas = 1<\/code> and <code>max replicas = 100<\/code>.<\/p>\n<p>Sometimes, Vertex AI will require the model to scale from 1 to 2.<\/p>\n<p>However, there seems to be an issue causing the number of replicas to go from <code>1 -&gt; 0 -&gt; 2<\/code> instead of <code>1 -&gt; 2<\/code>.<\/p>\n<p>This causes several 504 (Gateway Timeout) errors in my API and the way to solve that was setting <code>min replicas &gt; 1<\/code>, highly impacting the monthly cost of the application.<\/p>\n<p>Is this some known issue to Vertex AI\/GCP services, is there anyway to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640176354140,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":77.0,
        "Answer_body":"<p>The intermittent <code>504<\/code> errors could be a result of an endpoint that is under-provisioned to handle the load. It can also happen if too many prediction requests are sent to the endpoint before it has a chance to scale up.<\/p>\n<p>Traffic splitting of the incoming prediction requests is done randomly. So, multiple requests may end up on the same model server at the same time. This can happen even if the overall Queries Per Second (QPS) is low, and especially when the QPS is spiky. This contributes to the requests being queued up if the model server isn't able to handle the load. This is what results in a 504 error.<\/p>\n<p><strong>Recommendations to mitigate the <code>504<\/code> errors are as follows:<\/strong><\/p>\n<ul>\n<li><p>Improve the container's ability to use all resources in the container. One thing to keep in mind about resource utilization is whether the model server is single-threaded or multi-threaded. The container may not be using up all the cores and\/or requests may be queuing up, hence are served only one-at-a-time.<\/p>\n<\/li>\n<li><p>Autoscaling is happening, it just might need to be tuned to the prediction workload and expectations. A lower utilization threshold would trigger autoscaling sooner.<\/p>\n<\/li>\n<li><p>Perform an exponential backoff while the deployment is scaling. This way, there is a retry mechanism to handle failed requests.<\/p>\n<\/li>\n<li><p>Provision a higher minimum replica count for the endpoint, which you have already implemented.<\/p>\n<\/li>\n<\/ul>\n<p>If the above recommendations do not solve the problem or in general require further investigation of these errors, please reach out to <a href=\"https:\/\/cloud.google.com\/support-hub#section-2\" rel=\"nofollow noreferrer\">GCP support<\/a> in case you have a <a href=\"https:\/\/cloud.google.com\/support\/\" rel=\"nofollow noreferrer\">support plan<\/a>. Otherwise, please open an issue in the <a href=\"https:\/\/issuetracker.google.com\/issues\/new?component=1134259&amp;template=1640573\" rel=\"nofollow noreferrer\">issue tracker<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1641204696607,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70449117",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641201707430,
        "Question_original_content":"endpoint scale increas number replica endpoint east serv custom import model docker imag endpoint us min replica max replica requir model scale issu caus number replica instead caus gatewai timeout error api wai solv set min replica highli impact monthli cost applic known issu gcp servic fix",
        "Question_preprocessed_content":"endpoint scale increas number replica endpoint serv custom import model endpoint us requir model scale issu caus number replica instead caus error api wai solv set highli impact monthli cost applic known issu servic fix",
        "Question_gpt_summary_original":"The user is encountering challenges with Vertex AI Endpoints scaling from 1 to 2 replicas. Instead of going from 1 to 2, the number of replicas goes from 1 to 0 to 2, causing 504 errors in the API. The user had to set the minimum replicas to a value greater than 1 to solve the issue, which increased the monthly cost of the application. The user is seeking a solution to this issue and wondering if it is a known issue with Vertex AI\/GCP services.",
        "Question_gpt_summary":"user encount challeng endpoint scale replica instead go number replica goe caus error api user set minimum replica valu greater solv issu increas monthli cost applic user seek solut issu wonder known issu gcp servic",
        "Answer_original_content":"intermitt error result endpoint provis handl load happen predict request sent endpoint chanc scale traffic split incom predict request randomli multipl request end model server time happen overal queri second qp low especi qp spiki contribut request queu model server isn abl handl load result error recommend mitig error follow improv contain abil us resourc contain thing mind resourc util model server singl thread multi thread contain core request queu serv time autosc happen need tune predict workload expect lower util threshold trigger autosc sooner perform exponenti backoff deploy scale wai retri mechan handl fail request provis higher minimum replica count endpoint implement recommend solv problem gener requir investig error reach gcp support case support plan open issu issu tracker",
        "Answer_preprocessed_content":"intermitt error result endpoint handl load happen predict request sent endpoint chanc scale traffic split incom predict request randomli multipl request end model server time happen overal queri second low especi qp spiki contribut request queu model server isn abl handl load result error recommend mitig error follow improv contain abil us resourc contain thing mind resourc util model server contain core request queu serv autosc happen need tune predict workload expect lower util threshold trigger autosc sooner perform exponenti backoff deploy scale wai retri mechan handl fail request provis higher minimum replica count endpoint implement recommend solv problem gener requir investig error reach gcp support case support plan open issu issu tracker",
        "Answer_gpt_summary_original":"possible solutions to mitigate the 504 errors encountered by the user are improving the container's ability to use all resources, tuning autoscaling to the prediction workload, performing exponential backoff while scaling, and provisioning a higher minimum replica count for the endpoint. if these recommendations do not solve the problem, the user can reach out to gcp support or open an issue in the issue tracker.",
        "Answer_gpt_summary":"possibl solut mitig error encount user improv contain abil us resourc tune autosc predict workload perform exponenti backoff scale provis higher minimum replica count endpoint recommend solv problem user reach gcp support open issu issu tracker"
    },
    {
        "Question_id":52664415.0,
        "Question_title":"Why we need ML batch execution and update resource option in azure data factory",
        "Question_body":"<p>Why we need to ML Batch Execution and ML Update resource option in Data factory ? How this can be used to retrain machine learning when updating a blob file ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8KWH0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1538738198597,
        "Question_favorite_count":null,
        "Question_last_edit_time":1538846687100,
        "Question_score":1.0,
        "Question_view_count":141.0,
        "Answer_body":"<p>Ml Batch Execution- to call retraining experiment and get a .ilearner file as output.\nML Update Resource- Use the above .ilearner as input and call patch endpoint of predictive web service to Update resource.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52664415",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1539774604663,
        "Question_original_content":"need batch execut updat resourc option azur data factori need batch execut updat resourc option data factori retrain machin learn updat blob file",
        "Question_preprocessed_content":"need batch execut updat resourc option azur data factori need batch execut updat resourc option data factori retrain machin learn updat blob file",
        "Question_gpt_summary_original":"The user is questioning the need for ML Batch Execution and ML Update resource options in Azure Data Factory and how they can be used to retrain machine learning when updating a blob file.",
        "Question_gpt_summary":"user question need batch execut updat resourc option azur data factori retrain machin learn updat blob file",
        "Answer_original_content":"batch execut retrain experi ilearn file output updat resourc us ilearn input patch endpoint predict web servic updat resourc",
        "Answer_preprocessed_content":"batch execut retrain experi ilearn file output updat resourc us ilearn input patch endpoint predict web servic updat resourc",
        "Answer_gpt_summary_original":"possible solutions to retrain machine learning when updating a blob file in azure data factory include using ml batch execution to call a retraining experiment and obtain a .ilearner file as output. then, using ml update resource to use the .ilearner as input and call the patch endpoint of the predictive web service to update the resource.",
        "Answer_gpt_summary":"possibl solut retrain machin learn updat blob file azur data factori includ batch execut retrain experi obtain ilearn file output updat resourc us ilearn input patch endpoint predict web servic updat resourc"
    },
    {
        "Question_id":null,
        "Question_title":"How to improve particular tag accuracy in Form recognizer",
        "Question_body":"as you can see, I made a custom model(OCR) and trained it. But only some tags are well learned, and others are not well learned poorly. even if additional learning data set is added, training error occurs and learning is no longer possible.\nMy data format is pdf and not all parts of the file are tagged, but only some of them are tagged.\n\nis there a way to increase the accuracy of particular tag?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623650782897,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/434228\/how-to-improve-particular-tag-accuracy-in-form-rec.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-14T11:22:56.1Z",
                "Answer_score":0,
                "Answer_body":"@yurileeDBMSKE-4795 Thanks for the question. Can you please add more details about the training error that you are getting. You can improve model accuracy by labeling additional forms and retraining to create a new model. We recommend starting by labeling five forms and adding more forms as needed.\n\nPlease checkout the the Knowledge Extraction Recipes resource.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"improv particular tag accuraci form recogn custom model ocr train tag learn learn poorli addit learn data set ad train error occur learn longer possibl data format pdf part file tag tag wai increas accuraci particular tag",
        "Question_preprocessed_content":"improv particular tag accuraci form recogn custom model train tag learn learn poorli addit learn data set ad train error occur learn longer possibl data format pdf part file tag tag wai increas accuraci particular tag",
        "Question_gpt_summary_original":"The user has created a custom OCR model and trained it, but some tags are not well learned despite adding more data. The data format is PDF and only some parts of the file are tagged. The user is seeking a way to improve the accuracy of specific tags.",
        "Question_gpt_summary":"user creat custom ocr model train tag learn despit ad data data format pdf part file tag user seek wai improv accuraci specif tag",
        "Answer_original_content":"yurileedbmsk thank question add detail train error get improv model accuraci label addit form retrain creat new model recommend start label form ad form need checkout knowledg extract recip resourc",
        "Answer_preprocessed_content":"thank question add detail train error get improv model accuraci label addit form retrain creat new model recommend start label form ad form need checkout knowledg extract recip resourc",
        "Answer_gpt_summary_original":"possible solutions to improve the accuracy of particular tags in a form recognizer custom model are to label additional forms and retrain the model to create a new one. it is recommended to start with labeling five forms and adding more as needed. the user can also check out the knowledge extraction recipes resource for more information.",
        "Answer_gpt_summary":"possibl solut improv accuraci particular tag form recogn custom model label addit form retrain model creat new recommend start label form ad need user check knowledg extract recip resourc inform"
    },
    {
        "Question_id":null,
        "Question_title":"Google Vertex AI Automl Model ID is invalid. It should start with 3 letters Error",
        "Question_body":"",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1668912000000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":30.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Vertex-AI-Automl-Model-ID-is-invalid-It-should-start-with\/td-p\/491126\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nYou get the error because you are using the old AutoML API to run predictions. See model_id created when the old AutoML API is used.\n\nSince you have trained your model using Vertex AutoML Classification (you got the 18 digit number), you should use aiplatform to run your predictions. See sample prediction code."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"automl model invalid start letter error ",
        "Question_preprocessed_content":"automl model invalid start letter error ",
        "Question_gpt_summary_original":"The user is facing an error with Google Vertex AI Automl Model ID, which is invalid and should start with 3 letters.",
        "Question_gpt_summary":"user face error automl model invalid start letter",
        "Answer_original_content":"error old automl api run predict model creat old automl api train model vertex automl classif got digit number us aiplatform run predict sampl predict code",
        "Answer_preprocessed_content":"error old automl api run predict creat old automl api train model vertex automl classif us aiplatform run predict sampl predict code",
        "Answer_gpt_summary_original":"summary: the error occurred because the user was using an old automl api to create a model. the solution is to use aiplatform to run predictions since the model was trained using vertex automl classification. the user can refer to the sample prediction code for guidance.",
        "Answer_gpt_summary":"summari error occur user old automl api creat model solut us aiplatform run predict model train vertex automl classif user refer sampl predict code guidanc"
    },
    {
        "Question_id":null,
        "Question_title":"How to install wandb on a docker image for arm?",
        "Question_body":"<p>My docker building failed at the <code>RUN <\/code><\/p>\n<p>with:<\/p>\n<pre><code class=\"lang-auto\">(meta_learning) brandomiranda~ \u276f docker build -f ~\/iit-term-synthesis\/Dockerfile_arm -t brandojazz\/iit-term-synthesis:test_arm ~\/iit-term-synthesis\/\n\n[+] Building 184.7s (20\/28)\n =&gt; [internal] load build definition from Dockerfile_arm                                                                                           0.0s\n =&gt; =&gt; transferring dockerfile: 41B                                                                                                                0.0s\n =&gt; [internal] load .dockerignore                                                                                                                  0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                    0.0s\n =&gt; [internal] load metadata for docker.io\/continuumio\/miniconda3:latest                                                                           0.0s\n =&gt; [ 1\/24] FROM docker.io\/continuumio\/miniconda3                                                                                                  0.0s\n =&gt; https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main                                                                                     0.3s\n =&gt; CACHED [ 2\/24] RUN apt-get update   &amp;&amp; apt-get install -y --no-install-recommends     ssh     git     m4     libgmp-dev     opam     wget      0.0s\n =&gt; CACHED [ 3\/24] RUN useradd -m bot                                                                                                              0.0s\n =&gt; CACHED [ 4\/24] WORKDIR \/home\/bot                                                                                                               0.0s\n =&gt; CACHED [ 5\/24] ADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json                                                     0.0s\n =&gt; CACHED [ 6\/24] RUN opam init --disable-sandboxing                                                                                              0.0s\n =&gt; CACHED [ 7\/24] RUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda                     0.0s\n =&gt; CACHED [ 8\/24] RUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1                                                          0.0s\n =&gt; CACHED [ 9\/24] RUN eval $(opam env)                                                                                                            0.0s\n =&gt; CACHED [10\/24] RUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released                                                               0.0s\n =&gt; CACHED [11\/24] RUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released                                  0.0s\n =&gt; CACHED [12\/24] RUN opam update --all                                                                                                           0.0s\n =&gt; CACHED [13\/24] RUN opam pin add -y coq 8.11.0                                                                                                  0.0s\n =&gt; [14\/24] RUN opam install -y coq-serapi                                                                                                       176.3s\n =&gt; [15\/24] RUN eval $(opam env)                                                                                                                   0.2s\n =&gt; ERROR [16\/24] RUN pip install wandb --upgrade                                                                                                  8.0s\n------\n &gt; [16\/24] RUN pip install wandb --upgrade:\n#20 0.351 Defaulting to user installation because normal site-packages is not writeable\n#20 0.637 Collecting wandb\n#20 0.986   Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n#20 1.365 Requirement already satisfied: setuptools in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (61.2.0)\n#20 1.366 Requirement already satisfied: six&gt;=1.13.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (1.16.0)\n#20 1.409 Collecting promise&lt;3,&gt;=2.0\n#20 1.472   Downloading promise-2.3.tar.gz (19 kB)\n#20 2.087 Collecting PyYAML\n#20 2.154   Downloading PyYAML-6.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (731 kB)\n#20 2.431 Collecting protobuf&lt;4.0dev,&gt;=3.12.0\n#20 2.492   Downloading protobuf-3.20.1-cp39-cp39-manylinux2014_aarch64.whl (917 kB)\n#20 2.648 Collecting setproctitle\n#20 2.706   Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30 kB)\n#20 2.763 Collecting Click!=8.0.0,&gt;=7.0\n#20 2.818   Downloading click-8.1.3-py3-none-any.whl (96 kB)\n#20 2.902 Collecting sentry-sdk&gt;=1.0.0\n#20 2.962   Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n#20 3.112 Collecting psutil&gt;=5.0.0\n#20 3.172   Downloading psutil-5.9.2.tar.gz (479 kB)\n#20 3.871 Collecting pathtools\n#20 3.937   Downloading pathtools-0.1.2.tar.gz (11 kB)\n#20 4.431 Collecting shortuuid&gt;=0.5.0\n#20 4.509   Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n#20 4.512 Requirement already satisfied: requests&lt;3,&gt;=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from wandb) (2.27.1)\n#20 4.568 Collecting docker-pycreds&gt;=0.4.0\n#20 4.636   Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n#20 4.695 Collecting GitPython&gt;=1.0.0\n#20 4.781   Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n#20 4.834 Collecting gitdb&lt;5,&gt;=4.0.1\n#20 4.892   Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n#20 4.934 Collecting smmap&lt;6,&gt;=3.0.1\n#20 4.992   Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n#20 5.005 Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (1.26.8)\n#20 5.005 Requirement already satisfied: certifi&gt;=2017.4.17 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2021.10.8)\n#20 5.006 Requirement already satisfied: idna&lt;4,&gt;=2.5 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (3.3)\n#20 5.006 Requirement already satisfied: charset-normalizer~=2.0.0 in \/opt\/conda\/lib\/python3.9\/site-packages (from requests&lt;3,&gt;=2.0.0-&gt;wandb) (2.0.4)\n#20 5.075 Collecting urllib3&lt;1.27,&gt;=1.21.1\n#20 5.135   Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n#20 5.172 Building wheels for collected packages: promise, psutil, pathtools\n#20 5.172   Building wheel for promise (setup.py): started\n#20 5.851   Building wheel for promise (setup.py): finished with status 'done'\n#20 5.852   Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=6de0373376d2a8e995959e6173507e13cba502c79b648b5884b1eac45d1ec9ae\n#20 5.852   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/e1\/e8\/83\/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n#20 5.854   Building wheel for psutil (setup.py): started\n#20 6.226   Building wheel for psutil (setup.py): finished with status 'error'\n#20 6.226   ERROR: Command errored out with exit status 1:\n#20 6.226    command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-4y62c4eb\n#20 6.226        cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 6.226   Complete output (45 lines):\n#20 6.226   running bdist_wheel\n#20 6.226   running build\n#20 6.226   running build_py\n#20 6.226   creating build\n#20 6.226   creating build\/lib.linux-aarch64-3.9\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 6.226   creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 6.226   running build_ext\n#20 6.226   building 'psutil._psutil_linux' extension\n#20 6.226   creating build\/temp.linux-aarch64-3.9\n#20 6.226   creating build\/temp.linux-aarch64-3.9\/psutil\n#20 6.226   gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 6.226   error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 6.226   ----------------------------------------\n#20 6.226   ERROR: Failed building wheel for psutil\n#20 6.226   Running setup.py clean for psutil\n#20 6.550   Building wheel for pathtools (setup.py): started\n#20 7.135   Building wheel for pathtools (setup.py): finished with status 'done'\n#20 7.135   Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=8e205a0f68c9c7a3c0107d1cc40d94f1d2843c78270217378dcbe98212958b82\n#20 7.135   Stored in directory: \/home\/bot\/.cache\/pip\/wheels\/b7\/0a\/67\/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n#20 7.136 Successfully built promise pathtools\n#20 7.136 Failed to build psutil\n#20 7.195 Installing collected packages: smmap, urllib3, gitdb, shortuuid, setproctitle, sentry-sdk, PyYAML, psutil, protobuf, promise, pathtools, GitPython, docker-pycreds, Click, wandb\n#20 7.262   WARNING: The script shortuuid is installed in '\/home\/bot\/.local\/bin' which is not on PATH.\n#20 7.262   Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n#20 7.345     Running setup.py install for psutil: started\n#20 7.727     Running setup.py install for psutil: finished with status 'error'\n#20 7.727     ERROR: Command errored out with exit status 1:\n#20 7.727      command: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil\n#20 7.727          cwd: \/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/\n#20 7.727     Complete output (47 lines):\n#20 7.727     running install\n#20 7.727     \/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/command\/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n#20 7.727       warnings.warn(\n#20 7.727     running build\n#20 7.727     running build_py\n#20 7.727     creating build\n#20 7.727     creating build\/lib.linux-aarch64-3.9\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psosx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psbsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_common.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pswindows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psposix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_compat.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pslinux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_pssunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     copying psutil\/_psaix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\n#20 7.727     creating build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__main__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_process.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_aix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_misc.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_bsd.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_linux.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/runner.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/__init__.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_connections.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_unicode.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_windows.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_contracts.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_sunos.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_testutils.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_osx.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_memleaks.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_posix.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     copying psutil\/tests\/test_system.py -&gt; build\/lib.linux-aarch64-3.9\/psutil\/tests\n#20 7.727     running build_ext\n#20 7.727     building 'psutil._psutil_linux' extension\n#20 7.727     creating build\/temp.linux-aarch64-3.9\n#20 7.727     creating build\/temp.linux-aarch64-3.9\/psutil\n#20 7.727     gcc -pthread -B \/opt\/conda\/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -I\/opt\/conda\/include -fPIC -O2 -n1 .2-a+fp16+rcpc+dotprod+crypto -isystem \/opt\/conda\/include -fPIC -DPSUTIL_POSIX=1 -DPSUTIL_SIZEOF_PID_T=4 -DPSUTIL_VERSION=592 -DPSUTIL_LINUX=1 -I\/opt\/conda\/include\/python3.9 -c psutil\/_psutil_common.c -o build\/temp.linux-aarch64-3.9\/psutil\/_psutil_common.o\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: .2-a+fp16+rcpc+dotprod+crypto: No such file or directory\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     gcc: error: unrecognized command-line option \u2018-n1\u2019; did you mean \u2018-n\u2019?\n#20 7.727     error: command '\/usr\/bin\/gcc' failed with exit code 1\n#20 7.727     ----------------------------------------\n#20 7.728 ERROR: Command errored out with exit status 1: \/opt\/conda\/bin\/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-vgietl2j\/psutil_c905945489d349018aaad0a17600df0b\/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record \/tmp\/pip-record-gb2y421d\/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers \/home\/bot\/.local\/include\/python3.9\/psutil Check the logs for full command output.\n------\nexecutor failed running [\/bin\/sh -c pip install wandb --upgrade]: exit code: 1\n<\/code><\/pre>\n<p>why?<\/p>\n<p>Docker file so far:<\/p>\n<pre><code class=\"lang-auto\">FROM continuumio\/miniconda3\n\nRUN apt-get update \\\n  &amp;&amp; apt-get install -y --no-install-recommends \\\n    ssh \\\n    git \\\n    m4 \\\n    libgmp-dev \\\n    opam \\\n    wget \\\n    ca-certificates \\\n    rsync \\\n    strace\n\nRUN useradd -m bot\nWORKDIR \/home\/bot\nUSER bot\n\n## https:\/\/stackoverflow.com\/questions\/73642349\/how-to-have-miniconda-work-properly-with-docker-especially-naming-my-conda-en\n#RUN wget https:\/\/repo.anaconda.com\/miniconda\/Miniconda3-latest-Linux-x86_64.sh  \\\n#    &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b -f\n#ENV PATH=\"\/home\/bot\/miniconda3\/bin:${PATH}\"\n#RUN conda create -n pycoq python=3.9 -y\n## somehow this \"works\" but conda isn't fully aware of this. Fix later?\n#ENV PATH=\"\/home\/bot\/miniconda3\/envs\/pycoq\/bin:${PATH}\"\n\nADD https:\/\/api.github.com\/repos\/IBM\/pycoq\/git\/refs\/heads\/main version.json\n\n# -- setup opam like VP's PyCoq\nRUN opam init --disable-sandboxing\n# compiler + '_' + coq_serapi + '.' + coq_serapi_pin\nRUN opam switch create ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1 ocaml-variants.4.07.1+flambda\nRUN opam switch ocaml-variants.4.07.1+flambda_coq-serapi.8.11.0+0.11.1\nRUN eval $(opam env)\n\nRUN opam repo add coq-released https:\/\/coq.inria.fr\/opam\/released\n# RUN opam pin add -y coq 8.11.0\n# ['opam', 'repo', '--all-switches', 'add', '--set-default', 'coq-released', 'https:\/\/coq.inria.fr\/opam\/released']\nRUN opam repo --all-switches add --set-default coq-released https:\/\/coq.inria.fr\/opam\/released\nRUN opam update --all\nRUN opam pin add -y coq 8.11.0\n\n#RUN opam install -y --switch ocaml-variants.4.07.1+flambda_coq-serapi_coq-serapi_8.11.0+0.11.1 coq-serapi 8.11.0+0.11.1\nRUN opam install -y coq-serapi\n\nRUN eval $(opam env)\n\n# makes sure depedencies for pycoq are installed once already in the docker image\nENV WANDB_API_KEY=\"SECRET\"\nRUN pip install wandb --upgrade\n<\/code><\/pre>\n<aside class=\"onebox stackexchange\" data-onebox-src=\"https:\/\/stackoverflow.com\/questions\/73642527\/how-to-install-wandb-on-a-docker-image-for-arm\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/stackoverflow.com\/questions\/73642527\/how-to-install-wandb-on-a-docker-image-for-arm\" target=\"_blank\" rel=\"noopener nofollow ugc\">stackoverflow.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img alt=\"Charlie Parker\" src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e9c0a0caedbda92f5ad9bc087e52e143936f9f5.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n  <\/a>\n\n<h4>\n  <a href=\"https:\/\/stackoverflow.com\/questions\/73642527\/how-to-install-wandb-on-a-docker-image-for-arm\" target=\"_blank\" rel=\"noopener nofollow ugc\">How to install wandb on a docker image for arm?<\/a>\n<\/h4>\n\n<div class=\"tags\">\n  <strong>python, linux, docker, anaconda<\/strong>\n<\/div>\n\n<div class=\"date\">\n  asked by\n  \n  <a href=\"https:\/\/stackoverflow.com\/users\/1601580\/charlie-parker\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    Charlie Parker\n  <\/a>\n  on <a href=\"https:\/\/stackoverflow.com\/questions\/73642527\/how-to-install-wandb-on-a-docker-image-for-arm\" target=\"_blank\" rel=\"noopener nofollow ugc\">12:07AM - 08 Sep 22 UTC<\/a>\n<\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1662595762611,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":780.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-install-wandb-on-a-docker-image-for-arm\/3080",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-09T21:12:35.448Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/brando\">@brando<\/a> , thank-you for writing in. There are several errors that appear, but primarily the <code>exit code 1<\/code> could be attributed to you building the container using the base image cached layers. As a preliminary step, can you please try to build the container again and use the command <code>--no-cache<\/code> when doing so. Please let me know what results from this.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-16T05:19:31.238Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/brando\">@brando<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-15T05:20:19.823Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"instal docker imag arm docker build fail run meta learn brandomiranda docker build iit term synthesi dockerfil arm brandojazz iit term synthesi test arm iit term synthesi build intern load build definit dockerfil arm transfer dockerfil intern load dockerignor transfer context intern load metadata docker continuumio miniconda latest docker continuumio miniconda http api github com repo ibm pycoq git ref head main cach run apt updat apt instal instal recommend ssh git libgmp dev opam wget cach run useradd bot cach workdir home bot cach add http api github com repo ibm pycoq git ref head main version json cach run opam init disabl sandbox cach run opam switch creat ocaml variant flambda coq serapi ocaml variant flambda cach run opam switch ocaml variant flambda coq serapi cach run eval opam env cach run opam repo add coq releas http coq inria opam releas cach run opam repo switch add set default coq releas http coq inria opam releas cach run opam updat cach run opam pin add coq run opam instal coq serapi run eval opam env error run pip instal upgrad run pip instal upgrad default user instal normal site packag writeabl collect download whl requir satisfi setuptool opt conda lib python site packag requir satisfi opt conda lib python site packag collect promis download promis tar collect pyyaml download pyyaml manylinux aarch manylinux aarch whl collect protobuf download protobuf manylinux aarch whl collect setproctitl download setproctitl manylinux aarch manylinux aarch whl collect click download click whl collect sentri sdk download sentri sdk whl collect psutil download psutil tar collect pathtool download pathtool tar collect shortuuid download shortuuid whl requir satisfi request opt conda lib python site packag collect docker pycr download docker pycr whl collect gitpython download gitpython whl collect gitdb download gitdb whl collect smmap download smmap whl requir satisfi urllib opt conda lib python site packag request requir satisfi certifi opt conda lib python site packag request requir satisfi idna opt conda lib python site packag request requir satisfi charset normal opt conda lib python site packag request collect urllib download urllib whl build wheel collect packag promis psutil pathtool build wheel promis setup start build wheel promis setup finish statu creat wheel promis filenam promis whl size sha dedaeeecbacbbbeacdeca store directori home bot cach pip wheel ddeadbbcececaad build wheel psutil setup start build wheel psutil setup finish statu error error command error exit statu command opt conda bin python import sy setuptool token sy argv tmp pip instal vgietlj psutil cddadfb setup file tmp pip instal vgietlj psutil cddadfb setup getattr token open open file path exist file stringio setuptool import setup setup code read replac close exec compil code file exec bdist wheel tmp pip wheel yceb cwd tmp pip instal vgietlj psutil cddadfb complet output line run bdist wheel run build run build creat build creat build lib linux aarch creat build lib linux aarch psutil copi psutil psosx build lib linux aarch psutil copi psutil psbsd build lib linux aarch psutil copi psutil common build lib linux aarch psutil copi psutil pswindow build lib linux aarch psutil copi psutil psposix build lib linux aarch psutil copi psutil init build lib linux aarch psutil copi psutil compat build lib linux aarch psutil copi psutil pslinux build lib linux aarch psutil copi psutil pssuno build lib linux aarch psutil copi psutil psaix build lib linux aarch psutil creat build lib linux aarch psutil test copi psutil test main build lib linux aarch psutil test copi psutil test test process build lib linux aarch psutil test copi psutil test test aix build lib linux aarch psutil test copi psutil test test misc build lib linux aarch psutil test copi psutil test test bsd build lib linux aarch psutil test copi psutil test test linux build lib linux aarch psutil test copi psutil test runner build lib linux aarch psutil test copi psutil test init build lib linux aarch psutil test copi psutil test test connect build lib linux aarch psutil test copi psutil test test unicod build lib linux aarch psutil test copi psutil test test window build lib linux aarch psutil test copi psutil test test contract build lib linux aarch psutil test copi psutil test test suno build lib linux aarch psutil test copi psutil test test testutil build lib linux aarch psutil test copi psutil test test osx build lib linux aarch psutil test copi psutil test test memleak build lib linux aarch psutil test copi psutil test test posix build lib linux aarch psutil test copi psutil test test build lib linux aarch psutil test run build ext build psutil psutil linux extens creat build temp linux aarch creat build temp linux aarch psutil gcc pthread opt conda compil compat wno unus result wsign compar dndebug wall fpic rcpc dotprod crypto isystem opt conda includ opt conda includ fpic rcpc dotprod crypto isystem opt conda includ fpic dpsutil posix dpsutil sizeof pid dpsutil version dpsutil linux opt conda includ python psutil psutil common build temp linux aarch psutil psutil common gcc error rcpc dotprod crypto file directori gcc error rcpc dotprod crypto file directori gcc error unrecogn command line option mean gcc error unrecogn command line option mean error command usr bin gcc fail exit code error fail build wheel psutil run setup clean psutil build wheel pathtool setup start build wheel pathtool setup finish statu creat wheel pathtool filenam pathtool whl size sha eafccacdccdfdcdcbeb store directori home bot cach pip wheel adaacacccaebcddd successfulli built promis pathtool fail build psutil instal collect packag smmap urllib gitdb shortuuid setproctitl sentri sdk pyyaml psutil protobuf promis pathtool gitpython docker pycr click warn script shortuuid instal home bot local bin path consid ad directori path prefer suppress warn us warn script locat run setup instal psutil start run setup instal psutil finish statu error error command error exit statu command opt conda bin python import sy setuptool token sy argv tmp pip instal vgietlj psutil cddadfb setup file tmp pip instal vgietlj psutil cddadfb setup getattr token open open file path exist file stringio setuptool import setup setup code read replac close exec compil code file exec instal record tmp pip record gbyd instal record txt singl version extern manag user prefix compil instal header home bot local includ python psutil cwd tmp pip instal vgietlj psutil cddadfb complet output line run instal opt conda lib python site packag setuptool command instal setuptoolsdeprecationwarn setup instal deprec us build pip standard base tool warn warn run build run build creat build creat build lib linux aarch creat build lib linux aarch psutil copi psutil psosx build lib linux aarch psutil copi psutil psbsd build lib linux aarch psutil copi psutil common build lib linux aarch psutil copi psutil pswindow build lib linux aarch psutil copi psutil psposix build lib linux aarch psutil copi psutil init build lib linux aarch psutil copi psutil compat build lib linux aarch psutil copi psutil pslinux build lib linux aarch psutil copi psutil pssuno build lib linux aarch psutil copi psutil psaix build lib linux aarch psutil creat build lib linux aarch psutil test copi psutil test main build lib linux aarch psutil test copi psutil test test process build lib linux aarch psutil test copi psutil test test aix build lib linux aarch psutil test copi psutil test test misc build lib linux aarch psutil test copi psutil test test bsd build lib linux aarch psutil test copi psutil test test linux build lib linux aarch psutil test copi psutil test runner build lib linux aarch psutil test copi psutil test init build lib linux aarch psutil test copi psutil test test connect build lib linux aarch psutil test copi psutil test test unicod build lib linux aarch psutil test copi psutil test test window build lib linux aarch psutil test copi psutil test test contract build lib linux aarch psutil test copi psutil test test suno build lib linux aarch psutil test copi psutil test test testutil build lib linux aarch psutil test copi psutil test test osx build lib linux aarch psutil test copi psutil test test memleak build lib linux aarch psutil test copi psutil test test posix build lib linux aarch psutil test copi psutil test test build lib linux aarch psutil test run build ext build psutil psutil linux extens creat build temp linux aarch creat build temp linux aarch psutil gcc pthread opt conda compil compat wno unus result wsign compar dndebug wall fpic rcpc dotprod crypto isystem opt conda includ opt conda includ fpic rcpc dotprod crypto isystem opt conda includ fpic dpsutil posix dpsutil sizeof pid dpsutil version dpsutil linux opt conda includ python psutil psutil common build temp linux aarch psutil psutil common gcc error rcpc dotprod crypto file directori gcc error rcpc dotprod crypto file directori gcc error unrecogn command line option mean gcc error unrecogn command line option mean error command usr bin gcc fail exit code error command error exit statu opt conda bin python import sy setuptool token sy argv tmp pip instal vgietlj psutil cddadfb setup file tmp pip instal vgietlj psutil cddadfb setup getattr token open open file path exist file stringio setuptool import setup setup code read replac close exec compil code file exec instal record tmp pip record gbyd instal record txt singl version extern manag user prefix compil instal header home bot local includ python psutil check log command output executor fail run bin pip instal upgrad exit code docker file far continuumio miniconda run apt updat apt instal instal recommend ssh git libgmp dev opam wget certif rsync strace run useradd bot workdir home bot user bot http stackoverflow com question miniconda work properli docker especi name conda run wget http repo anaconda com miniconda miniconda latest linux bash miniconda latest linux env path home bot miniconda bin path run conda creat pycoq python work conda isn fulli awar fix later env path home bot miniconda env pycoq bin path add http api github com repo ibm pycoq git ref head main version json setup opam like pycoq run opam init disabl sandbox compil coq serapi coq serapi pin run opam switch creat ocaml variant flambda coq serapi ocaml variant flambda run opam switch ocaml variant flambda coq serapi run eval opam env run opam repo add coq releas http coq inria opam releas run opam pin add coq opam repo switch add set default coq releas http coq inria opam releas run opam repo switch add set default coq releas http coq inria opam releas run opam updat run opam pin add coq run opam instal switch ocaml variant flambda coq serapi coq serapi coq serapi run opam instal coq serapi run eval opam env make sure deped pycoq instal docker imag env api kei secret run pip instal upgrad stackoverflow com instal docker imag arm python linux docker anaconda ask charli parker sep utc",
        "Question_preprocessed_content":"instal docker imag arm docker build fail docker file far instal docker imag arm python linux docker anaconda ask charli parker sep utc",
        "Question_gpt_summary_original":"the user encountered a challenge while attempting to install a docker image for arm.",
        "Question_gpt_summary":"user encount challeng attempt instal docker imag arm",
        "Answer_original_content":"brando thank write error appear primarili exit code attribut build contain base imag cach layer preliminari step try build contain us command cach let know result brando heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"write error appear primarili attribut build contain base imag cach layer preliminari step try build contain us command let know result heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the solution suggested in the answer is to rebuild the container using the command --no-cache to avoid errors related to cached layers.",
        "Answer_gpt_summary":"solut suggest answer rebuild contain command cach avoid error relat cach layer"
    },
    {
        "Question_id":null,
        "Question_title":"update real interference pipeline",
        "Question_body":"I deployed my Training pipeline and my Real-time inference pipeline.\nWith the REST-Api of my training pipeline I'm able to retrain my ML model. Is it possible to use that retrained model automated in my real inference pipeline?\nWhen i trigger the pipeline in ML studio I have to update my real inference pipeline manually. Since I want to trigger my retraining external that is not possible.\nThanks in advance.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1615298736310,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, here's a reference on which technology to use based on a given scenario. For your scenario, you should be able to create an Azure Machine Learning pipeline using the SDK to trigger a pipeline based on a time\/change based schedule and then update the web service accordingly. Depending on the complexity of your triggers or data prep needs, you can leverage other technologies such as Logic Apps or Azure Data Factory to trigger your Azure Machine Learning pipeline. Currently, you can only use the Azure Machine Learning SDK to automatically update the web service. Hope this helps.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/305899\/update-real-interference-pipeline.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-12T19:59:09.187Z",
                "Answer_score":0,
                "Answer_body":"Hi, here's a reference on which technology to use based on a given scenario. For your scenario, you should be able to create an Azure Machine Learning pipeline using the SDK to trigger a pipeline based on a time\/change based schedule and then update the web service accordingly. Depending on the complexity of your triggers or data prep needs, you can leverage other technologies such as Logic Apps or Azure Data Factory to trigger your Azure Machine Learning pipeline. Currently, you can only use the Azure Machine Learning SDK to automatically update the web service. Hope this helps.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-03-09T21:20:25.76Z",
                "Answer_score":0,
                "Answer_body":"Hi, when you deploy a web service from designer, you can select to \"deploy as a new real-time endpoint\" or \"replace an existing real-time endpoint\". The option (replace an existing real-time endpoint) enables you to update the previous endpoint. Currently, there's no way to trigger an update to real-time endpoint, it's still a manual process (I will verify and share updates). To programmatically update your web service using Azure ML SDK, please refer to this document.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1615579149187,
        "Question_original_content":"updat real interfer pipelin deploi train pipelin real time infer pipelin rest api train pipelin abl retrain model possibl us retrain model autom real infer pipelin trigger pipelin studio updat real infer pipelin manual want trigger retrain extern possibl thank advanc",
        "Question_preprocessed_content":"updat real interfer pipelin deploi train pipelin infer pipelin train pipelin abl retrain model possibl us retrain model autom real infer pipelin trigger pipelin studio updat real infer pipelin manual want trigger retrain extern possibl thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge in updating their real-time inference pipeline with a retrained ML model. They are unable to automate the process of updating the pipeline when triggering the retraining externally.",
        "Question_gpt_summary":"user face challeng updat real time infer pipelin retrain model unabl autom process updat pipelin trigger retrain extern",
        "Answer_original_content":"refer technolog us base given scenario scenario abl creat pipelin sdk trigger pipelin base time chang base schedul updat web servic accordingli depend complex trigger data prep need leverag technolog logic app azur data factori trigger pipelin current us sdk automat updat web servic hope help",
        "Answer_preprocessed_content":"refer technolog us base given scenario scenario abl creat pipelin sdk trigger pipelin base base schedul updat web servic accordingli depend complex trigger data prep need leverag technolog logic app azur data factori trigger pipelin current us sdk automat updat web servic hope help",
        "Answer_gpt_summary_original":"possible solutions from the answer include creating a pipeline using the sdk to trigger a pipeline based on a time\/change based schedule and then updating the web service accordingly. depending on the complexity of the triggers or data prep needs, other technologies such as logic apps or azure data factory can be leveraged to trigger the pipeline. currently, the only way to automatically update the web service is through the sdk.",
        "Answer_gpt_summary":"possibl solut answer includ creat pipelin sdk trigger pipelin base time chang base schedul updat web servic accordingli depend complex trigger data prep need technolog logic app azur data factori leverag trigger pipelin current wai automat updat web servic sdk"
    },
    {
        "Question_id":69962965.0,
        "Question_title":"How to get an AWS Feature Store feature group into the ACTIVE state?",
        "Question_body":"<p>I am trying to ingest some rows into a Feature Store on AWS using:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>feature_group.ingest(data_frame=df, max_workers=8, wait=True)\n<\/code><\/pre>\n<p>but I am getting the following error:<\/p>\n<blockquote>\n<p>Failed to ingest row 1: An error occurred (ValidationError) when\ncalling the PutRecord operation: Validation Error: FeatureGroup\n[feature-group] is not in ACTIVE state.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1636892751657,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":383.0,
        "Answer_body":"<p>It turns out the status of a feature group after its creation is <code>Created<\/code> but before you can ingest any rows you need to simply wait until it's <code>Active<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>while status != 'Created':\n        try:\n            status = feature_group.describe()['OfflineStoreStatus']['Status']\n        except:\n            pass\n        print('Offline store status: {}'.format(status))    \n        sleep(15)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1646290913523,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69962965",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1636979984023,
        "Question_original_content":"aw featur store featur group activ state try ingest row featur store aw featur group ingest data frame max worker wait true get follow error fail ingest row error occur validationerror call putrecord oper valid error featuregroup featur group activ state",
        "Question_preprocessed_content":"aw featur store featur group activ state try ingest row featur store aw get follow error fail ingest row error occur call putrecord oper valid error featuregroup activ state",
        "Question_gpt_summary_original":"The user is encountering a challenge in getting an AWS Feature Store feature group into the ACTIVE state while trying to ingest rows into it. The error message indicates that the feature group is not in the ACTIVE state, resulting in a validation error when calling the PutRecord operation.",
        "Question_gpt_summary":"user encount challeng get aw featur store featur group activ state try ingest row error messag indic featur group activ state result valid error call putrecord oper",
        "Answer_original_content":"turn statu featur group creation creat ingest row need simpli wait activ statu creat try statu featur group offlinestorestatu statu pass print offlin store statu format statu sleep",
        "Answer_preprocessed_content":"turn statu featur group creation ingest row need simpli wait",
        "Answer_gpt_summary_original":"solution: the user needs to wait until the feature group is in an active state before ingesting rows into the feature store on aws. they can do this by running a loop that checks the status of the feature group and waits until it is active.",
        "Answer_gpt_summary":"solut user need wait featur group activ state ingest row featur store aw run loop check statu featur group wait activ"
    },
    {
        "Question_id":null,
        "Question_title":"MLFlow Get Run information in R API and NGINX issue",
        "Question_body":"Hi all,\n\n\nGreat work on mlflow, we have been using it to run many experiments.\n\n\nI am trying to get a list of runs for a specific experiment. I am hosting mlflow in a container running on an EC2 instance in AWS.\u00a0\n\n\n\nI am using the R API.\n\n\n# set tracking URI\nmlflow_set_tracking_uri(ML_FLOW_TRACKING_URI)\n\n\n# init MLflow client object\nclient <- mlflow:::mlflow_client(ML_FLOW_TRACKING_URI)\n\n\n# extract experimment information\nd_exp <- mlflow:::mlflow_client_get_experiment(client, EXPERIMENT_ID)\n\n\n\nNow when I use this on experiments with few runs (<30) I get back the data in a named list with the data being stores in d_exp$runs\n\n\nHowever, for experiments with many runs I receive a cURL error as follow:\n\n\nError in curl::curl_fetch_memory(url, handle = handle) :\u00a0\n\u00a0 Timeout was reached: Operation timed out after 1003 milliseconds with 0 bytes received\n\n\nHas anyone had a similar\u00a0problem and happened to fix it? Or is this an issue with the API?\n\n\nNGINX Issue\n\n\nAlso another issue I have is sending requests to mlflow when I secure it with SSL certs using nGINX (with authentication) as a reverse proxy.\n\n\nIs there an example implementation on how to get this setup and working? I saw that there is a HostCreds class in the source code, but how do I initialise this?\n\n\nCurrent fix has been to whitelist the IP address, however this is obviously not a long-term solution.\n\n\nCheers,\n--\n\nVivek Katial\nData Scientist\n\nLevel 1, 155 Karangahape Road, Auckland Central,\u00a01010\nvivek....@quantiful.co.nz | \u00a00210435892\nwww.quantiful.co.nz",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1541534149000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":31.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/6nWaATd4uTw",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-08T13:13:45",
                "Answer_body":"It looks like the default timeout in the R API is 1 second (per\u00a0this code). This seems relatively aggressive, especially for a remote server. I think you can change this timeout by using\n\n\noptions(\"mlflow.rest.timeout\" = 60)\n\n\nto set it to 60 seconds, for example.\n\n\nRegarding the TLS stuff, let me add Tomas -- I know he was working on improving the R client support for authenticating against remote servers.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CABGg2YG0C8uPjWmg6G2B2rU3AZeJpM%2B-i5QGc4XfA7y1o_1NuQ%40mail.gmail.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run inform api nginx issu great work run experi try list run specif experi host contain run instanc aw api set track uri set track uri flow track uri init client object client client flow track uri extract experim inform exp client experi client experi us experi run data name list data store exp run experi run receiv curl error follow error curl curl fetch memori url handl handl timeout reach oper time millisecond byte receiv similarproblem happen fix issu api nginx issu issu send request secur ssl cert nginx authent revers proxi exampl implement setup work saw hostcr class sourc code initialis current fix whitelist address obvious long term solut cheer vivek katial data scientist level karangahap road auckland central vivek quanti quanti",
        "Question_preprocessed_content":"run inform api nginx issu great work run experi try list run specif experi host contain run instanc aw api set track uri init client object client extract experim inform us experi run data name list data store experi run receiv curl error follow error handl handl timeout reach oper time millisecond byte receiv similarproblem happen fix issu api nginx issu issu send request secur ssl cert nginx revers proxi exampl implement setup work saw hostcr class sourc code initialis current fix whitelist address obvious solut cheer vivek katial data scientist level karangahap road auckland central",
        "Question_gpt_summary_original":"The user is encountering two challenges with MLFlow. Firstly, when using the R API to get a list of runs for an experiment with many runs, they receive a cURL error due to a timeout. Secondly, when securing MLFlow with SSL certs using NGINX as a reverse proxy, they are unable to send requests and are seeking an example implementation to resolve the issue. The user has temporarily fixed the issue by whitelisting the IP address, but this is not a long-term solution.",
        "Question_gpt_summary":"user encount challeng firstli api list run experi run receiv curl error timeout secondli secur ssl cert nginx revers proxi unabl send request seek exampl implement resolv issu user temporarili fix issu whitelist address long term solut",
        "Answer_original_content":"look like default timeout api second perthi code rel aggress especi remot server think chang timeout option rest timeout set second exampl tl stuff let add toma know work improv client support authent remot server receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user cabggygcupjwmggbruazejpm iqgcxfayo nuq mail gmail com option visit http group googl com optout",
        "Answer_preprocessed_content":"look like default timeout api second rel aggress especi remot server think chang timeout set second exampl tl stuff let add toma know work improv client support authent remot server receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit",
        "Answer_gpt_summary_original":"Solution 1: Change the timeout value in the R API to a higher value, such as 60 seconds, using the \"options\" function.\n\nSolution 2: Tomas is working on improving the R client support for authenticating against remote servers, which may help with the TLS issue.\n\nThere are no long-term solutions mentioned, but a temporary fix was to whitelist the address.",
        "Answer_gpt_summary":"solut chang timeout valu api higher valu second option function solut toma work improv client support authent remot server help tl issu long term solut mention temporari fix whitelist address"
    },
    {
        "Question_id":null,
        "Question_title":"Why can't I access the Dataset class using azureml.core?",
        "Question_body":"Hi, I am unable to use Dataset class in azureml.core with new version of azureml.core==1.35.0\n\nImportError: cannot import name 'Dataset' from 'azureml.core'\n\nAlso, I am unable to downgrade or reinstall azureml.core to a lower version (1.32.0). Please find the snapshot below of pip install azureml.core==1.32.0\n\nCan someone please guide me to a possible alternative. I need to use the Dataset methods -\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634060274827,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/587772\/why-can39t-i-access-the-dataset-class-using-azurem.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-13T00:43:10.873Z",
                "Answer_score":0,
                "Answer_body":"Hi, I'm not able to reproduce this issue. I was able to import Dataset using version 1.35.0 as shown below. Perhaps try uninstalling and reinstalling the package?\n\n\n\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"access dataset class core unabl us dataset class core new version core importerror import dataset core unabl downgrad reinstal core lower version snapshot pip instal core guid possibl altern need us dataset method thank",
        "Question_preprocessed_content":"access dataset class core unabl us dataset class core new version importerror import dataset unabl downgrad reinstal core lower version snapshot pip instal guid possibl altern need us dataset method thank",
        "Question_gpt_summary_original":"The user is facing challenges in accessing the Dataset class in azureml.core with the new version of azureml.core==1.35.0. They are unable to import the Dataset class and cannot downgrade or reinstall to a lower version (1.32.0). The user is seeking guidance on possible alternatives to access the Dataset methods.",
        "Question_gpt_summary":"user face challeng access dataset class core new version core unabl import dataset class downgrad reinstal lower version user seek guidanc possibl altern access dataset method",
        "Answer_original_content":"abl reproduc issu abl import dataset version shown try uninstal reinstal packag kindli accept answer inform help thank",
        "Answer_preprocessed_content":"abl reproduc issu abl import dataset version shown try uninstal reinstal packag kindli accept answer inform help thank",
        "Answer_gpt_summary_original":"possible solutions to the issue of not being able to access the dataset class using .core with the new version of .core are to try uninstalling and reinstalling the package or to use version 1.35.0 of the package.",
        "Answer_gpt_summary":"possibl solut issu abl access dataset class core new version core try uninstal reinstal packag us version packag"
    },
    {
        "Question_id":null,
        "Question_title":"AutoML: problem with univariate time series forecasting",
        "Question_body":"I'm having troubles generating univariate time series forecasts with Azure Automated Machine Learning (I know...).\n\nWhat I'm doing\n\nSo I have about 5 years worth of monthly observations in a dataframe that looks like this:\n\n\tdate\ttarget_value\t\n\t2015-02-01\t123\t\n\t2015-03-01\t456\t\n\t2015-04-01\t789\t\n\t...\t...\t\n\nI want to forecast target_value based on past values of target_value, i.e. univariate forecasting like ARIMA for instance.\nSo I am setting up the AutoML forecast like this:\n\n\n\n # that's the dataframe as shown above\n train_data = Dataset.Tabular.from_delimited_files(path=datastore.path(my_remote_filename))\n    \n # ...other code...\n    \n forecasting_parameters = ForecastingParameters(\n     time_column_name='date',\n     forecast_horizon=2,\n     target_lags='auto',\n     freq='MS'\n )\n    \n automl_config = AutoMLConfig(task='forecasting',\n                              debug_log='automl_forecasting_function.log',\n                              primary_metric='normalized_root_mean_squared_error',\n                              enable_dnn=True,\n                              experiment_timeout_hours=8.0,\n                              enable_early_stopping=True,\n                              training_data=train_data,\n                              compute_target='my-cluster',\n                              n_cross_validations=3,\n                              verbosity=logging.INFO,\n                              max_concurrent_iterations=4,\n                              max_cores_per_iteration=-1,\n                              label_column_name='target_value',\n                              forecasting_parameters=forecasting_parameters)\n\n\n\nWhat the problem is\n\nBut AutoML does not seem to generate the forecast for target_value based on past values of target_value. It seems to use the date column as the independent variable!\nThe feature importance chart also shows date as the input feature:\n\nAs a side note: running multivariate forecasts works fine.\nWhen I use a dataset like this, feature_1 and feature_2 are used (i.e. as the X) to forecast target_value (i.e. the y)\n\n\tdate\tfeature_1\tfeature_2\ttarget_value\t\n\t2015-02-01\t10\t7\t123\t\n\t2015-03-01\t30\t2\t456\t\n\t2015-04-01\t20\t5\t789\t\n\t...\t...\t...\t...\t\n\nMy questions therefore\nHow do I need to set up a univariate AutoML forecast to forecast target_value based on past observations target_value?\nI assumed generating lagged values for target_value etc. is exactly what AutoML is supposed to do.\n\nThanks!",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1617739313070,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/346598\/automl-problem-with-univariate-time-series-forecas.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-08T04:12:36.817Z",
                "Answer_score":0,
                "Answer_body":"@movingabout-2877 Thanks, AutoML does use the date column as an independent variable. We engineer several features from it, this is a standard practice for learning seasonal patterns. In the given scenario the date column will be featurized to represent 'day', 'month', 'day of week' etc. This is done to train regression-based model on this data, which will use the generated columns for prediction.\n\nPlease remove the target_lags='auto' to allow selection of Arima. We have to block certain models (e.g. Arima) when the target lags are set. This is a product gap that we're in the process of fixing.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"automl problem univari time seri forecast have troubl gener univari time seri forecast azur autom machin learn know year worth monthli observ datafram look like date target valu want forecast target valu base past valu target valu univari forecast like arima instanc set automl forecast like datafram shown train data dataset tabular delimit file path datastor path remot filenam code forecast paramet forecastingparamet time column date forecast horizon target lag auto freq automl config automlconfig task forecast debug log automl forecast function log primari metric normal root mean squar error enabl dnn true experi timeout hour enabl earli stop true train data train data comput target cluster cross valid verbos log info max concurr iter max core iter label column target valu forecast paramet forecast paramet problem automl gener forecast target valu base past valu target valu us date column independ variabl featur import chart show date input featur note run multivari forecast work fine us dataset like featur featur forecast target valu date featur featur target valu question need set univari automl forecast forecast target valu base past observ target valu assum gener lag valu target valu exactli automl suppos thank",
        "Question_preprocessed_content":"automl problem univari time seri forecast have troubl gener univari time seri forecast azur autom machin learn year worth monthli observ datafram look like date want forecast base past valu univari forecast like arima instanc set automl forecast like datafram shown forecastingparamet automlconfig problem automl gener forecast base past valu us date column independ variabl featur import chart show date input featur note run multivari forecast work fine us dataset like forecast date question need set univari automl forecast forecast base past observ assum gener lag valu exactli automl suppos thank",
        "Question_gpt_summary_original":"The user is facing challenges in generating univariate time series forecasts with Azure Automated Machine Learning. The AutoML forecast seems to use the date column as the independent variable instead of past values of target_value. The feature importance chart also shows date as the input feature. The user is seeking guidance on how to set up a univariate AutoML forecast to forecast target_value based on past observations of target_value.",
        "Question_gpt_summary":"user face challeng gener univari time seri forecast azur autom machin learn automl forecast us date column independ variabl instead past valu target valu featur import chart show date input featur user seek guidanc set univari automl forecast forecast target valu base past observ target valu",
        "Answer_original_content":"movingabout thank automl us date column independ variabl engin featur standard practic learn season pattern given scenario date column featur repres dai month dai week train regress base model data us gener column predict remov target lag auto allow select arima block certain model arima target lag set product gap process fix",
        "Answer_preprocessed_content":"thank automl us date column independ variabl engin featur standard practic learn season pattern given scenario date column featur repres dai month dai week train model data us gener column predict remov allow select arima block certain model target lag set product gap process fix",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- azure automated machine learning uses the date column as an independent variable and engineers several features from it to learn seasonal patterns.\n- the date column will be featurized to represent 'day', 'month', 'day of week', etc. to train a regression-based model on this data, which will use the generated columns for prediction.\n- to allow selection of arima, remove the target_lags='auto' as certain models (e.g. arima) are blocked when the target lags are set.\n- there is a product gap that azure automated machine learning is in the process of fixing.",
        "Answer_gpt_summary":"possibl solut answer azur autom machin learn us date column independ variabl engin featur learn season pattern date column featur repres dai month dai week train regress base model data us gener column predict allow select arima remov target lag auto certain model arima block target lag set product gap azur autom machin learn process fix"
    },
    {
        "Question_id":null,
        "Question_title":"Discontinuation of Azure ML Studio (Classic)",
        "Question_body":"Hi Everyone\n\nI am currently using Machine Learning Studio (classic).\n\n'From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) experiments and web services. Beginning 1 December 2021, new creation of Machine Learning Studio (classic) resources will not be available.'\n\nAs mentioned above, what do resources mean? Can you please be specific about which services won't be available? Web services? Experiments? Projects?\n\nWill I be able to create new BLANK EXPERIMENTS after December 2021?\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635919954397,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/613623\/discontinuation-of-azure-ml-studio-classic.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-03T12:31:28.82Z",
                "Answer_score":0,
                "Answer_body":"@NitinVarshney-0181 Resources refer to the classic workspace that cannot be created from Azure portal after 1st Dec 2021. If you have already created a classic workspace you can create an experiment, webservice with that resource. Please ensure the workspace is not deleted from Azure portal. It is recommended to use the Azure ML workspace or migrate to the new workspace for newer features of Azure ML. Please lookup a similar conversation about the same on this thread. Thanks!!\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"discontinu studio classic current machin learn studio classic august continu us exist machin learn studio classic experi web servic begin decemb new creation machin learn studio classic resourc avail mention resourc mean specif servic won avail web servic experi project abl creat new blank experi decemb thank",
        "Question_preprocessed_content":"discontinu studio current machin learn studio august continu us exist machin learn studio experi web servic begin decemb new creation machin learn studio resourc mention resourc mean specif servic won avail web servic experi project abl creat new blank experi decemb thank",
        "Question_gpt_summary_original":"The user is facing challenges due to the discontinuation of Azure ML Studio (Classic). They are uncertain about which services will no longer be available and whether they will be able to create new blank experiments after December 2021. They are seeking clarification on the meaning of \"resources\" and which specific services will be affected.",
        "Question_gpt_summary":"user face challeng discontinu studio classic uncertain servic longer avail abl creat new blank experi decemb seek clarif mean resourc specif servic affect",
        "Answer_original_content":"nitinvarshnei resourc refer classic workspac creat azur portal dec creat classic workspac creat experi webservic resourc ensur workspac delet azur portal recommend us workspac migrat new workspac newer featur lookup similar convers thread thank answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"resourc refer classic workspac creat azur portal dec creat classic workspac creat experi webservic resourc ensur workspac delet azur portal recommend us workspac migrat new workspac newer featur lookup similar convers thread thank answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"the answer suggests that after december 2021, resources related to the classic workspace of machine learning studio cannot be created from the azure portal. however, if a classic workspace has already been created, experiments and web services can still be created using that resource. it is recommended to either use the existing workspace or migrate to the new workspace for newer features. the user is also advised to refer to a similar conversation on the thread and upvote helpful answers.",
        "Answer_gpt_summary":"answer suggest decemb resourc relat classic workspac machin learn studio creat azur portal classic workspac creat experi web servic creat resourc recommend us exist workspac migrat new workspac newer featur user advis refer similar convers thread upvot help answer"
    },
    {
        "Question_id":null,
        "Question_title":"Shared Deployments of MLFlow",
        "Question_body":"Good afternoon,\n\n\n\nAre there any resources available to centralize MLFlow to a dedicated server which can:\n\n\n1) Archive models\n\n2) Host MLFlow web UI\n3) Host deployed model REST API\n\n\n\nIdeally, each model could be secured by groups or individual users, and shared with other groups \/ users.\n\n\n\nDo any existing example deployments exist? I see Databricks appears to have a hosted version of MLFlow per this announcement: https:\/\/mlflow.org\/news\/2019\/09\/10\/MLflow-Community-Edition\/index.html\n\n\nThanks!\n\n\n-Rob",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569414037000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":13.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/mLqqkTBBel0",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-25T14:29:54",
                "Answer_body":"Hi Robert.\n\n\n1. In the oss, you can use ``mlflow server`` to start a rest api tracking server. The server will host MLflow web UI and tracking rest api.\u00a0\n2. The tracking server does not store models. The models are stored as artifacts and will be stored depending on where you set the artifact root uri for each experiment.\u00a0\nIf you want the models stored on the tracking server, you would need to be able to access it via one of the supported\u00a0artifact\u00a0store\u00a0apis (e.g.\u00a0 sftp server)\n3. The tracking server does not host deployed models. You can deploy models locally via ``mlflow serve``.\u00a0\n4. Yes, Databricks has a hosted MLFlow and it is also available in the (free) Databricks Community edition, you can\u00a0 try it here\u00a0https:\/\/databricks.com\/try-databricks\n\n\nLet me know if you have any questions.\n\n\nTomas\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/61bf1576-410f-4706-a316-47b0a0669052%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"share deploy good afternoon resourc avail central dedic server archiv model host web host deploi model rest api ideal model secur group individu user share group user exist exampl deploy exist databrick appear host version announc http org new commun edit index html thank rob",
        "Question_preprocessed_content":"share deploy good afternoon resourc avail central dedic server archiv model host web host deploi model rest api ideal model secur group individu user share group user exist exampl deploy exist databrick appear host version announc thank rob",
        "Question_gpt_summary_original":"The user is seeking resources to centralize MLFlow to a dedicated server that can archive models, host MLFlow web UI, and host deployed model REST API. They are looking for a solution that can secure each model by groups or individual users and share them with other groups\/users. The user is also inquiring about existing example deployments and mentions Databricks' hosted version of MLFlow.",
        "Question_gpt_summary":"user seek resourc central dedic server archiv model host web host deploi model rest api look solut secur model group individu user share group user user inquir exist exampl deploy mention databrick host version",
        "Answer_original_content":"robert oss us server start rest api track server server host web track rest api track server store model model store artifact store depend set artifact root uri experi want model store track server need abl access supportedartifactstoreapi sftp server track server host deploi model deploi model local serv ye databrick host avail free databrick commun edit try herehttp databrick com try databrick let know question toma receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com view discuss web visit http group googl com msgid user googlegroup com",
        "Answer_preprocessed_content":"robert oss us start rest api track server server host web track rest api track server store model model store artifact store depend set artifact root uri experi want model store track server need abl access supportedartifactstoreapi track server host deploi model deploi model local ye databrick host avail databrick commun edit try let know question toma receiv messag subscrib googl group group unsubscrib group stop receiv email send email view discuss web visit",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use \"mlflow server\" to start a rest api tracking server that will host MLflow web UI and tracking rest api.\n2. Store models as artifacts and set the artifact root uri for each experiment. If you want the models stored on the tracking server, you would need to be able to access it via one of the supported artifact store apis (e.g. sftp server).\n3. Deploy models locally via \"mlflow serve\".\n4. Use Databricks hosted MLFlow or try it in the free Databricks Community edition.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss us server start rest api track server host web track rest api store model artifact set artifact root uri experi want model store track server need abl access support artifact store api sftp server deploi model local serv us databrick host try free databrick commun edit person opinion bias includ summari"
    },
    {
        "Question_id":null,
        "Question_title":"Stopping ML Server Engine",
        "Question_body":"I'm running the Microsoft Machine Learning Server on my computer. Right now, there are no tasks\/nodes running, and task manager is showing multiple instances of the \"Microsoft ML Server Engine\" that are using nearly all my computer resources. I've gone into the administration utility, but can't seem to find a way of stopping this, short of brute \"End Task\" within TM.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1612666795983,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/262003\/stopping-ml-server-engine.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-07T04:05:59.56Z",
                "Answer_score":1,
                "Answer_body":"HI @ThomasGraham-8689\n\nIn Machine Learning Server 9.3 and later, you can use admin extension of the Azure Command-Line Interface (Azure CLI) to set up and manage your configuration, including stopping and starting services.\nMonitor, stop, and start web & compute nodes\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stop server engin run microsoft machin learn server right task node run task manag show multipl instanc microsoft server engin nearli resourc gone administr util wai stop short brute end task",
        "Question_preprocessed_content":"stop server engin run microsoft machin learn server right run task manag show multipl instanc microsoft server engin nearli resourc gone administr util wai stop short brute end task",
        "Question_gpt_summary_original":"The user is facing challenges with stopping the Microsoft Machine Learning Server on their computer as multiple instances of the \"Microsoft ML Server Engine\" are using nearly all their computer resources. They have tried accessing the administration utility but have not found a way to stop it other than using the \"End Task\" function in Task Manager.",
        "Question_gpt_summary":"user face challeng stop microsoft machin learn server multipl instanc microsoft server engin nearli resourc tri access administr util wai stop end task function task manag",
        "Answer_original_content":"thomasgraham machin learn server later us admin extens azur command line interfac azur cli set manag configur includ stop start servic monitor stop start web comput node answer help click accept answer vote benefici commun member",
        "Answer_preprocessed_content":"machin learn server later us admin extens azur interfac set manag configur includ stop start servic monitor stop start web comput node answer help click accept answer benefici commun member",
        "Answer_gpt_summary_original":"possible solutions to stop the microsoft machine learning server engine from consuming computer resources include using the admin extension of the azure command-line interface to manage configuration and stop or start web and compute nodes.",
        "Answer_gpt_summary":"possibl solut stop microsoft machin learn server engin consum resourc includ admin extens azur command line interfac manag configur stop start web comput node"
    },
    {
        "Question_id":null,
        "Question_title":"How to create a serverless endpoint configuration?",
        "Question_body":"based on the sample code provided here , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\n\nI created a model via lambda, now when i try to create a serverless endpoint config (sample code below) , i keep getting -> parameter validation failed unknown parameter in ProductVariants [ 0 ]: \"ServerlessConfig\", must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...\n\nresponse = client.create_endpoint_config(\n   EndpointConfigName=\"endpoint-new\",\n   ProductionVariants=[\n        {\n            \"ModelName\": \"MyModel\",\n            \"VariantName\": \"AllTraffic\",\n            \"ServerlessConfig\": {\n                \"MemorySizeInMB\": 2048,\n                \"MaxConcurrency\": 10\n            }\n        } \n    ]\n)",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1645067206226,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":129.0,
        "Answer_body":"The cause might be that your SageMaker Python SDK is not updated to the latest version. Please make sure you update it to the latest version as well as the AWS SDK for Python (boto3). You can use pip:\n\npip install --upgrade boto3\npip install --upgrade sagemaker\n\n\nFor a sample notebook you can have a look here. More information on the documentation page.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmAxh_aDQiS2nk0gbDicsg\/how-to-create-a-serverless-endpoint-configuration",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-17T09:30:12.370Z",
                "Answer_score":1,
                "Answer_body":"The cause might be that your SageMaker Python SDK is not updated to the latest version. Please make sure you update it to the latest version as well as the AWS SDK for Python (boto3). You can use pip:\n\npip install --upgrade boto3\npip install --upgrade sagemaker\n\n\nFor a sample notebook you can have a look here. More information on the documentation page.",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-17T09:37:24.602Z",
                "Answer_score":1,
                "Answer_body":"Hi,\n\nCan you confirm the version of boto3 that you are using? This error is likely to be caused because of an older version of boto3 that does not include capability for serverless inference.\n\nServerless inference was introduced in version 1.20.18\n\nHope this helps,\n\nGeorgios",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645090212368,
        "Question_original_content":"creat serverless endpoint configur base sampl code provid http doc aw amazon com latest serverless endpoint creat html serverless endpoint creat config creat model lambda try creat serverless endpoint config sampl code get paramet valid fail unknown paramet productvari serverlessconfig vairantnam modelnam initialinstancecount instancetyp respons client creat endpoint config endpointconfignam endpoint new productionvari modelnam mymodel variantnam alltraff serverlessconfig memorysizeinmb maxconcurr",
        "Question_preprocessed_content":"creat serverless endpoint configur base sampl code provid creat model lambda try creat serverless endpoint config get paramet valid fail unknown paramet productvari serverlessconfig vairantnam modelnam initialinstancecount respons productionvari",
        "Question_gpt_summary_original":"The user is facing challenges in creating a serverless endpoint configuration for a model created via lambda. The user is receiving an error message stating that the \"ServerlessConfig\" parameter is unknown and must be one of \"VariantName\", \"ModelName\", \"InitialInstanceCount\", or \"InstanceType\".",
        "Question_gpt_summary":"user face challeng creat serverless endpoint configur model creat lambda user receiv error messag state serverlessconfig paramet unknown variantnam modelnam initialinstancecount instancetyp",
        "Answer_original_content":"caus python sdk updat latest version sure updat latest version aw sdk python boto us pip pip instal upgrad boto pip instal upgrad sampl notebook look inform document page",
        "Answer_preprocessed_content":"caus python sdk updat latest version sure updat latest version aw sdk python us pip pip instal boto pip instal sampl notebook look inform document page",
        "Answer_gpt_summary_original":"possible solutions to the challenge of receiving an error regarding an unknown parameter in the productvariants array when creating a serverless endpoint configuration are to update the python sdk and aws sdk for python (boto3) to the latest version using pip. additionally, a sample notebook and more information can be found on the documentation page.",
        "Answer_gpt_summary":"possibl solut challeng receiv error unknown paramet productvari arrai creat serverless endpoint configur updat python sdk aw sdk python boto latest version pip addition sampl notebook inform document page"
    },
    {
        "Question_id":56848293.0,
        "Question_title":"What is Random seed in Azure Machine Learning?",
        "Question_body":"<p>I am learning Azure Machine Learning. I am frequently encountering the <strong>Random Seed<\/strong> in some of the steps like,<\/p>\n\n<ol>\n<li>Split Data<\/li>\n<li>Untrained algorithm models as Two Class Regression, Multi-class regression, Tree, Forest,..<\/li>\n<\/ol>\n\n<p>In the tutorial, they choose Random Seed as '123'; trained model has high accuracy but when I try to choose other random integers like 245, 256, 12, 321,.. it did not do well.<\/p>\n\n<hr>\n\n<p><strong>Questions<\/strong><\/p>\n\n<ul>\n<li>What is a Random Seed Integer?<\/li>\n<li>How to carefully choose a Random Seed from range of integer values? What is the key or strategy to choose it?<\/li>\n<li>Why does Random Seed significantly affect the ML Scoring, Prediction and Quality of the trained model?<\/li>\n<\/ul>\n\n<hr>\n\n<p><strong>Pretext<\/strong><\/p>\n\n<ol>\n<li>I have <a href=\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/iris\/iris.data\" rel=\"nofollow noreferrer\">Iris-Sepal-Petal-Dataset<\/a> with Sepal (<em>Length &amp; Width<\/em>) and Petal (<em>Length &amp; Width<\/em>)<\/li>\n<li>Last column in data-set is 'Binomial ClassName'<\/li>\n<li>I am training the data-set with Multiclass Decision Forest Algorithm and splitting the data with different random seeds 321, 123 and 12345 in order<\/li>\n<li>It affects the final quality of trained model. Random seed#123 being best of Prediction probability score: 1.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/12OyD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/12OyD.png\" alt=\"ML Studio Snap\"><\/a><\/p>\n\n<hr>\n\n<p><strong>Observations<\/strong><\/p>\n\n<p><strong>1. Random seed: 321<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YcsiS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YcsiS.png\" alt=\"Random-seed-321\"><\/a><\/p>\n\n<p><strong>2. Random seed: 123<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Qrk4G.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Qrk4G.png\" alt=\"Random-seed-123\"><\/a><\/p>\n\n<p><strong>3. Random seed: 12345<\/strong><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/D1Rki.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/D1Rki.png\" alt=\"Random-seed-12345\"><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":9,
        "Question_creation_time":1562056038867,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1562066943383,
        "Question_score":0.0,
        "Question_view_count":2046.0,
        "Answer_body":"<blockquote>\n  <p>What is a Random Seed Integer?<\/p>\n<\/blockquote>\n\n<p>Will not go into any details regarding what a random seed is in general; there is plenty of material available by a simple web search (see for example <a href=\"https:\/\/stackoverflow.com\/questions\/22639587\/random-seed-what-does-it-do\">this SO thread<\/a>).<\/p>\n\n<p>Random seed serves just to initialize the (pseudo)random number generator, mainly in order to make ML examples reproducible.<\/p>\n\n<blockquote>\n  <p>How to carefully choose a Random Seed from range of integer values? What is the key or strategy to choose it?<\/p>\n<\/blockquote>\n\n<p>Arguably this is already answered implicitly above: you are simply not supposed to choose any particular random seed, and your results should be roughly the same across different random seeds.<\/p>\n\n<blockquote>\n  <p>Why does Random Seed significantly affect the ML Scoring, Prediction and Quality of the trained model?<\/p>\n<\/blockquote>\n\n<p>Now, to the heart of your question. The answer <em>here<\/em> (i.e. with the iris dataset) is the <strong>small-sample effects<\/strong>...<\/p>\n\n<p>To start with, your reported results across different random seeds are not <em>that<\/em> different. Nevertheless, I agree that, at first sight, a difference in macro-average precision of 0.9 and 0.94 might <em>seem<\/em> large; but looking more closely it is revealed that the difference is really not an issue. Why?<\/p>\n\n<p>Using the 20% of your (only) 150-samples dataset leaves you with only 30 samples in your test set (where the evaluation is performed); this is stratified, i.e. about 10 samples from each class. Now, for datasets of <em>that<\/em> small size, it is not difficult to imagine that a difference in the correct classification of <strong>only 1-2<\/strong> samples can have this apparent difference in the performance metrics reported.<\/p>\n\n<p>Let's try to verify this in scikit-learn using a decision tree classifier (the essence of the issue does not depend on the specific framework or the ML algorithm used):<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321, stratify=y)\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n<\/code><\/pre>\n\n<p>Result:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  9  1]\n [ 0  0 10]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.90      0.95        10\n           2       0.91      1.00      0.95        10\n\n   micro avg       0.97      0.97      0.97        30\n   macro avg       0.97      0.97      0.97        30\nweighted avg       0.97      0.97      0.97        30\n<\/code><\/pre>\n\n<p>Let's repeat the code above, changing only the <code>random_state<\/code> argument in <code>train_test_split<\/code>; for <code>random_state=123<\/code> we get:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  7  3]\n [ 0  2  8]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       0.78      0.70      0.74        10\n           2       0.73      0.80      0.76        10\n\n   micro avg       0.83      0.83      0.83        30\n   macro avg       0.84      0.83      0.83        30\nweighted avg       0.84      0.83      0.83        30\n<\/code><\/pre>\n\n<p>while for <code>random_state=12345<\/code> we get:<\/p>\n\n<pre><code>[[10  0  0]\n [ 0  8  2]\n [ 0  0 10]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        10\n           1       1.00      0.80      0.89        10\n           2       0.83      1.00      0.91        10\n\n   micro avg       0.93      0.93      0.93        30\n   macro avg       0.94      0.93      0.93        30\nweighted avg       0.94      0.93      0.93        30\n<\/code><\/pre>\n\n<p>Looking at the <em>absolute numbers<\/em> of the 3 confusion matrices (in <em>small samples<\/em>, percentages can be <strong>misleading<\/strong>), you should be able to convince yourself that the differences are not that big, and they can be arguably justified by the random element inherent in the whole procedure (here the exact split of the dataset into training and test).<\/p>\n\n<p>Should your test set be significantly bigger, these discrepancies would be practically negligible... <\/p>\n\n<p>A last notice; I have used the exact same seed numbers as you, but this does not actually mean anything, as in general the random number generators <em>across<\/em> platforms &amp; languages are not the same, hence the corresponding seeds are not actually compatible. See own answer in <a href=\"https:\/\/stackoverflow.com\/questions\/52293899\/are-random-seeds-compatible-between-systems\">Are random seeds compatible between systems?<\/a> for a demonstration.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1562070151168,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56848293",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1562069850056,
        "Question_original_content":"random seed learn frequent encount random seed step like split data untrain algorithm model class regress multi class regress tree forest tutori choos random seed train model high accuraci try choos random integ like question random seed integ carefulli choos random seed rang integ valu kei strategi choos random seed significantli affect score predict qualiti train model pretext iri sepal petal dataset sepal length width petal length width column data set binomi classnam train data set multiclass decis forest algorithm split data differ random seed order affect final qualiti train model random seed best predict probabl score observ random seed random seed random seed",
        "Question_preprocessed_content":"random seed learn frequent encount random seed step like split data untrain algorithm model class regress regress tree tutori choos random seed train model high accuraci try choos random integ like question random seed integ carefulli choos random seed rang integ valu kei strategi choos random seed significantli affect score predict qualiti train model pretext sepal petal column binomi classnam train multiclass decis forest algorithm split data differ random seed order affect final qualiti train model random seed best predict probabl score observ random seed random seed random seed",
        "Question_gpt_summary_original":"The user is encountering challenges in Azure Machine Learning related to the Random Seed integer, which significantly affects the ML scoring, prediction, and quality of the trained model. The user is struggling to choose a random seed from a range of integer values and is observing that the trained model's accuracy varies significantly based on the chosen random seed. The user has tried different random seeds, including 245, 256, 12, 321, and 12345, but found that the model's accuracy was highest when the random seed was set to 123.",
        "Question_gpt_summary":"user encount challeng relat random seed integ significantli affect score predict qualiti train model user struggl choos random seed rang integ valu observ train model accuraci vari significantli base chosen random seed user tri differ random seed includ model accuraci highest random seed set",
        "Answer_original_content":"random seed integ detail random seed gener plenti materi avail simpl web search exampl thread random seed serv initi pseudo random number gener mainli order exampl reproduc carefulli choos random seed rang integ valu kei strategi choos arguabl answer implicitli simpli suppos choos particular random seed result roughli differ random seed random seed significantli affect score predict qualiti train model heart question answer iri dataset small sampl effect start report result differ random seed differ agre sight differ macro averag precis larg look close reveal differ issu sampl dataset leav sampl test set evalu perform stratifi sampl class dataset small size difficult imagin differ correct classif sampl appar differ perform metric report let try verifi scikit learn decis tree classifi essenc issu depend specif framework algorithm sklearn dataset import load iri sklearn tree import decisiontreeclassifi sklearn metric import confus matrix classif report sklearn model select import train test split load iri return true train test train test train test split test size random state stratifi decisiontreeclassifi fit train train pred predict test print confus matrix test pred print classif report test pred result precis recal score support micro avg macro avg weight avg let repeat code chang random state argument train test split random state precis recal score support micro avg macro avg weight avg random state precis recal score support micro avg macro avg weight avg look absolut number confus matric small sampl percentag mislead abl convinc differ big arguabl justifi random element inher procedur exact split dataset train test test set significantli bigger discrep practic neglig notic exact seed number actual mean gener random number gener platform languag correspond seed actual compat answer random seed compat system demonstr",
        "Answer_preprocessed_content":"random seed integ detail random seed gener plenti materi avail simpl web search random seed serv initi random number gener mainli order exampl reproduc carefulli choos random seed rang integ valu kei strategi choos arguabl answer implicitli simpli suppos choos particular random seed result roughli differ random seed random seed significantli affect score predict qualiti train model heart question answer start report result differ random seed differ agre sight differ precis larg look close reveal differ issu dataset leav sampl test set stratifi sampl class dataset small size difficult imagin differ correct classif sampl appar differ perform metric report let try verifi decis tree classifi result let repeat code chang argument look absolut number confus matric abl convinc differ big arguabl justifi random element inher procedur test set significantli bigger discrep practic notic exact seed number actual mean gener random number gener platform languag correspond seed actual compat answer random seed compat system demonstr",
        "Answer_gpt_summary_original":"the answer suggests that there is no specific strategy to choose a random seed for machine learning models, and that the results should be roughly the same across different random seeds. the answer also explains that the small sample size of the dataset can lead to apparent differences in performance metrics when using different random seeds. the answer provides an example using the iris dataset and a decision tree classifier to demonstrate this. finally, the answer notes that the same seed numbers may not be compatible across different platforms and languages.",
        "Answer_gpt_summary":"answer suggest specif strategi choos random seed machin learn model result roughli differ random seed answer explain small sampl size dataset lead appar differ perform metric differ random seed answer provid exampl iri dataset decis tree classifi demonstr final answer note seed number compat differ platform languag"
    },
    {
        "Question_id":null,
        "Question_title":"\"dvc.api.get_url()\" is not working for --external outputs",
        "Question_body":"<p>I have added dvc remote external output to track (external cache and external data storage),<\/p>\n<ul>\n<li>\n<p>Following is the .dvc generated,<br>\n<strong>path in github:<\/strong> remoteTrack\/wine-quality.csv.dvc<br>\nouts:<\/p>\n<ul>\n<li>etag: 5d6f24258e3c50bb01a61194b5401f5d<br>\nsize: 264426<br>\npath: remote:\/\/s3remote\/wine-quality.csv<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>But same works if not mentioned as external data, following is .csv for non external<br>\n<strong>path in github:<\/strong> wine-quality.csv.dvc<br>\nouts:<\/p>\n<ul>\n<li>md5: 5d6f24258e3c50bb01a61194b5401f5d<br>\nsize: 264426<br>\npath: wine-quality.csv<br>\n.<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95.png\" data-download-href=\"\/uploads\/short-url\/nRHnFdVZK97Uul8v5E9CBTwtFPv.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95.png\" alt=\"image\" data-base62-sha1=\"nRHnFdVZK97Uul8v5E9CBTwtFPv\" width=\"685\" height=\"500\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/a743fa995cf8a15725ea820f6753ee1cc0d30c95_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">900\u00d7656 37.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nEven after mentioning path as \u201cremote:\/\/s3remote\/wine-quality.csv\u201d, it is not working.<br>\n<strong>Error<\/strong>: PathMissingError: The path \u2018remoteTrack\/wine-quality.csv\u2019 does not exist in the target repository  neither as a DVC output nor as a Git-tracked file.<\/p>\n<p>What should be the path for remote external data?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1618813629880,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":651.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-api-get-url-is-not-working-for-external-outputs\/730",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-19T08:38:28.405Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"1\" data-topic=\"730\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>What should be the path for remote external data?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Support for external outputs in <code>dvc.api.*<\/code> is highly experimental, and it seems like it doesn\u2019t work for your use case. If this is a single use case, I\u2019d advise just expanding <code>remote:\/\/s3remote\/wine-quality.csv<\/code> to <code>s3:\/\/bucket\/wine-quality.csv<\/code> and use boto etc to read it.<\/p>",
                "Answer_score":21.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-19T08:59:41.110Z",
                "Answer_body":"<p>okay.<br>\nI will try with \u201cS3:\/\/bucket\u201d instead of remote,<br>\nBut does that solve the issue?<br>\nThe problem am facing is when reading the URL using dvc api for different versions of data.<br>\nThe data is being stored correctly and .dvc files are generated.<br>\nI can manually enter s3 URL and read file from that location (or download using boto3 if I know the URL).<\/p>\n<p>Example URLs for different versions of data from cache,<br>\ns3:\/\/datasource-bucket\/cache\/559\/27fce671701990608798ea11403459<br>\ns3:\/\/datasource-bucket\/cache\/5d\/6f24258e3c50bb01a61194b5401f5d<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-20T07:26:37.499Z",
                "Answer_body":"<p>By the way, what is your use case that requires using external outputs? It might be just simpler to use regular outputs and push\/pull. It is an advanced feature with some parts are still in an experimental mode.<\/p>",
                "Answer_score":15.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-20T09:05:53.400Z",
                "Answer_body":"<p>The use case am exploring is trying to track data in remote location without storing\/downloading it locally(I will not push or pull data locally).<br>\nThere can be other systems, which can change data in remote, which again I want to track from git dvc but without downloading it.<br>\nAnd am trying to load different versions of remote data in jupyter notebook using dvc api.(the notebook is in cloud).<br>\nExcept --external option, other options\/features download the data locally right either in cache or in local system?<\/p>",
                "Answer_score":15.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-20T09:19:28.697Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"veeresh\" data-post=\"5\" data-topic=\"730\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/v\/df705f\/40.png\" class=\"avatar\"> veeresh:<\/div>\n<blockquote>\n<p>Except --external option, other options\/features download the data locally right either in cache or in local system?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Actually, you can use <code>--to-remote<\/code>, which should sync your data from the remote source to the remote storage and then you can use <code>dvc.api.read()<\/code> etc.  See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage\">https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-remote-storage<\/a><\/p>",
                "Answer_score":30.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"api url work extern output ad remot extern output track extern cach extern data storag follow gener path github remotetrack wine qualiti csv out etag dfecbbabfd size path remot sremot wine qualiti csv work mention extern data follow csv non extern path github wine qualiti csv out dfecbbabfd size path wine qualiti csv imag mention path remot sremot wine qualiti csv work error pathmissingerror path remotetrack wine qualiti csv exist target repositori output git track file path remot extern data",
        "Question_preprocessed_content":"work output ad remot extern output track follow gener path github out etag size path work mention extern data follow csv non extern path github out size path imag mention path work error pathmissingerror path exist target repositori output file path remot extern data",
        "Question_gpt_summary_original":"The user is facing challenges with the dvc.api.get_url() function while trying to track external cache and data storage. The user has mentioned the path as \"remote:\/\/s3remote\/wine-quality.csv\" but is still encountering an error stating that the path does not exist in the target repository. The user is seeking guidance on the correct path for remote external data.",
        "Question_gpt_summary":"user face challeng api url function try track extern cach data storag user mention path remot sremot wine qualiti csv encount error state path exist target repositori user seek guidanc correct path remot extern data",
        "Answer_original_content":"veeresh path remot extern data support extern output api highli experiment like doesnt work us case singl us case advis expand remot sremot wine qualiti csv bucket wine qualiti csv us boto read okai try bucket instead remot solv issu problem face read url api differ version data data store correctli file gener manual enter url read file locat download boto know url exampl url differ version data cach datasourc bucket cach fceea datasourc bucket cach fecbbabfd wai us case requir extern output simpler us regular output push pull advanc featur part experiment mode us case explor try track data remot locat store download local push pull data local system chang data remot want track git download try load differ version remot data jupyt notebook api notebook cloud extern option option featur download data local right cach local veeresh extern option option featur download data local right cach local actual us remot sync data remot sourc remot storag us api read http org doc command refer add exampl transfer remot storag",
        "Answer_preprocessed_content":"veeresh path remot extern data support extern output highli experiment like doesnt work us case singl us case advis expand us boto read okai try instead remot solv issu problem face read url api differ version data data store correctli file gener manual enter url read file locat exampl url differ version data cach wai us case requir extern output simpler us regular output advanc featur part experiment mode us case explor try track data remot locat local system chang data remot want track git download try load differ version remot data jupyt notebook notebook cloud option download data local right cach local veeresh option download data local right cach local actual us sync data remot sourc remot storag us",
        "Answer_gpt_summary_original":"possible solutions to the issue with the \".api.get_url()\" function not working for external outputs are:\n\n- expanding the remote url to an s3 url and using boto to read the file.\n- using regular outputs and push\/pull instead of external outputs.\n- using the --to-remote option to sync the data from the remote source to the remote storage and then using .api.read().",
        "Answer_gpt_summary":"possibl solut issu api url function work extern output expand remot url url boto read file regular output push pull instead extern output remot option sync data remot sourc remot storag api read"
    },
    {
        "Question_id":null,
        "Question_title":"Move a .dvc stage without re-run",
        "Question_body":"<p>I just ran a <code>dvc run ...<\/code> but forgot to specify the <code>-f<\/code> flag so it created a <code>.dvc<\/code> file in an unintended location. Is there a smart way of moving it without re-running the stage? This method should take care of location and relative paths <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1589374711255,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":445.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/move-a-dvc-stage-without-re-run\/384",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-13T15:12:12.312Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/drorata\">@drorata<\/a>,<\/p>\n<p>Unfortunatelly it seems you need to just move the DVC-file and manually change the paths in it to make them relative to the new location. Or you can change them into absolute paths.<\/p>\n<p>If you think you may be moving stage files in the future, maybe consider specifying the dependencies and outputs as absolute paths when you use <code>dvc run<\/code>.<\/p>\n<p>It would be great to have this as a feature request though, would you like to open an issue in out Git repo?<\/p>\n<p>Thanks<\/p>",
                "Answer_score":436.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-13T17:36:12.540Z",
                "Answer_body":"<p>Done! <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3795\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/issues\/3795<\/a><\/p>",
                "Answer_score":15.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-13T18:50:45.040Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/drorata\">@drorata<\/a> have you had chance to try the pre-release 1.0a? It has changed a lot the way pipelines are organized - it\u2019s a single file now that is intended to be human readable\/editable. Would be really great if you give it a try and let us know if solves the problem with renaming\/moving things.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-13T19:03:45.378Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> as I mentioned in the ticket, I didn\u2019t have the chance to put in action the changes introduced  in 1.0a. Most of my time with DVC is currently invested in a single project where I\u2019m worried about backward incompatibilities.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stage run ran run forgot specifi flag creat file unintend locat smart wai move run stage method care locat rel path",
        "Question_preprocessed_content":"stage ran forgot specifi flag creat file unintend locat smart wai move stage method care locat rel path",
        "Question_gpt_summary_original":"The user forgot to specify the -f flag while running a dvc command, resulting in the creation of a .dvc file in an unintended location. The user is seeking a way to move the file without re-running the stage, while ensuring that the location and relative paths are taken care of.",
        "Question_gpt_summary":"user forgot specifi flag run command result creation file unintend locat user seek wai file run stage ensur locat rel path taken care",
        "Answer_original_content":"drorata unfortunatelli need file manual chang path rel new locat chang absolut path think move stage file futur mayb consid specifi depend output absolut path us run great featur request like open issu git repo thank http github com iter issu drorata chanc try pre releas chang lot wai pipelin organ singl file intend human readabl edit great try let know solv problem renam move thing shcheklein mention ticket didnt chanc action chang introduc time current invest singl project worri backward incompat",
        "Answer_preprocessed_content":"unfortunatelli need file manual chang path rel new locat chang absolut path think move stage file futur mayb consid specifi depend output absolut path us great featur request like open issu git repo thank chanc try chang lot wai pipelin organ singl file intend human great try let know solv problem thing mention ticket didnt chanc action chang introduc time current invest singl project worri backward incompat",
        "Answer_gpt_summary_original":"possible solutions from the answer are: manually changing the paths in the file to make them relative to the new location or changing them into absolute paths. another solution is to specify the dependencies and outputs as absolute paths when using run. the answer also suggests trying the pre-release 1.0a, which has changed the way pipelines are organized and is intended to be human-readable\/editable.",
        "Answer_gpt_summary":"possibl solut answer manual chang path file rel new locat chang absolut path solut specifi depend output absolut path run answer suggest try pre releas chang wai pipelin organ intend human readabl edit"
    },
    {
        "Question_id":null,
        "Question_title":"How to add multiple csv's as .zip file new Azure Machine Learning Designer (Non-Classic)",
        "Question_body":"In classic mode there was option of adding multiple csv files zipped into a folder and uploaded into the Azure ML studio (classic). I want to achieve the same in the new Azure Machine Learning designer (non-classic) but I don't see the option of upload .zip file.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608479520287,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/204887\/how-to-add-multiple-csv39s-as-zip-file-new-azure-m.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-21T08:35:52.17Z",
                "Answer_score":0,
                "Answer_body":"@SoumyakantSahoo-4033 I believe you are referring to this feature of the classic version where the ability to unzip multiple csv files is available for a zip file.\nThis module in the designer though is not ported and the recommendation to import multiple files if they are in a similar format is to use the import data module with the files on your datastore where the tabular data can be imported with the following conditions:\n\nTo include all data files in the folder, you need to input folder_name\/** for Path.\n\n\nAll data files must be encoded in unicode-8.\n\n\nAll data files must have the same column numbers and column names.\n\n\nThe result of importing multiple data files is concatenating all rows from multiple files in order.\n\nAfter the above module runs successfully you can use the data manipulation modules to further process the data and train your model.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"add multipl csv zip file new design non classic classic mode option ad multipl csv file zip folder upload studio classic want achiev new design non classic option upload zip file",
        "Question_preprocessed_content":"add multipl csv zip file new design classic mode option ad multipl csv file zip folder upload studio want achiev new design option upload zip file",
        "Question_gpt_summary_original":"The user is facing a challenge in adding multiple CSV files as a zipped folder in the new Azure Machine Learning Designer (non-classic) as they cannot find the option to upload a .zip file.",
        "Question_gpt_summary":"user face challeng ad multipl csv file zip folder new design non classic option upload zip file",
        "Answer_original_content":"soumyakantsahoo believ refer featur classic version abil unzip multipl csv file avail zip file modul design port recommend import multipl file similar format us import data modul file datastor tabular data import follow condit includ data file folder need input folder path data file encod unicod data file column number column name result import multipl data file concaten row multipl file order modul run successfulli us data manipul modul process data train model",
        "Answer_preprocessed_content":"believ refer featur classic version abil unzip multipl csv file avail zip file modul design port recommend import multipl file similar format us import data modul file datastor tabular data import follow condit includ data file folder need input path data file encod data file column number column name result import multipl data file concaten row multipl file order modul run successfulli us data manipul modul process data train model",
        "Answer_gpt_summary_original":"the classic feature of unzipping multiple csv files in a .zip file is not available in the new designer mode. however, the user can import multiple files in a similar format using the import data module with the files on their datastore. the data files must be encoded in unicode-8, have the same column numbers and column names, and the result of importing multiple data files is concatenating all rows from multiple files in order. after importing, the user can use data manipulation modules to further process the data and train their model.",
        "Answer_gpt_summary":"classic featur unzip multipl csv file zip file avail new design mode user import multipl file similar format import data modul file datastor data file encod unicod column number column name result import multipl data file concaten row multipl file order import user us data manipul modul process data train model"
    },
    {
        "Question_id":72210628.0,
        "Question_title":"How to manage out of the box monitoring using Kubernetes online endpoints",
        "Question_body":"<p>I am learning about Kubernetes online endpoints and my objective is to monitor my resources which are deployed using Kubernetes endpoints. Is there any provision to get out-of-the-box monitoring to Kubernetes online endpoints to check the performance. I am new to this domain. Any help is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1652334408657,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":72.0,
        "Answer_body":"<p>The general monitoring is available and supportive in AKS, but the out of the box implementation was not supportive unfortunately. Check the below documentation and screenshot to refer supportive formats of monitoring in AKS<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/0uxW5.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0uxW5.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-endpoints<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72210628",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1652337837120,
        "Question_original_content":"manag box monitor kubernet onlin endpoint learn kubernet onlin endpoint object monitor resourc deploi kubernet endpoint provis box monitor kubernet onlin endpoint check perform new domain help appreci",
        "Question_preprocessed_content":"manag box monitor kubernet onlin endpoint learn kubernet onlin endpoint object monitor resourc deploi kubernet endpoint provis monitor kubernet onlin endpoint check perform new domain help appreci",
        "Question_gpt_summary_original":"The user is seeking help in managing out-of-the-box monitoring for Kubernetes online endpoints to monitor the performance of their deployed resources. They are new to this domain and are looking for assistance.",
        "Question_gpt_summary":"user seek help manag box monitor kubernet onlin endpoint monitor perform deploi resourc new domain look assist",
        "Answer_original_content":"gener monitor avail support ak box implement support unfortun check document screenshot refer support format monitor ak http doc microsoft com azur machin learn concept endpoint",
        "Answer_preprocessed_content":"gener monitor avail support ak box implement support unfortun check document screenshot refer support format monitor ak",
        "Answer_gpt_summary_original":"possible solutions: \n- check the documentation provided by microsoft for supportive formats of monitoring in aks.\n- use general monitoring available and supportive in aks.",
        "Answer_gpt_summary":"possibl solut check document provid microsoft support format monitor ak us gener monitor avail support ak"
    },
    {
        "Question_id":null,
        "Question_title":"ParallelRunStep doesn\u00b4t support multiple input datasets",
        "Question_body":"Currently we are using the parallelrunstep in AML, Is there any workaround\/other approach to support distribute datasets in the parallel run step.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1647759270327,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/779233\/parallelrunstep-doesnt-support-multiple-input-data.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-21T05:31:00.423Z",
                "Answer_score":0,
                "Answer_body":"@AI-6562 Thanks for the question. An Azure ML dataset is just metadata pointing to a path or collection of paths in an Azure storage account. You should first \"merge\" those datasets into a collection of adjacent folders (e.g. root\/dataset1\/, root\/dataset2\/, ...) and then run PRS against root\/**.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"parallelrunstep doesnt support multipl input dataset current parallelrunstep aml workaround approach support distribut dataset parallel run step",
        "Question_preprocessed_content":"parallelrunstep doesnt support multipl input dataset current parallelrunstep aml approach support distribut dataset parallel run step",
        "Question_gpt_summary_original":"The user is facing a challenge with the parallelrunstep in AML as it does not support multiple input datasets. They are seeking a workaround or alternative approach to distribute datasets in the parallel run step.",
        "Question_gpt_summary":"user face challeng parallelrunstep aml support multipl input dataset seek workaround altern approach distribut dataset parallel run step",
        "Answer_original_content":"thank question dataset metadata point path collect path azur storag account merg dataset collect adjac folder root dataset root dataset run pr root",
        "Answer_preprocessed_content":"thank question dataset metadata point path collect path azur storag account merg dataset collect adjac folder run pr",
        "Answer_gpt_summary_original":"possible solution: merge datasets into a collection of adjacent folders and run parallel run step against the root folder.",
        "Answer_gpt_summary":"possibl solut merg dataset collect adjac folder run parallel run step root folder"
    },
    {
        "Question_id":null,
        "Question_title":"Getting Error Deadline Exceeded when deploying model from Cloud Firestoer functions",
        "Question_body":"Hello,I currently have a firebase function that is set to deploy my AutoML tables model everyday at 5am. This has been working fine for the past month, up until the last week. I have been getting the following error below when the function attempts to deploy the model.I watched a google tutorial and it recommend to return a promise from my cloud function. That seemed to work for 1 day, but I received the error again this morning.I am going to try to implement a retry function, but I figured I would ask on here as well. Also, I am thinking that moving from autoML to VertexAI might help alleviate my issues. Any guidance here is helpful.See below for my deploy model code:  ",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1638530220000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":860.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Getting-Error-Deadline-Exceeded-when-deploying-model-from-Cloud\/td-p\/177128\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-03T12:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Is deploying the model fails via Cloud Functions only, or does it also fail when the model is deployed manually, or by any other means?"
            },
            {
                "Answer_creation_time":"2021-12-03T15:00:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Manually (through the cloud console) it usually works. I have been testing using an http call in Firebase functions. Sometimes (not consistantly) that fails if I have recently undeployed the model. Although, I do not get any type of error notification, I just know it fails by checking the cloud console.\n\nThe deploy model function only runs once a day though, and the model has typically been undeployed for at least 20 hours before that, so I don't think I am getting that error because I am calling it too often."
            },
            {
                "Answer_creation_time":"2021-12-09T10:39:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"So after further investigation it looks like the model is failing when I am requesting predictions through firebase functions. I get the same error \"DEADLINE_EXCEEDED\". This is not consistent though, it worked for the previous 2 days before this and today failed again. I haven't changed anything.\n\nI have 2 questions:\n\n1) is it possible that congestion on the network is causing these to fail? Would it help if I moved the prediction to a different time? Currently I have it set at 6am PST.\n\n2) Since autoML is beta, would it help if I moved the model to VertexAI? I can make that move if it helps"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get error deadlin exceed deploi model cloud firesto function hello current firebas function set deploi automl tabl model everydai work fine past month week get follow error function attempt deploi model watch googl tutori recommend return promis cloud function work dai receiv error morn go try implement retri function figur ask think move automl vertexai help allevi issu guidanc help deploi model code",
        "Question_preprocessed_content":"get error deadlin exceed deploi model cloud firesto function hello current firebas function set deploi automl tabl model everydai work fine past month week get follow error function attempt deploi watch googl tutori recommend return promis cloud function work dai receiv error go try implement retri function figur ask think move automl vertexai help allevi issu guidanc deploi model code",
        "Question_gpt_summary_original":"The user is encountering an error of \"Deadline Exceeded\" when deploying their AutoML tables model from Cloud Firestore functions. This issue has been occurring for the past week, despite implementing a solution recommended in a Google tutorial. The user plans to implement a retry function and is considering switching to VertexAI to resolve the issue.",
        "Question_gpt_summary":"user encount error deadlin exceed deploi automl tabl model cloud firestor function issu occur past week despit implement solut recommend googl tutori user plan implement retri function consid switch vertexai resolv issu",
        "Answer_original_content":"deploi model fail cloud function fail model deploi manual mean manual cloud consol usual work test http firebas function consistantli fail recent undeploi model type error notif know fail check cloud consol deploi model function run dai model typic undeploi hour think get error call investig look like model fail request predict firebas function error deadlin exceed consist work previou dai todai fail haven chang question possibl congest network caus fail help move predict differ time current set pst automl beta help move model vertexai help",
        "Answer_preprocessed_content":"deploi model fail cloud function fail model deploi manual mean manual usual work test http firebas function fail recent undeploi model type error notif know fail check cloud consol deploi model function run dai model typic undeploi hour think get error call investig look like model fail request predict firebas function error consist work previou dai todai fail haven chang question possibl congest network caus fail help move predict differ time current set pst automl beta help move model vertexai help",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n1. the user can try deploying the model manually through the cloud console instead of using an http call in firebase functions. \n2. the user can check if network congestion is causing the issue and try moving the prediction to a different time. \n3. the user can consider moving the model to vertexai as it may help since automl is still in beta.",
        "Answer_gpt_summary":"possibl solut answer user try deploi model manual cloud consol instead http firebas function user check network congest caus issu try move predict differ time user consid move model vertexai help automl beta"
    },
    {
        "Question_id":null,
        "Question_title":"How to retrieve all the artifacts lineage for a set of runs, based on both the name of the run, and the name of the artifact?",
        "Question_body":"From slack\n\nI have several runs with the named RUN_NAME each run is logging an artifact named ARTIFACT_NAME. I would like to query all these artifacts from all this subset of runs.\n\nWhat I\u2019m using to try to achieve this is calling RunClient.client.runs_v1.get_runs_artifacts_lineage, but in this function I get all artifacts from all runs ever independently from the run name.\nAlso, when listing the artifacts from these runs, for some reason the path is always None, even though if I use the function you suggested last time, get_artifacts_lineage , for a specific run, I do get values on the path field.\nSo my two main issues are:\n\nHow do I get all artifacts from a set of runs with the same run_name?\nWhy the path is empty in case this is the correct function to use?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649410426000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1486",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-08T09:35:06Z",
                "Answer_score":1,
                "Answer_body":"To filter all artifacts lineage directly by run name and artifact name:\n\nfrom polyaxon.client import RunClient\n\nRunClient.client.runs_v1.get_runs_artifacts_lineage(project=\"PROJECT_NAME\", query=\"run.name: RUN_NAME, kind: ARTIFACT_KIND, name: ARTIFACT_NAME\")"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0,
        "Question_closed_time":null,
        "Question_original_content":"retriev artifact lineag set run base run artifact slack run name run run log artifact name artifact like queri artifact subset run try achiev call runclient client run run artifact lineag function artifact run independ run list artifact run reason path us function suggest time artifact lineag specif run valu path field main issu artifact set run run path case correct function us",
        "Question_preprocessed_content":"retriev artifact lineag set run base run artifact slack run name run log artifact name like queri artifact subset run try achiev call function artifact run independ run list artifact run reason path us function suggest time specif run valu path field main issu artifact set run path case correct function us",
        "Question_gpt_summary_original":"The user is facing two main challenges. Firstly, they are unable to retrieve all artifacts from a subset of runs with the same run name using the RunClient.client.runs_v1.get_runs_artifacts_lineage function. Secondly, when listing the artifacts from these runs, the path is always empty, even though the get_artifacts_lineage function for a specific run returns values on the path field.",
        "Question_gpt_summary":"user face main challeng firstli unabl retriev artifact subset run run runclient client run run artifact lineag function secondli list artifact run path artifact lineag function specif run return valu path field",
        "Answer_original_content":"filter artifact lineag directli run artifact client import runclient runclient client run run artifact lineag project project queri run run kind artifact kind artifact",
        "Answer_preprocessed_content":"filter artifact lineag directli run artifact client import runclient kind",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to use the Polyaxon client's `get_runs_artifacts_lineage` function to filter all artifacts lineage directly by run name and artifact name. The code snippet provided in the discussion shows how to use this function to retrieve the desired subset of artifacts. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss us client run artifact lineag function filter artifact lineag directli run artifact code snippet provid discuss show us function retriev desir subset artifact solut mention discuss"
    },
    {
        "Question_id":null,
        "Question_title":"store data preprocessing information",
        "Question_body":"Hi,\nI've tried mlflow to store metrics, params, artifacts and models.\nI'd like to know if it can store also data-preprocessing information (feature extraction, selection, transformation, data cleansing, etc.).\nThanks\n--giacomo",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1600770098000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":35.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/gZC21Br284g",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-27T04:29:51",
                "Answer_body":"You can store arbitrary information with log_param\/log_params - you're not\nlimited to the parameters of the model itself. For example,\nmlflow.spark.autolog\n(https:\/\/mlflow.org\/docs\/latest\/tracking.html#spark-experimental) could log\nthe location of the data, like, source path, version, etc.\n\nGiacomo Bonanni at \"Tue, 22 Sep 2020 07:21:38 -0700 (PDT)\" wrote:\nGB> Hi,\nGB> I've tried mlflow to store metrics, params, artifacts and models.\nGB> I'd like to know if it can store also data-preprocessing information (feature extraction, selection, transformation, data cleansing, etc.).\nGB> Thanks\nGB> --giacomo\n\n\n\n--\nWith best wishes, Alex Ott\nhttp:\/\/alexott.net\/\nTwitter: alexott_en (English), alexott (Russian)"
            },
            {
                "Answer_creation_time":"2020-10-08T05:25:57",
                "Answer_body":"I'm struggling with a same question as I'm new to mlflow.\n\n\n\nE.g. suppose you want to store the parameters (mean, var) from the sklearn StandardScaler. Both are a list of floats.\n\nThese parameters are required when you want to run the model (in my case the model is stored using `mlflow.pytorch.log_model`). In the predict\nfunction the model is loaded again with `mlflow.pytorch.load_model`, and my question is, how do I access the scaling parameters (mean and var)?\n\nI think they should be part of the artifacts, right? Or should my model include the scaling as part of a pipeline?\n\n\nThanks,\nErwin\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"store data preprocess inform tri store metric param artifact model like know store data preprocess inform featur extract select transform data cleans thank giacomo",
        "Question_preprocessed_content":"store data preprocess inform tri store metric param artifact model like know store inform thank giacomo",
        "Question_gpt_summary_original":"The user is inquiring about whether mlflow can store data-preprocessing information such as feature extraction, selection, transformation, and data cleansing, in addition to metrics, params, artifacts, and models.",
        "Question_gpt_summary":"user inquir store data preprocess inform featur extract select transform data cleans addit metric param artifact model",
        "Answer_original_content":"store arbitrari inform log param log param limit paramet model exampl spark autolog http org doc latest track html spark experiment log locat data like sourc path version giacomo bonanni tue sep pdt wrote tri store metric param artifact model like know store data preprocess inform featur extract select transform data cleans thank giacomo best wish alex ott http alexott net twitter alexott english alexott russian struggl question new suppos want store paramet mean var sklearn standardscal list float paramet requir want run model case model store pytorch log model predict function model load pytorch load model question access scale paramet mean var think artifact right model includ scale pipelin thank erwin",
        "Answer_preprocessed_content":"store arbitrari inform limit paramet model exampl log locat data like sourc path version giacomo bonanni tue sep wrote tri store metric param artifact model like know store inform thank best wish alex ott twitter alexott struggl question new suppos want store paramet sklearn standardscal list float paramet requir want run model predict function model load question access scale paramet think artifact right model includ scale pipelin thank erwin",
        "Answer_gpt_summary_original":"Solution:\nThe discussion suggests that you can store arbitrary information with log_param\/log_params, and you are not limited to the parameters of the model itself. For example, mlflow.spark.autolog could log the location of the data, like source path, version, etc. However, no specific solution is provided for storing data-preprocessing information such as feature extraction, selection, transformation, data cleansing, etc.",
        "Answer_gpt_summary":"solut discuss suggest store arbitrari inform log param log param limit paramet model exampl spark autolog log locat data like sourc path version specif solut provid store data preprocess inform featur extract select transform data cleans"
    },
    {
        "Question_id":56353814.0,
        "Question_title":"Batch transform job results in \"InternalServerError\" with data file >100MB",
        "Question_body":"<p>I'm using Sagemaker in order to perform binary classification on time series, each sample being a numpy array of shape [24,11] (24h, 11features). I used a tensorflow model in script mode, my script being very similar to the one I used as reference:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\/mnist.py<\/a><\/p>\n\n<p>The training reported success and I was able to deploy a model for batch transformation. The transform job works fine when I input just a few samples (say, [10,24,11]), but it returns an <code>InternalServerError<\/code> when I input more samples for prediction (for example, [30000, 24, 11], which size is >100MB).<\/p>\n\n<p>Here is the error:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-0c46f7563389&gt; in &lt;module&gt;()\n     32 \n     33 # Then wait until transform job is completed\n---&gt; 34 tf_transformer.wait()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    133     def wait(self):\n    134         self._ensure_last_transform_job()\n--&gt; 135         self.latest_transform_job.wait()\n    136 \n    137     def _ensure_last_transform_job(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n    207 \n    208     def wait(self):\n--&gt; 209         self.sagemaker_session.wait_for_transform_job(self.job_name)\n    210 \n    211     @staticmethod\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n    893         \"\"\"\n    894         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n--&gt; 895         self._check_job_status(job, desc, 'TransformJobStatus')\n    896         return desc\n    897 \n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    915             reason = desc.get('FailureReason', '(No reason provided)')\n    916             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 917             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    918 \n    919     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Transform job Tensorflow-batch-transform-2019-05-29-02-56-00-477: Failed Reason: InternalServerError: We encountered an internal error.  Please try again.\n\n<\/code><\/pre>\n\n<p>I tried to use both SingleRecord and MultiRecord parameters when deploying the model but the result was the same, so I decided to keep MultiRecord. My transformer looks like that:<\/p>\n\n<pre><code>transformer = tf_estimator.transformer(\n    instance_count=1, \n    instance_type='ml.m4.xlarge',\n    max_payload = 100,\n    assemble_with = 'Line',\n    strategy='MultiRecord'\n)\n<\/code><\/pre>\n\n<p>At first I was using a json file as input for the transform job, and it threw the error : <\/p>\n\n<pre><code>Too much data for max payload size\n<\/code><\/pre>\n\n<p>So next I tried the jsonlines format (the .npy format is not supported as far as I understand), thinking that jsonlines could get split by Line and thus avoid the size error, but that's where I got the <code>InternalServerError<\/code>. Here is the related code:<\/p>\n\n<pre><code>#Convert test_x to jsonlines and save\ntest_x_list = test_x.tolist()\nfile_path ='data_cnn_test\/test_x.jsonl'\nfile_name='test_x.jsonl'\n\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)    \n\ninput_key = 'batch_transform_tf\/input\/{}'.format(file_name)\noutput_key = 'batch_transform_tf\/output'\ntest_input_location = 's3:\/\/{}\/{}'.format(bucket, input_key)\ntest_output_location = 's3:\/\/{}\/{}'.format(bucket, output_key)\n\ns3.upload_file(file_path, bucket, input_key)\n\n# Initialize the transformer object\ntf_transformer = sagemaker.transformer.Transformer(\n    base_transform_job_name='Tensorflow-batch-transform',\n    model_name='sagemaker-tensorflow-scriptmode-2019-05-29-02-46-36-162',\n    instance_count=1,\n    instance_type='ml.c4.2xlarge',\n    output_path=test_output_location,\n    assemble_with = 'Line'\n    )\n\n# Start the transform job\ntf_transformer.transform(test_input_location, content_type='application\/jsonlines', split_type='Line')\n<\/code><\/pre>\n\n<p>The list named test_x_list has a shape [30000, 24, 11], which corresponds to 30000 samples so I would like to return 30000 predictions.<\/p>\n\n<p>I suspect my jsonlines file isn't being split by Line and is of course too big to be processed in one batch, which throws the error, but I don't understand why it doesn't get split correctly. I am using the default output_fn and input_fn (I did not re-write those functions in my script).<\/p>\n\n<p>Any insight on what I could be doing wrong would be greatly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559108619707,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1826.0,
        "Answer_body":"<p>I assume this is a duplicate of this AWS Forum post: <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0\" rel=\"nofollow noreferrer\">https:\/\/forums.aws.amazon.com\/thread.jspa?threadID=303810&amp;tstart=0<\/a><\/p>\n\n<p>Anyway, for completeness I'll answer here as well.<\/p>\n\n<p>The issue is that you are serializing your dataset incorrectly when converting it into jsonlines:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    writer.write(test_x_list)   \n<\/code><\/pre>\n\n<p>What the above is doing is creating a very large single-line containing your full dataset which is too big for single inference call to consume.<\/p>\n\n<p>I suggest you change your code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset:<\/p>\n\n<pre><code>test_x_list = test_x.tolist()\n...\nwith jsonlines.open(file_path, 'w') as writer:\n    for sample in test_x_list:\n        writer.write(sample)\n<\/code><\/pre>\n\n<p>If one sample at a time is too slow you can also play around with the <code>max_concurrent_transforms<\/code>, <code>strategy<\/code>, and <code>max_payload<\/code> parameters to be able to batch the data as well as run concurrent transforms if your algorithm can run in parallel - also, of course, you can split the data into multiple files and run transformations with more than just one node. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/latest\/transformer.html<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html<\/a> for additional detail on what these parameters do.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56353814",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1559329920120,
        "Question_original_content":"batch transform job result internalservererror data file order perform binari classif time seri sampl numpi arrai shape featur tensorflow model script mode script similar refer http github com awslab amazon exampl blob master python sdk tensorflow script mode train serv mnist train report success abl deploi model batch transform transform job work fine input sampl return internalservererror input sampl predict exampl size error valueerror traceback recent wait transform job complet transform wait anaconda env tensorflow lib python site packag transform wait self def wait self self ensur transform job self latest transform job wait def ensur transform job self anaconda env tensorflow lib python site packag transform wait self def wait self self session wait transform job self job staticmethod anaconda env tensorflow lib python site packag session wait transform job self job poll desc wait lambda transform job statu self client job poll self check job statu job desc transformjobstatu return desc anaconda env tensorflow lib python site packag session check job statu self job desc statu kei reason desc failurereason reason provid job type statu kei replac jobstatu job rais valueerror error reason format job type job statu reason def wait endpoint self endpoint poll valueerror error transform job tensorflow batch transform fail reason internalservererror encount intern error try tri us singlerecord multirecord paramet deploi model result decid multirecord transform look like transform estim transform instanc count instanc type xlarg max payload assembl line strategi multirecord json file input transform job threw error data max payload size tri jsonlin format npy format support far understand think jsonlin split line avoid size error got internalservererror relat code convert test jsonlin save test list test tolist file path data cnn test test jsonl file test jsonl jsonlin open file path writer writer write test list input kei batch transform input format file output kei batch transform output test input locat format bucket input kei test output locat format bucket output kei upload file file path bucket input kei initi transform object transform transform transform base transform job tensorflow batch transform model tensorflow scriptmod instanc count instanc type xlarg output path test output locat assembl line start transform job transform transform test input locat content type applic jsonlin split type line list name test list shape correspond sampl like return predict suspect jsonlin file isn split line cours big process batch throw error understand split correctli default output input write function script insight wrong greatli appreci",
        "Question_preprocessed_content":"batch transform job result internalservererror data file order perform binari classif time seri sampl numpi arrai shape tensorflow model script mode script similar refer train report success abl deploi model batch transform transform job work fine input sampl return input sampl predict error tri us singlerecord multirecord paramet deploi model result decid multirecord transform look like json file input transform job threw error tri jsonlin format think jsonlin split line avoid size error got relat code list name shape correspond sampl like return predict suspect jsonlin file isn split line cours big process batch throw error understand split correctli default insight wrong greatli appreci",
        "Question_gpt_summary_original":"The user is encountering an \"InternalServerError\" error when attempting to perform binary classification on time series data using Sagemaker. The error occurs when attempting to input a large data file (>100MB) for prediction. The user has tried using both SingleRecord and MultiRecord parameters, as well as json and jsonlines formats, but the error persists. The user suspects that the jsonlines file is not being split correctly and is too large to be processed in one batch.",
        "Question_gpt_summary":"user encount internalservererror error attempt perform binari classif time seri data error occur attempt input larg data file predict user tri singlerecord multirecord paramet json jsonlin format error persist user suspect jsonlin file split correctli larg process batch",
        "Answer_original_content":"assum duplic aw forum post http forum aw amazon com thread jspa threadid tstart complet answer issu serial dataset incorrectli convert jsonlin test list test tolist jsonlin open file path writer writer write test list creat larg singl line contain dataset big singl infer consum suggest chang code line singl sampl infer place individu sampl instead dataset test list test tolist jsonlin open file path writer sampl test list writer write sampl sampl time slow plai max concurr transform strategi max payload paramet abl batch data run concurr transform algorithm run parallel cours split data multipl file run transform node http readthedoc latest transform html http doc aw amazon com latest api createtransformjob html addit paramet",
        "Answer_preprocessed_content":"assum duplic aw forum post complet answer issu serial dataset incorrectli convert jsonlin creat larg contain dataset big singl infer consum suggest chang code line singl sampl infer place individu sampl instead dataset sampl time slow plai paramet abl batch data run concurr transform algorithm run parallel cours split data multipl file run transform node addit paramet",
        "Answer_gpt_summary_original":"the solution to the \"internalservererror\" encountered when attempting to perform a batch transformation job with a data file larger than 100mb is to change the code to make each line a single sample so that inference can take place on individual samples instead of the whole dataset. if one sample at a time is too slow, the user can also play around with the max_concurrent_transforms, strategy, and max_payload parameters to be able to batch the data as well as run concurrent transforms if their algorithm can run in parallel. additionally, the user can split the data into multiple files and run transformations with more than just one node.",
        "Answer_gpt_summary":"solut internalservererror encount attempt perform batch transform job data file larger chang code line singl sampl infer place individu sampl instead dataset sampl time slow user plai max concurr transform strategi max payload paramet abl batch data run concurr transform algorithm run parallel addition user split data multipl file run transform node"
    },
    {
        "Question_id":null,
        "Question_title":"How to deploy a scikit learn regression model as a web service?",
        "Question_body":"Hello,\n\nI find the documentation related with ML model deployment overwhelming and I'm struggling with the most basic \"Hello world\" tutorial even after several days of research.\n\nAll I want is to deploy the most basic model as a web service that can be consumed via an API through Power BI or any other web app to serve as a POC. Then we can think about \"scale\", \"dockers\", \"containers\", etc...\n\nThis is my code in Python 3.6:\n\n\n\n import joblib\n    \n from sklearn.datasets import load_diabetes\n from sklearn.linear_model import Ridge    \n    \n dataset_x, dataset_y = load_diabetes(return_X_y=True)\n    \n model = Ridge().fit(dataset_x, dataset_y)\n    \n joblib.dump(model, 'sklearn_regression_model.pkl')\n\n\n\n\nThis model as features as an array like:\n\n array([[ 0.03807591,  0.05068012],[ ... , ...]])\n\n\n\n\nAs you can see, I've got the model serialized into a sklearn_regression_model.pkl file, I've also got the ML environment setup in Azure ML, a compute instance, I'm familiar with Python and the designer and prefer notebooks.\n\nHow can I deploy this model through an API like:\n\nhttps:\/\/api.xxx.com&parameters....\n\nThanks for any help!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629821122530,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/526203\/how-to-deploy-a-scikit-learn-regression-model-as-a.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-25T17:00:18.437Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nThanks for reaching out to us. Please refer to this guidance to deploy your model and service. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli#deploy-again-and-call-your-service\n\nPlease refer to deploy from local file for your scenario. The workflow is similar no matter where you deploy your model:\n\nRegister the model - Please register from local file\nPrepare an entry script\nPrepare an inference configuration\nDeploy the model locally to ensure everything works\nChoose a compute target\nRe-deploy the model to the cloud\nTest the resulting web service\n\nHope this helps.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deploi scikit learn regress model web servic hello document relat model deploy overwhelm struggl basic hello world tutori dai research want deploi basic model web servic consum api power web app serv poc think scale docker contain code python import joblib sklearn dataset import load diabet sklearn linear model import ridg dataset dataset load diabet return true model ridg fit dataset dataset joblib dump model sklearn regress model pkl model featur arrai like arrai got model serial sklearn regress model pkl file got environ setup comput instanc familiar python design prefer notebook deploi model api like http api com paramet thank help",
        "Question_preprocessed_content":"deploi scikit learn regress model web servic hello document relat model deploy overwhelm struggl basic hello world tutori dai research want deploi basic model web servic consum api power web app serv poc think scale docker contain code python import joblib import import ridg model model featur arrai like arrai got model serial file got environ setup comput instanc familiar python design prefer notebook deploi model api like thank help",
        "Question_gpt_summary_original":"The user is struggling to deploy a basic scikit learn regression model as a web service that can be consumed via an API through Power BI or any other web app to serve as a POC. Despite having the model serialized into a file and the ML environment set up in Azure ML, the user is finding the documentation related to ML model deployment overwhelming and is unable to deploy the model through an API.",
        "Question_gpt_summary":"user struggl deploi basic scikit learn regress model web servic consum api power web app serv poc despit have model serial file environ set user find document relat model deploy overwhelm unabl deploi model api",
        "Answer_original_content":"hello thank reach refer guidanc deploi model servic http doc microsoft com azur machin learn deploi tab azcli deploi servic refer deploi local file scenario workflow similar matter deploi model regist model regist local file prepar entri script prepar infer configur deploi model local ensur work choos comput target deploi model cloud test result web servic hope help regard yutong",
        "Answer_preprocessed_content":"hello thank reach refer guidanc deploi model servic refer deploi local file scenario workflow similar matter deploi model regist model regist local file prepar entri script prepar infer configur deploi model local ensur work choos comput target model cloud test result web servic hope help regard yutong",
        "Answer_gpt_summary_original":"the answer provides a link to a guidance document on how to deploy a machine learning model as a web service using azure. the guidance suggests registering the model from a local file, preparing an entry script, preparing an inference configuration, deploying the model locally, choosing a compute target, and re-deploying the model to the cloud. the user is advised to test the resulting web service.",
        "Answer_gpt_summary":"answer provid link guidanc document deploi machin learn model web servic azur guidanc suggest regist model local file prepar entri script prepar infer configur deploi model local choos comput target deploi model cloud user advis test result web servic"
    },
    {
        "Question_id":null,
        "Question_title":"Getting \"Unsupported URL\" when adding SSH remote",
        "Question_body":"<p>Hello,<\/p>\n<p>I am trying to add a new ssh remote.<\/p>\n<p><code>dvc remote add base ssh:\/\/elena@server_name.company.lan:\/home\/elena\/test_dvc\/data<\/code><\/p>\n<p>and I get a <code>Initialization error: Config file error: Unsupported URL<\/code> when trying <code>dvc status\/pull\/push<\/code>.<\/p>\n<p>I tried adding<br>\n<code>dvc remote modify base credentialpath ~\/.ssh\/config<\/code><br>\nbut the error persists.<\/p>\n<p>Can anyone help with an example on how to fully configure an ssh remote for dvc?<\/p>\n<p>Later edit:<br>\nSolved: Typo in the remote url (the remote path was missing \u2018\/\u2019 )<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1533818634630,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1379.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/getting-unsupported-url-when-adding-ssh-remote\/64",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-08-09T12:58:51.489Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/elena\">@elena<\/a> !<\/p>\n<p>Glad it resolved itself. Feel free to ask any questions, we are always happy to help!<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":17.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-08-09T16:07:44.784Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>Thanks for your prompt reply <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=5\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\">.<br>\nI have another question related to ssh remotes for dvc.<br>\nCurrently in my config file, the remote url has a \u2018hardocded\u2019 to my user name<br>\n<code>['remote \"base\"'] url = ssh:\/\/elena@server.lan:\/mnt\/data<\/code><\/p>\n<p>Ideally, I\u2019d want my team to just do:<br>\n<code>git pull &amp;&amp; dvc pull<\/code> and have everything set.<br>\nWith my current setup however, they\u2019ll have to manually edit the config file or edit the remote like so:<br>\n<code>dvc remote modify base url ssh:\/\/john@server.lan:\/mnt\/data<\/code><\/p>\n<p>Is there any way to make that dynamic or use an environment variable($USER) in the config instead?<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-08-09T16:43:44.779Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/elena\">@elena<\/a> !<\/p>\n<p>We are actually currently defaulting to <code>getuser()<\/code>(i.e. $USER) if user is not specified in the url or with the <code>dvc remote modify base user john<\/code>, so it should actually work as is. Have you tried it?(i.e. just removing user from your url)<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-08-09T17:14:27.520Z",
                "Answer_body":"<p>Yes, I see.<br>\nThe problem is that the username on the remote servers differ slightly from the $USER on my local machine.<\/p>\n<p>Thanks,<\/p>",
                "Answer_score":22.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-08-09T17:44:49.900Z",
                "Answer_body":"<p>Ah, I see. In such case, you and your colleagues could use <code>.dvc\/config.local<\/code>, which is a \u201clocal\u201d version of <code>.dvc\/config<\/code>, that is set to be ignored by git, so it stays on your machine in your repository. You and your colleagues will have to set it up just once when they first clone the git repository. The syntax would be:<\/p>\n<pre><code class=\"lang-auto\">dvc remote add --local base ssh:\/\/john@server.lan:\/mnt\/data\n<\/code><\/pre>\n<p>Would that suit your needs?<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":12.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-08-09T18:51:35.998Z",
                "Answer_body":"<p>Wonderful!<\/p>\n<p>Thank you!<\/p>",
                "Answer_score":2.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get unsupport url ad ssh remot hello try add new ssh remot remot add base ssh elena server compani lan home elena test data initi error config file error unsupport url try statu pull push tri ad remot modifi base credentialpath ssh config error persist help exampl fulli configur ssh remot later edit solv typo remot url remot path miss thank",
        "Question_preprocessed_content":"get unsupport url ad ssh remot hello try add new ssh remot try tri ad error persist help exampl fulli configur ssh remot later edit solv typo remot url thank",
        "Question_gpt_summary_original":"The user encountered an \"Unsupported URL\" error when trying to add a new SSH remote and was unable to perform dvc status\/pull\/push. They attempted to modify the credential path but the error persisted. The user requested help on how to fully configure an SSH remote for dvc. The issue was later resolved as a typo in the remote URL.",
        "Question_gpt_summary":"user encount unsupport url error try add new ssh remot unabl perform statu pull push attempt modifi credenti path error persist user request help fulli configur ssh remot issu later resolv typo remot url",
        "Answer_original_content":"elena glad resolv feel free ask question happi help thank ruslan thank prompt repli question relat ssh remot current config file remot url hardocd user remot base url ssh elena server lan mnt data ideal want team git pull pull set current setup theyll manual edit config file edit remot like remot modifi base url ssh john server lan mnt data wai dynam us environ variabl user config instead elena actual current default getus user user specifi url remot modifi base user john actual work tri remov user url thank ruslan ye problem usernam remot server differ slightli user local machin thank case colleagu us config local local version config set ignor git stai machin repositori colleagu set clone git repositori syntax remot add local base ssh john server lan mnt data suit need thank ruslan wonder thank",
        "Answer_preprocessed_content":"glad resolv feel free ask question happi help thank ruslan thank prompt repli question relat ssh remot current config file remot url hardocd user ideal want team set current setup theyll manual edit config file edit remot like wai dynam us environ variabl config instead actual current default user specifi url actual work tri remov user url thank ruslan ye problem usernam remot server differ slightli user local machin thank case colleagu us local version set ignor git stai machin repositori colleagu set clone git repositori syntax suit need thank ruslan wonder thank",
        "Answer_gpt_summary_original":"the solution to the problem of an \"unsupported url\" error when adding an ssh remote is to use an environment variable ($user) in the config file or to create a local version of the config file that is ignored by git. the latter option requires setting up the local config file once when cloning the git repository.",
        "Answer_gpt_summary":"solut problem unsupport url error ad ssh remot us environ variabl user config file creat local version config file ignor git option requir set local config file clone git repositori"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to invert the logged images?",
        "Question_body":"<p>Hi,<\/p>\n<p>is it possible to invert my logged images ? Or is there maybe a way to do that in pytorch ?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660223927393,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":110.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-invert-the-logged-images\/2907",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-15T21:10:22.932Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/sophie_jo\">@sophie_jo<\/a> ,<\/p>\n<p>There isn\u2019t an ability to invert images directly from the UI. You are correct that this can be done in <a href=\"https:\/\/pytorch.org\/vision\/stable\/generated\/torchvision.transforms.functional.invert.html\" rel=\"noopener nofollow ugc\">pytorch<\/a>.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-14T21:10:34.462Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"possibl invert log imag possibl invert log imag mayb wai pytorch",
        "Question_preprocessed_content":"possibl invert log imag possibl invert log imag mayb wai pytorch",
        "Question_gpt_summary_original":"The user is facing a challenge of inverting their logged images and is seeking a solution, possibly through the use of PyTorch.",
        "Question_gpt_summary":"user face challeng invert log imag seek solut possibl us pytorch",
        "Answer_original_content":"sophi isnt abil invert imag directli correct pytorch topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"isnt abil invert imag directli correct pytorch topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions: \n- inverting images cannot be done directly from the ui, but it can be done in pytorch.",
        "Answer_gpt_summary":"possibl solut invert imag directli pytorch"
    },
    {
        "Question_id":54295445.0,
        "Question_title":"TensorFlow Serving send data as b64 instead of Numpy Array",
        "Question_body":"<p>I have a TensorFlow Serving container in a SageMaker endpoint. I'm able to take a batch of images as a Numpy array and get back predictions like this:<\/p>\n\n<pre><code>import numpy as np\nimport sagemaker\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\nimage = np.random.uniform(low=-1.0, high=1.0, size=(1,128,128,3)).astype(np.float32)    \nimage = {'instances': image}\nimage = json_serializer(image)\n\nrequest_args = {}\nrequest_args['Body'] = image\nrequest_args['EndpointName'] = endpoint_name\nrequest_args['ContentType'] = 'application\/json'\nrequest_args['Accept'] = 'application\/json'\n\n# works successfully\nresponse = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\nresponse_body = response['Body']\npredictions = json_deserializer(response_body, response['ContentType'])\n<\/code><\/pre>\n\n<p>The size of the <code>request_args<\/code> payload is large doing it this way. I'm wondering, is there a way to send this in a more compressed format? <\/p>\n\n<p>I've tried experimenting with <code>base64<\/code> and <code>json.dumps<\/code>, but can't get past <code>Invalid argument: JSON Value: ...<\/code> errors. Not sure if this isn't supported or if I'm just doing it incorrectly.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1548093365813,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":796.0,
        "Answer_body":"<p>I've talked to AWS support about this (see <a href=\"https:\/\/stackoverflow.com\/questions\/54090270\/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s\">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?<\/a>).<\/p>\n\n<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).<\/p>\n\n<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54295445",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1548251290416,
        "Question_original_content":"tensorflow serv send data instead numpi arrai tensorflow serv contain endpoint abl batch imag numpi arrai predict like import numpi import predictor import json serial json deseri imag random uniform low high size astyp float imag instanc imag imag json serial imag request arg request arg bodi imag request arg endpointnam endpoint request arg contenttyp applic json request arg accept applic json work successfulli respons session runtim client invok endpoint request arg respons bodi respons bodi predict json deseri respons bodi respons contenttyp size request arg payload larg wai wonder wai send compress format tri experi base json dump past invalid argument json valu error sure isn support incorrectli",
        "Question_preprocessed_content":"tensorflow serv send data instead numpi arrai tensorflow serv contain endpoint abl batch imag numpi arrai predict like size payload larg wai wonder wai send compress format tri experi past error sure isn support incorrectli",
        "Question_gpt_summary_original":"The user is facing a challenge in sending a batch of images as a Numpy array to a TensorFlow Serving container in a SageMaker endpoint. The payload size of the request is large, and the user is looking for a way to send it in a more compressed format. The user has tried using base64 and json.dumps but is encountering errors.",
        "Question_gpt_summary":"user face challeng send batch imag numpi arrai tensorflow serv contain endpoint payload size request larg user look wai send compress format user tri base json dump encount error",
        "Answer_original_content":"talk aw support effici wai send request json deploi tensorflow model suggest possibl pass custom input serv contain unpack compress format protobuf test soon hopefulli stuff work add lot flexibl input process",
        "Answer_preprocessed_content":"talk aw support suggest possibl pass custom serv contain unpack compress format test soon hopefulli stuff work add lot flexibl input process",
        "Answer_gpt_summary_original":"the solution to the challenge of sending a large payload to a tensorflow serving container in a compressed format is to pass in a custom input_fn that can unpack a compressed format such as protobuf. this solution was suggested by aws support and the user plans to test it soon.",
        "Answer_gpt_summary":"solut challeng send larg payload tensorflow serv contain compress format pass custom input unpack compress format protobuf solut suggest aw support user plan test soon"
    },
    {
        "Question_id":61225077.0,
        "Question_title":"How to delete worker task templates in AWS Augmented AI?",
        "Question_body":"<p>I've created the worker task template for test in AWS Augmented AI.\nHowever, I don't know how to delete those template.<\/p>\n\n<p>Please tell me how to do it.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/CZkiz.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1586941750363,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":"<p>You cannot currently delete HumanTaskUis. That may be a capability added in the future.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61225077",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1587598650403,
        "Question_original_content":"delet worker task templat aw augment creat worker task templat test aw augment know delet templat tell imag",
        "Question_preprocessed_content":"delet worker task templat aw augment creat worker task templat test aw augment know delet templat tell imag",
        "Question_gpt_summary_original":"The user is facing a challenge in deleting worker task templates in AWS Augmented AI and is seeking guidance on how to do it.",
        "Question_gpt_summary":"user face challeng delet worker task templat aw augment seek guidanc",
        "Answer_original_content":"current delet humantaskui capabl ad futur",
        "Answer_preprocessed_content":"current delet humantaskui capabl ad futur",
        "Answer_gpt_summary_original":"there are currently no solutions available for deleting worker task templates in aws augmented ai. the capability to do so may be added in the future.",
        "Answer_gpt_summary":"current solut avail delet worker task templat aw augment capabl ad futur"
    },
    {
        "Question_id":52684987.0,
        "Question_title":"AWS NoCredentials in training",
        "Question_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":3,
        "Question_creation_time":1538879533133,
        "Question_favorite_count":null,
        "Question_last_edit_time":1539893383687,
        "Question_score":1.0,
        "Question_view_count":1374.0,
        "Answer_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1542529234207,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1539818808056,
        "Question_original_content":"aw nocredenti train attempt run exampl code local gpu copi code jupyt notebook follow python script import boto import subprocess import mxnet import mxnet mxnet import gluon import execut role import session session instanc type local subprocess nvidia smi set type gpu present instanc type local gpu role execut role gluon data vision mnist data train train true gluon data vision mnist data test train fals successfulli connect upload data input session upload data path data kei prefix data mnist hyperparamet batch size epoch learn rate momentum log interv mxnet mnist role role train instanc count train instanc type instanc type framework version hyperparamet hyperparamet fail docker contain fit input predictor deploi initi instanc count instanc type instanc type delet endpoint referenc mnist file exactli specifi github script fail fit docker contain follow error algo duu download mxnet sourc sourcedir tar tmp script tar algo duu error contain support train uncaught except train unabl locat credenti algo duu traceback recent algo duu file usr local lib python dist packag contain support train line start algo duu train algo duu file usr local lib python dist packag mxnet contain train line train algo duu mxnet env download user modul algo duu file usr local lib python dist packag contain support environ line download user modul algo duu download resourc self user script archiv tmp algo duu file usr local lib python dist packag contain support util line download resourc algo duu script bucket download file script kei target algo duu file usr local lib python dist packag boto inject line bucket download file algo duu extraarg extraarg callback callback config config algo duu file usr local lib python dist packag boto inject line download file algo duu extra arg extraarg callback callback algo duu file usr local lib python dist packag boto transfer line download file algo duu futur result algo duu file usr local lib python dist packag stransfer futur line result algo duu return self coordin result algo duu file usr local lib python dist packag stransfer futur line result algo duu rais self except algo duu nocredentialserror unabl locat credenti confus authent outsid contain pload train test data docker contain guess issu pass aw credenti docker contain gener docker compos file network local local servic algo duu command train environ aw region west train job mxnet imag dkr ecr west amazonaw com mxnet gpu network local alias algo duu stdin open true tty true volum tmp tmpskarx algo duu input opt input tmp tmpskarx algo duu output opt output tmp tmpskarx algo duu output data opt output data tmp tmpskarx model opt model version aw credenti pass enviroment variabl upgrad instal read boto instal local mode effect check credenti fetch session outsid contain appear blank aw config aw credenti file token time fetcher access kei frozen credenti refresh secret kei expiri time method assum role refresh lock new aw know diagnos issu aw credenti aw config file follow inform placehold valu default output json region west role arn arn aw iam role sourc profil test profil test output json region west test profil amazonfullaccess iam manag consol aw credenti file follow inform repres placehold valu default aw access kei aw secret access kei zyxwvutsrqponmlkjihgfedcba test aw access kei aw secret access kei abcdefghijklmopqrstuvwxyz lastli version applic librari pip freez awscli boto boto botocor docker docker compos mxnet post let know left relev inform thank help feedback provid updat thank help attempt suggest fix notic boto date updat boto botocor appear resolv issu abl document issu boto help understand issu boto happi mark answer correct",
        "Question_preprocessed_content":"aw nocredenti train attempt run exampl code local gpu copi code jupyt notebook follow python script referenc file exactli specifi github script fail docker contain follow error confus authent outsid contain docker contain guess issu pass aw credenti docker contain gener file aw credenti pass enviroment variabl upgrad instal read boto instal local mode effect check credenti fetch session appear blank file new aw know diagnos issu aw credenti file follow inform profil iam manag consol file follow inform lastli version applic librari let know left relev inform thank provid updat thank help attempt suggest fix notic date updat appear resolv issu abl document issu help understand issu happi mark answer correct",
        "Question_gpt_summary_original":"The user is encountering an error when attempting to run example code for Amazon Sagemaker on a local GPU. The error occurs during the Docker container training process and is related to the inability to locate AWS credentials. The user has checked their AWS credentials and found that they appear to be blank, despite having a .aws\/config and .aws\/credentials file. The user is unsure how to diagnose the issue and is seeking help to understand the problem with their boto3 library.",
        "Question_gpt_summary":"user encount error attempt run exampl code local gpu error occur docker contain train process relat inabl locat aw credenti user check aw credenti appear blank despit have aw config aw credenti file user unsur diagnos issu seek help understand problem boto librari",
        "Answer_original_content":"local mode design pick credenti avail boto session pass docker contain environ variabl version sdk earlier ignor credenti includ token usual indic short live credenti won remain valid long train job complet endpoint us temporari credenti try replac perman on run instanc notebook appropri instanc role assign sdk handl credenti chang later temporari credenti pass local mode contain warn messag upgrad newer version try pip instal try upgrad boto chang try latest version",
        "Answer_preprocessed_content":"local mode design pick credenti avail boto session pass docker contain environ variabl version sdk ignor credenti includ token usual indic credenti won remain valid long train job complet endpoint us temporari credenti try replac perman on run instanc appropri instanc role assign sdk handl credenti chang later temporari credenti pass local mode contain warn messag upgrad newer version try try upgrad chang try latest version",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge with authenticating to aws within a docker container include replacing temporary credentials with permanent ones, running from an ec2 instance or notebook with an appropriate instance role assigned, upgrading to a newer version of the sdk, and upgrading to the latest version of boto3.",
        "Answer_gpt_summary":"possibl solut user challeng authent aw docker contain includ replac temporari credenti perman on run instanc notebook appropri instanc role assign upgrad newer version sdk upgrad latest version boto"
    },
    {
        "Question_id":null,
        "Question_title":"SSH remote: unexpected error - Permission denied",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m trying to configure an SSH remote storage. When I try to push my data directory I get:<\/p>\n<pre><code class=\"lang-auto\">$  dvc push\nERROR: unexpected error - Permission denied: Permission denied                                                                    \n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n<\/code><\/pre>\n<p>I can confirm that:<\/p>\n<ul>\n<li>I can connect to the server via <code>ssh<\/code> and <code>sftp<\/code>\n<\/li>\n<li>the URL is correct<\/li>\n<li>I have write permissions in the path (already tried by manually uploading the files via <code>sftp<\/code>.<\/li>\n<\/ul>\n<p>I can\u2019t really make sense of the debug message:<\/p>\n<pre><code class=\"lang-auto\">$  dvc push -v\n2022-08-15 11:38:48,112 DEBUG: Lockfile for 'dvc.yaml' not found      \n2022-08-15 11:38:48,148 WARNING: Output 'data\/numpy'(stage: 'convert_puf_responses') is missing version info. Cache for it will not be collected. Use `dvc repro` to get your pipeline up to date.\n2022-08-15 11:38:48,273 DEBUG: Preparing to transfer data from '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/sram_startup\/.dvc\/cache' to '\/net\/archive\/rodriguez\/aging_indicators_data\/sram'\n2022-08-15 11:38:48,273 DEBUG: Preparing to collect status from '\/net\/archive\/rodriguez\/aging_indicators_data\/sram'\n2022-08-15 11:38:48,273 DEBUG: Collecting status from '\/net\/archive\/rodriguez\/aging_indicators_data\/sram'\n2022-08-15 11:38:48,274 DEBUG: Querying 1 oids via object_exists                                                                  \n2022-08-15 11:38:48,425 ERROR: unexpected error - Permission denied: Permission denied                                            \n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 91, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.10\/contextlib.py\", line 619, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/misc.py\", line 274, in __aenter__\n    self._coro_result = await self._coro\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 7707, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 408, in wait_for\n    return await fut\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 440, in _connect\n    await options.waiter\nasyncssh.misc.PermissionDenied: Permission denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/__init__.py\", line 185, in main\n    ret = cmd.do_run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 109, in push\n    return self.transfer(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 88, in transfer\n    return transfer(src_odb, dest_odb, objs, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/transfer.py\", line 158, in transfer\n    status = compare_status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 179, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 136, in status\n    exists = hashes.intersection(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 56, in _indexed_dir_hashes\n    dir_exists.update(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/tqdm\/std.py\", line 1195, in __iter__\n    for obj in iterable:\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/db.py\", line 255, in list_oids_exists\n    yield from itertools.compress(oids, in_remote)\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 609, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 446, in result\n    return self.__get_result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/base.py\", line 269, in exists\n    return self.fs.exists(path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/implementations\/ssh.py\", line 115, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/spec.py\", line 76, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 76, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 86, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 66, in sync\n    raise return_result\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 26, in _runner\n    result[0] = await coro\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 445, in wait_for\n    return fut.result()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 29, in wrapper\n    raise PermissionError(exc.reason) from exc\nPermissionError: Permission denied\n------------------------------------------------------------\n2022-08-15 11:38:48,557 DEBUG: [Errno 95] no more link types left to try out: [Errno 95] Operation not supported\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 27, in wrapper\n    return await func(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 91, in _connect\n    client = await self._stack.enter_async_context(_raw_client)\n  File \"\/usr\/lib\/python3.10\/contextlib.py\", line 619, in enter_async_context\n    result = await _cm_type.__aenter__(cm)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/misc.py\", line 274, in __aenter__\n    self._coro_result = await self._coro\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 7707, in connect\n    return await asyncio.wait_for(\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 408, in wait_for\n    return await fut\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/asyncssh\/connection.py\", line 440, in _connect\n    await options.waiter\nasyncssh.misc.PermissionDenied: Permission denied\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/__init__.py\", line 185, in main\n    ret = cmd.do_run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/cli\/command.py\", line 22, in do_run\n    return self.run()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/commands\/data_sync.py\", line 58, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/__init__.py\", line 48, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/repo\/push.py\", line 68, in push\n    pushed += self.cloud.push(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 109, in push\n    return self.transfer(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc\/data_cloud.py\", line 88, in transfer\n    return transfer(src_odb, dest_odb, objs, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/transfer.py\", line 158, in transfer\n    status = compare_status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 179, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 136, in status\n    exists = hashes.intersection(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_data\/status.py\", line 56, in _indexed_dir_hashes\n    dir_exists.update(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/tqdm\/std.py\", line 1195, in __iter__\n    for obj in iterable:\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/db.py\", line 255, in list_oids_exists\n    yield from itertools.compress(oids, in_remote)\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 609, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 446, in result\n    return self.__get_result()\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"\/usr\/lib\/python3.10\/concurrent\/futures\/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/base.py\", line 269, in exists\n    return self.fs.exists(path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 50, in __get__\n    return prop.__get__(instance, type)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/funcy\/objects.py\", line 28, in __get__\n    res = instance.__dict__[self.fget.__name__] = self.fget(instance)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/implementations\/ssh.py\", line 115, in fs\n    return _SSHFileSystem(**self.fs_args)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/spec.py\", line 76, in __call__\n    obj = super().__call__(*args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/spec.py\", line 76, in __init__\n    self._client, self._pool = self.connect(\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 86, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 66, in sync\n    raise return_result\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/fsspec\/asyn.py\", line 26, in _runner\n    result[0] = await coro\n  File \"\/usr\/lib\/python3.10\/asyncio\/tasks.py\", line 445, in wait_for\n    return fut.result()\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/sshfs\/utils.py\", line 29, in wrapper\n    raise PermissionError(exc.reason) from exc\nPermissionError: Permission denied\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 68, in _try_links\n    return _link(link, from_fs, from_path, to_fs, to_path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 28, in _link\n    func(from_path, to_path)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/base.py\", line 288, in reflink\n    return self.fs.reflink(from_info, to_info)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/implementations\/local.py\", line 157, in reflink\n    return system.reflink(path1, path2)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/system.py\", line 105, in reflink\n    _reflink_linux(source, link_name)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/system.py\", line 91, in _reflink_linux\n    fcntl.ioctl(d.fileno(), FICLONE, s.fileno())\nOSError: [Errno 95] Operation not supported\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 127, in _test_link\n    _try_links([link], from_fs, from_file, to_fs, to_file)\n  File \"\/home\/lanzieri\/.local\/lib\/python3.10\/site-packages\/dvc_objects\/fs\/generic.py\", line 76, in _try_links\n    raise OSError(\nOSError: [Errno 95] no more link types left to try out\n------------------------------------------------------------\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/.ZsCDGL8UDEPrdDmbm7D6mS.tmp'\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/.ZsCDGL8UDEPrdDmbm7D6mS.tmp'\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/.ZsCDGL8UDEPrdDmbm7D6mS.tmp'\n2022-08-15 11:38:48,558 DEBUG: Removing '\/home\/lanzieri\/phd\/aging_indicators\/aging-indicators-experiments\/sram_startup\/.dvc\/cache\/.QXMYC5qmirjwhLDdpceujW.tmp'\n2022-08-15 11:38:48,569 DEBUG: Version info for developers:\nDVC version: 2.12.0 (pip)\n---------------------------------\nPlatform: Python 3.10.5 on Linux-5.10.135-1-MANJARO-x86_64-with-glibc2.36\nSupports:\n\twebhdfs (fsspec = 2022.5.0),\n\thttp (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),\n\thttps (aiohttp = 3.8.1, aiohttp-retry = 2.5.0),\n\tssh (sshfs = 2022.6.0)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/mapper\/luks-9d8db46c-e044-463a-9dda-ad428bb54390\nCaches: local\nRemotes: ssh\nWorkspace directory: ext4 on \/dev\/mapper\/luks-9d8db46c-e044-463a-9dda-ad428bb54390\nRepo: dvc (subdir), git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-08-15 11:38:48,571 DEBUG: Analytics is enabled.\n2022-08-15 11:38:48,606 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp8enuwu2w']'\n2022-08-15 11:38:48,607 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp8enuwu2w']'\n<\/code><\/pre>\n<p>What am I missing? Any ideas?<br>\nThanks!<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1660556492786,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":500.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/ssh-remote-unexpected-error-permission-denied\/1300",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-17T09:27:32.187Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/leandrolanzieri\">@leandrolanzieri<\/a> !<\/p>\n<p>How did you set up the remote? Could you share the output of <code>dvc config -l --show-origin<\/code>?<\/p>",
                "Answer_score":11.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-17T09:46:36.224Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>,<\/p>\n<p>To set up the remote I used:<\/p>\n<pre><code class=\"lang-auto\">dvc remote add -d archive_remote ssh:\/\/rodriguez@archive.inet.haw-hamburg.de\/net\/archive\/rodriguez\/aging_indicators_data\/sram\n<\/code><\/pre>\n<p>The configuration looks like this:<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">$ dvc config -l --show-origin\n.dvc\/config\tremote.archive_remote.url=ssh:\/\/rodriguez@archive.inet.haw-hamburg.de\/net\/archive\/rodriguez\/aging_indicators_data\/sram\n.dvc\/config\tcore.remote=archive_remote\n<\/code><\/pre>",
                "Answer_score":11.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-17T10:49:36.922Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/leandrolanzieri\">@leandrolanzieri<\/a> !<\/p>\n<p>When you access the remote via <code>ssh<\/code>, do you provide any port\/password\/etc or you can access it directly with<br>\n<code>ssh rodriguez@archive.inet.haw-hamburg.de\/net\/archive\/rodriguez\/aging_indicators_data\/sram<\/code><br>\n?<\/p>\n<p>p.d. Could you also try to update to the latest version <code>pip install --upgrade 'dvc[ssh]'<\/code>?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-18T07:32:23.083Z",
                "Answer_body":"<p>Yes <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, I need to enter the passphrase for my RSA key to connect. Although of course I have to connect <code>ssh rodriguez@archive.inet.haw-hamburg.de<\/code> and then change directories. From the docs I understood that the path to the storing directory should also be part of the URL, right?<\/p>\n<p>It seems I have the latest version<\/p>",
                "Answer_score":10.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-18T07:46:42.206Z",
                "Answer_body":"<p>Got it.<br>\nCould you try to set your passphrase in the DVC config:<\/p>\n<pre><code class=\"lang-auto\">dvc remote modify --local archive_remote password YOUR_ASSPHRASE\n<\/code><\/pre>",
                "Answer_score":25.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-18T07:53:37.063Z",
                "Answer_body":"<p>Thanks! That did the trick. I see that <code>config.local<\/code> is ignored, still a bit unsettling to have the passphrase lying around there. I would\u2019ve expected to be prompted for it when attempting to connect. Anyways, thanks again <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a> !<\/p>",
                "Answer_score":10.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-18T07:55:18.731Z",
                "Answer_body":"<blockquote>\n<p>I would\u2019ve expected to be prompted for it when attempting to connect.<\/p>\n<\/blockquote>\n<p>For that, you can instead set up the <code>ask_password<\/code> option. See all options here <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#ssh\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">remote modify<\/a><\/p>",
                "Answer_score":20.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-18T07:56:35.818Z",
                "Answer_body":"<p>Awesome! I missed that part<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"ssh remot unexpect error permiss deni try configur ssh remot storag try push data directori push error unexpect error permiss deni permiss deni have troubl hit http org support happi help confirm connect server ssh sftp url correct write permiss path tri manual upload file sftp sens debug messag push debug lockfil yaml warn output data numpi stage convert puf respons miss version info cach collect us repro pipelin date debug prepar transfer data home lanzieri phd ag indic ag indic experi sram startup cach net archiv rodriguez ag indic data sram debug prepar collect statu net archiv rodriguez ag indic data sram debug collect statu net archiv rodriguez ag indic data sram debug queri oid object exist error unexpect error permiss deni permiss deni traceback recent file home lanzieri local lib python site packag sshf util line wrapper return await func arg kwarg file home lanzieri local lib python site packag sshf spec line connect client await self stack enter async context raw client file usr lib python contextlib line enter async context result await type aenter file home lanzieri local lib python site packag asyncssh misc line aenter self coro result await self coro file home lanzieri local lib python site packag asyncssh connect line connect return await asyncio wait file usr lib python asyncio task line wait return await fut file home lanzieri local lib python site packag asyncssh connect line connect await option waiter asyncssh misc permissiondeni permiss deni except direct caus follow except traceback recent file home lanzieri local lib python site packag cli init line main ret cmd run file home lanzieri local lib python site packag cli command line run return self run file home lanzieri local lib python site packag command data sync line run process file count self repo push file home lanzieri local lib python site packag repo init line wrapper return repo arg kwarg file home lanzieri local lib python site packag repo push line push push self cloud push file home lanzieri local lib python site packag data cloud line push return self transfer file home lanzieri local lib python site packag data cloud line transfer return transfer src odb dest odb obj kwarg file home lanzieri local lib python site packag data transfer line transfer statu compar statu file home lanzieri local lib python site packag data statu line compar statu dest exist dest miss statu file home lanzieri local lib python site packag data statu line statu exist hash intersect file home lanzieri local lib python site packag data statu line index dir hash dir exist updat file home lanzieri local lib python site packag tqdm std line iter obj iter file home lanzieri local lib python site packag object line list oid exist yield itertool compress oid remot file usr lib python concurr futur base line result iter yield pop result file usr lib python concurr futur base line result return self result file usr lib python concurr futur base line result rais self except file usr lib python concurr futur thread line run result self self arg self kwarg file home lanzieri local lib python site packag object base line exist return self exist path file home lanzieri local lib python site packag funci object line return prop instanc type file home lanzieri local lib python site packag funci object line re instanc dict self fget self fget instanc file home lanzieri local lib python site packag object implement ssh line return sshfilesystem self arg file home lanzieri local lib python site packag fsspec spec line obj super arg kwarg file home lanzieri local lib python site packag sshf spec line init self client self pool self connect file home lanzieri local lib python site packag fsspec asyn line wrapper return sync self loop func arg kwarg file home lanzieri local lib python site packag fsspec asyn line sync rais return result file home lanzieri local lib python site packag fsspec asyn line runner result await coro file usr lib python asyncio task line wait return fut result file home lanzieri local lib python site packag sshf util line wrapper rais permissionerror exc reason exc permissionerror permiss deni debug errno link type left try errno oper support traceback recent file home lanzieri local lib python site packag sshf util line wrapper return await func arg kwarg file home lanzieri local lib python site packag sshf spec line connect client await self stack enter async context raw client file usr lib python contextlib line enter async context result await type aenter file home lanzieri local lib python site packag asyncssh misc line aenter self coro result await self coro file home lanzieri local lib python site packag asyncssh connect line connect return await asyncio wait file usr lib python asyncio task line wait return await fut file home lanzieri local lib python site packag asyncssh connect line connect await option waiter asyncssh misc permissiondeni permiss deni except direct caus follow except traceback recent file home lanzieri local lib python site packag cli init line main ret cmd run file home lanzieri local lib python site packag cli command line run return self run file home lanzieri local lib python site packag command data sync line run process file count self repo push file home lanzieri local lib python site packag repo init line wrapper return repo arg kwarg file home lanzieri local lib python site packag repo push line push push self cloud push file home lanzieri local lib python site packag data cloud line push return self transfer file home lanzieri local lib python site packag data cloud line transfer return transfer src odb dest odb obj kwarg file home lanzieri local lib python site packag data transfer line transfer statu compar statu file home lanzieri local lib python site packag data statu line compar statu dest exist dest miss statu file home lanzieri local lib python site packag data statu line statu exist hash intersect file home lanzieri local lib python site packag data statu line index dir hash dir exist updat file home lanzieri local lib python site packag tqdm std line iter obj iter file home lanzieri local lib python site packag object line list oid exist yield itertool compress oid remot file usr lib python concurr futur base line result iter yield pop result file usr lib python concurr futur base line result return self result file usr lib python concurr futur base line result rais self except file usr lib python concurr futur thread line run result self self arg self kwarg file home lanzieri local lib python site packag object base line exist return self exist path file home lanzieri local lib python site packag funci object line return prop instanc type file home lanzieri local lib python site packag funci object line re instanc dict self fget self fget instanc file home lanzieri local lib python site packag object implement ssh line return sshfilesystem self arg file home lanzieri local lib python site packag fsspec spec line obj super arg kwarg file home lanzieri local lib python site packag sshf spec line init self client self pool self connect file home lanzieri local lib python site packag fsspec asyn line wrapper return sync self loop func arg kwarg file home lanzieri local lib python site packag fsspec asyn line sync rais return result file home lanzieri local lib python site packag fsspec asyn line runner result await coro file usr lib python asyncio task line wait return fut result file home lanzieri local lib python site packag sshf util line wrapper rais permissionerror exc reason exc permissionerror permiss deni handl except except occur traceback recent file home lanzieri local lib python site packag object gener line try link return link link path path file home lanzieri local lib python site packag object gener line link func path path file home lanzieri local lib python site packag object base line reflink return self reflink info info file home lanzieri local lib python site packag object implement local line reflink return reflink path path file home lanzieri local lib python site packag object line reflink reflink linux sourc link file home lanzieri local lib python site packag object line reflink linux fcntl ioctl fileno ficlon fileno oserror errno oper support except direct caus follow except traceback recent file home lanzieri local lib python site packag object gener line test link try link link file file file home lanzieri local lib python site packag object gener line try link rais oserror oserror errno link type left try debug remov home lanzieri phd ag indic ag indic experi zscdgludeprddmbmdm tmp debug remov home lanzieri phd ag indic ag indic experi zscdgludeprddmbmdm tmp debug remov home lanzieri phd ag indic ag indic experi zscdgludeprddmbmdm tmp debug remov home lanzieri phd ag indic ag indic experi sram startup cach qxmycqmirjwhlddpceujw tmp debug version info develop version pip platform python linux manjaro glibc support webhdf fsspec http aiohttp aiohttp retri http aiohttp aiohttp retri ssh sshf cach type hardlink symlink cach directori ext dev mapper luk ddbc dda adbb cach local remot ssh workspac directori ext dev mapper luk ddbc dda adbb repo subdir git have troubl hit http org support happi help debug analyt enabl debug try spawn daemon analyt tmp tmpenuwuw debug spawn daemon analyt tmp tmpenuwuw miss idea thank",
        "Question_preprocessed_content":"ssh remot unexpect error permiss deni try configur ssh remot storag try push data directori confirm connect server url correct write permiss path tri manual upload file sens debug messag miss idea thank",
        "Question_gpt_summary_original":"the user is encountering an unexpected error when attempting to configure an ssh remote storage, and is seeking assistance to resolve the issue.",
        "Question_gpt_summary":"user encount unexpect error attempt configur ssh remot storag seek assist resolv issu",
        "Answer_original_content":"leandrolanzieri set remot share output config origin daavoo set remot remot add archiv remot ssh rodriguez archiv inet haw hamburg net archiv rodriguez ag indic data sram configur look like config origin config remot archiv remot url ssh rodriguez archiv inet haw hamburg net archiv rodriguez ag indic data sram config core remot archiv remot thank leandrolanzieri access remot ssh provid port password access directli ssh rodriguez archiv inet haw hamburg net archiv rodriguez ag indic data sram try updat latest version pip instal upgrad ssh ye daavoo need enter passphras rsa kei connect cours connect ssh rodriguez archiv inet haw hamburg chang directori doc understood path store directori url right latest version got try set passphras config remot modifi local archiv remot password assphras thank trick config local ignor bit unsettl passphras ly wouldv expect prompt attempt connect anywai thank daavoo wouldv expect prompt attempt connect instead set ask password option option remot modifi awesom miss",
        "Answer_preprocessed_content":"set remot share output set remot configur look like thank access remot provid access directli try updat latest version ye need enter passphras rsa kei connect cours connect chang directori doc understood path store directori url right latest version got try set passphras config thank trick ignor bit unsettl passphras ly wouldv expect prompt attempt connect anywai thank wouldv expect prompt attempt connect instead set option option remot modifi awesom miss",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- share the output of `config -l --show-origin` to get assistance in setting up the remote.\n- update to the latest version of pip install --upgrade '[ssh]'.\n- try setting the passphrase in the config using `remote modify --local archive_remote password your_assphrase`.\n- set up the `ask_password` option to be prompted for the passphrase when attempting to connect.",
        "Answer_gpt_summary":"possibl solut extract answer share output config origin assist set remot updat latest version pip instal upgrad ssh try set passphras config remot modifi local archiv remot password assphras set ask password option prompt passphras attempt connect"
    },
    {
        "Question_id":null,
        "Question_title":"Web service REST Type POST in Azure Machine learning from data factory",
        "Question_body":"I try to use en the pipe line the web activity i cant to vinculate the dataset, but only with de body its succesfull de calling\n\n\n\n\n{\n\"name\": \"pipeline1\",\n\"properties\": {\n\"activities\": [\n{\n\"name\": \"Web1\",\n\"type\": \"WebActivity\",\n\"dependsOn\": [],\n\"policy\": {\n\"timeout\": \"7.00:00:00\",\n\"retry\": 0,\n\"retryIntervalInSeconds\": 30,\n\"secureOutput\": false,\n\"secureInput\": false\n},\n\"userProperties\": [],\n\"typeProperties\": {\n\"url\": \"http:\/\/80a36f92-5b73-4852-8e72-247379fa0bd6.westeurope.azurecontainer.io\/score\",\n\"method\": \"POST\",\n\"body\": {\n\"Inputs\": {\n\"data\": [\n{\n\"Product ID\": \"410\",\n\"Week\": 24,\n\"Year\": 2022,\n\"Customer ID\": \"3959\",\n\"Score\": 12\n}\n]\n},\n\"GlobalParameters\": 1\n},\n\"datasets\": [\n{\n\"referenceName\": \"Json2\",\n\"type\": \"DatasetReference\"\n}\n]\n}\n}\n],\n\"annotations\": [],\n\"lastPublishTime\": \"2022-05-20T11:40:24Z\"\n}\n}\n\n\n\n\nAnswer: Error Code 2108. user configuration issue. run () got an unexpected keyword argument 'datasets', i use postman and everything ist ok\n\n\n\n\nin other way, in the dataflow i use the external call bur the error its, connection failed. Error Code DFExecutorUserError. Some(list index out of range), Status code: 502. Please check your request url and body.\n\nWhen i probe the conection in the edit linked service, the test conection its ok. but in the property linked service the conection ist failed\n\nI consume this web service without any problem in power bi.\n\n\n\n\n{\n\"name\": \"dataflow_RFM\",\n\"properties\": {\n\"type\": \"MappingDataFlow\",\n\"typeProperties\": {\n\"sources\": [\n{\n\"dataset\": {\n\"referenceName\": \"Salmonsurdb\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"TransWeekCPto\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"Salmonsurdb\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"Salmonsurdb\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"PredictData\"\n}\n],\n\"sinks\": [\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM11\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"SSTransRFM\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM15\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"BStorageSSFRQ\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM14\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM13\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"BStorageSSFRQ\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM12\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFQ10\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM9\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM8\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"BStorageSSFRQ\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM7\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"BStorageSSFRQ\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM6\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM5\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"BStorageSSFRQ\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM4\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"RFM3\"\n},\n{\n\"linkedService\": {\n\"referenceName\": \"AzureBlobStorage1\",\n\"type\": \"LinkedServiceReference\"\n},\n\"name\": \"sink1\"\n},\n{\n\"dataset\": {\n\"referenceName\": \"SSTransRFM\",\n\"type\": \"DatasetReference\"\n},\n\"name\": \"sink2\"\n}\n],\n\"transformations\": [\n{\n\"name\": \"externalCall1\",\n\"linkedService\": {\n\"referenceName\": \"RestService1\",\n\"type\": \"LinkedServiceReference\"\n}\n},\n{\n\"name\": \"join1\"\n},\n{\n\"name\": \"RemoveColumns11\",\n\"description\": \"Generado autom\u00e1ticamente por acciones de vista previa de datos\"\n},\n{\n\"name\": \"Cluster14\"\n},\n{\n\"name\": \"Cluster15\"\n},\n{\n\"name\": \"Cluster11\"\n},\n{\n\"name\": \"Cluster13\"\n},\n{\n\"name\": \"Cluster12\"\n},\n{\n\"name\": \"Cluster10\"\n},\n{\n\"name\": \"Cluster9\"\n},\n{\n\"name\": \"Cluster8\"\n},\n{\n\"name\": \"Cluster7\"\n},\n{\n\"name\": \"Cluster6\"\n},\n{\n\"name\": \"Cluster5\"\n},\n{\n\"name\": \"Cluster4\"\n},\n{\n\"name\": \"Cluster3\"\n},\n{\n\"name\": \"filter1\"\n},\n{\n\"name\": \"join2\"\n},\n{\n\"name\": \"RemoveColumns12\",\n\"description\": \"Autogenerated by data preview actions\"\n},\n{\n\"name\": \"filter2\"\n}\n],\n\"scriptLines\": [\n\"source(output(\",\n\" {Customer ID} as string,\",\n\" {Product ID} as string,\",\n\" Week as integer,\",\n\" Year as integer,\",\n\" Quantity as decimal(19,4)\",\n\" ),\",\n\" allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" isolationLevel: 'READ_UNCOMMITTED',\",\n\" query: 'SELECT --p.[document no_] AS \\\\'Sales ID\\\\',\\\\n --p.[entry type],\\\\n p.[source no_] AS \\\\'Customer ID\\\\',\\\\n p.[item no_] AS \\\\'Product ID\\\\',\\\\n --p.[document type] AS \\\\'Type\\\\',\\\\n --p.[posting date] AS \\\\'Transaction Date\\\\',\\\\n Datepart(wk,p.[posting date] ) AS \\\\'Week\\\\',\\\\n Year(p.[posting date] ) AS \\\\'Year\\\\',\\\\n SUM( CONVERT(MONEY, p.[Shipped Qty_ Not Returned]))-1 AS \\\\'Quantity\\\\'\\\\n\\\\n FROM [dbo].[salmonsur,s_a_$item ledger entry$437dbf0e-84ff-417a-965d-ed2bb9650972] p\\\\n where p.[document type] = 1\\\\n GROUP BY p.[document no_],\\\\n p.[source no_],\\\\n p.[item no_],\\\\n p.[document type],\\\\n p.[posting date],\\\\n quantity\\\\n',\",\n\" format: 'query') ~> TransWeekCPto\",\n\"source(output(\",\n\" {Customer ID} as string,\",\n\" Recency as integer,\",\n\" Frecuency as integer,\",\n\" MonetaryValue as decimal(19,4),\",\n\" R as long,\",\n\" F as long,\",\n\" M as long,\",\n\" Score as long\",\n\" ),\",\n\" allowSchemaDrift: false,\",\n\" validateSchema: false,\",\n\" isolationLevel: 'READ_UNCOMMITTED',\",\n\" query: 'select RFM.[Customer ID]\\\\n --,RFM.[Product ID]\\\\n ,AVG(RFM.[Recency]) AS \\\\'Recency\\\\'\\\\n ,AVG(RFM.[Frecuency]) AS \\\\'Frecuency\\\\' \\\\n ,AVG(RFM.[Sales Amount]) As \\\\'MonetaryValue\\\\'\\\\n ,NTILE(5) OVER(ORDER BY AVG(RFM.[Recency]) DESC) As \\\\'R\\\\'\\\\n ,NTILE(5) OVER(ORDER BY AVG(RFM.[Frecuency])ASC) As \\\\'F\\\\'\\\\n ,NTILE(5) OVER(ORDER BY AVG(RFM.[Sales Amount])ASC) As \\\\'M\\\\'\\\\n ,NTILE(5) OVER(ORDER BY AVG(RFM.[Recency]) DESC) \\\\n +NTILE(5) OVER(ORDER BY AVG(RFM.[Frecuency]))\\\\n +NTILE(5) OVER(ORDER BY AVG(RFM.[Sales Amount])) as \\\\'Score\\\\'\\\\n\\\\nfrom (\\\\n-- Consulta con informaci\u00f3n de RFM \\\\n-- Recency: D\u00edas ultima compra\\\\n-- Frecuency: Promedio de Frecuencia de compra del cliente Mensual\\\\n-- SalesAmount: Promedio del valor de la factura de venta Mensual\\\\n\\\\n SELECT \\\\n p.[source no_] AS \\\\'Customer ID\\\\'\\\\n ,p.[item no_] AS \\\\'Product ID\\\\'\\\\n ,p.[document type] AS \\\\'Type\\\\'\\\\n ,year(p.[posting date]) AS \\\\'TransactionYear\\\\'\\\\n ,Month(p.[posting date]) AS \\\\'TransactionMonth\\\\'\\\\n ,max(ti.Recency) AS \\\\'Recency\\\\'\\\\n ,Count(p.[document no_]) AS \\\\'Frecuency\\\\'\\\\n ,SUM(Ve.[Sales Amount]) AS \\\\'Sales Amount\\\\' \\\\n \\\\n FROM\\\\n [dbo].[salmonsur,s_a_$item ledger entry$437dbf0e-84ff-417a-965d-ed2bb9650972] p\\\\n outer apply(\\\\n select \\\\n pp.[source no_] AS \\\\'Customer ID\\\\'\\\\n ,pp.[item no_] AS \\\\'Product ID\\\\'\\\\n ,datediff(day,max(pp.[posting date]), getdate()) AS \\\\'Recency\\\\'\\\\n From [dbo].[salmonsur,s_a_$item ledger entry$437dbf0e-84ff-417a-965d-ed2bb9650972] pp\\\\n where pp.[source no_] = p.[source no_] \\\\n and pp.[item no_] = p.[item no_]\\\\n \\\\n group by pp.[source no_] , pp.[item no_]\\\\n ) ti\\\\n \\\\n \\\\n LEFT OUTER JOIN ( SELECT [Item Ledger Entry No_]\\\\n , Iif( Sum(CONVERT(MONEY, [Sales Amount (Actual)], 0)) = 0, \\\\n Sum(CONVERT(MONEY, [Sales Amount (Expected)], 0)), \\\\n Sum(CONVERT(MONEY, [Sales Amount (Actual)], 0))) AS \\\\'Sales Amount\\\\'\\\\n ,Iif(CONVERT(MONEY, Sum([Cost Amount (Actual)]), 0) = 0,\\\\n CONVERT(MONEY, Sum([Cost Amount (Expected)]), 0),\\\\n CONVERT(MONEY, Sum([Cost Amount (Actual)]), 0)) AS \\\\'Cost amount\\\\'\\\\n ,CONVERT(MONEY, Sum([Discount Amount]), 0) AS \\\\'Discount amount\\\\'\\\\n FROM [salmonsur,s_a_$value entry$437dbf0e-84ff-417a-965d-ed2bb9650972] \\\\n GROUP BY [Item Ledger Entry No_]\\\\n ) Ve\\\\n ON p.[entry no_]= Ve.[Item Ledger Entry No_]\\\\n\\\\n \\\\n\\\\n where --p.[source no_] = \\\\'2772\\\\' \\\\n --and p.[item no_] = \\\\'215\\\\' and\\\\n p.[posting date] >= DATEADD(month,-12,GETDATE())\\\\n and p.[document type] = \\\\'1\\\\'\\\\n\\\\n GROUP BY \\\\n p.[source no_],\\\\n p.[item no_],\\\\n p.[document type],\\\\n Year(p.[posting date]),\\\\n Month(p.[posting date])\\\\n )RFM\\\\n\\\\n--Where RFM.[Customer ID] = \\\\'2772\\\\' \\\\n\\\\nGroup by\\\\n RFM.[Customer ID]\\\\n --,RFM.[Product ID]\\\\n\\\\n--Order by RFM.[Customer ID]\\\\n \\\\n',\",\n\" format: 'query',\",\n\" partitionBy('hash', 1)) ~> RFM\",\n\"source(output(\",\n\" {Customer ID} as string,\",\n\" {Product ID} as string,\",\n\" Week as integer,\",\n\" Year as integer\",\n\" ),\",\n\" allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" isolationLevel: 'READ_UNCOMMITTED',\",\n\" query: 'select A.[Customer ID] AS \\\\'Customer ID\\\\'\\\\n ,a.[Product ID] AS \\\\'Product ID\\\\'\\\\n ,Datepart(wk,B.[Transaction Date]) As \\\\'Week\\\\'\\\\n ,year(B.[Transaction Date]) As \\\\'Year\\\\'\\\\n , \\\\'1\\\\' as \\\\'Predict\\\\'\\\\n\\\\nfrom (\\\\n\\\\n SELECT p.[source no_] AS \\\\'Customer ID\\\\'\\\\n ,p.[item no_] AS \\\\'Product ID\\\\'\\\\n --convert(date, p.[posting date]) AS \\\\'Transaction Date\\\\',\\\\n --Datepart(wk,p.[posting date] ) AS \\\\'Week\\\\',\\\\n --Year(p.[posting date] ) AS \\\\'Year\\\\',\\\\n --SUM(CONVERT(MONEY, p.[Shipped Qty_ Not Returned]))-1 AS \\\\'Quantity\\\\'\\\\n\\\\n FROM [dbo].[salmonsur,s_a_$item ledger entry$437dbf0e-84ff-417a-965d-ed2bb9650972] p\\\\n where p.[document type] = 1\\\\n GROUP BY p.[source no_],\\\\n p.[item no_]\\\\n ) A, \\\\n (\\\\n select convert(date, max(p.[posting date])+8) AS \\\\'Transaction Date\\\\' FROM [dbo].[salmonsur,s_a_$item ledger entry$437dbf0e-84ff-417a-965d-ed2bb9650972] p\\\\n UNION ALL\\\\n select convert(date, max(p.[posting date])+16) AS \\\\'Transaction Date\\\\' FROM [dbo].[salmonsur,s_a_$item ledger entry$437dbf0e-84ff-417a-965d-ed2bb9650972] p\\\\n UNION ALL\\\\n select convert(date, max(p.[posting date])+24) AS \\\\'Transaction Date\\\\' FROM [dbo].[salmonsur,s_a_$item ledger entry$437dbf0e-84ff-417a-965d-ed2bb9650972] p\\\\n UNION ALL\\\\n select convert(date, max(p.[posting date])+32) AS \\\\'Transaction Date\\\\' FROM [dbo].[salmonsur,s_a_$item ledger entry$437dbf0e-84ff-417a-965d-ed2bb9650972] p\\\\n )B\\\\n',\",\n\" format: 'query') ~> PredictData\",\n\"filter2 call(output(\",\n\" headers as [string,string],\",\n\" status as string\",\n\" ),\",\n\" allowSchemaDrift: true,\",\n\" format: 'rest',\",\n\" store: 'restservice',\",\n\" timeout: 5,\",\n\" requestInterval: 5000,\",\n\" httpMethod: 'POST',\",\n\" headerColumnName: 'headers',\",\n\" statusColumnName: 'status',\",\n\" addResponseCode: true,\",\n\" requestFormat: ['type' -> 'json'],\",\n\" responseFormat: ['type' -> 'json', 'documentForm' -> 'arrayOfDocuments']) ~> externalCall1\",\n\"TransWeekCPto, RFM join(TransWeekCPto@{Customer ID} == RFM@{Customer ID},\",\n\" joinType:'right',\",\n\" broadcast: 'auto')~> join1\",\n\"join1 select(mapColumn(\",\n\" {Customer ID} = TransWeekCPto@{Customer ID},\",\n\" {Product ID},\",\n\" Week,\",\n\" Year,\",\n\" Quantity,\",\n\" Score\",\n\" ),\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true) ~> RemoveColumns11\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 14) ~> Cluster14\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 15) ~> Cluster15\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 11) ~> Cluster11\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 13) ~> Cluster13\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 12) ~> Cluster12\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 10) ~> Cluster10\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 9) ~> Cluster9\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 8) ~> Cluster8\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 7) ~> Cluster7\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 6) ~> Cluster6\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 5) ~> Cluster5\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 4) ~> Cluster4\",\n\"RemoveColumns11 filter({Customer ID}!='' && Score == 3) ~> Cluster3\",\n\"RemoveColumns11 filter({Customer ID}!='') ~> filter1\",\n\"PredictData, RFM join(PredictData@{Customer ID} == RFM@{Customer ID},\",\n\" joinType:'right',\",\n\" partitionBy('hash', 1),\",\n\" broadcast: 'auto')~> join2\",\n\"join2 select(mapColumn(\",\n\" {Product ID},\",\n\" Week,\",\n\" Year,\",\n\" {Customer ID},\",\n\" Score,\",\n\" {Customer ID} = PredictData@{Customer ID}\",\n\" ),\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true) ~> RemoveColumns12\",\n\"RemoveColumns12 filter({Customer ID}!='') ~> filter2\",\n\"Cluster11 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['RFM11.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM11\",\n\"filter1 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['TransRFM.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> SSTransRFM\",\n\"Cluster15 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['RFM15.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM15\",\n\"Cluster14 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" Column_1 as string\",\n\" ),\",\n\" partitionFileNames:['RFM14.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM14\",\n\"Cluster13 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['RFM13.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM13\",\n\"Cluster12 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" Column_1 as string\",\n\" ),\",\n\" partitionFileNames:['RFM12.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM12\",\n\"Cluster10 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['RFM10.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFQ10\",\n\"Cluster9 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['RFM9.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM9\",\n\"Cluster8 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['RFM8.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM8\",\n\"Cluster7 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" Column_1 as string\",\n\" ),\",\n\" partitionFileNames:['RFM7.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM7\",\n\"Cluster6 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" Column_1 as string\",\n\" ),\",\n\" partitionFileNames:['RFM6.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM6\",\n\"Cluster5 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['RFM5.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM5\",\n\"Cluster4 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" Column_1 as string\",\n\" ),\",\n\" partitionFileNames:['RFM4.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM4\",\n\"Cluster3 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['RFM3.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" header: ([\\\"Customer ID, Product ID, Week, Year, Quantity, Score\\\"]),\",\n\" partitionBy('hash', 1)) ~> RFM3\",\n\"filter2 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" format: 'json',\",\n\" container: 'azureml-blobstore-282b5a81-86c2-4495-83cf-ebc234b5549c',\",\n\" folderPath: 'RFM',\",\n\" partitionFileNames:['jsonPredict'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" partitionBy('hash', 1)) ~> sink1\",\n\"filter2 sink(allowSchemaDrift: true,\",\n\" validateSchema: false,\",\n\" input(\",\n\" {Customer ID} as string,\",\n\" { Product ID} as string,\",\n\" { Week} as string,\",\n\" { Year} as string,\",\n\" { Quantity} as string,\",\n\" { Score} as string\",\n\" ),\",\n\" partitionFileNames:['RFMPredict.csv'],\",\n\" skipDuplicateMapInputs: true,\",\n\" skipDuplicateMapOutputs: true,\",\n\" partitionBy('hash', 1)) ~> sink2\"\n]\n}\n}\n}",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1654098119170,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/873192\/web-service-rest-type-post-in-azure-machine-learni.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-03T15:38:32.127Z",
                "Answer_score":0,
                "Answer_body":"Hi @LuisVivarDQS-9277 ,\n\nThank you for posting query in Microsoft Q&A Platform.\n\nI was able to reproduce your scenario and error.\n\nTo avoid this error, kindly remove dataset selection inside web activity.\n\nPlease check below screenshots for better idea.\n![208249-image.png][1]\n\n![208217-image.png][2]\n\nPlease note, when you want to supply pass your dataset\n[1]: \/answers\/storage\/attachments\/208249-image.png\n[2]: \/answers\/storage\/attachments\/208217-image.png",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-03T15:39:56.397Z",
                "Answer_score":0,
                "Answer_body":"Hi @LuisVivarDQS-9277 ,\n\nThank you for posting query in Microsoft Q&A Platform.\n\nI was able to reproduce your scenario and error.\n\nTo avoid this error, kindly remove dataset selection inside web activity.\n\nPlease check below screenshots for better idea.\n\n\nPlease note, when you want to supply pass your datasets to be passed to your end point then only we need these dataset references. Click here to know more about web activity properties.\n\nHope this helps.\n\nPlease consider hitting Accept Answer button. Accepted answers help community as well.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":27.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"web servic rest type post data factori try us pipe line web activ vincul dataset bodi succesful call pipelin properti activ web type webact dependson polici timeout retri retryintervalinsecond secureoutput fals secureinput fals userproperti typeproperti url http fabd westeurop azurecontain score method post bodi input data product week year custom score globalparamet dataset referencenam json type datasetrefer annot lastpublishtim answer error code user configur issu run got unexpect keyword argument dataset us postman ist wai dataflow us extern bur error connect fail error code dfexecutorusererror list index rang statu code check request url bodi probe conect edit link servic test conect properti link servic conect ist fail consum web servic problem power dataflow rfm properti type mappingdataflow typeproperti sourc dataset referencenam salmonsurdb type datasetrefer transweekcpto dataset referencenam salmonsurdb type datasetrefer rfm dataset referencenam salmonsurdb type datasetrefer predictdata sink dataset referencenam sstransrfm type datasetrefer rfm dataset referencenam sstransrfm type datasetrefer sstransrfm dataset referencenam sstransrfm type datasetrefer rfm dataset referencenam bstoragessfrq type datasetrefer rfm dataset referencenam sstransrfm type datasetrefer rfm dataset referencenam bstoragessfrq type datasetrefer rfm dataset referencenam sstransrfm type datasetrefer rfq dataset referencenam sstransrfm type datasetrefer rfm dataset referencenam sstransrfm type datasetrefer rfm dataset referencenam bstoragessfrq type datasetrefer rfm dataset referencenam bstoragessfrq type datasetrefer rfm dataset referencenam sstransrfm type datasetrefer rfm dataset referencenam bstoragessfrq type datasetrefer rfm dataset referencenam sstransrfm type datasetrefer rfm linkedservic referencenam azureblobstorag type linkedservicerefer sink dataset referencenam sstransrfm type datasetrefer sink transform externalcal linkedservic referencenam restservic type linkedservicerefer join removecolumn descript generado automticament por accion vista previa dato cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster cluster filter join removecolumn descript autogener data preview action filter scriptlin sourc output custom string product string week integ year integ quantiti decim allowschemadrift true validateschema fals isolationlevel read uncommit queri select document sale entri type sourc custom item product document type type post date transact date datepart post date week year post date year sum convert monei ship qty return quantiti dbo salmonsur item ledger entri dbfe edbb document type group document sourc item document type post date quantiti format queri transweekcpto sourc output custom string recenc integ frecuenc integ monetaryvalu decim long long long score long allowschemadrift fals validateschema fals isolationlevel read uncommit queri select rfm custom rfm product avg rfm recenc recenc avg rfm frecuenc frecuenc avg rfm sale monetaryvalu ntile order avg rfm recenc desc ntile order avg rfm frecuenc asc ntile order avg rfm sale asc ntile order avg rfm recenc desc ntile order avg rfm frecuenc ntile order avg rfm sale score nfrom consulta informacin rfm recenc da ultima compra frecuenc promedio frecuencia compra del client mensual salesamount promedio del valor factura venta mensual select sourc custom item product document type type year post date transactionyear month post date transactionmonth max recenc recenc count document frecuenc sum sale sale dbo salmonsur item ledger entri dbfe edbb outer appli select sourc custom item product datediff dai max post date getdat recenc dbo salmonsur item ledger entri dbfe edbb sourc sourc item item group sourc item left outer join select item ledger entri iif sum convert monei sale actual sum convert monei sale expect sum convert monei sale actual sale iif convert monei sum cost actual convert monei sum cost expect convert monei sum cost actual cost convert monei sum discount discount salmonsur valu entri dbfe edbb group item ledger entri entri item ledger entri sourc item post date dateadd month getdat document type group sourc item document type year post date month post date rfm rfm custom ngroup rfm custom rfm product order rfm custom format queri partitionbi hash rfm sourc output custom string product string week integ year integ allowschemadrift true validateschema fals isolationlevel read uncommit queri select custom custom product product datepart transact date week year transact date year predict nfrom select sourc custom item product convert date post date transact date datepart post date week year post date year sum convert monei ship qty return quantiti dbo salmonsur item ledger entri dbfe edbb document type group sourc item select convert date max post date transact date dbo salmonsur item ledger entri dbfe edbb union select convert date max post date transact date dbo salmonsur item ledger entri dbfe edbb union select convert date max post date transact date dbo salmonsur item ledger entri dbfe edbb union select convert date max post date transact date dbo salmonsur item ledger entri dbfe edbb format queri predictdata filter output header string string statu string allowschemadrift true format rest store restservic timeout requestinterv httpmethod post headercolumnnam header statuscolumnnam statu addresponsecod true requestformat type json responseformat type json documentform arrayofdocu externalcal transweekcpto rfm join transweekcpto custom rfm custom jointyp right broadcast auto join join select mapcolumn custom transweekcpto custom product week year quantiti score skipduplicatemapinput true skipduplicatemapoutput true removecolumn removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom score cluster removecolumn filter custom filter predictdata rfm join predictdata custom rfm custom jointyp right partitionbi hash broadcast auto join join select mapcolumn product week year custom score custom predictdata custom skipduplicatemapinput true skipduplicatemapoutput true removecolumn removecolumn filter custom filter cluster sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm filter sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam transrfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash sstransrfm cluster sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input column string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input column string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfq cluster sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input column string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input column string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input column string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm cluster sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam rfm csv skipduplicatemapinput true skipduplicatemapoutput true header custom product week year quantiti score partitionbi hash rfm filter sink allowschemadrift true validateschema fals format json contain blobstor ebcbc folderpath rfm partitionfilenam jsonpredict skipduplicatemapinput true skipduplicatemapoutput true partitionbi hash sink filter sink allowschemadrift true validateschema fals input custom string product string week string year string quantiti string score string partitionfilenam rfmpredict csv skipduplicatemapinput true skipduplicatemapoutput true partitionbi hash sink",
        "Question_preprocessed_content":"web servic rest type post data factori try us pipe line web activ vincul dataset bodi succesful call pipelin properti userproperti typeproperti globalparamet dataset annot lastpublishtim answer error code user configur issu run got unexpect keyword argument dataset us postman ist wai dataflow us extern bur error connect fail error code dfexecutorusererror statu code check request url bodi probe conect edit link servic test conect properti link servic conect ist fail consum web servic problem power properti transweekcpto dataset rfm dataset predictdata sink transform scriptlin type date date date sum ledger type group format queri transweekcpto sourc long long long score long allowschemadrift fals validateschema fals isolationlevel queri select ntile desc ntile asc ntile asc ntile desc ntile ntile date ledger outer select date getdat ledger group left outer join sum sum iif convert convert convert group ledger entri date type group format queri partitionbi rfm sourc allowschemadrift true validateschema fals isolationlevel queri select date date date date ledger type group ledger union select convert ledger union select convert ledger union select convert ledger format queri predictdata filter allowschemadrift true format rest store restservic timeout requestinterv httpmethod post headercolumnnam header statuscolumnnam statu addresponsecod true requestformat responseformat externalcal transweekcpto rfm join join join select skipduplicatemapinput true skipduplicatemapoutput true removecolumn removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter cluster removecolumn filter filter predictdata rfm join broadcast auto join join select skipduplicatemapinput true skipduplicatemapoutput true removecolumn removecolumn filter filter cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm filter sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi sstransrfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfq cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm cluster sink skipduplicatemapinput true skipduplicatemapoutput true header partitionbi rfm filter sink sink filter sink skipduplicatemapinput true skipduplicatemapoutput true partitionbi sink",
        "Question_gpt_summary_original":"the user encountered multiple challenges while attempting to use a web service rest type post in from data factory, including connection errors, configuration issues, and unexpected keyword arguments.",
        "Question_gpt_summary":"user encount multipl challeng attempt us web servic rest type post data factori includ connect error configur issu unexpect keyword argument",
        "Answer_original_content":"luisvivardq thank post queri microsoft platform abl reproduc scenario error avoid error kindli remov dataset select insid web activ check screenshot better idea imag png imag png note want suppli pass dataset answer storag attach imag png answer storag attach imag png luisvivardq thank post queri microsoft platform abl reproduc scenario error avoid error kindli remov dataset select insid web activ check screenshot better idea note want suppli pass dataset pass end point need dataset refer click know web activ properti hope help consid hit accept answer button accept answer help commun",
        "Answer_preprocessed_content":"thank post queri microsoft platform abl reproduc scenario error avoid error kindli remov dataset select insid web activ check screenshot better idea note want suppli pass dataset thank post queri microsoft platform abl reproduc scenario error avoid error kindli remov dataset select insid web activ check screenshot better idea note want suppli pass dataset pass end point need dataset refer click know web activ properti hope help consid hit accept answer button accept answer help commun",
        "Answer_gpt_summary_original":"the solution to avoid connection errors, configuration issues, and unexpected keyword arguments while using a web service rest type post in from data factory is to remove dataset selection inside web activity. the answer provides screenshots for better understanding and suggests passing datasets to the endpoint only when needed. the user is also directed to click a link to know more about web activity properties.",
        "Answer_gpt_summary":"solut avoid connect error configur issu unexpect keyword argument web servic rest type post data factori remov dataset select insid web activ answer provid screenshot better understand suggest pass dataset endpoint need user direct click link know web activ properti"
    },
    {
        "Question_id":null,
        "Question_title":"Code running slow on Sagemaker notebook instance for the first time it runs",
        "Question_body":"Hello!\n\nI've an issue running the code on SageMaker. I am running my code on SageMaker, which runs my code slowly for the first time, but runs with proper speed, the second time around (I guess there's something getting stored in the cache). Few days back, it was running with the same speed all the time. Whatever I run, be it a model (The model which took just 5 minutes for one epoch when it worked fine estimates 3 hours of running time) \/ just a code reading the data present in my files, it runs too slow. What could be a possible solution for this? I tried changing the notebook instance types as well, but in vain. I've been struggling for 2 days. It'll be great if someone could help me out a bit soon so that I progress ahead in my project. Thanks in advance!\n\nEdited by: vbsrinivasan on Jun 10, 2020 10:47 AM",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1591811233000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":615.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUeQJN4BFCTsikAct_m9BeZw\/code-running-slow-on-sagemaker-notebook-instance-for-the-first-time-it-runs",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-26T10:12:46.000Z",
                "Answer_score":0,
                "Answer_body":"Can confirm the speed issues. Migrated yesterday to Sagemaker and code runs very slowly the first time, the second time is way faster but is slowing down again mid training. With the same code and training data, training on a K80 is way slower than on Colabs K80.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-18T10:07:41.000Z",
                "Answer_score":0,
                "Answer_body":"Yeah, you are right. Also, there's one update. Like, whenever I turn off the notebook instance and turn on once again, the code runs very very slowly. But, after running the code once, if I don't turn off the notebook, it runs faster. But, this incurs a lot of cost for me. It would be great if someone from AWS or an expert responds to this!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-13T23:33:54.000Z",
                "Answer_score":0,
                "Answer_body":"Can you confirm if this is the notebook taking time to spin up the kernel, then load the libraries (at the start of your script presumably) or if this is all cells taking longer to run?\n\nEdited by: MikeChambers on Jun 13, 2020 4:20 AM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-08-18T04:56:07.000Z",
                "Answer_score":0,
                "Answer_body":"Hello!\n\nThe notebook is taking time to run every cell. Not just libraries and stuff.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-11T11:25:54.000Z",
                "Answer_score":0,
                "Answer_body":"Exact same experience here. Seeing substantial variation in runtimes between instance restarts. Running the exact same code can take up to a factor of 3 longer (regardless of whether this is just I\/O, model training or something else entirely). I had originally attributed this to slow EBS I\/O (which by experience has been patchy in the past) but doesn't seem to be related. Real showstopper for sagemaker at this point.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-13T11:20:32.000Z",
                "Answer_score":0,
                "Answer_body":"Having the same problem too. But for me it is mainly disk I\/O. So every time I stop and restart the notebook instance, I need to re-download the data even though they are sitting right there on disk, because if I don't, then it takes a insane amount of time to load the data (even slower than re-download the data and load them). Quite annoying but have not idea how to fix it.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-06-12T17:25:56.000Z",
                "Answer_score":0,
                "Answer_body":"Yeah, I've narrowed it down to disk I\/O. Extremely slow on first read -- as if the files aren't on the EBS volume but downloaded from elsewhere. Moving away from Sagemaker NBs now for interactive work",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"code run slow notebook instanc time run hello issu run code run code run code slowli time run proper speed second time guess get store cach dai run speed time run model model took minut epoch work fine estim hour run time code read data present file run slow possibl solut tri chang notebook instanc type vain struggl dai great help bit soon progress ahead project thank advanc edit vbsrinivasan jun",
        "Question_preprocessed_content":"code run slow notebook instanc time run hello issu run code run code run code slowli time run proper speed second time dai run speed time run model code read data present file run slow possibl solut tri chang notebook instanc type vain struggl dai great help bit soon progress ahead project thank advanc edit vbsrinivasan jun",
        "Question_gpt_summary_original":"The user is experiencing slow code running on their Sagemaker notebook instance, which runs slowly for the first time but runs with proper speed the second time around. The user has tried changing the notebook instance types but it did not help. The user is seeking a possible solution to this issue as it is causing a delay in their project progress.",
        "Question_gpt_summary":"user experienc slow code run notebook instanc run slowli time run proper speed second time user tri chang notebook instanc type help user seek possibl solut issu caus delai project progress",
        "Answer_original_content":"confirm speed issu migrat yesterdai code run slowli time second time wai faster slow mid train code train data train wai slower colab yeah right updat like turn notebook instanc turn code run slowli run code turn notebook run faster incur lot cost great aw expert respond confirm notebook take time spin kernel load librari start script presum cell take longer run edit mikechamb jun hello notebook take time run cell librari stuff exact experi see substanti variat runtim instanc restart run exact code factor longer regardless model train entir origin attribut slow eb experi patchi past relat real showstopp point have problem mainli disk time stop restart notebook instanc need download data sit right disk take insan time load data slower download data load annoi idea fix yeah narrow disk extrem slow read file aren eb volum download move awai nb interact work",
        "Answer_preprocessed_content":"confirm speed issu migrat yesterdai code run slowli time second time wai faster slow mid train code train data train wai slower colab yeah right updat like turn notebook instanc turn code run slowli run code turn notebook run faster incur lot cost great aw expert respond confirm notebook take time spin kernel load librari cell take longer run edit mikechamb jun hello notebook take time run cell librari stuff exact experi see substanti variat runtim instanc restart run exact code factor longer origin attribut slow eb relat real showstopp point have problem mainli disk time stop restart notebook instanc need data sit right disk take insan time load data annoi idea fix yeah narrow disk extrem slow read file aren eb volum download move awai nb interact work",
        "Answer_gpt_summary_original":"possible solutions to the challenge of slow code running on a notebook instance include: \n- running the code once before turning off the notebook instance to avoid the slow start-up time when the notebook is turned on again. \n- using a different notebook instance type, such as colab's k80, which may run faster. \n- checking if the slow performance is due to slow ebs i\/o or disk i\/o, and finding ways to optimize these processes. \n- seeking help from aws or an expert to address the issue.",
        "Answer_gpt_summary":"possibl solut challeng slow code run notebook instanc includ run code turn notebook instanc avoid slow start time notebook turn differ notebook instanc type colab run faster check slow perform slow eb disk find wai optim process seek help aw expert address issu"
    },
    {
        "Question_id":68532457.0,
        "Question_title":"How to create a Logs Router Sink when a Vertex AI training job failed (after 3 attempts)?",
        "Question_body":"<p>I am running a <code>Vertex AI custom training job<\/code> (machine learnin training using custom container) on <code>GCP<\/code>. I would like to create a <code>Pub\/Sub<\/code> message when the job failed so I can post a message on some chat like Slack. Logfile (<code>Cloud Logging)<\/code> is looking like that:<\/p>\n<pre><code>{\ninsertId: &quot;xxxxx&quot;\nlabels: {\nml.googleapis.com\/endpoint: &quot;&quot;\nml.googleapis.com\/job_state: &quot;FAILED&quot;\n}\nlogName: &quot;projects\/xxx\/logs\/ml.googleapis.com%2F1113875647681265664&quot;\nreceiveTimestamp: &quot;2021-07-09T15:05:52.702295640Z&quot;\nresource: {\nlabels: {\njob_id: &quot;1113875647681265664&quot;\nproject_id: &quot;xxx&quot;\ntask_name: &quot;service&quot;\n}\ntype: &quot;ml_job&quot;\n}\nseverity: &quot;INFO&quot;\ntextPayload: &quot;Job failed.&quot;\ntimestamp: &quot;2021-07-09T15:05:52.187968162Z&quot;\n}\n<\/code><\/pre>\n<p>I am creating a Logs Router Sink with the following query:<\/p>\n<pre><code>resource.type=&quot;ml_job&quot; AND textPayload:&quot;Job failed&quot; AND labels.&quot;ml.googleapis.com\/job_state&quot;:&quot;FAILED&quot;\n<\/code><\/pre>\n<p>The issue I am facing is that Vertex AI will retry the job 3 times before declaring the job as a failure but in the logfile the message is identical. Below you have 3 examples, only the last one that failed 3 times really failed at the end.\n<a href=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NSGPb.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>In the logfile, I don't have any count id for example. Any idea how to solve this ? Creating a BigQuery table to keep track of the number of failure per <code>resource.labels.job_id<\/code> seems to be an overkill if I need to do that in all my project. Is there a way to do a group by <code>resource.labels.job_id<\/code> and count within Logs Router Sink ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1627312838200,
        "Question_favorite_count":null,
        "Question_last_edit_time":1627313437400,
        "Question_score":0.0,
        "Question_view_count":216.0,
        "Answer_body":"<p>The log sink is quite simple: provide a filter, it will publish in a PubSub topic each entry which match this filter. No group by, no count, nothing!!<\/p>\n<p>I propose you to use a combination of log-based metrics and Cloud monitoring.<\/p>\n<ol>\n<li>Firstly, create a <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\" rel=\"nofollow noreferrer\">log based metrics<\/a> on your job failed log entry<\/li>\n<li>Create an <a href=\"https:\/\/cloud.google.com\/logging\/docs\/logs-based-metrics\/charts-and-alerts\" rel=\"nofollow noreferrer\">alert on this log based metrics<\/a> with the following key values<\/li>\n<\/ol>\n<ul>\n<li>Set the group by that you want, for example, the jobID (i don't know what is the relevant value for VertexAI job)<\/li>\n<li>Set an alert when the threshold is equal or above 3<\/li>\n<li>Add a notification channel and set a PubSub notification (still in beta)<\/li>\n<\/ul>\n<p>With this configuration, the alert will be posted only once in PubSub when 3 occurrences of the same jobID will occur.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68532457",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1627331282972,
        "Question_original_content":"creat log router sink train job fail attempt run custom train job machin learnin train custom contain gcp like creat pub sub messag job fail post messag chat like slack logfil cloud log look like insertid label googleapi com endpoint googleapi com job state fail lognam project log googleapi com receivetimestamp resourc label job project task servic type job sever info textpayload job fail timestamp creat log router sink follow queri resourc type job textpayload job fail label googleapi com job state fail issu face retri job time declar job failur logfil messag ident exampl fail time fail end logfil count exampl idea solv creat bigqueri tabl track number failur resourc label job overkil need project wai group resourc label job count log router sink",
        "Question_preprocessed_content":"creat log router sink train job fail run like creat messag job fail post messag chat like slack logfil look like creat log router sink follow queri issu face retri job time declar job failur logfil messag ident exampl fail time fail end logfil count exampl idea solv creat bigqueri tabl track number failur overkil need project wai group count log router sink",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a Logs Router Sink to generate a Pub\/Sub message when a Vertex AI custom training job fails on GCP. The issue is that Vertex AI retries the job three times before declaring it as a failure, but the log message is identical. The user is looking for a solution to count the number of failures per resource.labels.job_id without creating a BigQuery table for each project.",
        "Question_gpt_summary":"user face challeng creat log router sink gener pub sub messag custom train job fail gcp issu retri job time declar failur log messag ident user look solut count number failur resourc label job creat bigqueri tabl project",
        "Answer_original_content":"log sink simpl provid filter publish pubsub topic entri match filter group count propos us combin log base metric cloud monitor firstli creat log base metric job fail log entri creat alert log base metric follow kei valu set group want exampl jobid know relev valu vertexai job set alert threshold equal add notif channel set pubsub notif beta configur alert post pubsub occurr jobid occur",
        "Answer_preprocessed_content":"log sink simpl provid filter publish pubsub topic entri match filter group count propos us combin metric cloud monitor firstli creat log base metric job fail log entri creat alert log base metric follow kei valu set group want exampl jobid set alert threshold equal add notif channel set pubsub notif configur alert post pubsub occurr jobid occur",
        "Answer_gpt_summary_original":"the solution proposed is to use log-based metrics and cloud monitoring to create an alert when a custom training job on gcp fails after 3 attempts. the user can create a log-based metric on the job failed log entry and set an alert with the relevant group by value, such as jobid. when the threshold is equal or above 3, the alert will be posted only once in pubsub.",
        "Answer_gpt_summary":"solut propos us log base metric cloud monitor creat alert custom train job gcp fail attempt user creat log base metric job fail log entri set alert relev group valu jobid threshold equal alert post pubsub"
    },
    {
        "Question_id":null,
        "Question_title":"Add_reference() with nested folders",
        "Question_body":"<p>We need to set a dataset folder in S3 as an artifact.  The folder has many sub-directories (only one layer though).<br>\nWhen I use the a <code>add_reference()<\/code> command it only stores the directory names of the top-level.<br>\nOf course, I could loop across it, but I\u2019m wondering if there is a command option to make the operation recursive?<\/p>\n<pre><code class=\"lang-auto\">run  = wandb.init(project=WB_PROJECT)\nart = wandb.Artifact(WB_ENTITY, type=WB_DATASET)\nart.add_reference(s3_full, max_objects=WB_MAX_OBJECTS_TO_UPLOAD)\nrun.log_artifact(art)\nwandb.finish()\n<\/code><\/pre>\n<p>EDIT 1: I conclude that the all files are not being added because the <code>Num Files<\/code> in the Artifact Overview shows only <code>5<\/code>.  If I click on the directories, it seems I can see the files, but I assume they are not actually there because of the <code>5<\/code> being reported for the number of files.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1662827790113,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":370.0,
        "Answer_body":"<p>Hi Kevin,<\/p>\n<p>Thanks for the detailed explanation! I see your issue, I will create a request for this feature, thanks for reporting it! May I help you with any other issue?<\/p>\n<p>Best,<br>\nLuis<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/add-reference-with-nested-folders\/3092",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-14T13:35:14.013Z",
                "Answer_body":"<p>Hi Kevin,<\/p>\n<p>Thanks for your question! I think that the only thing that may work for you would be adding an S3 prefix without an explicit name (documentation here), the other way could be using a loop. Please let me know if this would be helpful<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-19T04:29:06.402Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/system\">@system<\/a> (luis) Thank you for the reply.  I think the core question is: why does \u201cNum Files\u201d (In the Artifact view) incorrectly list only the top-level files?  I may have 200K files in the artifact, but the \u201cNum Files\u201d only says \u201c5\u201d.  See example below.  Perhaps this could be fixed?  It is somewhat distressing for this parameter to be so wrong.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e.png\" data-download-href=\"\/uploads\/short-url\/pLJ7xdYJeVEm5H2IwETfNri6E2O.png?dl=1\" title=\"Artifact\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_690x150.png\" alt=\"Artifact\" data-base62-sha1=\"pLJ7xdYJeVEm5H2IwETfNri6E2O\" width=\"690\" height=\"150\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_690x150.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_1035x225.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_1380x300.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Artifact<\/span><span class=\"informations\">2228\u00d7486 49.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-21T11:32:02.548Z",
                "Answer_body":"<p>Hi Kevin,<\/p>\n<p>Thanks for the detailed explanation! I see your issue, I will create a request for this feature, thanks for reporting it! May I help you with any other issue?<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-11-18T04:29:28.422Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1663759922548,
        "Question_original_content":"add refer nest folder need set dataset folder artifact folder sub directori layer us add refer command store directori name level cours loop wonder command option oper recurs run init project project art artifact entiti type dataset art add refer max object max object upload run log artifact art finish edit conclud file ad num file artifact overview show click directori file assum actual report number file",
        "Question_preprocessed_content":"nest folder need set dataset folder artifact folder us command store directori name cours loop wonder command option oper recurs edit conclud file ad artifact overview show click directori file assum actual report number file",
        "Question_gpt_summary_original":"The user is facing a challenge while using the `add_reference()` command to store a dataset folder in S3 as an artifact. The folder has many sub-directories, but the command only stores the directory names of the top-level. The user is wondering if there is a command option to make the operation recursive. The user also noticed that not all files are being added to the artifact, as the \"Num Files\" in the Artifact Overview shows only 5, despite being able to see the files when clicking on the directories.",
        "Question_gpt_summary":"user face challeng add refer command store dataset folder artifact folder sub directori command store directori name level user wonder command option oper recurs user notic file ad artifact num file artifact overview show despit abl file click directori",
        "Answer_original_content":"kevin thank detail explan issu creat request featur thank report help issu best lui",
        "Answer_preprocessed_content":"kevin thank detail explan issu creat request featur thank report help issu best lui",
        "Answer_gpt_summary_original":"there are no specific solutions mentioned in the answer. however, the responder acknowledges the issue and promises to create a request for the feature.",
        "Answer_gpt_summary":"specif solut mention answer respond acknowledg issu promis creat request featur"
    },
    {
        "Question_id":null,
        "Question_title":"Online deployment startup failed",
        "Question_body":"Online deployment (locally) startup failed with this error message (full log is at the end):\n\nModuleNotFoundError: No module named 'sklearn'\n\n\n\n\nThe import is from this line of the train.py python script:\n\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nBy running a local Docker container manually, this above import statement works if I activate the inf-conda-env conda environment, where scikit-learn>=1.0.2 as specified in my environment ile is installed.\n\nLooking into the full logs, it seems that the container is using the amlenv conda environment instead when this error happends. I could not change the actual environment where scikit-learn is installed by changing the name within both the environment YML file and the deployment YML file. This dependent package is always installed in inf-conda-env and the inference server is always run in amlenv.\n\nenv_azure.yml:\n\nname: myenv\ndependencies:\n  - python=3.7\n  - pip:\n    - scikit-learn>=1.0.2\n\n\n\n\nblue-deployment.yml:\n\n$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: george-mlops-endpoint\nmodel: azureml:stock-pred-lstm:2\ncode_configuration:\n  code: ..\/\n  scoring_script: score_azure.py\nenvironment:\n  name: myenv\n  conda_file: ..\/env_azure.yml\n  image: mcr.microsoft.com\/azureml\/tensorflow-2.4-ubuntu18.04-py37-cpu-inference:latest\ninstance_type: Standard_D2as_v4\ninstance_count: 1\n\n\n\n\n\n\nfull log:\n\n$ az ml online-deployment get-logs -n blue -e $ENDPOINT_NAME_2 --local\n2022-06-14T08:54:05,693704187+00:00 - gunicorn\/run \n2022-06-14T08:54:05,695924060+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,698181633+00:00 | gunicorn\/run | ###############################################\n2022-06-14T08:54:05,701762750+00:00 | gunicorn\/run | AzureML Container Runtime Information\n2022-06-14T08:54:05,702955089+00:00 | gunicorn\/run | ###############################################\n2022-06-14T08:54:05,704188929+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,705658377+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,707062223+00:00 - rsyslog\/run \n2022-06-14T08:54:05,710995351+00:00 - nginx\/run \n2022-06-14T08:54:05,714649671+00:00 | gunicorn\/run | AzureML image information: tensorflow-2.4-ubuntu18.04-py37-cpu-inference:20220516.v10\nnginx: [warn] the \"user\" directive makes sense only if the master process runs with super-user privileges, ignored in \/etc\/nginx\/nginx.conf:1\n2022-06-14T08:54:05,721274687+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,722691033+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,723795369+00:00 | gunicorn\/run | PATH environment variable: \/opt\/miniconda\/envs\/inf-conda-env\/bin:\/opt\/miniconda\/condabin:\/opt\/miniconda\/envs\/amlenv\/bin:\/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\n2022-06-14T08:54:05,724812902+00:00 | gunicorn\/run | PYTHONPATH environment variable: \n2022-06-14T08:54:05,726167546+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,727421387+00:00 | gunicorn\/run | Pip Dependencies (before dynamic installation)\ncertifi==2022.5.18.1\njoblib==1.1.0\nnumpy==1.21.6\nscikit-learn==1.0.2\nscipy==1.7.3\nthreadpoolctl==3.1.0\n2022-06-14T08:54:05,972350679+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,973666822+00:00 | gunicorn\/run | Entry script directory: \/var\/azureml-app\/stock-pred\/\/.\n2022-06-14T08:54:05,974753257+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,975907695+00:00 | gunicorn\/run | ###############################################\n2022-06-14T08:54:05,977260339+00:00 | gunicorn\/run | Dynamic Python Package Installation\n2022-06-14T08:54:05,978331974+00:00 | gunicorn\/run | ###############################################\n2022-06-14T08:54:05,979721920+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,981003161+00:00 | gunicorn\/run | Dynamic Python package installation is disabled.\n2022-06-14T08:54:05,982319204+00:00 | gunicorn\/run | \n2022-06-14T08:54:05,983685849+00:00 | gunicorn\/run | ###############################################\n2022-06-14T08:54:05,984905389+00:00 | gunicorn\/run | AzureML Inference Server\n2022-06-14T08:54:05,986270433+00:00 | gunicorn\/run | ###############################################\n2022-06-14T08:54:05,987434371+00:00 | gunicorn\/run | \n2022-06-14T08:54:06,001278423+00:00 | gunicorn\/run | Starting AzureML Inference Server HTTP.\nAzure ML Inferencing HTTP server v0.6.1\nServer Settings\n---------------\nEntry Script Name: score_azure.py\nModel Directory: \/var\/azureml-app\/azureml-models\/\/stock-pred-lstm\/2\nWorker Count: 1\nWorker Timeout (seconds): 300\nServer Port: 31311\nApplication Insights Enabled: false\nApplication Insights Key: None\nInferencing HTTP server version: azmlinfsrv\/0.6.1\nServer Routes\n---------------\nLiveness Probe: GET   127.0.0.1:31311\/\nScore:          POST  127.0.0.1:31311\/score\nStarting gunicorn 20.1.0\nListening at: http:\/\/0.0.0.0:31311 (26)\nUsing worker: sync\nBooting worker with pid: 69\nException in worker process\nTraceback (most recent call last):\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py\", line 589, in spawn_worker\n    worker.init_process()\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py\", line 134, in init_process\n    self.load_wsgi()\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py\", line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py\", line 67, in wsgi\n    self.callable = self.load()\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py\", line 58, in load\n    return self.load_wsgiapp()\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py\", line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/util.py\", line 359, in import_app\n    mod = importlib.import_module(module)\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/importlib\/__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/entry.py\", line 1, in <module>\n    import create_app\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/create_app.py\", line 24, in <module>\n    from routes import main\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/routes.py\", line 39, in <module>\n    from aml_blueprint import AMLBlueprint\n  File \"\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/aml_blueprint.py\", line 33, in <module>\n    main_module_spec.loader.exec_module(main)\n  File \"\/var\/azureml-app\/stock-pred\/score_azure.py\", line 7, in <module>\n    from train import load_data, load_model, load_scaler, extract_x_y\n  File \"\/var\/azureml-app\/stock-pred\/train.py\", line 5, in <module>\n    from sklearn.preprocessing import StandardScaler\nModuleNotFoundError: No module named 'sklearn'\nWorker exiting (pid: 69)\nShutting down: Master\nReason: Worker failed to boot.\n2022-06-14T08:54:07,134158894+00:00 - gunicorn\/finish 3 0\n2022-06-14T08:54:07,135337927+00:00 - Exit code 3 is not normal. Killing image.\nERROR conda.cli.main_run:execute(34): Subprocess for 'conda run ['runsvdir', '\/var\/runit']' command failed.  (See above for error)\n2022-06-14T08:54:07,141403097+00:00 - rsyslog\/finish 0 0\n2022-06-14T08:54:07,143002342+00:00 - Exit code 0 is not normal. Restarting rsyslog.\n2022-06-14T08:54:07,153991050+00:00 - nginx\/finish 0 0\n2022-06-14T08:54:07,156038707+00:00 - Exit code 0 is not normal. Killing image.\nrunsvdir: no process found",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655197807017,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/888458\/online-deployment-startup-failed.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-15T07:45:10.197Z",
                "Answer_score":1,
                "Answer_body":"After a few more tries today, I found the problem stemed from the use of the mcr.microsoft.com\/azureml\/tensorflow-2.4-ubuntu18.04-py37-cpu-inference image. When I replace it with a base image mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:latest and modify the related YML files, it works finally!\n\nI thought the documentation should explicitly mention that curated images are not meant for user extension.\n\nupdated blue-deployment.yml:\n\n$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: george-mlops-endpoint\nmodel: azureml:stock-pred-lstm:2\ncode_configuration:\n  code: ..\/\n  scoring_script: score_azure.py\nenvironment:\n  conda_file: ..\/env_azure.yml\n  image: mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:latest\ninstance_type: Standard_D2as_v4\ninstance_count: 1\n\n\n\n\nupdated env_azure.yml:\n\nname: model-env\nchannels:\n  - conda-forge\ndependencies:\n  # flask 1.1.* is needed by azureml inference server\n  - flask=1.1.4\n  - python=3.8\n  - pip=21.2.4\n  - scikit-learn>=1.0.2\n  - tensorflow==2.8.0\n  - pip:\n    - inference-schema[numpy-support]==1.3.0\n    - mlflow== 1.26.0\n    - azureml-mlflow==1.41.0\n    - opencensus-ext-azure==1.1.4",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"onlin deploy startup fail onlin deploy local startup fail error messag log end modulenotfounderror modul name sklearn import line train python script sklearn preprocess import standardscal run local docker contain manual import statement work activ inf conda env conda environ scikit learn specifi environ il instal look log contain amlenv conda environ instead error happend chang actual environ scikit learn instal chang environ yml file deploy yml file depend packag instal inf conda env infer server run amlenv env azur yml myenv depend python pip scikit learn blue deploy yml schema http schema azureedg net latest managedonlinedeploy schema json blue endpoint georg mlop endpoint model stock pred lstm code configur code score script score azur environ myenv conda file env azur yml imag mcr microsoft com tensorflow ubuntu cpu infer latest instanc type standard da instanc count log onlin deploy log blue endpoint local gunicorn run gunicorn run gunicorn run gunicorn run contain runtim inform gunicorn run gunicorn run gunicorn run rsyslog run nginx run gunicorn run imag inform tensorflow ubuntu cpu infer nginx warn user direct make sens master process run super user privileg ignor nginx nginx conf gunicorn run gunicorn run gunicorn run path environ variabl opt miniconda env inf conda env bin opt miniconda condabin opt miniconda env amlenv bin opt miniconda bin usr local sbin usr local bin usr sbin usr bin sbin bin gunicorn run pythonpath environ variabl gunicorn run gunicorn run pip depend dynam instal certifi joblib numpi scikit learn scipi threadpoolctl gunicorn run gunicorn run entri script directori var app stock pred gunicorn run gunicorn run gunicorn run dynam python packag instal gunicorn run gunicorn run gunicorn run dynam python packag instal disabl gunicorn run gunicorn run gunicorn run infer server gunicorn run gunicorn run gunicorn run start infer server http inferenc http server server set entri script score azur model directori var app model stock pred lstm worker count worker timeout second server port applic insight enabl fals applic insight kei inferenc http server version azmlinfsrv server rout live probe score post score start gunicorn listen http worker sync boot worker pid except worker process traceback recent file opt miniconda env amlenv lib python site packag gunicorn arbit line spawn worker worker init process file opt miniconda env amlenv lib python site packag gunicorn worker base line init process self load wsgi file opt miniconda env amlenv lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file opt miniconda env amlenv lib python site packag gunicorn app base line wsgi self callabl self load file opt miniconda env amlenv lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file opt miniconda env amlenv lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file opt miniconda env amlenv lib python site packag gunicorn util line import app mod importlib import modul modul file opt miniconda env amlenv lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file opt miniconda env amlenv lib python site packag infer server http server entri line import creat app file opt miniconda env amlenv lib python site packag infer server http server creat app line rout import main file opt miniconda env amlenv lib python site packag infer server http server rout line aml blueprint import amlblueprint file opt miniconda env amlenv lib python site packag infer server http server aml blueprint line main modul spec loader exec modul main file var app stock pred score azur line train import load data load model load scaler extract file var app stock pred train line sklearn preprocess import standardscal modulenotfounderror modul name sklearn worker exit pid shut master reason worker fail boot gunicorn finish exit code normal kill imag error conda cli main run execut subprocess conda run runsvdir var runit command fail error rsyslog finish exit code normal restart rsyslog nginx finish exit code normal kill imag runsvdir process",
        "Question_preprocessed_content":"onlin deploy startup fail onlin deploy startup fail error messag modulenotfounderror modul name sklearn import line python script import standardscal run local docker contain manual import statement work activ conda environ specifi environ il instal look log contain amlenv conda environ instead error happend chang actual environ instal chang environ yml file deploy yml file depend packag instal infer server run amlenv myenv depend pip schema blue model code environ myenv imag log blue contain runtim inform imag inform nginx user direct make sens master process run privileg ignor path environ variabl pythonpath environ variabl pip depend entri script directori dynam python packag instal dynam python packag instal disabl infer server start infer server http inferenc http server server set entri script model directori worker count worker timeout server port applic insight enabl fals applic insight kei inferenc http server version server rout live probe score post start gunicorn listen worker sync boot worker pid except worker process traceback file line file line file line file line wsgi file line load return file line return file line mod file line return packag level file line file line file line file line file line file line file line import file line rout import main file line import amlblueprint file line file line train import file line import standardscal modulenotfounderror modul name sklearn worker exit shut master reason worker fail boot exit code normal kill imag error subprocess conda run command fail exit code normal restart rsyslog exit code normal kill imag runsvdir process",
        "Question_gpt_summary_original":"The user encountered an error message \"ModuleNotFoundError: No module named 'sklearn'\" while trying to deploy a model online. The error occurred because the inference server was running in a different environment than the one where scikit-learn was installed. The user tried to change the environment in both the environment YML file and the deployment YML file, but the dependent package was always installed in inf-conda-env and the inference server was always run in amlenv.",
        "Question_gpt_summary":"user encount error messag modulenotfounderror modul name sklearn try deploi model onlin error occur infer server run differ environ scikit learn instal user tri chang environ environ yml file deploy yml file depend packag instal inf conda env infer server run amlenv",
        "Answer_original_content":"tri todai problem steme us mcr microsoft com tensorflow ubuntu cpu infer imag replac base imag mcr microsoft com openmpi ubuntu latest modifi relat yml file work final thought document explicitli mention curat imag meant user extens updat blue deploy yml schema http schema azureedg net latest managedonlinedeploy schema json blue endpoint georg mlop endpoint model stock pred lstm code configur code score script score azur environ conda file env azur yml imag mcr microsoft com openmpi ubuntu latest instanc type standard da instanc count updat env azur yml model env channel conda forg depend flask need infer server flask python pip scikit learn tensorflow pip infer schema numpi support opencensu ext azur",
        "Answer_preprocessed_content":"tri todai problem steme us imag replac base imag modifi relat yml file work final thought document explicitli mention curat imag meant user extens updat schema blue model code environ imag updat channel depend flask need infer server pip",
        "Answer_gpt_summary_original":"the solution to the modulenotfounderror when importing scikit-learn from the inf-conda-env conda environment is to replace the mcr.microsoft.com\/\/tensorflow-2.4-ubuntu18.04-py37-cpu-inference image with a base image mcr.microsoft.com\/\/openmpi3.1.2-ubuntu18.04:latest and modify the related yml files. the documentation should explicitly mention that curated images are not meant for user extension. the updated blue-deployment.yml and env_azure.yml files are provided as examples.",
        "Answer_gpt_summary":"solut modulenotfounderror import scikit learn inf conda env conda environ replac mcr microsoft com tensorflow ubuntu cpu infer imag base imag mcr microsoft com openmpi ubuntu latest modifi relat yml file document explicitli mention curat imag meant user extens updat blue deploy yml env azur yml file provid exampl"
    },
    {
        "Question_id":57692681.0,
        "Question_title":"Sagemaker to use processed pickled ndarray instead of csv files from S3",
        "Question_body":"<p>I understand that you can pass a CSV file from S3 into a Sagemaker XGBoost container using the following code<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>train_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\nvalid_channel = sagemaker.session.s3_input(validation_data, content_type='text\/csv')\n\ndata_channels = {'train': train_channel, 'validation': valid_channel}\nxgb_model.fit(inputs=data_channels,  logs=True)\n<\/code><\/pre>\n\n<p>But I have an ndArray stored in S3 bucket. These are processed, label encoded, feature engineered arrays. I would want to pass this into the container instead of the csv. I do understand I can always convert my ndarray into csv files before saving it in S3. Just checking if there is an array option.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1566996073187,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>There are multiple options for algorithms in SageMaker:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Built-in algorithms<\/a>, like the SageMaker XGBoost you mention<\/li>\n<li>Custom, user-created algorithm code, which can be:\n\n<ul>\n<li>Written for a pre-built docker image, available for Sklearn, TensorFlow, Pytorch, MXNet<\/li>\n<li>Written in your own container<\/li>\n<\/ul><\/li>\n<\/ol>\n\n<p>When you use built-ins (option 1), your choice of data format options is limited to what the built-ins support, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">which is only csv and libsvm in the case of the built-in XGBoost<\/a>. If you want to use custom data formats and pre-processing logic before XGBoost, it is absolutely possible if you use your own script leveraging the open-source XGBoost. You can get inspiration from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">Random Forest demo<\/a> to see how to create custom models in pre-built containers<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57692681",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1568417822248,
        "Question_original_content":"us process pickl ndarrai instead csv file understand pass csv file xgboost contain follow code train channel session input train data content type text csv valid channel session input valid data content type text csv data channel train train channel valid valid channel xgb model fit input data channel log true ndarrai store bucket process label encod featur engin arrai want pass contain instead csv understand convert ndarrai csv file save check arrai option",
        "Question_preprocessed_content":"us process pickl ndarrai instead csv file understand pass csv file xgboost contain follow code ndarrai store bucket process label encod featur engin arrai want pass contain instead csv understand convert ndarrai csv file save check arrai option",
        "Question_gpt_summary_original":"The user wants to pass a processed pickled ndarray from an S3 bucket into a Sagemaker XGBoost container instead of using CSV files. They are looking for an array option and are aware that they can convert their ndarray into CSV files before saving it in S3.",
        "Question_gpt_summary":"user want pass process pickl ndarrai bucket xgboost contain instead csv file look arrai option awar convert ndarrai csv file save",
        "Answer_original_content":"multipl option algorithm built algorithm like xgboost mention custom user creat algorithm code written pre built docker imag avail sklearn tensorflow pytorch mxnet written contain us built in option choic data format option limit built in support csv libsvm case built xgboost want us custom data format pre process logic xgboost absolut possibl us script leverag open sourc xgboost inspir random forest demo creat custom model pre built contain",
        "Answer_preprocessed_content":"multipl option algorithm algorithm like xgboost mention custom algorithm code written docker imag avail sklearn tensorflow pytorch mxnet written contain us choic data format option limit support csv libsvm case xgboost want us custom data format logic xgboost absolut possibl us script leverag xgboost inspir random forest demo creat custom model contain",
        "Answer_gpt_summary_original":"possible solutions to the challenge of using a processed pickled ndarray instead of csv files from s3 for xgboost container are: \n1. use built-in algorithms like xgboost or other options available for sklearn, tensorflow, pytorch, mxnet. however, the data format options are limited to what the built-ins support, which is only csv and libsvm in the case of the built-in xgboost.\n2. write custom, user-created algorithm code for a pre-built docker image or in your own container using open-source xgboost. this allows for the use of custom data formats and pre-processing logic before xgboost. inspiration can be taken from the random forest demo to create custom models in pre-built containers.",
        "Answer_gpt_summary":"possibl solut challeng process pickl ndarrai instead csv file xgboost contain us built algorithm like xgboost option avail sklearn tensorflow pytorch mxnet data format option limit built in support csv libsvm case built xgboost write custom user creat algorithm code pre built docker imag contain open sourc xgboost allow us custom data format pre process logic xgboost inspir taken random forest demo creat custom model pre built contain"
    },
    {
        "Question_id":null,
        "Question_title":"Translating streaming audio into text",
        "Question_body":"Hi, I'm using @Google-cloud\/media-translation in node with express js server. I want to translate media file (\".wav\" format) with media-translation. At first, i got an error because of authentication and I fixed it with env variable as specified in documentation, I followed each and every step exactly told in the documentation but I'm getting no response from server. When i looked into APIs & Services tab it only recorded my failed auth attempts no other API calls are recorded. Please help because there is no help available online about this product and it doesn't even send error responses so i can debug. ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649687580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":33.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translating-streaming-audio-into-text\/td-p\/412679\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nWould you please share with us the error message ? Please make sure there are PII in it.\n\nYou can also share the reproduction steps?\n\nThanks"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"translat stream audio text googl cloud media translat node express server want translat media file wav format media translat got error authent fix env variabl specifi document follow step exactli told document get respons server look api servic tab record fail auth attempt api call record help help avail onlin product send error respons debug",
        "Question_preprocessed_content":"translat stream audio text node express server want translat media file got error authent fix env variabl specifi document follow step exactli told document get respons server look api servic tab record fail auth attempt api call record help help avail onlin product send error respons debug",
        "Question_gpt_summary_original":"The user is facing challenges in translating streaming audio into text using Google Cloud Media Translation in node with express js server. The user encountered an error due to authentication, which was fixed by using an env variable as specified in the documentation. However, the user is not receiving any response from the server and no API calls are being recorded in the APIs & Services tab. The user is seeking help as there is no online support available for this product and there are no error responses to debug the issue.",
        "Question_gpt_summary":"user face challeng translat stream audio text googl cloud media translat node express server user encount error authent fix env variabl specifi document user receiv respons server api call record api servic tab user seek help onlin support avail product error respons debug issu",
        "Answer_original_content":"share error messag sure pii share reproduct step thank",
        "Answer_preprocessed_content":"share error messag sure pii share reproduct step thank",
        "Answer_gpt_summary_original":"the answer does not provide any solutions to the challenges faced by the user with using google cloud's media translation api. instead, the answer requests the user to share the error message and reproduction steps to better understand the issue.",
        "Answer_gpt_summary":"answer provid solut challeng face user googl cloud media translat api instead answer request user share error messag reproduct step better understand issu"
    },
    {
        "Question_id":67051900.0,
        "Question_title":"How can I generate prediction intervals for Azure AutoML timeseries forecasts?",
        "Question_body":"<p>Is it possible to generate prediction intervals for time series forecasts when using a Azure AutoML trained models? Could we get the training errors out of the process and use them for bootstrapping?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618193869380,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":205.0,
        "Answer_body":"<p>You can generate forecast quantiles. See the following notebook for more details: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67051900",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618942132740,
        "Question_original_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Question_preprocessed_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Question_gpt_summary_original":"The user is facing challenges in generating prediction intervals for time series forecasts using Azure AutoML trained models. They are unsure if it is possible to obtain training errors for bootstrapping purposes.",
        "Question_gpt_summary":"user face challeng gener predict interv time seri forecast azur automl train model unsur possibl obtain train error bootstrap purpos",
        "Answer_original_content":"gener forecast quantil follow notebook detail http github com azur machinelearningnotebook blob master us autom machin learn forecast forecast function auto forecast function ipynb",
        "Answer_preprocessed_content":"gener forecast quantil follow notebook detail",
        "Answer_gpt_summary_original":"possible solution: the user can generate forecast quantiles using an azure automl trained model. they can refer to a specific notebook for more details on how to do this.",
        "Answer_gpt_summary":"possibl solut user gener forecast quantil azur automl train model refer specif notebook detail"
    },
    {
        "Question_id":null,
        "Question_title":"Deployment of Multiple Models to Container Instance Fails in Azure DevOps",
        "Question_body":"Hi Team,\n\nI am trying to deploy 2 ML models ( which is registered in Model Registry ) to Azure Container Instance using DevOps Release pipeline using AZ CLI ML extension\n\nMy ACI Configuration is :\n\ncontainerResourceRequirements: cpu: 1 memoryInGB: 4 computeType: ACI\n\nInference Config :\n\nentryScript: score.py runtime: python condaFile: conda_dependencies.yml extraDockerfileSteps: schemaFile: sourceDirectory: enableGpu: False baseImage: baseImageRegistry:\n\nAll score.py, conda_dependencies.yml, aciDeploymentConfig.yml is placed in a flattened directory which is publised in to DevOps pipeline artifcat and looks like\n\n\n\n\n\nDevOps Deploy command looks like\n\naz ml model deploy -g $(ml.resourceGroup) -w $(ml.workspace) --name $(service.name.staging) -f .\/model.json -m \"GloVe:4\" --dc aciDeploymentConfig.yml --ic inferenceConfig.yml --overwrite --debug\n\nAlso i have set the working directory as the folder where all above files are placed. something like\n\n$(System.DefaultWorkingDirectory)\/_Symptom-Code-Indexing\/symptom_model\/a\n\nIts getting in to an exception as\n\n2020-06-08T12:50:27.9202657Z \"error\": {\n2020-06-08T12:50:27.9208361Z \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"BadRequest\\\",\\\"statusCode\\\":400,\\\"message\\\":\\\"The request is invalid.\\\",\\\"details\\\":[{\\\"code\\\":\\\"InvalidOverwriteRequest\\\",\\\"message\\\":\\\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\\\"}],\\\"correlation\\\":{\\\"RequestId\\\":\\\"823e8483923846b1958c08ffaba074ff\\\"}}'\"\n2020-06-08T12:50:27.9212109Z }\n2020-06-08T12:50:27.9212376Z }}\n2020-06-08T12:50:27.9213437Z {'Azure-cli-ml Version': '1.6.0', 'Error': WebserviceException:\n2020-06-08T12:50:27.9214158Z Message: Received bad response from Model Management Service:\n2020-06-08T12:50:27.9214688Z Response Code: 400\n2020-06-08T12:50:27.9217800Z Headers: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\n2020-06-08T12:50:27.9222115Z Content: b'{\"code\":\"BadRequest\",\"statusCode\":400,\"message\":\"The request is invalid.\",\"details\":[{\"code\":\"InvalidOverwriteRequest\",\"message\":\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\"}],\"correlation\":{\"RequestId\":\"823e8483923846b1958c08ffaba074ff\"}}'\n2020-06-08T12:50:27.9223705Z InnerException None\n2020-06-08T12:50:27.9224049Z ErrorResponse\n2020-06-08T12:50:27.9224320Z {\n2020-06-08T12:50:27.9224617Z \"error\": {\n2020-06-08T12:50:27.9229025Z \"message\": \"Received bad response from Model Management Service:\\nResponse Code: 400\\nHeaders: {'Date': 'Mon, 08 Jun 2020 12:50:27 GMT', 'Content-Type': 'application\/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Request-Context': 'appId=cid-v1:2d2e8e63-272e-4b3c-8598-4ee570a0e70d', 'x-ms-client-request-id': '823e8483923846b1958c08ffaba074ff', 'x-ms-client-session-id': '62e84a29-b77c-456f-9d91-5ca6be26f79c', 'api-supported-versions': '1.0, 2018-03-01-preview, 2018-11-19', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload'}\\nContent: b'{\\\"code\\\":\\\"BadRequest\\\",\\\"statusCode\\\":400,\\\"message\\\":\\\"The request is invalid.\\\",\\\"details\\\":[{\\\"code\\\":\\\"InvalidOverwriteRequest\\\",\\\"message\\\":\\\"Invalid overwrite request - cannot update container resource requirements, dns name label, or deployment type. Please delete and redeploy this service.\\\"}],\\\"correlation\\\":{\\\"RequestId\\\":\\\"823e8483923846b1958c08ffaba074ff\\\"}}'\"\n2020-06-08T12:50:27.9230782Z }\n2020-06-08T12:50:27.9230908Z }}\n2020-06-08T12:50:27.9231134Z Event: Cli.PostExecute [<function AzCliLogging.deinit_cmd_metadata_logging at 0x7fea2ca1f730>]\n2020-06-08T12:50:27.9231431Z az_command_data_logger : exit code: 1\n2020-06-08T12:50:27.9275693Z telemetry.save : Save telemetry record of length 7390 in cache\n2020-06-08T12:50:27.9280735Z telemetry.check : Negative: The \/home\/vsts\/work\/_temp\/.azclitask\/telemetry.txt was modified at 2020-06-08 12:47:41.161160, which in less than 600.000000 s\n2020-06-08T12:50:27.9290480Z command ran in 55.735 seconds.\n2020-06-08T12:50:28.1525434Z ##[error]Script failed with exit code: 1\n2020-06-08T12:50:28.1536650Z [command]\/opt\/hostedtoolcache\/Python\/3.6.10\/x64\/bin\/az account clear\n2020-06-08T12:50:29.9078943Z ##[section]Finishing: Deploy Model to ACI\n\nBut when i tried to Deploy it using Python SDK it works as well. Is there any permission issues or login to be set before using DevOps Release. I have not done any sort of login in my DevOps Build pipeline.\n\nAny pointers on what is going wrong here ? It would be really helpful.\n\nThanks,\nSrijith",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592223294357,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"They're actively answering Devops question in dedicated forums here.\n\n\n\n\nhttps:\/\/developercommunity.visualstudio.com\/spaces\/21\/index.html\n\n\n\n\n--please don't forget to Accept as answer if the reply is helpful--\n\nRegards, Dave Patrick ....\nMicrosoft Certified Professional\nMicrosoft MVP [Windows Server] Datacenter Management\n\n\n\n\nDisclaimer: This posting is provided \"AS IS\" with no warranties or guarantees, and confers no rights.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/36104\/deployment-of-multiple-models-to-container-instanc.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-15T12:17:00.34Z",
                "Answer_score":1,
                "Answer_body":"They're actively answering Devops question in dedicated forums here.\n\n\n\n\nhttps:\/\/developercommunity.visualstudio.com\/spaces\/21\/index.html\n\n\n\n\n--please don't forget to Accept as answer if the reply is helpful--\n\nRegards, Dave Patrick ....\nMicrosoft Certified Professional\nMicrosoft MVP [Windows Server] Datacenter Management\n\n\n\n\nDisclaimer: This posting is provided \"AS IS\" with no warranties or guarantees, and confers no rights.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592223420340,
        "Question_original_content":"deploy multipl model contain instanc fail azur devop team try deploi model regist model registri azur contain instanc devop releas pipelin cli extens aci configur containerresourcerequir cpu memoryingb computetyp aci infer config entryscript score runtim python condafil conda depend yml extradockerfilestep schemafil sourcedirectori enablegpu fals baseimag baseimageregistri score conda depend yml acideploymentconfig yml place flatten directori publis devop pipelin artifcat look like devop deploi command look like model deploi resourcegroup workspac servic stage model json glove acideploymentconfig yml inferenceconfig yml overwrit debug set work directori folder file place like defaultworkingdirectori symptom code index symptom model get except error messag receiv bad respons model manag servic nrespons code nheader date mon jun gmt content type applic json transfer encod chunk connect aliv request context appid cid dee eea client request ebcffabaff client session cabefc api support version preview strict transport secur max ag includesubdomain preload ncontent code badrequest statuscod messag request invalid detail code invalidoverwriterequest messag invalid overwrit request updat contain resourc requir dn label deploy type delet redeploi servic correl requestid ebcffabaff azur cli version error webserviceexcept messag receiv bad respons model manag servic respons code header date mon jun gmt content type applic json transfer encod chunk connect aliv request context appid cid dee eea client request ebcffabaff client session cabefc api support version preview strict transport secur max ag includesubdomain preload content code badrequest statuscod messag request invalid detail code invalidoverwriterequest messag invalid overwrit request updat contain resourc requir dn label deploy type delet redeploi servic correl requestid ebcffabaff innerexcept errorrespons error messag receiv bad respons model manag servic nrespons code nheader date mon jun gmt content type applic json transfer encod chunk connect aliv request context appid cid dee eea client request ebcffabaff client session cabefc api support version preview strict transport secur max ag includesubdomain preload ncontent code badrequest statuscod messag request invalid detail code invalidoverwriterequest messag invalid overwrit request updat contain resourc requir dn label deploy type delet redeploi servic correl requestid ebcffabaff event cli postexecut command data logger exit code telemetri save save telemetri record length cach telemetri check neg home vst work temp azclitask telemetri txt modifi command ran second error script fail exit code command opt hostedtoolcach python bin account clear section finish deploi model aci tri deploi python sdk work permiss issu login set devop releas sort login devop build pipelin pointer go wrong help thank srijith",
        "Question_preprocessed_content":"deploy multipl model contain instanc fail azur devop team try deploi model azur contain instanc devop releas pipelin cli extens aci configur containerresourcerequir cpu memoryingb computetyp aci infer config entryscript runtim python condafil extradockerfilestep schemafil sourcedirectori enablegpu fals baseimag baseimageregistri place flatten directori publis devop pipelin artifcat look like devop deploi command look like model deploi glove set work directori folder file place like get except error request overwrit request updat contain resourc requir dn label deploy type delet redeploi content correl requestid ffaba innerexcept errorrespons request overwrit request updat contain resourc requir dn label deploy type delet redeploi event exit code save telemetri record length cach neg modifi command ran second error script fail exit code account clear section finish deploi model aci tri deploi python sdk work permiss issu login set devop releas sort login devop build pipelin pointer go wrong help thank srijith",
        "Question_gpt_summary_original":"The user is encountering an exception while trying to deploy two ML models to Azure Container Instance using DevOps Release pipeline using AZ CLI ML extension. The error message suggests that there is an invalid overwrite request and the container resource requirements, DNS name label, or deployment type cannot be updated. The user is unsure if there are any permission issues or login requirements that need to be set before using DevOps Release.",
        "Question_gpt_summary":"user encount except try deploi model azur contain instanc devop releas pipelin cli extens error messag suggest invalid overwrit request contain resourc requir dn label deploy type updat user unsur permiss issu login requir need set devop releas",
        "Answer_original_content":"activ answer devop question dedic forum http developercommun visualstudio com space index html forget accept answer repli help regard dave patrick microsoft certifi profession microsoft mvp window server datacent manag disclaim post provid warranti guarante confer right",
        "Answer_preprocessed_content":"activ answer devop question dedic forum forget accept answer repli regard dave patrick microsoft certifi profession microsoft mvp datacent manag disclaim post provid warranti guarante confer right",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the user is directed to a dedicated forum where they can ask their question and receive help from the community.",
        "Answer_gpt_summary":"solut provid answer user direct dedic forum ask question receiv help commun"
    },
    {
        "Question_id":null,
        "Question_title":"SDK v1 or V2",
        "Question_body":"We are planing for next gen of product. Will V2 provide way more changes than V1?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661977244390,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @nam-4027\n\nThanks for using Microsoft Q&A. I will recommend you keeping in V1 at this moment.\n\nSDK v2 is currently in public preview. The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see Supplemental Terms of Use for Microsoft Azure Previews.\n\nhttps:\/\/azure.microsoft.com\/support\/legal\/preview-supplemental-terms\/\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/989368\/sdk-v1-or-v2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-31T21:32:40.817Z",
                "Answer_score":0,
                "Answer_body":"Hello @nam-4027\n\nThanks for using Microsoft Q&A. I will recommend you keeping in V1 at this moment.\n\nSDK v2 is currently in public preview. The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see Supplemental Terms of Use for Microsoft Azure Previews.\n\nhttps:\/\/azure.microsoft.com\/support\/legal\/preview-supplemental-terms\/\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1661981560816,
        "Question_original_content":"sdk plane gen product provid wai chang",
        "Question_preprocessed_content":"sdk plane gen product provid wai chang",
        "Question_gpt_summary_original":"The user is facing the challenge of deciding whether to use SDK v1 or v2 for their next generation product, and they are unsure if v2 will provide significant improvements over v1.",
        "Question_gpt_summary":"user face challeng decid us sdk gener product unsur provid signific improv",
        "Answer_original_content":"hello nam thank microsoft recommend keep moment sdk current public preview preview version provid servic level agreement recommend product workload certain featur support constrain capabl inform supplement term us microsoft azur preview http azur microsoft com support legal preview supplement term hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_preprocessed_content":"hello thank microsoft recommend keep moment sdk current public preview preview version provid servic level agreement recommend product workload certain featur support constrain capabl inform supplement term us microsoft azur preview hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_gpt_summary_original":"the answer recommends using sdk v1 at the moment and not using v2 for production workloads as it is currently in public preview and may have constrained capabilities. the answer also provides a link to the supplemental terms of use for microsoft azure previews for more information.",
        "Answer_gpt_summary":"answer recommend sdk moment product workload current public preview constrain capabl answer provid link supplement term us microsoft azur preview inform"
    },
    {
        "Question_id":null,
        "Question_title":"How do I achieve the least-access secure networking for SageMaker Training on Amazon FSx for Lustre?",
        "Question_body":"I'm trying to figure out a minimally permissive yet operational network configuration for Amazon SageMaker training to train on data from Amazon FSx for Lustre. My understanding is that both the file system and the SageMaker instance can have their own security groups and that FSx uses TCP on ports 988 and 1021-1023. Therefore, I think a good network configuration for using SageMaker with FSx is the following:\n\nSageMaker EC2 equipped with the security group SM-SG that allows Inbound only with TCP on 988 and 1021-1023 from FSX-SG only.\nAmazon FSx equipped with the security group FSX-SG that allows outbound only with TCP on 988 and 1021-1023 towards SM-SG only. Is this configuration enough for the training to work? Do FSx and SageMaker need other ports and sources to be opened to operate normally?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1605279993000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Answer_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\nFSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\nFSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1]. [1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUrTkxH_kIT-a_LJSGYS5SXA\/how-do-i-achieve-the-least-access-secure-networking-for-sage-maker-training-on-amazon-f-sx-for-lustre",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-13T15:26:19.000Z",
                "Answer_score":0,
                "Answer_body":"For the security group for Amazon FSx (Example: FSx-SG), you need to add the following additional rules:\n\nFSx-SG needs inbound access from the security group for SageMaker (Example: SM-SG). The SageMaker instance needs to initiate a connection to the Amazon FSx file system, which is an inbound TCP packet to FSx.\nFSx-SG needs inbound and outbound access to itself. This is because, Amazon FSx for Lustre is a clustered file system, where each file system is typically powered by multiple file servers, and the file servers need to communicate with one another.\n\nFor more information on the minimum set of rules required for FSx-SG, see [File system access control with Amazon VPC][1]. [1]: https:\/\/docs.aws.amazon.com\/fsx\/latest\/LustreGuide\/limit-access-security-groups.html",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1605281179000,
        "Question_original_content":"achiev access secur network train amazon fsx lustr try figur minim permiss oper network configur train train data amazon fsx lustr understand file instanc secur group fsx us tcp port think good network configur fsx follow equip secur group allow inbound tcp fsx amazon fsx equip secur group fsx allow outbound tcp configur train work fsx need port sourc open oper normal",
        "Question_preprocessed_content":"achiev secur network train amazon fsx lustr try figur minim permiss oper network configur train train data amazon fsx lustr understand file instanc secur group fsx us tcp port think good network configur fsx follow equip secur group allow inbound tcp amazon fsx equip secur group allow outbound tcp configur train work fsx need port sourc open oper normal",
        "Question_gpt_summary_original":"The user is trying to configure a secure network for Amazon SageMaker training to use data from Amazon FSx for Lustre. They are trying to determine the minimal permissions needed for both the file system and the SageMaker instance, and have identified that FSx uses TCP on ports 988 and 1021-1023. The user proposes a network configuration that includes security groups for both SageMaker and FSx, allowing inbound and outbound traffic only on the specified ports between the two security groups. The user is unsure if this configuration is sufficient for normal operation and is seeking clarification on whether other ports or sources need to be opened.",
        "Question_gpt_summary":"user try configur secur network train us data amazon fsx lustr try determin minim permiss need file instanc identifi fsx us tcp port user propos network configur includ secur group fsx allow inbound outbound traffic specifi port secur group user unsur configur suffici normal oper seek clarif port sourc need open",
        "Answer_original_content":"secur group amazon fsx exampl fsx need add follow addit rule fsx need inbound access secur group exampl instanc need initi connect amazon fsx file inbound tcp packet fsx fsx need inbound outbound access amazon fsx lustr cluster file file typic power multipl file server file server need commun inform minimum set rule requir fsx file access control amazon vpc http doc aw amazon com fsx latest lustreguid limit access secur group html",
        "Answer_preprocessed_content":"secur group amazon fsx need add follow addit rule need inbound access secur group instanc need initi connect amazon fsx file inbound tcp packet fsx need inbound outbound access amazon fsx lustr cluster file file typic power multipl file server file server need commun inform minimum set rule requir",
        "Answer_gpt_summary_original":"possible solutions from the answer include adding additional rules to the security group for amazon fsx, allowing inbound access from the security group for the instance, allowing the instance to initiate a connection to the amazon fsx file system, and allowing inbound and outbound access to itself. the answer also provides a link to more information on the minimum set of rules required for the security group.",
        "Answer_gpt_summary":"possibl solut answer includ ad addit rule secur group amazon fsx allow inbound access secur group instanc allow instanc initi connect amazon fsx file allow inbound outbound access answer provid link inform minimum set rule requir secur group"
    },
    {
        "Question_id":null,
        "Question_title":"An error occurred (ModelError) when calling the InvokeEndpoint operation",
        "Question_body":"Hello,\nI received the following error message when I tried to send an array to my model:\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from container-1 with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n<title>500 Internal Server Error<\/title> <h1>Internal Server Error<\/h1> <p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.<\/p> \". See https:\/\/us-east-1.console.aws.amazon.com\/cloudwatch\/home?region=us-east-1#logEventViewer:group\n\nI have created inference pipeline containing preprocessing and autoencoder model and deployed it to a single endpoint. Am trying to send raw data in text\/csv format. EX: \"39, 4, 9, 8, contact\"\n\nPlease help me out in this.\n\nMuch appreciated,\nKarthik",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623820480000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":831.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU68gR5B3JRdqOEGlwE2-pnA\/an-error-occurred-model-error-when-calling-the-invoke-endpoint-operation",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-22T16:37:09.000Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nSo the issue here is most likely with your inference code and how you are parsing\/transforming the data coming in. Your endpoint is up and running but the format in which you are feeding it data is confusing it. The endpoint is expecting encoded data thus you need to convert your payload into the appropriate data format, there are two manners in which you can approach this.\n\nUse a serializer, when creating your endpoint with the predictor class you want to use the SageMaker Serializer to automatically encode\/decode your data, this is configured while creating your endpoint. Look at the following code snippet below.\n\nfrom sagemaker.predictor import csv_serializer\nrf_pred = rf.deploy(1, \"ml.m4.xlarge\", serializer=csv_serializer)\n\n#for prediction, decode the data properly\nprint(rf_pred.predict(payload).decode('utf-8'))\n\nIf you choose not to use the serializer you want to encode the data on your own using something such as json.dumps(payload) to encode your data properly before sending the data to the endpoint.\n\nExtra Resources:\nSageMaker Serializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nHope this helps!",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error occur modelerror call invokeendpoint oper hello receiv follow error messag tri send arrai model error occur modelerror call invokeendpoint oper receiv server error contain messag intern server error intern server error server encount intern error unabl complet request server overload error applic http east consol aw amazon com cloudwatch home region east logeventview group creat infer pipelin contain preprocess autoencod model deploi singl endpoint try send raw data text csv format contact help appreci karthik",
        "Question_preprocessed_content":"error occur call invokeendpoint oper hello receiv follow error messag tri send arrai model error occur call invokeendpoint oper receiv server error messag intern server error intern server error server encount intern error unabl complet request server overload error applic creat infer pipelin contain preprocess autoencod model deploi singl endpoint try send raw data format contact help appreci karthik",
        "Question_gpt_summary_original":"The user encountered an error (ModelError) when trying to send an array to their model. The error message received was a server error (500) with a message indicating an internal server error or an error in the application. The user has created an inference pipeline containing preprocessing and autoencoder model and deployed it to a single endpoint. They are trying to send raw data in text\/csv format but are facing challenges.",
        "Question_gpt_summary":"user encount error modelerror try send arrai model error messag receiv server error messag indic intern server error error applic user creat infer pipelin contain preprocess autoencod model deploi singl endpoint try send raw data text csv format face challeng",
        "Answer_original_content":"hello issu like infer code pars transform data come endpoint run format feed data confus endpoint expect encod data need convert payload appropri data format manner approach us serial creat endpoint predictor class want us serial automat encod decod data configur creat endpoint look follow code snippet predictor import csv serial pred deploi xlarg serial csv serial predict decod data properli print pred predict payload decod utf choos us serial want encod data json dump payload encod data properli send data endpoint extra resourc serial http readthedoc stabl api infer serial html hope help",
        "Answer_preprocessed_content":"hello issu like infer code data come endpoint run format feed data confus endpoint expect encod data need convert payload appropri data format manner approach us serial creat endpoint predictor class want us serial automat data configur creat endpoint look follow code snippet predictor import predict decod data properli choos us serial want encod data encod data properli send data endpoint extra resourc serial hope help",
        "Answer_gpt_summary_original":"possible solutions to the error encountered when sending an array to the model are to use a serializer to automatically encode\/decode the data or to encode the data on your own using something like json.dumps(payload) before sending it to the endpoint. the endpoint is expecting encoded data, so the payload needs to be converted into the appropriate data format. extra resources on serializers are also provided.",
        "Answer_gpt_summary":"possibl solut error encount send arrai model us serial automat encod decod data encod data like json dump payload send endpoint endpoint expect encod data payload need convert appropri data format extra resourc serial provid"
    },
    {
        "Question_id":null,
        "Question_title":"NotFoundError()",
        "Question_body":"<p>Hi,<\/p>\n<p>I was wondering if you have any suggestions on what could be causing the following error:<\/p>\n<p>wandb: Synced 6 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)<br>\nwandb: Find logs at: .\/wandb\/run-20220825_163228-73lfow6y\/logs<br>\nRun 73lfow6y errored: NotFoundError()<br>\nwandb: ERROR Run 73lfow6y errored: NotFoundError()<\/p>\n<p>Please note, I am using:<br>\nPython 3.7.12<br>\nwandb, version 0.13.2<\/p>\n<p>Thank you!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1661445735566,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":268.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/notfounderror\/3006",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-25T18:34:14.111Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dfrances\">@dfrances<\/a>,<br>\nCould you possibly share a minimal replication of your code? Also, if you are using our public cloud service would you mind sending a link to your workspace and I can take a look?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-01T22:07:42.973Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dfrances\">@dfrances<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-31T22:08:12.417Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"notfounderror wonder suggest caus follow error sync file media file artifact file file log run lfowi log run lfowi error notfounderror error run lfowi error notfounderror note python version thank",
        "Question_preprocessed_content":"notfounderror wonder suggest caus follow error sync file media file artifact file file log run lfow error notfounderror error run lfow error notfounderror note python version thank",
        "Question_gpt_summary_original":"The user is encountering a NotFoundError() while using wandb version 0.13.2 with Python 3.7.12. The error occurred during the syncing of 6 W&B files, 0 media files, 0 artifact files, and 0 other files. The user is seeking suggestions on what could be causing the error.",
        "Question_gpt_summary":"user encount notfounderror version python error occur sync file media file artifact file file user seek suggest caus error",
        "Answer_original_content":"dfranc possibl share minim replic code public cloud servic mind send link workspac look thank nate dfranc heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"possibl share minim replic code public cloud servic mind send link workspac look thank nate heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the answer is a request for more information and a notification that the conversation will be closed if there is no response.",
        "Answer_gpt_summary":"solut provid answer answer request inform notif convers close respons"
    },
    {
        "Question_id":null,
        "Question_title":"xAxis settings for line plot over different runs",
        "Question_body":"<p>I have developed a script to estimate some hardware evaluations for running my neural network. As parameter for this script the number of layers can be defined. This parameter is also passed to Wand.config.<br>\nThen the network trains over several epochs and evaluates the hardware. The results are all logged. At the end I would like to have a line chart with the number of layers as xaxis and for example the energy consumption on the yaxis. However, it is not possible to select the num_layers parameter in the menu.<br>\nMy guess is that line charts can only be plotted across a run and not as in my case where per value of the xaxis (num_layers) a different run specifies the value.<\/p>\n<p>Is there any way to implement my plan in Wandb in an automated way? Otherwise I would have to export the data and plot it with matplotlib etc.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1660160908832,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/xaxis-settings-for-line-plot-over-different-runs\/2894",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-15T21:06:21.541Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chwolters\">@chwolters<\/a> , can you please provide a link to your workspace for us to review. Thanks<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-16T19:31:56.771Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a>! You can have a look here: <a href=\"https:\/\/wandb.ai\/duke-tum\/example?workspace=user-chwolters\" class=\"inline-onebox\">Weights &amp; Biases<\/a><br>\nIn this workspace there are 6 runs; 3 for each learning rule which I want to compare. These two sets of runs have an increasing network depth (2, 3, 4). Now I\u2019d like to plot different metrics such as accuracy over the different network depts. So, on the x-axis there would be the depth with values 2, 3, 4; while measuring accuracy on the y-axis.<br>\nAll this should be plotted as a line chart with one line for each learning rule, i.e. comparing \u201cBP\u201d with \u201cDFA\u201d.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-24T07:32:00.510Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chwolters\">@chwolters<\/a> , this is definitely doable with <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/custom-charts\">custom charts<\/a>. Specifically a chart with a custom x axis of 2,3,4, and three y fields values to referencing the runs accuracy. Alternatively yes, using Matplotlib is an option, however, with custom charts, you can save the preset for future use.<\/p>\n<p>If you decide for a custom chart, some of your Vega code may  appear as follows<\/p>\n<p>Define Multiple Y fields<\/p>\n<pre><code class=\"lang-auto\">\"transform\": [\n  {\"filter\": {\"field\": \"${field:yfield1}\", \"valid\": true}},\n  {\"filter\": {\"field\": \"${field:yfield2}\", \"valid\": true}},\n  {\"filter\": {\"field\": \"${field:yfield3}\", \"valid\": true}}\n],\n<\/code><\/pre>\n<p>Define custom x axis<\/p>\n<pre><code class=\"lang-auto\">\"x\":{\n  \"type\": \"quantitative\",\n  \"axis\": {\n    \"values\": [2, 3, 4]\n  }\n},\n<\/code><\/pre>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-29T19:55:08.679Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chwolters\">@chwolters<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-28T19:55:26.820Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"xaxi set line plot differ run develop script estim hardwar evalu run neural network paramet script number layer defin paramet pass wand config network train epoch evalu hardwar result log end like line chart number layer xaxi exampl energi consumpt yaxi possibl select num layer paramet menu guess line chart plot run case valu xaxi num layer differ run specifi valu wai implement plan autom wai export data plot matplotlib",
        "Question_preprocessed_content":"xaxi set line plot differ run develop script estim hardwar evalu run neural network paramet script number layer defin paramet pass network train epoch evalu hardwar result log end like line chart number layer xaxi exampl energi consumpt yaxi possibl select paramet menu guess line chart plot run case valu xaxi differ run specifi valu wai implement plan autom wai export data plot matplotlib",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a line chart with the number of layers as the x-axis and energy consumption as the y-axis. The issue is that the num_layers parameter cannot be selected in the menu, and it seems that line charts can only be plotted across a run rather than per value of the x-axis. The user is seeking an automated way to implement their plan in Wandb, or they will have to export the data and plot it with matplotlib.",
        "Question_gpt_summary":"user face challeng creat line chart number layer axi energi consumpt axi issu num layer paramet select menu line chart plot run valu axi user seek autom wai implement plan export data plot matplotlib",
        "Answer_original_content":"chwolter provid link workspac review thank mohammadbakir look workspac run learn rule want compar set run increas network depth like plot differ metric accuraci differ network dept axi depth valu measur accuraci axi plot line chart line learn rule compar dfa chwolter definit doabl custom chart specif chart custom axi field valu referenc run accuraci altern ye matplotlib option custom chart save preset futur us decid custom chart vega code appear follow defin multipl field transform filter field field yfield valid true filter field field yfield valid true filter field field yfield valid true defin custom axi type quantit axi valu chwolter heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"provid link workspac review thank look workspac run learn rule want compar set run increas network depth like plot differ metric accuraci differ network dept depth valu measur accuraci plot line chart line learn rule compar dfa definit doabl custom chart specif chart custom axi field valu referenc run accuraci altern ye matplotlib option custom chart save preset futur us decid custom chart vega code appear follow defin multipl field defin custom axi heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge of plotting a line chart with the number of layers as the x-axis and the energy consumption on the y-axis are: using custom charts with a custom x-axis of 2, 3, 4 and three y fields values to reference the runs' accuracy, or using matplotlib. custom charts can save the preset for future use.",
        "Answer_gpt_summary":"possibl solut user challeng plot line chart number layer axi energi consumpt axi custom chart custom axi field valu refer run accuraci matplotlib custom chart save preset futur us"
    },
    {
        "Question_id":67953241.0,
        "Question_title":"mlflow run git-uri clone to specific directory",
        "Question_body":"<p>I am using mlflow run with a GitHub uri.<\/p>\n<p>When I run using the below command<\/p>\n<pre><code>mlflow run &lt;git-uri&gt;\n<\/code><\/pre>\n<p>The command sets up a conda environment and then <em>clones the Git repo into a <strong>temp<\/strong> directory, But I need it setup in a <strong>specific<\/strong> directory<\/em><\/p>\n<p>I checked the entire document, but I can't find it. Is there no such option to do so in one shot?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623534343453,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":239.0,
        "Answer_body":"<p>For non-local URIs, MLflow uses the Python's <code>tempfile.mkdtemp<\/code> function (<a href=\"https:\/\/github.com\/mlflow\/mlflow\/blob\/1c43176cefb5531fbb243975b9c8c5bfb9775e66\/mlflow\/projects\/utils.py#L140\" rel=\"nofollow noreferrer\">source code<\/a>), that creates the temporary directory.  You may have some control over it by setting the <code>TMPDIR<\/code> environment variable as described in <a href=\"https:\/\/docs.python.org\/3\/library\/tempfile.html#tempfile.mkstemp\" rel=\"nofollow noreferrer\">Python docs<\/a> (it lists <code>TMP<\/code> &amp; <code>TEMP<\/code> as well, but they didn't work for me on MacOS) - but it will set only &quot;base path&quot; for temporary directories and files, the directory\/file names are still will be random.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1623614468896,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67953241",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1623570743820,
        "Question_original_content":"run git uri clone specif directori run github uri run command run command set conda environ clone git repo temp directori need setup specif directori check entir document option shot",
        "Question_preprocessed_content":"run clone specif directori run github uri run command command set conda environ clone git repo temp directori need setup specif directori check entir document option shot",
        "Question_gpt_summary_original":"The user is facing a challenge while using mlflow run with a GitHub uri. The command clones the Git repo into a temp directory, but the user needs it to be set up in a specific directory. The user is unable to find an option to do so in one shot.",
        "Question_gpt_summary":"user face challeng run github uri command clone git repo temp directori user need set specif directori user unabl option shot",
        "Answer_original_content":"non local uri us python tempfil mkdtemp function sourc code creat temporari directori control set tmpdir environ variabl describ python doc list tmp temp work maco set base path temporari directori file directori file name random",
        "Answer_preprocessed_content":"uri us python function creat temporari directori control set environ variabl describ python doc set base path temporari directori file name random",
        "Answer_gpt_summary_original":"possible solutions to the challenge of running a git-uri clone to a specific directory include using python's tempfile.mkdtemp function to create a temporary directory for non-local uris. the user can also set the tmpdir environment variable to have some control over the temporary directory's base path. however, the directory and file names will still be random.",
        "Answer_gpt_summary":"possibl solut challeng run git uri clone specif directori includ python tempfil mkdtemp function creat temporari directori non local uri user set tmpdir environ variabl control temporari directori base path directori file name random"
    },
    {
        "Question_id":59599721.0,
        "Question_title":"Is AWS Sage Maker Auto Pilot suitable for NLP?",
        "Question_body":"<p>Is AWS Sage Maker Auto Pilot suitable for NLP?<\/p>\n\n<p>We currently have a tensorflow model that does classification on input of a sequence of URLS (\nWe transform the URLs to Word vec and Char vec to feed it to the model).<\/p>\n\n<p>Looking at Sage Maker Auto Pilot documentation it says that it works on input in tabular form.\nI was wondering if we could use it to for our use case.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1578226347093,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":261.0,
        "Answer_body":"<p>No. SageMaker AutoPilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. Technically, I guess you could pass embeddings in CSV format, and pray that XGBoost figures them out, but I seriously doubt that this would deliver meaningful results :)<\/p>\n\n<p>Amazon Comprehend does support fully managed custom classification models <a href=\"https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html<\/a>. It may be worth taking a look at it.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59599721",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1578756147747,
        "Question_original_content":"aw sage maker auto pilot suitabl nlp aw sage maker auto pilot suitabl nlp current tensorflow model classif input sequenc url transform url word vec char vec feed model look sage maker auto pilot document sai work input tabular form wonder us us case",
        "Question_preprocessed_content":"aw sage maker auto pilot suitabl nlp aw sage maker auto pilot suitabl nlp current tensorflow model classif input sequenc url look sage maker auto pilot document sai work input tabular form wonder us us case",
        "Question_gpt_summary_original":"The user is questioning whether AWS Sage Maker Auto Pilot is suitable for their Natural Language Processing (NLP) task, as their current tensorflow model uses a sequence of URLs transformed into Word and Char vectors for classification. They are unsure if Auto Pilot, which works on tabular input, can be used for their use case.",
        "Question_gpt_summary":"user question aw sage maker auto pilot suitabl natur languag process nlp task current tensorflow model us sequenc url transform word char vector classif unsur auto pilot work tabular input us case",
        "Answer_original_content":"autopilot support deep learn moment classif regress problem tabular data technic guess pass embed csv format prai xgboost figur serious doubt deliv meaning result amazon comprehend support fulli manag custom classif model http doc aw amazon com comprehend latest document classif html worth take look",
        "Answer_preprocessed_content":"autopilot support deep learn moment classif regress problem tabular data technic guess pass embed csv format prai xgboost figur serious doubt deliv meaning result amazon comprehend support fulli manag custom classif model worth take look",
        "Answer_gpt_summary_original":"the answer states that aws sage maker auto pilot is not suitable for natural language processing (nlp) as it currently only supports classification and regression problems on tabular data. however, the answer suggests using amazon comprehend, which supports fully managed custom classification models and may be worth exploring for nlp needs.",
        "Answer_gpt_summary":"answer state aw sage maker auto pilot suitabl natur languag process nlp current support classif regress problem tabular data answer suggest amazon comprehend support fulli manag custom classif model worth explor nlp need"
    },
    {
        "Question_id":66216294.0,
        "Question_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Question_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613428648627,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":276.0,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1613774515323,
        "Question_original_content":"chang conf file work jupyt notebook instal packag jupyt server instal aw need store artifact model aw bucket want specifi credenti conf file chang conf file instanc look like permiss deni folder mayb somebodi suggest better approach",
        "Question_preprocessed_content":"chang conf file work jupyt notebook instal packag jupyt server instal aw need store artifact model aw bucket want specifi credenti conf file chang conf file instanc look like permiss deni folder mayb somebodi suggest better approach",
        "Question_gpt_summary_original":"The user is facing challenges in changing the clearml.conf file in AWS Sagemaker Jupyter notebook to specify credentials to S3 for storing artifacts and models. The user is encountering permission denied errors while attempting to change the file and is seeking suggestions for a better approach.",
        "Question_gpt_summary":"user face challeng chang conf file jupyt notebook specifi credenti store artifact model user encount permiss deni error attempt chang file seek suggest better approach",
        "Answer_original_content":"disclaim train team set credenti server host us task set credenti specifi bucket output artifact debug imag matter set file server exampl import task task set credenti api host http server web host http server file host bucket folder kei add kei secret add kei secret pass credenti add cell jupyt notebook set standard aw environ variabl import environ aw access kei bucket kei environ aw secret access kei bucket secret option environ aw default region bucket region",
        "Answer_preprocessed_content":"disclaim team set credenti us specifi bucket output artifact set exampl pass credenti add cell jupyt notebook set standard aw environ variabl",
        "Answer_gpt_summary_original":"possible solutions to the challenge of changing the .conf file in  to store artifacts and models in an s3 bucket include using task.set_credentials to set credentials and -server hosts, and setting the s3 bucket as the files_server for all artifacts and debug images. to pass s3 credentials, one can add a cell at the top of the jupyter notebook and set the standard aws s3 environment variables.",
        "Answer_gpt_summary":"possibl solut challeng chang conf file store artifact model bucket includ task set credenti set credenti server host set bucket file server artifact debug imag pass credenti add cell jupyt notebook set standard aw environ variabl"
    },
    {
        "Question_id":null,
        "Question_title":"Why am I being charged for Azure Machine Learning?",
        "Question_body":"Context: I created an Azure Machine Learning workspace and uploaded a dataset that's less than 1mb. I created a compute instance and a compute cluster to test. They're shutdown, and haven't used the workspace since Oct 25, yet today I log in and find out I have been charged around $0.40 daily for no apparent reason at all.\n\nThe cost management shows the following:\n\n\n\n\nAnd it's even showing that i'm supposedly using a premium SSD, which is what's costing the most:\n\nI don't remember getting such a disk. And according to the portal, there isn't any:\n\n\n\n\nAccording to the cost management again, it's not storage what's causing the high cost, it's the Azure ML resource:\n\nThere's even this tag: amlresourcetype: provisioner.batch\nI tried to google it, but I can't find anything about it.\n\nSo finally, I check the computer instances but everything seems ok:\n\nSo, does anyone have any remote idea as to why I'm being charged? Why does the cost management says I'm been charged for storage but then it says it's the Azure ML resource? How can I get rid of the supposedly premium SSD I'm using?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1667150713980,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1068344\/why-am-i-being-charged-for-azure-machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-30T18:07:19.763Z",
                "Answer_score":1,
                "Answer_body":"Hi,\n\nI will suggest you to raise a support case with the billing team so they can assist you further over here\n\n\n\n\nHope this helps.\nJS\n\n==\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":32.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"charg context creat workspac upload dataset creat comput instanc comput cluster test shutdown haven workspac oct todai log charg daili appar reason cost manag show follow show supposedli premium ssd cost rememb get disk accord portal isn accord cost manag storag caus high cost resourc tag amlresourcetyp provision batch tri googl final check instanc remot idea charg cost manag sai charg storag sai resourc rid supposedli premium ssd",
        "Question_preprocessed_content":"charg context creat workspac upload dataset creat comput instanc comput cluster test shutdown haven workspac oct todai log charg daili appar reason cost manag show follow show supposedli premium ssd cost rememb get disk accord portal isn accord cost manag storag caus high cost resourc tag amlresourcetyp tri googl final check instanc remot idea charg cost manag sai charg storag sai resourc rid supposedli premium ssd",
        "Question_gpt_summary_original":"The user created an Azure Machine Learning workspace and uploaded a small dataset, but has been charged around $0.40 daily for no apparent reason. The cost management shows that the user is supposedly using a premium SSD, which is what's costing the most, but the user doesn't remember getting such a disk. The cost management also shows that it's not storage that's causing the high cost, but the Azure ML resource, specifically the provisioner.batch tag. The user checked the computer instances but everything seems okay. The user is seeking help to understand why they are being charged and how to get rid of the supposedly premium SSD.",
        "Question_gpt_summary":"user creat workspac upload small dataset charg daili appar reason cost manag show user supposedli premium ssd cost user rememb get disk cost manag show storag caus high cost resourc specif provision batch tag user check instanc okai user seek help understand charg rid supposedli premium ssd",
        "Answer_original_content":"suggest rais support case bill team assist hope help accept answer inform help help commun",
        "Answer_preprocessed_content":"suggest rais support case bill team assist hope help accept answer inform help help commun",
        "Answer_gpt_summary_original":"possible solution: the user should raise a support case with the billing team to get further assistance with understanding the charges for premium ssd and amlresourcetype: provisioner.batch resource despite having all their compute instances shutdown.",
        "Answer_gpt_summary":"possibl solut user rais support case bill team assist understand charg premium ssd amlresourcetyp provision batch resourc despit have comput instanc shutdown"
    },
    {
        "Question_id":null,
        "Question_title":"Training a TensorFlow model in Azure ML",
        "Question_body":"I am following the link below for training a TensorFlow model in Azure ML:\n\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/ml-frameworks\/tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb\n\nHowever, as my training dataset is in a container named \"sample-datasets\" in ADLS Gen2, I changed the following code (in the above link) to refer to the paths in my data lake. So I replaced code A (in the link above) with code B (my code)\n\nCode A:\n\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/train-labels-idx1-ubyte.gz',\nfilename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/azureopendatastorage.blob.core.windows.net\/mnist\/t10k-labels-idx1-ubyte.gz',\nfilename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))\n\n\n\n\nCode B:\n\nfrom azureml.core.dataset import Dataset\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\nurllib.request.urlretrieve('https:\/\/lakehousestgenrichedzone.dfs.core.windows.net\/sample-datasets\/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))\n\nBut I receive the following error:\n\nHTTPError: HTTP Error 401: Server failed to authenticate the request. Please refer to the information in the www-authenticate header.\n\nCan you please let me know how I can train the model using my data which are stored in the data lake? More precisely, how my Python code can copy the training dataset from my data lake into data_folder?\n\nPS: Please note that I have already granted the Blob Storage data Contributor role on my data lake storage account to my Azure ML workspace as a managed identity.",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1649367124903,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"anonymous user I have not worked on ADLS scenarios with Azure ML but I have added the ADLS tag to this thread for others to chip in and add their views.\n\nBased on the documentation for ADLS REST API it supports Azure Active Directory (Azure AD), Shared Key, and shared access signature (SAS) authorization with the APIs that are available to download the files from its storage. So, I think a direct download might not work in this case without authentication.\n\nI think the easiest way to get your files locally from ADLS is to use the python SDK to authenticate using account key or AD as listed here.\n\nIf you have many files that needs to be downloaded and referenced in your ML experiments then you may also consider to use the import data module of designer for designer experiments or register them as dataset from dataset tab of ml.azure.com which can also be referenced using the Azure ML SDK.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/804968\/training-a-tensorflow-model-in-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-08T12:21:38.973Z",
                "Answer_score":0,
                "Answer_body":"anonymous user I have not worked on ADLS scenarios with Azure ML but I have added the ADLS tag to this thread for others to chip in and add their views.\n\nBased on the documentation for ADLS REST API it supports Azure Active Directory (Azure AD), Shared Key, and shared access signature (SAS) authorization with the APIs that are available to download the files from its storage. So, I think a direct download might not work in this case without authentication.\n\nI think the easiest way to get your files locally from ADLS is to use the python SDK to authenticate using account key or AD as listed here.\n\nIf you have many files that needs to be downloaded and referenced in your ML experiments then you may also consider to use the import data module of designer for designer experiments or register them as dataset from dataset tab of ml.azure.com which can also be referenced using the Azure ML SDK.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-04-19T15:42:44.427Z",
                "Answer_score":0,
                "Answer_body":"I solved the problem by assigning an user-assigned managed identity to the target compute to access my ASDLS Gen2",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649420498972,
        "Question_original_content":"train tensorflow model follow link train tensorflow model http github com azur machinelearningnotebook blob master us framework tensorflow train hyperparamet tune deploi tensorflow train hyperparamet tune deploi tensorflow ipynb train dataset contain name sampl dataset adl gen chang follow code link refer path data lake replac code link code code code urllib request urlretriev http azureopendatastorag blob core window net mnist train imag idx ubyt filenam path join data folder train imag idx ubyt urllib request urlretriev http azureopendatastorag blob core window net mnist train label idx ubyt filenam path join data folder train label idx ubyt urllib request urlretriev http azureopendatastorag blob core window net mnist imag idx ubyt filenam path join data folder imag idx ubyt urllib request urlretriev http azureopendatastorag blob core window net mnist label idx ubyt filenam path join data folder label idx ubyt code core dataset import dataset urllib request urlretriev http lakehousestgenrichedzon df core window net sampl dataset train imag idx ubyt filenam path join data folder train imag idx ubyt urllib request urlretriev http lakehousestgenrichedzon df core window net sampl dataset train label idx ubyt filenam path join data folder train label idx ubyt urllib request urlretriev http lakehousestgenrichedzon df core window net sampl dataset imag idx ubyt filenam path join data folder imag idx ubyt urllib request urlretriev http lakehousestgenrichedzon df core window net sampl dataset label idx ubyt filenam path join data folder label idx ubyt receiv follow error httperror http error server fail authent request refer inform authent header let know train model data store data lake precis python code copi train dataset data lake data folder note grant blob storag data contributor role data lake storag account workspac manag ident",
        "Question_preprocessed_content":"train tensorflow model follow link train tensorflow model train dataset contain name adl gen chang follow code refer path data lake replac code code code code import dataset receiv follow error httperror http error server fail authent request refer inform header let know train model data store data lake precis python code copi train dataset data lake note grant blob storag data contributor role data lake storag account workspac manag ident",
        "Question_gpt_summary_original":"The user is encountering an error while trying to train a TensorFlow model in Azure ML using their own training dataset stored in a container named \"sample-datasets\" in ADLS Gen2. They have modified the code to refer to the paths in their data lake, but they are receiving an HTTPError 401: Server failed to authenticate the request. They are seeking assistance on how to copy the training dataset from their data lake into the data_folder using Python code. The user has already granted the Blob Storage data Contributor role on their data lake storage account to their Azure ML workspace as a managed identity.",
        "Question_gpt_summary":"user encount error try train tensorflow model train dataset store contain name sampl dataset adl gen modifi code refer path data lake receiv httperror server fail authent request seek assist copi train dataset data lake data folder python code user grant blob storag data contributor role data lake storag account workspac manag ident",
        "Answer_original_content":"anonym user work adl scenario ad adl tag thread chip add view base document adl rest api support azur activ directori azur share kei share access signatur sa author api avail download file storag think direct download work case authent think easiest wai file local adl us python sdk authent account kei list file need download referenc experi consid us import data modul design design experi regist dataset dataset tab azur com referenc sdk answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"anonym user work adl scenario ad adl tag thread chip add view base document adl rest api support azur activ directori share kei share access signatur author api avail download file storag think direct download work case authent think easiest wai file local adl us python sdk authent account kei list file need download referenc experi consid us import data modul design design experi regist dataset dataset tab referenc sdk answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"possible solutions to the challenge of training a tensorflow model using data stored in a data lake and receiving an authentication error when attempting to copy the training dataset into the data folder are: using the python sdk to authenticate using account key or azure active directory (ad), using the import data module of designer for designer experiments, or registering the files as a dataset from the dataset tab of ml.azure.com which can also be referenced using the sdk.",
        "Answer_gpt_summary":"possibl solut challeng train tensorflow model data store data lake receiv authent error attempt copi train dataset data folder python sdk authent account kei azur activ directori import data modul design design experi regist file dataset dataset tab azur com referenc sdk"
    },
    {
        "Question_id":null,
        "Question_title":"File Dataset not supported in Automated ML",
        "Question_body":"Hi Team,\n\nI'm trying to select a File dataset (file from Power BI) in Automated ML for a Regressive Machine Learning Model. However, Machine learning studio is displaying the dataset name under unsupported dataset (Screenshot attached). I wanted to know why the Power BI file is not being supported in the Automated ML? Additionally, I wanted to know what is the way I can upload the Power BI file (.pbix extension) in Machine learning studio to train the Regressive machine learning model?\n\nPlease let me know if you need any details from me.\n\nRegards,\nAmbarish",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1632158977537,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/559228\/file-dataset-not-supported-in-automated-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-22T03:55:38.717Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. Currently, AutoML only supports TabularDatasets, so the dataset type should default to Tabular. You can create TabularDatasets from .csv, .tsv, .parquet, .jsonl files, and from SQL query results. You may need to convert the Power BI file to a supported file format. Hope this helps!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"file dataset support autom team try select file dataset file power autom regress machin learn model machin learn studio displai dataset unsupport dataset screenshot attach want know power file support autom addition want know wai upload power file pbix extens machin learn studio train regress machin learn model let know need detail regard ambarish",
        "Question_preprocessed_content":"file dataset support autom team try select file dataset autom regress machin learn model machin learn studio displai dataset unsupport dataset want know power file support autom addition want know wai upload power file machin learn studio train regress machin learn model let know need detail regard ambarish",
        "Question_gpt_summary_original":"The user is facing a challenge with selecting a File dataset from Power BI in Automated ML for a Regressive Machine Learning Model. The dataset is being displayed as unsupported in Machine Learning Studio. The user is seeking clarification on why the Power BI file is not supported and how to upload it in Machine Learning Studio to train the model.",
        "Question_gpt_summary":"user face challeng select file dataset power autom regress machin learn model dataset displai unsupport machin learn studio user seek clarif power file support upload machin learn studio train model",
        "Answer_original_content":"thank reach current automl support tabulardataset dataset type default tabular creat tabulardataset csv tsv parquet jsonl file sql queri result need convert power file support file format hope help",
        "Answer_preprocessed_content":"thank reach current automl support tabulardataset dataset type default tabular creat tabulardataset csv tsv parquet jsonl file sql queri result need convert power file support file format hope help",
        "Answer_gpt_summary_original":"possible solutions to the challenge of uploading a power bi file (.pbix extension) to machine learning studio in order to train a regressive machine learning model are: \n- convert the power bi file to a supported file format such as .csv, .tsv, .parquet, or .jsonl\n- create a tabular dataset from the converted file using the supported file formats or from sql query results.",
        "Answer_gpt_summary":"possibl solut challeng upload power file pbix extens machin learn studio order train regress machin learn model convert power file support file format csv tsv parquet jsonl creat tabular dataset convert file support file format sql queri result"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning Exit Code 143",
        "Question_body":"Hi There,\nI am running a very simple pipeline that contains a dataset and a SQL transformation task. When i run the two tasks i get an error : 2021\/09\/07 17:49:47 Wrapper cmd failed with err: exit status 143 which i can't seem to find anywhere. I am running a compute VM DS1.\nany direction?\nThanks,",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631037752020,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Incase anyone is wondering, you must increase the compute with more memory to avoid this...",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/543071\/azure-machine-learning-exit-code-143.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-07T18:40:59.28Z",
                "Answer_score":1,
                "Answer_body":"Incase anyone is wondering, you must increase the compute with more memory to avoid this...",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1631040059280,
        "Question_original_content":"exit code run simpl pipelin contain dataset sql transform task run task error wrapper cmd fail err exit statu run comput direct thank",
        "Question_preprocessed_content":"exit code run simpl pipelin contain dataset sql transform task run task error wrapper cmd fail err exit statu run comput direct thank",
        "Question_gpt_summary_original":"The user is encountering an error with exit code 143 while running a simple pipeline containing a dataset and a SQL transformation task on Azure Machine Learning. The error message is not familiar to the user, and they are seeking guidance to resolve the issue. The user is running a compute VM DS1.",
        "Question_gpt_summary":"user encount error exit code run simpl pipelin contain dataset sql transform task error messag familiar user seek guidanc resolv issu user run comput",
        "Answer_original_content":"incas wonder increas comput memori avoid",
        "Answer_preprocessed_content":"incas wonder increas comput memori avoid",
        "Answer_gpt_summary_original":"the solution to avoid encountering error with exit code 143 while running a simple pipeline containing a dataset and a sql transformation task on a compute vm ds1 is to increase the compute with more memory.",
        "Answer_gpt_summary":"solut avoid encount error exit code run simpl pipelin contain dataset sql transform task comput increas comput memori"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker Studio - Jupyter proxy function does not work",
        "Question_body":"I am trying to run TensorBoard 2.9.0 in SageMaker Studio, as described here.\n\nHowever, after launching TensorBoard, when I navigate to the proxy URL https:\/\/<DOMAIN>.studio.<REGION>.sagemaker.aws\/jupyter\/default\/proxy\/6006\/, I get error 500.\n\nI am using the \"PyTorch 1.10 Python 3.8 CPU Optimized\" image. Any suggestions?\n\nI have also tried with the new JupyterLab 3 server version, but same result.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1654087666657,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":44.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuptYf-6wTNmhZaNCJWRhRA\/sage-maker-studio-jupyter-proxy-function-does-not-work",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-01T14:27:14.708Z",
                "Answer_score":0,
                "Answer_body":"I managed to reproduce the error 500 by not starting Tensorboard. So, please, make sure you executed in Studio's terminal the command to launch Tensorboard first: tensorboard --logdir path\/to\/your\/logs\n\nYou need to see something like:\n\nTensorFlow installation not found - running with reduced feature set.\n\nNOTE: Using experimental fast data loading logic. To disable, pass\n    \"--load_fast=false\" and report issues on GitHub. More details:\n    https:\/\/github.com\/tensorflow\/tensorboard\/issues\/4784\n\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.9.1 at http:\/\/localhost:6006\/ (Press CTRL+C to quit)\n\n\n\nThen, after that you can open a new tab and try to open it.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"studio jupyt proxi function work try run tensorboard studio describ launch tensorboard navig proxi url http studio aw jupyt default proxi error pytorch python cpu optim imag suggest tri new jupyterlab server version result",
        "Question_preprocessed_content":"studio jupyt proxi function work try run tensorboard studio describ launch tensorboard navig proxi url error pytorch python cpu optim imag suggest tri new jupyterlab server version result",
        "Question_gpt_summary_original":"The user is facing challenges while trying to run TensorBoard 2.9.0 in SageMaker Studio. The Jupyter proxy function is not working, and the user is getting an error 500 when navigating to the proxy URL. The user has tried using both the \"PyTorch 1.10 Python 3.8 CPU Optimized\" image and the new JupyterLab 3 server version, but the problem persists.",
        "Question_gpt_summary":"user face challeng try run tensorboard studio jupyt proxi function work user get error navig proxi url user tri pytorch python cpu optim imag new jupyterlab server version problem persist",
        "Answer_original_content":"manag reproduc error start tensorboard sure execut studio termin command launch tensorboard tensorboard logdir path log need like tensorflow instal run reduc featur set note experiment fast data load logic disabl pass load fast fals report issu github detail http github com tensorflow tensorboard issu serv tensorboard localhost expos network us proxi pass bind tensorboard http localhost press ctrl quit open new tab try open",
        "Answer_preprocessed_content":"manag reproduc error start tensorboard sure execut studio termin command launch tensorboard tensorboard need like tensorflow instal run reduc featur set note experiment fast data load logic disabl pass report issu github detail serv tensorboard localhost expos network us proxi pass tensorboard open new tab try open",
        "Answer_gpt_summary_original":"the solution to the issue of jupyter proxy function not working when running tensorboard 2.9.0 in studio is to first ensure that tensorboard is launched using the command \"tensorboard --logdir path\/to\/your\/logs\" in studio's terminal. after launching tensorboard, it should be served on localhost and can be accessed using a proxy or by passing \"--bind_all\".",
        "Answer_gpt_summary":"solut issu jupyt proxi function work run tensorboard studio ensur tensorboard launch command tensorboard logdir path log studio termin launch tensorboard serv localhost access proxi pass bind"
    },
    {
        "Question_id":null,
        "Question_title":"How to avoid data duplication between cache and workspace",
        "Question_body":"<p>I am using DVC on macOS. According to the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\" rel=\"noopener nofollow ugc\">documentation<\/a>, DVC won\u2019t save the same file twice (one in the workspace and one in the cache).<\/p>\n<p><code>In order to have the files present in both directories without duplication, DVC can automatically create **file links** to the cached data in the workspace. In fact, by default it will attempt to use reflinks* if supported by the file system.<\/code><\/p>\n<p>However, for me, it keeps two copies of the same file, thereby increasing the disk space significantly. For example, if I have a file of around 5GB, the disk space for that project goes to around 10GB. Here is the output of the <code>dvc version<\/code> command. According to the docs, the reflinks are supported by macOS and can be seen from dvc version output then why isn\u2019t it working in my case? Any ideas on what I am missing here?<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.18.0 (brew)\n---------------------------------\nPlatform: Python 3.10.6 on macOS-12.5.1-x86_64-i386-64bit\nSupports:\n        azure (adlfs = 2022.7.0, knack = 0.9.0, azure-identity = 1.10.0),\n        gdrive (pydrive2 = 1.14.0),\n        gs (gcsfs = 2022.7.1),\n        webhdfs (fsspec = 2022.7.1),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.8.3),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.8.3),\n        s3 (s3fs = 2022.7.1, boto3 = 1.21.21),\n        ssh (sshfs = 2022.6.0),\n        oss (ossfs = 2021.8.0),\n        webdav (webdav4 = 0.9.7),\n        webdavs (webdav4 = 0.9.7)\nCache types: reflink, hardlink, symlink\nCache directory: apfs on \/dev\/disk1s5s1\nCaches: local\nRemotes: s3\nWorkspace directory: apfs on \/dev\/disk1s5s1\nRepo: dvc (subdir), git\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1663141540288,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":171.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-avoid-data-duplication-between-cache-and-workspace\/1340",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-22T10:48:45.591Z",
                "Answer_body":"<p>I have the same issue using Ubuntu.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-22T12:05:02.407Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/drsb\">@drsb<\/a> <a class=\"mention\" href=\"\/u\/lcs72\">@lcs72<\/a><br>\nI am afraid that the problem is with the system rather than with DVC.<br>\nLet me explain that:<br>\nhere is a <code>test_links.sh<\/code> script that will help you understand:<\/p>\n<pre><code class=\"lang-auto\">#!\/bin\/bash\n\nset -xu\npushd $TMPDIR\n\nwsp=test_wspace\nrep=test_repo\n\nrm -rf $wsp &amp;&amp; mkdir $wsp &amp;&amp; pushd $wsp\nmain=$(pwd)\n\nmkdir $rep &amp;&amp; pushd $rep\n\ndd if=\/dev\/urandom of=data bs=1M count=1\n\nsleep 1\n\ncp data data_copy\n\nsleep 1\npython -c \"from dvc.fs import system;system.symlink('data', 'data_symlink')\"\n\nsleep 1\npython -c \"from dvc.fs import system;system.hardlink('data', 'data_hardlink')\"\n\nsleep 1\npython -c \"from dvc.fs import system;system.reflink('data', 'data_reflink')\"\n\nstat -f \"inode: %i access: %a modification: %m changed: %c birth: %B %N\" data data_copy data_symlink data_hardlink data_reflink\n\n\ndu -hd1\nls -alh\n<\/code><\/pre>\n<p>The result:<\/p>\n<pre><code class=\"lang-bash\">+ stat -f 'inode: %i access: %a modification: %m changed: %c birth: %B %N' data data_copy data_symlink data_hardlink data_reflink\ninode: 139026482 access: 1663847851 modification: 1663847851 changed: 1663847855 birth: 1663847851 data\ninode: 139026485 access: 1663847852 modification: 1663847852 changed: 1663847852 birth: 1663847852 data_copy\ninode: 139026488 access: 1663847853 modification: 1663847853 changed: 1663847853 birth: 1663847853 data_symlink\ninode: 139026482 access: 1663847851 modification: 1663847851 changed: 1663847855 birth: 1663847851 data_hardlink\ninode: 139026495 access: 1663847851 modification: 1663847851 changed: 1663847856 birth: 1663847851 data_reflink\n+ du -hd1\n3.0M    .\n+ ls -alh\ntotal 8192\ndrwxr-xr-x  7 pawelredzynski  staff   224B Sep 22 13:57 .\ndrwxr-xr-x  3 pawelredzynski  staff    96B Sep 22 13:57 ..\n-rw-r--r--  2 pawelredzynski  staff   1.0M Sep 22 13:57 data\n-rw-r--r--  1 pawelredzynski  staff   1.0M Sep 22 13:57 data_copy\n-rw-r--r--  2 pawelredzynski  staff   1.0M Sep 22 13:57 data_hardlink\n-rw-r--r--  1 pawelredzynski  staff   1.0M Sep 22 13:57 data_reflink\nlrwxr-xr-x  1 pawelredzynski  staff     4B Sep 22 13:57 data_symlink -&gt; data\n<\/code><\/pre>\n<p>We create 1MB file and try out different link types.<br>\nAs we can see all of them has been created in different second (that\u2019s why I put sleep there).<br>\nLets focus on reflink - looking at stat, it yields that only inode and <code>changed<\/code> time is different than <code>data<\/code> - its <code>birthtime<\/code> is from before we actually created the reflink - that\u2019s because its a reflink. The problem that you are facing is that system doesn\u2019t have a simple way of summarizing that - hence we get <code>3MB<\/code> in <code>du -hd1<\/code> - 1 MB for <code>data\/data_copy\/data_reflink<\/code> (in ls -alh you can see that symlink takes only 4B). If you want to check that hardlink does not count, just comment out its creation - <code>du<\/code> result will not change.<\/p>\n<p>To summarize - if you want your system to pick up the fact that you are using links you would need to go for hardlinks or symlinks. To be sure that DVC will first try all links before going for copy you can run <code>dvc config cache.type \"reflink,symlink,hardlink,copy\"<\/code>.<\/p>",
                "Answer_score":11.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"avoid data duplic cach workspac maco accord document wont save file twice workspac cach order file present directori duplic automat creat file link cach data workspac fact default attempt us reflink support file keep copi file increas disk space significantli exampl file disk space project goe output version command accord doc reflink support maco seen version output isnt work case idea miss version brew platform python maco bit support azur adlf knack azur ident gdrive pydriv gcsf webhdf fsspec http aiohttp aiohttp retri http aiohttp aiohttp retri sf boto ssh sshf oss ossf webdav webdav webdav webdav cach type reflink hardlink symlink cach directori apf dev diskss cach local remot workspac directori apf dev diskss repo subdir git",
        "Question_preprocessed_content":"avoid data duplic cach workspac maco accord document wont save file twice keep copi file increas disk space significantli exampl file disk space project goe output command accord doc reflink support maco seen version output isnt work case idea miss",
        "Question_gpt_summary_original":"The user is encountering challenges with DVC on macOS as it is not saving the same file twice, but it keeps two copies of the same file, thereby increasing the disk space significantly. The user is seeking ideas on what they are missing here as the reflinks are supported by macOS and can be seen from the dvc version output.",
        "Question_gpt_summary":"user encount challeng maco save file twice keep copi file increas disk space significantli user seek idea miss reflink support maco seen version output",
        "Answer_original_content":"issu ubuntu drsb lc afraid problem let explain test link script help understand bin bash set pushd tmpdir wsp test wspace rep test repo wsp mkdir wsp pushd wsp main pwd mkdir rep pushd rep dev urandom data count sleep data data copi sleep python import symlink data data symlink sleep python import hardlink data data hardlink sleep python import reflink data data reflink stat inod access modif chang birth data data copi data symlink data hardlink data reflink alh result stat inod access modif chang birth data data copi data symlink data hardlink data reflink inod access modif chang birth data inod access modif chang birth data copi inod access modif chang birth data symlink inod access modif chang birth data hardlink inod access modif chang birth data reflink alh total drwxr pawelredzynski staff sep drwxr pawelredzynski staff sep pawelredzynski staff sep data pawelredzynski staff sep data copi pawelredzynski staff sep data hardlink pawelredzynski staff sep data reflink lrwxr pawelredzynski staff sep data symlink data creat file try differ link type creat differ second that sleep let focu reflink look stat yield inod chang time differ data birthtim actual creat reflink that reflink problem face doesnt simpl wai summar data data copi data reflink alh symlink take want check hardlink count comment creation result chang summar want pick fact link need hardlink symlink sure try link go copi run config cach type reflink symlink hardlink copi",
        "Answer_preprocessed_content":"issu ubuntu afraid problem let explain script help understand result creat file try differ link type creat differ second let focu reflink look stat yield inod time differ actual creat reflink that reflink problem face doesnt simpl wai summar want check hardlink count comment creation result chang summar want pick fact link need hardlink symlink sure try link go copi run",
        "Answer_gpt_summary_original":"possible solutions to the challenge of data duplication between cache and workspace due to reflinks not working include using hardlinks or symlinks instead of reflinks, and configuring the cache type to try all links before resorting to copying. the issue seems to be with the system rather than the code.",
        "Answer_gpt_summary":"possibl solut challeng data duplic cach workspac reflink work includ hardlink symlink instead reflink configur cach type try link resort copi issu code"
    },
    {
        "Question_id":70513398.0,
        "Question_title":"I can not install \"git-lfs\" on aws sagemaker notebook instance",
        "Question_body":"<p>I Can not run <code>apt to install git-lfs<\/code> on sagemaker notebook instance. I want to run git commands in my notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640732204667,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":489.0,
        "Answer_body":"<p>use the following commands to install git-lfs<\/p>\n<pre><code>!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\n\n!sudo yum install git-lfs -y\n\n!git lfs install\n<\/code><\/pre>\n<p>that should make it work<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70513398",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1640732355180,
        "Question_original_content":"instal git lf notebook instanc run apt instal git lf notebook instanc want run git command notebook",
        "Question_preprocessed_content":"instal notebook instanc run notebook instanc want run git command notebook",
        "Question_gpt_summary_original":"The user is facing a challenge in installing \"git-lfs\" on their AWS Sagemaker notebook instance, which is preventing them from running git commands in their notebook.",
        "Question_gpt_summary":"user face challeng instal git lf notebook instanc prevent run git command notebook",
        "Answer_original_content":"us follow command instal git lf curl http packagecloud instal repositori github git lf script rpm sudo bash sudo yum instal git lf git lf instal work",
        "Answer_preprocessed_content":"us follow command instal work",
        "Answer_gpt_summary_original":"to solve the problem of being unable to install \"git-lfs\" and run git commands in a notebook instance, the user can use the following commands to install git-lfs: \"!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.rpm.sh | sudo bash\", \"!sudo yum install git-lfs -y\", and \"!git lfs install\".",
        "Answer_gpt_summary":"solv problem unabl instal git lf run git command notebook instanc user us follow command instal git lf curl http packagecloud instal repositori github git lf script rpm sudo bash sudo yum instal git lf git lf instal"
    },
    {
        "Question_id":58815367.0,
        "Question_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Question_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1573549904240,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1573550204732,
        "Question_score":1.0,
        "Question_view_count":4605.0,
        "Answer_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573630807523,
        "Question_original_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file amazon codeset ann nginx conf predictor serv train wsgi dockerfil open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint exampl fail reason primari contain product variant alltraff pass ping health check check cloudwatch log endpoint check log follow crit connect unix tmp gunicorn sock fail file directori connect upstream client server request ping http upstream http unix tmp gunicorn sock ping host model aw local traceback recent file usr local bin serv line sy exit main file usr local lib python dist packag contain cli serv line main server start env servingenv framework modul file usr local lib python dist packag contain server line start modul app file usr lib python subprocess line init eead errwrit file usr lib python subprocess line execut child rais child except tri deploi model file local model deploi successfulli insid aw face problem serv file code futur import print function import multiprocess import import signal import subprocess import sy cpu count multiprocess cpu count model server timeout environ model server timeout model server worker int environ model server worker cpu count def sigterm handler nginx pid gunicorn pid try kill nginx pid signal sigquit oserror pass try kill gunicorn pid signal sigterm oserror pass sy exit def start server print start infer server worker format model server worker link log stream stdout err log contain log subprocess check dev stdout var log nginx access log subprocess check dev stderr var log nginx error log nginx subprocess popen nginx opt code nginx conf gunicorn subprocess popen gunicorn timeout str model server timeout unix tmp gunicorn sock str model server worker wsgi app signal signal signal sigterm lambda sigterm handler nginx pid gunicorn pid subprocess exit pid set nginx pid gunicorn pid true pid wait pid pid break sigterm handler nginx pid gunicorn pid print infer server exit main routin invok start function main start server deploi model follow predictor classifi deploi medium serial csv serial kindli let know mistak deploi",
        "Question_preprocessed_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint fail reason primari contain product variant alltraff pass ping health check check cloudwatch log check log follow connect fail connect upstream client server request upstream host traceback file line file line main file line start file line init eead errwrit file line rais tri deploi model file local model deploi successfulli insid aw face problem serv file code deploi model follow predictor kindli let know mistak deploi",
        "Question_gpt_summary_original":"The user is facing an error while deploying a custom Keras model in AWS Sagemaker. The training is done correctly, but while deploying, the primary container for production variant AllTraffic did not pass the ping health check. The user checked the logs and found that the connection to unix:\/tmp\/gunicorn.sock failed, and the subprocess failed to execute child. The user tried to deploy the same model in AWS Sagemaker with the same files on their local computer, and the model was deployed successfully, but inside AWS, they are facing this problem. The user has provided the serve file code and the deployment code used.",
        "Question_gpt_summary":"user face error deploi custom kera model train correctli deploi primari contain product variant alltraff pass ping health check user check log connect unix tmp gunicorn sock fail subprocess fail execut child user tri deploi model file local model deploi successfulli insid aw face problem user provid serv file code deploy code",
        "Answer_original_content":"script mode simpler deal contain nginx low level stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet http github com aw sampl amazon script mode blob master sentiment script mode sentiment analysi ipynb",
        "Answer_preprocessed_content":"script mode simpler deal contain nginx stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet",
        "Answer_gpt_summary_original":"the possible solution suggested in the answer is to use script mode instead of dealing with container and nginx low-level stuff. the user can use training scripts similar to those used outside with prebuilt containers for various deep learning frameworks such as tensorflow, pytorch, and apache mxnet. the link to the github repository for an example of using script mode with a custom keras model is also provided.",
        "Answer_gpt_summary":"possibl solut suggest answer us script mode instead deal contain nginx low level stuff user us train script similar outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet link github repositori exampl script mode custom kera model provid"
    },
    {
        "Question_id":null,
        "Question_title":"Error while trying to get explanation from (custom container) model deployed on Vertex AI",
        "Question_body":"Hi,I created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get explainable predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.Please take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.https:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container(Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key.)",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1658799480000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":166.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-while-trying-to-get-explanation-from-custom-container\/td-p\/446817\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-26T01:38:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nI created a custom docker container to deploy my model on Vertex AI. The model uses LightGBM, so I can't use the pre-built container images available for TF\/SKL\/XGBoost. I was able to deploy the model and get predictions, but I get errors while trying to get\u00a0explainable\u00a0predictions from the model. I have tried to follow the Vertex AI guidelines to configure the model for explanations.\nThe example below shows a simplified version of the model that still reproduces the issue, with only two input features 'A' and 'B'.\n\nPlease take a look and tell me if the explanation metadata is supposed to be set differently, or if there is something wrong with this approach.\n\nEnvironment details\nGoogle Cloud Notebook\nPython version: 3.7.12\npip version: 21.3.1\ngoogle-cloud-aiplatform\u00a0version: 1.15.0\nReference\n\nhttps:\/\/cloud.google.com\/vertex-ai\/docs\/explainable-ai\/configuring-explanations#custom-container\n\nexplanation-metadata.json\n\n(Model output is unkeyed. The Vertex AI guide suggests using any memorable string for output key.)\n\n{\n    \"inputs\": {\n        \"A\": {},\n        \"B\": {}\n    },\n    \"outputs\": {\n        \"Y\": {}\n    }\n}\nModel upload with explanation parameters and metadata\n! gcloud ai models upload \\\n  --region=$REGION \\\n  --display-name=$MODEL_NAME \\\n  --container-image-uri=$PRED_IMAGE_URI \\\n  --artifact-uri=$ARTIFACT_LOCATION_GCS \\\n  --explanation-method=sampled-shapley \\\n  --explanation-path-count=10 \\\n  --explanation-metadata-file=explanation-metadata.json\nPrediction\/Explanation Input\ninstances = [{\"A\": 1.1, \"B\": 20}, {\"A\": 2.2, \"B\": 21}]\n# Prediction (works fine):\nendpoint.predict(instances=instances)\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\nendpoint.explain(instances=instances) # Returns error (1) shown in stack trace below\n\n# Another example\ninstances_2 = [[1.1,20], [2.2,21]]\n# Prediction (works fine):\nendpoint.predict(instances=instances_2)\n# Prediction output: predictions=[0, 1], deployed_model_id='<>', model_version_id='', model_resource_name='<>', explanations=None\nendpoint.explain(instances=instances_2) # Returns error\n# Error: Nameless inputs are allowed only if there is a single input in the explanation metadata.\nPrediction Server (Flask)\n# Custom Flask server to serve online predictions\n# Input for prediction\nraw_input = request.get_json()\ninput = raw_input['instances']\ndf = pd.DataFrame(input, columns = ['A', 'B'])\n# Prediction from model (loaded from GCP bucket)\npredictions = model.predict(df).tolist() # [0, 1]\nresponse = jsonify({\"predictions\": predictions})\nreturn response\nStack trace of error (1)\n---------------------------------------------------------------------------\n_InactiveRpcError                         Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     49         try:\n---> 50             return callable_(*args, **kwargs)\n     51         except grpc.RpcError as exc:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in __call__(self, request, timeout, metadata, credentials, wait_for_ready, compression)\n    945                                       wait_for_ready, compression)\n--> 946         return _end_unary_response_blocking(state, call, False, None)\n    947 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_channel.py in _end_unary_response_blocking(state, call, with_call, deadline)\n    848     else:\n--> 849         raise _InactiveRpcError(state)\n    850 \n\n_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\"\n\tdebug_error_string = \"{\"created\":\"@1658310559.755090975\",\"description\":\"Error received from peer ipv4:74.125.133.95:443\",\"file\":\"src\/core\/lib\/surface\/call.cc\",\"file_line\":1069,\"grpc_message\":\"{\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\",\"grpc_status\":3}\"\n>\n\nThe above exception was the direct cause of the following exception:\n\nInvalidArgument                           Traceback (most recent call last)\n\/tmp\/ipykernel_2590\/4024017963.py in <module>\n----> 3 print(endpoint.explain(instances=instances, parameters={}))\n\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/models.py in explain(self, instances, parameters, deployed_model_id, timeout)\n   1563             parameters=parameters,\n   1564             deployed_model_id=deployed_model_id,\n-> 1565             timeout=timeout,\n   1566         )\n   1567 \n\n~\/.local\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform_v1\/services\/prediction_service\/client.py in explain(self, request, endpoint, instances, parameters, deployed_model_id, retry, timeout, metadata)\n    917             retry=retry,\n    918             timeout=timeout,\n--> 919             metadata=metadata,\n    920         )\n    921 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method.py in __call__(self, timeout, retry, *args, **kwargs)\n    152             kwargs[\"metadata\"] = metadata\n    153 \n--> 154         return wrapped_func(*args, **kwargs)\n    155 \n    156 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers.py in error_remapped_callable(*args, **kwargs)\n     50             return callable_(*args, **kwargs)\n     51         except grpc.RpcError as exc:\n---> 52             raise exceptions.from_grpc_error(exc) from exc\n     53 \n     54     return error_remapped_callable\n\nInvalidArgument: 400 {\"error\": \"Unable to explain the requested instance(s) because: Invalid response from prediction server - the response field predictions is missing. Response: {'error': '400 Bad Request: The browser (or proxy) sent a request that this server could not understand.'}\"}\n---------------------------------------------------------------------------\n# https:\/\/github.com\/googleapis\/python-aiplatform\/issues\/1526"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error try explan custom contain model deploi creat custom docker contain deploi model model us lightgbm us pre built contain imag avail skl xgboost abl deploi model predict error try explain predict model tri follow guidelin configur model explan exampl show simplifi version model reproduc issu input featur look tell explan metadata suppos set differ wrong approach http cloud googl com vertex doc explain configur explan custom contain model output unkei guid suggest memor string output kei",
        "Question_preprocessed_content":"error try explan model deploi creat custom docker contain deploi model model us lightgbm us contain imag avail abl deploi model predict error try explain predict model tri follow guidelin configur model explan exampl show simplifi version model reproduc issu input featur look tell explan metadata suppos set differ wrong output unkei guid suggest memor string output",
        "Question_gpt_summary_original":"The user created a custom docker container to deploy a model on Vertex AI that uses LightGBM. While they were able to deploy the model and get predictions, they encountered errors while trying to get explainable predictions from the model. They followed the Vertex AI guidelines to configure the model for explanations but still faced issues. The user provided a simplified version of the model that reproduces the issue. They are seeking guidance on whether the explanation metadata is supposed to be set differently or if there is something wrong with their approach.",
        "Question_gpt_summary":"user creat custom docker contain deploi model us lightgbm abl deploi model predict encount error try explain predict model follow guidelin configur model explan face issu user provid simplifi version model reproduc issu seek guidanc explan metadata suppos set differ wrong approach",
        "Answer_original_content":"creat custom docker contain deploi model model us lightgbm us pre built contain imag avail skl xgboost abl deploi model predict error try getexplainablepredict model tri follow guidelin configur model explan exampl show simplifi version model reproduc issu input featur look tell explan metadata suppos set differ wrong approach environ detail googl cloud notebook python version pip version googl cloud aiplatformvers refer http cloud googl com vertex doc explain configur explan custom contain explan metadata json model output unkei guid suggest memor string output kei input output model upload explan paramet metadata gcloud model upload region region displai model contain imag uri pred imag uri artifact uri artifact locat gc explan method sampl shaplei explan path count explan metadata file explan metadata json predict explan input instanc predict work fine endpoint predict instanc instanc predict output predict deploi model model version model resourc explan endpoint explain instanc instanc return error shown stack trace exampl instanc predict work fine endpoint predict instanc instanc predict output predict deploi model model version model resourc explan endpoint explain instanc instanc return error error nameless input allow singl input explan metadata predict server flask custom flask server serv onlin predict input predict raw input request json input raw input instanc datafram input column predict model load gcp bucket predict model predict tolist respons jsonifi predict predict return respons stack trace error inactiverpcerror traceback recent opt conda lib python site packag googl api core grpc helper error remap callabl arg kwarg try return callabl arg kwarg grpc rpcerror exc opt conda lib python site packag grpc channel self request timeout metadata credenti wait readi compress wait readi compress return end unari respons block state fals opt conda lib python site packag grpc channel end unari respons block state deadlin rais inactiverpcerror state inactiverpcerror except direct caus follow except invalidargu traceback recent tmp ipykernel print endpoint explain instanc instanc paramet local lib python site packag googl cloud aiplatform model explain self instanc paramet deploi model timeout paramet paramet deploi model deploi model timeout timeout local lib python site packag googl cloud aiplatform servic predict servic client explain self request endpoint instanc paramet deploi model retri timeout metadata retri retri timeout timeout metadata metadata opt conda lib python site packag googl api core gapic method self timeout retri arg kwarg kwarg metadata metadata return wrap func arg kwarg opt conda lib python site packag googl api core grpc helper error remap callabl arg kwarg return callabl arg kwarg grpc rpcerror exc rais except grpc error exc exc return error remap callabl invalidargu error unabl explain request instanc invalid respons predict server respons field predict miss respons error bad request browser proxi sent request server understand http github com googleapi python aiplatform issu",
        "Answer_preprocessed_content":"creat custom docker contain deploi model model us lightgbm us contain imag avail abl deploi model predict error try getexplainablepredict model tri follow guidelin configur model explan exampl show simplifi version model reproduc issu input featur look tell explan metadata suppos set differ wrong approach environ detail googl cloud notebook python version pip version refer model output unkei guid suggest memor string output input output model upload explan paramet metadata gcloud model upload input instanc predict predict output predict explan return error shown stack trace exampl predict predict output predict explan return error error nameless input allow singl input explan metadata predict server custom flask server serv onlin predict input predict input column predict model predict respons jsonifi return respons stack trace error traceback kwarg try return kwarg exc request timeout metadata credenti compress compress return fals deadlin rais except direct caus follow except invalidargu traceback paramet explain paramet paramet timeout timeout explain retri retri timeout timeout metadata metadata timeout retri arg kwarg kwarg metadata return kwarg kwarg return kwarg exc rais exc return invalidargu",
        "Answer_gpt_summary_original":"the user is encountering errors while trying to get explainable predictions from a custom container model deployed on . the user has tried to follow the guidelines to configure the model for explanations, but is still facing issues. the error message suggests that the response field \"predictions\" is missing. the user has provided a simplified version of the model that still reproduces the issue.",
        "Answer_gpt_summary":"user encount error try explain predict custom contain model deploi user tri follow guidelin configur model explan face issu error messag suggest respons field predict miss user provid simplifi version model reproduc issu"
    },
    {
        "Question_id":null,
        "Question_title":"Cannot create new compute instance",
        "Question_body":"I'm trying to create my first compute instance using Azure Machine Learning Studio, but it fails every time with little to no details.\n\nHere are the parameters I use to create a new instance:\n\n\nThis is what the instance looks like during creation:\n\nAs well as additional attributes\n\n\nIt runs like that for an hour or so, and then fails.\nThis is what the instance looks like after an error:\n\n\nI can also see another error message in the logs:\n\n\nCan I please get some help to create new compute instance. Everything looks pretty straight forward, I would definitely expect to have no errors there.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662003553540,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/989732\/cannot-create-new-compute-instance.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-01T05:41:36.307Z",
                "Answer_score":0,
                "Answer_body":"@gkozyrev I think the error in this case is specific to your subscription or request. I am able to create a new compute instance without any errors from the studio.\n\nIs this a managed account by a 3rd party with some restrictions? If Yes, you could reach out to the admin of the team to check if you have all permissions to create a compute instance.\n\nIf this is your personal account and since the details of the failure are not obvious, from the message you could report the failure screen shot of the request id by using the smiley icon on the top right-hand corner of the studio. This enables the service team who has access to your request details to contact you for any failures related to the service.\nYou also have the option to contact support team through Azure portal using the Help + Support blade for 1:1 support from a support team engineer.\n\nI hope this helps!!",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"creat new comput instanc try creat comput instanc studio fail time littl detail paramet us creat new instanc instanc look like creation addit attribut run like hour fail instanc look like error error messag log help creat new comput instanc look pretti straight forward definit expect error",
        "Question_preprocessed_content":"creat new comput instanc try creat comput instanc studio fail time littl detail paramet us creat new instanc instanc look like creation addit attribut run like hour fail instanc look like error error messag log help creat new comput instanc look pretti straight forward definit expect error",
        "Question_gpt_summary_original":"The user is facing challenges in creating a new compute instance using Azure Machine Learning Studio. The instance fails to create every time with little to no details provided. The user has tried creating the instance with various parameters and attributes, but it still fails after running for an hour or so. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user face challeng creat new comput instanc studio instanc fail creat time littl detail provid user tri creat instanc paramet attribut fail run hour user seek help resolv issu",
        "Answer_original_content":"gkozyrev think error case specif subscript request abl creat new comput instanc error studio manag account parti restrict ye reach admin team check permiss creat comput instanc person account detail failur obviou messag report failur screen shot request smilei icon right hand corner studio enabl servic team access request detail contact failur relat servic option contact support team azur portal help support blade support support team engin hope help",
        "Answer_preprocessed_content":"think error case specif subscript request abl creat new comput instanc error studio manag account parti restrict ye reach admin team check permiss creat comput instanc person account detail failur obviou messag report failur screen shot request smilei icon corner studio enabl servic team access request detail contact failur relat servic option contact support team azur portal help support blade support support team engin hope help",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. if the user's account is managed by a third party with restrictions, they should reach out to the admin of the team to check if they have all permissions to create a compute instance.\n2. if the user's account is a personal account and the details of the failure are not obvious, they could report the failure screenshot of the request id by using the smiley icon on the top right-hand corner of the studio. this enables the service team who has access to their request details to contact them for any failures related to the service.\n3. the user also has the option to contact the support team through the azure portal using the help + support blade for 1:1 support from a support team engineer.",
        "Answer_gpt_summary":"possibl solut answer user account manag parti restrict reach admin team check permiss creat comput instanc user account person account detail failur obviou report failur screenshot request smilei icon right hand corner studio enabl servic team access request detail contact failur relat servic user option contact support team azur portal help support blade support support team engin"
    },
    {
        "Question_id":null,
        "Question_title":"Logging scalars when running ray[tune] tuning fails in a guild run",
        "Question_body":"<p>In my project I have a bit of automatic tuning of my pytorch-lightning models using ray and then I also automatically apply the model. The logging that ray[tune] uses is a SummaryWriter from tensorboardX package. I am also using tensorboardX SummaryWriter for logging other things in my project. For my own logging, there is no issue with this, but for some reason guild fails with the calls to <code>add_scalar()<\/code> when it\u2019s called from the tune library.<\/p>\n<p>The trace:<\/p>\n<pre><code>3\/8\/2021 5:40:52 PM\nTraceback (most recent call last):\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 594, in _process_trial\n3\/8\/2021 5:40:52 PM\ndecision = self._process_trial_result(trial, result)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/trial_runner.py\", line 666, in _process_trial_result\n3\/8\/2021 5:40:52 PM\nself._callbacks.on_trial_result(\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/callback.py\", line 192, in on_trial_result\n3\/8\/2021 5:40:52 PM\ncallback.on_trial_result(**info)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/logger.py\", line 393, in on_trial_result\n3\/8\/2021 5:40:52 PM\nself.log_trial_result(iteration, trial, result)\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/ray\/tune\/logger.py\", line 631, in log_trial_result\n3\/8\/2021 5:40:52 PM\nself._trial_writer[trial].add_scalar(\n3\/8\/2021 5:40:52 PM\nFile \"\/home\/davina\/miniconda3\/envs\/ap\/lib\/python3.8\/site-packages\/guild\/python_util.py\", line 239, in wrapper\n3\/8\/2021 5:40:52 PM\ncb(wrapped_bound, *args, **kw)\n3\/8\/2021 5:40:52 PM\nTypeError: _handle_scalar() got an unexpected keyword argument 'global_step'\n<\/code><\/pre>\n<p>The line from tune in question in full is <code>self._trial_writer[trial].add_scalar(full_attr, value, global_step=step)<\/code>. This fails.<\/p>\n<p>In my own project I have the following line: <code> logger.add_scalar(f\"{prefix}\/{tag}\", scalar_value, global_step, walltime)<\/code> and this does not fail.<\/p>\n<p>So I went into the ray.tune library and I changed the call to <code>self._trial_writer[trial].add_scalar(full_attr, value, step)<\/code> and reran it. The failure went away.<\/p>\n<p>I dug into the <a href=\"https:\/\/github.com\/guildai\/guildai\/blob\/e9271824141583b96a6de7d1d5cebd44a04e43fe\/guild\/plugins\/summary_util.py#L181\" rel=\"noopener nofollow ugc\">github<\/a> source, and it looks like <code>_handle_scalar()<\/code> is expecting <code>step<\/code> and not <code>global_step<\/code>.<\/p>\n<p>I originally needed help with this but as I wrote this I ended up figuring out the answer. Looks like there\u2019s a potential bug here?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1615255994962,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":427.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/logging-scalars-when-running-ray-tune-tuning-fails-in-a-guild-run\/557",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-11T15:24:43.849Z",
                "Answer_body":"<p>Thanks for the report - and my apologies for the late reply!<\/p>\n<p>Taking a look at this now.<\/p>\n<p>In the meantime, I suggest disabling Guild\u2019s output scalar support if you\u2019re not using it. Generally if you\u2019re logging directly using tensorboardX or another TF summary logger, you can turn off output scalars as there\u2019s no point doing any extra work looking at the script output for summaries.<\/p>\n<pre><code class=\"lang-yaml\">op:\n  output-scalars: off\n<\/code><\/pre>\n<p>If you have a lot of operations that you need to configure, you can use the <code>operation-defaults<\/code> attribute of a model:<\/p>\n<pre><code>- model: ''  # or use the model name\n  operation-defaults:\n    output-scalars: off\n  operations:\n    op1: {}\n    op2: {}\n<\/code><\/pre>\n<p>I\u2019ll update here with my findings on the behavior you\u2019re seeing (a bug in Guild somewhere no doubt, just not sure what yet).<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-11T20:06:43.796Z",
                "Answer_body":"<p>Hi, no problem.<\/p>\n<p>Ah I think I had turned off output scalars for my base operation and had a bunch of other operations and assumed that it would percolate up for some reason. I\u2019ll ad that in to all my operations.<\/p>\n<p>Yeah I think that in your <code>_handle_scalars()<\/code> method, you\u2019ve named the argument <code>step<\/code> but tensorboardX calls it <code>global_step<\/code>, so if a call to tensorboardX uses the argument name when being passed in (e.g., <code>global_step=step<\/code>) it complains because the <code>_handle_scalars()<\/code> called it <code>step<\/code> instead of <code>global_step<\/code> so we get <code>_handle_scalar() got an unexpected keyword argument 'global_step'<\/code>. That\u2019s my guess, though I may be diagnosing something irrelevant haha<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-11T20:22:41.466Z",
                "Answer_body":"<p>Your assessment is right! I\u2019m cleaning these up now with some tests.<\/p>\n<p>In retrospect this is unrelated to output scalars so please ignore my earlier examples.<\/p>\n<p>If you have defined plugins for the operation in question, you should be able to fix this issue by removing that line or setting it to an empty list. Guild may be auto-detecting something there, so setting this explicitly may solve the issue:<\/p>\n<pre><code class=\"lang-yaml\">op:\n  plugins: []\n<\/code><\/pre>\n<p>I\u2019ll update here when a fix is ready.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"log scalar run rai tune tune fail run project bit automat tune pytorch lightn model rai automat appli model log rai tune us summarywrit tensorboardx packag tensorboardx summarywrit log thing project log issu reason fail call add scalar call tune librari trace traceback recent file home davina miniconda env lib python site packag rai tune trial runner line process trial decis self process trial result trial result file home davina miniconda env lib python site packag rai tune trial runner line process trial result self callback trial result file home davina miniconda env lib python site packag rai tune callback line trial result callback trial result info file home davina miniconda env lib python site packag rai tune logger line trial result self log trial result iter trial result file home davina miniconda env lib python site packag rai tune logger line log trial result self trial writer trial add scalar file home davina miniconda env lib python site packag python util line wrapper wrap bound arg typeerror handl scalar got unexpect keyword argument global step line tune question self trial writer trial add scalar attr valu global step step fail project follow line logger add scalar prefix tag scalar valu global step walltim fail went rai tune librari chang self trial writer trial add scalar attr valu step reran failur went awai dug github sourc look like handl scalar expect step global step origin need help wrote end figur answer look like there potenti bug",
        "Question_preprocessed_content":"log scalar run rai tune fail run project bit automat tune model rai automat appli model log rai us summarywrit tensorboardx packag tensorboardx summarywrit log thing project log issu reason fail call call tune librari trace line tune question fail project follow line fail went librari chang reran failur went awai dug github sourc look like expect origin need help wrote end figur answer look like there potenti bug",
        "Question_gpt_summary_original":"The user is encountering an issue with logging scalars when running ray[tune] tuning in a guild run. The logging that ray[tune] uses is a SummaryWriter from tensorboardX package, and the user is also using tensorboardX SummaryWriter for logging other things in their project. However, guild fails with the calls to add_scalar() when it\u2019s called from the tune library. The user found that the failure is due to the fact that _handle_scalar() is expecting \"step\" and not \"global_step\". The user was able to resolve the issue by changing the call to self._trial_writer[trial].add_scalar(full_attr, value, step) in the ray.tune library.",
        "Question_gpt_summary":"user encount issu log scalar run rai tune tune run log rai tune us summarywrit tensorboardx packag user tensorboardx summarywrit log thing project fail call add scalar call tune librari user failur fact handl scalar expect step global step user abl resolv issu chang self trial writer trial add scalar attr valu step rai tune librari",
        "Answer_original_content":"thank report apolog late repli take look meantim suggest disabl output scalar support your gener your log directli tensorboardx summari logger turn output scalar there point extra work look script output summari output scalar lot oper need configur us oper default attribut model model us model oper default output scalar oper ill updat find behavior your see bug doubt sure problem think turn output scalar base oper bunch oper assum percol reason ill oper yeah think handl scalar method youv name argument step tensorboardx call global step tensorboardx us argument pass global step step complain handl scalar call step instead global step handl scalar got unexpect keyword argument global step that guess diagnos irrelev haha assess right clean test retrospect unrel output scalar ignor earlier exampl defin plugin oper question abl fix issu remov line set list auto detect set explicitli solv issu plugin ill updat fix readi",
        "Answer_preprocessed_content":"thank report apolog late repli take look meantim suggest disabl output scalar support your gener your log directli tensorboardx summari logger turn output scalar there point extra work look script output summari lot oper need configur us attribut model ill updat find behavior your see problem think turn output scalar base oper bunch oper assum percol reason ill oper yeah think method youv name argument tensorboardx call tensorboardx us argument pass complain call instead that guess diagnos irrelev haha assess right clean test retrospect unrel output scalar ignor earlier exampl defin plugin oper question abl fix issu remov line set list set explicitli solv issu ill updat fix readi",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- disable guilds output scalar support if not using it.\n- turn off output scalars if logging directly using tensorboardx or another tf summary logger.\n- use the operation-defaults attribute of a model to configure a lot of operations.\n- set the output-scalars attribute to off in the operation-defaults attribute of a model.\n- remove or set the plugins line to an empty list if defined for the operation in question.\n\noverall, the answer provides various solutions to the challenge encountered with logging scalars when running ray[tune] tuning in a guild run.",
        "Answer_gpt_summary":"possibl solut extract answer disabl output scalar support turn output scalar log directli tensorboardx summari logger us oper default attribut model configur lot oper set output scalar attribut oper default attribut model remov set plugin line list defin oper question overal answer provid solut challeng encount log scalar run rai tune tune run"
    },
    {
        "Question_id":null,
        "Question_title":"Why my sagemaker training job slower than notebook from studiolab.sagemaker.aws?",
        "Question_body":"I run neural network tensorflow train on studiolab. and I got:\n\nEpoch 145\/4000\n1941\/1941 - 10s - ... - 10s\/epoch - 5ms\/step\n\n\nthen I try to make a train job with script_mode with ml.c5.xlarge\n\nestimator = TensorFlow(entry_point='untitled.py',\n                       source_dir='.\/training\/',\n                       instance_type='ml.c5.xlarge',\n                       instance_count=1,\n                       output_path=\"s3:\/\/sagemaker-[skip]\",\n                       role=sagemaker.get_execution_role(),\n                       framework_version='2.8.0',\n                       py_version='py39',\n                       hyperparameters={...},\n                       metric_definitions=[...],\n                       script_mode=True)\n\n\nand its got:\n\nEpoch 19\/4000\n1941\/1941 - 49s - ... - 49s\/epoch - 25ms\/step\n\n\nWhy is it 5 times slower than studiolab notebook? Is it because instance type?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651627134522,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":84.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsEj-8jnJRTK3a3gXcH8dRw\/why-my-sagemaker-training-job-slower-than-notebook-from-studiolab-sagemaker-aws",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-05T23:51:05.130Z",
                "Answer_score":0,
                "Answer_body":"May I know which instance type you are using for training locally on your notebook instance. Including factors that influence training performance, hardware spec of the training node is very critical. You might be either getting bottlenecked on CPU, Storage or Memory. See here for more details",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"train job slower notebook studiolab aw run neural network tensorflow train studiolab got epoch epoch step try train job script mode xlarg estim tensorflow entri point untitl sourc dir train instanc type xlarg instanc count output path skip role execut role framework version version hyperparamet metric definit script mode true got epoch epoch step time slower studiolab notebook instanc type",
        "Question_preprocessed_content":"train job slower notebook run neural network tensorflow train studiolab got epoch try train job estim got epoch time slower studiolab notebook instanc type",
        "Question_gpt_summary_original":"The user is facing a challenge with their Sagemaker training job being slower than their notebook from StudioLab. They ran a neural network TensorFlow train on StudioLab and got a faster result than when they tried to make a train job with script_mode with ml.c5.xlarge. They are wondering if the instance type is the reason why it is 5 times slower.",
        "Question_gpt_summary":"user face challeng train job slower notebook studiolab ran neural network tensorflow train studiolab got faster result tri train job script mode xlarg wonder instanc type reason time slower",
        "Answer_original_content":"know instanc type train local notebook instanc includ factor influenc train perform hardwar spec train node critic get bottleneck cpu storag memori detail",
        "Answer_preprocessed_content":"know instanc type train local notebook instanc includ factor influenc train perform hardwar spec train node critic get bottleneck cpu storag memori detail",
        "Answer_gpt_summary_original":"possible solutions to the challenge of slow training job performance on aws include checking the instance type being used for training locally on the notebook instance and considering factors that influence training performance such as hardware specifications of the training node. the user may be experiencing bottlenecks on cpu, storage, or memory. more details on these factors can be found in the provided link.",
        "Answer_gpt_summary":"possibl solut challeng slow train job perform aw includ check instanc type train local notebook instanc consid factor influenc train perform hardwar specif train node user experienc bottleneck cpu storag memori detail factor provid link"
    },
    {
        "Question_id":null,
        "Question_title":"Add run to existing sweep",
        "Question_body":"<p>The last run of my sweep crashed, so I run the last one again, but now it\u2019s not in the sweep. Can I add this run to the sweep somehow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655055988205,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/add-run-to-existing-sweep\/2604",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-11T17:46:36.852Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"add run exist sweep run sweep crash run sweep add run sweep",
        "Question_preprocessed_content":"add run exist sweep run sweep crash run sweep add run sweep",
        "Question_gpt_summary_original":"The user's challenge is that the last run of their sweep crashed and when they tried to run it again, it was not included in the sweep. They are seeking a way to add the run to the existing sweep.",
        "Question_gpt_summary":"user challeng run sweep crash tri run includ sweep seek wai add run exist sweep",
        "Answer_original_content":"topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer as it only states that the topic has been closed and new replies are no longer allowed.",
        "Answer_gpt_summary":"solut provid answer state topic close new repli longer allow"
    },
    {
        "Question_id":63304005.0,
        "Question_title":"sudo: not found on AWS Sagemaker Studio",
        "Question_body":"<p>I was following a guide on mounting EFS in SageMaker studio, but when using the following as a notebook cell:<\/p>\n<pre><code>%%sh \n\nsudo mount -t nfs \\\n    -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\\n    172.31.5.227:\/ \\\n    ..\/efs\n\nsudo chmod go+rw ..\/efs\n<\/code><\/pre>\n<p>I get<\/p>\n<pre><code>sh: 2: sudo: not found\nsh: 7: sudo: not found\n<\/code><\/pre>\n<p>Even in the terminal ('image terminal'), sudo is not found: <code># sudo \/bin\/sh: 1: sudo: not found<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1596811147697,
        "Question_favorite_count":null,
        "Question_last_edit_time":1596823164336,
        "Question_score":0.0,
        "Question_view_count":980.0,
        "Answer_body":"<p>I managed to get sudo working in the &quot;System Terminal&quot; instead. The image terminals don't seem to have access to sudo.<\/p>\n<p>Unrelated: But then when I tried to mount EFS onto the SageMaker studio app, it simply failed, saying mount target is not a directory. Looks like I'm not using Sagemaker Studio this year.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1596815281143,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63304005",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1596811524960,
        "Question_original_content":"sudo studio follow guid mount ef studio follow notebook cell sudo mount nf nfsver rsize wsize hard timeo retran ef sudo chmod ef sudo sudo termin imag termin sudo sudo bin sudo",
        "Question_preprocessed_content":"sudo studio follow guid mount ef studio follow notebook cell termin sudo",
        "Question_gpt_summary_original":"The user encountered a challenge while trying to mount EFS in SageMaker studio. When using the given notebook cell, the user received an error message stating that \"sudo\" was not found. The same error message was also encountered when using the terminal.",
        "Question_gpt_summary":"user encount challeng try mount ef studio given notebook cell user receiv error messag state sudo error messag encount termin",
        "Answer_original_content":"manag sudo work termin instead imag termin access sudo unrel tri mount ef studio app simpli fail sai mount target directori look like studio year",
        "Answer_preprocessed_content":"manag sudo work termin instead imag termin access sudo unrel tri mount ef studio app simpli fail sai mount target directori look like studio year",
        "Answer_gpt_summary_original":"possible solutions: \n- use the \"system terminal\" instead of the image terminals to get sudo working.\n- if trying to mount efs onto the studio app, ensure that the mount target is a directory. \n\nsummary: the user was unable to use sudo in the image terminals but managed to get it working in the system terminal. they also encountered an issue with mounting efs onto the studio app, which failed due to an incorrect mount target.",
        "Answer_gpt_summary":"possibl solut us termin instead imag termin sudo work try mount ef studio app ensur mount target directori summari user unabl us sudo imag termin manag work termin encount issu mount ef studio app fail incorrect mount target"
    },
    {
        "Question_id":65509754.0,
        "Question_title":"Can ClearML (formerly Trains) work a local server?",
        "Question_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1609343679217,
        "Question_favorite_count":null,
        "Question_last_edit_time":1609427009848,
        "Question_score":5.0,
        "Question_view_count":740.0,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1661326401412,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65509754",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1609345139523,
        "Question_original_content":"train work local server try start wai known train document need server run platform remot machin aw like bypass restrict run experi local machin connect remot destin accord instal train server remot machin theori abl instal local machin requir kubernet docker luck train think api local server ubuntu",
        "Question_preprocessed_content":"work local server try start wai document need server run platform remot machin aw like bypass restrict run experi local machin connect remot destin accord instal remot machin theori abl instal local machin requir kubernet docker luck local server ubuntu",
        "Question_gpt_summary_original":"The user is facing a challenge in using ClearML (formerly Trains) on a local server. The documentation states that a server is required, either on the ClearML platform or on a remote machine using AWS. The user wants to bypass this restriction and run experiments on their local machine, but installing the trains-server on a local machine still requires Kubernetes or Docker, which the user is not using. The user is seeking advice on whether anyone has had success using ClearML on a local server.",
        "Question_gpt_summary":"user face challeng train local server document state server requir platform remot machin aw user want bypass restrict run experi local machin instal train server local machin requir kubernet docker user user seek advic success local server",
        "Answer_original_content":"disclaim member team train like bypass restrict run experi local machin connect remot destin option free trier offer free host experi experi access specif want share colleagu probabl easiest wai start instal server basic need docker instal fine instruct summari echo max map count tmp train conf sudo tmp train conf sysctl train conf sudo sysctl max map count sudo servic docker restart sudo curl http github com docker compos releas latest download docker compos unam unam usr local bin docker compos sudo chmod usr local bin docker compos sudo mkdir opt train data elast sudo mkdir opt train data mongo sudo mkdir opt train data mongo configdb sudo mkdir opt train data redi sudo mkdir opt train log sudo mkdir opt train config sudo mkdir opt train data fileserv sudo curl http raw githubusercont com allegroai train server master docker compos yml opt train docker compos yml docker compos opt train docker compos yml support offlin mode outsid connect experi complet manual import run server self host free tier server import task task set offlin true task task init project exampl task offlin mode experi process end link zip file contain output entir offlin session task offlin session store home user cach offlin offlin dbbdeacfeead zip later import session import task task import offlin session home user cach offlin offlin dbbdeacfeead zip",
        "Answer_preprocessed_content":"disclaim member team like bypass restrict run experi local machin connect remot destin option free trier offer free host experi experi access specif want share colleagu probabl easiest wai start instal server basic need docker instal fine instruct summari support offlin mode experi complet manual import run server process end link zip file contain output entir offlin session later import session",
        "Answer_gpt_summary_original":"the user is having difficulty using (formerly known as trains) on their local machine without connecting to a remote destination due to the requirement of kubernetes or docker. the answer suggests a few possible solutions. the first solution is to use the free trial that offers free hosting for experiments that are only accessible to the user unless they want to share them with colleagues. the second solution is to install the -server, which requires docker installed, and provides full offline mode. once the experiment completes, the user can manually import the run to their server from import task task.set_offline(true) task = task.init(project_name='examples', task_name='offline mode experiment'). the process will generate a link to a zip file containing the output of the entire offline session. later, the user can import the session with task.import_offline_session('\/home\/user\/.\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip').",
        "Answer_gpt_summary":"user have difficulti known train local machin connect remot destin requir kubernet docker answer suggest possibl solut solut us free trial offer free host experi access user want share colleagu second solut instal server requir docker instal provid offlin mode experi complet user manual import run server import task task set offlin true task task init project exampl task offlin mode experi process gener link zip file contain output entir offlin session later user import session task import offlin session home user cach offlin offlin dbbdeacfeead zip"
    },
    {
        "Question_id":null,
        "Question_title":"Anyone trying backend-store-uri ?",
        "Question_body":"I am trying to use backend-store-uri for a mysql db.\nI pulled from master today and built wheel\n\n\n\u00a0 \u00a0 \u00a0 \u00a0python3.6 setup.py bdist_wheel\n\n\nI am using the wheel in this Dockerfile below.\n\n\nGetting 403 error when I go to localhost:5000\n\n\nI checked my db connections using a local Sqlachemy script and it works fine.\n\n\nAnyone trying backend-store-uri ?\n------------------------\n\n\nFROM centos:6\n\n\nENV LC_ALL=en_US.utf-8\nENV LANG=en_US.utf-8\n\n\nRUN yum update -y\nRUN yum install yum-utils -y\nRUN yum install -y https:\/\/centos6.iuscommunity.org\/ius-release.rpm\nRUN yum install -y python36u python36u-libs python36u-devel python36u-pip\n\n\nRUN yum install -y which gcc\n\n\nRUN ln -fs \/usr\/bin\/pip3.6 \/bin\/pip\nRUN ln -fs \/usr\/bin\/python3.6 \/usr\/bin\/python\n\n\nRUN python --version\nRUN pip --version\n\n\n\n\nENV TERM linux\nENV BUCKET #####\n\n\n#ENVs for mysql Aurora\nENV USERNAME #######\nENV PASSWORD #######\nENV HOST #######\nENV DATABASE #######\n\n\nCOPY mlflow-0.8.3.dev0-py3-none-any.whl .\/\nRUN pip install mlflow-0.8.3.dev0-py3-none-any.whl\n\n\nRUN mkdir -p \/mlflow\/\n\n\nEXPOSE 5000\n\n\nCMD mlflow server \\\n\u00a0 \u00a0 --backend-store-uri mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE} \\\n\u00a0 \u00a0 --default-artifact-root s3:\/\/${BUCKET}\/mlflow-artifacts \\\n\u00a0 \u00a0 --host 0.0.0.0 --gunicorn-opts \"--access-logfile -\"",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1552644949000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":43.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/T6lt0HQxGr4",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-03-15T13:46:46",
                "Answer_body":"Hi\u00a0Paul,\n\n\nThe command seems to be set correctly. We have tested a local MySQL and RDS MySQL.\u00a0\n\n\nYou mentioned being able to connect with a SQLAlchemy script. Were you able to your Aurora instance from a local machine? Was the connection string format different? If you look at the SqlAlchemyStore constructor code, it passes in the exact argument connection string to create engine. If there is a different want to connect to Aurora, would be great if you could let us know so we can update the documentation and any internal code checks and setups, accordingly.\n\n\nThanks,\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/69fa7001-ca1f-47b2-b584-2219e8b31f7b%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-03-15T14:02:43",
                "Answer_body":"I am testing with an RDS MySQL Aurora DB.\n\n\nExample of creating table from local machine.\n\n\n\n\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey\nfrom sqlalchemy import inspect\n\n\nUSERNAME = '#####'\nPASSWORD = '#####'\nHOST = '#####'\nDATABASE = '#####'\nPORT = '3306'\n\n\nmetadata = MetaData()\nbooks = Table('book', metadata,\n\u00a0 Column('id', Integer, primary_key=True),\n\u00a0 Column('title', String(20)),\n\u00a0 Column('primary_author', String(20)),\n)\n\n\nconnection_string = 'mysql:\/\/' + USERNAME + ':' + PASSWORD + '@' + HOST + ':' + PORT + '\/' + DATABASE\n\n\nengine = create_engine(connection_string)\n\n\nmetadata.create_all(engine)\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-03-15T14:17:27",
                "Answer_body":"This matches how SqlAlchemyStore calls sqlalchemy.create_engine. Could this be related to IAM roles and ACLs on machine you are running the server? Can you try running this command on the same (local) machine which you ran this script on.\n\n\n\u00a0 \u00a0mlflow ui\u00a0--backend-store-uri\u00a0mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE}\n\n\nand then pull up the UI that this command is listening to (most probably\u00a0http:\/\/127.0.0.1:5000). You may not have any data in the tables, but want to see if there are any errors that pop up when you run the above command.\n\n\nOne more question. Does that ${DATABASE} on Aurora? If not, does it help to manually create it. If I use the wrong name database name for a local MySQL -- I get this error message. Not sure if RDS\/Aurora drops that message.\n\n\n\u00a0 \u00a0 super(Connection, self).__init__(*args, **kwargs2)\nOperationalError: (_mysql_exceptions.OperationalError) (1049, \"Unknown database 'mlflow3'\") (Background on this error at: http:\/\/sqlalche.me\/e\/e3q8)\n\n\nReally appreciate you helping us test this.\n\n\n\nMani Parkhe\n\nma...@databricks.com\n\n\n\n\n\n\n\u00a0\n\n\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/aa731c3c-d323-49fa-8e2a-7b3a1fe261b4%40googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-03-15T14:43:58",
                "Answer_body":"Yes, the DB is already in Aurora.\n\n\nOn the terminal...looks ok\n\n\nEXPORT USERNAME #####\nEXPORT PASSWORD #####\nEXPORT HOST #####\nEXPORT DATABASE #####\n\n\n$ mlflow ui --backend-store-uri mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE}\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Starting gunicorn 19.9.0\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Listening at: http:\/\/127.0.0.1:5000 (18566)\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Using worker: sync\n[2019-03-15 11:39:31 -0700] [18570] [INFO] Booting worker with pid: 18570\n\n\n\n\nBrowser at http:\/\/localhost:5000\n\n\nNot Found\nThe requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-03-15T14:49:24",
                "Answer_body":"\ue5d3"
            },
            {
                "Answer_creation_time":"2019-03-15T14:53:32",
                "Answer_body":"I am in Python3.6 Anaconda venv.\npip install mlflow-0.8.3.dev0-py3-none-any.whl\n\n\nOn the terminal...looks ok\n\n\n$ EXPORT USERNAME #####\n$ EXPORT PASSWORD #####\n$ EXPORT HOST #####\n$ EXPORT DATABASE #####\n\n\n$ mlflow ui --backend-store-uri mysql:\/\/${USERNAME}:${PASSWORD}@${HOST}:${PORT}\/${DATABASE}\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Starting gunicorn 19.9.0\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Listening at: http:\/\/127.0.0.1:5000 (18566)\n[2019-03-15 11:39:31 -0700] [18566] [INFO] Using worker: sync\n[2019-03-15 11:39:31 -0700] [18570] [INFO] Booting worker with pid: 18570\n\n\n--------------------------------------------------\nBrowser at http:\/\/localhost:5000\n\n\nNot Found\nThe requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.\n\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"try backend store uri try us backend store uri mysql pull master todai built wheel python setup bdist wheel wheel dockerfil get error localhost check connect local sqlachemi script work fine try backend store uri cento env utf env lang utf run yum updat run yum instal yum util run yum instal http cento iuscommun org iu releas rpm run yum instal pythonu pythonu lib pythonu devel pythonu pip run yum instal gcc run usr bin pip bin pip run usr bin python usr bin python run python version run pip version env term linux env bucket env mysql aurora env usernam env password env host env databas copi dev whl run pip instal dev whl run mkdir expos cmd server backend store uri mysql usernam password host port databas default artifact root bucket artifact host gunicorn opt access logfil",
        "Question_preprocessed_content":"try try us mysql pull master todai built wheel wheel dockerfil get error localhost check connect local sqlachemi script work fine try cento env env run yum updat run yum instal run yum instal run yum instal python run yum instal gcc run run run python run pip env term linux env bucket env mysql aurora env usernam env password env host env databas copi run pip instal run mkdir expos cmd server",
        "Question_gpt_summary_original":"The user is encountering a 403 error when trying to access localhost:5000 while using backend-store-uri for a MySQL database. The user has checked their database connections using a local Sqlalchemy script and it works fine. The user is seeking help from anyone who has tried backend-store-uri.",
        "Question_gpt_summary":"user encount error try access localhost backend store uri mysql databas user check databas connect local sqlalchemi script work fine user seek help tri backend store uri",
        "Answer_original_content":"hipaul command set correctli test local mysql rd mysql mention abl connect sqlalchemi script abl aurora instanc local machin connect string format differ look sqlalchemystor constructor code pass exact argument connect string creat engin differ want connect aurora great let know updat document intern code check setup accordingli thank mani parkh databrick com receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user caf ebfb googlegroup com option visit http group googl com optout test rd mysql aurora exampl creat tabl local machin import sqlalchemi sqlalchemi import creat engin sqlalchemi import tabl column integ string metadata foreignkei sqlalchemi import inspect usernam password host databas port metadata metadata book tabl book metadata column integ primari kei true column titl string column primari author string connect string mysql usernam password host port databas engin creat engin connect string metadata creat engin match sqlalchemystor call sqlalchemi creat engin relat iam role acl machin run server try run command local machin ran script backend store urimysql usernam password host port databas pull command listen probablyhttp data tabl want error pop run command question databas aurora help manual creat us wrong databas local mysql error messag sure rd aurora drop messag super connect self init arg kwarg operationalerror mysql except operationalerror unknown databas background error http sqlalch appreci help test mani parkh databrick com view discuss web visit http group googl com msgid user aacc bafeb googlegroup com ye aurora termin look export usernam export password export host export databas backend store uri mysql usernam password host port databas info start gunicorn info listen http info worker sync info boot worker pid browser http localhost request url server enter url manual check spell try python anaconda venv pip instal dev whl termin look export usernam export password export host export databas backend store uri mysql usernam password host port databas info start gunicorn info listen http info worker sync info boot worker pid browser http localhost request url server enter url manual check spell try",
        "Answer_preprocessed_content":"hipaul command set correctli test local mysql rd mysql mention abl connect sqlalchemi script abl aurora instanc local machin connect string format differ look sqlalchemystor constructor code pass exact argument connect string creat engin differ want connect aurora great let know updat document intern code check setup accordingli thank mani parkh receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit test rd mysql aurora exampl creat tabl local machin import sqlalchemi sqlalchemi import sqlalchemi import tabl column integ string metadata foreignkei sqlalchemi import inspect usernam password host databas port metadata metadata book tabl column string usernam password host port databas engin match sqlalchemystor call relat iam role acl machin run server try run command machin ran script pull command listen data tabl want error pop run command question aurora help manual creat us wrong databas local mysql error messag sure drop messag super kwarg operationalerror appreci help test mani parkh view discuss web visit ye aurora export usernam export password export host export databas start gunicorn listen worker sync boot worker pid browser request url server enter url manual check spell try anaconda venv pip instal export usernam export password export host export databas start gunicorn listen worker sync boot worker pid browser request url server enter url manual check spell try",
        "Answer_gpt_summary_original":"No solutions are provided in this discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb.watch not logging parameters",
        "Question_body":"<p>I just started to use w&amp;b to monitor the training of my few-shot learning NNs in Pytorch. I use wandb.watch(model, log=\u2018all\u2019) but it only logs the gradients. Any idea what could be causing this? Also, is there an easy way to log the activation histograms of the different layers for pytorch?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":19,
        "Question_comment_count":0,
        "Question_creation_time":1635863467980,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":353.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-watch-not-logging-parameters\/1197",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-03T14:20:17.285Z",
                "Answer_body":"<p>Hi Nora, that is an interesting bug. The default is to log gradients. Can you try putting \u201call\u201d in double quotes instead of single quotes?<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T16:22:39.471Z",
                "Answer_body":"<p>Hi Leslie,<\/p>\n<p>Thanks for the reply. Unfortunately, the double quotes didn\u2019t change the result, still only gradients. When I put \u2018parameters\u2019 nothing is logged.<\/p>",
                "Answer_score":2.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-04T15:23:53.687Z",
                "Answer_body":"<p>Can you tell me what version of wandb you\u2019re using?<\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-05T09:10:05.410Z",
                "Answer_body":"<p>I recently installed it, version is wandb 0.12.5.<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-15T15:48:59.267Z",
                "Answer_body":"<p>Are you using wandb.log in your code?<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-15T17:28:52.053Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/norav\">@norav<\/a> , you might want to make sure if you\u2019re logging at least one piece of data with <code>wandb.log()<\/code> <strong>first<\/strong>. Also, it would be worth mentioning that we only log gradients every 1000 steps by default.<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-18T16:52:07.945Z",
                "Answer_body":"<p>Hi Nora, we wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-19T09:52:31.833Z",
                "Answer_body":"<p>Hi Leslie,<\/p>\n<p>The issue has not been resolved. I do use wandb.log() in my code for the loss and I use it now as well to log the parameters using hooks. I\u2019m afraid this slows down my code but see no other solution since wandb.watch() is only logging gradients.<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-22T16:36:24.771Z",
                "Answer_body":"<p>Can you let us know how many steps you are running?<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-25T11:43:13.037Z",
                "Answer_body":"<p>I am running 50-100 steps. I use the command watch(model, log=\u2018all\u2019, log_freq=1)<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-01T19:34:21.420Z",
                "Answer_body":"<p>Can you send an image of what you see on your dashboard as to what is appearing and give us a script of how you are implementing wandb.log into your code?<\/p>",
                "Answer_score":11.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-02T15:40:54.418Z",
                "Answer_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/17f2c16284d2751c60649611a962c64992e1497e.png\" data-download-href=\"\/uploads\/short-url\/3pR58CbGFOHxQpDJumZ9RJM8ca2.png?dl=1\" title=\"panelwb\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/17f2c16284d2751c60649611a962c64992e1497e_2_690x405.png\" alt=\"panelwb\" data-base62-sha1=\"3pR58CbGFOHxQpDJumZ9RJM8ca2\" width=\"690\" height=\"405\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/17f2c16284d2751c60649611a962c64992e1497e_2_690x405.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/17f2c16284d2751c60649611a962c64992e1497e_2_1035x607.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/17f2c16284d2751c60649611a962c64992e1497e.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/17f2c16284d2751c60649611a962c64992e1497e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">panelwb<\/span><span class=\"informations\">1197\u00d7704 25.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nThis is the dashboard where I only get the gradients, not the parameters.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-02T15:42:55.570Z",
                "Answer_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/363b37aefcf9ef3c5681e1df6a1a3f8c1ef0c5c2.png\" data-download-href=\"\/uploads\/short-url\/7JKEGYOa3SP7bDY1ts5q71113NM.png?dl=1\" title=\"wan\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/363b37aefcf9ef3c5681e1df6a1a3f8c1ef0c5c2.png\" alt=\"wan\" data-base62-sha1=\"7JKEGYOa3SP7bDY1ts5q71113NM\" width=\"690\" height=\"85\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/363b37aefcf9ef3c5681e1df6a1a3f8c1ef0c5c2_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">wan<\/span><span class=\"informations\">1294\u00d7160 9.55 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Here I initialize the wandb run and I pass it to the train functino (wrapper.fit()) where I log the loss with the next code snippet<\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-02T15:43:27.327Z",
                "Answer_body":"<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/86e52e1bb508b59925192897385b870cd9694f8b.png\" alt=\"loggingwb\" data-base62-sha1=\"jfkZP5Bitmo6D7bFouZRrzpqzXl\" width=\"649\" height=\"112\"><\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-03T17:37:11.752Z",
                "Answer_body":"<p>Can you use wandb.log once before you use wanbd.watch? Wandb.watch won\u2019t work properly if wandb.log is not there beforehand<\/p>",
                "Answer_score":11.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-06T13:09:36.367Z",
                "Answer_body":"<p>Still no parameters with logging something before the watch:<br>\nrun.log({\u2018trainidx\u2019:trainidx})<br>\nrun.watch(model, log='all, log_freq=1)<\/p>",
                "Answer_score":26.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-06T20:07:23.430Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/norav\">@norav<\/a> , is there any chance you\u2019re calling <code>model.forward(inputs)<\/code> instead of <code>model(inputs)<\/code>? Also, if you could share your script, that would be very helpful to reproduce the issue.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-07T10:24:01.906Z",
                "Answer_body":"<p>Yes this seems to be it. Thank you!<\/p>",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-05T10:24:20.399Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"watch log paramet start us monitor train shot learn nn pytorch us watch model log log gradient idea caus easi wai log activ histogram differ layer pytorch thank",
        "Question_preprocessed_content":"watch log paramet start us monitor train learn nn pytorch us watch log gradient idea caus easi wai log activ histogram differ layer pytorch thank",
        "Question_gpt_summary_original":"The user is facing challenges with wandb.watch not logging parameters for their few-shot learning NNs in Pytorch. The user is seeking advice on why wandb.watch is only logging gradients and if there is an easy way to log activation histograms of different layers for Pytorch.",
        "Question_gpt_summary":"user face challeng watch log paramet shot learn nn pytorch user seek advic watch log gradient easi wai log activ histogram differ layer pytorch",
        "Answer_original_content":"nora interest bug default log gradient try put doubl quot instead singl quot lesli thank repli unfortun doubl quot didnt chang result gradient paramet log tell version your recent instal version log code norav want sure your log piec data log worth mention log gradient step default nora want follow support request heard let know assist issu resolv lesli issu resolv us log code loss us log paramet hook afraid slow code solut watch log gradient let know step run run step us command watch model log log freq send imag dashboard appear script implement log code panelwb dashboard gradient paramet wan initi run pass train functino wrapper fit log loss code snippet us log us wanbd watch watch wont work properli log paramet log watch run log trainidx trainidx run watch model log log freq norav chanc your call model forward input instead model input share script help reproduc issu ye thank topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"nora interest bug default log gradient try put doubl quot instead singl quot lesli thank repli unfortun doubl quot didnt chang result gradient paramet log tell version your recent instal version log code want sure your log piec data worth mention log gradient step default nora want follow support request heard let know assist issu resolv lesli issu resolv us log code loss us log paramet hook afraid slow code solut watch log gradient let know step run run step us command watch send imag dashboard appear script implement log code panelwb dashboard gradient paramet wan initi run pass train functino log loss code snippet us log us watch wont work properli log paramet log watch log chanc your call instead share script help reproduc issu ye thank topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are: \n- try putting all in double quotes instead of single quotes.\n- make sure to log at least one piece of data with .log() first.\n- check the version of pytorch being used.\n- use .log once before using wanbd.watch.\n- call model(inputs) instead of model.forward(inputs).\n- share the script to reproduce the issue.",
        "Answer_gpt_summary":"possibl solut extract answer try put doubl quot instead singl quot sure log piec data log check version pytorch us log wanbd watch model input instead model forward input share script reproduc issu"
    },
    {
        "Question_id":null,
        "Question_title":"Meetup on machine translation for low-resource languages this Friday!",
        "Question_body":"The last machine translation meetup featured a PM for the Google Cloud Translation API in person.The next machine translation meetup is all about low-resource machine translation and it'll be online.",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1666181580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":19.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Meetup-on-machine-translation-for-low-resource-languages-this\/td-p\/479955\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-19T12:13:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"The last machine translation meetup featured a PM for the Google Cloud Translation API in person.\n\nThe next machine translation meetup is all about low-resource machine translation\u00a0and it'll be online.\n\n\u00a0\nmachinetranslate.org\/meetup\n\u00a0\nThe 25-minute panel features guests from Meta AI,\u00a0NeuralSpace, LoResMT, and Masakhane!\n\nRegister to join us\u00a0this Friday at 8am PST"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"meetup machin translat low resourc languag fridai machin translat meetup featur googl cloud translat api person machin translat meetup low resourc machin translat onlin",
        "Question_preprocessed_content":"meetup machin translat languag fridai machin translat meetup featur googl cloud translat api machin translat meetup machin translat onlin",
        "Question_gpt_summary_original":"The user is facing challenges related to low-resource machine translation and is interested in attending an upcoming online meetup on the topic.",
        "Question_gpt_summary":"user face challeng relat low resourc machin translat interest attend upcom onlin meetup topic",
        "Answer_original_content":"machin translat meetup featur googl cloud translat api person machin translat meetup low resourc machin translationand onlin machinetransl org meetup minut panel featur guest meta neuralspac loresmt masakhan regist join usthi fridai pst",
        "Answer_preprocessed_content":"machin translat meetup featur googl cloud translat api person machin translat meetup machin translationand onlin panel featur guest meta neuralspac loresmt masakhan regist join usthi fridai pst",
        "Answer_gpt_summary_original":"summary: the next machine translation meetup is about low-resource machine translation and will be held online. the 25-minute panel will feature guests from meta ai, neuralspace, loresmt, and masakhane. the user can register to join on friday at 8am pst.",
        "Answer_gpt_summary":"summari machin translat meetup low resourc machin translat held onlin minut panel featur guest meta neuralspac loresmt masakhan user regist join fridai pst"
    },
    {
        "Question_id":null,
        "Question_title":"DVC local storage usecase",
        "Question_body":"<p>Hi, I\u2019m trying to understand if DVC is a good solution for our company use-case. We currently have several tens of TB of data, and constantly adding to it every week. I would like to add versioning to this, so that our scientists can run experiments on various subsets and track all changes.<\/p>\n<p>All the data is stored locally in a network drive accessible from the scientists\u2019 computers. My question would be if DVC can be used in this way for this use-case, and how to get around copying the data multiple time (e.g. if 5 scientists access 40TB worth of data, this shouldn\u2019t be copied to their versioned repos).<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1611043180303,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":842.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-local-storage-usecase\/628",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-19T08:39:42.177Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cotton84\">@cotton84<\/a>,<\/p>\n<aside class=\"quote no-group\" data-username=\"cotton84\" data-post=\"1\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>We currently have several tens of TB of data, and constantly adding to it every week.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Can you give a bit more details on what your data storage looks like? Is it a single dataset, several ones, are you adding new files or appending to existing files? The best implementation of data versioning with DVC depends on those factors, but in general yes: this is something we aim to solve <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<p>For now I would recommend you check out <a href=\"https:\/\/dvc.org\/doc\/use-cases\/versioning-data-and-model-files\">https:\/\/dvc.org\/doc\/use-cases\/versioning-data-and-model-files<\/a> (and the tutorial under that).<\/p>\n<aside class=\"quote no-group\" data-username=\"cotton84\" data-post=\"1\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>run experiments on various subsets<\/p>\n<\/blockquote>\n<\/aside>\n<p>It would also be great to know how these subsets are formed to give you a more specific answer. For now I can share <a href=\"https:\/\/dvc.org\/doc\/start\/experiments\">https:\/\/dvc.org\/doc\/start\/experiments<\/a> on this.<\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"1\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>network drive accessible from the scientists\u2019 computers\u2026<br>\nhow to get around copying the data multiple time<\/p>\n<\/blockquote>\n<\/aside>\n<p>This is actually a use case we have documented specifically in: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\">https:\/\/dvc.org\/doc\/use-cases\/shared-development-server<\/a> \u2014 please take a look and lmk if that was not what you meant. Thanks!<\/p>",
                "Answer_score":59.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-19T10:03:56.368Z",
                "Answer_body":"<p>Hey, thanks for the really quick reply!<\/p>\n<p>Some more details about how our data is structured:<\/p>\n<ul>\n<li>\n<p>We have several recording campaigns during the year, almost every month, where we go to various customers and record data on their premises. These are put into several folders i.e.<\/p>\n<ul>\n<li>Client1\/2020-01<\/li>\n<li>Client2\/2020-01<\/li>\n<li>Client2\/2020-04<\/li>\n<li>Client3\/2020-01 etc.<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>The files we are working with range from several hundred MB to several tens of GB in size <strong>per file<\/strong><\/p>\n<\/li>\n<li>\n<p>We then have projects using combinations of these for each client, e.g.:<\/p>\n<ul>\n<li>Project1: Client1\/2020-01, Client2\/2020-01, Client3\/2020-01<\/li>\n<li>Project2: Client2\/2020-01, Client2\/2020-04<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>For each project we have several experiments that can have subsets of these datasets for debugging the feature extraction stages<\/p>\n<\/li>\n<li>\n<p>At any point we can add data to the existing datasets \/ projects<\/p>\n<\/li>\n<li>\n<p>There can be 2-3 people working on one project at once, with various subsets of the datasets<\/p>\n<\/li>\n<\/ul>\n<p>I think this is a very common use-case in the industry. Right now we are organizing these with simple folders, and with files that list the current dataset that each experiment is using. This is getting out of hand as we add more and more data, and very easy to miss something.<\/p>\n<p>The problem is that although there are many tutorials on DVC, there is not one that goes over more complex on-prem setups with \u2018big\u2019 data such as this one.<\/p>",
                "Answer_score":38.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-19T17:54:21.829Z",
                "Answer_body":"<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"3\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>The files we are working with range from several hundred MB to several tens of GB\u2026<br>\nThese are put into several folders\u2026<br>\nAt any point we can add data\u2026<\/p>\n<\/blockquote>\n<\/aside>\n<p>Thanks for the info! So it sounds like you don\u2019t append to existing files, right? This is the most efficient way to version large datasets with DVC at the moment (although when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/829\">chunking<\/a> is implemented, it won\u2019t matter as much). That\u2019s because DVC has no awareness of data formats inside files, so changing even a single byte requires storing the whole file again as a separate version.<\/p>\n<p>Were you able to check out the <strong>Data Versioning Tutorial<\/strong>? The <a href=\"https:\/\/dvc.org\/doc\/use-cases\/versioning-data-and-model-files\/tutorial#second-model-version\">Second model version<\/a> section shows how to add files to tracked directories to create a new dataset version.<\/p>\n<p>It\u2019s just important to think through which directories you consider dataset \u201cunits\u201d and will be tracking <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#adding-entire-directories\">with <code>dvc add<\/code><\/a>. For example, in your case you can pick between tracking each <code>Client*\/&lt;Y-m&gt;<\/code> or going higher level (just each <code>Client*\/<\/code>)?<\/p>\n<p>Each tracked directory will produce a .dvc file, so each strategy results in less or more .dvc files to manage (with Git). But keep in mind that \u201csync\u201d commands support targeting files\/dir inside tracked dirs granularly (see for ex. <a href=\"https:\/\/dvc.org\/doc\/command-reference\/checkout\"><code>dvc checkout<\/code><\/a>) so in either case your ability to pull and push specific files is the same.<\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"3\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>We then have projects using combinations of these\u2026<br>\nFor each project we have several experiments that can have subsets of these data\u2026<br>\n\u2026files that list the current dataset that each experiment is using.<\/p>\n<\/blockquote>\n<\/aside>\n<p>For this pattern we have the <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\">Data Registries<\/a> use case: you can have a DVC repo dedicated to versioning all your datasets, and then secondary DVC projects that <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\"><code>get<\/code><\/a> or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/import\"><code>import<\/code><\/a> the specific dirs or files you need (also supports granularity)<\/p>\n<p>Please also take a look at <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\">Pipelines<\/a> as a way to start codifying your experiments in a manageable way.<\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"3\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>There can be 2-3 people working on one project at once, with various subsets\u2026<br>\nvery common use-case in the industry\u2026 complex on-prem setups with \u2018big\u2019 data\u2026<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yep. What did you think of the \u201cshared external cache\u201d pattern? Does it help in your team\u2019s org? In this case that setup would apply mainly to the secondary DVC projects consuming from the data registry (although even that repo could share it too).<\/p>\n<aside class=\"onebox allowlistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon.ico\" class=\"site-icon\" width=\"64\" height=\"64\">\n      <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" target=\"_blank\" rel=\"noopener\">dvc.org<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" target=\"_blank\" rel=\"noopener\">Shared Development Server<\/a><\/h3>\n\n<p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":43.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T03:57:13.932Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/cotton84\">@cotton84<\/a> we have essentially an <em>append-only immutable<\/em> pool of many, many files and each data scientist could pick any subset of those file to do some experimentation with them, right?<\/p>\n<p>In this case I usually don\u2019t recommend using the current version of DVC at all <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> You are totally fine versioning a \u201cfilter\u201d file- a file that contains specific files that are being used in the specific commit, specific project. It should be enough in terms of reproducibility and versioning if <strong>data in the storage is immutable and only new files are being added.<\/strong><\/p>\n<p>Now, you have another requirement though - <code>avoid copying the data multiple time<\/code>! This is indeed what shared cache\/shared dev server DVC use case for. It\u2019s very powerful and solves the problem. The question here is how to solve both simultaneously - flexibility (give data scientists and interface to checkout (in DVC terms) arbitrary subset of files to work with from the pool + avoid copying.<\/p>\n<p>To come with a workaround, it would be helpful to understand how do you create this file in the:<\/p>\n<blockquote>\n<p>with files that list the current dataset that each experiment is using<\/p>\n<\/blockquote>\n<p>how do data scientists select the files they need from the pool of all those directories?<\/p>",
                "Answer_score":28.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T07:24:55.186Z",
                "Answer_body":"<p>Thank you both for the quick replies! I was able to go through the tutorial and set up an external cache with a small-ish dataset yesterday, so everything is a bit more clear.<\/p>\n<p>To answer your questions:<\/p>\n<ul>\n<li>Yes, the files are generally immutable, we only add more files, or if necessary cull some (e.g. corrupted files or very old ones).<\/li>\n<li>We would like to have fine granularity when working with datasets (individual files), but you are saying that it doesn\u2019t matter if I add specific files (or recurse through directories), or directories themselves, they can be accessed individually either way, right?<\/li>\n<li>With regards to external caches, I have a question - right now after I push the commit, the data gets copied to the external cache. Doe DVC automatically delete the files in the repository if it is set to hard link the files, so that there is only one copy of the file, the one in the cache?<\/li>\n<li>The scientists have the big pool of files in the dataset. Depending on their experiment needs, they can select subsets, e.g. files that were recorded in one specific location, or within a specific timeframe. The files are not copied, just the lists are used to access them during training. To filter the files they use labels and metadata that are in separate ground-truth files, which brings me to another question:<\/li>\n<li>Is there a recommended way to handle label files? These could change over time, adding of modifying metadata. Right now they are stored in a separate repository and they point to each of the dataset files in particular (have the same name). Would it be a good idea to let DVC also track this? Or is there a preferred tool (e.g. database) that can be used to better manage it?<\/li>\n<\/ul>\n<p>Thanks again for the help, much appreciated!<\/p>",
                "Answer_score":32.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T19:02:00.057Z",
                "Answer_body":"<aside class=\"quote no-group quote-modified\" data-username=\"shcheklein\" data-post=\"5\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/shcheklein\/40\/11_2.png\" class=\"avatar\"> shcheklein:<\/div>\n<blockquote>\n<p><em>append-only immutable<\/em> pool\u2026<br>\nI usually don\u2019t recommend using the current version of DVC<\/p>\n<\/blockquote>\n<\/aside>\n<p>This is a good point, the proposed file structure already implements basic versioning by file name (<code>Client*\/&lt;Y-m&gt;<\/code> format).I think that this goes back to asking ourselves what we want to consider \u201cdata versioning\u201d (more info: <a href=\"https:\/\/www.w3.org\/TR\/dwbp\/#dataVersioning\" class=\"inline-onebox\">Data on the Web Best Practices<\/a>).<\/p>\n<p>But, regardless, the patterns previously discussed could still make it desirable to track those assets with DVC. It\u2019s up to you <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<aside class=\"quote no-group\" data-username=\"cotton84\" data-post=\"6\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<ul>\n<li>you are saying that it doesn\u2019t matter if I add specific files (or recurse through directories), or directories themselves, they can be accessed individually either way, right?<\/li>\n<\/ul>\n<\/blockquote>\n<\/aside>\n<p>Correct.<\/p>\n<aside class=\"quote no-group\" data-username=\"cotton84\" data-post=\"6\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>Doe DVC automatically delete the files in the repository if it is set to hard link the files, so that there is only one copy of the file, the one in the cache?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yes, DVC tries to preserve only one copy of that data in the drive whether the cache is local or external to the project (unless configured to copy or if the file system doesn\u2019t support file links). See <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a><\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"6\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>Is there a recommended way to handle label files? Right now they are stored in a separate repository<\/p>\n<\/blockquote>\n<\/aside>\n<p>They can be in the same repository, and you can version those directly with Git along with any other code or config files.<\/p>\n<p>But it\u2019s not cleat to me that you\u2019d still need some of those files (e.g. the lists that define a data subset) if\/after adopting DVC. For example, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files\/dvc-yaml\">dvc.yaml<\/a> files already codify the dependencies for each stage in a <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\">data pipeline<\/a>.<\/p>",
                "Answer_score":47.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"local storag usecas try understand good solut compani us case current ten data constantli ad week like add version scientist run experi subset track chang data store local network drive access scientist comput question wai us case copi data multipl time scientist access worth data shouldnt copi version repo",
        "Question_preprocessed_content":"local storag usecas try understand good solut compani current ten data constantli ad week like add version scientist run experi subset track chang data store local network drive access scientist comput question wai copi data multipl time",
        "Question_gpt_summary_original":"The user is trying to determine if DVC is a suitable solution for their company's use-case, which involves versioning several tens of TB of data that is stored locally in a network drive accessible from scientists' computers. The user is concerned about copying the data multiple times to scientists' versioned repositories when accessed by multiple users.",
        "Question_gpt_summary":"user try determin suitabl solut compani us case involv version ten data store local network drive access scientist comput user concern copi data multipl time scientist version repositori access multipl user",
        "Answer_original_content":"cotton cotton current ten data constantli ad week bit detail data storag look like singl dataset on ad new file append exist file best implement data version depend factor gener ye aim solv recommend check http org doc us case version data model file tutori cotton run experi subset great know subset form specif answer share http org doc start experi cotton network drive access scientist comput copi data multipl time actual us case document specif http org doc us case share develop server look lmk meant thank hei thank quick repli detail data structur record campaign year month custom record data premis folder client client client client file work rang ten size file project combin client project client client client project client client project experi subset dataset debug featur extract stage point add data exist dataset project peopl work project subset dataset think common us case industri right organ simpl folder file list current dataset experi get hand add data easi miss problem tutori goe complex prem setup big data cotton file work rang ten folder point add data thank info sound like dont append exist file right effici wai version larg dataset moment chunk implement wont matter that awar data format insid file chang singl byte requir store file separ version abl check data version tutori second model version section show add file track directori creat new dataset version import think directori consid dataset unit track add exampl case pick track client go higher level client track directori produc file strategi result file manag git mind sync command support target file dir insid track dir granularli checkout case abil pull push specif file cotton project combin project experi subset data file list current dataset experi pattern data registri us case repo dedic version dataset secondari project import specif dir file need support granular look pipelin wai start codifi experi manag wai cotton peopl work project subset common us case industri complex prem setup big data yep think share extern cach pattern help team org case setup appli mainli secondari project consum data registri repo share org share develop server open sourc version control data scienc machin learn project git like experi organ data model experi cotton essenti append immut pool file data scientist pick subset file experiment right case usual dont recommend current version total fine version filter file file contain specif file specif commit specif project term reproduc version data storag immut new file ad requir avoid copi data multipl time share cach share dev server us case power solv problem question solv simultan flexibl data scientist interfac checkout term arbitrari subset file work pool avoid copi come workaround help understand creat file file list current dataset experi data scientist select file need pool directori thank quick repli abl tutori set extern cach small ish dataset yesterdai bit clear answer question ye file gener immut add file necessari cull corrupt file old on like fine granular work dataset individu file sai doesnt matter add specif file recurs directori directori access individu wai right regard extern cach question right push commit data get copi extern cach doe automat delet file repositori set hard link file copi file cach scientist big pool file dataset depend experi need select subset file record specif locat specif timefram file copi list access train filter file us label metadata separ ground truth file bring question recommend wai handl label file chang time ad modifi metadata right store separ repositori point dataset file particular good idea let track prefer tool databas better manag thank help appreci shcheklein append immut pool usual dont recommend current version good point propos file structur implement basic version file client format think goe ask want consid data version info data web best practic regardless pattern previous discuss desir track asset cotton sai doesnt matter add specif file recurs directori directori access individu wai right correct cotton doe automat delet file repositori set hard link file copi file cach ye tri preserv copi data drive cach local extern project configur copi file doesnt support file link http org doc user guid larg dataset optim cotton recommend wai handl label file right store separ repositori repositori version directli git code config file cleat youd need file list defin data subset adopt exampl yaml file codifi depend stage data pipelin",
        "Answer_preprocessed_content":"cotton current ten data constantli ad week bit detail data storag look like singl dataset on ad new file append exist file best implement data version depend factor gener ye aim solv recommend check cotton run experi subset great know subset form specif answer share cotton network drive access scientist comput copi data multipl time actual us case document specif look lmk meant thank hei thank quick repli detail data structur record campaign year month custom record data premis folder file work rang ten size file project combin client project project project experi subset dataset debug featur extract stage point add data exist dataset project peopl work project subset dataset think common industri right organ simpl folder file list current dataset experi get hand add data easi miss problem tutori goe complex setup big data cotton file work rang ten folder point add data thank info sound like dont append exist file right effici wai version larg dataset moment that awar data format insid file chang singl byte requir store file separ version abl check data version tutori second model version section show add file track directori creat new dataset version import think directori consid dataset unit track exampl case pick track go higher level track directori produc file strategi result file manag mind sync command support target insid track dir granularli case abil pull push specif file cotton project combin project experi subset data file list current dataset experi pattern data registri us case repo dedic version dataset secondari project specif dir file need look pipelin wai start codifi experi manag wai cotton peopl work project subset common industri complex setup big data yep think share extern cach pattern help team org case setup appli mainli secondari project consum data registri org share develop server version control data scienc machin learn project experi organ data model experi essenti immut pool file data scientist pick subset file experiment right case usual dont recommend current version total fine version filter file file contain specif file specif commit specif project term reproduc version data storag immut new file ad requir share dev server us case power solv problem question solv simultan flexibl arbitrari subset file work pool avoid copi come workaround help understand creat file file list current dataset experi data scientist select file need pool directori thank quick repli abl tutori set extern cach dataset yesterdai bit clear answer question ye file gener immut add file necessari cull like fine granular work dataset sai doesnt matter add specif file directori access individu wai right regard extern cach question right push commit data get copi extern cach doe automat delet file repositori set hard link file copi file cach scientist big pool file dataset depend experi need select subset file record specif locat specif timefram file copi list access train filter file us label metadata separ file bring question recommend wai handl label file chang time ad modifi metadata right store separ repositori point dataset file particular good idea let track prefer tool better manag thank help appreci shcheklein immut pool usual dont recommend current version good point propos file structur implement basic version file think goe ask want consid data version regardless pattern previous discuss desir track asset cotton sai doesnt matter add specif file directori access individu wai right correct cotton doe automat delet file repositori set hard link file copi file cach ye tri preserv copi data drive cach local extern project cotton recommend wai handl label file right store separ repositori repositori version directli git code config file cleat youd need file adopt exampl yaml file codifi depend stage data pipelin",
        "Answer_gpt_summary_original":"possible solutions for the user's use-case of versioning tens of terabytes of data stored in a network drive and accessed by multiple scientists include using data registries, pipelines, and shared development servers. the user can track each client folder or go higher level, and can use filter files to select specific files or directories. it is recommended to use an external cache to avoid copying data multiple times. label files can be stored in the same repository and versioned directly with git.",
        "Answer_gpt_summary":"possibl solut user us case version ten terabyt data store network drive access multipl scientist includ data registri pipelin share develop server user track client folder higher level us filter file select specif file directori recommend us extern cach avoid copi data multipl time label file store repositori version directli git"
    },
    {
        "Question_id":null,
        "Question_title":"Determining the \"right\" instance type running Jupyter notebook in Sagemaker when reading\/writing a huge parquet file?",
        "Question_body":"I am unclear as o how to determine the \"right\" instance type running Jupyter notebook in Sagemaker. When reading\/writing a small size parquet file, no problem; but when I try to read\/write a huge parquet file, the program stops and gives an error, \"Job aborted due to stage failure: Task 21 in stage 33.0 failed 1 times, most recent failure: Lost task 21.0 in stage 33.0 (TID 1755, localhost, executor driver\" I would appreciate any insights please... thanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655268194182,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvBSWuAqZSru0kQHeulQqLw\/determining-the-right-instance-type-running-jupyter-notebook-in-sagemaker-when-reading-writing-a-huge-parquet-file",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-30T08:40:43.692Z",
                "Answer_score":0,
                "Answer_body":"For notebook instance it's mostly trial-and-error, at least for now. Once your model is ready to be deployed, there is the SageMaker Inference Recommender that can do automated load testing and give you recommendation on the instance size.\n\nIt's hard to give a recommendation on the notebook instance because you might test a 100MB dataset today, but choose to go with a 500GB dataset tomorrow, so the recommendations are no longer valid.\n\nYou might want to try experimenting with a smaller dataset sampled from the original big dataset, once you are confident with the model training code, use distributed training to run it on the complete big dataset.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"determin right instanc type run jupyt notebook read write huge parquet file unclear determin right instanc type run jupyt notebook read write small size parquet file problem try read write huge parquet file program stop give error job abort stage failur task stage fail time recent failur lost task stage tid localhost executor driver appreci insight thank",
        "Question_preprocessed_content":"determin right instanc type run jupyt notebook huge parquet file unclear determin right instanc type run jupyt notebook small size parquet file problem try huge parquet file program stop give error job abort stage failur task stage fail time recent failur lost task stage tid localhost executor driver appreci insight thank",
        "Question_gpt_summary_original":"The user is facing challenges in determining the appropriate instance type for running Jupyter notebook in Sagemaker when dealing with a large parquet file. The program stops and gives an error when attempting to read\/write the file, and the user is seeking insights on how to resolve this issue.",
        "Question_gpt_summary":"user face challeng determin appropri instanc type run jupyt notebook deal larg parquet file program stop give error attempt read write file user seek insight resolv issu",
        "Answer_original_content":"notebook instanc trial error model readi deploi infer recommend autom load test recommend instanc size hard recommend notebook instanc test dataset todai choos dataset tomorrow recommend longer valid want try experi smaller dataset sampl origin big dataset confid model train code us distribut train run complet big dataset",
        "Answer_preprocessed_content":"notebook instanc model readi deploi infer recommend autom load test recommend instanc size hard recommend notebook instanc test dataset todai choos dataset tomorrow recommend longer valid want try experi smaller dataset sampl origin big dataset confid model train code us distribut train run complet big dataset",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n\n1. for determining the \"right\" instance type running jupyter notebook, it's mostly trial-and-error, at least for now. \n\n2. once the model is ready to be deployed, there is the inference recommender that can do automated load testing and give a recommendation on the instance size. \n\n3. it's hard to give a recommendation on the notebook instance because the dataset size might vary. \n\n4. one can try experimenting with a smaller dataset sampled from the original big dataset. \n\n5. once confident with the model training code, use distributed training to run it on the complete big dataset. \n\nin summary, the answer suggests that determining the \"right\" instance type running jupyter notebook is mostly trial-and-error, and recommends using the inference recommender for automated load testing and experimenting with a smaller dataset before running the complete big dataset using distributed training.",
        "Answer_gpt_summary":"possibl solut answer determin right instanc type run jupyt notebook trial error model readi deploi infer recommend autom load test recommend instanc size hard recommend notebook instanc dataset size vari try experi smaller dataset sampl origin big dataset confid model train code us distribut train run complet big dataset summari answer suggest determin right instanc type run jupyt notebook trial error recommend infer recommend autom load test experi smaller dataset run complet big dataset distribut train"
    },
    {
        "Question_id":null,
        "Question_title":"Error: Unexpected Error - Database is Locked",
        "Question_body":"<p>Hi all! I\u2019m trying to install dvc in a new environments with no luck. I\u2019m probably messing something up on my end but I can\u2019t find it.<\/p>\n<p>install method: pip install dvc[all]<br>\nOS: Ubuntu 20.04<br>\nPython: Python3.9<br>\nPip: 21.3.1<\/p>\n<p>The install appears successful, however, any dvc command hangs, times out, and then returns the following:<\/p>\n<pre><code class=\"lang-auto\">ERROR: unexpected error - database is locked\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/main.py\", line 55, in main\n    ret = cmd.do_run()\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/command\/base.py\", line 59, in do_run\n    return self.run()\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/command\/init.py\", line 44, in run\n    with Repo.init(\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 352, in init\n    return init(\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/init.py\", line 77, in init\n    proj = Repo(root_dir)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 214, in __init__\n    self.state = State(self.root_dir, state_db_dir, self.dvcignore)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/state.py\", line 64, in __init__\n    self.links = Cache(directory=os.path.join(tmp_dir, \"links\"), **config)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 481, in __init__\n    self.reset(key, value, update=False)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 2452, in reset\n    sql('PRAGMA %s = %s' % (pragma, value)).fetchall()\nsqlite3.OperationalError: database is locked\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dvc\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/main.py\", line 88, in main\n    dvc_info = get_dvc_info()\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/info.py\", line 38, in get_dvc_info\n    with Repo() as repo:\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/repo\/__init__.py\", line 214, in __init__\n    self.state = State(self.root_dir, state_db_dir, self.dvcignore)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/dvc\/state.py\", line 64, in __init__\n    self.links = Cache(directory=os.path.join(tmp_dir, \"links\"), **config)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 481, in __init__\n    self.reset(key, value, update=False)\n  File \"\/usr\/local\/lib\/python3.9\/site-packages\/diskcache\/core.py\", line 2452, in reset\n    sql('PRAGMA %s = %s' % (pragma, value)).fetchall()\nsqlite3.OperationalError: database is locked\n<\/code><\/pre>\n<p>Could this be a permission issue with accessing one of the dvc db files?<\/p>\n<p>Any help is much appreciated as I am new to dvc.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1636415219705,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":282.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-database-is-locked\/960",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-09T00:24:09.631Z",
                "Answer_body":"<blockquote>\n<p>SQLite is meant to be a lightweight database, and thus can\u2019t support a high level of concurrency. OperationalError: database is locked errors indicate that your application is experiencing more concurrency than sqlite can handle in default configuration. This error means that one thread or process has an exclusive lock on the database connection and another thread timed out waiting for the lock the be released.<\/p>\n<\/blockquote>\n<p>Probably you have another connection in your code that is not closed or not committed and this cause this error.<\/p>",
                "Answer_score":11.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-11T17:11:23.772Z",
                "Answer_body":"<p>I have not written any code yet. I have only install dvc and attempted to initialize dvc with dvc init. Does dvc have settings for a locking strategy that can be adjusted?<\/p>",
                "Answer_score":11.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-12T14:39:29.634Z",
                "Answer_body":"<p>Looking at the <code>diskcache<\/code> code, it seems it\u2019s possible to get <code>database is unlocked<\/code> error, but <a href=\"https:\/\/github.com\/grantjenks\/python-diskcache\/blob\/master\/diskcache\/core.py#L2445-L2448\">it does try for 60 seconds before giving up<\/a>. Do you see that delay of 60 seconds before it raises any error?<\/p>\n<p>Unfortunately, I don\u2019t have any workaround to suggest.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-09T16:49:41.261Z",
                "Answer_body":"<p>I also have the same exact issue on <code>dvc.init<\/code>. Have you found a solution to this?<\/p>\n<p>Yes <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a>, it definitely waits for more than 60 seconds or so before raising the error. If <code>dvc.init<\/code> doesn\u2019t work, how is one supposed to move forward!? <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error unexpect error databas lock try instal new environ luck probabl mess end instal method pip instal ubuntu python python pip instal appear success command hang time return follow error unexpect error databas lock traceback recent file usr local lib python site packag main line main ret cmd run file usr local lib python site packag command base line run return self run file usr local lib python site packag command init line run repo init file usr local lib python site packag repo init line init return init file usr local lib python site packag repo init line init proj repo root dir file usr local lib python site packag repo init line init self state state self root dir state dir self ignor file usr local lib python site packag state line init self link cach directori path join tmp dir link config file usr local lib python site packag diskcach core line init self reset kei valu updat fals file usr local lib python site packag diskcach core line reset sql pragma pragma valu fetchal sqlite operationalerror databas lock handl except except occur traceback recent file usr local bin line sy exit main file usr local lib python site packag main line main info info file usr local lib python site packag info line info repo repo file usr local lib python site packag repo init line init self state state self root dir state dir self ignor file usr local lib python site packag state line init self link cach directori path join tmp dir link config file usr local lib python site packag diskcach core line init self reset kei valu updat fals file usr local lib python site packag diskcach core line reset sql pragma pragma valu fetchal sqlite operationalerror databas lock permiss issu access file help appreci new",
        "Question_preprocessed_content":"error unexpect error databas lock try instal new environ luck probabl mess end instal method pip instal ubuntu python pip instal appear success command hang time return follow permiss issu access file help appreci new",
        "Question_gpt_summary_original":"The user is encountering an error while trying to install dvc in a new environment. The installation appears successful, but any dvc command hangs, times out, and returns an error message stating that the database is locked. The user suspects that this could be a permission issue with accessing one of the dvc db files.",
        "Question_gpt_summary":"user encount error try instal new environ instal appear success command hang time return error messag state databas lock user suspect permiss issu access file",
        "Answer_original_content":"sqlite meant lightweight databas support high level concurr operationalerror databas lock error indic applic experienc concurr sqlite handl default configur error mean thread process exclus lock databas connect thread time wait lock releas probabl connect code close commit caus error written code instal attempt initi init set lock strategi adjust look diskcach code possibl databas unlock error try second give delai second rais error unfortun dont workaround suggest exact issu init solut ye skshetri definit wait second rais error init doesnt work suppos forward",
        "Answer_preprocessed_content":"sqlite meant lightweight databas support high level concurr operationalerror databas lock error indic applic experienc concurr sqlite handl default configur error mean thread process exclus lock databas connect thread time wait lock releas probabl connect code close commit caus error written code instal attempt initi init set lock strategi adjust look code possibl error try second give delai second rais error unfortun dont workaround suggest exact issu solut ye definit wait second rais error doesnt work suppos forward",
        "Answer_gpt_summary_original":"possible solutions to the unexpected error \"database is locked\" while installing a new environment on ubuntu 20.04 with python 3.9 and pip 21.3.1 are: checking for any unclosed or uncommitted connections in the code, adjusting the locking strategy settings, and waiting for more than 60 seconds before the error is raised. however, the answerer does not have any specific workaround to suggest.",
        "Answer_gpt_summary":"possibl solut unexpect error databas lock instal new environ ubuntu python pip check unclos uncommit connect code adjust lock strategi set wait second error rais answer specif workaround suggest"
    },
    {
        "Question_id":null,
        "Question_title":"Not able to convert Hugging Face fine-tuned BERT model into AWS Neuron",
        "Question_body":"Hi Team,\n\nI have a fine-tuned BERT model which was trained using following libraries. torch == 1.8.1+cu111 transformers == 4.19.4\n\nAnd not able to convert that fine-tuned BERT model into AWS neuron and getting following compilation errors. Could you please help me to resolve this issue?\n\nNote: Trying to compile BERT model on SageMaker notebook instance and with \"conda_python3\" conda environment.\n\nInstallation:\n\nSet Pip repository to point to the Neuron repository\n\n!pip config set global.extra-index-url https:\/\/pip.repos.neuron.amazonaws.com\n\nInstall Neuron PyTorch - Note: Tried both options below.\n\n\"#!pip install torch-neuron==1.8.1.* neuron-cc[tensorflow] \"protobuf<4\" torchvision sagemaker>=2.79.0 transformers==4.17.0 --upgrade\" !pip install --upgrade torch-neuron neuron-cc[tensorflow] \"protobuf<4\" torchvision\n\nModel compilation:\n\nimport os\nimport tensorflow  # to workaround a protobuf version conflict issue\nimport torch\nimport torch.neuron\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nmodel_path = 'model\/' # Model artifacts are stored in 'model\/' directory\n\n# load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path, torchscript=True)\n\n# create dummy input for max length 128\ndummy_input = \"dummy input which will be padded later\"\nmax_length = 128\nembeddings = tokenizer(dummy_input, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\nneuron_inputs = tuple(embeddings.values())\n\n# compile model with torch.neuron.trace and update config\nmodel_neuron = torch.neuron.trace(model, neuron_inputs)\nmodel.config.update({\"traced_sequence_length\": max_length})\n\n# save tokenizer, neuron model and config for later use\nsave_dir=\"tmpd\"\nos.makedirs(\"tmpd\",exist_ok=True)\nmodel_neuron.save(os.path.join(save_dir,\"neuron_model.pt\"))\ntokenizer.save_pretrained(save_dir)\nmodel.config.save_pretrained(save_dir)\n\n\nModel artifacts: We have got this model artifacts from multi-label topic classification model.\n\nconfig.json model.tar.gz pytorch_model.bin special_tokens_map.json tokenizer_config.json tokenizer.json\n\nError logs:\n\nINFO:Neuron:There are 3 ops of 1 different types in the TorchScript that are not compiled by neuron-cc: aten::embedding, (For more information see https:\/\/github.com\/aws\/aws-neuron-sdk\/blob\/master\/release-notes\/neuron-cc-ops\/neuron-cc-ops-pytorch.md)\nINFO:Neuron:Number of arithmetic operators (pre-compilation) before = 565, fused = 548, percent fused = 96.99%\nINFO:Neuron:Number of neuron graph operations 1601 did not match traced graph 1323 - using heuristic matching of hierarchical information\nWARNING:tensorflow:From \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/ops\/aten.py:2022: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nINFO:Neuron:Compiling function _NeuronGraph$698 with neuron-cc\nINFO:Neuron:Compiling with command line: '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/neuron-cc compile \/tmp\/tmpv4gg13ze\/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output \/tmp\/tmpv4gg13ze\/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Linear_5\/aten_linear\/Add:0\"]} --verbose 35'\nINFO:Neuron:Compile command returned: -9\nWARNING:Neuron:torch.neuron.trace failed on _NeuronGraph$698; falling back to native python function call\nERROR:Neuron:neuron-cc failed with the following command line call:\n\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/neuron-cc compile \/tmp\/tmpv4gg13ze\/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output \/tmp\/tmpv4gg13ze\/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Linear_5\/aten_linear\/Add:0\"]}' --verbose 35\nTraceback (most recent call last):\n  File \"\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/convert.py\", line 382, in op_converter\n    item, inputs, compiler_workdir=sg_workdir, **kwargs)\n  File \"\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/decorators.py\", line 220, in trace\n    'neuron-cc failed with the following command line call:\\n{}'.format(command))\nsubprocess.SubprocessError: neuron-cc failed with the following command line call:\n\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/neuron-cc compile \/tmp\/tmpv4gg13ze\/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output \/tmp\/tmpv4gg13ze\/graph_def.neff --io-config '{\"inputs\": {\"0:0\": [[1, 128, 768], \"float32\"], \"1:0\": [[1, 1, 1, 128], \"float32\"]}, \"outputs\": [\"Linear_5\/aten_linear\/Add:0\"]}' --verbose 35\nINFO:Neuron:Number of arithmetic operators (post-compilation) before = 565, compiled = 0, percent compiled = 0.0%\nINFO:Neuron:The neuron partitioner created 1 sub-graphs\nINFO:Neuron:Neuron successfully compiled 0 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 0.0%\nINFO:Neuron:Compiled these operators (and operator counts) to Neuron:\nINFO:Neuron:Not compiled operators (and operator counts) to Neuron:\nINFO:Neuron: => aten::Int: 97 [supported]\nINFO:Neuron: => aten::add: 39 [supported]\nINFO:Neuron: => aten::contiguous: 12 [supported]\nINFO:Neuron: => aten::div: 12 [supported]\nINFO:Neuron: => aten::dropout: 38 [supported]\nINFO:Neuron: => aten::embedding: 3 [not supported]\nINFO:Neuron: => aten::gelu: 12 [supported]\nINFO:Neuron: => aten::layer_norm: 25 [supported]\nINFO:Neuron: => aten::linear: 74 [supported]\nINFO:Neuron: => aten::matmul: 24 [supported]\nINFO:Neuron: => aten::mul: 1 [supported]\nINFO:Neuron: => aten::permute: 48 [supported]\nINFO:Neuron: => aten::rsub: 1 [supported]\nINFO:Neuron: => aten::select: 1 [supported]\nINFO:Neuron: => aten::size: 97 [supported]\nINFO:Neuron: => aten::slice: 5 [supported]\nINFO:Neuron: => aten::softmax: 12 [supported]\nINFO:Neuron: => aten::tanh: 1 [supported]\nINFO:Neuron: => aten::to: 1 [supported]\nINFO:Neuron: => aten::transpose: 12 [supported]\nINFO:Neuron: => aten::unsqueeze: 2 [supported]\nINFO:Neuron: => aten::view: 48 [supported]\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-1-97bba321d013> in <module>\n     18 \n     19 # compile model with torch.neuron.trace and update config\n---> 20 model_neuron = torch.neuron.trace(model, neuron_inputs)\n     21 model.config.update({\"traced_sequence_length\": max_length})\n     22 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/convert.py in trace(func, example_inputs, fallback, op_whitelist, minimum_segment_size, subgraph_builder_function, subgraph_inputs_pruning, skip_compiler, debug_must_trace, allow_no_ops_on_neuron, compiler_workdir, dynamic_batch_size, compiler_timeout, _neuron_trace, compiler_args, optimizations, verbose, **kwargs)\n    182         logger.debug(\"skip_inference_context - trace with fallback at {}\".format(get_file_and_line()))\n    183         neuron_graph = cu.compile_fused_operators(neuron_graph, **compile_kwargs)\n--> 184     cu.stats_post_compiler(neuron_graph)\n    185 \n    186     # Wrap the compiled version of the model in a script module. Note that this is\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/torch_neuron\/convert.py in stats_post_compiler(self, neuron_graph)\n    491         if succesful_compilations == 0 and not self.allow_no_ops_on_neuron:\n    492             raise RuntimeError(\n--> 493                 \"No operations were successfully partitioned and compiled to neuron for this model - aborting trace!\")\n    494 \n    495         if percent_operations_compiled < 50.0:\n\nRuntimeError: No operations were successfully partitioned and compiled to neuron for this model - aborting trace!\n\n\nThanks a lot.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657030067879,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":114.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzDs6ITDqQYegTLVfTmNKOA\/not-able-to-convert-hugging-face-fine-tuned-bert-model-into-aws-neuron",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-07T13:27:24.982Z",
                "Answer_score":0,
                "Answer_body":"The error message in your log shows Compile command returned: -9. This message typically indicates that the compiler process was killed. Normally this is due to the the OOM (out of memory) killer (run by the linux operating system) killing the compilation process due to memory exhaustion. The most recent version of torch-neuron should provide an updated message for -9 errors that reflects the typical cause for this failure mode.\n\nWe recommend you try compiling on an instance with more memory, such as an inf1.6xlarge. Note: you only need the larger instance for compilation; you can still use a smaller instance (such as an inf1.xlarge) to run inference.\n\nPlease let us know if compiling on a larger instance resolved the error you\u2019re seeing.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"abl convert hug face fine tune bert model aw neuron team fine tune bert model train follow librari torch transform abl convert fine tune bert model aw neuron get follow compil error help resolv issu note try compil bert model notebook instanc conda python conda environ instal set pip repositori point neuron repositori pip config set global extra index url http pip repo neuron amazonaw com instal neuron pytorch note tri option pip instal torch neuron neuron tensorflow protobuf transform upgrad pip instal upgrad torch neuron neuron tensorflow protobuf aten int support info neuron aten add support info neuron aten contigu support info neuron aten div support info neuron aten dropout support info neuron aten embed support info neuron aten gelu support info neuron aten layer norm support info neuron aten linear support info neuron aten matmul support info neuron aten mul support info neuron aten permut support info neuron aten rsub support info neuron aten select support info neuron aten size support info neuron aten slice support info neuron aten softmax support info neuron aten tanh support info neuron aten support info neuron aten transpos support info neuron aten unsqueez support info neuron aten view support runtimeerror traceback recent compil model torch neuron trace updat config model neuron torch neuron trace model neuron input model config updat trace sequenc length max length anaconda env python lib python site packag torch neuron convert trace func exampl input fallback whitelist minimum segment size subgraph builder function subgraph input prune skip compil debug trace allow op neuron compil workdir dynam batch size compil timeout neuron trace compil arg optim verbos kwarg logger debug skip infer context trace fallback format file line neuron graph compil fuse oper neuron graph compil kwarg stat post compil neuron graph wrap compil version model script modul note anaconda env python lib python site packag torch neuron convert stat post compil self neuron graph succes compil self allow op neuron rais runtimeerror oper successfulli partit compil neuron model abort trace percent oper compil runtimeerror oper successfulli partit compil neuron model abort trace thank lot",
        "Question_preprocessed_content":"abl convert hug face bert model aw neuron team bert model train follow librari torch transform abl convert bert model aw neuron get follow compil error help resolv issu note try compil bert model notebook instanc conda environ instal set pip repositori point neuron repositori pip config set instal neuron pytorch note tri option pip instal protobuf pip instal protobuf aten int info neuron aten add info neuron aten contigu info neuron aten div info neuron aten dropout info neuron aten embed info neuron aten gelu info neuron info neuron aten linear info neuron aten matmul info neuron aten mul info neuron aten permut info neuron aten rsub info neuron aten select info neuron aten size info neuron aten slice info neuron aten softmax info neuron aten tanh info neuron aten info neuron aten transpos info neuron aten unsqueez info neuron aten view runtimeerror traceback compil model updat config trace trace fallback wrap compil version model script modul note rais runtimeerror runtimeerror oper successfulli partit compil neuron model abort trace thank lot",
        "Question_gpt_summary_original":"The user is facing challenges in converting a fine-tuned BERT model trained using torch and transformers libraries into AWS Neuron. The user is encountering compilation errors while trying to compile the model on SageMaker notebook instance with \"conda_python3\" conda environment. The error logs indicate that the model is failing to compile with neuron-cc and falling back to native python function call. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face challeng convert fine tune bert model train torch transform librari aw neuron user encount compil error try compil model notebook instanc conda python conda environ error log indic model fail compil neuron fall nativ python function user seek help resolv issu",
        "Answer_original_content":"error messag log show compil command return messag typic indic compil process kill normal oom memori killer run linux oper kill compil process memori exhaust recent version torch neuron provid updat messag error reflect typic caus failur mode recommend try compil instanc memori inf xlarg note need larger instanc compil us smaller instanc inf xlarg run infer let know compil larger instanc resolv error your see",
        "Answer_preprocessed_content":"error messag log show compil command return messag typic indic compil process kill normal oom killer kill compil process memori exhaust recent version provid updat messag error reflect typic caus failur mode recommend try compil instanc memori note need larger instanc compil us smaller instanc run infer let know compil larger instanc resolv error your see",
        "Answer_gpt_summary_original":"possible solutions to the compilation errors encountered when converting a hugging face fine-tuned bert model into aws neuron are to try compiling on an instance with more memory, such as an inf1.6xlarge, and to use a smaller instance (such as an inf1.xlarge) to run inference.",
        "Answer_gpt_summary":"possibl solut compil error encount convert hug face fine tune bert model aw neuron try compil instanc memori inf xlarg us smaller instanc inf xlarg run infer"
    },
    {
        "Question_id":null,
        "Question_title":"GOCD Plugin for MLflow",
        "Question_body":"Hi All,\n\n\nWe have built a new GoCD plugin[1] which works with mlflow and helps us in continuous delivery of models.\n\nHope Y'all find this useful.\u00a0\n\n\n[1] https:\/\/github.com\/indix\/mlflow-gocd\/\n\n\nBest,\nKrishna Sangeeth",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1541984493000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":22.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CIihDdCS014",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-20T21:44:38",
                "Answer_body":"This is very cool, Krishna; thanks for sharing! We also just saw your blog post at https:\/\/stacktoheap.com\/blog\/2018\/11\/19\/mlflow-model-repository-ci-cd\/.\n\nIf you\u2019d like, we could feature this blog on mlflow.org or link to the plugin in the docs somewhere; please let us know whether you\u2019d be interested.\n\nMatei\n\n\ue5d3\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CADub9hP%3DtTiJ6uMLL6pauzq_BorF26OD0mvp%3D9gzMhOkVuZesw%40mail.gmail.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-11-21T01:43:34",
                "Answer_body":"<Reposting after joining group>\n\n\nHi Matei,\n\n\nThanks for the response.\n\n\nWe would definitely be happy if you can feature the blog and\/or the plugin. Let us know if you need anything from us.\n\n\nThanks\nManoj\n\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"gocd plugin built new gocd plugin work help continu deliveri model hope us http github com indix gocd best krishna sangeeth",
        "Question_preprocessed_content":"gocd plugin built new gocd plugin work help continu deliveri model hope us best krishna sangeeth",
        "Question_gpt_summary_original":"The user has encountered challenges in implementing continuous delivery of models using mlflow and GoCD, which led them to develop a new GoCD plugin that works with mlflow.",
        "Question_gpt_summary":"user encount challeng implement continu deliveri model gocd led develop new gocd plugin work",
        "Answer_original_content":"cool krishna thank share saw blog post http stacktoheap com blog model repositori youd like featur blog org link plugin doc let know youd interest matei receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user cadubhp dttijumllpauzq borfodmvp dgzmhokvuzesw mail gmail com option visit http group googl com optout matei thank respons definit happi featur blog plugin let know need thank manoj",
        "Answer_preprocessed_content":"cool krishna thank share saw blog post youd like featur blog org link plugin doc let know youd interest matei receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit matei thank respons definit happi featur blog plugin let know need thank manoj",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_id":null,
        "Question_title":"AWS Sagemaker - Either the training channel is empty or the mini-batch size",
        "Question_body":"I am trying to train a linear learner model in Sagemaker. My training set is 422 rows split into 4 files on AWS S3. The mini-batch size that I set is 50.\n\nI keep on getting this error in Sagemaker.\n\nCustomer Error: No training data processed. Either the training\nchannel is empty or the mini-batch size is too high. Verify that\ntraining data contains non-empty files and the mini-batch size is less\nthan the number of records per training host.\n\nI am using this InputDataConfig\n\nInputDataConfig=\\[  \n            {  \n                'ChannelName': 'train',  \n                'DataSource': {  \n                    'S3DataSource': {  \n                        'S3DataType': 'S3Prefix',  \n                        'S3Uri': 's3:\/\/MY_S3_BUCKET\/REST_OF_PREFIX\/exported\/',  \n                        'S3DataDistributionType': 'FullyReplicated'  \n                    }  \n                },  \n                'ContentType': 'text\/csv',  \n                'CompressionType': 'Gzip'  \n            }  \n        ],  \n\n\nI am not sure what I am doing wrong here. I tried increasing the number of records to 5547495 split across 6 files. The same error. That makes me think that somehow the config itself has something missing. Due to which it seems to think training channel is just not present. I tried changing 'train' to 'training' as that is what the erorr message is saying. But then I got\n\nCustomer Error: Unable to initialize the algorithm. Failed to validate\ninput data configuration. (caused by ValidationError)\n\nCaused by: {u'training': {u'TrainingInputMode': u'Pipe',\nu'ContentType': u'text\/csv', u'RecordWrapperType': u'None',\nu'S3DistributionType': u'FullyReplicated'}} is not valid under any of\nthe given schemas\n\nI went back to train as that seems to be what is needed. But what am I doing wrong with that?\n\nEdited by: anshbansal on Jun 3, 2019 12:06 AM",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559545390000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq0KSPPCBT1qrotLu0NJyBw\/aws-sagemaker-either-the-training-channel-is-empty-or-the-mini-batch-size",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-03T13:03:18.000Z",
                "Answer_score":0,
                "Answer_body":"Found the problem. The CompressionType was mentioned as 'Gzip' but I had changed the actual file to be not compressed when doing the exports. As soon as I changed it to be 'None' the training went smoothly.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"train channel mini batch size try train linear learner model train set row split file aw mini batch size set get error custom error train data process train channel mini batch size high verifi train data contain non file mini batch size number record train host inputdataconfig inputdataconfig channelnam train datasourc sdatasourc sdatatyp sprefix suri bucket rest prefix export sdatadistributiontyp fullyrepl contenttyp text csv compressiontyp gzip sure wrong tri increas number record split file error make think config miss think train channel present tri chang train train erorr messag sai got custom error unabl initi algorithm fail valid input data configur caus validationerror caus train traininginputmod pipe contenttyp text csv recordwrappertyp sdistributiontyp fullyrepl valid given schema went train need wrong edit anshbans jun",
        "Question_preprocessed_content":"train channel size try train linear learner model train set row split file aw size set get error custom error train data process train channel size high verifi train data contain file size number record train host inputdataconfig contenttyp compressiontyp gzip sure wrong tri increas number record split file error make think config miss think train channel present tri chang train train erorr messag sai got custom error unabl initi algorithm fail valid input data configur caus valid given schema went train need wrong edit anshbans jun",
        "Question_gpt_summary_original":"The user is encountering an error while trying to train a linear learner model in AWS Sagemaker. The error message suggests that either the training channel is empty or the mini-batch size is too high. The user has tried increasing the number of records and changing the channel name but is still unable to resolve the issue. The user suspects that there may be something missing in the InputDataConfig.",
        "Question_gpt_summary":"user encount error try train linear learner model error messag suggest train channel mini batch size high user tri increas number record chang channel unabl resolv issu user suspect miss inputdataconfig",
        "Answer_original_content":"problem compressiontyp mention gzip chang actual file compress export soon chang train went smoothli",
        "Answer_preprocessed_content":"problem compressiontyp mention gzip chang actual file compress export soon chang train went smoothli",
        "Answer_gpt_summary_original":"solution: the user should check the compression type of the file they are trying to export and ensure that it matches the compression type mentioned in the code. if the compression type is different, they should change it to match the code. specifically, if the compression type is mentioned as 'gzip' in the code, the file should also be compressed as 'gzip'. if the compression type is mentioned as 'none', the file should not be compressed.",
        "Answer_gpt_summary":"solut user check compress type file try export ensur match compress type mention code compress type differ chang match code specif compress type mention gzip code file compress gzip compress type mention file compress"
    },
    {
        "Question_id":null,
        "Question_title":"Problem of not seeing all the images in media,",
        "Question_body":"<p>In the workspace\/images, i am only seeing 30 images, when with oldest images getting deleted every time. Is there any limits that this images slot can show? If yes, is there any way that i can see more images that is saved on locally saved log folder??<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1666267346925,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":380.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/problem-of-not-seeing-all-the-images-in-media\/3292",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-20T13:10:32.472Z",
                "Answer_body":"<p>Hi ilhoon,<\/p>\n<p>Thanks for writing in! I was wondering if you could explain me how are you logging these images and if you could send me a link to the Workspace and so I can see the error. Thanks!<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-25T09:27:47.101Z",
                "Answer_body":"<p>Hi ilhoon,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-19T12:02:46.431Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"problem see imag media workspac imag see imag oldest imag get delet time limit imag slot ye wai imag save local save log folder",
        "Question_preprocessed_content":"problem see imag media see imag oldest imag get delet time limit imag slot ye wai imag save local save log folder",
        "Question_gpt_summary_original":"The user is facing a challenge of not being able to see all the images in the media folder as only 30 images are visible and the oldest ones get deleted. The user is seeking information on whether there is a limit to the number of images that can be displayed and if there is a way to view more images saved in the locally saved log folder.",
        "Question_gpt_summary":"user face challeng abl imag media folder imag visibl oldest on delet user seek inform limit number imag displai wai view imag save local save log folder",
        "Answer_original_content":"ilhoon thank write wonder explain log imag send link workspac error thank best lui ilhoon want follow support request heard let know assist issu resolv best lui topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"ilhoon thank write wonder explain log imag send link workspac error thank best lui ilhoon want follow support request heard let know assist issu resolv best lui topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no possible solutions provided in the answer. the answer is a response from a support team member asking for more information and offering further assistance.",
        "Answer_gpt_summary":"possibl solut provid answer answer respons support team member ask inform offer assist"
    },
    {
        "Question_id":null,
        "Question_title":"Plotting array-like values in a parallel coordinates plot",
        "Question_body":"<p>Hi,<\/p>\n<p>I have set up a sweep where one of the parameters possible values (arrays) are [0, 1, 3, 5], [0, 1, 5, 8], [0, 1, 8, 11], etc. The sweep is working fine and the model receives the correct value from the agent, but when visualizing the sweep using the parallel coordinates plot, every possible value of this hyperparameter is plotted at zero, since it is the first element of every array.<\/p>\n<p>I have thought of two solutions:<\/p>\n<ul>\n<li>Download all the data using the API, modify each value with an alias, and reupload the data.<\/li>\n<li>Write a custom plot to modify how that axe is plotted.<\/li>\n<\/ul>\n<p>However, I would like to know if there is a more straightforward solution so that each experiment line correctly passes through its value of this hyperparameter.<\/p>\n<p>Thank you in advance!<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1637668401807,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":280.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/plotting-array-like-values-in-a-parallel-coordinates-plot\/1371",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-23T20:43:14.807Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/guillesanbri\">@guillesanbri<\/a> , could you please link me to your sweeps page so I can take a look for you?<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-23T22:17:23.912Z",
                "Answer_body":"<p>Sure <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a> , here is the link <a href=\"https:\/\/wandb.ai\/guillesanbri\/DPT\/sweeps\/vyv6zl0c\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>\n<p>If you hover with the mouse over different runs, you can see that the hooks parameter is indeed different while going through zero.<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-29T17:13:50.721Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/guillesanbri\">@guillesanbri<\/a> , are the values for hooks multiple different lists?<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-30T12:45:21.395Z",
                "Answer_body":"<p>Yep <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a> , I would like each value (each different list) to be shown as a string like if they were categorical values in the plots, if that makes sense. I have remedied it in the following experiments defining the values as strings (ie. \u201ch[0,1,3,5]\u201d) and processing them inside the training script.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-06T23:56:03.290Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/guillesanbri\">@guillesanbri<\/a>, that\u2019s a good workaround and overall a good feature suggestion. I\u2019ll file something internally and keep you updated when there\u2019s progress on this.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-13T08:37:20.460Z",
                "Answer_body":"<p>Great! Thank you <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a> !<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-11T08:37:45.279Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"plot arrai like valu parallel coordin plot set sweep paramet possibl valu arrai sweep work fine model receiv correct valu agent visual sweep parallel coordin plot possibl valu hyperparamet plot zero element arrai thought solut download data api modifi valu alia reupload data write custom plot modifi ax plot like know straightforward solut experi line correctli pass valu hyperparamet thank advanc",
        "Question_preprocessed_content":"plot valu parallel coordin plot set sweep paramet possibl valu sweep work fine model receiv correct valu agent visual sweep parallel coordin plot possibl valu hyperparamet plot zero element arrai thought solut download data api modifi valu alia reupload data write custom plot modifi ax plot like know straightforward solut experi line correctli pass valu hyperparamet thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge while plotting array-like values in a parallel coordinates plot. The hyperparameter values are arrays, and every possible value of this hyperparameter is plotted at zero, since it is the first element of every array. The user has thought of two solutions, but is looking for a more straightforward solution so that each experiment line correctly passes through its value of this hyperparameter.",
        "Question_gpt_summary":"user face challeng plot arrai like valu parallel coordin plot hyperparamet valu arrai possibl valu hyperparamet plot zero element arrai user thought solut look straightforward solut experi line correctli pass valu hyperparamet",
        "Answer_original_content":"guillesanbri link sweep page look sure aidanjd link hover mous differ run hook paramet differ go zero guillesanbri valu hook multipl differ list yep aidanjd like valu differ list shown string like categor valu plot make sens remedi follow experi defin valu string process insid train script guillesanbri that good workaround overal good featur suggest ill file intern updat there progress great thank aidanjd topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"link sweep page look sure link hover mous differ run hook paramet differ go zero valu hook multipl differ list yep like valu shown string like categor valu plot make sens remedi follow experi defin valu string process insid train script that good workaround overal good featur suggest ill file intern updat there progress great thank topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the user is facing a challenge of plotting array-like values in a parallel coordinates plot, where each experiment line does not correctly pass through its value of the hyperparameter. the solution suggested is to define the values as strings and process them inside the training script. the user has already remedied the issue in the following experiments. the suggestion is considered a good workaround and a good feature suggestion. the user is waiting for updates on the progress of the feature suggestion.",
        "Answer_gpt_summary":"user face challeng plot arrai like valu parallel coordin plot experi line correctli pass valu hyperparamet solut suggest defin valu string process insid train script user remedi issu follow experi suggest consid good workaround good featur suggest user wait updat progress featur suggest"
    },
    {
        "Question_id":null,
        "Question_title":"ADFS Authentication to MLflow",
        "Question_body":"Hi all,\u00a0\n\n\nI was just wondering if there is any simpler way to apply Authentication to mlflow dashboards using active directory apart from nginx,\u00a0 or if any plugin is available to serve the purpose.\u00a0\n\n\nThanks\nIsha",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603122246000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":95.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/l3udtZwwdbU",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-19T14:39:56",
                "Answer_body":"I would like know it too.\nAt this moment I'm trying to user oauth2_proxy with Nginx.\nhttps:\/\/github.com\/bitly\/oauth2_proxy.\n\n\nlamba.isha12 do you use something like that?\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"adf authent wonder simpler wai appli authent dashboard activ directori apart nginx plugin avail serv purpos thank isha",
        "Question_preprocessed_content":"adf authent wonder simpler wai appli authent dashboard activ directori apart nginx plugin avail serv purpos thank isha",
        "Question_gpt_summary_original":"The user is seeking a simpler way to apply authentication to MLflow dashboards using active directory, without having to use nginx or any available plugin.",
        "Question_gpt_summary":"user seek simpler wai appli authent dashboard activ directori have us nginx avail plugin",
        "Answer_original_content":"like know moment try user oauth proxi nginx http github com bitli oauth proxi lamba isha us like",
        "Answer_preprocessed_content":"like know moment try user nginx us like",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_id":57291797.0,
        "Question_title":"Dask: AttributeError: 'DataFrame' object has no attribute '_getitem_array'",
        "Question_body":"<p>I've got some data on S3 bucket that I want to work with. <\/p>\n\n<p>I've imported it using:<\/p>\n\n<pre><code>import boto3\nimport dask.dataframe as dd\n\ndef import_df(key):\n        s3 = boto3.client('s3')\n        df = dd.read_csv('s3:\/\/...\/' + key ,encoding='latin1')\n        return df\n\nkey = 'Churn\/CLEANED_data\/file.csv'\ntrain = import_df(key)\n<\/code><\/pre>\n\n<p>I can see that the data has been imported correctly using:<\/p>\n\n<pre><code>train.head()\n<\/code><\/pre>\n\n<p>but when I try simple operation (<a href=\"https:\/\/docs.dask.org\/en\/latest\/dataframe.html\" rel=\"nofollow noreferrer\">taken from this dask doc<\/a>):<\/p>\n\n<pre><code>train_churn = train[train['CON_CHURN_DECLARATION'] == 1]\ntrain_churn.compute()\n<\/code><\/pre>\n\n<p>I've got Error:<\/p>\n\n<blockquote>\n  <p>AttributeError                            Traceback (most recent call\n  last)  in ()<\/p>\n  \n  <p>1 train_churn = train[train['CON_CHURN_DECLARATION'] == 1]<\/p>\n  \n  <p>----> 2 train_churn.compute()<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/dask\/base.py in\n  compute(self, **kwargs)\n      152         dask.base.compute\n      153         \"\"\"\n  --> 154         (result,) = compute(self, traverse=False, **kwargs)\n      155         return result\n      156<\/p>\n  \n  <p>AttributeError: 'DataFrame' object has no attribute '_getitem_array'<\/p>\n<\/blockquote>\n\n<p>Full error here: <a href=\"https:\/\/textuploader.com\/11lg7\" rel=\"nofollow noreferrer\">Error Upload<\/a><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_creation_time":1564579246737,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":2742.0,
        "Answer_body":"<p>I was facing a similar issue when trying to read from s3 files, ultimately solved by updating dask to most recent version (I think the one sagemaker instances start with by default is deprecated)<\/p>\n\n<h2>Install\/Upgrade packages and dependencies (from notebook)<\/h2>\n\n<pre><code>! python -m pip install --upgrade dask\n! python -m pip install fsspec\n! python -m pip install --upgrade s3fs\n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1574362465680,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57291797",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1574121568172,
        "Question_original_content":"dask attributeerror datafram object attribut getitem arrai got data bucket want work import import boto import dask datafram def import kei boto client read csv kei encod latin return kei churn clean data file csv train import kei data import correctli train head try simpl oper taken dask doc train churn train train churn declar train churn comput got error attributeerror traceback recent train churn train train churn declar train churn comput anaconda env python lib python site packag dask base comput self kwarg dask base comput result comput self travers fals kwarg return result attributeerror datafram object attribut getitem arrai error error upload",
        "Question_preprocessed_content":"dask attributeerror datafram object attribut got data bucket want work import data import correctli try simpl oper got error attributeerror traceback comput comput return result attributeerror datafram object attribut error error upload",
        "Question_gpt_summary_original":"The user is encountering an error while trying to perform a simple operation on a Dask DataFrame imported from an S3 bucket using the dd.read_csv() function. The error message states that the 'DataFrame' object has no attribute '_getitem_array'.",
        "Question_gpt_summary":"user encount error try perform simpl oper dask datafram import bucket read csv function error messag state datafram object attribut getitem arrai",
        "Answer_original_content":"face similar issu try read file ultim solv updat dask recent version think instanc start default deprec instal upgrad packag depend notebook python pip instal upgrad dask python pip instal fsspec python pip instal upgrad sf hope help",
        "Answer_preprocessed_content":"face similar issu try read file ultim solv updat dask recent version packag depend hope help",
        "Answer_gpt_summary_original":"the solution to the attributeerror encountered when attempting to perform a simple operation on a dask dataframe imported from an s3 bucket is to update dask to the most recent version, install\/upgrade packages and dependencies, and run the following commands from the notebook: \n\n! python -m pip install --upgrade dask \n! python -m pip install fsspec \n! python -m pip install --upgrade s3fs",
        "Answer_gpt_summary":"solut attributeerror encount attempt perform simpl oper dask datafram import bucket updat dask recent version instal upgrad packag depend run follow command notebook python pip instal upgrad dask python pip instal fsspec python pip instal upgrad sf"
    },
    {
        "Question_id":null,
        "Question_title":"Multiuser environment and Slurm",
        "Question_body":"Hi,\u00a0\n\n\nI have two questions about mlflow.\u00a0\n\n\nIs it possible to use mlflow for multiuser environment ? How they can access gui running their own experiments ?\u00a0\nIs there a way to integrate slurm job manager in mlflow ?\u00a0\n\n\nPlease let me know. Thank you.",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1539159503000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":null,
        "Question_view_count":330.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/33RwUm-BPh0",
        "Tool":"MLflow",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-10-10T09:51:29",
                "Answer_body":"MLflow can definitely be used in multiuser environments. For these, simply stand up a hosted tracking server, and point all users at the same one (and users can manage or share experiments).\n\n\nSince MLflow is API-based, I think you can simply use MLflow within a Slurm scheduler, and that would create runs appropriately within MLflow. Do you foresee some issue with that approach?\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/4c33ab2a-c9ca-4820-ae68-74257beaca24%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-10-10T14:45:07",
                "Answer_body":"Hi Aaron,\u00a0\n\n\nThank you for quick response. I am looking into Hosted Tracking Server. How does authentication system work in it ?\u00a0\nFor Slurm and MLflow, my understanding is that there should be an interface in MLflow that can use underlying resource managers such as slurm, PBS or others.\u00a0\nHas anyone tried to use any of resource manager with MLflow ?\u00a0\n\n\n\n\npragnesh\n\ue5d3"
            },
            {
                "Answer_creation_time":"2018-10-10T21:38:13",
                "Answer_body":"Currently, authentication is expected to be handled outside of MLflow -- that is, by setting up an nginx server or OAuth proxy server in front of MLflow.\n\n\nOn the Slurm side, unfortunately I am not familiar enough to say how that integration might work.\n\n\n\ue5d3\n\ue5d3\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/cd07b4e2-61d7-43cf-94f9-24388b339348%40googlegroups.com.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"multius environ slurm question possibl us multius environ access gui run experi wai integr slurm job manag let know thank",
        "Question_preprocessed_content":"multius environ slurm question possibl us multius environ access gui run experi wai integr slurm job manag let know thank",
        "Question_gpt_summary_original":"The user is facing challenges related to using mlflow in a multiuser environment and integrating slurm job manager in mlflow. They are specifically asking if it is possible for multiple users to access the mlflow GUI to run their own experiments and if there is a way to integrate slurm job manager with mlflow.",
        "Question_gpt_summary":"user face challeng relat multius environ integr slurm job manag specif ask possibl multipl user access gui run experi wai integr slurm job manag",
        "Answer_original_content":"definit multius environ simpli stand host track server point user user manag share experi api base think simpli us slurm schedul creat run appropri forese issu approach receiv messag subscrib googl group user group unsubscrib group stop receiv email send email user googlegroup com post group send email googlegroup com view discuss web visit http group googl com msgid user caba cca beaca googlegroup com option visit http group googl com optout aaron thank quick respons look host track server authent work slurm understand interfac us underli resourc manag slurm pb tri us resourc manag pragnesh current authent expect handl outsid set nginx server oauth proxi server slurm unfortun familiar integr work view discuss web visit http group googl com msgid user cdbe googlegroup com",
        "Answer_preprocessed_content":"definit multius environ simpli stand host track server point user think simpli us slurm schedul creat run appropri forese issu approach receiv messag subscrib googl group group unsubscrib group stop receiv email send email post group send email view discuss web visit option visit aaron thank quick respons look host track server authent work slurm understand interfac us underli resourc manag slurm pb tri us resourc manag pragnesh current authent expect handl outsid set nginx server oauth proxi server slurm unfortun familiar integr work view discuss web visit",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- MLflow can be used in multiuser environments by setting up a hosted tracking server and pointing all users at the same one.\n- MLflow is API-based and can be used within a Slurm scheduler to create runs appropriately within MLflow.\n\nNo solution was provided for the issue of using resource managers such as Slurm with MLflow.",
        "Answer_gpt_summary":"possibl solut mention discuss multius environ set host track server point user api base slurm schedul creat run appropri solut provid issu resourc manag slurm"
    },
    {
        "Question_id":58117200.0,
        "Question_title":"How to get status of Azure machine learning service pipeline run using Rest Api?",
        "Question_body":"<p>I have created an Azure Machine Learning Service Pipeline which i am invoking externally using its rest endpoint.\nBut i also need to monitor its run , whether it got completed or failed, periodically.\n<strong>Is there a methodinside a machine learning pipeline's rest endpoint, which i can hit to check its run status?<\/strong>\nI have tried the steps mentioned in the link here \n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1569502183603,
        "Question_favorite_count":null,
        "Question_last_edit_time":1569502283203,
        "Question_score":1.0,
        "Question_view_count":898.0,
        "Answer_body":"<p>For getting status of run, you can use REST APIs described here <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane<\/a> <\/p>\n\n<p>Specifically you need <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json<\/a><\/p>\n\n<p>use this call to get run information including status:<\/p>\n\n<blockquote>\n  <p>\/history\/v1.0\/subscriptions\/{subscriptionId}\/resourceGroups\/{resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/{workspaceName}\/experiments\/{experimentName}\/runs\/{runId}\/details<\/p>\n<\/blockquote>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58117200",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1569516992870,
        "Question_original_content":"statu servic pipelin run rest api creat servic pipelin invok extern rest endpoint need monitor run got complet fail period methodinsid machin learn pipelin rest endpoint hit check run statu tri step mention link http github com azur machinelearningnotebook blob master us machin learn pipelin pipelin batch score pipelin batch score ipynb",
        "Question_preprocessed_content":"statu servic pipelin run rest api creat servic pipelin invok extern rest endpoint need monitor run got complet fail period methodinsid machin learn pipelin rest endpoint hit check run statu tri step mention link",
        "Question_gpt_summary_original":"The user is facing a challenge in monitoring the status of an Azure Machine Learning Service Pipeline run using its Rest API. They are seeking a method to check the run status periodically and have tried the steps mentioned in a provided link without success.",
        "Question_gpt_summary":"user face challeng monitor statu servic pipelin run rest api seek method check run statu period tri step mention provid link success",
        "Answer_original_content":"get statu run us rest api describ http github com azur azur rest api spec tree master specif machinelearningservic data plane specif need http github com azur azur rest api spec blob master specif machinelearningservic data plane microsoft machinelearningservic preview runhistori json us run inform includ statu histori subscript subscriptionid resourcegroup resourcegroupnam provid microsoft machinelearningservic workspac workspacenam experi experimentnam run runid detail",
        "Answer_preprocessed_content":"get statu run us rest api describ specif need us run inform includ statu",
        "Answer_gpt_summary_original":"the answer suggests that the user can use rest apis to check the run status of a machine learning pipeline. specifically, the user can use the \/history\/v1.0\/subscriptions\/{subscriptionid}\/resourcegroups\/{resourcegroupname}\/providers\/microsoft.machinelearningservices\/workspaces\/{workspacename}\/experiments\/{experimentname}\/runs\/{runid}\/details call to get run information, including status.",
        "Answer_gpt_summary":"answer suggest user us rest api check run statu machin learn pipelin specif user us histori subscript subscriptionid resourcegroup resourcegroupnam provid microsoft machinelearningservic workspac workspacenam experi experimentnam run runid detail run inform includ statu"
    },
    {
        "Question_id":null,
        "Question_title":"First run of DVC - getting a \"failed to reproduce\" error",
        "Question_body":"<p>Hi,<br>\nI just cloned a project from a colleague; in this project there is a <em>.dvc<\/em> file, with some <em>deps<\/em> and some <em>outs<\/em>.<br>\nWhen I run <em>dvc repro -f myfile.dvc<\/em>, I got an error<\/p>\n<blockquote>\n<p>\u2026failed to reproduce \u2018myfile.dvc\u2019: output \u2018xxx\u2019 does not exist<\/p>\n<\/blockquote>\n<p>Well, obviously the <em>output<\/em> does not exist, this is the first time I run the script, so nothing was created yet !<br>\nI got the same error when I run with the \u2018\u2013force\u2019 flag<\/p>\n<p>How do I run the pipeline for the first time ?<br>\nI am missing something ?<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1554389891804,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":3811.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/first-run-of-dvc-getting-a-failed-to-reproduce-error\/171",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-04-04T15:05:35.809Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/spiette\">@spiette<\/a> !<\/p>\n<p>Could you show us <code>cat myfile.dvc<\/code>? I suspect it is a <code>dvc add<\/code>'ed file, which means that it is not reproducible, but should be either placed manually or pulled from a remote. Try running <code>dvc pull<\/code> to download needed data first.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":38.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-04T15:28:13.109Z",
                "Answer_body":"<blockquote>\n<p>cmd: \u201cconda activate myenv\\n python src\/myscript.py -c config_all.ini;\\n<br>\n\\ python src\/myscript2.py -c config_all.ini\u201d<br>\ndeps:<br>\n- md5: 78da773cf0aa66adb3dc21647992ecfc<br>\npath: src\/myscript.py<br>\n- md5: 8394510c4a7e1c7fa7b36645c92545a4<br>\npath: src\/myscript2.py<br>\n- md5: 1e3216f583241b07eb5f67a454718966<br>\npath: src\/myscript3.py<br>\n- md5: 0cf90aca2f6dfc86f586696839309291<br>\npath: config_all.ini<br>\nmd5: 3422060a09515220f65dcc0befe3367a<br>\nouts:<br>\n- cache: true<br>\nmd5: 246087ad63ed1d4d0b34959e48335213<br>\nmetric: false<br>\npath: \u2026\/abc\/output1.pkl<br>\n- cache: true<br>\nmd5: e78ff278a3cc1544d9593d0da83e2344<br>\nmetric: false<br>\npath: \u2026\/abc\/output2.csv<br>\n- cache: true<br>\nmd5: cbf185aeee1a963812b4338577ae4b04<br>\nmetric: false<br>\npath: \u2026\/abc\/output4.csv<br>\n- cache: true<br>\nmd5: dde8676c4e199de6fb47fd495edf91ed.dir<br>\nmetric: false<br>\npath: \u2026\/abc\/data_all<br>\n- cache: false<br>\nmd5: 273a2fced4472c0959c775d37380915f<br>\nmetric: true<br>\npath: \u2026\/abc\/some.json<br>\nwdir: .<\/p>\n<\/blockquote>\n<p><code>dvc pull<\/code> just throw an error; we haven\u2019t set up remote data yet<\/p>",
                "Answer_score":63.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-04T15:31:45.427Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/spiette\">@spiette<\/a>  Thanks! I think I need a bit more info. So what is <code>xxx<\/code> from original error? Is it one of myfile.dvc outputs? Or is it defined in some other file like <code>xxx.dvc<\/code>?<\/p>",
                "Answer_score":33.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-04T15:42:16.638Z",
                "Answer_body":"<p>The error comes from the first outs : \u2026\/abc\/output1.pkl<\/p>\n<blockquote>\n<pre><code> dvc repro .\\myfile.dvc\nWarning: Output '..\\abc\\output1.pkl' of 'myfile.dvc' changed because it is 'not in cache'\nWarning: Dvc file 'myfile.dvc' changed.\nStage 'myfile.dvc' changed.\nReproducing 'myfile.dvc'\nRunning command:\n        conda activate myenv\n python src\/myscript.py -c config_all.ini;\n python src\/myscript2.py -c config_all.ini\nError: failed to reproduce 'myfile.dvc': output '..\\abc\\output1.pkl' does not exist\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!<\/code><\/pre>\n<\/blockquote>",
                "Answer_score":107.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-04T15:47:27.902Z",
                "Answer_body":"<p>Thanks! Dvc ran your command, but the command didn\u2019t produce <code>..\\abc\\output1.pkl<\/code> for some reason, which is why dvc is complaining. Could you look around your project, maybe your command created <code>output1.pkl<\/code> at some other path?<\/p>",
                "Answer_score":62.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-04T16:16:04.063Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/spiette\">@spiette<\/a> Also, I see you are running windows. Was your colleague using linux or mac? If so, you might want to check your script to handle os.path correctly. E.g. it might output to <code>..\/abc\/output1.pkl<\/code> which is not a valid subdirs path on windows, which might\u2019ve resulted in a file <code>..\/abc\/output1.pkl<\/code>(no subdirs, slashes are handled as a part of the filename) created on windows instead of <code>..\\abc\\output1.pkl<\/code><\/p>",
                "Answer_score":52.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-11T09:44:25.073Z",
                "Answer_body":"<p>Hi,<br>\nI think I found my problem; The \u201ccmd\u201d part of the dvc file contain multiple commands. While it works on *unix, it seems to just start one command on powershell, ignoring the other \u2026<br>\nPutting all these command in one script file solve the problem<\/p>",
                "Answer_score":177.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run get fail reproduc error clone project colleagu project file dep out run repro myfil got error fail reproduc myfil output exist obvious output exist time run script creat got error run forc flag run pipelin time miss",
        "Question_preprocessed_content":"run get fail reproduc error clone project colleagu project file dep out run repro got error fail reproduc output exist obvious output exist time run script creat got error run forc flag run pipelin time miss",
        "Question_gpt_summary_original":"The user encountered a \"failed to reproduce\" error when running a DVC project for the first time. The error message indicated that an output file did not exist, which was expected since it was the first run. The user tried running with the \"--force\" flag but still encountered the same error. The user is seeking guidance on how to run the pipeline for the first time.",
        "Question_gpt_summary":"user encount fail reproduc error run project time error messag indic output file exist expect run user tri run forc flag encount error user seek guidanc run pipelin time",
        "Answer_original_content":"spiett cat myfil suspect add file mean reproduc place manual pull remot try run pull download need data thank ruslan cmd conda activ myenv python src myscript config ini python src myscript config ini dep dacfaaadbdcecfc path src myscript caecfabca path src myscript efbebfa path src myscript cfacafdfcf path config ini afdccbefea out cach true adedddb metric fals path abc output pkl cach true effaccddda metric fals path abc output csv cach true cbfaabaeb metric fals path abc output csv cach true ddecedefbfdedf dir metric fals path abc data cach fals afcedccdf metric true path abc json wdir pull throw error havent set remot data spiett thank think need bit info origin error myfil output defin file like error come out abc output pkl repro myfil warn output abc output pkl myfil chang cach warn file myfil chang stage myfil chang reproduc myfil run command conda activ myenv python src myscript config ini python src myscript config ini error fail reproduc myfil output abc output pkl exist have troubl hit http org support happi help thank ran command command didnt produc abc output pkl reason complain look project mayb command creat output pkl path spiett run window colleagu linux mac want check script handl path correctli output abc output pkl valid subdir path window mightv result file abc output pkl subdir slash handl filenam creat window instead abc output pkl think problem cmd file contain multipl command work unix start command powershel ignor put command script file solv problem",
        "Answer_preprocessed_content":"suspect file mean reproduc place manual pull remot try run download need data thank ruslan cmd conda activ python python dep adb ecfc path path path aca dfc path dcc befe out cach true metric fals path cach true metric fals path cach true cbf metric fals path cach true metric fals path cach fals fced metric true path wdir throw error havent set remot data thank think need bit info origin error myfil output defin file like error come out thank ran command command didnt produc reason complain look project mayb command creat path run window colleagu linux mac want check script handl correctli output valid subdir path window mightv result file creat window instead think problem cmd file contain multipl command work unix start command powershel ignor put command script file solv problem",
        "Answer_gpt_summary_original":"possible solutions to the challenge of encountering a \"failed to reproduce\" error when running a pipeline for the first time include checking if the file is an added file and manually placing it or pulling it from a remote, running pull to download needed data first, looking around the project to see if the command created output1.pkl at some other path, and checking the script to handle os.path correctly. additionally, putting all the commands in one script file may solve the problem.",
        "Answer_gpt_summary":"possibl solut challeng encount fail reproduc error run pipelin time includ check file ad file manual place pull remot run pull download need data look project command creat output pkl path check script handl path correctli addition put command script file solv problem"
    },
    {
        "Question_id":null,
        "Question_title":"Check size prior to download",
        "Question_body":"<p>Hi,<\/p>\n<p>DVC newbie here. Thanks for developing a much-needed tool.<\/p>\n<p>I was wondering if there is a way to obtain the size information of the dvc files\/folders before issuing a dvc pull. I have searched through the manual but failed to find any information.<\/p>\n<p>Say I have <code>datasets.dvc<\/code> file uploaded by a teammate and I would like to learn it\u2019s size prior to downloading it.<\/p>\n<p>DVC version: 1.7.9<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1605811292214,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":6.0,
        "Question_view_count":496.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/check-size-prior-to-download\/559",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-19T18:45:45.131Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ozyo\">@ozyo<\/a> !<\/p>\n<p>There is no such feature right now, but we\u2019ve recently added size fields to our dvc files <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3256\">https:\/\/github.com\/iterative\/dvc\/issues\/3256<\/a> and we will be able to use them to approximate how much data we need to download\/upload without taking deduplication into account. We also plan to add a more precise mechanism to account for deduplication in such donwload\/upload predictions. Both will be added to commands like <code>dvc status -c<\/code>\/<code>dvc diff<\/code>\/<code>dvc ls<\/code>\/etc (e.g. <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1312\">https:\/\/github.com\/iterative\/dvc\/issues\/1312<\/a>).  So stay tuned! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_score":61.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"check size prior download newbi thank develop need tool wonder wai obtain size inform file folder issu pull search manual fail inform dataset file upload teammat like learn size prior download version",
        "Question_preprocessed_content":"check size prior download newbi thank develop tool wonder wai obtain size inform issu pull search manual fail inform file upload teammat like learn size prior download version",
        "Question_gpt_summary_original":"The user is a DVC newbie and is looking for a way to obtain the size information of DVC files\/folders before issuing a DVC pull. They have searched through the manual but failed to find any information. The user wants to know the size of a specific file, \"datasets.dvc\", uploaded by a teammate before downloading it.",
        "Question_gpt_summary":"user newbi look wai obtain size inform file folder issu pull search manual fail inform user want know size specif file dataset upload teammat download",
        "Answer_original_content":"ozyo featur right weve recent ad size field file http github com iter issu abl us approxim data need download upload take dedupl account plan add precis mechan account dedupl donwload upload predict ad command like statu diff http github com iter issu stai tune",
        "Answer_preprocessed_content":"featur right weve recent ad size field file abl us approxim data need take dedupl account plan add precis mechan account dedupl predict ad command like stai tune",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer are:\n- the tool does not have a feature to check the size of files\/folders before downloading them using version 1.7.9.\n- the tool has recently added size fields to their files, which can be used to approximate how much data needs to be downloaded\/uploaded without taking deduplication into account.\n- the tool plans to add a more precise mechanism to account for deduplication in download\/upload predictions.\n- both of these features will be added to commands like status -c\/ diff\/ ls\/etc.",
        "Answer_gpt_summary":"possibl solut mention answer tool featur check size file folder download version tool recent ad size field file approxim data need download upload take dedupl account tool plan add precis mechan account dedupl download upload predict featur ad command like statu diff"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Synapse ML predict [Errno 20] Not a directory",
        "Question_body":"I follow the official tutotial from microsoft: https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\n\nBut when I execute:\n\n #Bind model within Spark session\n model = pcontext.bind_model(\n     return_types=RETURN_TYPES, \n     runtime=RUNTIME, \n     model_alias=\"Sales\", #This alias will be used in PREDICT call to refer  this   model\n     model_uri=AML_MODEL_URI, #In case of AML, it will be AML_MODEL_URI\n     aml_workspace=ws #This is only for AML. In case of ADLS, this parameter can be removed\n ).register()\n\n\n\nI\u00b4ve got:\n\n\n\n\nNotADirectoryError: [Errno 20] Not a directory: '\/mnt\/var\/hadoop\/tmp\/nm-local-dir\/usercache\/trusted-service-user\/appcache\/application_1648328086462_0002\/spark-3d802a7e-15b7-4eb6-88c5-f0e01f8cdb35\/userFiles-fbe23a43-67d3-4e65-a879-4a497e804b40\/68603955220f5f8646700d809b71be9949011a2476a34965a3d5c0f3d14de79b.pkl\/MLmodel'\nTraceback (most recent call last):\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_context.py\", line 47, in bind_model\nudf = _create_udf(\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_udf.py\", line 104, in _create_udf\nmodel_runtime = runtime_gen._create_runtime()\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_runtime.py\", line 103, in _create_runtime\nif self._check_model_runtime_compatibility(model_runtime):\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_runtime.py\", line 166, in _check_model_runtime_compatibility\nmodel_wrapper = self._load()\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_runtime.py\", line 78, in _load\nreturn SynapsePredictModelCache._get_or_load(\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/core\/_cache.py\", line 172, in _get_or_load\nmodel = load_model(runtime, model_uri, functions)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 257, in load_model\nmodel = loader.load(model_uri, functions)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 122, in load\nmodel = self._load(model_uri)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 215, in _load\nreturn self._load_mlflow(model_uri)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/azure\/synapse\/ml\/predict\/utils\/_model_loader.py\", line 59, in _load_mlflow\nmodel = mlflow.pyfunc.load_model(model_uri)\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/mlflow\/pyfunc\/`init`.py\", line 640, in load_model\nmodel_meta = Model.load(os.path.join(local_path, MLMODEL_FILE_NAME))\n\nFile \"\/home\/trusted-service-user\/cluster-env\/env\/lib\/python3.8\/site-packages\/mlflow\/models\/model.py\", line 124, in load\nwith open(path) as f:\n\nNotADirectoryError: [Errno 20] Not a directory: '\/mnt\/var\/hadoop\/tmp\/nm-local-dir\/usercache\/trusted-service-user\/appcache\/application_1648328086462_0002\/spark-3d802a7e-15b7-4eb6-88c5-f0e01f8cdb35\/userFiles-fbe23a43-67d3-4e65-a879-4a497e804b40\/68603955220f5f8646700d809b71be9949011a2476a34965a3d5c0f3d14de79b.pkl\/MLmodel'\n\nHow can I fix that error ?",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1648335577807,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @ThiloBarth-2620,\n\nThanks for the question and using MS Q&A platform.\n\n(UPDATE:29\/3\/2022): You will experiencing this error message if you model does not contains all the required files in the ML model.\n\nAs per the repro, I had created two ML models named:\n\nsklearn_regression_model: Which contains only sklearn_regression_model.pkl file.\n\nWhen I predict for MLFLOW packaged model named sklearn_regression_model, getting same error as shown above:\n\nlinear_regression: Which contains the below files:\n\nWhen I predict for MLFLOW packaged model named linear_regression, it works as excepted.\n\n--------------------------------------------------\n\nIt should be AML_MODEL_URI = \"<aml model uri>\" #In URI \":x\" => Rossman_Sales:2\n\nBefore running this script, update it with the URI for ADLS Gen2 data file along with model output return data type and ADLS\/AML URI for the model file.\n\n #Set model URI\n        #Set AML URI, if trained model is registered in AML\n           AML_MODEL_URI = \"<aml model uri>\" #In URI \":x\" signifies model version in AML. You can   choose which model version you want to run. If \":x\" is not provided then by default   latest version will be picked.\n    \n        #Set ADLS URI, if trained model is uploaded in ADLS\n           ADLS_MODEL_URI = \"abfss:\/\/<filesystemname>@<account name>.dfs.core.windows.net\/<model   mlflow folder path>\"\n\nModel URI from AML Workspace:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"aml:\/\/mlflow_sklearn:1\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\nModel URI uploaded to ADLS Gen2:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/linear_regression\/linear_regression\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\n\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/788637\/azure-synapse-ml-predict-errno-20-not-a-directory.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-28T11:26:12.193Z",
                "Answer_score":1,
                "Answer_body":"Hello @ThiloBarth-2620,\n\nThanks for the question and using MS Q&A platform.\n\n(UPDATE:29\/3\/2022): You will experiencing this error message if you model does not contains all the required files in the ML model.\n\nAs per the repro, I had created two ML models named:\n\nsklearn_regression_model: Which contains only sklearn_regression_model.pkl file.\n\nWhen I predict for MLFLOW packaged model named sklearn_regression_model, getting same error as shown above:\n\nlinear_regression: Which contains the below files:\n\nWhen I predict for MLFLOW packaged model named linear_regression, it works as excepted.\n\n--------------------------------------------------\n\nIt should be AML_MODEL_URI = \"<aml model uri>\" #In URI \":x\" => Rossman_Sales:2\n\nBefore running this script, update it with the URI for ADLS Gen2 data file along with model output return data type and ADLS\/AML URI for the model file.\n\n #Set model URI\n        #Set AML URI, if trained model is registered in AML\n           AML_MODEL_URI = \"<aml model uri>\" #In URI \":x\" signifies model version in AML. You can   choose which model version you want to run. If \":x\" is not provided then by default   latest version will be picked.\n    \n        #Set ADLS URI, if trained model is uploaded in ADLS\n           ADLS_MODEL_URI = \"abfss:\/\/<filesystemname>@<account name>.dfs.core.windows.net\/<model   mlflow folder path>\"\n\nModel URI from AML Workspace:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"aml:\/\/mlflow_sklearn:1\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\nModel URI uploaded to ADLS Gen2:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/linear_regression\/linear_regression\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\n\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":7,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1648466772192,
        "Question_original_content":"azur synaps predict errno directori follow offici tutoti microsoft http doc microsoft com azur synaps analyt machin learn tutori score model predict spark pool execut bind model spark session model pcontext bind model return type return type runtim runtim model alia sale alia predict refer model model uri aml model uri case aml aml model uri aml workspac aml case adl paramet remov regist iv got notadirectoryerror errno directori mnt var hadoop tmp local dir usercach trust servic user appcach applic spark dae fefcdb userfil fbea aeb ffdbbeaaadcfddeb pkl mlmodel traceback recent file home trust servic user cluster env env lib python site packag azur synaps predict core context line bind model udf creat udf file home trust servic user cluster env env lib python site packag azur synaps predict core udf line creat udf model runtim runtim gen creat runtim file home trust servic user cluster env env lib python site packag azur synaps predict core runtim line creat runtim self check model runtim compat model runtim file home trust servic user cluster env env lib python site packag azur synaps predict core runtim line check model runtim compat model wrapper self load file home trust servic user cluster env env lib python site packag azur synaps predict core runtim line load return synapsepredictmodelcach load file home trust servic user cluster env env lib python site packag azur synaps predict core cach line load model load model runtim model uri function file home trust servic user cluster env env lib python site packag azur synaps predict util model loader line load model model loader load model uri function file home trust servic user cluster env env lib python site packag azur synaps predict util model loader line load model self load model uri file home trust servic user cluster env env lib python site packag azur synaps predict util model loader line load return self load model uri file home trust servic user cluster env env lib python site packag azur synaps predict util model loader line load model pyfunc load model model uri file home trust servic user cluster env env lib python site packag pyfunc init line load model model meta model load path join local path mlmodel file file home trust servic user cluster env env lib python site packag model model line load open path notadirectoryerror errno directori mnt var hadoop tmp local dir usercach trust servic user appcach applic spark dae fefcdb userfil fbea aeb ffdbbeaaadcfddeb pkl mlmodel fix error",
        "Question_preprocessed_content":"azur synaps predict directori follow offici tutoti microsoft execut bind model spark session model runtim runtim alia predict refer model case aml aml case adl paramet remov iv got notadirectoryerror directori traceback file line udf file line file line file line file line return file line model function file line model function file line load model file line return file line model file line file line load open notadirectoryerror directori fix error",
        "Question_gpt_summary_original":"The user encountered an error while executing a tutorial from Microsoft on Azure Synapse ML predict. The error message indicates that there is an issue with the directory and the user is unable to load the model. The user is seeking help to fix the error.",
        "Question_gpt_summary":"user encount error execut tutori microsoft azur synaps predict error messag indic issu directori user unabl load model user seek help fix error",
        "Answer_original_content":"hello thilobarth thank question platform updat experienc error messag model contain requir file model repro creat model name sklearn regress model contain sklearn regress model pkl file predict packag model name sklearn regress model get error shown linear regress contain file predict packag model name linear regress work except aml model uri uri rossman sale run script updat uri adl gen data file model output return data type adl aml uri model file set model uri set aml uri train model regist aml aml model uri uri signifi model version aml choos model version want run provid default latest version pick set adl uri train model upload adl adl model uri abfss df core window net model uri aml workspac data file abfss data cheprasynaps df core window net aml lengthofstai cook small csv aml model uri sklearn aml sklearn signifi model version aml choos version want run provid default latest version pick return type int runtim model uri upload adl gen data file abfss data cheprasynaps df core window net aml lengthofstai cook small csv aml model uri sklearn abfss data cheprasynaps df core window net linear regress linear regress signifi model version aml choos version want run provid default latest version pick return type int runtim hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"hello thank question platform experienc error messag model contain requir file model repro creat model name contain file predict packag model name get error shown contain file predict packag model name work except uri run script updat uri adl gen data file model output return data type uri model file set model uri set aml uri train model regist aml uri signifi model version aml choos model version want run provid default latest version pick set adl uri train model upload adl model uri aml workspac signifi model version aml choos version want run provid default latest version pick int runtim model uri upload adl gen signifi model version aml choos version want run provid default latest version pick int runtim hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_gpt_summary_original":"possible solutions to the \"notadirectoryerror: [errno 20] not a directory\" error when binding a model within a spark session using azure synapse ml are: ensuring that the model contains all the required files, updating the aml_model_uri and adls_model_uri with the correct uris for the model file and data file, respectively, and specifying the return type and runtime. additionally, the user can choose which model version to run by providing the model version number in the uri.",
        "Answer_gpt_summary":"possibl solut notadirectoryerror errno directori error bind model spark session azur synaps ensur model contain requir file updat aml model uri adl model uri correct uri model file data file respect specifi return type runtim addition user choos model version run provid model version number uri"
    },
    {
        "Question_id":null,
        "Question_title":"Visual Bug in Documentation",
        "Question_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/fd0f258d7049524defde72d94701022480abd440.png\" data-download-href=\"\/uploads\/short-url\/A6FfBmGoIRNds6FR4YCdptOz7Fe.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_690x264.png\" alt=\"image\" data-base62-sha1=\"A6FfBmGoIRNds6FR4YCdptOz7Fe\" width=\"690\" height=\"264\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_690x264.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_1035x396.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_1380x528.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/fd0f258d7049524defde72d94701022480abd440_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2560\u00d7981 120 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>I think there is a missing ``` to finish the code block<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1654672339920,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":156.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/visual-bug-in-documentation\/2572",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-08T09:08:22.120Z",
                "Answer_body":"<p>Hey Aryan, thank you for flagging this. I\u2019ll update the docs to fix this.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-07T07:12:39.513Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"visual bug document imag think miss finish code block",
        "Question_preprocessed_content":"visual bug document imag think miss finish code block",
        "Question_gpt_summary_original":"The user has encountered a challenge with a visual bug in the documentation, specifically a missing ``` to finish a code block.",
        "Question_gpt_summary":"user encount challeng visual bug document specif miss finish code block",
        "Answer_original_content":"hei aryan thank flag ill updat doc fix topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei aryan thank flag ill updat doc fix topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"the solution to the visual bug in the documentation is that the person who answered the question will update the docs to fix it. however, it is also mentioned that new replies are no longer allowed as the topic was automatically closed 60 days after the last reply.",
        "Answer_gpt_summary":"solut visual bug document person answer question updat doc fix mention new repli longer allow topic automat close dai repli"
    },
    {
        "Question_id":null,
        "Question_title":"What to do if the file is not found in the repo?",
        "Question_body":"<p>FileNotFoundError: [Errno 2] No such file or directory: \u2018\/home\/runner\/work\/lrg-omics\/lrg-omics\/.dvc\/cache\/0d\/ce0a9a9337eac485acfc7859b1e28e.dir\u2019<\/p>\n<p>This is what I get in a GitHub action.<\/p>\n<p>I tried <code>dvc checkout --relink<\/code> locally, but that did not fix it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658515683027,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":93.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-to-do-if-the-file-is-not-found-in-the-repo\/1263",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-23T15:17:34.624Z",
                "Answer_body":"<p>Hey, what is the command you\u2019re trying to run when you get the following error? Did you run <code>dvc pull<\/code> beforehand?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"file repo filenotfounderror errno file directori home runner work lrg omic lrg omic cach ceaaeacacfcbe dir github action tri checkout relink local fix",
        "Question_preprocessed_content":"file repo filenotfounderror file directori github action tri local fix",
        "Question_gpt_summary_original":"The user encountered a \"FileNotFoundError\" while using GitHub action, which indicates that the file or directory is not found in the repository. The user attempted to fix the issue by using \"dvc checkout --relink\" locally, but it did not resolve the problem.",
        "Question_gpt_summary":"user encount filenotfounderror github action indic file directori repositori user attempt fix issu checkout relink local resolv problem",
        "Answer_original_content":"hei command your try run follow error run pull",
        "Answer_preprocessed_content":"hei command your try run follow error run",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer. the responder is asking for more information about the command the user is trying to run and if they ran pull beforehand.",
        "Answer_gpt_summary":"specif solut provid answer respond ask inform command user try run ran pull"
    },
    {
        "Question_id":65769868.0,
        "Question_title":"Where does the Azure Machine ACI Webservice deploy?",
        "Question_body":"<p>When we deploy a model as an ACIWebService in Azure Machine Learning Service, we do not need to specify any <code>deployment_target<\/code>.<\/p>\n<p>According to the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#deploy-workspace--name--models--inference-config-none--deployment-config-none--deployment-target-none--overwrite-false-\" rel=\"nofollow noreferrer\">AzureML documentation<\/a> for <code>azureml.core.model.model<\/code> class,<\/p>\n<pre><code>deployment_target\nComputeTarget\ndefault value: None\nA ComputeTarget to deploy the Webservice to. As Azure Container Instances has no associated ComputeTarget, leave this parameter as None to deploy to Azure Container Instances.\n<\/code><\/pre>\n<p>What does Microsoft mean by<\/p>\n<blockquote>\n<p>As Azure Container Instances has no associated ComputeTarget<\/p>\n<\/blockquote>\n<p>In which &quot;Compute Target&quot; is an ACIWebService deployed?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610952043250,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":317.0,
        "Answer_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-instances\/container-instances-overview\" rel=\"nofollow noreferrer\">Azure Container Instances<\/a> itself is the compute platform. It spins up a container in a serverless-fashion.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65769868",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1610958655960,
        "Question_original_content":"azur machin aci webservic deploi deploi model aciwebservic servic need specifi deploy target accord document core model model class deploy target computetarget default valu computetarget deploi webservic azur contain instanc associ computetarget leav paramet deploi azur contain instanc microsoft mean azur contain instanc associ computetarget comput target aciwebservic deploi",
        "Question_preprocessed_content":"azur machin aci webservic deploi deploi model aciwebservic servic need specifi accord document class microsoft mean azur contain instanc associ computetarget comput target aciwebservic deploi",
        "Question_gpt_summary_original":"The user is facing a challenge in understanding where an Azure Machine ACI Webservice is deployed as they do not need to specify a deployment target. The AzureML documentation states that the deployment target for an ACI Webservice is None, as Azure Container Instances has no associated ComputeTarget. The user is seeking clarification on where the Compute Target for an ACI Webservice is deployed.",
        "Question_gpt_summary":"user face challeng understand azur machin aci webservic deploi need specifi deploy target document state deploy target aci webservic azur contain instanc associ computetarget user seek clarif comput target aci webservic deploi",
        "Answer_original_content":"azur contain instanc comput platform spin contain serverless fashion",
        "Answer_preprocessed_content":"azur contain instanc comput platform spin contain",
        "Answer_gpt_summary_original":"the solution to deploying an azure machine aci webservice without specifying a deployment target is to use azure container instances as the compute platform. it operates in a serverless manner by creating a container.",
        "Answer_gpt_summary":"solut deploi azur machin aci webservic specifi deploy target us azur contain instanc comput platform oper serverless manner creat contain"
    },
    {
        "Question_id":66576663.0,
        "Question_title":"Can we append data to an existing csv file stored in Azure blob storage through python?",
        "Question_body":"<p>I have a machine learning model deployed in azure designer studio. I need to retrain it everyday with new data through python code. I need to keep the existing csv data in the blob storage and also add some more data to the existing csv and retrain it. If I retrain the model with only the new data, the old data is lost so I need to retrain the model by appending new data to existing data. Is there any way to do it through python coding?<\/p>\n<p>I have also researched about append blob but they add only in the end of the blob. In the documentation, they have mentioned we cannot update or add to an existing blob.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1615438269293,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1615455239928,
        "Question_score":2.0,
        "Question_view_count":1079.0,
        "Answer_body":"<p>I'm not sure why it has to be one csv file. There are many Python-based libraries for working with a dataset spread across multiple csvs.<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.dask.org\/en\/latest\/dataframe.html\" rel=\"nofollow noreferrer\">Dask DataFrame API<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/37639956\/how-to-import-multiple-csv-files-in-a-single-load\"><code>pyspark<\/code><\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930#create-a-tabulardataset\" rel=\"nofollow noreferrer\">Azure Machine Learning Tabular Dataset<\/a><\/li>\n<\/ul>\n<p>In all of the examples, you pass a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Glob_%28programming%29\" rel=\"nofollow noreferrer\"><code>glob<\/code> pattern<\/a>, that will match multiple files. This pattern works very naturally with Azure ML Dataset which you can use as your input. See this excerpt from the docs link above.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace, Datastore, Dataset\n\ndatastore_name = 'your datastore name'\n\n# get existing workspace\nworkspace = Workspace.from_config()\n    \n# retrieve an existing datastore in the workspace by name\ndatastore = Datastore.get(workspace, datastore_name)\n\n# create a TabularDataset from 3 file paths in datastore\ndatastore_paths = [(datastore, 'weather\/2018\/11.csv'),\n                   (datastore, 'weather\/2018\/12.csv'),\n                   (datastore, 'weather\/2019\/*.csv')] # here's the glob pattern\n\nweather_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n<\/code><\/pre>\n<p>Assuming that all the csvs can fit into memory, you can turn these datasets easily into <code>pandas<\/code> dataframes. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#access-dataset-in-training-script\" rel=\"nofollow noreferrer\">with Azure ML Datasets,<\/a> you call<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># get the input dataset by name\ndataset = Dataset.get_by_name(ws, name=dataset_name)\n# load the TabularDataset to pandas DataFrame\ndf = dataset.to_pandas_dataframe()\n<\/code><\/pre>\n<p>With Dask Dataframe, <a href=\"https:\/\/github.com\/dask\/dask\/issues\/1651#issuecomment-253187846\" rel=\"nofollow noreferrer\">this GitHub issue<\/a> says you can call<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>df = my_dask_df.compute()\n<\/code><\/pre>\n<p>As far as output datasets, you can control this by reading in the output CSV as a dataframe, appending data to it then overwriting it to the same location.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1615448010367,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66576663",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1615440806576,
        "Question_original_content":"append data exist csv file store azur blob storag python machin learn model deploi azur design studio need retrain everydai new data python code need exist csv data blob storag add data exist csv retrain retrain model new data old data lost need retrain model append new data exist data wai python code research append blob add end blob document mention updat add exist blob",
        "Question_preprocessed_content":"append data exist csv file store azur blob storag python machin learn model deploi azur design studio need retrain everydai new data python code need exist csv data blob storag add data exist csv retrain retrain model new data old data lost need retrain model append new data exist data wai python code research append blob add end blob document mention updat add exist blob",
        "Question_gpt_summary_original":"The user is facing a challenge of appending new data to an existing CSV file stored in Azure blob storage through Python code. The user needs to retrain a machine learning model with new data every day while keeping the existing data in the blob storage. The user has researched about append blob but found that it only adds data at the end of the blob and cannot update or add to an existing blob.",
        "Question_gpt_summary":"user face challeng append new data exist csv file store azur blob storag python code user need retrain machin learn model new data dai keep exist data blob storag user research append blob add data end blob updat add exist blob",
        "Answer_original_content":"sure csv file python base librari work dataset spread multipl csv dask datafram api pyspark tabular dataset exampl pass glob pattern match multipl file pattern work natur dataset us input excerpt doc link core import workspac datastor dataset datastor datastor exist workspac workspac workspac config retriev exist datastor workspac datastor datastor workspac datastor creat tabulardataset file path datastor datastor path datastor weather csv datastor weather csv datastor weather csv glob pattern weather dataset tabular delimit file path datastor path assum csv fit memori turn dataset easili panda datafram dataset input dataset dataset dataset dataset load tabulardataset panda datafram dataset panda datafram dask datafram github issu sai dask comput far output dataset control read output csv datafram append data overwrit locat",
        "Answer_preprocessed_content":"sure csv file librari work dataset spread multipl csv dask datafram api tabular dataset exampl pass pattern match multipl file pattern work natur dataset us input excerpt doc link assum csv fit memori turn dataset easili datafram dataset dask datafram github issu sai far output dataset control read output csv datafram append data overwrit locat",
        "Answer_gpt_summary_original":"the answer suggests that instead of appending data to an existing csv file, the user can work with multiple csv files using python-based libraries such as dask dataframe api, pyspark, and tabular dataset. the user can pass a glob pattern that matches multiple files and create a tabulardataset from multiple file paths. assuming all csvs can fit into memory, the user can turn these datasets into pandas dataframes. for output datasets, the user can read in the output csv as a dataframe, append data to it, and overwrite it to the same location.",
        "Answer_gpt_summary":"answer suggest instead append data exist csv file user work multipl csv file python base librari dask datafram api pyspark tabular dataset user pass glob pattern match multipl file creat tabulardataset multipl file path assum csv fit memori user turn dataset panda datafram output dataset user read output csv datafram append data overwrit locat"
    },
    {
        "Question_id":null,
        "Question_title":"how can i retrain the model after a period of time",
        "Question_body":"Hello everyone, i'm using lambda architecture to build a fraud detection project , i build my model using machine learning in databricks , after saving the model , i load the model in the speed layer to predict the incoming data, i want to know how can i retrain this model using new incoming data from eventhub ??\ndoes the retrain should be in the batch layer ?\nthanks for helping",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1619232734657,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/370080\/how-can-i-retrain-the-model-after-a-period-of-time.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-27T06:45:34.227Z",
                "Answer_score":0,
                "Answer_body":"@NadineBenharrath-4116 Thanks, Please follow the document to retrain the model using the new data.\n\nCurrently Data Drift Monitor (Data Drift->EventGrid->LogicApp->Trigger Pipeline) allows to trigger a pipeline when dataset drift has been detected.\n\nPublic Repo of the architecture and code: https:\/\/github.com\/Microsoft\/MLOps",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-27T07:10:26.91Z",
                "Answer_score":0,
                "Answer_body":"@ramr-msft the problem that i trained my model using databricks and i tracked it with mlflow , i'm not using azure ML because my dataset is imbalanced ?",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"retrain model period time hello lambda architectur build fraud detect project build model machin learn databrick save model load model speed layer predict incom data want know retrain model new incom data eventhub retrain batch layer thank help",
        "Question_preprocessed_content":"retrain model period time hello lambda architectur build fraud detect project build model machin learn databrick save model load model speed layer predict incom data want know retrain model new incom data eventhub retrain batch layer thank help",
        "Question_gpt_summary_original":"The user is facing a challenge of retraining a machine learning model used for fraud detection in a lambda architecture. They want to know how to retrain the model using new incoming data from eventhub and whether the retraining should be done in the batch layer.",
        "Question_gpt_summary":"user face challeng retrain machin learn model fraud detect lambda architectur want know retrain model new incom data eventhub retrain batch layer",
        "Answer_original_content":"nadinebenharrath thank follow document retrain model new data current data drift monitor data drift eventgrid logicapp trigger pipelin allow trigger pipelin dataset drift detect public repo architectur code http github com microsoft mlop ramr msft problem train model databrick track dataset imbalanc",
        "Answer_preprocessed_content":"thank follow document retrain model new data current data drift monitor allow trigger pipelin dataset drift detect public repo architectur code problem train model databrick track dataset imbalanc",
        "Answer_gpt_summary_original":"possible solutions from the answer include following a document to retrain the machine learning model using new data, using a data drift monitor to trigger a pipeline when dataset drift is detected, and accessing a public repository of the architecture and code. the answer also mentions a problem with using  due to an imbalanced dataset.",
        "Answer_gpt_summary":"possibl solut answer includ follow document retrain machin learn model new data data drift monitor trigger pipelin dataset drift detect access public repositori architectur code answer mention problem imbalanc dataset"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Designer- Language Detection",
        "Question_body":"Hello,\n\nWe have a clustering model in AzureML designer and we are using the \"Preprocess Text\" component to clean the messages.\nBut, the messages can be in different languages and in the dropdown of the Language in the \"Preprocess Text\" component there is only English and no other language. Is there a way to detect the language of a text in azureml designer?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647103939673,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/769728\/azure-ml-designer-language-detection.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-14T06:26:50.423Z",
                "Answer_score":0,
                "Answer_body":"@ValerieHAYECK-2072 Pre-process text component only supports English language. Currently, there is no ready to use module in azure ml designer to detect text from a dataset.\nAzure cognitive services does offer some services that can auto detect language and translate text to a different language. Are you looking to translate data of a particular dataset?\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"design languag detect hello cluster model design preprocess text compon clean messag messag differ languag dropdown languag preprocess text compon english languag wai detect languag text design",
        "Question_preprocessed_content":"design languag detect hello cluster model design preprocess text compon clean messag messag differ languag dropdown languag preprocess text compon english languag wai detect languag text design",
        "Question_gpt_summary_original":"The user is facing a challenge in AzureML designer while using the \"Preprocess Text\" component to clean messages as the dropdown for language only includes English and not other languages. The user is seeking a solution to detect the language of a text in AzureML designer.",
        "Question_gpt_summary":"user face challeng design preprocess text compon clean messag dropdown languag includ english languag user seek solut detect languag text design",
        "Answer_original_content":"valeriehayeck pre process text compon support english languag current readi us modul design detect text dataset azur cognit servic offer servic auto detect languag translat text differ languag look translat data particular dataset answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"text compon support english languag current readi us modul design detect text dataset azur cognit servic offer servic auto detect languag translat text differ languag look translat data particular dataset answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer include using azure cognitive services to auto-detect language and translate text to a different language. however, there is currently no ready-to-use module in designer to detect the language of a text. the answer also suggests upvoting helpful answers to benefit other community members.",
        "Answer_gpt_summary":"possibl solut mention answer includ azur cognit servic auto detect languag translat text differ languag current readi us modul design detect languag text answer suggest upvot help answer benefit commun member"
    },
    {
        "Question_id":null,
        "Question_title":"Get stuck at uploading at the end",
        "Question_body":"<p>My program finished before a few hours, but wandb get stuck at uploading files until now. I asked my friend and he said he had the same problem. So there are any problem for wandb server in the past  few hours and how to fix it?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf.jpeg\" data-download-href=\"\/uploads\/short-url\/kwlk7giIWlj5N5PsCmnxPYQd8d1.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf_2_690x83.jpeg\" alt=\"image\" data-base62-sha1=\"kwlk7giIWlj5N5PsCmnxPYQd8d1\" width=\"690\" height=\"83\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf_2_690x83.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf_2_1035x124.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf_2_1380x166.jpeg 2x\" data-dominant-color=\"181717\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2122\u00d7256 120 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1669253840661,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":576.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/get-stuck-at-uploading-at-the-end\/3453",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-29T01:15:31.935Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/yueyang\">@yueyang<\/a> , happy to look into this for you. We did not have any network related issues last week that would have prevented users from logging experiments. Are you still experiencing this issue? If yes, could you please:<\/p>\n<ul>\n<li>Expand on the type of experiment you were running and approximately how many files and their size you were attempting to upload.<\/li>\n<li>Share the <code>debug.log<\/code> and <code>debug-internal.log<\/code> files of the hanging runs. These files are located in the working directory under<code> wandb\/<\/code> folder<\/li>\n<li>Link to your private workspace for us to review<\/li>\n<\/ul>\n<p>Thank you<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-01T23:18:35.210Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/yueyang\">@yueyang<\/a>  since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-30T23:19:17.434Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stuck upload end program finish hour stuck upload file ask friend said problem problem server past hour fix imag",
        "Question_preprocessed_content":"stuck upload end program finish hour stuck upload file ask friend said problem problem server past hour fix imag",
        "Question_gpt_summary_original":"The user's program finished a few hours ago, but they are unable to upload files to wandb as it is stuck. The user's friend also faced the same issue, and they are wondering if there is a problem with the wandb server and how to fix it.",
        "Question_gpt_summary":"user program finish hour ago unabl upload file stuck user friend face issu wonder problem server fix",
        "Answer_original_content":"yueyang happi look network relat issu week prevent user log experi experienc issu ye expand type experi run approxim file size attempt upload share debug log debug intern log file hang run file locat work directori folder link privat workspac review thank yueyang heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"happi look network relat issu week prevent user log experi experienc issu ye expand type experi run approxim file size attempt upload share file hang run file locat work directori folder link privat workspac review thank heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no clear solutions provided in the answer. the responder is asking for more information about the issue, such as the type and size of files being uploaded, and requesting debug logs to review. the responder also notes that if they do not hear back from the user, the request will be closed.",
        "Answer_gpt_summary":"clear solut provid answer respond ask inform issu type size file upload request debug log review respond note hear user request close"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to change the scoring URI in ML Service deployment",
        "Question_body":"Hi,\n\nI'm trying to deploy a model as an endpoint in azure ML Studio. model.deploy call returns service which has a scoring_uri. I'm wondering if it is possible to change the scoring_uri from \/score to something else more appropriate. Or potentially register multiple of the uri's under the same FQDN like \/score, \/test, \/retrain under same principal guid service name.\n\ninference_config = InferenceConfig(entry_script=ENTRY_SCRIPT, environment=keras_env)\naci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n\nservice = Model.deploy(workspace=ws,\nname=SERVICE_NAME,\nmodels=[model],\ninference_config=inference_config,\ndeployment_config=aci_config,\noverwrite=True)\nservice.wait_for_deployment(show_output=True)\n\nuri = service.scoring_uri\n\nhttp:\/\/XXXXXXXX-XXXX-4000-XXXX-aa1a5e7XXXXX.westus2.azurecontainer.io\/score\n\nCan we change the last part from \/score to something else? like \/test or register multiple endpoints.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630356655677,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/533289\/is-it-possible-to-change-the-scoring-uri-in-ml-ser.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-31T04:25:11.45Z",
                "Answer_score":0,
                "Answer_body":"Hi, you cannot modify the web service uri.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"possibl chang score uri servic deploy try deploi model endpoint studio model deploi return servic score uri wonder possibl chang score uri score appropri potenti regist multipl uri fqdn like score test retrain princip guid servic infer config inferenceconfig entri script entri script environ kera env aci config aciwebservic deploi configur cpu core memori servic model deploi workspac servic model model infer config infer config deploy config aci config overwrit true servic wait deploy output true uri servic score uri http aaaexx westu azurecontain score chang score like test regist multipl endpoint",
        "Question_preprocessed_content":"possibl chang score uri servic deploy try deploi model endpoint studio return servic wonder possibl chang appropri potenti regist multipl uri fqdn like princip guid servic servic model model overwrit true uri chang like regist multipl endpoint",
        "Question_gpt_summary_original":"The user is trying to deploy a model as an endpoint in Azure ML Studio and is wondering if it is possible to change the scoring_uri from \/score to something else or register multiple URIs under the same FQDN like \/score, \/test, \/retrain under the same principal guid service name.",
        "Question_gpt_summary":"user try deploi model endpoint studio wonder possibl chang score uri score regist multipl uri fqdn like score test retrain princip guid servic",
        "Answer_original_content":"modifi web servic uri",
        "Answer_preprocessed_content":"modifi web servic uri",
        "Answer_gpt_summary_original":"there are no solutions to changing the scoring_uri from \/score or registering multiple uris under the same fqdn.",
        "Answer_gpt_summary":"solut chang score uri score regist multipl uri fqdn"
    },
    {
        "Question_id":72637756.0,
        "Question_title":"Azure Machine Learning Model deployment as AKS web service from multiple workspaces",
        "Question_body":"<p>I am trying to determine how the Az ML model deployment works with AKS. If you have a single AKS cluster but attach from two separate workspaces, will models from both workspaces get deployed into different azureml-fe's with different IP addresses OR a single azureml-fe with a single IP address? Reason I ask is because I want to purchase a certificate but am unsure if all the models (regardless of workspace) will get exposed under the same IP Address OR multiple IP Addresses? If its the former, I can do it with one certificate...otherwise I have to do it with multiple certificates or SAN based certificates. So if anyone has experience with this, please let me know!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1655326759547,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":127.0,
        "Answer_body":"<p>By checking AKS webservice class, we can do the multiple services links to single AKS cluster. The endpoint management was described in <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aks.akswebservice?view=azure-ml-py\" rel=\"nofollow noreferrer\">document<\/a>, but this is representing one-to-one cluster and service. For multiple workspaces refer <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice.aksendpoint?view=azure-ml-py#azureml-core-webservice-aksendpoint-create-version\" rel=\"nofollow noreferrer\">document<\/a>.<\/p>\n<p>Regarding <strong><code>azureml-fe<\/code><\/strong>. There will be one <strong>azureml-fe<\/strong> for one cluster. That means, when we are using different workspaces for deployment into one AKS, then only <strong>one<\/strong> <strong>azureml-fe<\/strong> and can be considered to take <strong>one certificate.<\/strong><\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-deploy-azure-kubernetes-service?tabs=python#azure-ml-router<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72637756",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1655345708048,
        "Question_original_content":"model deploy ak web servic multipl workspac try determin model deploy work ak singl ak cluster attach separ workspac model workspac deploi differ differ address singl singl address reason ask want purchas certif unsur model regardless workspac expos address multipl address certif multipl certif san base certif experi let know",
        "Question_preprocessed_content":"model deploy ak web servic multipl workspac try determin model deploy work ak singl ak cluster attach separ workspac model workspac deploi differ differ address singl singl address reason ask want purchas certif unsur model expos address multipl address multipl certif san base certif experi let know",
        "Question_gpt_summary_original":"The user is facing challenges in determining how Azure Machine Learning model deployment works with AKS when attaching from multiple workspaces. They are unsure if models from both workspaces will be deployed into different azureml-fe's with different IP addresses or a single azureml-fe with a single IP address. The user wants to purchase a certificate but is unsure if all the models will get exposed under the same IP address or multiple IP addresses. They are seeking advice from anyone with experience in this matter.",
        "Question_gpt_summary":"user face challeng determin model deploy work ak attach multipl workspac unsur model workspac deploi differ differ address singl singl address user want purchas certif unsur model expos address multipl address seek advic experi matter",
        "Answer_original_content":"check ak webservic class multipl servic link singl ak cluster endpoint manag describ document repres cluster servic multipl workspac refer document cluster mean differ workspac deploy ak consid certif http doc microsoft com azur machin learn deploi azur kubernet servic tab python azur router",
        "Answer_preprocessed_content":"check ak webservic class multipl servic link singl ak cluster endpoint manag describ document repres cluster servic multipl workspac refer document cluster mean differ workspac deploy ak consid certif",
        "Answer_gpt_summary_original":"the answer suggests that by checking the aks webservice class, multiple services can be linked to a single aks cluster. however, the endpoint management described in the document represents a one-to-one cluster and service. for multiple workspaces, the document should be referred to. regarding -fe, there will be one -fe for one cluster, meaning that when using different workspaces for deployment into one aks, only one -fe can be considered to take one certificate.",
        "Answer_gpt_summary":"answer suggest check ak webservic class multipl servic link singl ak cluster endpoint manag describ document repres cluster servic multipl workspac document refer cluster mean differ workspac deploy ak consid certif"
    },
    {
        "Question_id":62407943.0,
        "Question_title":"Restrict the number of nodes used by an Azure Machine Learning pipeine",
        "Question_body":"<p>I have written a pipeline that I want to run on a remote compute cluster within Azure Machine Learning. My aim is to process a large amount of historical data, and to do this I will need to run the pipeline on a large number of input parameter combinations.<\/p>\n\n<p>Is there a way to restrict the number of nodes that the pipeline uses on the cluster? By default it will use all the nodes available to the cluster, and I would like to restrict it so that it only uses a pre-defined maximum. This allows me to leave the rest of the cluster free for other users.<\/p>\n\n<p>My current code to start the pipeline looks like this:<\/p>\n\n<pre><code># Setup the pipeline\nsteps = [data_import_step] # Contains PythonScriptStep\npipeline = Pipeline(workspace=ws, steps=steps)\npipeline.validate()\n\n# Big long list of historical dates that I want to process data for\ndts = pd.date_range('2019-01-01', '2020-01-01', freq='6H', closed='left')\n# Submit the pipeline job\nfor dt in dts:\n    pipeline_run = Experiment(ws, 'my-pipeline-run').submit(\n        pipeline,\n        pipeline_parameters={\n            'import_datetime': dt.strftime('%Y-%m-%dT%H:00'),\n        }\n    )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1592308824897,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":274.0,
        "Answer_body":"<p>For me, the killer feature of Azure ML is not having to worry about load balancing like this. Our team has a compute target with <code>max_nodes=100<\/code> for every feature branch and we have <code>Hyperdrive<\/code> pipelines that result in 130 runs for each pipeline.<\/p>\n<p>We can submit multiple <code>PipelineRun<\/code>s back-to-back and the orchestrator does the heavy lifting of queuing, submitting, all the runs so that the <code>PipelineRun<\/code>s execute in the serial order I submitted them, and that the cluster is never overloaded. This works without issue for us 99% of the time.<\/p>\n<p>If what you're looking for is that you'd like the <code>PipelineRun<\/code>s to be executed in parallel, then you should check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-parallel-run-step#build-and-run-the-pipeline-containing-parallelrunstep\" rel=\"nofollow noreferrer\"><code>ParallelRunStep<\/code><\/a>.<\/p>\n<p>Another option is to isolate your computes. You can have up to 200 <code>ComputeTarget<\/code>s per workspace. Two 50-node <code>ComputeTarget<\/code>s cost the same as one 100-node <code>ComputeTarget<\/code>.<\/p>\n<p>On our team, we use <a href=\"https:\/\/www.pygit2.org\/\" rel=\"nofollow noreferrer\"><code>pygit2<\/code><\/a> to have a <code>ComputeTarget<\/code> created for each feature branch, so that, as data scientists, we can be confident that we're not stepping on our coworkers' toes.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1592842949400,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62407943",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592597207007,
        "Question_original_content":"restrict number node pipein written pipelin want run remot comput cluster aim process larg histor data need run pipelin larg number input paramet combin wai restrict number node pipelin us cluster default us node avail cluster like restrict us pre defin maximum allow leav rest cluster free user current code start pipelin look like setup pipelin step data import step contain pythonscriptstep pipelin pipelin workspac step step pipelin valid big long list histor date want process data dt date rang freq close left submit pipelin job dt pipelin run experi pipelin run submit pipelin pipelin paramet import datetim strftime",
        "Question_preprocessed_content":"restrict number node pipein written pipelin want run remot comput cluster aim process larg histor data need run pipelin larg number input paramet combin wai restrict number node pipelin us cluster default us node avail cluster like restrict us maximum allow leav rest cluster free user current code start pipelin look like",
        "Question_gpt_summary_original":"The user wants to run a pipeline on a remote compute cluster within Azure Machine Learning to process a large amount of historical data. However, the user wants to restrict the number of nodes used by the pipeline on the cluster to a pre-defined maximum, leaving the rest of the cluster free for other users.",
        "Question_gpt_summary":"user want run pipelin remot comput cluster process larg histor data user want restrict number node pipelin cluster pre defin maximum leav rest cluster free user",
        "Answer_original_content":"killer featur have worri load balanc like team comput target max node featur branch hyperdr pipelin result run pipelin submit multipl pipelinerun orchestr heavi lift queu submit run pipelinerun execut serial order submit cluster overload work issu time look like pipelinerun execut parallel check parallelrunstep option isol comput computetarget workspac node computetarget cost node computetarget team us pygit computetarget creat featur branch data scientist confid step cowork toe",
        "Answer_preprocessed_content":"killer featur have worri load balanc like team comput target featur branch pipelin result run pipelin submit multipl orchestr heavi lift queu submit run execut serial order submit cluster overload work issu time look like execut parallel check option isol comput workspac cost team us creat featur branch data scientist confid step cowork toe",
        "Answer_gpt_summary_original":"possible solutions to the challenge of restricting the number of nodes used by a pipeline to process a large amount of historical data, while leaving the rest of the cluster free for other users, include using a compute target with max_nodes=100 for every feature branch and hyperdrive pipelines, submitting multiple pipelineruns back-to-back, using parallelrunstep to execute pipelineruns in parallel, and isolating computes by creating a computetarget for each feature branch using pygit2.",
        "Answer_gpt_summary":"possibl solut challeng restrict number node pipelin process larg histor data leav rest cluster free user includ comput target max node featur branch hyperdr pipelin submit multipl pipelinerun parallelrunstep execut pipelinerun parallel isol comput creat computetarget featur branch pygit"
    },
    {
        "Question_id":null,
        "Question_title":"Sync S3 remote bucket auth w\/GitHub repo access settings",
        "Question_body":"<p>Hi folks,<\/p>\n<p>My org is a happy GitHub user and we\u2019re looking to adopt DVC. Question: has anyone attempted a way to sync a GitHub repo\u2019s access settings with an S3 bucket? I\u2019d like to avoid having to replicate the same access settings used on every GitHub repo w\/its associated DVC S3 remote AWS permissions.<\/p>\n<p><strong>edit: similar to how <a href=\"https:\/\/help.github.com\/en\/github\/managing-large-files\/collaboration-with-git-large-file-storage\" rel=\"nofollow noopener\">LFS auth appears to \u201cjust work\u201d<\/a> when used w\/Gitub.<\/strong><\/p>\n<p>I\u2019m referring to the \u201cManage Access\u201d area of a GitHub repo:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a.png\" data-download-href=\"\/uploads\/short-url\/foQOK0UABlsex4GkRdJJ5CU3UT0.png?dl=1\" title=\"Manage_access\" rel=\"nofollow noopener\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_690x467.png\" alt=\"Manage_access\" data-base62-sha1=\"foQOK0UABlsex4GkRdJJ5CU3UT0\" width=\"690\" height=\"467\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_690x467.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_1035x700.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_1380x934.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/6befc803d4e5c278c7c00cc29333e1ab783e079a_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Manage_access<\/span><span class=\"informations\">1504\u00d71018 70.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1587768239117,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":533.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/sync-s3-remote-bucket-auth-w-github-repo-access-settings\/365",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-04-25T00:13:54.435Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dlite\">@dlite<\/a>,<\/p>\n<p>DVC does not integrate with Github or provid an interface to manage remote storage access. But feel free to send an issue request on <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\">https:\/\/github.com\/iterative\/dvc\/issues<\/a> about that.<\/p>\n<p>Maybe you could implement a script that uses the <a href=\"https:\/\/developer.github.com\/v3\/orgs\/members\/\">GH API \/orgs\/members endpoints<\/a> to fetch the org user access settings periodically and apply them to S3 via its own CLI or API (which I didn\u2019t check to see if exposes these resources).<\/p>\n<p>Best<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-25T12:10:42.908Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> - that\u2019s a good idea. I\u2019ll investigate.<\/p>",
                "Answer_score":47.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-29T01:06:04.719Z",
                "Answer_body":"<p>It would be a really great feature, <a class=\"mention\" href=\"\/u\/dlite\">@dlite<\/a>!<\/p>\n<p>How about writing a Github action?<\/p>\n<p>It is sad I don\u2019t see an obvious trigger in this list - <a href=\"https:\/\/help.github.com\/en\/actions\/reference\/events-that-trigger-workflows\">https:\/\/help.github.com\/en\/actions\/reference\/events-that-trigger-workflows<\/a><\/p>\n<p>But may be we it can be a cron like even that you run every 5 minutes - <a href=\"https:\/\/help.github.com\/en\/actions\/reference\/workflow-syntax-for-github-actions#onschedule\">https:\/\/help.github.com\/en\/actions\/reference\/workflow-syntax-for-github-actions#onschedule<\/a> ?<\/p>\n<p>This is how an action that check some ACLs look like - <a href=\"https:\/\/github.com\/ludeeus\/action-accesscontrol\/blob\/master\/runaction.py\">https:\/\/github.com\/ludeeus\/action-accesscontrol\/blob\/master\/runaction.py<\/a> \u2026 I hope something similar could be done to read the whole list of collaborators\/team members and ping AWS via API to sync permissions?<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-29T13:55:48.847Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> - thanks for the starting points!<\/p>\n<p>I\u2019m hoping to carve out some time to investigate in the next couple of weeks. If I get something going, I\u2019ll post an update to this thread.<\/p>",
                "Answer_score":56.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"sync remot bucket auth github repo access set folk org happi github user look adopt question attempt wai sync github repo access set bucket like avoid have replic access set github repo associ remot aw permiss edit similar lf auth appear work gitub refer manag access area github repo manag access",
        "Question_preprocessed_content":"sync remot bucket auth repo access set folk org happi github user look adopt question attempt wai sync github repo access set bucket like avoid have replic access set github repo associ remot aw permiss edit similar lf auth appear work refer manag access area github repo",
        "Question_gpt_summary_original":"The user is looking for a way to sync a GitHub repo's access settings with an S3 bucket to avoid replicating the same access settings used on every GitHub repo with its associated DVC S3 remote AWS permissions. They are specifically referring to the \"Manage Access\" area of a GitHub repo.",
        "Question_gpt_summary":"user look wai sync github repo access set bucket avoid replic access set github repo associ remot aw permiss specif refer manag access area github repo",
        "Answer_original_content":"dlite integr github provid interfac manag remot storag access feel free send issu request http github com iter issu mayb implement script us api org member endpoint fetch org user access set period appli cli api didnt check expos resourc best thank jorgeorpinel that good idea ill investig great featur dlite write github action sad dont obviou trigger list http help github com action refer event trigger workflow cron like run minut http help github com action refer workflow syntax github action onschedul action check acl look like http github com ludeeu action accesscontrol blob master runact hope similar read list collabor team member ping aw api sync permiss shcheklein thank start point hope carv time investig coupl week go ill post updat thread",
        "Answer_preprocessed_content":"integr github provid interfac manag remot storag access feel free send issu request mayb implement script us api endpoint fetch org user access set period appli cli api best thank that good idea ill investig great featur write github action sad dont obviou trigger list cron like run minut action check acl look like hope similar read list member ping aw api sync permiss thank start point hope carv time investig coupl week go ill post updat thread",
        "Answer_gpt_summary_original":"the answer suggests that there is no direct integration between github and s3 for syncing access settings, but the user can send an issue request on github to implement this feature. the answer also suggests implementing a script that uses the github api to fetch user access settings and apply them to s3 via its own cli or api. another suggestion is to write a github action that checks the list of collaborators\/team members and pings aws via api to sync permissions. the user plans to investigate these options in the next couple of weeks and will post an update to the thread if they make progress.",
        "Answer_gpt_summary":"answer suggest direct integr github sync access set user send issu request github implement featur answer suggest implement script us github api fetch user access set appli cli api suggest write github action check list collabor team member ping aw api sync permiss user plan investig option coupl week post updat thread progress"
    },
    {
        "Question_id":64359321.0,
        "Question_title":"creating python server in AWS without breaching free tier",
        "Question_body":"<p>Just a conceptual question here.<\/p>\n<p>I'm a newbie to aws. I have a node app and a python file that is currently on a Flask server. The node app sends data to the Py server and gets data back. This takes approx 3.2 secs to happen. I am not sure how I can apply this to AWS. I tried sagemaker but it was really costly for me. Is there anyway I can create a Python server with an endpoint in AWS within the free tier?<\/p>\n<p>Thanks<\/p>\n<p>Rushi<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602699610963,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":451.0,
        "Answer_body":"<p>You do not need to use sagemaker to deploy your flask application to AWS. AWS has a nice documentation to deploy a Flask Application to an <a href=\"https:\/\/docs.aws.amazon.com\/elasticbeanstalk\/latest\/dg\/create-deploy-python-flask.html\" rel=\"nofollow noreferrer\">AWS Elastic Beanstalk<\/a> environment.<\/p>\n<p>Other than that you can also deploy the application using two methods.<\/p>\n<ol>\n<li>via EC2<\/li>\n<li>via Lambda<\/li>\n<\/ol>\n<h3>EC2 Instances<\/h3>\n<p>You can launch the ec2 instance with public IP with SSH enabled from your IP address. Then SSH into the instance and install the python, it's libraries and your application.<\/p>\n<h3>Lambda<\/h3>\n<p>AWS lambda is the perfect solution. It scales automatically, depends upon the requests your application will receive.\nAs lambda needs your dependencies be available in the package, so you need to install them using <code>--target<\/code> parameter, zip the python code along with the installed packages and then upload to the Lambda.<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>pip install --target .\/package Flask\ncd package\nzip -r9 function.zip . # Create a ZIP archive of the dependencies.\ncd .. &amp;&amp; zip -g function.zip lambda_function.py # Add your function code to the archive.\n<\/code><\/pre>\n<p>For more detailed instructions you can read these documentations<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/lambda\/latest\/dg\/lambda-python.html\" rel=\"nofollow noreferrer\">Lambda<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64359321",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1602704406347,
        "Question_original_content":"creat python server aw breach free tier conceptu question newbi aw node app python file current flask server node app send data server get data take approx sec happen sure appli aw tri costli creat python server endpoint aw free tier thank rushi",
        "Question_preprocessed_content":"creat python server aw breach free tier conceptu question newbi aw node app python file current flask server node app send data server get data take approx sec happen sure appli aw tri costli creat python server endpoint aw free tier thank rushi",
        "Question_gpt_summary_original":"The user is a newbie to AWS and is facing challenges in creating a Python server with an endpoint in AWS without breaching the free tier. They have a node app and a Python file on a Flask server, which takes approximately 3.2 seconds to send and receive data. The user tried using Sagemaker but found it to be too costly.",
        "Question_gpt_summary":"user newbi aw face challeng creat python server endpoint aw breach free tier node app python file flask server take approxim second send receiv data user tri costli",
        "Answer_original_content":"need us deploi flask applic aw aw nice document deploi flask applic aw elast beanstalk environ deploi applic method lambda instanc launch instanc public ssh enabl address ssh instanc instal python librari applic lambda aw lambda perfect solut scale automat depend request applic receiv lambda need depend avail packag need instal target paramet zip python code instal packag upload lambda pip instal target packag flask packag zip function zip creat zip archiv depend zip function zip lambda function add function code archiv detail instruct read document lambda",
        "Answer_preprocessed_content":"need us deploi flask applic aw aw nice document deploi flask applic aw elast beanstalk environ deploi applic method lambda instanc launch instanc public ssh enabl address ssh instanc instal python librari applic lambda aw lambda perfect solut scale automat depend request applic receiv lambda need depend avail packag need instal paramet zip python code instal packag upload lambda detail instruct read document lambda",
        "Answer_gpt_summary_original":"possible solutions for creating a python server with an endpoint in aws within the free tier for a node app and a python file currently on a flask server are: deploying the flask application to an aws elastic beanstalk environment, launching an ec2 instance with public ip and installing python and its libraries, or using aws lambda which automatically scales and requires dependencies to be available in the package. for lambda, the dependencies need to be installed using the --target parameter, zipped along with the python code, and uploaded to lambda.",
        "Answer_gpt_summary":"possibl solut creat python server endpoint aw free tier node app python file current flask server deploi flask applic aw elast beanstalk environ launch instanc public instal python librari aw lambda automat scale requir depend avail packag lambda depend need instal target paramet zip python code upload lambda"
    },
    {
        "Question_id":null,
        "Question_title":"How to using client to upload data\/artifacts to s3 bucket?",
        "Question_body":"How to use the client to upload data\/artifacts to the s3 bucket?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1665738037000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1516",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-14T10:05:16Z",
                "Answer_score":1,
                "Answer_body":"To upload artifacts to a newly created run, you can use a snippet like this one:\n\n...\n# upload_to should be a value like `path\/code`\nupload_to = upload_to or DEFAULT_UPLOADS_PATH\nmeta_info = {META_UPLOAD_ARTIFACTS: upload_to}\nclient = RunClient(owner=owner, project=project_name)\nrun_instance = client.create(\n    name=name,\n    description=description,\n    tags=tags,\n    content=op_spec,\n    is_managed=True,\n    meta_info=meta_info,\n   pending=V1RunPending.UPLOAD,  # Very important to tell the server to wait for the upload to finish\n)\n...\n# If path from then check if file otherwise it's local directory\nis_file = os.path.isfile(path_from) if path_from else False \nclient = RunClient(owner=owner, project=project_name, run_uuid=run_instance.uuid)\nif is_file:\n    response = client.upload_artifact(filepath=path_from, path=path_to, overwrite=True)\nelse:\n    response = client.upload_artifacts_dir(dirpath=path_from, path=path_to, overwrite=True, relative_to=path_from)"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0,
        "Question_closed_time":null,
        "Question_original_content":"client upload data artifact bucket us client upload data artifact bucket",
        "Question_preprocessed_content":"client upload bucket us client upload bucket",
        "Question_gpt_summary_original":"The user is facing challenges in understanding how to use the client to upload data\/artifacts to an s3 bucket.",
        "Question_gpt_summary":"user face challeng understand us client upload data artifact bucket",
        "Answer_original_content":"upload artifact newli creat run us snippet like upload valu like path code upload upload default upload path meta info meta upload artifact upload client runclient owner owner project project run instanc client creat descript descript tag tag content spec manag true meta info meta info pend vrunpend upload import tell server wait upload finish path check file local directori file path isfil path path fals client runclient owner owner project project run uuid run instanc uuid file respons client upload artifact filepath path path path overwrit true respons client upload artifact dir dirpath path path path overwrit true rel path",
        "Answer_preprocessed_content":"upload artifact newli creat run us snippet like valu like client runclient descript descript tag tag import tell server wait upload finish path check file local directori fals client runclient respons overwrit true respons overwrit true",
        "Answer_gpt_summary_original":"Solution: The solution provided is to use a code snippet to upload artifacts to a newly created run. The code snippet includes specifying the path to upload the artifacts to, creating a client instance, creating a run instance, and uploading the artifacts using the client instance. The code also includes checking if the path is a file or a directory and uploading accordingly.",
        "Answer_gpt_summary":"solut solut provid us code snippet upload artifact newli creat run code snippet includ specifi path upload artifact creat client instanc creat run instanc upload artifact client instanc code includ check path file directori upload accordingli"
    },
    {
        "Question_id":null,
        "Question_title":"machine learning Studio",
        "Question_body":"Hi:\n\nI wonder when will the classic Machine Learning Studio retire?\n\nAlso in order to save my data and file, any preparation or migration should be done to avoid any loss?\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648415916797,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @dontbelazy-2604\n\nThanks for reaching out to us, I think you are mentioning Azure Machine Learning Studio(classic). Machine Learning Studio (classic) will retire on 31 August 2024.\n\nFrom now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic). Beginning 1 December 2021, you will not be able to create new Machine Learning Studio (classic) resources.\n\nRequired action to avoid loss:\n\nFollow these steps to transition using Azure Machine Learning before 31 August 2024. Pricing may be subject to change, please view pricing here.\n\nHope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/789066\/machine-learning-studio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-27T23:20:57.9Z",
                "Answer_score":0,
                "Answer_body":"Hello @dontbelazy-2604\n\nThanks for reaching out to us, I think you are mentioning Azure Machine Learning Studio(classic). Machine Learning Studio (classic) will retire on 31 August 2024.\n\nFrom now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic). Beginning 1 December 2021, you will not be able to create new Machine Learning Studio (classic) resources.\n\nRequired action to avoid loss:\n\nFollow these steps to transition using Azure Machine Learning before 31 August 2024. Pricing may be subject to change, please view pricing here.\n\nHope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks!",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1648423257900,
        "Question_original_content":"machin learn studio wonder classic machin learn studio retir order save data file prepar migrat avoid loss thank",
        "Question_preprocessed_content":"machin learn studio wonder classic machin learn studio retir order save data file prepar migrat avoid loss thank",
        "Question_gpt_summary_original":"The user is concerned about the retirement of the classic Machine Learning Studio and wants to know if any preparation or migration is required to avoid data and file loss.",
        "Question_gpt_summary":"user concern retir classic machin learn studio want know prepar migrat requir avoid data file loss",
        "Answer_original_content":"hello dontbelazi thank reach think mention studio classic machin learn studio classic retir august august continu us exist machin learn studio classic begin decemb abl creat new machin learn studio classic resourc requir action avoid loss follow step transit august price subject chang view price hope help regard yutong kindli accept answer feel help thank",
        "Answer_preprocessed_content":"hello thank reach think mention studio machin learn studio retir august august continu us exist machin learn studio begin decemb abl creat new machin learn studio resourc requir action avoid loss follow step transit august price subject chang view price hope help regard yutong kindli accept answer feel help thank",
        "Answer_gpt_summary_original":"the classic machine learning studio will retire on august 31, 2024. users can continue to use the existing machine learning studio (classic) until that date, but they will not be able to create new resources after december 1, 2021. to avoid any loss of data or files, users should follow the transition steps before august 31, 2024. pricing may be subject to change, and users should view the pricing page for more information.",
        "Answer_gpt_summary":"classic machin learn studio retir august user continu us exist machin learn studio classic date abl creat new resourc decemb avoid loss data file user follow transit step august price subject chang user view price page inform"
    },
    {
        "Question_id":62515398.0,
        "Question_title":"Out-of-memory error webservice deployed with Azure ML Studio",
        "Question_body":"<p>My webservice deployed with Azure Machine Learning Studio exposes a classification model. Since the last re-training and re-deployment, in circa 1% of the processed cases in production, I get either of the two  following out-of-memory (possibly correlated) errors:<\/p>\n<ol>\n<li>&quot;The model consumed more memory than was appropriated for it. Maximum allowed memory for the model is 2560 MB. Please check your model for issues.&quot;<\/li>\n<li>&quot;The following error occurred during evaluation of R script: R_tryEval: return error: Error: cannot allocate vector of size 57.6 Mb&quot;<\/li>\n<\/ol>\n<p>I did not mange to find anything to solve this issue according to the official documentation.<\/p>\n<p>Note that these errors occur exclusively while trying to consume the webservice (and not while training, evaluation and deployment).<\/p>\n<p>Also, consuming the webservice in batch mode, as suggested <a href=\"https:\/\/social.microsoft.com\/Forums\/azure\/he-IL\/ccf4c683-f904-4117-8a4e-3258a56515f9\/azureml-execure-r-script-cannot-allocate-vector-of-size-818-mb?forum=MachineLearning\" rel=\"nofollow noreferrer\">here<\/a>, is not a viable option for my business use case.<\/p>\n<p>Is there a way to increase the memory usage limit for webservices deployed in Azure ML Studio?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592831881063,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":341.0,
        "Answer_body":"<p>Currently, there's no way to increase memory limit in Classic Studio. We encouraged customers to try <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-designer\" rel=\"nofollow noreferrer\">Azure Machine Learning designer (preview)<\/a>, which provides similar drag and drop ML modules plus scalability, version control, and enterprise security. Furthermore, with Designer, the endpoints are deployed to AKS where no limit other than cluster resource is imposed.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62515398",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592917003232,
        "Question_original_content":"memori error webservic deploi studio webservic deploi studio expos classif model train deploy circa process case product follow memori possibl correl error model consum memori appropri maximum allow memori model check model issu follow error occur evalu script tryeval return error error alloc vector size mang solv issu accord offici document note error occur exclus try consum webservic train evalu deploy consum webservic batch mode suggest viabl option busi us case wai increas memori usag limit webservic deploi studio",
        "Question_preprocessed_content":"error webservic deploi studio webservic deploi studio expos classif model circa process case product follow error model consum memori appropri maximum allow memori model check model follow error occur evalu script return error error alloc vector size mang solv issu accord offici document note error occur exclus try consum webservic consum webservic batch mode suggest viabl option busi us case wai increas memori usag limit webservic deploi studio",
        "Question_gpt_summary_original":"The user's webservice deployed with Azure Machine Learning Studio is encountering out-of-memory errors while processing 1% of the cases in production. The errors occur exclusively while trying to consume the webservice, and not while training, evaluation, and deployment. The user has not found any solution to the issue according to the official documentation, and batch mode consumption is not a viable option for their business use case. The user is seeking a way to increase the memory usage limit for webservices deployed in Azure ML Studio.",
        "Question_gpt_summary":"user webservic deploi studio encount memori error process case product error occur exclus try consum webservic train evalu deploy user solut issu accord offici document batch mode consumpt viabl option busi us case user seek wai increas memori usag limit webservic deploi studio",
        "Answer_original_content":"current wai increas memori limit classic studio encourag custom try design preview provid similar drag drop modul plu scalabl version control enterpris secur furthermor design endpoint deploi ak limit cluster resourc impos",
        "Answer_preprocessed_content":"current wai increas memori limit classic studio encourag custom try design provid similar drag drop modul plu scalabl version control enterpris secur furthermor design endpoint deploi ak limit cluster resourc impos",
        "Answer_gpt_summary_original":"possible solutions to the out-of-memory errors when consuming a webservice deployed with  studio are to try using the designer (preview) which provides similar drag and drop ml modules plus scalability, version control, and enterprise security. additionally, with designer, the endpoints are deployed to aks where no limit other than cluster resource is imposed. however, currently, there's no way to increase memory limit in classic studio.",
        "Answer_gpt_summary":"possibl solut memori error consum webservic deploi studio try design preview provid similar drag drop modul plu scalabl version control enterpris secur addition design endpoint deploi ak limit cluster resourc impos current wai increas memori limit classic studio"
    },
    {
        "Question_id":null,
        "Question_title":"How does wandb.tensorboard.patch works?",
        "Question_body":"<p>Hi everyone,<br>\nI\u2019m trying to move from <code>tensorboard<\/code> to <code>wandb<\/code> that seems more flexible and has some useful additional feature to customize plots.<\/p>\n<p>In my code I defined two<code> Tensorboard<\/code> <code>SummaryWriter<\/code>; I\u2019d like to import the plot associated to one of them in <code>wandb<\/code>.<\/p>\n<p>In other words I want to include in <code>wandb<\/code> only a part of the <code>tensorboard<\/code> logs of my code.<br>\nIs it possible to do it?<br>\nIf, for example, I have the 2 summaries stored in 2 different folders a and a\/b<\/p>\n<pre><code class=\"lang-auto\">Sum1 = SummaryWriter('a')\nSum2 = SummaryWriter('a\/b')\n<\/code><\/pre>\n<p>and I\u2019m only interested in import Sum2 in<code> wandb<\/code> .<\/p>\n<p>I thought that<br>\n<code>wandb.tensorboard.patch(root_logdir='a\/b', pytorch=True)<\/code><br>\nwas the way but it doesn\u2019t seem the case.<br>\nIn fact I get this message:<\/p>\n<p><code>e[34me[1mwandbe[0m: e[33mWARNINGe[0m Found logdirectory outside of given root_logdir, dropping given root_logdir for eventfile in a<\/code><\/p>\n<p>I want that paths outside root_logdir to be ignored.<br>\nApart from this page I did\u2019t found a clear documentation about wandb.tensorboard.patch.<br>\nAny idea about how to proceed?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1637846804538,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":250.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-does-wandb-tensorboard-patch-works\/1386",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-30T16:03:43.715Z",
                "Answer_body":"<p>Have you been able to look at this doc (<a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/tensorboard#how-do-i-configure-tensorboard-when-im-using-it-with-wandb\" class=\"inline-onebox-loading\">https:\/\/docs.wandb.ai\/guides\/integrations\/tensorboard#how-do-i-configure-tensorboard-when-im-using-it-with-wandb<\/a>)<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-06T17:33:41.670Z",
                "Answer_body":"<p>Hi Emanuele, was the document I provided able to help you, or are you still running into this issue?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-24T13:27:02.380Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"tensorboard patch work try tensorboard flexibl us addit featur custom plot code defin tensorboard summarywrit like import plot associ word want includ tensorboard log code possibl exampl summari store differ folder sum summarywrit sum summarywrit interest import sum thought tensorboard patch root logdir pytorch true wai doesnt case fact messag mwarning logdirectori outsid given root logdir drop given root logdir eventfil want path outsid root logdir ignor apart page didt clear document tensorboard patch idea proce",
        "Question_preprocessed_content":"work try flexibl us addit featur custom plot code defin like import plot associ word want includ log code possibl exampl summari store differ folder interest import sum thought wai doesnt case fact messag want path outsid ignor apart page didt clear document idea proce",
        "Question_gpt_summary_original":"The user is facing a challenge in importing a specific Tensorboard log into Wandb. They have defined two Tensorboard SummaryWriters and want to import only one of them into Wandb. The user attempted to use the wandb.tensorboard.patch function but received a warning message indicating that the log directory was outside the given root_logdir. The user is seeking guidance on how to proceed and is looking for clear documentation on wandb.tensorboard.patch.",
        "Question_gpt_summary":"user face challeng import specif tensorboard log defin tensorboard summarywrit want import user attempt us tensorboard patch function receiv warn messag indic log directori outsid given root logdir user seek guidanc proce look clear document tensorboard patch",
        "Answer_original_content":"abl look doc http doc guid integr tensorboard configur tensorboard emanuel document provid abl help run issu topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"abl look doc emanuel document provid abl help run issu topic automat close dai repli new repli longer allow",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer. the responder suggests checking a documentation link and asks if it was helpful. the topic is closed, indicating that further discussion is not possible.",
        "Answer_gpt_summary":"specif solut provid answer respond suggest check document link ask help topic close indic discuss possibl"
    },
    {
        "Question_id":null,
        "Question_title":"How can I check whether an artifact is available?",
        "Question_body":"<p>Hi, just started to use W&amp;B and managed to refactor some code to use artifact versioning today. What I could not find is (and sorry if this is very basic): during the first run of the program I would like to check if there is already some artifact (raw data) f\u00fcr that project \/ artifact name \/ type available: If yes, use it. If no, prepare it (might take a while). I am looking for the equivalent of <code>&lt;filename&gt;.is_file()<\/code> but for artifacts. I could use\/download the artifact in a <code>try, except<\/code> clause but that\u2019s not very pretty (throwing errors on the console, not sure what the correct Exception is). The API does not seem to provide such a functionality?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1643300292817,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":168.0,
        "Answer_body":"<p>Hey Stephan,<\/p>\n<p>Thanks for your response. I think the code you have written is the best way to check if an artifact exists if you do not know a priori if it really exists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-can-i-check-whether-an-artifact-is-available\/1826",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-27T17:06:02.937Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/hogru\">@hogru<\/a>,<\/p>\n<p>You should be able to access the artifacts for a run through <code>run.logged_artifacts()<\/code> in our API. Here is the link to our <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/run#logged_artifacts\">docs<\/a> for this.<\/p>\n<p>Please let me know if this is what you were looking for.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-28T10:11:44.470Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a> ,<\/p>\n<p>thank you very much for the quick response. Unfortunately this does not do what I need (or I don\u2019t get it yet).<\/p>\n<p>I have 2 issues with that:<\/p>\n<p>(1) It does not do what (I understand) it should do:<\/p>\n<ul>\n<li>In the wandb UI (the website) I see an artifact, e.g. <code>iris-raw:v2<\/code> (toy example)<\/li>\n<li>This artifact is \u201cUsed by\u201d a run, e.g. <code>ancient-waterfall-39<\/code>\n<\/li>\n<li>This run has a \u201cRun path\u201d in its overview page<\/li>\n<li>I then use this \u201cRun path\u201d in <code>run = api.run(\"Run Path\")<\/code> (OK) and call <code>artifacts = run.logged_artifacts() <\/code> (OK)<\/li>\n<li>I can\u2019t find the artifact <code>iris-raw:v2<\/code> (should I?). From inspecting the variable I see a <code>length<\/code> of 0 and and empty <code>objects<\/code> list<\/li>\n<\/ul>\n<p>(2) I need to know the \u201cRun path\u201d<\/p>\n<ul>\n<li>I would like to check, which (if any) artifacts already exist and have been created for a given <code>entity\/project\/<\/code>\n<\/li>\n<li>In this situation (the program starts up and wants to see what\u2019s already there) I don\u2019t know neither the name nor the run path of the (previous) run(s)<\/li>\n<\/ul>\n<p>I could of course check for local copies but I am trying to switch that logic to wandb.<\/p>\n<p>Any more hints, APIs, clarifications? It\u2019s totally possible that I overlook something.<\/p>\n<p>Best,<br>\nStephan<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-31T23:18:14.554Z",
                "Answer_body":"<p>Hi Stephan,<\/p>\n<ol>\n<li>\n<p>Used artifacts and Logged artifacts are separate and serve different purposes. For example, when you train a model and log it as an artifact, but now you want to fine tune this pretrained model, a new run \u201cuses\u201d the artifact and then \u201clogs\u201d a new version (assuming it is being logged into the same artifact).<\/p>\n<p>We allow for you to check both through <code>run.used_artifacts()<\/code> and <code>run.logged_artifacts()<\/code> respectively.<\/p>\n<\/li>\n<li>\n<p>An artifact does not need an exact Run ID to be referred, it can be referred as:<\/p>\n<\/li>\n<\/ol>\n<pre><code class=\"lang-auto\">artifact = api.Artifact('&lt;project&gt;\/&lt;artifact&gt;:&lt;alias&gt;')\n<\/code><\/pre>\n<p>where <code>alias<\/code> will mostly refer to the version of the artifact (or \u201clatest\u201d). However, if you want the artifacts associated to a run (as in the artifacts used or logged by a run), you would need to call <code>used_artifact<\/code> or <code>logged_artifact<\/code>.<\/p>\n<p>I hope this clarifies your doubts for you. Please let me know if you have any further questions.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-04T18:19:46.703Z",
                "Answer_body":"<p>Hi Stephan,<\/p>\n<p>I wanted to follow up on this request as we have not heard back from you. Please let us know if we can be of further assistance or if this issue has been resolved.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-06T20:33:25.626Z",
                "Answer_body":"<p>Hi Ramit,<\/p>\n<p>thank you for asking. The answer is both yes and no <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=12\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> Yes in the sense that I have learned a lot about wandb because of your answers and no in the sense that my initial question is still open (but I have a \u201cworkaround\u201d). The current code snippet looks like that:<\/p>\n<pre><code class=\"lang-auto\">with wandb.init(project=settings.project_name,\n                    job_type='load_data',\n                    dir=wb_dir,\n                    tags=[settings['ENV_FOR_DYNACONF']]) as run:\n\ntry:\n    wb_data_prep = run.use_artifact(f'{wb_prep_name}:latest')\n    wb_data_prep.download(root=data_prep_dir)\n    prep_data = False\n\nexcept wandb.CommError as exception:\n    log.debug(f'wandb raised exception: {exception}')\n    log.debug('Preprocessed data not available from wandb, create anew from raw data')\n    prep_data = True\n<\/code><\/pre>\n<p>The context is that I want to check if wandb has the data available (it\u2019s created in another script\/run) and I was looking for an if statement to check whether the file exists in order to avoid the try\/except. But the run I am in does not have any used or logged artifacts at that time (in fact it even doesn\u2019t offer the methods in this context). But this bit of code works so for the moment I (think I) can leave it as is.<\/p>\n<p>If you feel I am missing something I am happy to learn.<\/p>\n<p>Best,<br>\nStephan<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-09T18:16:35.263Z",
                "Answer_body":"<p>Hey Stephan,<\/p>\n<p>Thanks for your response. I think the code you have written is the best way to check if an artifact exists if you do not know a priori if it really exists.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-22T07:58:33.625Z",
                "Answer_body":"<p>Is it possible to get an artifact from multiple runs in a project just using the alias and artifact type?<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-23T07:58:41.020Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1644430595263,
        "Question_original_content":"check artifact avail start us manag refactor code us artifact version todai sorri basic run program like check artifact raw data project artifact type avail ye us prepar look equival file artifact us download artifact try claus that pretti throw error consol sure correct except api provid function",
        "Question_preprocessed_content":"check artifact avail start us manag refactor code us artifact version todai run program like check artifact project artifact type avail ye us prepar look equival artifact artifact claus that pretti api provid function",
        "Question_gpt_summary_original":"The user is facing a challenge in checking whether an artifact is available during the first run of a program. They are looking for an equivalent of <code>&lt;filename&gt;.is_file()<\/code> but for artifacts and are unsure of the correct exception to use in a <code>try, except<\/code> clause. The API does not seem to provide this functionality.",
        "Question_gpt_summary":"user face challeng check artifact avail run program look equival file artifact unsur correct except us try claus api provid function",
        "Answer_original_content":"hei stephan thank respons think code written best wai check artifact exist know priori exist thank ramit",
        "Answer_preprocessed_content":"hei stephan thank respons think code written best wai check artifact exist know priori exist thank ramit",
        "Answer_gpt_summary_original":"there are no specific solutions mentioned in the answer. the answer simply acknowledges the code provided by stephan as the best way to check if an artifact exists when it is not known beforehand.",
        "Answer_gpt_summary":"specif solut mention answer answer simpli acknowledg code provid stephan best wai check artifact exist known"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML - Notebook - Jupyter Kernel Error - No Kernel Connecl",
        "Question_body":"In Azure ML i am following the tutorial to execute a notebook and when i open the notebook and have a valid compute (running as well as the green icon) it shows an error 'Jupyter Kernel Error'. On the far right of the screen it says 'No Kernel Connnected'. What is needed to connect the kernel?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1595605454123,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/52057\/azure-ml-notebook-jupyter-kernel-error-no-kernel-c.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-28T02:40:35.287Z",
                "Answer_score":0,
                "Answer_body":"Hi,\nmay I know which tutorial you are mentioning? If you are facing issue with notebooks.azure.com then the most likely scenario is your project is not running. Please navigate back to the project page and run your project before opening the required notebook in a new window which should show all the options to select the kernel and run a cell.\n\nThanks,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-19T06:18:42.667Z",
                "Answer_score":0,
                "Answer_body":"In new ML workspace you need to setup the computing resource to run the jupyter. The resource can be single VM, cluster (available set) or container set. You also can select using GPU or not.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":0.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"notebook jupyt kernel error kernel connecl follow tutori execut notebook open notebook valid comput run green icon show error jupyt kernel error far right screen sai kernel coect need connect kernel",
        "Question_preprocessed_content":"notebook jupyt kernel error kernel connecl follow tutori execut notebook open notebook valid comput show error jupyt kernel error far right screen sai kernel coect need connect kernel",
        "Question_gpt_summary_original":"The user is encountering a Jupyter Kernel Error while trying to execute a notebook in Azure ML. Despite having a valid compute with a green icon, the error message indicates that there is no kernel connected. The user is seeking guidance on how to connect the kernel.",
        "Question_gpt_summary":"user encount jupyt kernel error try execut notebook despit have valid comput green icon error messag indic kernel connect user seek guidanc connect kernel",
        "Answer_original_content":"know tutori mention face issu notebook azur com like scenario project run navig project page run project open requir notebook new window option select kernel run cell thank yutong new workspac need setup comput resourc run jupyt resourc singl cluster avail set contain set select gpu",
        "Answer_preprocessed_content":"know tutori mention face issu like scenario project run navig project page run project open requir notebook new window option select kernel run cell thank yutong new workspac need setup comput resourc run jupyt resourc singl cluster contain set select gpu",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. if the user is facing an issue with notebooks.azure.com, the most likely scenario is that their project is not running. they should navigate back to the project page and run their project before opening the required notebook in a new window, which should show all the options to select the kernel and run a cell.\n\n2. in the new ml workspace, the user needs to set up the computing resource to run the jupyter notebook. the resource can be a single vm, cluster (available set), or container set. they can also select whether to use gpu or not.",
        "Answer_gpt_summary":"possibl solut answer user face issu notebook azur com like scenario project run navig project page run project open requir notebook new window option select kernel run cell new workspac user need set comput resourc run jupyt notebook resourc singl cluster avail set contain set select us gpu"
    },
    {
        "Question_id":53891907.0,
        "Question_title":"Pandas dataframe and apply - Can't figure out why resulting values are negative",
        "Question_body":"<p>Here's a picture of my data, the column of interest RUL is on the far right the names got cut off (I'm using the Turbo Engine Degradation dataset from NASA) can be found here: <a href=\"https:\/\/data.nasa.gov\/widgets\/vrks-gjie\" rel=\"nofollow noreferrer\">https:\/\/data.nasa.gov\/widgets\/vrks-gjie<\/a><\/p>\n\n<p>I'm doing this in Azure ML Studio but code snippet below, I have 2 helper functions get_engine_last_cycle (which when I unit test it seems to do as expected - compute the last cycle for that engine, for example engine 2 has a max cycle in this dataset of 287 when it fails). The final helper function I call get_engine_remainig_life, takes the engine and cycle as arguments and returns the max cycle - current cycle for that engine (again I've unit tested this and it seems to give me expected results).<\/p>\n\n<p>For some reason this isn't working when I run my notebook. The column which I call \"RUL\" should return a sequence of decreasing, positive integers for example 287, 286, 285 284, etc for engine #2. However, it's giving me negative values. I can't seem to figure out why but know the problem is likely with this one piece of code<\/p>\n\n<pre><code> df['RUL'] = df[['engine', 'cycle']].apply(lambda x: get_engine_remaining_life(*x), axis=1)\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/8IYOx.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8IYOx.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<pre><code>    def get_engine_last_cycle(engine):\n        return int(df.loc[engine, ['cycle']].max())\n\n\n    def get_engine_remaining_life(engine, cycle):\n        return get_engine_last_cycle(engine) - int(cycle)\n\n    df['RUL'] = df[['engine', 'cycle']].apply(lambda x: get_engine_remaining_life(*x), axis=1)\n\n    return df\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1545435517580,
        "Question_favorite_count":null,
        "Question_last_edit_time":1545438768860,
        "Question_score":0.0,
        "Question_view_count":66.0,
        "Answer_body":"<p>Just for the sake of trying, this is how I'd implement this. Maybe it will help you.<\/p>\n\n<pre><code>df['RUL'] = df.loc[:, ['engine', 'cycle']].groupby('engine').transform('max')\ndf['RUL'] = df['RUL'] - df['cycle']\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53891907",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1545439231323,
        "Question_original_content":"panda datafram appli figur result valu neg pictur data column rul far right name got cut turbo engin degrad dataset nasa http data nasa gov widget vrk gjie studio code snippet helper function engin cycl unit test expect comput cycl engin exampl engin max cycl dataset fail final helper function engin remainig life take engin cycl argument return max cycl current cycl engin unit test expect result reason isn work run notebook column rul return sequenc decreas posit integ exampl engin give neg valu figur know problem like piec code rul engin cycl appli lambda engin remain life axi def engin cycl engin return int loc engin cycl max def engin remain life engin cycl return engin cycl engin int cycl rul engin cycl appli lambda engin remain life axi return",
        "Question_preprocessed_content":"panda datafram appli figur result valu neg pictur data column rul far right name got cut studio code snippet helper function final helper function take engin cycl argument return max cycl current cycl engin reason isn work run notebook column rul return sequenc decreas posit integ exampl engin give neg valu figur know problem like piec code",
        "Question_gpt_summary_original":"The user is encountering a challenge with their code that involves pandas dataframe and apply. They are trying to compute the remaining life of an engine using two helper functions, but the resulting values are negative instead of positive. The user suspects that the problem is with the code that applies the helper functions to the dataframe.",
        "Question_gpt_summary":"user encount challeng code involv panda datafram appli try comput remain life engin helper function result valu neg instead posit user suspect problem code appli helper function datafram",
        "Answer_original_content":"sake try implement mayb help rul loc engin cycl groupbi engin transform max rul rul cycl",
        "Answer_preprocessed_content":"sake try implement mayb help",
        "Answer_gpt_summary_original":"possible solution: the answer suggests implementing a code snippet that involves using the groupby function to transform the dataframe and subtracting one column from another. this may help resolve the issue of negative values in the pandas dataframe.",
        "Answer_gpt_summary":"possibl solut answer suggest implement code snippet involv groupbi function transform datafram subtract column help resolv issu neg valu panda datafram"
    },
    {
        "Question_id":null,
        "Question_title":"RunHistory finalization failed: ServiceException: Code: 400",
        "Question_body":"I am trying to log my model metrics and hyperparam space in the run metrics.\nI tried to also reduce the variable lengths throughout but still get consistently the following error:\ni find in the documentation that we can only have 15 columns for every row in the run metric.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-limits-quotas-capacity . But is there a way to increase this capacity? Thanks\n\nIs there a way to increase this capacity?\n\n RunHistory finalization failed: ServiceException:\n  Code: 400\n  Message: (UserError) A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  Details:\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Value.Data\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Value.Data\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Value.Data\/Count, Limit=15, Size=16. See",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643880887487,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@AntaraDas-4298 Some of the limits are soft limits which can be increased for a subscription or a workspace. Usually these limits can be increased by using a support case with appropriate usage scenario mentioned in the details of the case. Once the case is submitted it is reviewed by the service team and the limits are increased if it is possible to do so.\n\nPlease create a support case from Azure portal and use the following settings from the drop downs and mention the summary detail as \"Increase limit of columns per metric row\"\n\nIf you do not have a valid support subscription we could help you with a one time free support case that could help you to create one for this scenario.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/721282\/runhistory-finalization-failed-serviceexception-co.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-03T15:44:22.357Z",
                "Answer_score":0,
                "Answer_body":"@AntaraDas-4298 Some of the limits are soft limits which can be increased for a subscription or a workspace. Usually these limits can be increased by using a support case with appropriate usage scenario mentioned in the details of the case. Once the case is submitted it is reviewed by the service team and the limits are increased if it is possible to do so.\n\nPlease create a support case from Azure portal and use the following settings from the drop downs and mention the summary detail as \"Increase limit of columns per metric row\"\n\nIf you do not have a valid support subscription we could help you with a one time free support case that could help you to create one for this scenario.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643903062356,
        "Question_original_content":"runhistori final fail serviceexcept code try log model metric hyperparam space run metric tri reduc variabl length consist follow error document column row run metric http doc microsoft com azur machin learn resourc limit quota capac wai increas capac thank wai increas capac runhistori final fail serviceexcept code messag usererror field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document detail field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvvalu data count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvvalu data count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvvalu data count limit size",
        "Question_preprocessed_content":"runhistori final fail serviceexcept code try log model metric hyperparam space run metric tri reduc variabl length consist follow error document column row run metric wai increas capac thank wai increas capac runhistori final fail serviceexcept code messag field entiti size limit limit size servic limit document detail field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size",
        "Question_gpt_summary_original":"The user is encountering an error message \"RunHistory finalization failed: ServiceException: Code: 400\" while trying to log model metrics and hyperparameter space in the run metrics. The error message indicates that the user has exceeded the limit of 15 columns for every row in the run metric. The user is seeking a way to increase this capacity.",
        "Question_gpt_summary":"user encount error messag runhistori final fail serviceexcept code try log model metric hyperparamet space run metric error messag indic user exceed limit column row run metric user seek wai increas capac",
        "Answer_original_content":"antarada limit soft limit increas subscript workspac usual limit increas support case appropri usag scenario mention detail case case submit review servic team limit increas possibl creat support case azur portal us follow set drop down mention summari increas limit column metric row valid support subscript help time free support case help creat scenario answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"limit soft limit increas subscript workspac usual limit increas support case appropri usag scenario mention detail case case submit review servic team limit increas possibl creat support case azur portal us follow set drop down mention summari increas limit column metric row valid support subscript help time free support case help creat scenario answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"possible solutions to the challenge of exceeding the 15 column limit for every row in the run metric are to increase the soft limits for a subscription or a workspace, which can be done by creating a support case from the azure portal and mentioning the summary detail as \"increase limit of columns per metric row\". if the user does not have a valid support subscription, they can get a one-time free support case to create one for this scenario. upvoting helpful answers can also benefit other community members.",
        "Answer_gpt_summary":"possibl solut challeng exceed column limit row run metric increas soft limit subscript workspac creat support case azur portal mention summari increas limit column metric row user valid support subscript time free support case creat scenario upvot help answer benefit commun member"
    },
    {
        "Question_id":null,
        "Question_title":"Logging a table from a list of python dicts",
        "Question_body":"<p>Is it possible to log tables to WANDB from a list \/sequence of dicts where the keys are the column names and the values wandb.Images (for example)?<\/p>\n<p>Once the my tables are logged to WANDB, if they share a column, can I join them in the WANDB web GUI?<\/p>\n<p>Thanks  beforehand!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1657037898222,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":116.0,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/fisikillo\">@fisikillo<\/a> ,<\/p>\n<p>Thanks for writing in. We support logging tables using list\/nested listed  with multiple images, see <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\">here.<\/a> You can also join tables directly from the web GUI.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-a-table-from-a-list-of-python-dicts\/2701",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-07T19:38:05.552Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/fisikillo\">@fisikillo<\/a> ,<\/p>\n<p>Thanks for writing in. We support logging tables using list\/nested listed  with multiple images, see <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\">here.<\/a> You can also join tables directly from the web GUI.<\/p>",
                "Answer_score":15.6,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-13T18:21:51.238Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/fisikillo\">@fisikillo<\/a> , following up on this. Do you still need assistance when logging to workspace?<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-13T18:44:39.826Z",
                "Answer_body":"<p>Hi Mohammad,<\/p>\n<p>No, it\u2019s cristal clear, sorry for the delay ;).<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-11T18:45:04.344Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1657222685552,
        "Question_original_content":"log tabl list python dict possibl log tabl list sequenc dict kei column name valu imag exampl tabl log share column join web gui thank",
        "Question_preprocessed_content":"log tabl list python dict possibl log tabl list dict kei column name valu imag tabl log share column join web gui thank",
        "Question_gpt_summary_original":"The user is seeking to log tables to WANDB from a list of python dicts where the keys are the column names and the values are wandb.Images. They also want to know if they can join the tables in the WANDB web GUI if they share a column.",
        "Question_gpt_summary":"user seek log tabl list python dict kei column name valu imag want know join tabl web gui share column",
        "Answer_original_content":"fisikillo thank write support log tabl list nest list multipl imag join tabl directli web gui",
        "Answer_preprocessed_content":"thank write support log tabl list multipl imag join tabl directli web gui",
        "Answer_gpt_summary_original":"possible solutions: \n- the user can log tables from a list of python dictionaries using list\/nested lists with multiple images.\n- the user can join tables directly from the web gui if they share a column.",
        "Answer_gpt_summary":"possibl solut user log tabl list python dictionari list nest list multipl imag user join tabl directli web gui share column"
    },
    {
        "Question_id":null,
        "Question_title":"How to reenable automatic synchronisation",
        "Question_body":"<p>I made a change to my script and now I have to manually synchronise my runs, my script contains<\/p>\n<pre><code>if args.dry_run:\n    os.environ['WANDB_MODE'] = 'dryrun'\n\nwandb.init(project=args.project_name, notes=args.notes)\n\n# log all experimental args to wandb\nwandb.config.update(args)\n<\/code><\/pre>\n<p>The change I made was the first line, setting <code>WANDB_MODE=dryrun<\/code>. From that point on I cannot re-enable automatic synchronisation.<\/p>\n<p>I\u2019ve run <code>wandb online<\/code> and run my script with <code>dryrun=False<\/code>. I also realised that this doesn\u2019t unset WANDB_MODE so I tried setting it to \u2018online\u2019 when <code>dryrun==False<\/code>. But it always ends up logging to <code>wandb\/offline-run-*<\/code> and I have to manually sync it.<\/p>\n<p>Is there another step to re-enable sync\u2019ing?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1662254971968,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":130.0,
        "Answer_body":"<p>I\u2019ve found a way around this - I\u2019m not really sure why it\u2019s happening but I noticed that the huggingface trainer logs the metrics at the end of training as follows:<\/p>\n<pre><code>                if not args.load_best_model_at_end\n                else {\n                    f\"eval\/{args.metric_for_best_model}\": state.best_metric,\n                    \"train\/total_floss\": state.total_flos,\n                }\n<\/code><\/pre>\n<p>Meaning it logs the validation loss, but only if you train with <code>load_best_model_at_end=True<\/code> and set <code>save_strategy==evaluation_strategy<\/code> (epoch or steps) and <code>save_steps=eval_steps<\/code>.<\/p>\n<p>Doing this means I didn\u2019t need to perform the separate eval step since it\u2019s already logged the evaluation loss from the best model during training.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-reenable-automatic-synchronisation\/3061",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-05T04:26:38.381Z",
                "Answer_body":"<p>I fixed the original problem, I had a bug in my argument parser that meant that dry_run was always true.<\/p>\n<p>However I still cannot get all my metrics synchronised. I\u2019m using the Huggingface trainer, the trainer first trains the models and logs the training metrics, then performed a separate evaluation step.<\/p>\n<p>When I look at the local <code>wandb-summary.json<\/code> it contains all the metrics, including <code>eval\/loss<\/code> but none of the evaluation metrics are available on the cloud.<\/p>\n<p>Also if I look at <code>files\/output.log<\/code> it\u2019s different to the cloud version - it\u2019s complete and the cloud version is truncated part way through the evaluation.<\/p>\n<p>But if I run <code>wandb sync --sync-all<\/code> it says \u201cnothing to sync\u201d, despite there being a clear difference - it seems to think the run has already ended too early. Perhaps there\u2019s a bug in the Huggingface integration that causes it to mark the run complete after training but before evaluation?<\/p>\n<p>For example this is the log on the cloud<\/p>\n<pre><code class=\"lang-auto\">09\/05\/2022 14:01:29 - INFO - __main__ -   *** Evaluate ***\n***** Running Evaluation *****\n  Num examples = 3820356\n  Batch size = 1024\n\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 3665\/3731 [11:10&lt;00:13,  4.92it\/s]\n<\/code><\/pre>\n<p>Compared to the local version<\/p>\n<pre><code class=\"lang-auto\">100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 3728\/3731 [11:23&lt;00:00,  5.47it\/s]\n***** eval metrics *****\n  epoch                   =        1.0\n  eval_loss               =     4.4664\n  eval_runtime            = 0:11:23.99\n  eval_samples            =    3820356\n  eval_samples_per_second =   5585.376\n  eval_steps_per_second   =      5.455\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3731\/3731 [11:23&lt;00:00,  5.46it\/s]\n09\/05\/2022 14:12:54 - INFO - __main__ -   *** Train Finished ***\n<\/code><\/pre>\n<p>Yet<\/p>\n<pre><code class=\"lang-auto\">$ wandb sync --sync-all\nwandb: ERROR Nothing to sync.\n<\/code><\/pre>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-05T05:21:29.729Z",
                "Answer_body":"<p>I\u2019ve found a way around this - I\u2019m not really sure why it\u2019s happening but I noticed that the huggingface trainer logs the metrics at the end of training as follows:<\/p>\n<pre><code>                if not args.load_best_model_at_end\n                else {\n                    f\"eval\/{args.metric_for_best_model}\": state.best_metric,\n                    \"train\/total_floss\": state.total_flos,\n                }\n<\/code><\/pre>\n<p>Meaning it logs the validation loss, but only if you train with <code>load_best_model_at_end=True<\/code> and set <code>save_strategy==evaluation_strategy<\/code> (epoch or steps) and <code>save_steps=eval_steps<\/code>.<\/p>\n<p>Doing this means I didn\u2019t need to perform the separate eval step since it\u2019s already logged the evaluation loss from the best model during training.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-09-09T08:57:04.890Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/david-waterworth\">@david-waterworth<\/a> thank you for reporting this. Has this been now resolved for you with the arguments you mentioned at your last message? Regarding the initial post, it seems that the <code>dryrun<\/code> mode was due to your argument parser. Does this mean you are now running in <code>online<\/code> mode? that would explain why the syncing with <code>--sync-all<\/code> would output the message <code>nothing to sync<\/code>.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-09T23:27:01.416Z",
                "Answer_body":"<p>Hi, yes it\u2019s working now thanks.<\/p>",
                "Answer_score":10.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-12T08:49:27.147Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/david-waterworth\">@david-waterworth<\/a> thank you for confirming this, and glad the issue is now resolved for you! I will close the ticket for now, but please feel free to re-open the ticket by posting here any further questions related to this issue and we will be happy to keep investigating.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-11T08:49:36.762Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662355289728,
        "Question_original_content":"reenabl automat synchronis chang script manual synchronis run script contain arg dry run environ mode dryrun init project arg project note arg note log experiment arg config updat arg chang line set mode dryrun point enabl automat synchronis iv run onlin run script dryrun fals realis doesnt unset mode tri set onlin dryrun fals end log offlin run manual sync step enabl sync",
        "Question_preprocessed_content":"reenabl automat synchronis chang script manual synchronis run script contain chang line set point automat synchronis iv run run script realis doesnt unset tri set onlin end log manual sync step sync",
        "Question_gpt_summary_original":"The user made a change to their script by setting WANDB_MODE to 'dryrun', which disabled automatic synchronization. They have tried running the script with 'dryrun=False' and setting WANDB_MODE to 'online' when 'dryrun==False', but the script still logs to 'wandb\/offline-run-*' and requires manual synchronization. The user is seeking advice on how to re-enable automatic synchronization.",
        "Question_gpt_summary":"user chang script set mode dryrun disabl automat synchron tri run script dryrun fals set mode onlin dryrun fals script log offlin run requir manual synchron user seek advic enabl automat synchron",
        "Answer_original_content":"iv wai sure happen notic huggingfac trainer log metric end train follow arg load best model end eval arg metric best model state best metric train total floss state total flo mean log valid loss train load best model end true set save strategi evalu strategi epoch step save step eval step mean didnt need perform separ eval step log evalu loss best model train",
        "Answer_preprocessed_content":"iv wai sure happen notic huggingfac trainer log metric end train follow mean log valid loss train set mean didnt need perform separ eval step log evalu loss best model train",
        "Answer_gpt_summary_original":"the answer suggests a workaround for the issue of re-enabling automatic synchronization after making a change to the script. the solution involves logging the validation loss during training using the huggingface trainer and setting certain parameters such as load_best_model_at_end and save_strategy to true. this eliminates the need for a separate evaluation step.",
        "Answer_gpt_summary":"answer suggest workaround issu enabl automat synchron make chang script solut involv log valid loss train huggingfac trainer set certain paramet load best model end save strategi true elimin need separ evalu step"
    },
    {
        "Question_id":null,
        "Question_title":"403: AuthorizationPermissionMismatch",
        "Question_body":"Hi,\n\nI have my source data in a storage account and I am trying to access it from my Machine learning workspace. I have assigned system assigned managed identity to my compute cluster and I have also added \"Storage Blob Contributor\/Reader access to my machine learning workspace in the storage account IAM.\n\nBut still I am getting the below error\n\n03: AuthorizationPermissionMismatch'. Please make sure the compute or login identity has 'Storage Blob Data Reader' or 'Storage Blob Data Owner' role in the storage IAM.\nThis request is not authorized to perform this operation using this permission\n\n\n\n\n\n\n\n\nPlease help me to sort out this issue",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1657807579977,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/926988\/403-authorizationpermissionmismatch.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-19T05:48:40.483Z",
                "Answer_score":1,
                "Answer_body":"The issue is resolved. The compute managed identity is listed under service principal of IAM Access.. I selected that and I gave Storage Blob Contributor access. It then worked!! Thanks",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-19T07:15:24.157Z",
                "Answer_score":0,
                "Answer_body":"Hello @mumu-3266\n\nThanks for kindly sharing the solution to us and we are glad to know the issue has been resolved.\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"authorizationpermissionmismatch sourc data storag account try access machin learn workspac assign assign manag ident comput cluster ad storag blob contributor reader access machin learn workspac storag account iam get error authorizationpermissionmismatch sure comput login ident storag blob data reader storag blob data owner role storag iam request author perform oper permiss help sort issu",
        "Question_preprocessed_content":"authorizationpermissionmismatch sourc data storag account try access machin learn workspac assign assign manag ident comput cluster ad storag blob access machin learn workspac storag account iam get error authorizationpermissionmismatch sure comput login ident storag blob data reader storag blob data owner role storag iam request author perform oper permiss help sort issu",
        "Question_gpt_summary_original":"The user is encountering an error message \"403: AuthorizationPermissionMismatch\" while trying to access source data from a storage account in their Machine Learning workspace. They have assigned a system assigned managed identity to their compute cluster and added \"Storage Blob Contributor\/Reader\" access to their workspace in the storage account IAM. However, they are still unable to access the data and are being prompted to ensure that the compute or login identity has 'Storage Blob Data Reader' or 'Storage Blob Data Owner' role in the storage IAM.",
        "Question_gpt_summary":"user encount error messag authorizationpermissionmismatch try access sourc data storag account machin learn workspac assign assign manag ident comput cluster ad storag blob contributor reader access workspac storag account iam unabl access data prompt ensur comput login ident storag blob data reader storag blob data owner role storag iam",
        "Answer_original_content":"issu resolv comput manag ident list servic princip iam access select gave storag blob contributor access work thank hello mumu thank kindli share solut glad know issu resolv regard yutong",
        "Answer_preprocessed_content":"issu resolv comput manag ident list servic princip iam select gave storag blob contributor access work thank hello thank kindli share solut glad know issu resolv regard yutong",
        "Answer_gpt_summary_original":"the solution to the 403 error is to ensure that the compute managed identity is listed under the service principal of iam access and given storage blob contributor access. this resolved the issue for the user who asked the question.",
        "Answer_gpt_summary":"solut error ensur comput manag ident list servic princip iam access given storag blob contributor access resolv issu user ask question"
    },
    {
        "Question_id":62422682.0,
        "Question_title":"sagemaker notebook instance Elastic Inference tensorflow model local deployment",
        "Question_body":"<p>I am trying to replicate <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb<\/a><\/p>\n\n<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:<\/p>\n\n<pre><code>%%time\nimport boto3\n\nregion = boto3.Session().region_name\nsaved_model = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/model\/resnet\/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)\n\nimport sagemaker\nfrom sagemaker.tensorflow.serving import Model\n\nrole = sagemaker.get_execution_role()\n\ntensorflow_model = Model(model_data=saved_model,\nrole=role,\nframework_version='1.14')\ntf_predictor = tensorflow_model.deploy(initial_instance_count=1,\ninstance_type='local',\naccelerator_type='local_sagemaker_notebook')\n<\/code><\/pre>\n\n<p>I am getting following log in the notebook:<\/p>\n\n<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1\nalgo-1-7ynb1_1 | INFO:main:starting services\nalgo-1-7ynb1_1 | INFO:main:using default model name: Servo\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving model config:\nalgo-1-7ynb1_1 | model_config_list: {\nalgo-1-7ynb1_1 | config: {\nalgo-1-7ynb1_1 | name: \"Servo\",\nalgo-1-7ynb1_1 | base_path: \"\/opt\/ml\/model\/export\/Servo\",\nalgo-1-7ynb1_1 | model_platform: \"tensorflow\"\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:nginx config:\nalgo-1-7ynb1_1 | load_module modules\/ngx_http_js_module.so;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_processes auto;\nalgo-1-7ynb1_1 | daemon off;\nalgo-1-7ynb1_1 | pid \/tmp\/nginx.pid;\nalgo-1-7ynb1_1 | error_log \/dev\/stderr error;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_rlimit_nofile 4096;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | events {\nalgo-1-7ynb1_1 | worker_connections 2048;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | http {\nalgo-1-7ynb1_1 | include \/etc\/nginx\/mime.types;\nalgo-1-7ynb1_1 | default_type application\/json;\nalgo-1-7ynb1_1 | access_log \/dev\/stdout combined;\nalgo-1-7ynb1_1 | js_include tensorflow-serving.js;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream tfs_upstream {\nalgo-1-7ynb1_1 | server localhost:8501;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream gunicorn_upstream {\nalgo-1-7ynb1_1 | server unix:\/tmp\/gunicorn.sock fail_timeout=1;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | server {\nalgo-1-7ynb1_1 | listen 8080 deferred;\nalgo-1-7ynb1_1 | client_max_body_size 0;\nalgo-1-7ynb1_1 | client_body_buffer_size 100m;\nalgo-1-7ynb1_1 | subrequest_output_buffer_size 100m;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | set $tfs_version 1.14;\nalgo-1-7ynb1_1 | set $default_tfs_model Servo;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/tfs {\nalgo-1-7ynb1_1 | rewrite ^\/tfs\/(.) \/$1 break;\nalgo-1-7ynb1_1 | proxy_redirect off;\nalgo-1-7ynb1_1 | proxy_pass_request_headers off;\nalgo-1-7ynb1_1 | proxy_set_header Content-Type 'application\/json';\nalgo-1-7ynb1_1 | proxy_set_header Accept 'application\/json';\nalgo-1-7ynb1_1 | proxy_pass http:\/\/tfs_upstream;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ping {\nalgo-1-7ynb1_1 | js_content ping;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/invocations {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location ~ ^\/models\/(.)\/invoke {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/models {\nalgo-1-7ynb1_1 | proxy_pass http:\/\/gunicorn_upstream\/models;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ {\nalgo-1-7ynb1_1 | return 404 '{\"error\": \"Not Found\"}';\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | keepalive_timeout 3;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)\nalgo-1-7ynb1_1 | INFO:main:nginx version info:\nalgo-1-7ynb1_1 | nginx version: nginx\/1.16.1\nalgo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)\nalgo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016\nalgo-1-7ynb1_1 | TLS SNI support enabled\nalgo-1-7ynb1_1 | configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\nalgo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\nalgo-1-7ynb1_1 |\n!algo-1-7ynb1_1 | 172.18.0.1 - - [17\/Jun\/2020:05:02:10 +0000] \"GET \/ping HTTP\/1.1\" 200 3 \"-\" \"-\"\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-7ynb1_1 | EI Error Code: [3, 16, 8]\nalgo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | EI Client Version: 1.5.3\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\n`\n<\/code><\/pre>\n\n<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.<\/p>\n\n<p>I can see the docker of the model running\n<a href=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I try to infer\/predict from that model, I get error:<\/p>\n\n<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]\n\nalgo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-iikpj_1 | EI Error Code: [3, 16, 8]\nalgo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-iikpj_1 | EI Client Version: 1.5.3\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-iikpj_1 | INFO:main:tensorflow version info:\nalgo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-iikpj_1 | TensorFlow Library: 1.14.0\nalgo-1-iikpj_1 | EI Version: EI-1.4\nalgo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 172.18.0.1 - - [17\/Jun\/2020:05:29:47 +0000] \"POST \/invocations HTTP\/1.1\" 502 157 \"-\" \"-\"\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nJSONDecodeError Traceback (most recent call last)\nin ()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/serving.py in predict(self, data, initial_args)\n116 args[\"CustomAttributes\"] = self._model_attributes\n117\n--&gt; 118 return super(Predictor, self).predict(data, args)\n119\n120\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model)\n109 request_args = self._create_request_args(data, initial_args, target_model)\n110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n--&gt; 111 return self._handle_response(response)\n112\n113 def _handle_response(self, response):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in _handle_response(self, response)\n119 if self.deserializer is not None:\n120 # It's the deserializer's responsibility to close the stream\n--&gt; 121 return self.deserializer(response_body, response[\"ContentType\"])\n122 data = response_body.read()\n123 response_body.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in call(self, stream, content_type)\n578 \"\"\"\n579 try:\n--&gt; 580 return json.load(codecs.getreader(\"utf-8\")(stream))\n581 finally:\n582 stream.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n297 cls=cls, object_hook=object_hook,\n298 parse_float=parse_float, parse_int=parse_int,\n--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n300\n301\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 \"\"\"\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\n<\/code><\/pre>\n\n<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.\nI also tried Rest API post to docker deployed model:<\/p>\n\n<pre><code>curl -v -X POST \\ -H 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/Servo:predict\nbut still getting error:\n[![enter image description here][1]][1]\n<\/code><\/pre>\n\n<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.<\/p>\n\n<p>Please help me out. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592375129670,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":350.0,
        "Answer_body":"<p>Solved it. The error I was getting is due to roles\/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":-1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62422682",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592876274340,
        "Question_original_content":"notebook instanc elast infer tensorflow model local deploy try replic http github com awslab amazon exampl blob master python sdk tensorflow serv elast infer model tensorflow serv pretrain model elast infer ipynb elast infer acceler attach notebook instanc conda amazonei tensorflow kernel accord document chang local time import boto region boto session region save model sampl data tensorflow model resnet resnet nchw tar format region import tensorflow serv import model role execut role tensorflow model model model data save model role role framework version predictor tensorflow model deploi initi instanc count instanc type local acceler type local notebook get follow log notebook attach tmpuqysel algo ynb algo ynb info main start servic algo ynb info main default model servo algo ynb info main tensorflow serv model config algo ynb model config list algo ynb config algo ynb servo algo ynb base path opt model export servo algo ynb model platform tensorflow algo ynb algo ynb algo ynb algo ynb algo ynb info main nginx config algo ynb load modul modul ngx http modul algo ynb algo ynb worker process auto algo ynb daemon algo ynb pid tmp nginx pid algo ynb error log dev stderr error algo ynb algo ynb worker rlimit nofil algo ynb algo ynb event algo ynb worker connect algo ynb algo ynb algo ynb http algo ynb includ nginx mime type algo ynb default type applic json algo ynb access log dev stdout combin algo ynb includ tensorflow serv algo ynb algo ynb upstream tf upstream algo ynb server localhost algo ynb algo ynb algo ynb upstream gunicorn upstream algo ynb server unix tmp gunicorn sock fail timeout algo ynb algo ynb algo ynb server algo ynb listen defer algo ynb client max bodi size algo ynb client bodi buffer size algo ynb subrequest output buffer size algo ynb algo ynb set tf version algo ynb set default tf model servo algo ynb algo ynb locat tf algo ynb rewrit tf break algo ynb proxi redirect algo ynb proxi pass request header algo ynb proxi set header content type applic json algo ynb proxi set header accept applic json algo ynb proxi pass http tf upstream algo ynb algo ynb algo ynb locat ping algo ynb content ping algo ynb algo ynb algo ynb locat invoc algo ynb content invoc algo ynb algo ynb algo ynb locat model invok algo ynb content invoc algo ynb algo ynb algo ynb locat model algo ynb proxi pass http gunicorn upstream model algo ynb algo ynb algo ynb locat algo ynb return error algo ynb algo ynb algo ynb keepal timeout algo ynb algo ynb algo ynb algo ynb algo ynb info main tensorflow version info algo ynb tensorflow modelserv dev sha algo ynb tensorflow librari algo ynb version algo ynb info main tensorflow serv command tensorflow model server port rest api port model config file model config cfg algo ynb info main start tensorflow serv pid algo ynb info main nginx version info algo ynb nginx version nginx algo ynb built gcc ubuntu ubuntu algo ynb built openssl mar algo ynb tl sni support enabl algo ynb configur argument prefix nginx sbin path usr sbin nginx modul path usr lib nginx modul conf path nginx nginx conf error log path var log nginx error log http log path var log nginx access log pid path var run nginx pid lock path var run nginx lock http client bodi temp path var cach nginx client temp http proxi temp path var cach nginx proxi temp http fastcgi temp path var cach nginx fastcgi temp http uwsgi temp path var cach nginx uwsgi temp http scgi temp path var cach nginx scgi temp user nginx group nginx compat file aio thread http addit modul http auth request modul http dav modul http flv modul http gunzip modul http gzip static modul http modul http random index modul http realip modul http secur link modul http slice modul http ssl modul http stub statu modul http sub modul http modul mail mail ssl modul stream stream realip modul stream ssl modul stream ssl preread modul opt fpie fstack protector strong wformat werror format secur fortifi sourc fpic opt bsymbol function fpie pie relro need pie algo ynb info main start nginx pid algo ynb tensorflow serv model server server core ad updat model algo ynb tensorflow serv model server server core ad model servo algo ynb tensorflow serv core basic manag successfulli reserv resourc load servabl servo version algo ynb tensorflow serv core loader har approv load servabl version servo version algo ynb tensorflow serv core loader har load servabl version servo version algo ynb extern org tensorflow tensorflow contrib session bundl bundl shim attempt load nativ savedmodelbundl bundl shim opt model export servo algo ynb extern org tensorflow tensorflow save model reader read savedmodel opt model export servo algo ynb extern org tensorflow tensorflow save model reader read meta graph tag serv algo ynb extern org tensorflow tensorflow save model loader restor savedmodel bundl algo ynb extern org tensorflow tensorflow save model loader run initi savedmodel bundl path opt model export servo algo ynb amazon elast infer client librari version algo ynb number elast infer acceler avail algo ynb elast infer acceler eia fcebcefb algo ynb elast infer acceler type eia medium algo ynb elast infer acceler ordin algo ynb algo ynb jun ping http algo ynb wed jun execut engin error get applic context tensorflow algo ynb wed jun execut engin tensorflow fail error algo ynb error code algo ynb error descript unabl authent acceler algo ynb request ac acceler eia fcebcefb algo ynb client version algo ynb extern org tensorflow tensorflow contrib session eia session non statu swapexstatewithei tmp input tmp output tmp freez statu intern fail initi oper whitelist server algo ynb warn main unexpect tensorflow serv exit statu restart algo ynb info main tensorflow version info algo ynb tensorflow modelserv dev sha algo ynb tensorflow librari algo ynb version algo ynb info main tensorflow serv command tensorflow model server port rest api port model config file model config cfg algo ynb info main start tensorflow serv pid enter code algo ynb tensorflow serv model server server core ad updat model algo ynb tensorflow serv model server server core ad model servo algo ynb tensorflow serv core basic manag successfulli reserv resourc load servabl servo version algo ynb tensorflow serv core loader har approv load servabl version servo version algo ynb tensorflow serv core loader har load servabl version servo version algo ynb extern org tensorflow tensorflow contrib session bundl bundl shim attempt load nativ savedmodelbundl bundl shim opt model export servo algo ynb extern org tensorflow tensorflow save model reader read savedmodel opt model export servo algo ynb extern org tensorflow tensorflow save model reader read meta graph tag serv algo ynb extern org tensorflow tensorflow save model loader restor savedmodel bundl algo ynb extern org tensorflow tensorflow save model loader run initi savedmodel bundl path opt model export servo algo ynb amazon elast infer client librari version algo ynb number elast infer acceler avail algo ynb elast infer acceler eia fcebcefb algo ynb elast infer acceler type eia medium algo ynb elast infer acceler ordin log stop notebook keep throw notebook cell sure model deploi correctli docker model run try infer predict model error algo iikpj wed jun execut engin error get applic context tensorflow algo iikpj wed jun execut engin tensorflow fail error algo iikpj error code algo iikpj error descript unabl authent acceler algo iikpj request adecdef adfdcdf acceler eia fcebcefb algo iikpj client version algo iikpj extern org tensorflow tensorflow contrib session eia session non statu swapexstatewithei tmp input tmp output tmp freez statu intern fail initi oper whitelist server algo iikpj warn main unexpect tensorflow serv exit statu restart algo iikpj info main tensorflow version info algo iikpj tensorflow modelserv dev sha algo iikpj tensorflow librari algo iikpj version algo iikpj info main tensorflow serv command tensorflow model server port rest api port model config file model config cfg algo iikpj info main start tensorflow serv pid algo iikpj tensorflow serv model server server core ad updat model algo iikpj tensorflow serv model server server core ad model servo algo iikpj error connect fail connect refus connect upstream client server request post invoc http subrequest model servo predict upstream http model servo predict host localhost algo iikpj error connect fail connect refus connect upstream client server request post invoc http subrequest model servo predict upstream http model servo predict host localhost algo iikpj jun post invoc http algo iikpj tensorflow serv core basic manag successfulli reserv resourc load servabl servo version algo iikpj tensorflow serv core loader har approv load servabl version servo version algo iikpj tensorflow serv core loader har load servabl version servo version algo iikpj extern org tensorflow tensorflow contrib session bundl bundl shim attempt load nativ savedmodelbundl bundl shim opt model export servo algo iikpj extern org tensorflow tensorflow save model reader read savedmodel opt model export servo algo iikpj extern org tensorflow tensorflow save model reader read meta graph tag serv jsondecodeerror traceback recent anaconda env amazonei tensorflow lib python site packag tensorflow serv predict self data initi arg arg customattribut self model attribut return super predictor self predict data arg anaconda env amazonei tensorflow lib python site packag predictor predict self data initi arg target model request arg self creat request arg data initi arg target model respons self session runtim client invok endpoint request arg return self handl respons respons def handl respons self respons anaconda env amazonei tensorflow lib python site packag predictor handl respons self respons self deseri deseri respons close stream return self deseri respons bodi respons contenttyp data respons bodi read respons bodi close anaconda env amazonei tensorflow lib python site packag predictor self stream content type try return json load codec getread utf stream final stream close anaconda env amazonei tensorflow lib python json init load cl object hook pars float pars int pars constant object pair hook cl cl object hook object hook pars float pars float pars int pars int pars constant pars constant object pair hook object pair hook anaconda env amazonei tensorflow lib python json init load encod cl object hook pars float pars int pars constant object pair hook pars int pars float pars constant object pair hook return default decod decod cl cl jsondecod anaconda env amazonei tensorflow lib python json decod decod self obj end self raw decod idx end end end end end len anaconda env amazonei tensorflow lib python json decod raw decod self idx obj end self scan idx stopiter err rais jsondecodeerror expect valu err valu return obj end jsondecodeerror expect valu line column char algo iikpj extern org tensorflow tensorflow save model loader restor savedmodel bundl algo iikpj extern org tensorflow tensorflow save model loader run initi savedmodel bundl path opt model export servo algo iikpj amazon elast infer client librari version tri wai solv jsondecodeerror expect valu line column char json load json dump help tri rest api post docker deploi model curl post content type applic json data input http model servo predict get error enter imag descript help resolv issu initi try us tensorflow serv model get error thought follow model aw exampl notebook resnet nchw tar experi aw exampl notebook model provid sampl data help thank",
        "Question_preprocessed_content":"notebook instanc elast infer tensorflow model local deploy try replic elast infer acceler attach notebook instanc kernel accord document chang local get follow log notebook log stop notebook keep throw notebook cell sure model deploi correctli docker model run try model error tri wai solv jsondecodeerror expect valu line column help tri rest api post docker deploi model help resolv issu initi try us tensorflow serv model get error thought follow model aw exampl notebook experi aw exampl notebook model provid help thank",
        "Question_gpt_summary_original":"the user is encountering challenges with deploying a tensorflow model with elastic inference on a notebook instance, resulting in errors such as \"jsondecodeerror: expecting value: line 1 column 1 (char 0)\" and \"curl -v -x post \\ -h 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/servo",
        "Question_gpt_summary":"user encount challeng deploi tensorflow model elast infer notebook instanc result error jsondecodeerror expect valu line column char curl post content type applic json data input http model servo",
        "Answer_original_content":"solv error get role permiss elast infer attach notebook fix permiss devop team work expect http github com aw tensorflow serv contain issu",
        "Answer_preprocessed_content":"solv error get elast infer attach notebook fix permiss devop team work expect",
        "Answer_gpt_summary_original":"the solution to the error encountered while deploying a tensorflow model with elastic inference on a notebook instance is to fix the roles\/permissions of elastic inference attached to the notebook. the user's devops team fixed these permissions, and the deployment worked as expected.",
        "Answer_gpt_summary":"solut error encount deploi tensorflow model elast infer notebook instanc fix role permiss elast infer attach notebook user devop team fix permiss deploy work expect"
    },
    {
        "Question_id":54799512.0,
        "Question_title":"Retrain the classification model automatically based on updated data set",
        "Question_body":"<p>We have created an experiment in Azure ML Studio to predict some scheduling activities based on the system data and user data. System data consists of the CPU time, Heap Usage and other system parameters while user data has active sessions of the user and some user-specific data.\nOur experiment is working fine and returning the results quite similar to what we are expecting, but we are struggling with the following:-<\/p>\n\n<p>1) Our experiment is not considering the updated data for training its models.<\/p>\n\n<p>2) Every time we are required to upload the data and retrain the models manually.<\/p>\n\n<p>I wonder if it is really possible to feed in live data to the azure experiments using some web-services or by using Azure DB. We are trying to update the data in CSV file that we have created in Azure storage. That probably would solve our 1st query.<\/p>\n\n<p>Now, this updated data should be considered to train the model periodically automatically.<\/p>\n\n<p>It would be great if someone could help us out with it?<\/p>\n\n<p>Note: We are using our model using the web services created with the help of Azure studio.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1550724960020,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1550942353507,
        "Question_score":0.0,
        "Question_view_count":772.0,
        "Answer_body":"<p>Step 1 : Create 2 web services with Azure ML Studio ( One for the training model and one for the predictive model)<\/p>\n\n<p>Step 2: Create endpoint through the web service with the link Manage Endpoint on Azure ML Studio for each web service<\/p>\n\n<p>Step 3: Create 2 new connections on Azure Data Factory \/ Find Azure ML (on compute tab) and copy the Endpoint key and API Key that you will find under the Consume tab in the endpoint configuration (the one that you created on step 2) Endpoint Key = Batch Requests Key and API Key = Primary Key <\/p>\n\n<p>Set Disable Update Resource for the training model endpoint\nSet Enable Update Resource for the predictive model endpoint ( Update Resource End Point = Patch key )<\/p>\n\n<p>Step 4 : Create a pipeline with 2 activities ( ML Batch Execution and ML Update Resource)\nSet the AML Linked service for the ML batch Execution with the connection that has  disable Update Resource<\/p>\n\n<p>Set the AML Linked service for the ML Update Resource with the connection that has  Enable Update Resource<\/p>\n\n<p>Step 5 : Set the Web Service Inputs and Outputs<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54799512",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1551972299452,
        "Question_original_content":"retrain classif model automat base updat data set creat experi studio predict schedul activ base data user data data consist cpu time heap usag paramet user data activ session user user specif data experi work fine return result similar expect struggl follow experi consid updat data train model time requir upload data retrain model manual wonder possibl feed live data azur experi web servic azur try updat data csv file creat azur storag probabl solv queri updat data consid train model period automat great help note model web servic creat help azur studio",
        "Question_preprocessed_content":"retrain classif model automat base updat data set creat experi studio predict schedul activ base data user data data consist cpu time heap usag paramet user data activ session user data experi work fine return result similar expect struggl follow experi consid updat data train model time requir upload data retrain model manual wonder possibl feed live data azur experi azur try updat data csv file creat azur storag probabl solv queri updat data consid train model period automat great help note model web servic creat help azur studio",
        "Question_gpt_summary_original":"The user has created an experiment in Azure ML Studio to predict scheduling activities based on system and user data. However, they are facing challenges as the experiment is not considering updated data for training its models and they have to manually upload data and retrain models every time. The user is wondering if it is possible to feed live data to the Azure experiments using web services or Azure DB and how to train the model periodically automatically.",
        "Question_gpt_summary":"user creat experi studio predict schedul activ base user data face challeng experi consid updat data train model manual upload data retrain model time user wonder possibl feed live data azur experi web servic azur train model period automat",
        "Answer_original_content":"step creat web servic studio train model predict model step creat endpoint web servic link manag endpoint studio web servic step creat new connect azur data factori comput tab copi endpoint kei api kei consum tab endpoint configur creat step endpoint kei batch request kei api kei primari kei set disabl updat resourc train model endpoint set enabl updat resourc predict model endpoint updat resourc end point patch kei step creat pipelin activ batch execut updat resourc set aml link servic batch execut connect disabl updat resourc set aml link servic updat resourc connect enabl updat resourc step set web servic input output",
        "Answer_preprocessed_content":"step creat web servic studio step creat endpoint web servic link manag endpoint studio web servic step creat new connect azur data factori copi endpoint kei api kei consum tab endpoint configur endpoint kei batch request kei api kei primari kei set disabl updat resourc train model endpoint set enabl updat resourc predict model endpoint step creat pipelin activ set aml link servic batch execut connect disabl updat resourc set aml link servic updat resourc connect enabl updat resourc step set web servic input output",
        "Answer_gpt_summary_original":"The answer provides a step-by-step solution to update a data-trained model without requiring manual retraining and new data upload. The solution involves creating two web services with Azure ML Studio, creating endpoints through the web service, creating two new connections on Azure Data Factory, setting disable and enable update resource for the training and predictive model endpoints respectively, creating a pipeline with two activities, and setting the web service inputs and outputs.",
        "Answer_gpt_summary":"answer provid step step solut updat data train model requir manual retrain new data upload solut involv creat web servic studio creat endpoint web servic creat new connect azur data factori set disabl enabl updat resourc train predict model endpoint respect creat pipelin activ set web servic input output"
    }
]