[
    {
        "Question_title":"Accessing an Azure ML Model Registry from another Azure ML Workspace",
        "Question_body":"<p>Suppose I have two Azure ML workspaces:<\/p>\n<ol>\n<li><p>Workspace1 - This is being used by one team (Team1) who only train the model and store the model in model registry of Workspace1<\/p>\n<\/li>\n<li><p>Workspace2 - This is used by another team  (Team2) who containerise the model, push it to ACR and then deploy the containerised model in Azure ML Compute.<\/p>\n<\/li>\n<\/ol>\n<p>Is it possible for Team2 to access the model registry of Workspace1 from their Workspace2 and retrieve the model for containerisation and subsequent deployment? Alternatively, is there any concept of a shared model registry in Azure ML where both the teams can store and access a common model registry? If none of these are possible, then what is the way for Team1 and Team2 to work together on a single model with the given responsibilities as described above?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1638197253147,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":421.0,
        "Poster_created_time":1373471094267,
        "Poster_location":"United Kingdom",
        "Poster_reputation_count":395.0,
        "Poster_view_count":66.0,
        "Answer_body":"<p>As described, I think the best solution is to use one Workspace, not two.  It sounds like you have Team 1 and Team 2 sharing contributions on a single project.  What may work better is to define user roles in the Azure ML workspace, such that Team 2 has permissions to deploy models, and Team 1 has permission to create models.<\/p>\n<p>Otherwise you can always write Python code using the ML SDK to connect to any workspace given you know the subscription, resource group, workspace name etc.<\/p>\n<pre><code>from azure.core import Workspace, Model\n\n# connect to an existing workspace\nname = 'WorkspaceName'\nsub = 'subscriptionName'\nresource_group = 'resourceGroupName'\nws = Workspace.get(name=name, subscription_id=sub, resource_group=resource_group) \n\n# retrieve existing model\nmodel = Model(ws, name='your model name')\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1638210489528,
        "Answer_last_edit_time":1638220015580,
        "Answer_score_count":2.0,
        "Answerer_created_time":1627043269590,
        "Answerer_location":"Richmond, VA, USA",
        "Answerer_reputation_count":67.0,
        "Answerer_view_count":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70156610",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"access model registri workspac suppos workspac workspac team team train model store model model registri workspac workspac team team containeris model push acr deploi containeris model comput possibl team access model registri workspac workspac retriev model containeris subsequ deploy altern concept share model registri team store access common model registri possibl wai team team work singl model given respons describ",
        "Question_preprocessed_content":"model registri workspac workspac workspac team train model store model model registri workspac workspac team containeris model push acr deploi containeris model comput team model registri workspac workspac retriev model containeris subsequ deploy altern concept share model registri team store model registri wai team team work singl model given respons describ",
        "Question_gpt_summary_original":"The user is facing a challenge of accessing an Azure ML Model Registry from another Azure ML Workspace. They have two workspaces, one used by Team1 to train and store the model in the registry, and the other used by Team2 to containerize and deploy the model. The user is seeking a way for Team2 to access the model registry of Workspace1 from their Workspace2 or a shared model registry where both teams can store and access a common model registry.",
        "Question_gpt_summary":"user face challeng access model registri workspac workspac team train store model registri team container deploi model user seek wai team access model registri workspac workspac share model registri team store access common model registri"
    },
    {
        "Question_title":"os.makedir() in score.py, init() function, throws permission error while deploying",
        "Question_body":"<p>I have to create files and directories in order to perform prediction of my custom model.\nhere is my init function and deployment logs :<\/p>\n<pre><code class=\"lang-python\">\nimport os\n\ninputs_root = &quot;inputs&quot;\noutputs_root = &quot;outputs&quot;\n\ndef init():\n    if not os.path.exists(inputs_root):\n        os.mkdir(inputs_root)\n    if not os.path.exists(outputs_root):\n        os.mkdir(outputs_root)\n\n<\/code><\/pre>\n<p>It throws following error which can be seen from the deployment logs as follows : <\/p>\n<pre><code>2023-04-13 14:10:52,736 E [68] azmlinfsrv - Encountered Exception Traceback (most recent call last):\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.8\/site-packages\/azureml_inference_server_http\/server\/user_script.py&quot;, line 117, in invoke_init\n    self._user_init()\n  File &quot;\/var\/azureml-app\/avaj1870\/src\/score.py&quot;, line 71, in init\n    os.mkdir(inputs_root)\nPermissionError: [Errno 13] Permission denied: 'inputs'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.8\/site-packages\/azureml_inference_server_http\/server\/aml_blueprint.py&quot;, line 111, in setup\n    self.user_script.invoke_init()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.8\/site-packages\/azureml_inference_server_http\/server\/user_script.py&quot;, line 119, in invoke_init\n    raise UserScriptException(ex) from ex\nazureml_inference_server_http.server.user_script.UserScriptException: Caught an unhandled exception from the user script\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1681396615953,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1229685\/os-makedir()-in-score-py-init()-function-throws-pe",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"makedir score init function throw permiss error deploi creat file directori order perform predict custom model init function deploy log import input root input output root output def init path exist input root mkdir input root path exist output root mkdir output root throw follow error seen deploy log follow azmlinfsrv encount except traceback recent file opt miniconda env amlenv lib python site packag infer server http server user script line invok init self user init file var app avaj src score line init mkdir input root permissionerror errno permiss deni input except direct caus follow except traceback recent file opt miniconda env amlenv lib python site packag infer server http server aml blueprint line setup self user script invok init file opt miniconda env amlenv lib python site packag infer server http server user script line invok init rais userscriptexcept infer server http server user script userscriptexcept caught unhandl except user script",
        "Question_preprocessed_content":"init function throw deploi creat file directori order perform predict custom model init function deploy log throw deploy log",
        "Question_gpt_summary_original":"The user is encountering a permission error while deploying their custom model due to the os.makedir() function in the init() function of score.py. The error message indicates that the user does not have permission to create the 'inputs' directory.",
        "Question_gpt_summary":"user encount permiss error deploi custom model makedir function init function score error messag indic user permiss creat input directori"
    },
    {
        "Question_title":"Azure Text Analytics using C# giving errors",
        "Question_body":"<p>I am trying to use code that I got from github: <a href=\"https:\/\/github.com\/liamca\/azure-search-machine-learning-text-analytics\" rel=\"nofollow noreferrer\">https:\/\/github.com\/liamca\/azure-search-machine-learning-text-analytics<\/a> and the creating of an index works perfect, but the Keyphrase portion is giving me a 403 - Forbidden: Access is denied error. This happens in the TextExtractionHelper class on the following line of code:<\/p>\n\n<pre><code>if (!response.IsSuccessStatusCode)\n            {\n                throw new Exception(\"Call to get key phrases failed with HTTP status code: \" +\n                                    response.StatusCode + \" and contents: \" + content);\n            }\n<\/code><\/pre>\n\n<p>Based on the information in the comments, I created an account at this link: <a href=\"https:\/\/datamarket.azure.com\/account\/keys\" rel=\"nofollow noreferrer\">https:\/\/datamarket.azure.com\/account\/keys<\/a> and used the key that it provided, but I am getting the above error.<\/p>\n\n<p>Here is the code in case you do not want to download from github:<\/p>\n\n<pre><code> class Program\n    {\n        static string searchServiceName = \"&lt;removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/search-what-is-azure-search\/\n        static string searchServiceAPIKey = \"&lt;removed&gt;\";\n        static string azureMLTextAnalyticsKey = \"&lt;removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-apps-text-analytics\/\n\n        static string indexName = \"textanalytics\";\n        static SearchServiceClient serviceClient = new SearchServiceClient(searchServiceName, new SearchCredentials(searchServiceAPIKey));\n        static SearchIndexClient indexClient = serviceClient.Indexes.GetClient(indexName);\n\n        static void Main(string[] args)\n        {\n            string filetext = \"Build great search experiences for your web and mobile apps. \" +\n                \"Many applications use search as the primary interaction pattern for their users. When it comes to search, user expectations are high. They expect great relevance, suggestions, near-instantaneous responses, multiple languages, faceting, and more. Azure Search makes it easy to add powerful and sophisticated search capabilities to your website or application. The integrated Microsoft natural language stack, also used in Bing and Office, has been improved over 16 years of development. Quickly and easily tune search results, and construct rich, fine-tuned ranking models to tie search results to business goals. Reliable throughput and storage provide fast search indexing and querying to support time-sensitive search scenarios. \" +\n                \"Reduce complexity with a fully managed service. \" +\n                \"Azure Search removes the complexity of setting up and managing your own search index. This fully managed service helps you avoid the hassle of dealing with index corruption, service availability, scaling, and service updates. Create multiple indexes with no incremental cost per index. Easily scale up or down as the traffic and data volume of your application changes.\";\n\n            \/\/ Note, this will create a new Azure Search Index for the text and the key phrases\n            Console.WriteLine(\"Creating Azure Search index...\");\n            AzureSearch.CreateIndex(serviceClient, indexName);\n\n            \/\/ Apply the Machine Learning Text Extraction to retrieve only the key phrases\n            Console.WriteLine(\"Extracting key phrases from processed text... \\r\\n\");\n            KeyPhraseResult keyPhraseResult = TextExtraction.ProcessText(azureMLTextAnalyticsKey, filetext);\n\n            Console.WriteLine(\"Found the following phrases... \\r\\n\");\n            foreach (var phrase in keyPhraseResult.KeyPhrases)\n                Console.WriteLine(phrase);\n\n            \/\/ Take the resulting key phrases to a new Azure Search Index\n            \/\/ It is highly recommended that you upload documents in batches rather \n            \/\/ individually like is done here\n            Console.WriteLine(\"Uploading extracted text to Azure Search...\\r\\n\");\n            AzureSearch.UploadDocuments(indexClient, \"1\", keyPhraseResult);\n            Console.WriteLine(\"Wait 5 seconds for content to become searchable...\\r\\n\");\n            Thread.Sleep(5000);\n\n            \/\/ Execute a test search \n            Console.WriteLine(\"Execute Search...\");\n            AzureSearch.SearchDocuments(indexClient, \"Azure Search\");\n\n            Console.WriteLine(\"All done.  Press any key to continue.\");\n            Console.ReadLine();\n\n        }\n    }\n<\/code><\/pre>\n\n<p>The below is in the TextExtractionHelper class:<\/p>\n\n<pre><code>\/\/\/ &lt;summary&gt;\n\/\/\/ This is a sample program that shows how to use the Azure ML Text Analytics app (https:\/\/datamarket.azure.com\/dataset\/amla\/text-analytics)\n\/\/\/ &lt;\/summary&gt;\npublic class TextExtraction\n{\n    private const string ServiceBaseUri = \"https:\/\/api.datamarket.azure.com\/\";\n    public static KeyPhraseResult ProcessText(string accountKey, string inputText)\n    {\n        KeyPhraseResult keyPhraseResult = new KeyPhraseResult();\n        using (var httpClient = new HttpClient())\n        {\n            string inputTextEncoded = HttpUtility.UrlEncode(inputText);\n            httpClient.BaseAddress = new Uri(ServiceBaseUri);\n            string creds = \"AccountKey:\" + accountKey;\n            string authorizationHeader = \"Basic \" + Convert.ToBase64String(Encoding.ASCII.GetBytes(creds));\n            httpClient.DefaultRequestHeaders.Add(\"Authorization\", authorizationHeader);\n            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application\/json\"));\n            \/\/ get key phrases\n            string keyPhrasesRequest = \"data.ashx\/amla\/text-analytics\/v1\/GetKeyPhrases?Text=\" + inputTextEncoded;\n            Task&lt;HttpResponseMessage&gt; responseTask = httpClient.GetAsync(keyPhrasesRequest);\n            responseTask.Wait();\n            HttpResponseMessage response = responseTask.Result;\n            Task&lt;string&gt; contentTask = response.Content.ReadAsStringAsync();\n            contentTask.Wait();\n            string content = contentTask.Result;\n            if (!response.IsSuccessStatusCode)\n            {\n                throw new Exception(\"Call to get key phrases failed with HTTP status code: \" +\n                                    response.StatusCode + \" and contents: \" + content);\n            }\n            keyPhraseResult = JsonConvert.DeserializeObject&lt;KeyPhraseResult&gt;(content);\n        }\n        return keyPhraseResult;\n    }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Key Phrases call\n\/\/\/ &lt;\/summary&gt;\npublic class KeyPhraseResult\n{\n    public List&lt;string&gt; KeyPhrases { get; set; }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Sentiment call\n\/\/\/ &lt;\/summary&gt;\npublic class SentimentResult\n{\n    public double Score { get; set; }\n}\n\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Language detection call\n\/\/\/ &lt;\/summary&gt;\npublic class LanguageResult\n{\n    public bool UnknownLanguage { get; set; }\n    public IList&lt;DetectedLanguage&gt; DetectedLanguages { get; set; }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold information about a single detected language\n\/\/\/ &lt;\/summary&gt;\npublic class DetectedLanguage\n{\n    public string Name { get; set; }\n\n    \/\/\/ &lt;summary&gt;\n    \/\/\/ This is the short ISO 639-1 standard form of representing\n    \/\/\/ all languages. The short form is a 2 letter representation of the language.\n    \/\/\/ en = English, fr = French for example\n    \/\/\/ &lt;\/summary&gt;\n    public string Iso6391Name { get; set; }\n    public double Score { get; set; }\n}\n<\/code><\/pre>\n\n<p><strong>UPDATE<\/strong><\/p>\n\n<p>After many hours of taking different sample code and trying to put them together, I finally got something \"kind of\" working. Here is all my code:<\/p>\n\n<pre><code>class Program\n{\n    static string searchServiceName = \"&lt;removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/search-what-is-azure-search\/\n    static string searchServiceAPIKey = \"&lt;removed&gt;\";\n    \/\/static string azureMLTextAnalyticsKey = \"&lt;removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-apps-text-analytics\/\n\n    static string indexName = \"textanalytics\";\n    static SearchServiceClient serviceClient = new SearchServiceClient(searchServiceName, new SearchCredentials(searchServiceAPIKey));\n    static SearchIndexClient indexClient = serviceClient.Indexes.GetClient(indexName);\n\n    static void Main()\n    {\n        MakeRequests();\n        Console.WriteLine(\"Hit ENTER to exit...\");\n        Console.ReadLine();\n    }\n\n    static async void MakeRequests()\n    {\n        \/\/ Note, this will create a new Azure Search Index for the text and the key phrases\n        Console.WriteLine(\"Creating Azure Search index...\");\n        AzureSearch.CreateIndex(serviceClient, indexName);\n\n        \/\/ Apply the Machine Learning Text Extraction to retrieve only the key phrases\n        Console.WriteLine(\"Extracting key phrases from processed text... \\r\\n\");\n        KeyPhraseResult keyPhraseResult = await TextExtraction.ProcessText();\n\n        Console.WriteLine(\"Found the following phrases... \\r\\n\");\n        foreach (var phrase in keyPhraseResult.KeyPhrases)\n            Console.WriteLine(phrase);\n\n        \/\/ Take the resulting key phrases to a new Azure Search Index\n        \/\/ It is highly recommended that you upload documents in batches rather \n        \/\/ individually like is done here\n        Console.WriteLine(\"Uploading extracted text to Azure Search...\\r\\n\");\n        AzureSearch.UploadDocuments(indexClient, \"1\", keyPhraseResult);\n        Console.WriteLine(\"Wait 5 seconds for content to become searchable...\\r\\n\");\n        Thread.Sleep(5000);\n\n        \/\/ Execute a test search \n        Console.WriteLine(\"Execute Search...\");\n        AzureSearch.SearchDocuments(indexClient, \"Azure Search\");\n\n        Console.WriteLine(\"All done.  Press any key to continue.\");\n        Console.ReadLine();\n    }\n}\n<\/code><\/pre>\n\n<p>Here is my TextExtractionHelper class:<\/p>\n\n<pre><code>public class TextExtraction\n{\n    static string azureMLTextAnalyticsKey = \"&lt;Removed&gt;\";     \/\/ Learn more here: https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-apps-text-analytics\/\n    private const string ServiceBaseUri = \"https:\/\/westus.api.cognitive.microsoft.com\/\";\n    public static async Task&lt;KeyPhraseResult&gt; ProcessText()\n    {\n        string filetext = \"Build great search experiences for your web and mobile apps. \" +\n            \"Many applications use search as the primary interaction pattern for their users. When it comes to search, user expectations are high. They expect great relevance, suggestions, near-instantaneous responses, multiple languages, faceting, and more. Azure Search makes it easy to add powerful and sophisticated search capabilities to your website or application. The integrated Microsoft natural language stack, also used in Bing and Office, has been improved over 16 years of development. Quickly and easily tune search results, and construct rich, fine-tuned ranking models to tie search results to business goals. Reliable throughput and storage provide fast search indexing and querying to support time-sensitive search scenarios. \" +\n            \"Reduce complexity with a fully managed service. \" +\n            \"Azure Search removes the complexity of setting up and managing your own search index. This fully managed service helps you avoid the hassle of dealing with index corruption, service availability, scaling, and service updates. Create multiple indexes with no incremental cost per index. Easily scale up or down as the traffic and data volume of your application changes.\";\n\n\n        KeyPhraseResult keyPhraseResult = new KeyPhraseResult();\n        using (var httpClient = new HttpClient())\n        {\n\n            httpClient.BaseAddress = new Uri(ServiceBaseUri);\n\n            \/\/ Request headers.\n            httpClient.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", azureMLTextAnalyticsKey);\n            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application\/json\"));\n\n            byte[] byteData = Encoding.UTF8.GetBytes(\"{\\\"documents\\\":[\" +\n               \"{\\\"id\\\":\\\"1\\\",\\\"text\\\":\\\"\" + filetext + \"\\\"},]}\");\n\n            \/\/byte[] byteData = Encoding.UTF8.GetBytes(\"{\\\"documents\\\":[\" +\n            \/\/   \"{\\\"id\\\":\\\"1\\\",\\\"text\\\":\\\"Build great search experiences for your web and mobile apps.\" +\n            \/\/   \"Many applications use search as the primary interaction pattern for their users. When it comes to search, user expectations are high. They expect great relevance, suggestions, near-instantaneous responses, multiple languages, faceting, and more. Azure Search makes it easy to add powerful and sophisticated search capabilities to your website or application. The integrated Microsoft natural language stack, also used in Bing and Office, has been improved over 16 years of development. Quickly and easily tune search results, and construct rich, fine-tuned ranking models to tie search results to business goals. Reliable throughput and storage provide fast search indexing and querying to support time-sensitive search scenarios.\" +\n            \/\/   \"Reduce complexity with a fully managed service. \" +\n            \/\/   \"Azure Search removes the complexity of setting up and managing your own search index. This fully managed service helps you avoid the hassle of dealing with index corruption, service availability, scaling, and service updates. Create multiple indexes with no incremental cost per index. Easily scale up or down as the traffic and data volume of your application changes.\\\"},\" +\n            \/\/   \"]}\");\n\n            \/\/ Detect key phrases:\n            var keyPhrasesRequest = \"text\/analytics\/v2.0\/keyPhrases\";\n            \/\/var response = await CallEndpoint(httpClient, uri, byteData);\n\n            \/\/ get key phrases\n            using (var getcontent = new ByteArrayContent(byteData))\n            {\n                getcontent.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n                var response = await httpClient.PostAsync(keyPhrasesRequest, getcontent);\n\n                Task&lt;string&gt; contentTask = response.Content.ReadAsStringAsync();\n\n                string content = contentTask.Result;\n\n                if (!response.IsSuccessStatusCode)\n                {\n                    throw new Exception(\"Call to get key phrases failed with HTTP status code: \" +\n                                        response.StatusCode + \" and contents: \" + content);\n                }\n                keyPhraseResult = JsonConvert.DeserializeObject&lt;KeyPhraseResult&gt;(content);\n                \/\/return await response.Content.ReadAsStringAsync();\n            }\n        }\n        return keyPhraseResult;\n    }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Key Phrases call\n\/\/\/ &lt;\/summary&gt;\npublic class KeyPhraseResult\n{\n    public List&lt;string&gt; KeyPhrases { get; set; }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Sentiment call\n\/\/\/ &lt;\/summary&gt;\npublic class SentimentResult\n{\n    public double Score { get; set; }\n}\n\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Language detection call\n\/\/\/ &lt;\/summary&gt;\npublic class LanguageResult\n{\n    public bool UnknownLanguage { get; set; }\n    public IList&lt;DetectedLanguage&gt; DetectedLanguages { get; set; }\n}\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold information about a single detected language\n\/\/\/ &lt;\/summary&gt;\npublic class DetectedLanguage\n{\n    public string Name { get; set; }\n\n    \/\/\/ &lt;summary&gt;\n    \/\/\/ This is the short ISO 639-1 standard form of representing\n    \/\/\/ all languages. The short form is a 2 letter representation of the language.\n    \/\/\/ en = English, fr = French for example\n    \/\/\/ &lt;\/summary&gt;\n    public string Iso6391Name { get; set; }\n    public double Score { get; set; }\n}\n<\/code><\/pre>\n\n<p>So I am now able to pull the KeyPhrases from the text! But, now I am sitting with a problem where it doesnt seem like the JSON string is being deserialized and my keyPhraseResult is now getting a null value.<\/p>\n\n<p>What am I missing ?<\/p>\n\n<p>If anyone is able to help, I would greatly appreciate it.<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1480585773797,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1480606926256,
        "Question_score_count":2.0,
        "Question_view_count":1204.0,
        "Poster_created_time":1366363157040,
        "Poster_location":"South Africa",
        "Poster_reputation_count":1761.0,
        "Poster_view_count":328.0,
        "Answer_body":"<p>So I got it working! With the help from this link: <a href=\"https:\/\/stackoverflow.com\/questions\/40932669\/deserializing-json-using-c-sharp-to-return-items\">Deserializing JSON using C# to return items<\/a>, which i posted to simplify where my issue was occurring now.<\/p>\n\n<p>So what this code is doing is the following:<\/p>\n\n<ol>\n<li>Creating an Index in Azure called textanalytics.<\/li>\n<li>Creating a JSON string of the text provided.<\/li>\n<li>Retrieving the KeyPhrases and adding these to the Index created in point 1 above.<\/li>\n<\/ol>\n\n<p>Below is my entire code, in case it helps someone else:<\/p>\n\n<p>(Please ensure that you add the relevant references from Nuget packages: Microsoft.Azure.Search and Newtonsoft.Json)<\/p>\n\n<p>Program.cs(This is a console application):<\/p>\n\n<pre><code>using Microsoft.Azure.Search;\nusing System;\nusing System.Configuration;\nusing System.IO;\nusing System.Threading;\n\nnamespace AzureSearchTextAnalytics\n{\nclass Program\n{\n    static string searchServiceName = \"&lt;removed&gt;\";     \/\/ This is the Azure Search service name that you create in Azure\n    static string searchServiceAPIKey = \"&lt;removed&gt;\";   \/\/ This is the Primary key that is provided after creating a Azure Search Service\n\n    static string indexName = \"textanalytics\";\n    static SearchServiceClient serviceClient = new SearchServiceClient(searchServiceName, new SearchCredentials(searchServiceAPIKey));\n    static SearchIndexClient indexClient = serviceClient.Indexes.GetClient(indexName);\n\n    static void Main()\n    {\n        MakeRequests();\n        Console.WriteLine(\"Hit ENTER to exit...\");\n        Console.ReadLine();\n    }\n\n    static async void MakeRequests()\n    {\n        \/\/ Note, this will create a new Azure Search Index for the text and the key phrases\n        Console.WriteLine(\"Creating Azure Search index...\");\n        AzureSearch.CreateIndex(serviceClient, indexName);\n\n        \/\/ Apply the Machine Learning Text Extraction to retrieve only the key phrases\n        Console.WriteLine(\"Extracting key phrases from processed text... \\r\\n\");\n        KeyPhraseResult keyPhraseResult = await TextExtraction.ProcessText();\n\n        Console.WriteLine(\"Found the following phrases... \\r\\n\");\n        foreach (var phrase in keyPhraseResult.KeyPhrases)\n            Console.WriteLine(phrase);\n\n        \/\/ Take the resulting key phrases to a new Azure Search Index\n        \/\/ It is highly recommended that you upload documents in batches rather \n        \/\/ individually like is done here\n        Console.WriteLine(\"Uploading extracted text to Azure Search...\\r\\n\");\n        AzureSearch.UploadDocuments(indexClient, \"1\", keyPhraseResult);\n        Console.WriteLine(\"Wait 5 seconds for content to become searchable...\\r\\n\");\n        Thread.Sleep(5000);\n\n        \/\/ Execute a test search \n        Console.WriteLine(\"Execute Search...\");\n        AzureSearch.SearchDocuments(indexClient, \"Azure Search\");\n\n        Console.WriteLine(\"All done.  Press any key to continue.\");\n        Console.ReadLine();\n    }\n}\n}\n<\/code><\/pre>\n\n<p>My TextExtractionHelper.cs:<\/p>\n\n<pre><code>using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Net.Http;\nusing System.Net.Http.Headers;\nusing System.Text;\nusing System.Threading.Tasks;\nusing System.Web;\nusing Newtonsoft.Json;\nusing System.Configuration; \/\/ get it from http:\/\/www.newtonsoft.com\/json\nusing Newtonsoft.Json.Linq;\n\nnamespace AzureSearchTextAnalytics\n{\n\/\/\/ &lt;\/summary&gt;\npublic class TextExtraction\n{\n    static string azureMLTextAnalyticsKey = \"&lt;removed&gt;\";     \/\/ This key you will get when you have added TextAnalytics in Azure.\n    private const string ServiceBaseUri = \"https:\/\/westus.api.cognitive.microsoft.com\/\"; \/\/This you will get when you have added TextAnalytics in Azure\n    public static async Task&lt;KeyPhraseResult&gt; ProcessText()\n    {\n        string filetext = \"Build great search experiences for your web and mobile apps. \" +\n            \"Many applications use search as the primary interaction pattern for their users. When it comes to search, user expectations are high. They expect great relevance, suggestions, near-instantaneous responses, multiple languages, faceting, and more. Azure Search makes it easy to add powerful and sophisticated search capabilities to your website or application. The integrated Microsoft natural language stack, also used in Bing and Office, has been improved over 16 years of development. Quickly and easily tune search results, and construct rich, fine-tuned ranking models to tie search results to business goals. Reliable throughput and storage provide fast search indexing and querying to support time-sensitive search scenarios. \" +\n            \"Reduce complexity with a fully managed service. \" +\n            \"Azure Search removes the complexity of setting up and managing your own search index. This fully managed service helps you avoid the hassle of dealing with index corruption, service availability, scaling, and service updates. Create multiple indexes with no incremental cost per index. Easily scale up or down as the traffic and data volume of your application changes.\";\n\n\n        KeyPhraseResult keyPhraseResult = new KeyPhraseResult();\n        using (var httpClient = new HttpClient())\n        {\n\n            httpClient.BaseAddress = new Uri(ServiceBaseUri);\n\n            \/\/ Request headers.\n            httpClient.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", azureMLTextAnalyticsKey);\n            httpClient.DefaultRequestHeaders.Accept.Add(new MediaTypeWithQualityHeaderValue(\"application\/json\"));\n\n            byte[] byteData = Encoding.UTF8.GetBytes(\"{\\\"documents\\\":[\" +\n               \"{\\\"id\\\":\\\"1\\\",\\\"text\\\":\\\"\" + filetext + \"\\\"},]}\");\n\n            \/\/ Detect key phrases:\n            var keyPhrasesRequest = \"text\/analytics\/v2.0\/keyPhrases\";\n\n            \/\/ get key phrases\n            using (var getcontent = new ByteArrayContent(byteData))\n            {\n                getcontent.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n                var response = await httpClient.PostAsync(keyPhrasesRequest, getcontent);\n\n                Task&lt;string&gt; contentTask = response.Content.ReadAsStringAsync();\n\n                string content = contentTask.Result;\n\n                if (!response.IsSuccessStatusCode)\n                {\n                    throw new Exception(\"Call to get key phrases failed with HTTP status code: \" +\n                                        response.StatusCode + \" and contents: \" + content);\n                }\n\n\n                var result = JsonConvert.DeserializeObject&lt;RootObject&gt;(content);\n\n                keyPhraseResult.KeyPhrases = result.documents[0].keyPhrases;\n            }\n\n        }\n        return keyPhraseResult;\n    }\n}\n\npublic class Documents\n{\n    public List&lt;string&gt; keyPhrases { get; set; }\n    public string id { get; set; }\n}\n\npublic class RootObject\n{\n    public List&lt;Documents&gt; documents { get; set; }\n    public List&lt;object&gt; errors { get; set; }\n}\n\n\/\/\/ &lt;summary&gt;\n\/\/\/ Class to hold result of Key Phrases call\n\/\/\/ &lt;\/summary&gt;\npublic class KeyPhraseResult\n{\n    public List&lt;string&gt; KeyPhrases { get; set; }\n}\n}\n<\/code><\/pre>\n\n<p>AzureSearch.cs:<\/p>\n\n<pre><code>using Microsoft.Azure.Search;\nusing Microsoft.Azure.Search.Models;\nusing System;\nusing System.Collections.Generic;\nusing System.Configuration;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\n\nnamespace AzureSearchTextAnalytics\n{\npublic class AzureSearch\n{\n    public static void CreateIndex(SearchServiceClient serviceClient, string indexName)\n    {\n\n        if (serviceClient.Indexes.Exists(indexName))\n        {\n            serviceClient.Indexes.Delete(indexName);\n        }\n\n        var definition = new Index()\n        {\n            Name = indexName,\n            Fields = new[]\n            {\n                new Field(\"fileId\", DataType.String)                       { IsKey = true },\n                new Field(\"fileText\", DataType.String)                      { IsSearchable = true, IsFilterable = false, IsSortable = false, IsFacetable = false },\n                new Field(\"keyPhrases\", DataType.Collection(DataType.String)) { IsSearchable = true, IsFilterable = true,  IsFacetable = true }\n            }\n        };\n\n        serviceClient.Indexes.Create(definition);\n    }\n\n    public static void UploadDocuments(SearchIndexClient indexClient, string fileId, KeyPhraseResult keyPhraseResult)\n    {\n        List&lt;IndexAction&gt; indexOperations = new List&lt;IndexAction&gt;();\n        var doc = new Document();\n        doc.Add(\"fileId\", fileId);\n        doc.Add(\"keyPhrases\", keyPhraseResult.KeyPhrases.ToList());\n        indexOperations.Add(IndexAction.Upload(doc));\n\n        try\n        {\n            indexClient.Documents.Index(new IndexBatch(indexOperations));\n        }\n        catch (IndexBatchException e)\n        {\n            \/\/ Sometimes when your Search service is under load, indexing will fail for some of the documents in\n            \/\/ the batch. Depending on your application, you can take compensating actions like delaying and\n            \/\/ retrying. For this simple demo, we just log the failed document keys and continue.\n            Console.WriteLine(\n            \"Failed to index some of the documents: {0}\",\n                   String.Join(\", \", e.IndexingResults.Where(r =&gt; !r.Succeeded).Select(r =&gt; r.Key)));\n        }\n\n    }\n\n\n    public static void SearchDocuments(SearchIndexClient indexClient, string searchText)\n    {\n        \/\/ Search using the supplied searchText and output documents that match \n        try\n        {\n            var sp = new SearchParameters();\n\n            DocumentSearchResult&lt;OCRTextIndex&gt; response = indexClient.Documents.Search&lt;OCRTextIndex&gt;(searchText, sp);\n            foreach (SearchResult&lt;OCRTextIndex&gt; result in response.Results)\n            {\n                Console.WriteLine(\"File ID: {0}\", result.Document.fileId);\n                Console.WriteLine(\"Key Phrases: {0}\", string.Join(\",\", result.Document.keyPhrases));\n\n            }\n        }\n        catch (Exception e)\n        {\n            Console.WriteLine(\"Failed search: {0}\", e.Message.ToString());\n        }\n\n    }\n\n}\n}\n<\/code><\/pre>\n\n<p>DataModel.cs<\/p>\n\n<pre><code>using Microsoft.Azure.Search.Models;\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text;\nusing System.Threading.Tasks;\n\nnamespace AzureSearchTextAnalytics\n{\n[SerializePropertyNamesAsCamelCase]\npublic class OCRTextIndex\n{\n    public string fileId { get; set; }\n\n    public string[] keyPhrases { get; set; }\n\n}\n}\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1480685355832,
        "Answer_last_edit_time":1495542255267,
        "Answer_score_count":6.0,
        "Answerer_created_time":1366363157040,
        "Answerer_location":"South Africa",
        "Answerer_reputation_count":1761.0,
        "Answerer_view_count":328.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40907303",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur text analyt give error try us code got github http github com liamca azur search machin learn text analyt creat index work perfect keyphras portion give forbidden access deni error happen textextractionhelp class follow line code respons issuccessstatuscod throw new except kei phrase fail http statu code respons statuscod content content base inform comment creat account link http datamarket azur com account kei kei provid get error code case want download github class program static string searchservicenam learn http azur microsoft com document articl search azur search static string searchserviceapikei static string textanalyticskei learn http azur microsoft com document articl machin learn app text analyt static string indexnam textanalyt static searchservicecli servicecli new searchservicecli searchservicenam new searchcredenti searchserviceapikei static searchindexcli indexcli servicecli index getclient indexnam static void main string arg string filetext build great search experi web mobil app applic us search primari interact pattern user come search user expect high expect great relev suggest near instantan respons multipl languag facet azur search make easi add power sophist search capabl websit applic integr microsoft natur languag stack bing offic improv year develop quickli easili tune search result construct rich fine tune rank model tie search result busi goal reliabl throughput storag provid fast search index queri support time sensit search scenario reduc complex fulli manag servic azur search remov complex set manag search index fulli manag servic help avoid hassl deal index corrupt servic avail scale servic updat creat multipl index increment cost index easili scale traffic data volum applic chang note creat new azur search index text kei phrase consol writelin creat azur search index azuresearch createindex servicecli indexnam appli machin learn text extract retriev kei phrase consol writelin extract kei phrase process text keyphraseresult keyphraseresult textextract processtext textanalyticskei filetext consol writelin follow phrase foreach var phrase keyphraseresult keyphras consol writelin phrase result kei phrase new azur search index highli recommend upload document batch individu like consol writelin upload extract text azur search azuresearch uploaddocu indexcli keyphraseresult consol writelin wait second content searchabl thread sleep execut test search consol writelin execut search azuresearch searchdocu indexcli azur search consol writelin press kei continu consol readlin textextractionhelp class sampl program show us text analyt app http datamarket azur com dataset amla text analyt public class textextract privat const string servicebaseuri http api datamarket azur com public static keyphraseresult processtext string accountkei string inputtext keyphraseresult keyphraseresult new keyphraseresult var httpclient new httpclient string inputtextencod httputil urlencod inputtext httpclient baseaddress new uri servicebaseuri string cred accountkei accountkei string authorizationhead basic convert tobasestr encod ascii getbyt cred httpclient defaultrequesthead add author authorizationhead httpclient defaultrequesthead accept add new mediatypewithqualityheadervalu applic json kei phrase string keyphrasesrequest data ashx amla text analyt getkeyphras text inputtextencod task responsetask httpclient getasync keyphrasesrequest responsetask wait httpresponsemessag respons responsetask result task contenttask respons content readasstringasync contenttask wait string content contenttask result respons issuccessstatuscod throw new except kei phrase fail http statu code respons statuscod content content keyphraseresult jsonconvert deserializeobject content return keyphraseresult class hold result kei phrase public class keyphraseresult public list keyphras set class hold result sentiment public class sentimentresult public doubl score set class hold result languag detect public class languageresult public bool unknownlanguag set public ilist detectedlanguag set class hold inform singl detect languag public class detectedlanguag public string set short iso standard form repres languag short form letter represent languag english french exampl public string isonam set public doubl score set updat hour take differ sampl code try final got kind work code class program static string searchservicenam learn http azur microsoft com document articl search azur search static string searchserviceapikei static string textanalyticskei learn http azur microsoft com document articl machin learn app text analyt static string indexnam textanalyt static searchservicecli servicecli new searchservicecli searchservicenam new searchcredenti searchserviceapikei static searchindexcli indexcli servicecli index getclient indexnam static void main makerequest consol writelin hit enter exit consol readlin static async void makerequest note creat new azur search index text kei phrase consol writelin creat azur search index azuresearch createindex servicecli indexnam appli machin learn text extract retriev kei phrase consol writelin extract kei phrase process text keyphraseresult keyphraseresult await textextract processtext consol writelin follow phrase foreach var phrase keyphraseresult keyphras consol writelin phrase result kei phrase new azur search index highli recommend upload document batch individu like consol writelin upload extract text azur search azuresearch uploaddocu indexcli keyphraseresult consol writelin wait second content searchabl thread sleep execut test search consol writelin execut search azuresearch searchdocu indexcli azur search consol writelin press kei continu consol readlin textextractionhelp class public class textextract static string textanalyticskei learn http azur microsoft com document articl machin learn app text analyt privat const string servicebaseuri http westu api cognit microsoft com public static async task processtext string filetext build great search experi web mobil app applic us search primari interact pattern user come search user expect high expect great relev suggest near instantan respons multipl languag facet azur search make easi add power sophist search capabl websit applic integr microsoft natur languag stack bing offic improv year develop quickli easili tune search result construct rich fine tune rank model tie search result busi goal reliabl throughput storag provid fast search index queri support time sensit search scenario reduc complex fulli manag servic azur search remov complex set manag search index fulli manag servic help avoid hassl deal index corrupt servic avail scale servic updat creat multipl index increment cost index easili scale traffic data volum applic chang keyphraseresult keyphraseresult new keyphraseresult var httpclient new httpclient httpclient baseaddress new uri servicebaseuri request header httpclient defaultrequesthead add ocp apim subscript kei textanalyticskei httpclient defaultrequesthead accept add new mediatypewithqualityheadervalu applic json byte bytedata encod utf getbyt document text filetext byte bytedata encod utf getbyt document text build great search experi web mobil app applic us search primari interact pattern user come search user expect high expect great relev suggest near instantan respons multipl languag facet azur search make easi add power sophist search capabl websit applic integr microsoft natur languag stack bing offic improv year develop quickli easili tune search result construct rich fine tune rank model tie search result busi goal reliabl throughput storag provid fast search index queri support time sensit search scenario reduc complex fulli manag servic azur search remov complex set manag search index fulli manag servic help avoid hassl deal index corrupt servic avail scale servic updat creat multipl index increment cost index easili scale traffic data volum applic chang detect kei phrase var keyphrasesrequest text analyt keyphras var respons await callendpoint httpclient uri bytedata kei phrase var getcont new bytearraycont bytedata getcont header contenttyp new mediatypeheadervalu applic json var respons await httpclient postasync keyphrasesrequest getcont task contenttask respons content readasstringasync string content contenttask result respons issuccessstatuscod throw new except kei phrase fail http statu code respons statuscod content content keyphraseresult jsonconvert deserializeobject content return await respons content readasstringasync return keyphraseresult class hold result kei phrase public class keyphraseresult public list keyphras set class hold result sentiment public class sentimentresult public doubl score set class hold result languag detect public class languageresult public bool unknownlanguag set public ilist detectedlanguag set class hold inform singl detect languag public class detectedlanguag public string set short iso standard form repres languag short form letter represent languag english french exampl public string isonam set public doubl score set abl pull keyphras text sit problem doesnt like json string deseri keyphraseresult get null valu miss abl help greatli appreci thank",
        "Question_preprocessed_content":"azur text analyt give try us code got github creat index work perfect keyphras portion give deni textextractionhelp line code base inform creat link kei provid code case want download github textextractionhelp updat hour take sampl code try got kind work code textextractionhelp abl keyphras text problem doesnt like json string deseri keyphraseresult valu abl help greatli thank",
        "Question_gpt_summary_original":"The user is encountering a 403 - Forbidden: Access is denied error while using Azure Text Analytics with C#. The error occurs in the TextExtractionHelper class while trying to get key phrases. The user has created an account and used the provided key, but the error persists. The user has tried different sample codes and has managed to get the key phrases, but the JSON string is not being deserialized, resulting in a null value for keyPhraseResult. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount forbidden access deni error azur text analyt error occur textextractionhelp class try kei phrase user creat account provid kei error persist user tri differ sampl code manag kei phrase json string deseri result null valu keyphraseresult user seek help resolv issu"
    },
    {
        "Question_title":"How to use Azure DevOps artifacts repository as source for DatabricksStep of AzureML?",
        "Question_body":"<p>If we have PyPi Packages added as Artifacts to an Azure DevOps Project Feed, how can we use these packages as a source for installing packages in <code>DatabricksStep<\/code> of Azure Machine Learning Service?<\/p>\n<p>While using <code>pip<\/code> in any environment, we use our Azure DevOps Project Artifacts feed in the following way:<\/p>\n<pre><code>pip install example-package --index-url=https:\/\/&lt;Personal-Access-Token&gt;@pkgs.dev.azure.com\/&lt;Organization-Name&gt;\/_packaging\/&lt;Artifacts-Feed-Name&gt;\/pypi\/simple\/\n<\/code><\/pre>\n<p>The DatabricksStep class of the Azure Machine Learning Service accepts the following parameters:<\/p>\n<pre><code>python_script_name = &quot;&lt;Some-Script&gt;.py&quot;\nsource_directory = &quot;&lt;Path-To-Script&gt;&quot;\n\n&lt;Some-Placeholder-Name-for-the-step&gt; = DatabricksStep(\n    name=&lt;Some-Placeholder-Name-for-the-step&gt;,\n    num_workers=1,\n    python_script_name=python_script_name,\n    source_directory=source_directory,\n    run_name= &lt;Name-of-the-run&gt;,\n    compute_target=databricks_compute,\n    pypi_libraries = [\n                      PyPiLibrary(package = 'scikit-learn'), \n                      PyPiLibrary(package = 'scipy'), \n                      PyPiLibrary(package = 'azureml-sdk'), \n                      PyPiLibrary(package = 'joblib'), \n                      PyPiLibrary(package = 'azureml-dataprep[pandas]'),\n                      PyPiLibrary(package = 'example-package', repo='https:\/\/&lt;Personal-Access-Token&gt;@pkgs.dev.azure.com\/&lt;Organization-Name&gt;\/_packaging\/&lt;Artifacts-Feed-Name&gt;\/pypi\/simple\/')\n                    ], \n\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>However, <code>PyPiLibrary(package = 'example-package', repo='https:\/\/&lt;Personal-Access-Token&gt;@pkgs.dev.azure.com\/&lt;Organization-Name&gt;\/_packaging\/&lt;Artifacts-Feed-Name&gt;\/pypi\/simple\/')<\/code> will give an error. How exactly should we consume the Artifacts Feed as an input to the <code>PyPiLibrary<\/code> property of the <code>DatabricksStep<\/code> Class in Azure Machine Learning Service?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1611234834813,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":5.0,
        "Question_view_count":341.0,
        "Poster_created_time":1601729162436,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation_count":887.0,
        "Poster_view_count":130.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65828200",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us azur devop artifact repositori sourc databricksstep pypi packag ad artifact azur devop project feed us packag sourc instal packag databricksstep servic pip environ us azur devop project artifact feed follow wai pip instal exampl packag index url http pkg dev azur com packag pypi simpl databricksstep class servic accept follow paramet python script sourc directori databricksstep num worker python script python script sourc directori sourc directori run comput target databrick comput pypi librari pypilibrari packag scikit learn pypilibrari packag scipi pypilibrari packag sdk pypilibrari packag joblib pypilibrari packag dataprep panda pypilibrari packag exampl packag repo http pkg dev azur com packag pypi simpl allow reus true pypilibrari packag exampl packag repo http pkg dev azur com packag pypi simpl error exactli consum artifact feed input pypilibrari properti databricksstep class servic",
        "Question_preprocessed_content":"us azur devop artifact repositori sourc pypi packag artifact azur devop project us packag sourc packag servic environ us azur devop project artifact wai servic paramet exactli consum artifact input properti servic",
        "Question_gpt_summary_original":"The user is trying to use PyPi packages added as Artifacts to an Azure DevOps Project Feed as a source for installing packages in DatabricksStep of Azure Machine Learning Service. They are encountering an error while trying to consume the Artifacts Feed as an input to the PyPiLibrary property of the DatabricksStep Class in Azure Machine Learning Service.",
        "Question_gpt_summary":"user try us pypi packag ad artifact azur devop project feed sourc instal packag databricksstep servic encount error try consum artifact feed input pypilibrari properti databricksstep class servic"
    },
    {
        "Question_title":"AWS SageMaker ML DevOps tooling \/ architecture - Kubeflow?",
        "Question_body":"<p>I'm tasked with defining AWS tools for ML development at a medium-sized company. Assume about a dozen ML engineers plus other DevOps staff familiar with serverless ( lambdas and the framework ). The main questions are: a) what is an architecture that allows for the main tasks related to ML development (creating, training, fitting models, data pre-processing, hyper parameter optimization, job management, wrapping serverless services, gathering model metrics, etc ), b) what are the main tools that can be used for packaging and deploying things and c) what are the development tools (IDEs, SDKs, 'frameworks' ) used for it?\nI just want to set Jupyter notebooks aside for a second. Jupyter notebooks are great for proof-of-concepts and the closest thing to PowerPoint for management... But I have a problem with notebooks when thinking about deployable units of code.<br \/>\nMy intuition points to a preliminary target architecture with 5 parts:<\/p>\n<p>1 - A 'core' with ML models supporting basic model operations (create blank, create pre-trained, train, test\/fit, etc). I foresee core Python scripts here - no problem.<\/p>\n<p>2- (optional) A 'containerized-set-of-things' that performs hyper parameter optimization and\/or model versioning<\/p>\n<p>3- A 'contained-unit-of-Python-scripts-around-models' that exposes an API and that does job management and incorporates data pre-processing. This also reads and writes to S3 buckets.<\/p>\n<p>4-  A 'serverless layer' with high level API ( in Python ). It talks to #3 and\/or #1 above.<\/p>\n<p>5- Some container or bundling thing that will unpack files from Git and deploy them onto various AWS services creating things from the previous 3 points.<\/p>\n<p>As you can see, my terms are rather fuzzy:)  If someone can be specific with terms that will be helpful.\nMy intuition and my preliminary readings say that the answer will likely include a local IDE like PyCharm or Anaconda or a cloud-based IDE (what can these be? - don't mention notebooks please).\nThe point that I'm not really clear about is #5. Candidates include Amazon SageMaker Components for Kubeflow Pipelines and\/or Amazon SageMaker Components for Kubeflow Pipelines and\/or AWS Step Functions DS SDK For SageMaker. It's unclear to me how they can perform #5, however. Kubeflow looks very interesting but does it have enough adoption or will it die in 2 years? Are Amazon SageMaker Components for Kubeflow Pipelines, Amazon SageMaker Components for Kubeflow Pipelines and AWS Step Functions DS SDK For SageMaker mutually exclusive? How can each of them help with 'containerizing things' and with basic provisioning and deployment tasks?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1607384425877,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":762.0,
        "Poster_created_time":1519621319587,
        "Poster_location":null,
        "Poster_reputation_count":123.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65191030",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"devop tool architectur kubeflow task defin aw tool develop medium size compani assum dozen engin plu devop staff familiar serverless lambda framework main question architectur allow main task relat develop creat train fit model data pre process hyper paramet optim job manag wrap serverless servic gather model metric main tool packag deploi thing develop tool id sdk framework want set jupyt notebook asid second jupyt notebook great proof concept closest thing powerpoint manag problem notebook think deploy unit code intuit point preliminari target architectur part core model support basic model oper creat blank creat pre train train test fit forese core python script problem option container set thing perform hyper paramet optim model version contain unit python script model expos api job manag incorpor data pre process read write bucket serverless layer high level api python talk contain bundl thing unpack file git deploi aw servic creat thing previou point term fuzzi specif term help intuit preliminari read answer like includ local id like pycharm anaconda cloud base id mention notebook point clear candid includ compon kubeflow pipelin compon kubeflow pipelin aw step function sdk unclear perform kubeflow look interest adopt die year compon kubeflow pipelin compon kubeflow pipelin aw step function sdk mutual exclus help container thing basic provis deploy task",
        "Question_preprocessed_content":"devop architectur kubeflow task defin aw develop compani dozen plu devop familiar main question architectur main task relat develop main packag deploi thing develop want set jupyt asid second jupyt great closest thing powerpoint problem think deploy unit code intuit point preliminari target architectur part core model basic model oper core python script problem perform hyper paramet optim model version expos api job manag incorpor data read write bucket layer high level api talk contain bundl thing unpack file git deploi aw servic creat thing previou point term specif term help intuit preliminari read answer like includ local id like pycharm anaconda id point clear candid includ compon kubeflow pipelin compon kubeflow pipelin aw step function sdk unclear perform kubeflow interest adopt die year compon kubeflow pipelin compon kubeflow pipelin aw step function sdk exclus help container thing basic provis deploy task",
        "Question_gpt_summary_original":"The user is seeking guidance on defining AWS tools for ML development for a medium-sized company with a dozen ML engineers and other DevOps staff. They are looking for an architecture that allows for ML development tasks, tools for packaging and deploying, and development tools. The user is considering a preliminary target architecture with five parts, including a core with ML models, a containerized set of things for hyper parameter optimization and\/or model versioning, a contained unit of Python scripts around models that exposes an API, a serverless layer with a high-level API, and a container or bundling thing that deploys files from Git onto various AWS services. The user is unsure about the fifth part and is considering Amazon SageMaker Components for Kubeflow Pipelines and\/or",
        "Question_gpt_summary":"user seek guidanc defin aw tool develop medium size compani dozen engin devop staff look architectur allow develop task tool packag deploi develop tool user consid preliminari target architectur part includ core model container set thing hyper paramet optim model version contain unit python script model expos api serverless layer high level api contain bundl thing deploi file git aw servic user unsur fifth consid compon kubeflow pipelin"
    },
    {
        "Question_title":"Unable to get data from ADLS after changing the Client Secret",
        "Question_body":"<p>Hi<\/p>\n<p>I have AML Workspace connected to ADLS through registered dataset in AML. Last week, secrets have been updated for the Service Principal. So we have updated the secrets in Data Assest in AML Workspace through <strong>Update Authentication option.<\/strong> But now the python code fails with Request is not authorized to access. Not sure what else needs to be done. <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5.0,
        "Question_created_time":1677728012643,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1185676\/unable-to-get-data-from-adls-after-changing-the-cl",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl data adl chang client secret aml workspac connect adl regist dataset aml week secret updat servic princip updat secret data assest aml workspac updat authent option python code fail request author access sure need",
        "Question_preprocessed_content":"unabl data adl chang client secret aml workspac adl regist dataset aml secret updat servic princip updat secret data aml workspac updat authent option python code fail request author sure",
        "Question_gpt_summary_original":"The user is unable to access data from ADLS after updating the secrets for the Service Principal through the Update Authentication option in AML Workspace. The python code fails with an error message stating that the request is not authorized to access the data. The user is unsure of what else needs to be done to resolve the issue.",
        "Question_gpt_summary":"user unabl access data adl updat secret servic princip updat authent option aml workspac python code fail error messag state request author access data user unsur need resolv issu"
    },
    {
        "Question_title":"MLflow unfinished experiment saved as finished",
        "Question_body":"<p>when I create a run using <code>mlflow.start_run()<\/code> ,even if my script is interrupted before executing <code>mlflow.end_run()<\/code>, the run gets tagged as finished instead of unfinished in Status?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1618197962217,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":332.0,
        "Poster_created_time":1578750761196,
        "Poster_location":null,
        "Poster_reputation_count":3.0,
        "Poster_view_count":4.0,
        "Answer_body":"<p>When your notebook stops the run gets the status finished. However, if you want to continue logging metrics or artifacts to that run, you just need to use <code>mlflow.start_run(run_id=&quot;YourRunIDYouCanGetItFromUI&quot;)<\/code>. This is explained in the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1618223603527,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1522794798772,
        "Answerer_location":null,
        "Answerer_reputation_count":157.0,
        "Answerer_view_count":34.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67052295",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unfinish experi save finish creat run start run script interrupt execut end run run get tag finish instead unfinish statu",
        "Question_preprocessed_content":"unfinish experi save finish creat run script execut run get finish instead unfinish statu",
        "Question_gpt_summary_original":"The user is facing an issue where an MLflow experiment run, created using \"mlflow.start_run()\", is getting tagged as finished even if the script is interrupted before executing \"mlflow.end_run()\".",
        "Question_gpt_summary":"user face issu experi run creat start run get tag finish script interrupt execut end run"
    },
    {
        "Question_title":"Can I integrate DVC with perforce?",
        "Question_body":"<p>Hello,<\/p>\n<p>I am considering DVC as a tool that could help my team better organize our work. The problem is that we use perforce for versioning our code. I have read that DVC somehow depends on Git, but I\u2019m not sure in what way. Is Git the only versioning software that DVC can work with?<\/p>\n<p>Is it possible to integrate my DVC with perforce.<\/p>\n<p>Please let me know.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1550225525646,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":988.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-i-integrate-dvc-with-perforce\/121",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"integr perforc hello consid tool help team better organ work problem us perforc version code read depend git sure wai git version softwar work possibl integr perforc let know",
        "Question_preprocessed_content":"integr perforc consid help team organ work problem us perforc version code read depend git sure wai git version softwar work integr perforc let know",
        "Question_gpt_summary_original":"The user is considering using DVC to better organize their team's work, but they currently use perforce for versioning their code. They are unsure if DVC can work with perforce and if it is possible to integrate the two.",
        "Question_gpt_summary":"user consid better organ team work current us perforc version code unsur work perforc possibl integr"
    },
    {
        "Question_title":"How to develop people count of office?",
        "Question_body":"<p>I would like to count how many persons are in 1) conference room and 2) Office for 100 persons. I would like to have bar graph for room or office occupancy.<\/p>\n\n<p>I learned from vendor like Viametrics that camera like thermal camera must be installed to roof outside of conference room to count in and out of traffic. It seem regular camera can be used or some kind of mats as well.<\/p>\n\n<p>I tried Azure Cognitive Emotion API for people counting, but it is reliable only if camera sees whole face. Result of experiment was that we cannot use Emotion API.<\/p>\n\n<p>I would like to know that if I had camera and RaspBerry(or Arduino) plus Azure or Bluemix or VM, how to build people counting solution. Do you know any library or API? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1483287450873,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":426.0,
        "Poster_created_time":1478251050692,
        "Poster_location":"Finland",
        "Poster_reputation_count":1519.0,
        "Poster_view_count":375.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41416908",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"develop peopl count offic like count person confer room offic person like bar graph room offic occup learn vendor like viametr camera like thermal camera instal roof outsid confer room count traffic regular camera kind mat tri azur cognit emot api peopl count reliabl camera see face result experi us emot api like know camera raspberri arduino plu azur bluemix build peopl count solut know librari api",
        "Question_preprocessed_content":"develop peopl count like count person confer person like bar graph learn vendor like viametr camera like thermal camera outsid confer count regular camera kind mat tri azur cognit emot api peopl count reliabl camera face result experi us emot api like know camera plu azur bluemix build peopl count solut know librari api",
        "Question_gpt_summary_original":"The user is seeking advice on how to develop a people counting system for a conference room and an office with a capacity of 100 people. They have explored using a thermal camera or mats to count traffic in and out of the conference room, but have had issues with using the Azure Cognitive Emotion API for people counting. They are seeking advice on how to build a people counting solution using a camera and Raspberry Pi or Arduino, along with Azure, Bluemix, or VM, and are looking for recommendations on libraries or APIs to use.",
        "Question_gpt_summary":"user seek advic develop peopl count confer room offic capac peopl explor thermal camera mat count traffic confer room issu azur cognit emot api peopl count seek advic build peopl count solut camera raspberri arduino azur bluemix look recommend librari api us"
    },
    {
        "Question_title":"Unable to access data from S3 bucket to jupyter notebook of aws sagemaker",
        "Question_body":"<p>I need to train a model on aws sagemaker. I'm unable to access data in Jupiter notebook of sagemaker from S3 bucket. My bucket name is &quot;riceleaf&quot; there are four folders in the bucket named as s1,s2,s3,s4 and each folder contains 330 images named as 1.jpg and so on. It is created in Us-east zone. Bucket is private.<\/p>\n<p>One way i did was to access the object and when i displayed the key it shows me 1.jpg and so on. But when i try to open that image it didn't work. So i think I'm unable to get exact data path.<\/p>\n<p>In my code I need exact data path since I'm doing some random data generation in the code so need to access different folders. Therefore, I need a path till bucket so i can change next folder name and image name randomly in my code.<\/p>\n<p>Please help me to so that I can access the images in the Jupiter notebook of sagemaker.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_created_time":1647841162080,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1647863096848,
        "Question_score_count":0.0,
        "Question_view_count":733.0,
        "Poster_created_time":1647840996452,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71553217",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl access data bucket jupyt notebook need train model unabl access data jupit notebook bucket bucket riceleaf folder bucket name folder contain imag name jpg creat east zone bucket privat wai access object displai kei show jpg try open imag work think unabl exact data path code need exact data path random data gener code need access differ folder need path till bucket chang folder imag randomli code help access imag jupit notebook",
        "Question_preprocessed_content":"unabl data bucket jupyt train model unabl data jupit bucket bucket riceleaf folder bucket name folder contain imag name creat zone bucket privat wai object displai kei show try open imag work think unabl exact data path code exact data path random data gener code folder path bucket chang folder imag randomli code help imag jupit",
        "Question_gpt_summary_original":"The user is facing challenges in accessing data from an S3 bucket to a Jupyter notebook of AWS Sagemaker. The bucket contains four folders with 330 images each, and the user needs to access different folders for random data generation in their code. The user has tried accessing the object, but it did not work, and they need help in getting the exact data path to access the images in the Jupyter notebook.",
        "Question_gpt_summary":"user face challeng access data bucket jupyt notebook bucket contain folder imag user need access differ folder random data gener code user tri access object work need help get exact data path access imag jupyt notebook"
    },
    {
        "Question_title":"How to stop weights & biases (wandb) from creating random tmp files?",
        "Question_body":"<p>I have a million tmp files due to wandb on my home folder. I don\u2019t know why. Why are they being created &amp; how do I stop it?<\/p>\n<pre><code class=\"lang-auto\">anaconda\t\t\t\t\t   tmpa0wf77f_wandb-artifacts  tmpmv2ewoi1wandb-media\nanaconda.sh\t\t\t\t\t   tmpa3t655lewandb-media      tmpmvhp3i6ewandb-media\ndata\t\t\t\t\t\t   tmpa6roiz80wandb\t       tmpmwszwea4\ndebug-cli.brando9.log\t\t\t\t   tmpal7kticfwandb-artifacts  tmpn0p49w1hwandb-media\ndiversity-for-predictive-success-of-meta-learning  tmpalsnd4g1wandb-media      tmpnbxtnojbwandb-artifacts\ndiv_install_miniconda\t\t\t\t   tmpambb3rm9wandb-artifacts  tmpngpy96dawandb-artifacts\ndiv_install.out\t\t\t\t\t   tmpapco0xetwandb-media      tmpnqezeggnwandb-media\niit-term-synthesis\t\t\t\t   tmpaqf80v_hwandb-media      tmpo3cug5nzwandb-media\nmain.sh.e449240\t\t\t\t\t   tmpaqigpze6wandb-artifacts  tmpoc4x6l22wandb\nmain.sh.e457075\t\t\t\t\t   tmpaw1kvgtgwandb-media      tmpofql583uwandb-media\nmain.sh.e760266\t\t\t\t\t   tmpay63rbxgwandb-media      tmponqiggzswandb-artifacts\nmain.sh.err748250\t\t\t\t   tmpb2clycf5\t\t       tmpoqem6uclwandb-media\nmain.sh.err849450\t\t\t\t   tmpbbrfn_kmwandb-artifacts  tmpoqth0mgpwandb-media\nmain.sh.err923818\t\t\t\t   tmpbcxatqdiwandb-artifacts  tmppbd5bfm_wandb\nmain.sh.err962904\t\t\t\t   tmpbgewkz10wandb\t       tmppbnpm41gwandb-media\nmain.sh.o449240\t\t\t\t\t   tmpbsd96o99wandb-media      tmppwxmebn1\nmain.sh.o457075\t\t\t\t\t   tmpbtlp8zomwandb\t       tmpq396kfo1wandb-artifacts\nmain.sh.o748250\t\t\t\t\t   tmpby3a9u8ywandb\t       tmpq8jryat0wandb-media\nmain.sh.o760266\t\t\t\t\t   tmpc45e2nlxwandb-media      tmpqc65bfs0wandb\nmain.sh.o849450\t\t\t\t\t   tmpc4m5b21_\t\t       tmpqexdhp6gwandb-artifacts\nmain.sh.o923818\t\t\t\t\t   tmpcap20jmdwandb-media      tmpqh3uu7v2wandb-media\nmain.sh.o950686\t\t\t\t\t   tmpcl2sb6j_wandb\t       tmpqh99a72vwandb-media\nmain.sh.o962904\t\t\t\t\t   tmpcsncx8x4wandb-media      tmpqmim4sxywandb\nminiconda\t\t\t\t\t   tmpd7dhluxmwandb\t       tmpqpfcq9uwwandb\nminiconda.sh\t\t\t\t\t   tmpdbbb3hw_wandb-artifacts  tmpqtds4jdiwandb-artifacts\nnohup.out\t\t\t\t\t   tmpdfrjyk90wandb-media      tmp_qz8pu0xwandb-artifacts\nnohup.out449240\t\t\t\t\t   tmpdhqwaxygwandb\t       tmpr98qj7auwandb\nnohup.out457075\t\t\t\t\t   tmpdpj3bfz0wandb-artifacts  tmprfwooa22wandb-artifacts\nnohup.out760266\t\t\t\t\t   tmpdqzzy7v3\t\t       tmpri9xu8i_wandb-media\npycoq\t\t\t\t\t\t   tmpdr6fbpctwandb\t       tmprj4g0kkhwandb\ntest.py\t\t\t\t\t\t   tmpejwo7axlwandb\t       tmp_rla0cb9wandb-media\ntmp\t\t\t\t\t\t   tmpekqp7b2dwandb-media      tmprmrasn0fwandb-media\ntmp03kmjan0wandb\t\t\t\t   tmpf3pk0_3t\t\t       tmpr_yrhzj_wandb\ntmp07zhon11wandb-media\t\t\t\t   tmpf4w8yhsswandb-media      tmprzxltg0lwandb\ntmp0pkwjwg8wandb\t\t\t\t   tmpf_6vd6hkwandb-media      tmps0beul64wandb-media\ntmp0ypuhnktwandb-media\t\t\t\t   tmpf7vuwlipwandb\t       tmps5qf0_w0wandb\ntmp0zk3_ok1wandb\t\t\t\t   tmpfc8ltujrwandb-media      tmpsp2djjg6wandb-artifacts\ntmp14xa24j_wandb\t\t\t\t   tmpfmcmwgb8\t\t       tmpsqe0vylnwandb\ntmp1f3gqdq1wandb-media\t\t\t\t   tmpfqhl6c9vwandb\t       tmpstniop3twandb-media\ntmp1hmrx3xnwandb\t\t\t\t   tmpfvkvyklpwandb-media      tmpsv3n4fi7wandb-media\ntmp1nxq8dmowandb\t\t\t\t   tmpfxuc2zwjwandb-artifacts  tmp_t3mkuy4\ntmp1r2xah97wandb-media\t\t\t\t   tmpg051c49z\t\t       tmptb0urf26wandb\ntmp1sdb3vnqwandb-media\t\t\t\t   tmpg16e6zpxwandb-media      tmptgq1h308wandb-media\ntmp1wq9i7tmwandb-media\t\t\t\t   tmpg2qfjo5pwandb-artifacts  tmpthtghn1wwandb-media\ntmp27k3evykwandb-artifacts\t\t\t   tmpg34wt2g1wandb-media      tmptkp9qpgxwandb-media\ntmp2ncmg9jmwandb-media\t\t\t\t   tmpggaltim9wandb-media      tmptqn9w7rawandb-artifacts\ntmp2qxmugpjwandb-media\t\t\t\t   tmpgj6gyqw6wandb-media      tmptsqb0lwrwandb\ntmp2w92xlzowandb\t\t\t\t   tmpgpv_1hxk\t\t       tmptub9i1zzwandb-media\ntmp39lds7tywandb-media\t\t\t\t   tmpgswv7jpn\t\t       tmpu0k6cuycwandb-media\ntmp3ncj9tdewandb-artifacts\t\t\t   tmpgvz0_o1h\t\t       tmpu6uv_y0pwandb\ntmp3qlpfrylwandb-media\t\t\t\t   tmpgyarr2jxwandb\t       tmpumz7hmaiwandb-artifacts\ntmp3snbanfnwandb\t\t\t\t   tmph6m9dpa_wandb\t       tmpun08cdmwwandb-artifacts\ntmp3xrxd920wandb-artifacts\t\t\t   tmph8n3b36swandb-media      tmp_uqnbz5n\ntmp3zmnx6jxwandb-artifacts\t\t\t   tmphddkq3_3wandb\t       tmpurv7_fe2wandb\ntmp4103eum2wandb\t\t\t\t   tmphmva83y4wandb\t       tmpuwoxzzfvwandb-media\ntmp421qmhu3wandb\t\t\t\t   tmphs6erdxrwandb-media      tmpvb5bk2js\ntmp48khxd0nwandb-artifacts\t\t\t   tmphshrf9juwandb-artifacts  tmpvd_wklrtwandb\ntmp49fv73y2wandb-media\t\t\t\t   tmpi31q87a0wandb-artifacts  tmpvg_71vtdwandb-media\ntmp49sad_g1wandb-artifacts\t\t\t   tmpiu05wr2_wandb\t       tmpvlxyr3eawandb-media\ntmp4c4800_xwandb-media\t\t\t\t   tmpivnhmojfwandb\t       tmpvqmyjo4pwandb-media\ntmp4clbe6xvwandb-media\t\t\t\t   tmpj16iv0rbwandb-media      tmpw10pvrxxwandb-media\ntmp4nuizjduwandb-media\t\t\t\t   tmpj4nmef2_wandb-media      tmpw8eaus7xwandb-media\ntmp5aiik94rwandb-media\t\t\t\t   tmpj6k4pajlwandb-artifacts  tmpw97zp6pqwandb-media\ntmp5jusc1czwandb-media\t\t\t\t   tmpjetcrm92wandb-media      tmpwkzzglljwandb-media\ntmp5ks7vxpqwandb\t\t\t\t   tmp_jfnbfwcwandb-artifacts  tmpwlpoppuwwandb-media\ntmp5ss5gfoqwandb-media\t\t\t\t   tmpjhcfo3sjwandb-media      tmpwok9yxtqwandb-media\ntmp61l257guwandb-media\t\t\t\t   tmpjhkja0n4wandb-media      tmpwqbb7793wandb\ntmp66a_30crwandb\t\t\t\t   tmpjq3bc0iywandb-media      tmpwu7oid1swandb-media\ntmp6_95ss09\t\t\t\t\t   tmpjseq6pjrwandb\t       tmpwwmlqm3gwandb-artifacts\ntmp6eb3e1v_wandb-artifacts\t\t\t   tmpjywyihxswandb\t       tmpwys0txyz\ntmp6ev3bw0kwandb-media\t\t\t\t   tmpk7eb9cxxwandb-artifacts  tmpx0i8_uxdwandb-media\ntmp6j_pagmjwandb-media\t\t\t\t   tmpki8mvo7pwandb\t       tmpxby6g44swandb-media\ntmp6uz84wzpwandb\t\t\t\t   tmpkiqc2rxywandb-media      tmpxdsg3tk8wandb-artifacts\ntmp7dmpqrecwandb\t\t\t\t   tmpklsmildcwandb-media      tmpxm2j1915wandb\ntmp7fzpg3pjwandb-artifacts\t\t\t   tmpkvhsusnzwandb-artifacts  tmp_xmydpcnwandb-media\ntmp7iafm3cywandb-media\t\t\t\t   tmpkvt13pjiwandb\t       tmpxpj1qkhnwandb-media\ntmp7m0tkcx7wandb\t\t\t\t   tmpkxhoutmnwandb\t       tmpxqnwoio_wandb-media\ntmp7p7ko5c1\t\t\t\t\t   tmpl32i_q8cwandb-artifacts  tmpy3sbukw0wandb-artifacts\ntmp7xmnpnjxwandb-media\t\t\t\t   tmpldwit_dswandb-media      tmpy4tqgd9q\ntmp80lef2dvwandb\t\t\t\t   tmplf9oolt5wandb\t       tmpy5mlqvf2\ntmp89e0j4bjwandb-artifacts\t\t\t   tmplgmiofgnwandb-artifacts  tmpy5y0mxbrwandb\ntmp8h7rchd9wandb-artifacts\t\t\t   tmplw1n5b69wandb-media      tmpydoskv75wandb\ntmp8l4njuz2\t\t\t\t\t   tmplx9285iywandb\t       tmpyx791iakwandb-media\ntmp8lxb4u_0wandb\t\t\t\t   tmp_lzx3b9dwandb-media      tmpyy2hv95pwandb-artifacts\ntmp8lyo8smzwandb\t\t\t\t   tmpm1c4zy4twandb-artifacts  tmpz0gx4ikiwandb-media\ntmp8q4h8lu7wandb-artifacts\t\t\t   tmpm2755ginwandb-artifacts  tmpz26cajmh\ntmp_8uvnuf2wandb\t\t\t\t   tmpm56u1aa5wandb-media      tmpz5s198hnwandb-artifacts\ntmp96o0qfii\t\t\t\t\t   tmpm9gk_r6swandb-media      tmpz6oxqu4vwandb-media\ntmp974f9ciawandb-media\t\t\t\t   tmpm_9gv20owandb-media      tmpzgz2lbnmwandb\ntmp98ec7tz8wandb-media\t\t\t\t   tmpmdak3eqkwandb-media      tmpzyqel_hcwandb-artifacts\ntmp9i4bx28vwandb\t\t\t\t   tmpmekovp_5wandb-artifacts  tmpzzjlqqh8wandb-media\ntmp9l8xrnlqwandb-media\t\t\t\t   tmp_mpucxdiwandb-artifacts  ultimate-utils\ntmp9y_56adfwandb-media\t\t\t\t   tmpmsn3sy8mwandb-artifacts  wandb\n<\/code><\/pre>\n<h3>\n<a name=\"additional-files-1\" class=\"anchor\" href=\"#additional-files-1\"><\/a>Additional Files<\/h3>\n<p><em>No response<\/em><\/p>\n<h3>\n<a name=\"environment-2\" class=\"anchor\" href=\"#environment-2\"><\/a>Environment<\/h3>\n<p>WandB version:<br>\n(metalearning_gpu) brando9~ $ python<br>\nPython 3.9.13 (main, Oct 13 2022, 21:15:33)<br>\n[GCC 11.2.0] :: Anaconda, Inc. on linux<br>\nType \u201chelp\u201d, \u201ccopyright\u201d, \u201ccredits\u201d or \u201clicense\u201d for more information.<\/p>\n<blockquote>\n<blockquote>\n<blockquote>\n<p>import wandb<br>\nwandb.<strong>version<\/strong><br>\n\u20180.13.5\u2019<\/p>\n<\/blockquote>\n<\/blockquote>\n<\/blockquote>\n<p>OS: ubuntu\/linux<\/p>\n<p>(metalearning_gpu) brando9~ $ cat \/etc\/os-release<br>\nNAME=\u201cUbuntu\u201d<br>\nVERSION=\u201c16.04.7 LTS (Xenial Xerus)\u201d<br>\nID=ubuntu<br>\nID_LIKE=debian<br>\nPRETTY_NAME=\u201cUbuntu 16.04.7 LTS\u201d<br>\nVERSION_ID=\u201c16.04\u201d<br>\nHOME_URL=\u201c<a href=\"http:\/\/www.ubuntu.com\/\" rel=\"noopener nofollow ugc\">http:\/\/www.ubuntu.com\/<\/a>\u201d<br>\nSUPPORT_URL=\u201c<a href=\"http:\/\/help.ubuntu.com\/\" rel=\"noopener nofollow ugc\">http:\/\/help.ubuntu.com\/<\/a>\u201d<br>\nBUG_REPORT_URL=\u201c<a href=\"http:\/\/bugs.launchpad.net\/ubuntu\/\" rel=\"noopener nofollow ugc\">http:\/\/bugs.launchpad.net\/ubuntu\/<\/a>\u201d<br>\nVERSION_CODENAME=xenial<br>\nUBUNTU_CODENAME=xenial<\/p>\n<p>Python version: 3.9.13<\/p>\n<p>Versions of relevant libraries:<\/p>\n<hr>\n<p>related:<\/p>\n<ul>\n<li>cross: <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4535\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: Random tmp files being made -- why? \u00b7 Issue #4535 \u00b7 wandb\/wandb \u00b7 GitHub<\/a>\n<\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/74566670\/how-to-stop-weights-biases-wandb-from-creating-random-tmp-files\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">machine learning - How to stop weights &amp; biases (wandb) from creating random tmp files? - Stack Overflow<\/a><\/li>\n<\/ul>",
        "Question_answer_count":7,
        "Question_comment_count":null,
        "Question_created_time":1669328009419,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":655.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-stop-weights-biases-wandb-from-creating-random-tmp-files\/3460",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"stop creat random tmp file million tmp file home folder dont know creat stop anaconda tmpawff artifact tmpmvewoi media anaconda tmpatl media tmpmvhpie media data tmparoiz tmpmwszwea debug cli brando log tmpalkticf artifact tmpnpwh media divers predict success meta learn tmpalsndg media tmpnbxtnojb artifact div instal miniconda tmpambbrm artifact tmpngpyda artifact div instal tmpapcoxet media tmpnqezeggn media iit term synthesi tmpaqfv media tmpocugnz media main tmpaqigpz artifact tmpocxl main tmpawkvgtg media tmpofqlu media main tmpayrbxg media tmponqiggz artifact main err tmpbclycf tmpoqemucl media main err tmpbbrfn artifact tmpoqthmgp media main err tmpbcxatqdi artifact tmppbdbfm main err tmpbgewkz tmppbnpmg media main tmpbsdo media tmppwxmebn main tmpbtlpzom tmpqkfo artifact main tmpbyaui tmpqjryat media main tmpcenlx media tmpqcbf main tmpcmb tmpqexdhpg artifact main tmpcapjmd media tmpqhuuv media main tmpclsbj tmpqhav media main tmpcsncxx media tmpqmimsxi miniconda tmpddhluxm tmpqpfcquw miniconda tmpdbbbhw artifact tmpqtdsjdi artifact nohup tmpdfrjyk media tmp qzpux artifact nohup tmpdhqwaxyg tmprqjau nohup tmpdpjbfz artifact tmprfwooa artifact nohup tmpdqzzyv tmprixui media pycoq tmpdrfbpct tmprjgkkh test tmpejwoaxl tmp rlacb media tmp tmpekqpbd media tmprmrasnf media tmpkmjan tmpfpk tmpr yrhzj tmpzhon media tmpfwyhss media tmprzxltgl tmppkwjwg tmpf vdhk media tmpsbeul media tmpypuhnkt media tmpfvuwlip tmpsqf tmpzk tmpfcltujr media tmpspdjjg artifact tmpxaj tmpfmcmwgb tmpsqevyln tmpfgqdq media tmpfqhlcv tmpstniopt media tmphmrxxn tmpfvkvyklp media tmpsvnfi media tmpnxqdmo tmpfxuczwj artifact tmp tmkui tmprxah media tmpgcz tmptburf tmpsdbvnq media tmpgezpx media tmptgqh media tmpwqitm media tmpgqfjop artifact tmpthtghnw media tmpkevyk artifact tmpgwtg media tmptkpqpgx media tmpncmgjm media tmpggaltim media tmptqnwra artifact tmpqxmugpj media tmpgjgyqw media tmptsqblwr tmpwxlzo tmpgpv hxk tmptubizz media tmpldsty media tmpgswvjpn tmpukcuyc media tmpncjtde artifact tmpgvz tmpuuv tmpqlpfryl media tmpgyarrjx tmpumzhmai artifact tmpsnbanfn tmphmdpa tmpuncdmw artifact tmpxrxd artifact tmphnb media tmp uqnbzn tmpzmnxjx artifact tmphddkq tmpurv tmpeum tmphmvai tmpuwoxzzfv media tmpqmhu tmphserdxr media tmpvbbkj tmpkhxdn artifact tmphshrfju artifact tmpvd wklrt tmpfvy media tmpiqa artifact tmpvg vtd media tmpsad artifact tmpiuwr tmpvlxyrea media tmpc media tmpivnhmojf tmpvqmyjop media tmpclbexv media tmpjivrb media tmpwpvrxx media tmpnuizjdu media tmpjnmef media tmpweausx media tmpaiikr media tmpjkpajl artifact tmpwzppq media tmpjusccz media tmpjetcrm media tmpwkzzgllj media tmpksvxpq tmp jfnbfwc artifact tmpwlpoppuw media tmpssgfoq media tmpjhcfosj media tmpwokyxtq media tmplgu media tmpjhkjan media tmpwqbb tmpa tmpjqbcii media tmpwuoid media tmp tmpjseqpjr tmpwwmlqmg artifact tmpebev artifact tmpjywyihx tmpwystxyz tmpevbwk media tmpkebcxx artifact tmpxi uxd media tmpj pagmj media tmpkimvop tmpxbyg media tmpuzwzp tmpkiqcrxi media tmpxdsgtk artifact tmpdmpqrec tmpklsmildc media tmpxmj tmpfzpgpj artifact tmpkvhsusnz artifact tmp xmydpcn media tmpiafmci media tmpkvtpji tmpxpjqkhn media tmpmtkcx tmpkxhoutmn tmpxqnwoio media tmppkoc tmpli artifact tmpysbukw artifact tmpxmnpnjx media tmpldwit media tmpytqgdq tmplefdv tmplfoolt tmpymlqvf tmpejbj artifact tmplgmiofgn artifact tmpyymxbr tmphrchd artifact tmplwnb media tmpydoskv tmplnjuz tmplxii tmpyxiak media tmplxbu tmp lzxbd media tmpyyhvp artifact tmplyosmz tmpmczyt artifact tmpzgxiki media tmpqhlu artifact tmpmgin artifact tmpzcajmh tmp uvnuf tmpmuaa media tmpzshn artifact tmpoqfii tmpmgk media tmpzoxquv media tmpfcia media tmpm gvo media tmpzgzlbnm tmpectz media tmpmdakeqk media tmpzyqel artifact tmpibxv tmpmekovp artifact tmpzzjlqqh media tmplxrnlq media tmp mpucxdi artifact ultim util tmpy adf media tmpmsnsym artifact addit file respons environ version metalearn gpu brando python python main oct gcc anaconda linux type help copyright credit licens inform import version ubuntu linux metalearn gpu brando cat releas ubuntu version lt xenial xeru ubuntu like debian pretti ubuntu lt version home url http www ubuntu com support url http help ubuntu com bug report url http bug launchpad net ubuntu version codenam xenial ubuntu codenam xenial python version version relev librari relat cross cli random tmp file issu github machin learn stop creat random tmp file stack overflow",
        "Question_preprocessed_content":"stop creat random tmp file tmp file home folder dont know creat stop file respons environ version brando python python anaconda linux type help copyright credit licens inform import version brando cat ubuntu lt ubuntu lt python version version relev librari relat random tmp file github machin learn stop creat random tmp file stack overflow",
        "Question_gpt_summary_original":"The user is facing an issue where WandB is creating a large number of random tmp files in their home folder and they are unsure why this is happening. They are seeking advice on how to stop this from happening.",
        "Question_gpt_summary":"user face issu creat larg number random tmp file home folder unsur happen seek advic stop happen"
    },
    {
        "Question_title":"How can I use the predictive experiment of Azure ML on my website",
        "Question_body":"<p>I used Azure ML studio and I obtained a predictive experiment with an API key.<\/p>\n\n<p><strong>My question is:<\/strong><br>\nHow can I embed this API key or the text obtained or the predictive experiment on a website or on my portfolio?<br>\nSo that others can use the predictive experiment from the website...<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1524664856743,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1550566774003,
        "Question_score_count":1.0,
        "Question_view_count":28.0,
        "Poster_created_time":1524663632880,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50024278",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us predict experi websit studio obtain predict experi api kei question emb api kei text obtain predict experi websit portfolio us predict experi websit",
        "Question_preprocessed_content":"us predict experi websit studio obtain predict experi api kei question emb api kei text obtain predict experi websit portfolio us predict experi",
        "Question_gpt_summary_original":"The user has created a predictive experiment using Azure ML studio and obtained an API key. They are now seeking guidance on how to embed the API key or the text obtained or the predictive experiment on a website or portfolio so that others can use it.",
        "Question_gpt_summary":"user creat predict experi studio obtain api kei seek guidanc emb api kei text obtain predict experi websit portfolio us"
    },
    {
        "Question_title":"Sagemaker Studio UnkownServiceError for Session",
        "Question_body":"<p>When I run the following code in Sagemaker Studio, I get the <code>UnknownServiceError<\/code>:<\/p>\n<pre><code>import boto3\nimport sagemaker\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n<\/code><\/pre>\n<p>Any advice to overcome this would be greatly appreciated.<\/p>\n<p>The error message in the following:<\/p>\n<pre><code>---------------------------------------------------------------------------\nUnknownServiceError                       Traceback (most recent call last)\n&lt;ipython-input-47-2d2ae2d1e577&gt; in &lt;module&gt;\n      1 import boto3\n      2 import sagemaker\n----&gt; 3 sagemaker_session = sagemaker.Session()\n      4 role = get_execution_role()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in __init__(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client, default_bucket)\n    124             sagemaker_client=sagemaker_client,\n    125             sagemaker_runtime_client=sagemaker_runtime_client,\n--&gt; 126             sagemaker_featurestore_runtime_client=sagemaker_featurestore_runtime_client,\n    127         )\n    128 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _initialize(self, boto_session, sagemaker_client, sagemaker_runtime_client, sagemaker_featurestore_runtime_client)\n    164         else:\n    165             self.sagemaker_featurestore_runtime_client = self.boto_session.client(\n--&gt; 166                 &quot;sagemaker-featurestore-runtime&quot;\n    167             )\n    168 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/boto3\/session.py in client(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)\n    261             aws_access_key_id=aws_access_key_id,\n    262             aws_secret_access_key=aws_secret_access_key,\n--&gt; 263             aws_session_token=aws_session_token, config=config)\n    264 \n    265     def resource(self, service_name, region_name=None, api_version=None,\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/session.py in create_client(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)\n    833             is_secure=use_ssl, endpoint_url=endpoint_url, verify=verify,\n    834             credentials=credentials, scoped_config=self.get_scoped_config(),\n--&gt; 835             client_config=config, api_version=api_version)\n    836         monitor = self._get_internal_component('monitor')\n    837         if monitor is not None:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in create_client(self, service_name, region_name, is_secure, endpoint_url, verify, credentials, scoped_config, api_version, client_config)\n     76             'choose-service-name', service_name=service_name)\n     77         service_name = first_non_none_response(responses, default=service_name)\n---&gt; 78         service_model = self._load_service_model(service_name, api_version)\n     79         cls = self._create_client_class(service_name, service_model)\n     80         endpoint_bridge = ClientEndpointBridge(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/client.py in _load_service_model(self, service_name, api_version)\n    114     def _load_service_model(self, service_name, api_version=None):\n    115         json_model = self._loader.load_service_model(service_name, 'service-2',\n--&gt; 116                                                      api_version=api_version)\n    117         service_model = ServiceModel(json_model, service_name=service_name)\n    118         return service_model\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/loaders.py in _wrapper(self, *args, **kwargs)\n    130         if key in self._cache:\n    131             return self._cache[key]\n--&gt; 132         data = func(self, *args, **kwargs)\n    133         self._cache[key] = data\n    134         return data\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/loaders.py in load_service_model(self, service_name, type_name, api_version)\n    376             raise UnknownServiceError(\n    377                 service_name=service_name,\n--&gt; 378                 known_service_names=', '.join(sorted(known_services)))\n    379         if api_version is None:\n    380             api_version = self.determine_latest_version(\n\nUnknownServiceError: Unknown service: 'sagemaker-featurestore-runtime'. Valid service names are: accessanalyzer, acm, acm-pca, alexaforbusiness, amplify, apigateway, apigatewaymanagementapi, apigatewayv2, appconfig, application-autoscaling, application-insights, appmesh, appstream, appsync, athena, autoscaling, autoscaling-plans, backup, batch, braket, budgets, ce, chime, cloud9, clouddirectory, cloudformation, cloudfront, cloudhsm, cloudhsmv2, cloudsearch, cloudsearchdomain, cloudtrail, cloudwatch, codeartifact, codebuild, codecommit, codedeploy, codeguru-reviewer, codeguruprofiler, codepipeline, codestar, codestar-connections, codestar-notifications, cognito-identity, cognito-idp, cognito-sync, comprehend, comprehendmedical, compute-optimizer, config, connect, connectparticipant, cur, dataexchange, datapipeline, datasync, dax, detective, devicefarm, directconnect, discovery, dlm, dms, docdb, ds, dynamodb, dynamodbstreams, ebs, ec2, ec2-instance-connect, ecr, ecs, efs, eks, elastic-inference, elasticache, elasticbeanstalk, elastictranscoder, elb, elbv2, emr, es, events, firehose, fms, forecast, forecastquery, frauddetector, fsx, gamelift, glacier, globalaccelerator, glue, greengrass, groundstation, guardduty, health, honeycode, iam, imagebuilder, importexport, inspector, iot, iot-data, iot-jobs-data, iot1click-devices, iot1click-projects, iotanalytics, iotevents, iotevents-data, iotsecuretunneling, iotsitewise, iotthingsgraph, ivs, kafka, kendra, kinesis, kinesis-video-archived-media, kinesis-video-media, kinesis-video-signaling, kinesisanalytics, kinesisanalyticsv2, kinesisvideo, kms, lakeformation, lambda, lex-models, lex-runtime, license-manager, lightsail, logs, machinelearning, macie, macie2, managedblockchain, marketplace-catalog, marketplace-entitlement, marketplacecommerceanalytics, mediaconnect, mediaconvert, medialive, mediapackage, mediapackage-vod, mediastore, mediastore-data, mediatailor, meteringmarketplace, mgh, migrationhub-config, mobile, mq, mturk, neptune, networkmanager, opsworks, opsworkscm, organizations, outposts, personalize, personalize-events, personalize-runtime, pi, pinpoint, pinpoint-email, pinpoint-sms-voice, polly, pricing, qldb, qldb-session, quicksight, ram, rds, rds-data, redshift, rekognition, resource-groups, resourcegroupstaggingapi, robomaker, route53, route53domains, route53resolver, s3, s3control, sagemaker, sagemaker-a2i-runtime, sagemaker-runtime, savingsplans, schemas, sdb, secretsmanager, securityhub, serverlessrepo, service-quotas, servicecatalog, servicediscovery, ses, sesv2, shield, signer, sms, sms-voice, snowball, sns, sqs, ssm, sso, sso-oidc, stepfunctions, storagegateway, sts, support, swf, synthetics, textract, transcribe, transfer, translate, waf, waf-regional, wafv2, workdocs, worklink, workmail, workmailmessageflow, workspaces, xray\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1607680355900,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":6.0,
        "Question_view_count":387.0,
        "Poster_created_time":1604297242580,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65249110",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"studio unkownserviceerror session run follow code studio unknownserviceerror import boto import session session role execut role advic overcom greatli appreci error messag follow unknownserviceerror traceback recent import boto import session session role execut role opt conda lib python site packag session init self boto session client runtim client featurestor runtim client default bucket client client runtim client runtim client featurestor runtim client featurestor runtim client opt conda lib python site packag session initi self boto session client runtim client featurestor runtim client self featurestor runtim client self boto session client featurestor runtim opt conda lib python site packag boto session client self servic region api version us ssl verifi endpoint url aw access kei aw secret access kei aw session token config aw access kei aw access kei aw secret access kei aw secret access kei aw session token aw session token config config def resourc self servic region api version opt conda lib python site packag botocor session creat client self servic region api version us ssl verifi endpoint url aw access kei aw secret access kei aw session token config secur us ssl endpoint url endpoint url verifi verifi credenti credenti scope config self scope config client config config api version api version monitor self intern compon monitor monitor opt conda lib python site packag botocor client creat client self servic region secur endpoint url verifi credenti scope config api version client config choos servic servic servic servic non respons respons default servic servic model self load servic model servic api version cl self creat client class servic servic model endpoint bridg clientendpointbridg opt conda lib python site packag botocor client load servic model self servic api version def load servic model self servic api version json model self loader load servic model servic servic api version api version servic model servicemodel json model servic servic return servic model opt conda lib python site packag botocor loader wrapper self arg kwarg kei self cach return self cach kei data func self arg kwarg self cach kei data return data opt conda lib python site packag botocor loader load servic model self servic type api version rais unknownserviceerror servic servic known servic name join sort known servic api version api version self determin latest version unknownserviceerror unknown servic featurestor runtim valid servic name accessanalyz acm acm pca alexaforbusi amplifi apigatewai apigatewaymanagementapi apigatewayv appconfig applic autosc applic insight appmesh appstream appsync athena autosc autosc plan backup batch braket budget chime cloud clouddirectori cloudform cloudfront cloudhsm cloudhsmv cloudsearch cloudsearchdomain cloudtrail cloudwatch codeartifact codebuild codecommit codedeploi codeguru review codeguruprofil codepipelin codestar codestar connect codestar notif cognito ident cognito idp cognito sync comprehend comprehendmed comput optim config connect connectparticip cur dataexchang datapipelin datasync dax detect devicefarm directconnect discoveri dlm dm docdb dynamodb dynamodbstream eb instanc connect ecr ec ef ek elast infer elasticach elasticbeanstalk elastictranscod elb elbv emr event firehos fm forecast forecastqueri frauddetector fsx gamelift glacier globalacceler glue greengrass groundstat guardduti health honeycod iam imagebuild importexport inspector iot iot data iot job data iotclick devic iotclick project iotanalyt iotev iotev data iotsecuretunnel iotsitewis iotthingsgraph iv kafka kendra kinesi kinesi video archiv media kinesi video media kinesi video signal kinesisanalyt kinesisanalyticsv kinesisvideo km lakeform lambda lex model lex runtim licens manag lightsail log machinelearn maci maci managedblockchain marketplac catalog marketplac entitl marketplacecommerceanalyt mediaconnect mediaconvert medial mediapackag mediapackag vod mediastor mediastor data mediatailor meteringmarketplac mgh migrationhub config mobil mturk networkmanag opswork opsworkscm organ outpost person person event person runtim pinpoint pinpoint email pinpoint sm voic polli price qldb qldb session quicksight ram rd rd data redshift rekognit resourc group resourcegroupstaggingapi robomak rout routedomain routeresolv scontrol runtim runtim savingsplan schema sdb secretsmanag securityhub serverlessrepo servic quota servicecatalog servicediscoveri se sesv shield signer sm sm voic snowbal sn sq ssm sso sso oidc stepfunct storagegatewai st support swf synthet textract transcrib transfer translat waf waf region wafv workdoc worklink workmail workmailmessageflow workspac xrai",
        "Question_preprocessed_content":"studio run code studio advic overcom greatli",
        "Question_gpt_summary_original":"The user is encountering an UnknownServiceError when running code in Sagemaker Studio. The error message indicates that the service 'sagemaker-featurestore-runtime' is unknown, and provides a list of valid service names. The user is seeking advice on how to overcome this error.",
        "Question_gpt_summary":"user encount unknownserviceerror run code studio error messag indic servic featurestor runtim unknown provid list valid servic name user seek advic overcom error"
    },
    {
        "Question_title":"How to build a data to AI solution with BigQuery and Vertex AI",
        "Question_body":"In this article, learn how to build an end-to-end data to AI solution on Google Cloud, including a practical example of a real-time fraud detection system and the architecture behind it. You'll also discover how to train, deploy, and monitor machine learning models in production.\n\nThis article is based on a recent Cloud OnBoard session.\u00a0Register here to watch on demand.\u00a0\u00a0\u00a0\n\nIf you have any questions, please leave a comment on the blog (or below) and someone from the Community or Google Cloud team will be happy to help.\n\nRead the blog",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1684756920000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":90.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Great read!\u00a0\n\nView solution in original post",
        "Answer_comment_count":null,
        "Question_closed_time":"05-23-2023 09:25 AM",
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-build-a-data-to-AI-solution-with-BigQuery-and-Vertex-AI\/td-p\/595708\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"build data solut bigqueri articl learn build end end data solut googl cloud includ practic exampl real time fraud detect architectur discov train deploi monitor machin learn model product articl base recent cloud onboard session regist watch demand question leav comment blog commun googl cloud team happi help read blog",
        "Question_preprocessed_content":"build data solut bigqueri articl learn build data solut cloud includ practic exampl fraud detect architectur discov train deploi monitor machin learn model product articl base recent cloud onboard regist watch demand question leav blog cloud team help read blog",
        "Question_gpt_summary_original":"The article discusses how to create a data to AI solution on Google Cloud, with a focus on building a real-time fraud detection system. It covers the architecture, training, deployment, and monitoring of machine learning models in production. The reader is encouraged to ask questions in the comments section for assistance.",
        "Question_gpt_summary":"articl discuss creat data solut googl cloud focu build real time fraud detect cover architectur train deploy monitor machin learn model product reader encourag ask question comment section assist"
    },
    {
        "Question_title":"Send request as Json on UWP",
        "Question_body":"<p>I have deployed an AzureML published experiment with deployed web service. I tried to use the <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-consume-web-services\/\" rel=\"nofollow\">sample code provided in the configuration page<\/a>, but universal apps do not implement Http.Formatting yet, thus I couldn't use <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/hh944521(v=vs.118).aspx\" rel=\"nofollow\">postasjsonasync<\/a>.<\/p>\n\n<p>I tried to follow the sample code as much as possible, but I'm getting statuscode of 415 \"Unsupported Media Type\", What's the mistake I'm doing?<\/p>\n\n<pre><code>var client = new HttpClient();\nclient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue(\"Bearer\", apiKey);\n\/\/ client.BaseAddress = uri;\n\nvar scoreRequest = new\n{\n            Inputs = new Dictionary&lt;string, StringTable&gt;() {\n                    {\n                        \"dataInput\",\n                        new StringTable()\n                        {\n                            ColumnNames = new [] {\"Direction\", \"meanX\", \"meanY\", \"meanZ\"},\n                            Values = new [,] {  { \"\", x.ToString(), y.ToString(), z.ToString() },  }\n                        }\n                    },\n                },\n            GlobalParameters = new Dictionary&lt;string, string&gt;() { }\n };\n var stringContent = new StringContent(scoreRequest.ToString());\n HttpResponseMessage response = await client.PostAsync(uri, stringContent);\n<\/code><\/pre>\n\n<p>Many Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1452005613750,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":3194.0,
        "Poster_created_time":1352139399460,
        "Poster_location":"Cyprus",
        "Poster_reputation_count":820.0,
        "Poster_view_count":256.0,
        "Answer_body":"<p>You'll need to serialize the object to a JSON string (I recommend using NewtonSoft.Json to make it easier) and set the content type accordingly. Here's an implementation I'm using in my UWP apps (note that <code>_client<\/code> is an <code>HttpClient<\/code>):<\/p>\n\n<pre><code>    public async Task&lt;HttpResponseMessage&gt; PostAsJsonAsync&lt;T&gt;(Uri uri, T item)\n    {\n        var itemAsJson = JsonConvert.SerializeObject(item);\n        var content = new StringContent(itemAsJson);\n        content.Headers.ContentType = new MediaTypeHeaderValue(\"application\/json\");\n\n        return await _client.PostAsync(uri, content);\n    }\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1452007973623,
        "Answer_last_edit_time":null,
        "Answer_score_count":3.0,
        "Answerer_created_time":1395422283667,
        "Answerer_location":null,
        "Answerer_reputation_count":1411.0,
        "Answerer_view_count":45.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34614582",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"send request json uwp deploi publish experi deploi web servic tri us sampl code provid configur page univers app implement http format couldn us postasjsonasync tri follow sampl code possibl get statuscod unsupport media type mistak var client new httpclient client defaultrequesthead author new authenticationheadervalu bearer apikei client baseaddress uri var scorerequest new input new dictionari datainput new stringtabl columnnam new direct meanx meani meanz valu new tostr tostr tostr globalparamet new dictionari var stringcont new stringcont scorerequest tostr httpresponsemessag respons await client postasync uri stringcont thank",
        "Question_preprocessed_content":"send request json uwp deploi publish experi deploi web servic tri us sampl code provid configur page univers implement couldn us postasjsonasync tri sampl code statuscod media type mistak thank",
        "Question_gpt_summary_original":"The user is facing challenges while trying to send a request as JSON on UWP for an AzureML published experiment with a deployed web service. The sample code provided in the configuration page is not working as universal apps do not implement Http.Formatting yet, resulting in a status code of 415 \"Unsupported Media Type\". The user is seeking help to identify the mistake in the code.",
        "Question_gpt_summary":"user face challeng try send request json uwp publish experi deploi web servic sampl code provid configur page work univers app implement http format result statu code unsupport media type user seek help identifi mistak code"
    },
    {
        "Question_title":"How to tune conditional objective function using optuna or hyperopt",
        "Question_body":"<p>I tried to use optuna to tune hyperparameters. But my objective function is conditional which creates issues in getting optimal parameters.<\/p>\n\n<p>i want to get cwc only if the condtion is met otherwise continue trial for next hyperparameters.\nBut i guess since the condition is not met and objective func reurns cwc it gives error<\/p>\n\n<p>UnboundLocalError: local variable 'cwc_train' referenced before assignment<\/p>\n\n<pre><code>define objective (trial):\n    k_dis = trial.suggest_uniform('k_dis', 0.0, 5.0)\n    l_dis = trial.suggest_uniform('l_dis', 0.0, 5.0)\n    k_bound = trial.suggest_uniform('k_bound', 0.0, 5.0)\n    l_bound = trial.suggest_uniform('l_bound', 0.0, 5.0) \n\n    picp = .....\n    pinrw = .....\n\n\n    if picp_train &gt;= 0.8 and pinrw_train &lt; 0.18: \n        cwc_train = fc.CWC_proposed(predict_bound_train, Y_train)\n    else:\n        print(\"error = \")\n    return  cwc_train\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n<\/code><\/pre>\n\n<p>UnboundLocalError: local variable 'cwc_train' referenced before assignment<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1564371554813,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1564379241136,
        "Question_score_count":1.0,
        "Question_view_count":814.0,
        "Poster_created_time":1564369929972,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57246939",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tune condit object function hyperopt tri us tune hyperparamet object function condit creat issu get optim paramet want cwc condtion met continu trial hyperparamet guess condit met object func reurn cwc give error unboundlocalerror local variabl cwc train referenc assign defin object trial di trial suggest uniform di di trial suggest uniform di bound trial suggest uniform bound bound trial suggest uniform bound picp pinrw picp train pinrw train cwc train cwc propos predict bound train train print error return cwc train studi creat studi studi optim object trial unboundlocalerror local variabl cwc train referenc assign",
        "Question_preprocessed_content":"tune condit object function hyperopt tri us tune hyperparamet object function condit creat optim paramet want cwc condtion met continu trial hyperparamet condit met object func reurn cwc give local variabl referenc local variabl referenc",
        "Question_gpt_summary_original":"The user is trying to use optuna to tune hyperparameters for an objective function that is conditional. They want to get a specific value only if a certain condition is met, otherwise, they want to continue the trial for the next set of hyperparameters. However, the code is giving an error \"UnboundLocalError: local variable 'cwc_train' referenced before assignment\" because the condition is not met and the objective function returns 'cwc_train'.",
        "Question_gpt_summary":"user try us tune hyperparamet object function condit want specif valu certain condit met want continu trial set hyperparamet code give error unboundlocalerror local variabl cwc train referenc assign condit met object function return cwc train"
    },
    {
        "Question_title":"Get older version of data files",
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying out dvc for one of my ML pipeline poc. I see dvc add command to keep track of changes in data files. How do i revert back to an older version of data files using dvc cli?<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_created_time":1522591456249,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":2828.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/get-older-version-of-data-files\/18",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"older version data file try pipelin poc add command track chang data file revert older version data file cli thank",
        "Question_preprocessed_content":"older version data file try pipelin poc track chang data file revert older version data file cli thank",
        "Question_gpt_summary_original":"The user is trying to use dvc for an ML pipeline and wants to know how to revert back to an older version of data files using dvc cli.",
        "Question_gpt_summary":"user try us pipelin want know revert older version data file cli"
    },
    {
        "Question_title":"MLflow UI not loading with Nginx after authentication",
        "Question_body":"<p>I have a RHEL CENTOS7 EC2 instance with MLflow installed. The MLflow application is running on localhost port 5000 (127.0.0.1:5000). I have set up Nginx on port 443 with PAM Authentication and the proxy_redirect is to where the MLflow server is running on localhost (127.0.0.1:5000). I have included the server directive of Nginx below. After successful authentication the webpage for MLflow just hangs. Looking at the network tab I can see that some of the source files are just in the pending status.<\/p>\n<pre><code>server {\n    listen 443 ssl;\n    server_name _;\n    ssl_certificate \/ssl\/mlflow.crt;\n    ssl_certificate_key \/ssl\/mlflow.key;\n\n    location \/ {\n         auth_pam &quot;Secure area&quot;;\n         auth_pam_service_name &quot;mlflow&quot;;\n         proxy_pass http:\/\/127.0.0.1:5000\/;\n    }\n}\n<\/code><\/pre>\n<p>Any help is appreciated on why this is happening, and please let me know if any further clarification is needed.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1638475606127,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":223.0,
        "Poster_created_time":1630175744600,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70205829",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load nginx authent rhel cento instanc instal applic run localhost port set nginx port pam authent proxi redirect server run localhost includ server direct nginx success authent webpag hang look network tab sourc file pend statu server listen ssl server ssl certif ssl crt ssl certif kei ssl kei locat auth pam secur area auth pam servic proxi pass http help appreci happen let know clarif need",
        "Question_preprocessed_content":"load nginx authent rhel cento instanc localhost port set nginx port pam authent server localhost includ server direct nginx authent webpag hang network tab sourc file pend statu help let know clarif",
        "Question_gpt_summary_original":"The user has installed MLflow on a RHEL CENTOS7 EC2 instance and set up Nginx on port 443 with PAM Authentication. However, after successful authentication, the MLflow webpage hangs and some source files are in pending status. The user has shared the server directive of Nginx and is seeking help to understand why this is happening.",
        "Question_gpt_summary":"user instal rhel cento instanc set nginx port pam authent success authent webpag hang sourc file pend statu user share server direct nginx seek help understand happen"
    },
    {
        "Question_title":"How to rename a SageMaker notebook instance?",
        "Question_body":"<p>On Amazon SageMaker, it's possible to edit most properties of a notebook instance when the instance is not active, but it does not seem possible to change its name.<\/p>\n\n<p>Is there any way to rename an existing SageMaker notebook instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1556108497957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1556115031880,
        "Question_score_count":6.0,
        "Question_view_count":1311.0,
        "Poster_created_time":1283441031492,
        "Poster_location":"Lausanne, Switzerland",
        "Poster_reputation_count":4841.0,
        "Poster_view_count":261.0,
        "Answer_body":"<p>Thank you for using Amazon SageMaker. <\/p>\n\n<p>SageMaker Notebook Instance's name cannot be edited. <\/p>\n\n<p>Thanks,<br>\nNeelam<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1556304752800,
        "Answer_last_edit_time":null,
        "Answer_score_count":4.0,
        "Answerer_created_time":1424063473423,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":334.0,
        "Answerer_view_count":347.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55829940",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"renam notebook instanc possibl edit properti notebook instanc instanc activ possibl chang wai renam exist notebook instanc",
        "Question_preprocessed_content":"renam instanc edit properti instanc instanc activ chang wai renam exist instanc",
        "Question_gpt_summary_original":"The user is facing a challenge in renaming an existing SageMaker notebook instance as it does not seem possible to change its name even though most properties of the instance can be edited when it is not active.",
        "Question_gpt_summary":"user face challeng renam exist notebook instanc possibl chang properti instanc edit activ"
    },
    {
        "Question_title":"Which azure service should I use.",
        "Question_body":"<p>So I have a pretty big on-premise ssms database and I want to use some data from it in azure Machine Learning.   <br \/>\nI need to use just a small amount of data from my db, from certain tables.  <br \/>\nAlso this data updates from time to time.  <br \/>\nI want you to help me with the choice of correct azure service for my purpose.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1633510606170,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/579427\/which-azure-service-should-i-use",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur servic us pretti big premis ssm databas want us data need us small data certain tabl data updat time time want help choic correct azur servic purpos",
        "Question_preprocessed_content":"azur servic us big databas want us data us data certain tabl data updat time time want help choic azur servic purpos",
        "Question_gpt_summary_original":"The user has a large on-premise SSMS database and wants to use a small amount of data from certain tables in Azure Machine Learning. The data updates periodically, and the user needs help in choosing the appropriate Azure service for their purpose.",
        "Question_gpt_summary":"user larg premis ssm databas want us small data certain tabl data updat period user need help choos appropri azur servic purpos"
    },
    {
        "Question_title":"delete compute instance by python sdk",
        "Question_body":"<p>hello    <\/p>\n<p>I request your help to configure the deletion of the azure machine learning compute instance after the job is finished     <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1672862465810,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1151897\/delete-compute-instance-by-python-sdk",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"delet comput instanc python sdk hello request help configur delet comput instanc job finish",
        "Question_preprocessed_content":"delet comput instanc python sdk request help configur delet comput instanc job finish",
        "Question_gpt_summary_original":"The user is seeking assistance in configuring the deletion of an Azure machine learning compute instance using Python SDK after a job is completed.",
        "Question_gpt_summary":"user seek assist configur delet comput instanc python sdk job complet"
    },
    {
        "Question_title":"sagemaker online and offline store question",
        "Question_body":"I looked at SM feature store documentation and see a flag named is_online_Enable for ingestion.\nMy queries are - \n1. If i make it false, will it only store the data in offline store ?\n2. If I make it true will it store data both in online and offline store? if yes , both the store will have same data right?\n3. Is there any way to store my data only in online or only in offline store ? if yes, what will be the configuration?\n\nAlso is there any scenario while ingesting data there is some syn happens between online and offline store or vice versa. I see documentation mentioning  it will be available after some minutes and for our use case we want to know is there any sync happens( is there any scenario where data goes to online store first and then sync up with offline store or vice versa and some latency associated with the later store)?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1642578480105,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1668568862240,
        "Question_score_count":1.0,
        "Question_view_count":519.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxxGzLRsPQk6iAlG-yEv4VQ\/sagemaker-online-and-offline-store-question",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"onlin offlin store question look featur store document flag name onlin enabl ingest queri fals store data offlin store true store data onlin offlin store ye store data right wai store data onlin offlin store ye configur scenario ingest data syn happen onlin offlin store vice versa document mention avail minut us case want know sync happen scenario data goe onlin store sync offlin store vice versa latenc associ later store",
        "Question_preprocessed_content":"onlin store question featur store document flag name ingest queri fals store data store true store data onlin store ye store data right wai store data onlin store ye configur scenario ingest data syn onlin store vice versa document mention avail minut us case want know sync",
        "Question_gpt_summary_original":"The user is seeking clarification on the functionality of the is_online_Enable flag in the SageMaker feature store documentation. They have three queries: whether setting the flag to false will only store data in the offline store, whether setting it to true will store data in both online and offline stores, and if there is a way to store data only in one of the stores. The user also wants to know if there is any synchronization between the online and offline stores during data ingestion and if there is any latency associated with the offline store.",
        "Question_gpt_summary":"user seek clarif function onlin enabl flag featur store document queri set flag fals store data offlin store set true store data onlin offlin store wai store data store user want know synchron onlin offlin store data ingest latenc associ offlin store"
    },
    {
        "Question_title":"Batch transform sparse matrix with AWS SageMaker Python SDK",
        "Question_body":"<p>I have successfully trained a Scikit-Learn LSVC model with AWS SageMaker.<br>\nI want to make batch prediction (aka. batch transform) on a relatively big dataset which is a scipy sparse matrix with shape 252772 x 185128. (The number of features is high because there is one-hot-encoding of bag-of-words and ngrams features).   <\/p>\n\n<p>I struggle because of:   <\/p>\n\n<ul>\n<li><p>the size of the data   <\/p><\/li>\n<li><p>the format of the data<\/p><\/li>\n<\/ul>\n\n<p>I did several experiments to check what was going on:    <\/p>\n\n<h3>1. predict locally on sample sparse matrix data<\/h3>\n\n<p><strong>It works<\/strong><br>\nDeserialize the model artifact locally on a SageMaker notebook and predict on a sample of the sparse matrix.<br>\nThis was just to check that the model can predict on this kind of data.<\/p>\n\n<h3>2. Batch Transform on a sample csv data<\/h3>\n\n<p><strong>It works<\/strong><br>\nLaunch a Batch Transform Job on SageMaker and request to transform a small sample in dense csv format : it works but does not scale, obviously.<br>\nThe code is:  <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_model = SKLearnModel(\n    model_data=model_artifact_location_on_s3,\n    entry_point='my_script.py',\n    role=role,\n    sagemaker_session=sagemaker_session)\n\ntransformer = sklearn_model.transformer(\n   instance_count=1, \n   instance_type='ml.m4.xlarge', \n   max_payload=100)\n\ntransformer.transform(\n   data=batch_data, \n   content_type='text\/csv',\n   split_type=None)   \n\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n\n<\/code><\/pre>\n\n<p>where:  <\/p>\n\n<ul>\n<li>'my_script.py' implements a simple <code>model_fn<\/code> to deserialize the model artifact: <\/li>\n<\/ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n    return clf\n<\/code><\/pre>\n\n<ul>\n<li><code>batch_data<\/code> is the s3 path for the csv file.  <\/li>\n<\/ul>\n\n<h3>3. Batch Transform of a sample dense numpy dataset.<\/h3>\n\n<p><strong>It works<\/strong><br>\nI prepared a sample of the data and saved it to s3 in Numpy <code>.npy<\/code> format. According to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#sagemaker-scikit-learn-model-server\" rel=\"noreferrer\">this documentation<\/a>, SageMaker Scikit-learn model server can deserialize NPY-formatted data (along with JSON and CSV data).<br>\nThe only difference with the previous experiment (2) is the argument <code>content_type='application\/x-npy'<\/code> in <code>transformer.transform(...)<\/code>.   <\/p>\n\n<p>This solution does not scale and we would like to pass a Scipy sparse matrix: <\/p>\n\n<h3>4. Batch Transform of a big sparse matrix.<\/h3>\n\n<p><strong>Here is the problem<\/strong><br>\nSageMaker Python SDK does not support sparse matrix format out of the box.<br>\nFollowing this:  <\/p>\n\n<ul>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker\/<\/a>    <\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/55479366\/errors-running-sagemaker-batch-transformation-with-lda-model\">Errors running Sagemaker Batch Transformation with LDA model<\/a>  <\/li>\n<\/ul>\n\n<p>I used <code>write_spmatrix_to_sparse_tensor<\/code> to write the data to protobuf format on s3. The function I used is:  <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def write_protobuf(X_sparse, bucket, prefix, obj):\n    \"\"\"Write sparse matrix to protobuf format at location bucket\/prefix\/obj.\"\"\"\n    buf = io.BytesIO()\n    write_spmatrix_to_sparse_tensor(file=buf, array=X_sparse, labels=None)\n    buf.seek(0)\n    key = '{}\/{}'.format(prefix, obj)\n    boto3.resource('s3').Bucket(bucket).Object(key).upload_fileobj(buf)\n    return 's3:\/\/{}\/{}'.format(bucket, key)\n<\/code><\/pre>\n\n<p>Then the code used for launching the batch transform job is:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_model = SKLearnModel(\n    model_data=model_artifact_location_on_s3,\n    entry_point='my_script.py',\n    role=role,\n    sagemaker_session=sagemaker_session)\n\ntransformer = sklearn_model.transformer(\n   instance_count=1, \n   instance_type='ml.m4.xlarge', \n   max_payload=100)\n\ntransformer.transform(\n   data=batch_data, \n   content_type='application\/x-recordio-protobuf',\n   split_type='RecordIO')   \n\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>sagemaker_containers._errors.ClientError: Content type application\/x-recordio-protobuf is not supported by this framework.\n<\/code><\/pre>\n\n<p><strong>Questions:<\/strong><br>\n(Reference doc for Transformer: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/transformer.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/transformer.html<\/a>) <\/p>\n\n<ul>\n<li>If <code>content_type='application\/x-recordio-protobuf'<\/code> is not allowed, what should I use?<\/li>\n<li>Is <code>split_type='RecordIO'<\/code> the proper setting in this context?   <\/li>\n<li>Should I provide an <code>input_fn<\/code> function in my script to deserialize the data?<\/li>\n<li>Is there another better approach to tackle this problem?<\/li>\n<\/ul>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1571219806713,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":5.0,
        "Question_view_count":834.0,
        "Poster_created_time":1406715922230,
        "Poster_location":"Belgium",
        "Poster_reputation_count":1714.0,
        "Poster_view_count":307.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58410583",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"batch transform spars matrix python sdk successfulli train scikit learn lsvc model want batch predict aka batch transform rel big dataset scipi spars matrix shape number featur high hot encod bag word ngram featur struggl size data format data experi check go predict local sampl spars matrix data work deseri model artifact local notebook predict sampl spars matrix check model predict kind data batch transform sampl csv data work launch batch transform job request transform small sampl dens csv format work scale obvious code sklearn model sklearnmodel model data model artifact locat entri point script role role session session transform sklearn model transform instanc count instanc type xlarg max payload transform transform data batch data content type text csv split type print wait transform job transform latest transform job job transform wait script implement simpl model deseri model artifact def model model dir clf joblib load path join model dir model joblib return clf batch data path csv file batch transform sampl dens numpi dataset work prepar sampl data save numpi npy format accord document scikit learn model server deseri npy format data json csv data differ previou experi argument content type applic npy transform transform solut scale like pass scipi spars matrix batch transform big spars matrix problem python sdk support spars matrix format box follow http aw amazon com blog machin learn build movi recommend factor machin amazon error run batch transform lda model write spmatrix spars tensor write data protobuf format function def write protobuf spars bucket prefix obj write spars matrix protobuf format locat bucket prefix obj buf bytesio write spmatrix spars tensor file buf arrai spars label buf seek kei format prefix obj boto resourc bucket bucket object kei upload fileobj buf return format bucket kei code launch batch transform job sklearn model sklearnmodel model data model artifact locat entri point script role role session session transform sklearn model transform instanc count instanc type xlarg max payload transform transform data batch data content type applic recordio protobuf split type recordio print wait transform job transform latest transform job job transform wait follow error contain error clienterror content type applic recordio protobuf support framework question refer doc transform http readthedoc stabl transform html content type applic recordio protobuf allow us split type recordio proper set context provid input function script deseri data better approach tackl problem",
        "Question_preprocessed_content":"batch transform spars matrix python sdk train lsvc model want batch predict rel big dataset scipi spars matrix shape size data format data experi check go predict sampl spars matrix data work deseri model artifact predict sampl spars matrix check model predict kind data batch transform sampl csv data work launch batch transform job request transform sampl dens csv format work scale obvious code implement simpl deseri model artifact path csv file batch transform sampl dens numpi dataset work prepar sampl data save numpi format document model server deseri data previou experi argument solut scale like scipi spars matrix batch transform big spars matrix problem python sdk spars matrix format box batch transform lda model write data protobuf format function code launch batch transform job question refer doc transform us proper context provid function script deseri data tackl problem",
        "Question_gpt_summary_original":"The user has trained a Scikit-Learn LSVC model with AWS SageMaker and wants to make batch prediction on a large dataset which is a scipy sparse matrix. The user has tried several experiments to check the model's prediction on different data formats and sizes. However, the user is facing challenges in batch transforming a big sparse matrix as SageMaker Python SDK does not support sparse matrix format out of the box. The user has used write_spmatrix_to_sparse_tensor to write the data to protobuf format on s3 and is facing an error while launching the batch transform job. The user has several questions related to the content_type, split_type, input_fn, and better approaches to tackle this problem.",
        "Question_gpt_summary":"user train scikit learn lsvc model want batch predict larg dataset scipi spars matrix user tri experi check model predict differ data format size user face challeng batch transform big spars matrix python sdk support spars matrix format box user write spmatrix spars tensor write data protobuf format face error launch batch transform job user question relat content type split type input better approach tackl problem"
    },
    {
        "Question_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Question_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1655994282873,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1656234648443,
        "Question_score_count":1.0,
        "Question_view_count":182.0,
        "Poster_created_time":1572256318027,
        "Poster_location":"Israel",
        "Poster_reputation_count":1387.0,
        "Poster_view_count":224.0,
        "Answer_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1656329528480,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1408529239847,
        "Answerer_location":"Dublin, Ireland",
        "Answerer_reputation_count":1652.0,
        "Answerer_view_count":293.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Tool":"Weights & Biases",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"hyperparamet tun commerror sweep user valid try initi sweep ry us hyperparamet tun describ notebook datafram try random forest regressor instead try initi sweep error sweep configur test project method random entiti metric loss goal minim paramet dict estim valu max depth valu min sampl split valu min sampl leaf valu max featur valu sweep configur paramet paramet dict sweep sweep sweep configur respons execut graphql error messag sweep user valid path upsertsweep data upsertsweep null error error call api sweep user valid commerror sweep user valid end goal init sweep",
        "Question_preprocessed_content":"hyperparamet user valid try initi ry us hyperparamet describ try initi respons execut graphql data api user valid user valid end goal init",
        "Question_gpt_summary_original":"The user is trying to use wandb for hyperparameter tuning on a random forest regressor using their own dataframe. However, when trying to initialize the sweep, they receive a \"Sweep user not valid\" error. The end goal is to successfully initialize the sweep.",
        "Question_gpt_summary":"user try us hyperparamet tune random forest regressor datafram try initi sweep receiv sweep user valid error end goal successfulli initi sweep"
    },
    {
        "Question_title":"Saving an Matlabplot as an MLFlow artifact",
        "Question_body":"<p>I am using DataBricks and Spark 7.4ML,<\/p>\n<p>The following code successfully logs the params and metrics, and I can see the ROCcurve.png in the MLFLOW gui (just the item in the tree below the model). But the actually plot is blank. Why?<\/p>\n<pre><code>with mlflow.start_run(run_name=&quot;logistic-regression&quot;) as run:\n  pipeModel = pipe.fit(trainDF)\n  mlflow.spark.log_model(pipeModel, &quot;model&quot;)\n  predTest = pipeModel.transform(testDF)\n  predTrain = pipeModel.transform(trainDF)\n  evaluator=BinaryClassificationEvaluator(labelCol=&quot;arrivedLate&quot;)\n  trainROC = evaluator.evaluate(predTrain)\n  testROC = evaluator.evaluate(predTest)\n  print(f&quot;Train ROC: {trainROC}&quot;)\n  print(f&quot;Test ROC: {testROC}&quot;)\n  mlflow.log_param(&quot;Dataset Name&quot;, &quot;Flights &quot; + datasetName)\n  mlflow.log_metric(key=&quot;Train ROC&quot;, value=trainROC)\n  mlflow.log_metric(key=&quot;Test ROC&quot;, value=testROC)\n\n  lrModel = pipeModel.stages[3]\n  trainingSummary = lrModel.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n  plt.show()\n  plt.savefig(&quot;ROCcurve.png&quot;)\n  mlflow.log_artifact(&quot;ROCcurve.png&quot;)\n  plt.close()\n  \n  display(predTest.select(stringCols + [&quot;arrivedLate&quot;, &quot;prediction&quot;]))\n<\/code><\/pre>\n<p>What the notebook shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sCIN9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What the MLFlow shows:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/oXk8Y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1607094596640,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1607191847983,
        "Question_score_count":8.0,
        "Question_view_count":5219.0,
        "Poster_created_time":1316705139196,
        "Poster_location":"Boston, MA",
        "Poster_reputation_count":6711.0,
        "Poster_view_count":819.0,
        "Answer_body":"<p>Put <code>plt.show()<\/code> after <code>plt.savefig()<\/code> - <code>plt.show()<\/code> will remove your plot because it is shown already.<\/p>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1607094854147,
        "Answer_last_edit_time":null,
        "Answer_score_count":7.0,
        "Answerer_created_time":1598380609848,
        "Answerer_location":"Oxford, UK",
        "Answerer_reputation_count":38531.0,
        "Answerer_view_count":4137.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65145994",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"save matlabplot artifact databrick spark follow code successfulli log param metric roccurv png gui item tree model actual plot blank start run run logist regress run pipemodel pipe fit traindf spark log model pipemodel model predtest pipemodel transform testdf predtrain pipemodel transform traindf evalu binaryclassificationevalu labelcol arrivedl trainroc evalu evalu predtrain testroc evalu evalu predtest print train roc trainroc print test roc testroc log param dataset flight datasetnam log metric kei train roc valu trainroc log metric kei test roc valu testroc lrmodel pipemodel stage trainingsummari lrmodel summari roc trainingsummari roc topanda plt plot roc fpr roc tpr plt ylabel fals posit rate plt xlabel true posit rate plt titl roc curv plt plt savefig roccurv png log artifact roccurv png plt close displai predtest select stringcol arrivedl predict notebook show show",
        "Question_preprocessed_content":"save matlabplot artifact databrick spark code log param metric png gui plot blank show show",
        "Question_gpt_summary_original":"The user is trying to save a Matplotlib plot as an MLFlow artifact in DataBricks and Spark 7.4ML. The code successfully logs the params and metrics, and the ROCcurve.png is visible in the MLFLOW gui, but the plot is blank. The user is seeking help to understand why this is happening.",
        "Question_gpt_summary":"user try save matplotlib plot artifact databrick spark code successfulli log param metric roccurv png visibl gui plot blank user seek help understand happen"
    },
    {
        "Question_title":"How to set multiple environment variables in AWS Sagemaker notebook instance?",
        "Question_body":"<p>I am using the AWS Sagemaker notebook instances for some of my experiments. As I am also using the lifecycle configurations scripts that are executed during notebook startup and also want to set some environment variables.<\/p>\n\n<p>For some reason, when I set multiple env variables in the lifecycle shell script, they are not set by the instance, i.e. when I execute\n<code>echo $FOO<\/code> \nthe relevant variable is not printed. <\/p>\n\n<p>However, when I set only one env variable it is working and I can use it in my notebook session. <\/p>\n\n<p>My understanding is that I start the desired Kernel, <\/p>\n\n<p>I have also tried to set the env variables inside the notebook by running <code>export FOO=BAR<\/code> but that also did not work. \nFollowing the example script provided by AWS, I made my changes to set the variables, however when I print $FOO, it doesn't seem to be displayed.<\/p>\n\n<p>I have tried setting the envs before and after switching to ec2-user (before the commands are executed as root), still nothing helped.<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>#!\/bin\/bash\n\n\ncd \/home\/ec2-user\/anaconda3\/envs\/python3\nmkdir -p .\/etc\/conda\/activate.d\nmkdir -p .\/etc\/conda\/deactivate.d\ntouch .\/etc\/conda\/activate.d\/env_vars.sh\ntouch .\/etc\/conda\/deactivate.d\/env_vars.sh\n\necho export FOO=BAR &gt;&gt; .\/etc\/conda\/activate.d\/env_vars.sh\necho unset FOO &gt;&gt; .\/etc\/conda\/deactivate.d\/env_vars.sh\n\necho export FOO2=BAR2 &gt;&gt; .\/etc\/conda\/activate.d\/env_vars.sh\necho unset FOO2 &gt;&gt; .\/etc\/conda\/deactivate.d\/env_vars.sh\n\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\ncd \/home\/ec2-user\n\n# This will affect only the Jupyter kernel called \"conda_python3\".\nsource activate python3\n\npip install --upgrade pip\n\npip install scipy xgboost sklearn\n\n# You can also perform \"conda install\" here as well.\n\n\nsource deactivate\n\nEOF\n\n\n<\/code><\/pre>\n\n<p>I want to set multiple environment variables for this Sagemaker notebook upon start, what is the best way to do this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2.0,
        "Question_created_time":1565020147883,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1574242772027,
        "Question_score_count":4.0,
        "Question_view_count":5171.0,
        "Poster_created_time":1432024993980,
        "Poster_location":"Berlin, Germany",
        "Poster_reputation_count":411.0,
        "Poster_view_count":6.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57362038",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"set multipl environ variabl notebook instanc notebook instanc experi lifecycl configur script execut notebook startup want set environ variabl reason set multipl env variabl lifecycl shell script set instanc execut echo foo relev variabl print set env variabl work us notebook session understand start desir kernel tri set env variabl insid notebook run export foo bar work follow exampl script provid aw chang set variabl print foo displai tri set env switch user command execut root help bin bash home user anaconda env python mkdir conda activ mkdir conda deactiv touch conda activ env var touch conda deactiv env var echo export foo bar conda activ env var echo unset foo conda deactiv env var echo export foo bar conda activ env var echo unset foo conda deactiv env var sudo user eof home user affect jupyt kernel call conda python sourc activ python pip instal upgrad pip pip instal scipi xgboost sklearn perform conda instal sourc deactiv eof want set multipl environ variabl notebook start best wai",
        "Question_preprocessed_content":"set multipl environ variabl instanc instanc experi lifecycl configur script execut startup want set environ variabl reason set multipl env variabl lifecycl script set instanc execut relev variabl print set env variabl work us understand start desir kernel tri set env variabl insid work exampl script provid aw chang set variabl print displai tri env switch help want set multipl environ variabl start best wai",
        "Question_gpt_summary_original":"The user is facing issues while setting multiple environment variables in AWS Sagemaker notebook instance. They have tried setting the variables in the lifecycle shell script and also inside the notebook, but none of them seem to work. However, setting a single environment variable is working fine. The user has also shared a script that they have tried to modify to set the variables, but it did not work. The user is seeking advice on the best way to set multiple environment variables for the Sagemaker notebook upon start.",
        "Question_gpt_summary":"user face issu set multipl environ variabl notebook instanc tri set variabl lifecycl shell script insid notebook work set singl environ variabl work fine user share script tri modifi set variabl work user seek advic best wai set multipl environ variabl notebook start"
    },
    {
        "Question_title":"What is the role of feature type in AzureML?",
        "Question_body":"<p>I want to know what is the difference between <code>feature numeric<\/code> and <code>numeric<\/code> columns in Azure Machine Learning Studio.<\/p>\n\n<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">documentation site<\/a> states: <\/p>\n\n<blockquote>\n  <p>Because all columns are initially treated as features, for modules\n  that perform mathematical operations, you might need to use this\n  option to prevent numeric columns from being treated as variables.<\/p>\n<\/blockquote>\n\n<p>But nothing more. Not what a feature is, in which modules you need features. Nothing. <\/p>\n\n<p>I specifically would like to understand if the <code>clear feature<\/code> dropdown option in the <code>fields<\/code> in the <code>edit metadata<\/code>-module has any effect. Can somebody give me a szenario where this <code>clear feature<\/code>-operation changes the ML outcome? Thank you<\/p>\n\n<p>According to the documentation in ought to have an effect:<\/p>\n\n<blockquote>\n  <p>Use the Fields option if you want to change the way that Azure Machine\n  Learning uses the data in a model.<\/p>\n<\/blockquote>\n\n<p>But what can this effect be? Any example might help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1538054010173,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1538054394440,
        "Question_score_count":1.0,
        "Question_view_count":70.0,
        "Poster_created_time":1368739128832,
        "Poster_location":"Riga, Latvia",
        "Poster_reputation_count":1763.0,
        "Poster_view_count":380.0,
        "Answer_body":"<p>As you suspect, setting a column as <code>feature<\/code> does have an effect, and it's actually quite important - when training a model, the algorithms will only take into account columns with the <code>feature<\/code> flag, effectively ignoring the others. <\/p>\n\n<p>For example, if you have a dataset with columns <code>Feature1<\/code>, <code>Feature2<\/code>, and <code>Label<\/code> and you want to try out just <code>Feature1<\/code>, you would apply <code>clear feature<\/code> to the <code>Feature2<\/code> column (while making sure that <code>Feature1<\/code> has the <code>feature<\/code> label set, of course).<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1538116098500,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1250158552416,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52537861",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"role featur type want know differ featur numer numer column studio document site state column initi treat featur modul perform mathemat oper need us option prevent numer column treat variabl featur modul need featur specif like understand clear featur dropdown option field edit metadata modul effect somebodi szenario clear featur oper chang outcom thank accord document ought effect us field option want chang wai azur machin learn us data model effect exampl help",
        "Question_preprocessed_content":"role featur type want know column studio document site state column treat featur modul perform mathemat oper us option prevent numer column treat variabl featur modul featur like understand dropdown option modul somebodi szenario oper chang outcom thank document ought us field option want chang wai azur machin learn us data model exampl help",
        "Question_gpt_summary_original":"The user is seeking clarification on the difference between \"feature numeric\" and \"numeric\" columns in Azure Machine Learning Studio. They are also unsure about the purpose of the \"clear feature\" dropdown option in the \"edit metadata\" module and how it affects the ML outcome. The documentation provides limited information on these topics, and the user is looking for specific scenarios or examples.",
        "Question_gpt_summary":"user seek clarif differ featur numer numer column studio unsur purpos clear featur dropdown option edit metadata modul affect outcom document provid limit inform topic user look specif scenario exampl"
    },
    {
        "Question_title":"Logging volumetric data?",
        "Question_body":"<p>Hi,<\/p>\n<p>I was wondering if there was any tools that could be of use to log volumetric data? My data are essentially 3-d tensor where each values represent the density of a volume. Currently I use some third party tools to visualize them, but it would be nice to have everything in the wandb UI.<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_created_time":1669648525050,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":208.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-volumetric-data\/3476",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"log volumetr data wonder tool us log volumetr data data essenti tensor valu repres densiti volum current us parti tool visual nice",
        "Question_preprocessed_content":"volumetr data wonder us log volumetr data data tensor valu repres densiti volum us parti visual nice",
        "Question_gpt_summary_original":"The user is looking for tools to log volumetric data, which are 3D tensors representing the density of a volume. They currently use third-party tools for visualization and would like to have everything in the wandb UI.",
        "Question_gpt_summary":"user look tool log volumetr data tensor repres densiti volum current us parti tool visual like"
    },
    {
        "Question_title":"What is the use of git commits in mlflow?",
        "Question_body":"<p>Why mlflow tracks git commits, we already have run_id for tracking experiment. Can we use those commits to go back to previous commit like we do in git.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1650524426890,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":161.0,
        "Poster_created_time":1608194065656,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":6.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71950167",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us git commit track git commit run track experi us commit previou commit like git",
        "Question_preprocessed_content":"us git track git track experi us previou like git",
        "Question_gpt_summary_original":"The user is questioning the purpose of using git commits in mlflow and whether they can be used to go back to previous commits like in git. They are also wondering if using run_id for tracking experiments is sufficient.",
        "Question_gpt_summary":"user question purpos git commit previou commit like git wonder run track experi suffici"
    },
    {
        "Question_title":"Web service output - Azure ML Studio",
        "Question_body":"<p>I am new to Azure ML Studio. I tried creating an experiment that takes a numeric value as input and a gives a data table type output. I works fine when I run it in the portal , but not when I run it as a web service. It shows a single value numeric output , when it has to be a data table type.<\/p>\n\n<p>Is there a way to change the output type of web service output? <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/oq5Xb.png\" rel=\"nofollow noreferrer\">Visualizing output in portal<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wUmN7.png\" rel=\"nofollow noreferrer\">Test RRS output(web service)<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6.0,
        "Question_created_time":1485555724117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1485558200660,
        "Question_score_count":1.0,
        "Question_view_count":587.0,
        "Poster_created_time":1449123268407,
        "Poster_location":"East Newark, NJ, United States",
        "Poster_reputation_count":33.0,
        "Poster_view_count":43.0,
        "Answer_body":"<p>Make is a classic web service and see the JSON output getting from it. If it's providing all data you need.. go for it.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1486009486972,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1443426419048,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation_count":842.0,
        "Answerer_view_count":219.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41903982",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"web servic output studio new studio tri creat experi take numer valu input give data tabl type output work fine run portal run web servic show singl valu numer output data tabl type wai chang output type web servic output visual output portal test rr output web servic",
        "Question_preprocessed_content":"web servic output studio new studio tri creat experi take numer valu input give data tabl type output work fine run portal run web servic show singl valu numer output data tabl type wai chang output type web servic output visual output portal test output",
        "Question_gpt_summary_original":"The user is facing an issue with the output type of a web service in Azure ML Studio. The experiment works fine in the portal but shows a single value numeric output instead of a data table type when run as a web service. The user is seeking a solution to change the output type of the web service output.",
        "Question_gpt_summary":"user face issu output type web servic studio experi work fine portal show singl valu numer output instead data tabl type run web servic user seek solut chang output type web servic output"
    },
    {
        "Question_title":"SageMaker: PipelineModel and Hyperparameter Tuning",
        "Question_body":"<p>We want to tune a SageMaker <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> with a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/tuner.html?highlight=HyperparameterTuner\" rel=\"nofollow noreferrer\">HyperparameterTuner<\/a> (or something similar) where <em>several<\/em> components of the pipeline have associated hyperparameters. Both components in our case are realized via SageMaker containers for ML algorithms.<\/p>\n<pre><code>model = PipelineModel(..., models = [ our_model, xgb_model ])\ndeploy = Estimator(image_uri = model, ...)\n...\ntuner = HyperparameterTuner(deply, .... tune_parameters, ....)\ntuner.fit(...)\n<\/code><\/pre>\n<p>Now, there is of course the problem how to distribute the <code>tune_parameters<\/code> to the pipeline steps during the tuning.<\/p>\n<p>In scikit-learn this is achieved by specially naming the tuning parameters <code>&lt;StepName&gt;__&lt;ParameterName&gt;<\/code>.<\/p>\n<p>I don't see a way to achieve something similar with SageMaker, though. Also, search of the two keywords brings up the same question <a href=\"https:\/\/stackoverflow.com\/questions\/56308169\/creating-a-model-for-use-in-a-pipeline-from-a-hyperparameter-tuning-job\">here<\/a> but is not really what we want to do.<\/p>\n<p>Any suggestion how to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1646365382453,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":117.0,
        "Poster_created_time":1389092281248,
        "Poster_location":"Ishikawa Prefecture",
        "Poster_reputation_count":227.0,
        "Poster_view_count":53.0,
        "Answer_body":"<p>If both the models need to be jointly optimized, you could run a SageMaker HPO job in script mode and define both the models in the script. Or you could run two HPO jobs, optimize each model, and then create the Pipeline Model. There is no native support for doing an HPO job on a PipelineModel.<\/p>\n<p>I work at AWS and my opinions are my own.<\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1646418155780,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1623879163643,
        "Answerer_location":null,
        "Answerer_reputation_count":224.0,
        "Answerer_view_count":40.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71346372",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pipelinemodel hyperparamet tune want tune pipelinemodel hyperparametertun similar compon pipelin associ hyperparamet compon case realiz contain algorithm model pipelinemodel model model xgb model deploi estim imag uri model tuner hyperparametertun depli tune paramet tuner fit cours problem distribut tune paramet pipelin step tune scikit learn achiev special name tune paramet wai achiev similar search keyword bring question want suggest achiev",
        "Question_preprocessed_content":"pipelinemodel hyperparamet tune want tune pipelinemodel hyperparametertun compon pipelin hyperparamet compon case realiz contain algorithm cours problem distribut pipelin step tune achiev name tune paramet wai achiev similar search keyword bring question want achiev",
        "Question_gpt_summary_original":"The user wants to tune a SageMaker PipelineModel with a HyperparameterTuner, but is facing the challenge of how to distribute the tuning parameters to the pipeline steps during the tuning process. The user is looking for suggestions on how to achieve this.",
        "Question_gpt_summary":"user want tune pipelinemodel hyperparametertun face challeng distribut tune paramet pipelin step tune process user look suggest achiev"
    },
    {
        "Question_title":"Error while hyperparameter search",
        "Question_body":"<p>When I try wandb.sweep, it gives following error:  wandb.errors.CommError: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.<\/p>\n<p>Following this, is my sweep config<br>\n{\u2018method\u2019: \u2018random\u2019,<br>\n\u2018metric\u2019: {\u2018goal\u2019: \u2018minimize\u2019, \u2018name\u2019: \u2018loss\u2019},<br>\n\u2018parameters\u2019: {\u2018batch_size\u2019: {\u2018distribution\u2019: \u2018q_log_uniform_values\u2019,<br>\n\u2018max\u2019: 256,<br>\n\u2018min\u2019: 32,<br>\n\u2018q\u2019: 8},<br>\n\u2018epochs\u2019: {\u2018value\u2019: 10},<br>\n\u2018fc_layer_size\u2019: {\u2018values\u2019: [16, 32, 64]},<br>\n\u2018learning_rate\u2019: {\u2018distribution\u2019: \u2018uniform\u2019,<br>\n\u2018max\u2019: 0.1,<br>\n\u2018min\u2019: 0},<br>\n\u2018optimizer\u2019: {\u2018values\u2019: [\u2018adam\u2019, \u2018sgd\u2019]},<br>\n\u2018training_snr\u2019: {\u2018values\u2019: [0.3981071705534972,<br>\n0.44668359215096315,<br>\n0.5011872336272722,<br>\n0.5623413251903491,<br>\n0.6309573444801932,<br>\n0.7079457843841379,<br>\n0.7943282347242815,<br>\n0.8912509381337456,<br>\n1.0,<br>\n1.1220184543019633,<br>\n1.2589254117941673,<br>\n1.4125375446227544,<br>\n1.5848931924611136,<br>\n1.7782794100389228,<br>\n1.9952623149688795,<br>\n2.2387211385683394,<br>\n2.51188643150958,<br>\n2.8183829312644537,<br>\n3.1622776601683795,<br>\n3.548133892335755,<br>\n3.9810717055349722,<br>\n4.466835921509632,<br>\n5.011872336272722,<br>\n5.623413251903491,<br>\n6.309573444801933,<br>\n7.079457843841379,<br>\n7.943282347242816,<br>\n8.912509381337454,<br>\n10.0]}}}<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_created_time":1657819842704,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":306.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/error-while-hyperparameter-search\/2751",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error hyperparamet search try sweep give follow error error commerror bad request browser proxi sent request server understand follow sweep config method random metric goal minim loss paramet batch size distribut log uniform valu max min epoch valu layer size valu learn rate distribut uniform max min optim valu adam sgd train snr valu",
        "Question_preprocessed_content":"hyperparamet search try give bad request browser sent request server understand config method random metric paramet epoch optim",
        "Question_gpt_summary_original":"The user is encountering an error while using wandb.sweep for hyperparameter search. The error message is \"wandb.errors.CommError: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\" The user has also provided their sweep config, which includes various parameters such as batch size, epochs, learning rate, optimizer, and training signal-to-noise ratio.",
        "Question_gpt_summary":"user encount error sweep hyperparamet search error messag error commerror bad request browser proxi sent request server understand user provid sweep config includ paramet batch size epoch learn rate optim train signal nois ratio"
    },
    {
        "Question_title":"Using the Amazon SageMaker Studio Image Build CLI to build container images from Studio notebooks",
        "Question_body":"<p>I'm trying to build a Docker image from SageMaker studio using the CLI described here: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/sagemaker-studio-image-build-cli<\/a><\/p>\n<p>This should be straight forward but I'm missing something because when running the command <code>sm-docker build .<\/code> I have a &quot;invalid syntax&quot; error. Any idea what I'm doing wrong?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/i89fH.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/i89fH.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Thank you!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1620074577390,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":901.0,
        "Poster_created_time":1469978721363,
        "Poster_location":null,
        "Poster_reputation_count":415.0,
        "Poster_view_count":48.0,
        "Answer_body":"<p>No actually I was missing the &quot;<code>!<\/code>&quot; in front of <code>sm-docker build .<\/code>\nSo <code>!sm-docker build .<\/code> is working.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1620075508956,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1469978721363,
        "Answerer_location":null,
        "Answerer_reputation_count":415.0,
        "Answerer_view_count":48.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67375607",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"studio imag build cli build contain imag studio notebook try build docker imag studio cli describ http github com aw sampl studio imag build cli straight forward miss run command docker build invalid syntax error idea wrong thank",
        "Question_preprocessed_content":"studio imag build cli build contain imag studio try build docker imag studio cli describ straight forward invalid syntax idea wrong thank",
        "Question_gpt_summary_original":"The user is encountering an \"invalid syntax\" error while trying to build a Docker image from SageMaker studio using the CLI. The user is seeking help to identify the issue.",
        "Question_gpt_summary":"user encount invalid syntax error try build docker imag studio cli user seek help identifi issu"
    },
    {
        "Question_title":"How do I add a directory of .wav files to the Kedro data catalogue?",
        "Question_body":"<p>This is my first time trying to use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Kedro<\/a> package.<\/p>\n<p>I have a list of .wav files in an s3 bucket, and I'm keen to know how I can have them available within the Kedro data catalog.<\/p>\n<p>Any thoughts?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1611660152193,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":294.0,
        "Poster_created_time":1500383313376,
        "Poster_location":"London, UK",
        "Poster_reputation_count":851.0,
        "Poster_view_count":86.0,
        "Answer_body":"<p>I don't believe there's currently a dataset format that handles <code>.wav<\/code> files. You'll need to build a <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/03_custom_datasets.html\" rel=\"nofollow noreferrer\">custom dataset<\/a> that uses something like <a href=\"https:\/\/docs.python.org\/3\/library\/wave.html\" rel=\"nofollow noreferrer\">Wave<\/a> - not as much work as it sounds!<\/p>\n<p>This will enable you to do something like this in your catalog:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset:\n  type: my_custom_path.WaveDataSet\n  filepath: path\/to\/individual\/wav_file.wav # this can be a s3:\/\/url\n<\/code><\/pre>\n<p>and you can then access your WAV data natively within your Kedro pipeline. You can do this for each <code>.wav<\/code> file you have.<\/p>\n<p>If you wanted to be able to access a whole folders worth of wav files, you might want to explore the notion of a &quot;wrapper&quot; dataset like the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PartitionedDataSet.html\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a> whose <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset-definition\" rel=\"nofollow noreferrer\">usage guide<\/a> can be found in the documentation.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1611661436392,
        "Answer_last_edit_time":1611661825368,
        "Answer_score_count":1.0,
        "Answerer_created_time":1530092504712,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":915.0,
        "Answerer_view_count":288.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65900415",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"add directori wav file data catalogu time try us packag list wav file bucket keen know avail data catalog thought",
        "Question_preprocessed_content":"directori wav file data catalogu time try us packag list wav file bucket know avail data catalog thought",
        "Question_gpt_summary_original":"The user is seeking guidance on how to add a directory of .wav files from an s3 bucket to the Kedro data catalogue.",
        "Question_gpt_summary":"user seek guidanc add directori wav file bucket data catalogu"
    },
    {
        "Question_title":"Use PyTorch DistributedDataParallel with Hugging Face on Amazon SageMaker",
        "Question_body":"<p>Even for single-instance training, PyTorch DistributedDataParallel (DDP) is generally recommended over PyTorch DataParallel (DP) because DP's strategy is less performant and it uses more memory on the default device. (Per\u00a0<a href=\"https:\/\/discuss.pytorch.org\/t\/cuda-out-of-memory-error-when-using-multi-gpu\/72333\" rel=\"nofollow noreferrer\">this PyTorch forums thread<\/a>)<\/p>\n<p>Hugging Face\u00a0<a href=\"https:\/\/github.com\/huggingface\/transformers\/tree\/master\/examples\/pytorch#distributed-training-and-mixed-precision\" rel=\"nofollow noreferrer\">recommend<\/a>\u00a0to run distributed training via the\u00a0<code>python -m torch.distributed.launch<\/code>\u00a0launcher, because their Trainer API supports DDP but will fall back to DP if you don't. (Per\u00a0<a href=\"https:\/\/discuss.huggingface.co\/t\/multi-gpu-training\/4021\" rel=\"nofollow noreferrer\">this HF forums thread<\/a>)<\/p>\n<p>I recently ran in to this problem: scaling a HF training job from\u00a0<code>p3.8xlarge<\/code>\u00a0to\u00a0<code>p3.16xlarge<\/code>\u00a0increased memory consumption on (I think) one of the GPUs to the point where I had to significantly reduce batch size to avoid CUDA Out of Memory errors - basically losing all scaling advantage.<\/p>\n<p>So the good news is for p3.16xl+ I can just\u00a0<a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/train#distributed-training\" rel=\"nofollow noreferrer\">enable SageMaker Distributed Data Parallel<\/a>\u00a0and the PyToch DLC will automatically\u00a0<a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-training-toolkit\/blob\/88ca48a831bf4f099d4c57f3c18e0ff92fa2b48c\/src\/sagemaker_pytorch_container\/training.py#L23\" rel=\"nofollow noreferrer\">launch via torch.distributed for me<\/a>.<\/p>\n<p>The bad news for use cases with smaller workloads or wanting to test before they scale up, is that SMDistributed\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-parallel-intro.html#data-parallel-allreduce\" rel=\"nofollow noreferrer\">doesn't support all multi-GPU instance types<\/a>. No p3.8xl or g series, for example. I did try manually setting the\u00a0<code>sagemaker_distributed_dataparallel_enabled<\/code>\u00a0environment variable, but no joy.<\/p>\n<p>So how else can we launch HF Trainer scripts with PyTorch DDP on SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1662627802657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":16.0,
        "Poster_created_time":1662621266503,
        "Poster_location":null,
        "Poster_reputation_count":48.0,
        "Poster_view_count":6.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73646375",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us pytorch distributeddataparallel hug face singl instanc train pytorch distributeddataparallel ddp gener recommend pytorch dataparallel strategi perform us memori default devic perthi pytorch forum thread hug facerecommendto run distribut train thepython torch distribut launchlaunch trainer api support ddp fall perthi forum thread recent ran problem scale train job fromp xlargetop xlargeincreas memori consumpt think gpu point significantli reduc batch size avoid cuda memori error basic lose scale advantag good new justen distribut data paralleland pytoch dlc automaticallylaunch torch distribut bad new us case smaller workload want test scale smdistributeddoesn support multi gpu instanc type seri exampl try manual set distribut dataparallel enabledenviron variabl joi launch trainer script pytorch ddp",
        "Question_preprocessed_content":"us pytorch face train pytorch pytorch strategi perform us memori default devic run distribut train thelaunch trainer api recent ran problem scale train job fromtoincreas memori consumpt gpu point significantli reduc batch size avoid cuda memori lose scale advantag new justen distribut data pytoch dlc bad new us case workload want test scale instanc type seri exampl try variabl joi launch trainer script pytorch",
        "Question_gpt_summary_original":"The user encountered a problem while scaling a Hugging Face training job from p3.8xlarge to p3.16xlarge, which increased memory consumption on one of the GPUs and resulted in CUDA Out of Memory errors. The user can enable SageMaker Distributed Data Parallel for p3.16xl+ to launch via torch.distributed automatically. However, SMDistributed does not support all multi-GPU instance types, such as p3.8xl or g series. The user is looking for alternative ways to launch HF Trainer scripts with PyTorch DDP on SageMaker.",
        "Question_gpt_summary":"user encount problem scale hug face train job xlarg xlarg increas memori consumpt gpu result cuda memori error user enabl distribut data parallel launch torch distribut automat smdistribut support multi gpu instanc type seri user look altern wai launch trainer script pytorch ddp"
    },
    {
        "Question_title":"SageMaker: An error occurred (ModelError) when calling the InvokeEndpoint operation: unable to evaluate payload provided",
        "Question_body":"<p>I have a endpoint in Amazon SageMaker (Image-classification algorithm) in Jupyter notebook that works fine. In Lambda function works fine too, when I call the Lambda function from API Gateway, from test of API Gateway, works fine too.<\/p>\n<p>The problem is when I call the API from Postman according this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/39660074\/post-image-data-using-postman\">&quot;Post Image data using POSTMAN&quot;<\/a><\/p>\n<p>The code in Lambda is:<\/p>\n<pre><code>import boto3\nimport json\nimport base64\n\nENDPOINT_NAME = &quot;DEMO-XGBoostEndpoint-Multilabel&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\nimagen_ = &quot;\/tmp\/imageToProcess.jpg&quot;\n\ndef write_to_file(save_path, data):\n    with open(save_path, &quot;wb&quot;) as f:\n        f.write(base64.b64decode(data))\n\ndef lambda_handler(event, context):\n    img_json = json.loads(json.dumps(event))\n\n    write_to_file(imagen_, json.dumps(event, indent=2))\n\n    with open(imagen_, &quot;rb&quot;) as image:\n        f = image.read()\n        b = bytearray(f)\n\n    payload = b\n\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType=&quot;application\/x-image&quot;,\n                                       Body=payload)\n\n    #print(response)\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = [&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]\n    for idx, val in enumerate(classes):\n        print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n        predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n}\n<\/code><\/pre>\n<p>The error is:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 26, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 626, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;unable to evaluate payload provided&quot;. See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/DEMO-XGBoostEndpoint-Multilabel in account 866341179300 for more information. ```\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1595742856227,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1595997749087,
        "Question_score_count":2.0,
        "Question_view_count":3996.0,
        "Poster_created_time":1565376125572,
        "Poster_location":null,
        "Poster_reputation_count":31.0,
        "Poster_view_count":2.0,
        "Answer_body":"<p>I resolved with <a href=\"https:\/\/medium.com\/swlh\/upload-binary-files-to-s3-using-aws-api-gateway-with-aws-lambda-2b4ba8c70b8e\" rel=\"nofollow noreferrer\">this<\/a> post:<\/p>\n<p>Thank all<\/p>\n<p>Finally the code in lambda function is:<\/p>\n<pre><code>import os\nimport boto3\nimport json\nimport base64\n\nENDPOINT_NAME = os.environ['endPointName']\nCLASSES = &quot;[&quot;chair&quot;, &quot;handbag&quot;, &quot;person&quot;, &quot;traffic light&quot;, &quot;clock&quot;]&quot;\nruntime= boto3.client(&quot;runtime.sagemaker&quot;)\n\ndef lambda_handler(event, context):\n    file_content = base64.b64decode(event['content'])\n\n    payload = file_content\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType=&quot;application\/x-image&quot;, Body=payload)\n\n    result = json.loads(response[&quot;Body&quot;].read().decode())\n    print(result)\n    predicted_label=[]\n    classes = CLASSES\n    for idx, val in enumerate(classes):\n       print(&quot;%s:%f &quot;%(classes[idx], result[idx]), end=&quot;&quot;)\n       predicted_label += (classes[idx], result[idx])\n\n    return {\n      &quot;statusCode&quot;: 200,\n      &quot;headers&quot;: { &quot;content-type&quot;: &quot;application\/json&quot;},\n      &quot;body&quot;:  predicted_label\n    }\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1597003004372,
        "Answer_last_edit_time":1634524535896,
        "Answer_score_count":1.0,
        "Answerer_created_time":1565376125572,
        "Answerer_location":null,
        "Answerer_reputation_count":31.0,
        "Answerer_view_count":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63096583",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error occur modelerror call invokeendpoint oper unabl evalu payload provid endpoint imag classif algorithm jupyt notebook work fine lambda function work fine lambda function api gatewai test api gatewai work fine problem api postman accord answer post imag data postman code lambda import boto import json import base endpoint demo xgboostendpoint multilabel runtim boto client runtim imagen tmp imagetoprocess jpg def write file save path data open save path write base bdecod data def lambda handler event context img json json load json dump event write file imagen json dump event indent open imagen imag imag read bytearrai payload respons runtim invok endpoint endpointnam endpoint contenttyp applic imag bodi payload print respons result json load respons bodi read decod print result predict label class chair handbag person traffic light clock idx val enumer class print class idx result idx end predict label class idx result idx return statuscod header content type applic json bodi predict label error traceback recent file var task lambda function line lambda handler bodi payload file var runtim botocor client line api return self api oper kwarg file var runtim botocor client line api rais error class pars respons oper botocor errorfactori modelerror error occur modelerror call invokeendpoint oper receiv client error model messag unabl evalu payload provid http east consol aw amazon com cloudwatch home region east logeventview group aw endpoint demo xgboostendpoint multilabel account inform",
        "Question_preprocessed_content":"oper unabl evalu payload provid endpoint jupyt work fine lambda function work fine lambda function api gatewai test api gatewai work fine problem api postman answer post imag data postman code lambda",
        "Question_gpt_summary_original":"The user is encountering an error while calling an Amazon SageMaker endpoint for an image-classification algorithm from Postman. The Lambda function and API Gateway work fine, but the error occurs when calling the API from Postman. The error message indicates that the payload provided is unable to be evaluated.",
        "Question_gpt_summary":"user encount error call endpoint imag classif algorithm postman lambda function api gatewai work fine error occur call api postman error messag indic payload provid unabl evalu"
    },
    {
        "Question_title":"AWS Sagemaker - using cross validation instead of dedicated validation set?",
        "Question_body":"<p>When I train my model locally I use a 20% test set and then cross validation. Sagameker seems like it needs a dedicated valdiation set (at least in the tutorials I've followed). Currently I have 20% test, 10% validation leaving 70% to train - so I lose 10% of my training data compared to when I train locally, and there is some performance loss as a results of this. <\/p>\n\n<p>I could just take my locally trained models and overwrite the sagemaker models stored in s3, but that seems like a bit of a work around. Is there a way to use Sagemaker without having to have a dedicated validation set? <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1566578862943,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":1401.0,
        "Poster_created_time":1421343783700,
        "Poster_location":null,
        "Poster_reputation_count":1387.0,
        "Poster_view_count":153.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57630271",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"cross valid instead dedic valid set train model local us test set cross valid sagamek like need dedic valdiat set tutori follow current test valid leav train lose train data compar train local perform loss result local train model overwrit model store like bit work wai us have dedic valid set thank",
        "Question_preprocessed_content":"valid instead dedic valid set train model us test set valid sagamek like dedic valdiat set test valid leav train lose train data compar train perform result train model overwrit model store like bit work wai us have dedic valid set thank",
        "Question_gpt_summary_original":"The user is facing challenges while using AWS Sagemaker as it requires a dedicated validation set, unlike the 20% test set and cross-validation used during local model training. This results in a loss of 10% of training data and some performance loss. The user is seeking a way to use Sagemaker without a dedicated validation set.",
        "Question_gpt_summary":"user face challeng requir dedic valid set unlik test set cross valid local model train result loss train data perform loss user seek wai us dedic valid set"
    },
    {
        "Question_title":"Getting Conflict error for Amazon.RegionEndpoint with Sagemaker",
        "Question_body":"<p>I have created an applicataion in c#, where I need to put some data on S3 bucket, and to Invoke AWS sagemaker APIs.\nSince the same Amazon.RegionEndPoint class exists in both the references, it is giving below error.<\/p>\n\n<blockquote>\n  <p>The type 'RegionEndpoint' exists in both 'AWSSDK.Core,\n  Version=3.3.0.0, Culture=neutral, PublicKeyToken=885c28607f98e604' and\n  'AWSSDK, Version=2.3.55.2<\/p>\n<\/blockquote>\n\n<p>Basically I am trying to upload files on AWS S3, following code I have used.<\/p>\n\n<pre><code>AmazonS3Client s3Client = new AmazonS3Client(_AWS_ACCESS_KEY_ID, _AWS_SECRETE_ACCESS_KEY, Amazon.RegionEndpoint.USEast2);\n PutObjectRequest request = new PutObjectRequest\n  {\n    BucketName = _BucketName,\n    Key = i_sDestFileName,\n    FilePath = i_sSourceFilePath,\n    ContentType = \"text\/plain\"\n  };\n  s3Client.PutObject(request);\n<\/code><\/pre>\n\n<p>It is working fine on a single application, but when I integrated code with Sagemaker API invokation, the conflict occurs for Amazon.RegionEndpoint.USEast2.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3.0,
        "Question_created_time":1517316966157,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1517391033467,
        "Question_score_count":3.0,
        "Question_view_count":2527.0,
        "Poster_created_time":1337759214688,
        "Poster_location":"Pune India",
        "Poster_reputation_count":1036.0,
        "Poster_view_count":124.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48522013",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"get conflict error amazon regionendpoint creat applicataion need data bucket invok api amazon regionendpoint class exist refer give error type regionendpoint exist awssdk core version cultur neutral publickeytoken cfe awssdk version basic try upload file aw follow code amazonscli sclient new amazonscli aw access kei aw secret access kei amazon regionendpoint useast putobjectrequest request new putobjectrequest bucketnam bucketnam kei sdestfilenam filepath ssourcefilepath contenttyp text plain sclient putobject request work fine singl applic integr code api invok conflict occur amazon regionendpoint useast",
        "Question_preprocessed_content":"conflict creat data bucket invok api exist refer give type regionendpoint exist cultur neutral publickeytoken try upload file aw code work fine singl integr code api invok conflict",
        "Question_gpt_summary_original":"The user is encountering a conflict error with the Amazon.RegionEndpoint class while trying to upload files on AWS S3 and invoke AWS Sagemaker APIs in a C# application. The error is caused by the existence of the same class in both 'AWSSDK.Core' and 'AWSSDK' references. The code works fine on a single application but the conflict occurs when integrated with Sagemaker API invocation.",
        "Question_gpt_summary":"user encount conflict error amazon regionendpoint class try upload file aw invok api applic error caus exist class awssdk core awssdk refer code work fine singl applic conflict occur integr api invoc"
    },
    {
        "Question_title":"Question: how to define custom Model in Designer",
        "Question_body":"<p>In Azure ML Designer, I can't figure out how to define a custom Model. The builtin NN Regression module has many bugs (will open a separate issue for those), so I need to make my own custom model. The closest thing I've found so far is Create Python Model, but this has the following limitation:  <\/p>\n<ul>\n<li> Can't parametrize model, so doesn't work with Tune Model Hyperparameters module  <\/li>\n<\/ul>\n<p>Is there any way to design my own model, and is it possible to contribute this upstream for others to use?  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1594757599337,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/46909\/question-how-to-define-custom-model-in-designer",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"question defin custom model design design figur defin custom model builtin regress modul bug open separ issu need custom model closest thing far creat python model follow limit parametr model work tune model hyperparamet modul wai design model possibl contribut upstream us",
        "Question_preprocessed_content":"question defin custom model design design figur defin custom model builtin modul bug custom model closest thing far creat python model limit parametr model work tune model hyperparamet modul wai design model contribut upstream us",
        "Question_gpt_summary_original":"The user is facing challenges in defining a custom model in Azure ML Designer as the built-in NN Regression module has many bugs. The user has tried using the Create Python Model but it has limitations and cannot be parametrized to work with the Tune Model Hyperparameters module. The user is seeking a way to design their own model and is interested in contributing it upstream for others to use.",
        "Question_gpt_summary":"user face challeng defin custom model design built regress modul bug user tri creat python model limit parametr work tune model hyperparamet modul user seek wai design model interest contribut upstream us"
    },
    {
        "Question_title":"Sweeps while using MPI and SLURM",
        "Question_body":"<p>Hello! I am attempting to perform a hyperparameter search on my project, which uses MPI under the hood to aggregate the results of multiple agents. I have 63 agents that run an episode, returning a total reward at the end. At the end, each worker node sends their results to the main node, which logs the total reward of every 5th training run.<\/p>\n<p>I have tried to create a sweep with a custom command to use <code>mpirun<\/code>(as seen below) and running <code>wandb agent sweepid --count 1<\/code> in the SLURM script.  This results in using all the cores of the machine to start a sweep, effectively blocking my other agents from training.<\/p>\n<pre><code class=\"lang-auto\">program: src.sweep_mpi \ncommand:\n  - mpirun\n  - \"--mca\" \n  - opal_warn_on_missing_libcuda\n  - 0\n  - python\n  - \"-m\"\n  - ${program}\n  - ${args}\n<\/code><\/pre>\n<p>Next, I have tried setting up the sweeping agent inside the python code with a local controller, but this also led to issues regarding the parallelization. Currently, I need to initialize wandb using <code>settings=wandb.Settings(start_method=\"fork\")<\/code>, but I cannot find any way to specify this as a sweep parameter. Therefore, each  run crashes since it is not using the correct parallelization procedure.<\/p>\n<p>Is there anything I can do in this case? Or should I implement my own parameter search?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_created_time":1652695453529,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":753.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweeps-while-using-mpi-and-slurm\/2427",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sweep mpi slurm hello attempt perform hyperparamet search project us mpi hood aggreg result multipl agent agent run episod return total reward end end worker node send result main node log total reward train run tri creat sweep custom command us mpirun seen run agent sweepid count slurm script result core machin start sweep effect block agent train program src sweep mpi command mpirun mca opal warn miss libcuda python program arg tri set sweep agent insid python code local control led issu parallel current need initi set set start method fork wai specifi sweep paramet run crash correct parallel procedur case implement paramet search",
        "Question_preprocessed_content":"mpi slurm perform hyperparamet search project us mpi result multipl agent agent run episod return total reward end end worker node send result main node log total reward train run tri creat custom us slurm script result core machin start block agent train tri agent insid python code local led initi wai specifi paramet run crash procedur case implement paramet search",
        "Question_gpt_summary_original":"The user is facing challenges while performing a hyperparameter search on their project that uses MPI to aggregate results of multiple agents. They have tried creating a sweep with a custom command using mpirun, but it blocks other agents from training. They also tried setting up the sweeping agent inside the python code with a local controller, but it led to issues regarding parallelization. The user needs to initialize wandb using settings=wandb.Settings(start_method=\"fork\"), but cannot find a way to specify this as a sweep parameter, leading to each run crashing due to incorrect parallelization procedure. The user is seeking advice on what to do in this case.",
        "Question_gpt_summary":"user face challeng perform hyperparamet search project us mpi aggreg result multipl agent tri creat sweep custom command mpirun block agent train tri set sweep agent insid python code local control led issu parallel user need initi set set start method fork wai specifi sweep paramet lead run crash incorrect parallel procedur user seek advic case"
    },
    {
        "Question_title":"Java with Azureml",
        "Question_body":"<p>Hi all,    <\/p>\n<p>i am urgently looking for the possibility of running Java code on Azure ML.     <br \/>\nSo apparantly we have our code in Python and also in Java (let us say our ML Model and pre-processing is in Python).    <\/p>\n<p>What would be the best possible way to work around this ? Your suggestions would be helpful    <\/p>\n<p>Thanks in advance :) <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1670945661683,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hello <a href=\"\/users\/na\/?userid=f5d4c7c7-659d-41ce-a69c-2b7c4f838dad\">@antara.das  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A. I am sorry Azure Machine Learning now is not support Java.     <\/p>\n<p>You can use R &amp; Pyhton in Azure Machine Learning Designer, for Azure Machine Learning SDK, there is only Python SDK now.    <\/p>\n<p>The only one possible way if you want to run it by Java - Call Azure CLI from Java code, but this is not very recommended.    <br \/>\nAzure Machine Learning CLI reference - <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-cli?tabs=public\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-cli?tabs=public<\/a>    <\/p>\n<p>I can forward this feedback to product team for future consideration if you could provide more details about your scenario.     <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1670954670943,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1127395\/java-with-azureml",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"java urgent look possibl run java code apparantli code python java let model pre process python best possibl wai work suggest help thank advanc",
        "Question_preprocessed_content":"java urgent java code code python java best wai work help thank advanc",
        "Question_gpt_summary_original":"The user is seeking advice on how to run Java code on Azure ML while having their ML model and pre-processing in Python. They are looking for suggestions on the best way to work around this issue.",
        "Question_gpt_summary":"user seek advic run java code have model pre process python look suggest best wai work issu"
    },
    {
        "Question_title":"How to configure comet (comet.ml) to log Tensorflow?",
        "Question_body":"<p>Im trying to set comet (<a href=\"https:\/\/www.comet.ml\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml<\/a>) to track my Tensorflow experiment, after I create an Experiment and log the data set i dont get the accuracy in my report.<\/p>\n\n<p>my code:<\/p>\n\n<pre><code>mnist = get_data()\ntrain_step, cross_entropy, accuracy, x, y, y_ = build_model_graph(hyper_params)\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\nexperiment.log_multiple_params(hyper_params)\nexperiment.log_dataset_hash(mnist)\n<\/code><\/pre>\n\n<p>in the example account : <a href=\"https:\/\/www.comet.ml\/view\/Jon-Snow\" rel=\"nofollow noreferrer\">https:\/\/www.comet.ml\/view\/Jon-Snow<\/a> I see that accuracy is reported<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1506094686057,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1514341154200,
        "Question_score_count":3.0,
        "Question_view_count":338.0,
        "Poster_created_time":1506066897167,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":4.0,
        "Answer_body":"<p>you can report accuracy using this method:<\/p>\n\n<ul>\n<li><code>experiment.log_accuracy(train_accuracy)<\/code><\/li>\n<\/ul>\n\n<p>take a look at the full Tensorflow example in our guide:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow\" rel=\"nofollow noreferrer\">https:\/\/github.com\/comet-ml\/comet-quickstart-guide\/tree\/master\/tensorflow<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1506100257932,
        "Answer_last_edit_time":1513514205487,
        "Answer_score_count":3.0,
        "Answerer_created_time":1359884693920,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":9637.0,
        "Answerer_view_count":609.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46368389",
        "Tool":"Comet",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"configur log tensorflow try set http www track tensorflow experi creat experi log data set dont accuraci report code mnist data train step cross entropi accuraci build model graph hyper param experi experi api kei xxxx log code true experi log multipl param hyper param experi log dataset hash mnist exampl account http www view jon snow accuraci report",
        "Question_preprocessed_content":"configur log tensorflow try set track tensorflow experi creat experi log data set dont report code exampl report",
        "Question_gpt_summary_original":"The user is trying to configure comet.ml to track their Tensorflow experiment, but after creating an experiment and logging the data set, they are not getting accuracy in their report. They have shared their code and noticed that accuracy is reported in an example account on comet.ml.",
        "Question_gpt_summary":"user try configur track tensorflow experi creat experi log data set get accuraci report share code notic accuraci report exampl account"
    },
    {
        "Question_title":"Passing nested parameters in the extra_params of the load_context in Kedro",
        "Question_body":"<p>I am trying to load a Kedro context with some extra parameters. My intention is to update the configs in <code>parameters.yml<\/code> with only the ones passed in <code>extra_params<\/code> (<em>so rest of the configs should remain same<\/em>). I will then use this instance of context to run some nodes\/pipelines (<em>Note: I don't want to modify the <code>parameters.yml<\/code> file, just update it for this run<\/em>). I am able to do so for every configs defined at the top\/root level in <code>parameters.yml<\/code> but only the passed configs are persisted in case of nested parameters. <\/p>\n\n<p>To replicate the issue, <\/p>\n\n<p><strong>parameters.yml<\/strong><\/p>\n\n<pre><code>config_root1: \"name\"\nconfig_root2: \"surname\"\nconfig_root3:\n   config_leaf1: 10\n   config_leaf2: 20\n   config_leaf3: 30\n<\/code><\/pre>\n\n<p>Extra params defined as,<\/p>\n\n<pre><code>extra_params = {'config_root1': 'new_name', 'config_root3': {'config_leaf1': 11}}\n<\/code><\/pre>\n\n<p>Loading the context with <code>extra_params<\/code> by, <\/p>\n\n<pre><code>from kedro.framework.context import load_context    \ncontext = load_context(proj_path, extra_params=extra_params)\n<\/code><\/pre>\n\n<p>The parameters in context is updated to, <\/p>\n\n<pre><code>config_root1: \"new_name\"\nconfig_root2: \"surname\"\nconfig_root3:\n   config_leaf1: 11\n<\/code><\/pre>\n\n<p>Points to note, <\/p>\n\n<ol>\n<li><code>config_root1<\/code> was at root level and value was passed in <code>extra_params<\/code>, hence it was updated.<\/li>\n<li><code>config_root2<\/code> was at root level and no value was passed, hence it remains the same.<\/li>\n<li><code>config_leaf1<\/code> was at leaf level, value passed hence updated.<\/li>\n<li><code>config_leaf{2, 3}<\/code> were at leaf level, no value passed hence dropped.<\/li>\n<\/ol>\n\n<p>It seems the issue has to do with how <a href=\"https:\/\/stackoverflow.com\/questions\/38987\/how-do-i-merge-two-dictionaries-in-a-single-expression-in-python\">dictionary<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/7204805\/how-to-merge-dictionaries-of-dictionaries\/24088493#24088493\">nested dictionary<\/a> are merged. <\/p>\n\n<p>Any workarounds other than always passing all of the parameters in <code>extra_params<\/code> (<em>even if only few needs to be changed<\/em>)?<\/p>\n\n<p>Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1591359264147,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":347.0,
        "Poster_created_time":1338197185852,
        "Poster_location":"India",
        "Poster_reputation_count":1025.0,
        "Poster_view_count":228.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62215136",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pass nest paramet extra param load context try load context extra paramet intent updat config paramet yml on pass extra param rest config remain us instanc context run node pipelin note want modifi paramet yml file updat run abl config defin root level paramet yml pass config persist case nest paramet replic issu paramet yml config root config root surnam config root config leaf config leaf config leaf extra param defin extra param config root new config root config leaf load context extra param framework context import load context context load context proj path extra param extra param paramet context updat config root new config root surnam config root config leaf point note config root root level valu pass extra param updat config root root level valu pass remain config leaf leaf level valu pass updat config leaf leaf level valu pass drop issu dictionari nest dictionari merg workaround pass paramet extra param need chang thank",
        "Question_preprocessed_content":"nest paramet try load context extra paramet intent updat config on us instanc context run abl config defin level config persist case nest paramet replic extra param defin load context paramet context updat point note level valu updat level valu remain leaf level valu updat leaf level valu dictionari nest dictionari merg workaround paramet thank",
        "Question_gpt_summary_original":"The user is trying to load a Kedro context with extra parameters to update the configs in parameters.yml for a single run without modifying the file. While the user is able to update the top-level configs, only the passed configs are persisted in case of nested parameters. The issue seems to be related to how dictionaries and nested dictionaries are merged. The user is looking for a workaround other than passing all parameters in extra_params.",
        "Question_gpt_summary":"user try load context extra paramet updat config paramet yml singl run modifi file user abl updat level config pass config persist case nest paramet issu relat dictionari nest dictionari merg user look workaround pass paramet extra param"
    },
    {
        "Question_title":"How to provide fold information in WandbCallback?",
        "Question_body":"<p>How to provide fold information in multi-fold training?<\/p>\n<p>I am trying to do something like this:<\/p>\n<pre><code class=\"lang-auto\">for fold in range(5):\nmodel.fit()\n<\/code><\/pre>\n<p>So it actually creates a single graph where steps are continued from the last executed step of previous epoch.<br>\nFor ref see below:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5.png\" data-download-href=\"\/uploads\/short-url\/bUQfG2SdqWgNhB5QoDwBGjbyy3P.png?dl=1\" title=\"Screenshot 2021-12-23 at 12.19.28 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5_2_690x382.png\" alt=\"Screenshot 2021-12-23 at 12.19.28 PM\" data-base62-sha1=\"bUQfG2SdqWgNhB5QoDwBGjbyy3P\" width=\"690\" height=\"382\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5_2_690x382.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5_2_1035x573.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5384d8c3e8faa471e61a07d930f7f839a42650b5_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2021-12-23 at 12.19.28 PM<\/span><span class=\"informations\">1098\u00d7608 28.2 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_created_time":1640242246866,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":234.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-provide-fold-information-in-wandbcallback\/1599",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"provid fold inform callback provid fold inform multi fold train try like fold rang model fit actual creat singl graph step continu execut step previou epoch ref screenshot",
        "Question_preprocessed_content":"provid fold inform provid fold inform train try like creat singl graph step continu execut step previou epoch ref",
        "Question_gpt_summary_original":"The user is trying to provide fold information in multi-fold training using WandbCallback and is seeking guidance on how to create a single graph where steps are continued from the last executed step of the previous epoch.",
        "Question_gpt_summary":"user try provid fold inform multi fold train callback seek guidanc creat singl graph step continu execut step previou epoch"
    },
    {
        "Question_title":"azure datasource throwing error in Kedro datacatalog",
        "Question_body":"<p>I am facing error when configuring azure blob storage dataset in kedro datacatalog.<\/p>\n<p>I have the dataset defined in my catalog.yml as below:<\/p>\n<pre><code>brand_dataset:\n  type: pandas.CSVDataSet\n  filepath: &quot;abfs:\/\/container\/my_file.csv&quot;\n  credentials: my_creds\n  load_args:\n    sep:';'\n<\/code><\/pre>\n<p>When trying to access the dataset from ipython ,Facing the below error<\/p>\n<pre><code>DataSetError: \nunexpected unindent (core.py, line 160).\nFailed to instantiate DataSet 'brand_dataset' of type `kedro.extras.datasets.pandas.csv_dataset.CSVDataSet`.\n<\/code><\/pre>\n<p>I followed the document<br \/>\n<a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/01_data_catalog.html\" rel=\"nofollow noreferrer\">https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/01_data_catalog.html<\/a><br \/>\nparticularly example:15<br \/>\ncould not figure out why the error is?<br \/>\nI am working with kedro from inside docker container.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1637871347587,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1637872237812,
        "Question_score_count":0.0,
        "Question_view_count":130.0,
        "Poster_created_time":1495105930728,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":13.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70116785",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur datasourc throw error datacatalog face error configur azur blob storag dataset datacatalog dataset defin catalog yml brand dataset type panda csvdataset filepath abf contain file csv credenti cred load arg sep try access dataset ipython face error dataseterror unexpect unind core line fail instanti dataset brand dataset type extra dataset panda csv dataset csvdataset follow document http readthedoc stabl data data catalog html particularli exampl figur error work insid docker contain",
        "Question_preprocessed_content":"azur datasourc throw datacatalog face configur azur blob storag dataset datacatalog dataset defin try dataset ipython face document particularli exampl figur work insid docker contain",
        "Question_gpt_summary_original":"The user is encountering an error while configuring an Azure Blob Storage dataset in Kedro data catalog. The error occurs when trying to access the dataset from ipython and the error message suggests that there is an unexpected unindent in the code. The user has followed the documentation but is still unable to figure out the cause of the error. The user is working with Kedro from inside a Docker container.",
        "Question_gpt_summary":"user encount error configur azur blob storag dataset data catalog error occur try access dataset ipython error messag suggest unexpect unind code user follow document unabl figur caus error user work insid docker contain"
    },
    {
        "Question_title":"Change model file save location on AWS SageMaker Training Job",
        "Question_body":"<p>I am trying to run custom python\/sklearn sagemaker script on AWS, basically learning from these examples: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>All works fine, if define the arguments, train the model and output the file:<\/p>\n<pre><code>parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\nparser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\nparser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n# train the model...\n\njoblib.dump(model, os.path.join(args.model_dir, &quot;model.joblib&quot;))\n<\/code><\/pre>\n<p>And call the job with:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test'}, wait=False)\n<\/code><\/pre>\n<p>In this case model gets stored on different auto-generated bucket, which I do not want. I want to get the output (.joblib file) in the same s3 bucket I took data from. So I add the parameter <code>model-dir<\/code>:<\/p>\n<pre><code>aws_sklearn.fit({'train': 's3:\/\/path\/to\/train', 'test': 's3:\/\/path\/to\/test', `model-dir`: 's3:\/\/path\/to\/model'}, wait=False)\n<\/code><\/pre>\n<p>But it results in error:\n<code>FileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/path\/to\/model\/model.joblib'<\/code><\/p>\n<p>Same happens if I hardcode the output path inside the training script.<\/p>\n<p>So the main question, how can I get the output file in the bucket of my choice?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1610533493827,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":1244.0,
        "Poster_created_time":1572957474856,
        "Poster_location":null,
        "Poster_reputation_count":123.0,
        "Poster_view_count":18.0,
        "Answer_body":"<p>You can use parameter <code>output_path<\/code> when you define the estimator. If you use the\n<code>model_dir<\/code> I guess you have to create that bucket beforehand, but you have the advantage that artifacts can be saved in real time during the training (if the instance has rights on S3). You can take a look at my <a href=\"https:\/\/github.com\/roccopietrini\/TFSagemakerDetection\" rel=\"nofollow noreferrer\">repo<\/a> for this specific case.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1610545645387,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65699980",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"chang model file save locat train job try run custom python sklearn script aw basic learn exampl http github com aw amazon exampl blob master python sdk scikit learn randomforest sklearn endend ipynb work fine defin argument train model output file parser add argument model dir type str default environ model dir parser add argument train type str default environ channel train parser add argument test type str default environ channel test train model joblib dump model path join arg model dir model joblib job aw sklearn fit train path train test path test wait fals case model get store differ auto gener bucket want want output joblib file bucket took data add paramet model dir aw sklearn fit train path train test path test model dir path model wait fals result error filenotfounderror errno file directori path model model joblib happen hardcod output path insid train script main question output file bucket choic",
        "Question_preprocessed_content":"chang model file save locat train job try run custom script aw learn exampl work fine defin argument train model output file job case model get store bucket want want output bucket data paramet result hardcod output path insid train script main question output file bucket choic",
        "Question_gpt_summary_original":"The user is trying to run a custom python\/sklearn sagemaker script on AWS and wants to save the output (.joblib file) in the same S3 bucket from where the data was taken. However, when the user adds the parameter \"model-dir\" to specify the output location, it results in an error \"FileNotFoundError: [Errno 2] No such file or directory\". The user is seeking a solution to save the output file in the bucket of their choice.",
        "Question_gpt_summary":"user try run custom python sklearn script aw want save output joblib file bucket data taken user add paramet model dir specifi output locat result error filenotfounderror errno file directori user seek solut save output file bucket choic"
    },
    {
        "Question_title":"Update real-time webservice with the retrained endpoint or published pipeline",
        "Question_body":"<p>I have already tried with the documentation <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-update-web-service\">how-to-deploy-update-web-service<\/a>. I need a detailed explanation on how to update a web service with the experiment run or pipeline endpoint. There is no clear explanation in the link. I trained the model in the designer from scratch. I need to automate the whole flow of retraining the model and updating the web service. Any help is appreciated.    <\/p>\n<p>Please answer the following questions:    <\/p>\n<ol>\n<li> How do I give the model path for a trained model from the retrained experiment or published pipeline?    <\/li>\n<li> What is the tags field that I need to mention ?    <\/li>\n<li> what is the deploy environment I need to specify?    <\/li>\n<li> What to specify in the place of score.py?    <\/li>\n<\/ol>\n<blockquote>\n<p>ws = Workspace.get(name=&quot;xxx&quot;, subscription_id='xxx, resource_group='xxx')    <br \/>\nnew_model = Model.register(model_path=&quot;azureml\\ce6116ec-cd48-4115-a567-e0eeed49f5b5\\Trained_model&quot;,    <br \/>\n                           model_name=&quot;Trained_model&quot;,    <br \/>\n                           tags=tags)    <br \/>\ndeploy_env = Environment.get(workspace=ws, name=&quot;AzureML-PyTorch-1.4-GPU&quot;, version=&quot;3&quot;)    <br \/>\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;,    <br \/>\n                                   environment=deploy_env)    <\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1613014495637,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/268354\/update-real-time-webservice-with-the-retrained-end",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"updat real time webservic retrain endpoint publish pipelin tri document deploi updat web servic need detail explan updat web servic experi run pipelin endpoint clear explan link train model design scratch need autom flow retrain model updat web servic help appreci answer follow question model path train model retrain experi publish pipelin tag field need mention deploi environ need specifi specifi place score workspac xxx subscript xxx resourc group xxx new model model regist model path ceec eeeedfb train model model train model tag tag deploi env environ workspac pytorch gpu version infer config inferenceconfig entri script score environ deploi env",
        "Question_preprocessed_content":"updat webservic retrain endpoint publish pipelin tri document detail explan updat web servic experi run pipelin endpoint clear explan link train model design scratch autom flow retrain model updat web servic help answer question model path train model retrain experi publish pipelin tag field mention deploi environ specifi specifi place tag tag version",
        "Question_gpt_summary_original":"The user is seeking a detailed explanation on how to update a web service with the experiment run or pipeline endpoint. They have trained the model from scratch and need to automate the whole flow of retraining the model and updating the web service. The user has specific questions regarding how to give the model path for a trained model, what to mention in the tags field, what deploy environment to specify, and what to specify in place of score.py.",
        "Question_gpt_summary":"user seek detail explan updat web servic experi run pipelin endpoint train model scratch need autom flow retrain model updat web servic user specif question model path train model mention tag field deploi environ specifi specifi place score"
    },
    {
        "Question_title":"Multiple tables",
        "Question_body":"<p>I know how to create a table with a data frame programmatically. However, I have two data frames, and they have different number of rows, so I cannot combine them into a single data frame. How do I upload two different tables to a Weights&amp;Biases project? Somehow, I suspect that the following is not the correct approach:<\/p>\n<pre><code class=\"lang-python\">    train_df = pd.DataFrame({\n        'tx':train_x,\n        'ty':train_y,\n    })\n    valid_df = pd.DataFrame({\n        'vx':valid_x,\n        'vy':valid_y\n    })\n\n    # How to add multiple tables\n\n    wandb.log({\"table\": train_df}, commit=False)\n    wandb.log({\"table\": valid_df}, commit=False)\n<\/code><\/pre>\n<p>Any help is greatly appreciated.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1659744551959,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":236.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a> , see <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\">this document<\/a> on how to create tables from dataframes and please let me know if you have any questions.<\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1660166829977,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/multiple-tables\/2856",
        "Tool":"Weights & Biases",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"multipl tabl know creat tabl data frame programmat data frame differ number row combin singl data frame upload differ tabl weight bias project suspect follow correct approach train datafram train train valid datafram valid valid add multipl tabl log tabl train commit fals log tabl valid commit fals help greatli appreci",
        "Question_preprocessed_content":"multipl tabl know creat tabl data frame data frame number row combin singl data frame upload tabl weight bias project suspect help greatli",
        "Question_gpt_summary_original":"The user is trying to upload two different tables to a Weights&Biases project, but the tables have different numbers of rows and cannot be combined into a single data frame. They are seeking guidance on the correct approach to upload multiple tables.",
        "Question_gpt_summary":"user try upload differ tabl weight bias project tabl differ number row combin singl data frame seek guidanc correct approach upload multipl tabl"
    },
    {
        "Question_title":"Unable to serve an mlflow model locally",
        "Question_body":"<p>I have created an mlflow model with custom pyfunc. It shows the results when I send input to the loaded model in Jupyter notebook.\nHowever if I am trying to serve it to a local port<\/p>\n<pre><code>!mlflow models serve -m Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001\n<\/code><\/pre>\n<p>I am getting this error<\/p>\n<pre><code> Traceback (most recent call last):\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/bin\/mlflow&quot;, line 10, in &lt;module&gt;\n    sys.exit(cli())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 56, in serve\n    install_mlflow=install_mlflow).serve(model_uri=model_uri, port=port,\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/models\/cli.py&quot;, line 163, in _get_flavor_backend\n    append_to_uri_path(underlying_model_uri, &quot;MLmodel&quot;), output_path=tmp.path())\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/tracking\/artifact_utils.py&quot;, line 76, in _download_artifact_from_uri\n    artifact_path=artifact_path, dst_path=output_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 67, in download_artifacts\n    return super(LocalArtifactRepository, self).download_artifacts(artifact_path, dst_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 140, in download_artifacts\n    return download_file(artifact_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py&quot;, line 105, in download_file\n    self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/site-packages\/mlflow\/store\/artifact\/local_artifact_repo.py&quot;, line 95, in _download_file\n    shutil.copyfile(remote_file_path, local_path)\n  File &quot;\/home\/subhojyoti\/miniconda3\/envs\/python3-env\/lib\/python3.6\/shutil.py&quot;, line 120, in copyfile\n    with open(src, 'rb') as fsrc:\nFileNotFoundError: [Errno 2] No such file or directory: 'Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model\/MLmodel'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1611838887777,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":1277.0,
        "Poster_created_time":1573739890560,
        "Poster_location":null,
        "Poster_reputation_count":115.0,
        "Poster_view_count":25.0,
        "Answer_body":"<p>From your error traceback, the model artifact can't be located. In your code, you are executing the 'mlflow' command from within a Jupyter Notebook. I would suggest trying the following:<\/p>\n<ol>\n<li>Check if your models artifacts are on the path you are using Home\/miniconda3\/envs\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model<\/li>\n<li>Try opening a terminal, then <code>cd \/Home\/miniconda3\/envs<\/code> and  execute <code>mlflow models serve -m .\/mlruns\/0\/baa40963927a49258c845421e3175c06\/artifacts\/model -p 8001<\/code><\/li>\n<li>MLFlow offers different solutions to serve a model, you can try to register your model and refer to it as &quot;models:\/{model_name}\/{stage}&quot; as mentioned in the Model Registry <a href=\"https:\/\/mlflow.org\/docs\/latest\/model-registry.html#serving-an-mlflow-model-from-model-registry\" rel=\"nofollow noreferrer\">docs<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1611840603100,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1525393797416,
        "Answerer_location":"Philadelphia, USA",
        "Answerer_reputation_count":130.0,
        "Answerer_view_count":10.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65937623",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl serv model local creat model custom pyfunc show result send input load model jupyt notebook try serv local port model serv home miniconda env mlrun baaacec artifact model get error traceback recent file home subhojyoti miniconda env python env bin line sy exit cli file home subhojyoti miniconda env python env lib python site packag click core line return self main arg kwarg file home subhojyoti miniconda env python env lib python site packag click core line main self invok ctx file home subhojyoti miniconda env python env lib python site packag click core line invok return process result sub ctx command invok sub ctx file home subhojyoti miniconda env python env lib python site packag click core line invok return process result sub ctx command invok sub ctx file home subhojyoti miniconda env python env lib python site packag click core line invok return ctx invok self callback ctx param file home subhojyoti miniconda env python env lib python site packag click core line invok return callback arg kwarg file home subhojyoti miniconda env python env lib python site packag model cli line serv instal instal serv model uri model uri port port file home subhojyoti miniconda env python env lib python site packag model cli line flavor backend append uri path underli model uri mlmodel output path tmp path file home subhojyoti miniconda env python env lib python site packag track artifact util line download artifact uri artifact path artifact path dst path output path file home subhojyoti miniconda env python env lib python site packag store artifact local artifact repo line download artifact return super localartifactrepositori self download artifact artifact path dst path file home subhojyoti miniconda env python env lib python site packag store artifact artifact repo line download artifact return download file artifact path file home subhojyoti miniconda env python env lib python site packag store artifact artifact repo line download file self download file remot file path fullpath local path local file path file home subhojyoti miniconda env python env lib python site packag store artifact local artifact repo line download file shutil copyfil remot file path local path file home subhojyoti miniconda env python env lib python shutil line copyfil open src fsrc filenotfounderror errno file directori home miniconda env mlrun baaacec artifact model mlmodel",
        "Question_preprocessed_content":"unabl serv model creat model custom pyfunc show result send input load model jupyt try serv local port",
        "Question_gpt_summary_original":"The user is unable to serve an mlflow model locally and is encountering a \"FileNotFoundError\" when trying to run the model on a local port. The error message suggests that the file or directory specified in the model URI does not exist.",
        "Question_gpt_summary":"user unabl serv model local encount filenotfounderror try run model local port error messag suggest file directori specifi model uri exist"
    },
    {
        "Question_title":"MLFlow unable to log pytorch model",
        "Question_body":"<p>After training a model, I am trying to log it to mlflow with <code>mlflow.pytorch.log_model(model, artifact_path=&quot;model&quot;,pickle_module=pickle)<\/code> but i get the error:<\/p>\n<p><code>yaml.representer.RepresenterError: ('cannot represent an object', '1.11.0+cu102')<\/code><\/p>\n<p>I definitely send the model to cpu before doing so and confirm its there with <code>next(model.parameters()).device<\/code>. How can I get mlflow to log my model?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1654785162453,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1654788108543,
        "Question_score_count":0.0,
        "Question_view_count":142.0,
        "Poster_created_time":1652137865016,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72562160",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl log pytorch model train model try log pytorch log model model artifact path model pickl modul pickl error yaml represent representererror repres object definit send model cpu confirm model paramet devic log model",
        "Question_preprocessed_content":"unabl log pytorch model train model try log definit send model cpu confirm log model",
        "Question_gpt_summary_original":"The user is facing an issue while trying to log a PyTorch model to MLFlow using the \"mlflow.pytorch.log_model\" function. The error message \"yaml.representer.RepresenterError: ('cannot represent an object', '1.11.0+cu102')\" is displayed. The user has confirmed that the model is on the CPU and is seeking a solution to log the model to MLFlow.",
        "Question_gpt_summary":"user face issu try log pytorch model pytorch log model function error messag yaml represent representererror repres object displai user confirm model cpu seek solut log model"
    },
    {
        "Question_title":"When will Hebrew language be available in Text-To-Speech API?",
        "Question_body":"Hi everyone,\nWhen will Hebrew language be available in Text-To-Speech API?\nin this list\nhttps:\/\/cloud.google.com\/text-to-speech\/docs\/voices\n\u00a0\nthanks!",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1652854440000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":173.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/When-will-Hebrew-language-be-available-in-Text-To-Speech-API\/td-p\/424088\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"hebrew languag avail text speech api hebrew languag avail text speech api list http cloud googl com text speech doc voic thank",
        "Question_preprocessed_content":"hebrew languag avail api hebrew languag avail api list thank",
        "Question_gpt_summary_original":"The user is inquiring about the availability of Hebrew language in the Text-To-Speech API and is seeking information on when it will be added to the list of available languages.",
        "Question_gpt_summary":"user inquir avail hebrew languag text speech api seek inform ad list avail languag"
    },
    {
        "Question_title":"Getting ConnectTimeout in offline mode when trying to log an image",
        "Question_body":"<p>I am running wandb in offline mode since I don\u2019t have an internet connection on the compute nodes that I use for my experiments.<br>\nThis works fine when I\u2019m logging training loss and other things.<br>\nWhen I try to log images, however, I get the following warning <code>wandb: Network error (ConnectTimeout), entering retry loop.<\/code>  and the run waits forever.<\/p>\n<p>The logging happens through:<\/p>\n<pre><code class=\"lang-python\">images = []\nfor i in range(10):\n    images.append(wandb.Image(image[i], caption=f\"{caption}.{i}\"))\nwandb.log({category: images})\n<\/code><\/pre>\n<p>I can even see that the offline mode is active since I get the following output when I stop the run:<\/p>\n<pre><code class=\"lang-auto\">wandb: You can sync this run to the cloud by running:\nwandb: wandb sync \/scratch_emmy\/outputs\/wandb\/offline-run-20230208_142336-9685bcf5ea8d5d35ccc9d93b2d035832\n<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_created_time":1675863242911,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":157.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/getting-connecttimeout-in-offline-mode-when-trying-to-log-an-image\/3844",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"get connecttimeout offlin mode try log imag run offlin mode dont internet connect comput node us experi work fine log train loss thing try log imag follow warn network error connecttimeout enter retri loop run wait forev log happen imag rang imag append imag imag caption caption log categori imag offlin mode activ follow output stop run sync run cloud run sync scratch emmi output offlin run bcfeaddcccdbd",
        "Question_preprocessed_content":"mode try log imag mode dont internet comput node us experi work fine train thing try log imag warn run wait forev mode activ output stop run",
        "Question_gpt_summary_original":"The user is encountering a ConnectTimeout error when trying to log images in offline mode using wandb. The error occurs even though offline mode is active and the user is able to log other data successfully. The issue seems to be related to the way images are being logged using the wandb.Image function.",
        "Question_gpt_summary":"user encount connecttimeout error try log imag offlin mode error occur offlin mode activ user abl log data successfulli issu relat wai imag log imag function"
    },
    {
        "Question_title":"Multi user on one DB ok?",
        "Question_body":"When I use tracking directly on a Database (PostgreSQL) would it cause problems when two computer do tracking into the same DB at the same time?\n\n\n\nSame question with MLFlow UI: Is it ok when we connect two UIs from two different computers to one (the same) DB?\n\n\nMy guess is that it is no problem but just want to make sure.\n\n\nThanks\nPhilip",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1583597894000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":24.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/VQc6nWUNjYM",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"multi user us track directli databas postgresql caus problem track time question connect ui differ comput guess problem want sure thank philip",
        "Question_preprocessed_content":"multi user us track directli databas caus problem track time question ui comput problem want sure thank philip",
        "Question_gpt_summary_original":"The user is inquiring about the possibility of multiple users tracking data directly on a PostgreSQL database at the same time without causing any issues. They also want to know if connecting two MLFlow UIs from different computers to the same database would cause any problems. They are seeking confirmation that this is possible without any issues.",
        "Question_gpt_summary":"user inquir possibl multipl user track data directli postgresql databas time caus issu want know connect ui differ comput databas caus problem seek confirm possibl issu"
    },
    {
        "Question_title":"ERROR: Setup iteration failed: Unidentified error, check logs in portal \/ compute",
        "Question_body":"<p>Getting error when trying to run the autoML through training cluster. But it is running successfully via the local run. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1581335967710,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":91.0,
        "Poster_created_time":1522597272888,
        "Poster_location":"Kolkata, West Bengal, India",
        "Poster_reputation_count":671.0,
        "Poster_view_count":76.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60149987",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error setup iter fail unidentifi error check log portal comput get error try run automl train cluster run successfulli local run",
        "Question_preprocessed_content":"setup iter fail unidentifi check log portal comput try run automl train cluster local run",
        "Question_gpt_summary_original":"The user is encountering an error while trying to run autoML through a training cluster, but it runs successfully via local run. The error message states \"Setup iteration failed: Unidentified error, check logs in portal \/ compute\".",
        "Question_gpt_summary":"user encount error try run automl train cluster run successfulli local run error messag state setup iter fail unidentifi error check log portal comput"
    },
    {
        "Question_title":"Load the most recent data from Date partitioned folder",
        "Question_body":"<p>Hello,  <\/p>\n<p>I have set up a pipeline with Azure Data Factory in order to move data from my on-premises Oracle DB to parquet files in an Azure blob container every 10 days.  <br \/>\nThe folder structure of my blob container is as follows:  <\/p>\n<blockquote>\n<p>onpremises\/2022\/02\/18\/file1.parquet  <br \/>\nonpremises\/2022\/02\/18\/file2.parquet  <br \/>\nonpremises\/2022\/02\/18\/file3.parquet  <\/p>\n<p>onpremises\/2022\/02\/28\/file1.parquet  <br \/>\nonpremises\/2022\/02\/28\/file2.parquet  <br \/>\nonpremises\/2022\/02\/28\/file3.parquet  <\/p>\n<p>onpremises\/2022\/03\/08\/file1.parquet  <br \/>\nonpremises\/2022\/03\/08\/file2.parquet  <br \/>\nonpremises\/2022\/03\/08\/file3.parquet  <\/p>\n<p>...  <\/p>\n<\/blockquote>\n<p>Now I'm trying to set up a pipeline in Azure ML which will run every time new data is coming into this container.  <br \/>\nIn my script below, I start by getting a reference to my container before calling the function 'from_parquet_files' to read from Parquet files.  <br \/>\nProblem: the script reads all files from every folder and adds a data column to the dataset (I believe it is because of the parameter 'partition_format').  <\/p>\n<pre><code>from azureml.core import Workspace, Datastore\n\n# Get a reference to the workspace\nws = Workspace.from_config()\n\n# Reference to the datastore 'onpremises' from which we will contruct our dataset\ndata_store = Datastore(ws, &quot;onpremises&quot;)\n\nfrom azureml.core import Dataset\n\n# Create a dataset from the data stored in datastore 'onpremises' at the specified path\nspecs_dataset = Dataset.Tabular.from_parquet_files(path=(data_store, ''), partition_format='\/{PartitionDate:yyyy\/MM\/dd}\/')\n\n# Register the dataset to the workspace. Increments the version if dataset already exists.\nspecs_dataset.register(workspace=ws, name=&quot;specs&quot;, description=&quot;Specs data from on-premises&quot;, create_new_version=True)\n<\/code><\/pre>\n<p>What I would like to do is to read only the most recent set of files (in my case, files listed under 'onpremises\/2022\/03\/08\/').  <br \/>\nAs the pipeline will run automatically, it should detect what is the most recent data among the folder structure.  <br \/>\nIs there a simple way to achieve this programmatically?  <\/p>\n<p>Thanks in advance.  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1646726960863,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>@ThierryL-3166 You could only pass the required files by getting the year, month and day from the timestamp or date output. If your pipeline runs on schedule, you could list all the paths for all the days since the last run and load the files. I think something like below should work.    <\/p>\n<pre><code># create tabular dataset from multiple paths  \nfrom datetime import date  \nfrom datetime import timedelta  \n  \ntoday = date.today()  \n  \nyesterday = today - timedelta(1)  \n      \nd1 = today.strftime(&quot;%Y\/%m\/%d&quot;)  \nd2=yesterday.strftime(&quot;%Y\/%m\/%d&quot;)   \n  \n  \npath1 = 'onpremises\/'+ d1 + '\/*.parquet'  \npath2 = 'onpremises\/'+ d2 + '\/*.parquet'  \n  \ndata_paths = [(datastore, path1),(datastore, path2)]  \ntabular_dataset = Dataset.Tabular.from_parquet_files(path=data_paths)  \n<\/code><\/pre>\n<p>If an answer is helpful, please click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> which might help other community members reading this thread.    <\/p>\n",
        "Answer_comment_count":1.0,
        "Question_closed_time":1646813594767,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/763313\/load-the-most-recent-data-from-date-partitioned-fo",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load recent data date partit folder hello set pipelin azur data factori order data premis oracl parquet file azur blob contain dai folder structur blob contain follow onpremis file parquet onpremis file parquet onpremis file parquet onpremis file parquet onpremis file parquet onpremis file parquet onpremis file parquet onpremis file parquet onpremis file parquet try set pipelin run time new data come contain script start get refer contain call function parquet file read parquet file problem script read file folder add data column dataset believ paramet partit format core import workspac datastor refer workspac workspac config refer datastor onpremis contruct dataset data store datastor onpremis core import dataset creat dataset data store datastor onpremis specifi path spec dataset dataset tabular parquet file path data store partit format partitiond yyyi regist dataset workspac increment version dataset exist spec dataset regist workspac spec descript spec data premis creat new version true like read recent set file case file list onpremis pipelin run automat detect recent data folder structur simpl wai achiev programmat thank advanc",
        "Question_preprocessed_content":"load recent data date partit folder set pipelin azur data factori order data oracl parquet file azur blob contain dai folder structur blob contain try set pipelin run time new data come contain script start refer contain function read parquet file problem script read file folder data column dataset like read recent set file pipelin run detect recent data folder structur simpl wai achiev thank advanc",
        "Question_gpt_summary_original":"The user has set up a pipeline with Azure Data Factory to move data from an on-premises Oracle DB to parquet files in an Azure blob container every 10 days. The folder structure of the blob container is partitioned by date. The user is trying to set up a pipeline in Azure ML to read only the most recent set of files from the container, but the current script reads all files from every folder and adds a data column to the dataset. The user is looking for a way to programmatically detect the most recent data among the folder structure.",
        "Question_gpt_summary":"user set pipelin azur data factori data premis oracl parquet file azur blob contain dai folder structur blob contain partit date user try set pipelin read recent set file contain current script read file folder add data column dataset user look wai programmat detect recent data folder structur"
    },
    {
        "Question_title":"How send to image payload to the Deep learning model using lambda and send get the Image respone",
        "Question_body":"<p>I am working on Deploying background removal using AWS lambda, I didn't understand how to create a proper image payload to model and get the response back as an image.\nplease help me with this, I am struggling with this for one week.\nhere I tried:<\/p>\n<pre><code> model_dir='model\/u2net.pth'\n    model=load_models(model_dir)\n    \n    def lambda_handler(event, context):\n        \n        url = event['url']\n        img = io.imread(url)\n        # classify image\n        img = u2net_api_call(img, model)\n        output_image = img.numpy()\n        output_image = np.uint8(output_image.transpose(1, 2, 0) * 255)\n        output_image = Image.fromarray(output_image)\n    \n      # convert the PIL image to base64\n        result = {\n          &quot;output&quot;: img_to_base64_str(output_image)\n      }\n    \n      # send the result back to the client inside the body field\n        return {\n          &quot;statusCode&quot;: 200,\n          &quot;body&quot;: json.dumps(result),\n          &quot;headers&quot;: {\n              'Content-Type': 'application\/json',\n              'Access-Control-Allow-Origin': '*'\n          }\n      }\n<\/code><\/pre>\n<p>I have deployed these with lambda and docker images, I have created the docker image and push ECR and I am adding with lambda.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1646168848977,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":32.0,
        "Poster_created_time":1583415050007,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation_count":1.0,
        "Poster_view_count":15.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71314659",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"send imag payload deep learn model lambda send imag respon work deploi background remov aw lambda understand creat proper imag payload model respons imag help struggl week tri model dir model unet pth model load model model dir def lambda handler event context url event url img imread url classifi imag img unet api img model output imag img numpi output imag uint output imag transpos output imag imag fromarrai output imag convert pil imag base result output img base str output imag send result client insid bodi field return statuscod bodi json dump result header content type applic json access control allow origin deploi lambda docker imag creat docker imag push ecr ad lambda",
        "Question_preprocessed_content":"send imag payload learn model lambda send imag respon work deploi background remov aw lambda understand creat proper imag payload model respons imag help tri deploi lambda docker imag creat docker imag push ecr lambda",
        "Question_gpt_summary_original":"The user is struggling to create a proper image payload to send to a deep learning model using AWS Lambda and receive the response back as an image. They have tried deploying the model with Lambda and Docker images, but are still facing difficulties.",
        "Question_gpt_summary":"user struggl creat proper imag payload send deep learn model aw lambda receiv respons imag tri deploi model lambda docker imag face difficulti"
    },
    {
        "Question_title":"Amazon sagemaker. SKlearn estimator vs Tensorflow estimator - why requirements_file is not present in one of them?",
        "Question_body":"<p>I am looking at definitions of two estimators SKLearn and Tensorflow in Amazon Sagemaker:<\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.sklearn.html\" rel=\"nofollow noreferrer\">SKLearn<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/sagemaker.tensorflow.html\" rel=\"nofollow noreferrer\">Tensorflow<\/a><\/p>\n\n<pre><code>class sagemaker.sklearn.estimator.SKLearn(entry_point, framework_version='0.20.0', source_dir=None, hyperparameters=None, py_version='py3', image_name=None, **kwargs)\n\nclass sagemaker.tensorflow.estimator.TensorFlow(training_steps=None, evaluation_steps=None, checkpoint_path=None, py_version='py2', framework_version=None, model_dir=None, requirements_file='', image_name=None, script_mode=False, distributions=None, **kwargs)\n<\/code><\/pre>\n\n<p>Tensorflow has requirements_file parameter, while SKLearn does not. Is there reason why? How can I add <code>requirements.txt<\/code> to SKLearn estimator?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1557166184387,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":221.0,
        "Poster_created_time":1348677250368,
        "Poster_location":null,
        "Poster_reputation_count":6498.0,
        "Poster_view_count":988.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56010337",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sklearn estim tensorflow estim requir file present look definit estim sklearn tensorflow sklearn tensorflow class sklearn estim sklearn entri point framework version sourc dir hyperparamet version imag kwarg class tensorflow estim tensorflow train step evalu step checkpoint path version framework version model dir requir file imag script mode fals distribut kwarg tensorflow requir file paramet sklearn reason add requir txt sklearn estim",
        "Question_preprocessed_content":"sklearn estim tensorflow estim present definit estim sklearn tensorflow sklearn tensorflow tensorflow paramet sklearn reason sklearn estim",
        "Question_gpt_summary_original":"The user is comparing the definitions of two Amazon Sagemaker estimators, SKLearn and Tensorflow, and has noticed that Tensorflow has a \"requirements_file\" parameter while SKLearn does not. The user is seeking to understand the reason for this difference and how to add a \"requirements.txt\" file to the SKLearn estimator.",
        "Question_gpt_summary":"user compar definit estim sklearn tensorflow notic tensorflow requir file paramet sklearn user seek understand reason differ add requir txt file sklearn estim"
    },
    {
        "Question_title":"Value Error: Instances of wandb.Artifact and wandb.apis.public.Artifact can only be top level keys i",
        "Question_body":"<p>Hello, sorry for the bad english. I\u2019m using for a personal project wandb to train Yolov5 in a kaggle environment. Im getting this type of error:<\/p>\n<p>Traceback (most recent call last):<br>\nFile \u201ctrain.py\u201d, line 643, in<br>\nmain(opt)<br>\nFile \u201ctrain.py\u201d, line 539, in main<br>\ntrain(opt.hyp, opt, device, callbacks)<br>\nFile \u201ctrain.py\u201d, line 95, in train<br>\nloggers = Loggers(save_dir, weights, opt, hyp, LOGGER) # loggers instance<br>\nFile \u201c\/content\/yolov5\/utils\/loggers\/ <strong>init<\/strong> .py\u201d, line 73, in <strong>init<\/strong><br>\nself.wandb = WandbLogger(self.opt, run_id)<br>\nFile \u201c\/content\/yolov5\/utils\/loggers\/wandb\/wandb_utils.py\u201d, line 185, in <strong>init<\/strong><br>\nallow_val_change=True)<br>\nFile \u201c\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/wandb_config.py\u201d, line 181, in update<br>\nsanitized = self._update(d, allow_val_change)<br>\nFile \u201c\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/wandb_config.py\u201d, line 175, in _update<br>\nparsed_dict, allow_val_change, ignore_keys=locked_keys<br>\nFile \u201c\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/wandb_config.py\u201d, line 227, in _sanitize_dict<br>\nself._raise_value_error_on_nested_artifact(config_dict)<br>\nFile \u201c\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/sdk\/wandb_config.py\u201d, line 266, in _raise_value_error_on_nested_artifact<br>\n\u201cInstances of wandb.Artifact and wandb.apis.public.Artifact\u201d<br>\nValueError: Instances of wandb.Artifact and wandb.apis.public.Artifact can only be top level keys in wandb.config<\/p>\n<p>Here the debug log:<\/p>\n<p><a href=\"https:\/\/github.com\/wandb\/client\/files\/8171769\/debug.log\" rel=\"noopener nofollow ugc\">debug.log<\/a><br>\n<a href=\"https:\/\/github.com\/wandb\/client\/files\/8171770\/debug-internal.log\" rel=\"noopener nofollow ugc\">debug-internal.log<\/a><\/p>\n<p>With a dowgrade to version 0.12.10 just work.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1653665702174,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":150.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/value-error-instances-of-wandb-artifact-and-wandb-apis-public-artifact-can-only-be-top-level-keys-i\/2500",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"valu error instanc artifact api public artifact level kei hello sorri bad english person project train yolov kaggl environ get type error traceback recent file train line main opt file train line main train opt hyp opt devic callback file train line train logger logger save dir weight opt hyp logger logger instanc file content yolov util logger init line init self logger self opt run file content yolov util logger util line init allow val chang true file usr local lib python dist packag sdk config line updat sanit self updat allow val chang file usr local lib python dist packag sdk config line updat pars dict allow val chang ignor kei lock kei file usr local lib python dist packag sdk config line sanit dict self rais valu error nest artifact config dict file usr local lib python dist packag sdk config line rais valu error nest artifact instanc artifact api public artifact valueerror instanc artifact api public artifact level kei config debug log debug log debug intern log dowgrad version work",
        "Question_preprocessed_content":"valu instanc artifact level kei bad english person project train yolov environ type traceback file line main file line main opt devic file line train instanc file init line init self file line init file line updat sanit file line file line file line instanc artifact instanc artifact level kei config debug log dowgrad version work",
        "Question_gpt_summary_original":"The user encountered a ValueError while using wandb to train Yolov5 in a Kaggle environment. The error occurred when updating the wandb_config and was caused by instances of wandb.Artifact and wandb.apis.public.Artifact being nested within the config. The issue was resolved by downgrading to version 0.12.10. Debug logs are provided for further analysis.",
        "Question_gpt_summary":"user encount valueerror train yolov kaggl environ error occur updat config caus instanc artifact api public artifact nest config issu resolv downgrad version debug log provid analysi"
    },
    {
        "Question_title":"Deploying the sagemaker endpoint created as a service",
        "Question_body":"<p>I have trained a credit-fraud data set on AWS Sagemaker and created an endpoint of the model. Suppose I want to provide it as a service to my friend. He has some credit data and wanted to know whether the transaction is fraud or not. He wishes to use my endpoint. How do I share it?<\/p>\n\n<ol>\n<li>Should I share my ARN for endpoint? I don't think its the right way. without a common account he won't be able to use it.<\/li>\n<li>Or is there another way<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1573479066647,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":226.0,
        "Poster_created_time":1568318861627,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation_count":486.0,
        "Poster_view_count":75.0,
        "Answer_body":"<p>To share your model as an endpoint, you should use lambda and API Gateway to create your API.<\/p>\n\n<ol>\n<li>Create an API gateway that triggers a Lambda with the HTTP POST method;<\/li>\n<li>your lambda should instantiate the SageMaker endpoint, get the requested parameter in the event, call the SageMaker endpoint and return the predicted value. you can also create a DynamoDB to store commonly requested parameters with their answers;<\/li>\n<li>Send the API Gateway Endpoint to your friend.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/qLss4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qLss4.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1573653626700,
        "Answer_last_edit_time":1573654268900,
        "Answer_score_count":6.0,
        "Answerer_created_time":1464391892936,
        "Answerer_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Answerer_reputation_count":2243.0,
        "Answerer_view_count":148.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58802366",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi endpoint creat servic train credit fraud data set creat endpoint model suppos want provid servic friend credit data want know transact fraud wish us endpoint share share arn endpoint think right wai common account won abl us wai",
        "Question_preprocessed_content":"deploi endpoint creat servic train data set creat endpoint model want provid servic friend credit data want know transact fraud wish us endpoint share share arn endpoint think right wai won abl us wai",
        "Question_gpt_summary_original":"The user has trained a credit-fraud dataset on AWS Sagemaker and created an endpoint of the model. They want to share it with a friend who has some credit data and wants to know whether the transaction is fraud or not. The user is unsure of the correct way to share the endpoint and is considering sharing their ARN, but believes it may not be the right way. They are seeking advice on alternative methods.",
        "Question_gpt_summary":"user train credit fraud dataset creat endpoint model want share friend credit data want know transact fraud user unsur correct wai share endpoint consid share arn believ right wai seek advic altern method"
    },
    {
        "Question_title":"Disable Azure Datastore download message",
        "Question_body":"<p>Is there any way to disable the warning presented whenever a file is loaded for the first time in Azure ML using datastores?<\/p>\n<pre><code>Downloaded path: \/tmp\/tmp5dto5vq4\/path\/to\/file.txt is different from target path: \/tmp\/tmp5dto5vq4\/path\/to\/file.txt\n<\/code><\/pre>\n<p>I connect to the Datastore following the azure tutorial:<\/p>\n<pre><code>ws = Workspace.from_config()\nmounted_path = tempfile.mkdtemp()\ndataset = Dataset.get_by_name(workspace = ws, name = 'name')\nmount_context = dataset.mount(mounted_path)\n\nmount_context.start()\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1644884590780,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":106.0,
        "Poster_created_time":1490704811283,
        "Poster_location":"K\u00f8benhavn, Danmark",
        "Poster_reputation_count":94.0,
        "Poster_view_count":22.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71119807",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"disabl azur datastor download messag wai disabl warn present file load time datastor download path tmp tmpdtovq path file txt differ target path tmp tmpdtovq path file txt connect datastor follow azur tutori workspac config mount path tempfil mkdtemp dataset dataset workspac mount context dataset mount mount path mount context start",
        "Question_preprocessed_content":"disabl azur datastor download wai disabl warn present file load time datastor datastor azur tutori",
        "Question_gpt_summary_original":"The user is encountering a warning message when loading a file for the first time in Azure ML using datastores. They are looking for a way to disable this warning message. The user is following the Azure tutorial to connect to the Datastore.",
        "Question_gpt_summary":"user encount warn messag load file time datastor look wai disabl warn messag user follow azur tutori connect datastor"
    },
    {
        "Question_title":"How do I take an already existing MLflow model on my local filesystem and log it to a remote tracking server?",
        "Question_body":"<p>Let's say I already have an existing MLflow model on my local system of the <code>mlflow.pyfunc<\/code> flavor.<\/p>\n<p>The directory looks like this<\/p>\n<pre><code>model\/\n  data\/\n  code\/\n  conda.yml\n  MLmodel\n<\/code><\/pre>\n<p>Where <code>MLmodel<\/code> is something like<\/p>\n<pre><code>flavors:\n  python_function:\n    code: code\n    data: data\n    env: conda.yml\n    loader_module: loader # model\/code\/loader.py has the entrypoint\n<\/code><\/pre>\n<p>I now try and log this model to a remote tracking server using (I'm in the directory above <code>model\/<\/code>, so <code>.\/model\/data<\/code> works, etc)<\/p>\n<pre><code>import mlflow\nmlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.pyfunc.log_model(\n  &quot;my-model-artifact&quot;,\n  registered_model_name=&quot;my-model&quot;, # same for all model versions,\n  data_path=&quot;model\/data&quot;,\n  code_path=&quot;model\/code&quot;,\n  loader_module=&quot;model\/code\/loader&quot;\n)\n<\/code><\/pre>\n<p>The tracking server ends up logging a nested MLflow model.. this is inside of the <code>.\/artifacts\/my-model-artifact<\/code> directory on the tracking server<\/p>\n<pre><code>.\/artifacts\/my-model-artifact\n  conda.yaml\n  MLmodel # *not* my MLmodel, one newly generated by MLflow\n  data\/\n  code\/\n<\/code><\/pre>\n<p>Where <code>data<\/code> now points nested to my entire <code>model\/data<\/code> directory and <code>code<\/code> points to a nested <code>model\/code<\/code> directory.<\/p>\n<p>It's like it doesn't understand that I already have this full artifact..<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1646174584637,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":336.0,
        "Poster_created_time":1425965839876,
        "Poster_location":"Santa Cruz, CA",
        "Poster_reputation_count":3256.0,
        "Poster_view_count":164.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71315446",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"exist model local filesystem log remot track server let exist model local pyfunc flavor directori look like model data code conda yml mlmodel mlmodel like flavor python function code code data data env conda yml loader modul loader model code loader entrypoint try log model remot track server directori model model data work import set track uri http localhost pyfunc log model model artifact regist model model model version data path model data code path model code loader modul model code loader track server end log nest model insid artifact model artifact directori track server artifact model artifact conda yaml mlmodel mlmodel newli gener data code data point nest entir model data directori code point nest model code directori like understand artifact",
        "Question_preprocessed_content":"exist model local filesystem log remot track server let exist model local flavor directori like like try log model remot track server track server end nest insid directori track server point nest entir directori point nest directori like understand",
        "Question_gpt_summary_original":"The user is trying to log an existing MLflow model of the pyfunc flavor to a remote tracking server using the log_model function. However, the tracking server ends up logging a nested MLflow model instead of the existing one, which is causing confusion for the user.",
        "Question_gpt_summary":"user try log exist model pyfunc flavor remot track server log model function track server end log nest model instead exist caus confus user"
    },
    {
        "Question_title":"Vertex AI model version using Python SDK",
        "Question_body":"<p>Vertex AI offers a very interesting Model Registry that allows you to store all trained models and track all their versions.<\/p>\n<p>However, I don't manage to create new versions of the same model using the Python SDK. In particular, I have a Vertex AI Pipeline that performs: 1) data preprocessing, 2) feature engineering, 3) feature store creation, and in the end, 4) train a model with AutoML Tabular.<\/p>\n<p>The code of the Pipeline component dedicated to the point 4 is:<\/p>\n<pre><code> automl_training_electric_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n    project=project_bq,\n    model_display_name=&quot;pred-model&quot;,\n    display_name=&quot;pred-model&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-prc&quot;,\n    budget_milli_node_hours=1000,\n    dataset=comp5a.outputs[&quot;dataset&quot;],\n    target_column=&quot;fault&quot;,\n    location=location\n)\n<\/code><\/pre>\n<p>In the Google documentation I didn't find anything that could help me in creating new versions of the &quot;pred-model&quot;, in fact, any time I run the pipeline, Vertex AI creates a new model with the same name.<\/p>\n<p>I would like that at each training, AutoML creates a new version of the same model. E.g., v1, v2, v3.<\/p>\n<p>Here, the current situation, in which the same model is replicated and not versioned:\n<a href=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zpy2n.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1662054058413,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":58.0,
        "Poster_created_time":1415266912943,
        "Poster_location":"Milano, Metropolitan City of Milan, Italy",
        "Poster_reputation_count":107.0,
        "Poster_view_count":24.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73573487",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"model version python sdk offer interest model registri allow store train model track version manag creat new version model python sdk particular pipelin perform data preprocess featur engin featur store creation end train model automl tabular code pipelin compon dedic point automl train electr gcc aip automltabulartrainingjobrunop project project model displai pred model displai pred model optim predict type classif optim object maxim prc budget milli node hour dataset compa output dataset target column fault locat locat googl document help creat new version pred model fact time run pipelin creat new model like train automl creat new version model current situat model replic version",
        "Question_preprocessed_content":"model version python sdk interest model registri store train model track version manag creat new version model python sdk particular pipelin perform data featur featur store creation end train model automl tabular code pipelin compon dedic point document help creat new version fact time run pipelin creat new model like train automl creat new version model situat model replic version",
        "Question_gpt_summary_original":"The user is facing challenges in creating new versions of a model using the Python SDK in Vertex AI. The user has a pipeline that performs data preprocessing, feature engineering, feature store creation, and trains a model with AutoML Tabular. The user wants AutoML to create a new version of the same model at each training, but Vertex AI creates a new model with the same name instead of versioning it. The user is seeking help to create new versions of the model.",
        "Question_gpt_summary":"user face challeng creat new version model python sdk user pipelin perform data preprocess featur engin featur store creation train model automl tabular user want automl creat new version model train creat new model instead version user seek help creat new version model"
    },
    {
        "Question_title":"Getting error Missing required package \"azureml-dataset-runtime\" in VSCode",
        "Question_body":"<p>I am trying to setup my virtual environment for Azure in VS Code. I have installed the required packages, e.g., <code>azureml-core<\/code> and <code>azureml-widgets<\/code> and <code>azureml-dataset-runtime<\/code>. Both <code>azureml-core<\/code> and <code>azureml-widget<\/code> work fine, however, I keep getting an error missing required package for <code>azureml-dataset-runtime<\/code> although I installed it.<\/p>\n<p>I have Python 3.7 and Python 3.8 installed, both 64 bit and I tried both of them in my virtual environment, still no luck.<\/p>\n<p>Any suggestions?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1620402829140,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":7.0,
        "Question_view_count":433.0,
        "Poster_created_time":1599932188727,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67438154",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"get error miss requir packag dataset runtim vscode try setup virtual environ azur code instal requir packag core widget dataset runtim core widget work fine get error miss requir packag dataset runtim instal python python instal bit tri virtual environ luck suggest",
        "Question_preprocessed_content":"requir packag vscode try setup virtual environ azur code requir packag work fine requir packag python python bit tri virtual environ luck",
        "Question_gpt_summary_original":"The user is encountering an error while setting up their virtual environment for Azure in VS Code. They have installed the required packages, including azureml-core and azureml-widgets, but are receiving an error message stating that the azureml-dataset-runtime package is missing, despite having installed it. The user has tried using both Python 3.7 and 3.8 in their virtual environment, but the issue persists.",
        "Question_gpt_summary":"user encount error set virtual environ azur code instal requir packag includ core widget receiv error messag state dataset runtim packag miss despit have instal user tri python virtual environ issu persist"
    },
    {
        "Question_title":"Kedro cannot find run",
        "Question_body":"<p>As a part of upgrading Kedro from 0.16.2 to 0.17.3 in our organization, I've made changes to Kedro related files in our codebase based on Kedro starter pyspark-iris on 0.17.3.<\/p>\n<p>Now I get an error of Error: No such command 'run' on kedro run.<\/p>\n<p>setup.py<\/p>\n<pre><code>from setuptools import find_packages, setup\n\nentry_point = &quot;kedro-project = kedro-package.__main__:main&quot;\n\n# get the dependencies and installs\nwith open(&quot;requirements.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:\n    # Make sure we strip all comments and options (e.g &quot;--extra-index-url&quot;)\n    # that arise from a modified pip.conf file that configure global options\n    # when running kedro build-reqs\n    requires = []\n    for line in f:\n        req = line.split(&quot;#&quot;, 1)[0].strip()\n        if req and not req.startswith(&quot;--&quot;):\n            requires.append(req)\n\nsetup(\n    name=&quot;kedro-package&quot;,\n    version=&quot;0.1&quot;,\n    packages=find_packages(exclude=[&quot;tests&quot;]),\n    entry_points={&quot;console_scripts&quot;: [entry_point]},\n    install_requires=requires,\n    extras_require={\n        &quot;docs&quot;: [\n            &quot;sphinx~=3.4.3&quot;,\n            &quot;sphinx_rtd_theme==0.5.1&quot;,\n            &quot;nbsphinx==0.8.1&quot;,\n            &quot;nbstripout==0.3.3&quot;,\n            &quot;recommonmark==0.7.1&quot;,\n            &quot;sphinx-autodoc-typehints==1.11.1&quot;,\n            &quot;sphinx_copybutton==0.3.1&quot;,\n            &quot;jupyter_client&gt;=5.1.0, &lt;6.0&quot;,\n            &quot;tornado&gt;=4.2, &lt;6.0&quot;,\n            &quot;ipykernel~=5.3&quot;,\n        ]\n    },\n)\n\n<\/code><\/pre>\n<p><strong>main<\/strong>.py<\/p>\n<pre><code>from pathlib import Path\n\nfrom kedro.framework.project import configure_project\nimport logging\nfrom .cli import run\n\n\ndef main():\n    package_name = str(Path(__file__).resolve().parent.name)\n    logging.getLogger(__name__).info(f&quot;package name is: {package_name}&quot;)\n    configure_project(package_name=package_name)\n    run()\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n<\/code><\/pre>\n<p>and cli.py is at the same level as <strong>main<\/strong>.py which are directly inside the package (altered to kedro-package here for anonymity)<\/p>\n<p>This only happens when performing kedro run on the EMR. When we run locally we don't see that error. Rather it errors out because it can't connect to S3, which is expected.\nAdditionally, I've tried running<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1633712322793,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":413.0,
        "Poster_created_time":1477986647030,
        "Poster_location":"Atlanta, GA, USA",
        "Poster_reputation_count":171.0,
        "Poster_view_count":17.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69499388",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"run upgrad organ chang relat file codebas base starter pyspark iri error error command run run setup setuptool import packag setup entri point project packag main main depend instal open requir txt encod utf sure strip comment option extra index url aris modifi pip conf file configur global option run build req requir line req line split strip req req startswith requir append req setup packag version packag packag exclud test entri point consol script entri point instal requir requir extra requir doc sphinx sphinx rtd theme nbsphinx nbstripout recommonmark sphinx autodoc typehint sphinx copybutton jupyt client ipykernel main pathlib import path framework project import configur project import log cli import run def main packag str path file resolv parent log getlogg info packag packag configur project packag packag run main main cli level main directli insid packag alter packag anonym happen perform run emr run local error error connect expect addition tri run",
        "Question_preprocessed_content":"run upgrad organ chang relat file codebas base starter run run level directli insid packag perform run emr run expect tri",
        "Question_gpt_summary_original":"The user encountered an error of \"No such command 'run' on kedro run\" after upgrading Kedro from 0.16.2 to 0.17.3. The error only occurs when performing kedro run on EMR, but not locally. The user has made changes to Kedro related files in their codebase based on Kedro starter pyspark-iris on 0.17.3.",
        "Question_gpt_summary":"user encount error command run run upgrad error occur perform run emr local user chang relat file codebas base starter pyspark iri"
    },
    {
        "Question_title":"Wandb.Html() not displaying",
        "Question_body":"<p>I\u2019m trying to display a block of text (a confusion matrix actually, since they do not display in WB) using HTML.<br>\nThe code runs but it will not show up on run panel.  How can i see it?  It seems only images will display.<\/p>\n<pre><code class=\"lang-auto\">wandb.log({f\"ConfMatrix\" : wandb.Html(\"&lt;tt&gt;\"+my_confusion_matrix.ai2_confusion_matrix(y_true, y_pred...).replace(\"\\n\", \"&lt;P&gt;\").replace(\" \", \"&amp;nbsp;\"))})\n<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_created_time":1666068919749,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":309.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-html-not-displaying\/3271",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"html displai try displai block text confus matrix actual displai html code run run panel imag displai log confmatrix html confus matrix confus matrix true pred replac replac nbsp",
        "Question_preprocessed_content":"html displai try displai block text html code run run panel imag displai",
        "Question_gpt_summary_original":"The user is trying to display a confusion matrix using HTML in Wandb, but it is not showing up on the run panel. The user is seeking a solution to display the text block as only images seem to be displaying.",
        "Question_gpt_summary":"user try displai confus matrix html show run panel user seek solut displai text block imag displai"
    },
    {
        "Question_title":"Azure ML - Managed Identity for Compute Instance",
        "Question_body":"<p>We need to connect Azure Data Lake Storage Gen2 to Azure Machine Learning by means of a datastore. For security reasons we do not want to provide the credential-based authentication credentials (service principal or SAS token). Instead we want to connect with identity based access.  <\/p>\n<p>The problem we face is that we are not able to assign a managed identity to a compute instance, so we can connect from notebooks to the Data Lake. In the documentation is explained how to assign a managed identity to a cluster, but we need the same for the compute instance, as it is the only way to run commands directly from the notebook.  <\/p>\n<p>Is there a way to assign managed identity to an Azure Machine Learning Compute Instance? Otherwise, we would like to know the best approach to overcome this issue, considering that we do not want to introduce the credentials in the code.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1637143820660,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p><a href=\"\/users\/na\/?userid=cbcc7cd1-be1d-4352-9648-87343bfaafff\">@Nimbeo  <\/a> Thanks for the question. Currently It\u2019s not supported yet  to assign managed identity to an Azure Machine Learning Compute Instance, you\u2019d need to use credential-based access. We have forwarded to the product team to support in the near future.<\/p>\n",
        "Answer_comment_count":1.0,
        "Question_closed_time":1637157273153,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/630520\/azure-ml-managed-identity-for-compute-instance",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"manag ident comput instanc need connect azur data lake storag gen mean datastor secur reason want provid credenti base authent credenti servic princip sa token instead want connect ident base access problem face abl assign manag ident comput instanc connect notebook data lake document explain assign manag ident cluster need comput instanc wai run command directli notebook wai assign manag ident comput instanc like know best approach overcom issu consid want introduc credenti code",
        "Question_preprocessed_content":"manag ident comput instanc azur data lake storag gen mean datastor secur reason want provid authent credenti instead want ident base problem face abl manag ident comput instanc data lake document explain manag ident cluster comput instanc wai run directli wai manag ident comput instanc like know best overcom consid want introduc credenti code",
        "Question_gpt_summary_original":"The user is facing a challenge in connecting Azure Data Lake Storage Gen2 to Azure Machine Learning using a datastore due to security reasons. They are unable to assign a managed identity to a compute instance, which is necessary to connect from notebooks to the Data Lake. The user is seeking a solution to assign a managed identity to an Azure Machine Learning Compute Instance or an alternative approach to overcome this issue without introducing credentials in the code.",
        "Question_gpt_summary":"user face challeng connect azur data lake storag gen datastor secur reason unabl assign manag ident comput instanc necessari connect notebook data lake user seek solut assign manag ident comput instanc altern approach overcom issu introduc credenti code"
    },
    {
        "Question_title":"can I use mlflow python API to register a spark UDF & then use the UDF in Spark scala code?",
        "Question_body":"<p>I'm trying to use mlflow to do the machine learning work. I register the ML model as UDF using the following python code. The question is how can I use the UDF(test_predict) in my scala code? The reason is that our main code is in Scala. The problem is that UDF created below is a temporary UDF and SparkSession scoped. thanks!<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import sys\nimport mlflow\nfrom mlflow import pyfunc\nimport numpy as np\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark import SQLContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.types import *\n\nsc=SparkContext()\nspark = SparkSession.builder.appName(\"Python UDF example\").getOrCreate()\npyfunc_udf=mlflow.pyfunc.spark_udf(spark=spark, model_uri=\".\/sk\",result_type=\"float\")\nspark.udf.register(\"test_predict\",pyfunc_udf)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_created_time":1587824105457,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1588065231427,
        "Question_score_count":2.0,
        "Question_view_count":617.0,
        "Poster_created_time":1553506414607,
        "Poster_location":null,
        "Poster_reputation_count":129.0,
        "Poster_view_count":47.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61427012",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us python api regist spark udf us udf spark scala code try us machin learn work regist model udf follow python code question us udf test predict scala code reason main code scala problem udf creat temporari udf sparksess scope thank import sy import import pyfunc import numpi pyspark import sparkcontext sparkconf pyspark import sqlcontext pyspark sql session import sparksess pyspark sql type import sparkcontext spark sparksess builder appnam python udf exampl getorcr pyfunc udf pyfunc spark udf spark spark model uri result type float spark udf regist test predict pyfunc udf",
        "Question_preprocessed_content":"us python api regist spark udf us udf spark scala code try us machin learn work regist model udf python code question us scala code reason main code scala problem udf creat temporari udf scope thank",
        "Question_gpt_summary_original":"The user is trying to register a machine learning model as a UDF using mlflow Python API and wants to use the UDF in their Scala code. However, the UDF created is temporary and SparkSession scoped, which is causing a problem as their main code is in Scala.",
        "Question_gpt_summary":"user try regist machin learn model udf python api want us udf scala code udf creat temporari sparksess scope caus problem main code scala"
    },
    {
        "Question_title":"Problem with reading jpg image in Sagemaker, imageclassification algorithm",
        "Question_body":"Dears,\n\nI created a sagemaker job but I see the following error:\n\n[01\/06\/2023 11:29:21 ERROR 140033245599552] Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/dbimgraces\/output\/af\/af05.jpg\n\nAs far as I can see from my label file, the location is fine and the image also looks fine. Is any problem with the imread itself?\n\n![Enter image description here](\/media\/postImages\/original\/IML4VWT-mNRAmb7fExLcsiyw)",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1673348916663,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1673696451600,
        "Question_score_count":0.0,
        "Question_view_count":66.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUj0I1epyIR1GkzTj4rpgA0A\/problem-with-reading-jpg-image-in-sagemaker-imageclassification-algorithm",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"problem read jpg imag imageclassif algorithm dear creat job follow error error custom error imread read blank imag file opt input data train dbimgrac output jpg far label file locat fine imag look fine problem imread enter imag descript media postimag origin imlvwt mnrambfexlcsiyw",
        "Question_preprocessed_content":"problem read jpg imag algorithm dear creat job custom imread read blank imag file far label file locat fine imag fine problem imread",
        "Question_gpt_summary_original":"The user is encountering an error while trying to read a jpg image in Sagemaker's image classification algorithm. The error message suggests that the image file is blank, even though the location and image appear to be fine. The user suspects that there may be a problem with the imread function.",
        "Question_gpt_summary":"user encount error try read jpg imag imag classif algorithm error messag suggest imag file blank locat imag appear fine user suspect problem imread function"
    },
    {
        "Question_title":"ModuleNotFoundError while using AzureML pipeline with yml file based RunConfiguration and environment.yml",
        "Question_body":"<p>I am running into a ModuleNotFoundError for pandas while using the following code to orchestrate my Azure Machine Learning Pipeline:<\/p>\n<pre><code># Loading run config\nprint(&quot;Loading run config&quot;)\ntask_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \n\ntask_1_script_run_config = ScriptRunConfig(\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    run_config=task_1_run_config    \n)\n\ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name=task_1_script_run_config.script,\n    source_directory=task_1_script_run_config.source_directory,\n    compute_target=compute_target\n)\n\npipeline_run_config = Pipeline(workspace=workspace, steps=[task_1_py_script_step])#, task_2])\n\npipeline_run = Experiment(workspace, 'Test_Run_New_Pipeline').submit(pipeline_run_config)\npipeline_run.wait_for_completion()\n<\/code><\/pre>\n<p>The environment.yml<\/p>\n<pre><code>name: phinmo_pipeline_env\ndependencies:\n- python=3.8\n- pip:\n  - pandas\n  - azureml-core==1.43.0\n  - azureml-sdk\n  - scipy\n  - scikit-learn\n  - numpy\n  - pyyaml==6.0\n  - datetime\n  - azure\nchannels:\n  - conda-forge\n<\/code><\/pre>\n<p>The loaded RunConfiguration in T01_Test_Task.yml looks like this:<\/p>\n<pre><code># The script to run.\nscript: T01_Test_Task.py\n# The arguments to the script file.\narguments: [\n  &quot;--test&quot;, False,\n  &quot;--date&quot;, &quot;2022-07-26&quot;\n]\n# The name of the compute target to use for this run.\ncompute_target: phinmo-compute-cluster\n# Framework to execute inside. Allowed values are &quot;Python&quot;, &quot;PySpark&quot;, &quot;CNTK&quot;, &quot;TensorFlow&quot;, and &quot;PyTorch&quot;.\nframework: Python\n# Maximum allowed duration for the run.\nmaxRunDurationSeconds: 6000\n# Number of nodes to use for running job.\nnodeCount: 1\n\n#Environment details.\nenvironment:\n  # Environment name\n  name: phinmo_pipeline_env\n  # Environment version\n  version:\n  # Environment variables set for the run.\n  #environmentVariables:\n  #  EXAMPLE_ENV_VAR: EXAMPLE_VALUE\n  # Python details\n  python:\n    # user_managed_dependencies=True indicates that the environmentwill be user managed. False indicates that AzureML willmanage the user environment.\n    userManagedDependencies: false\n    # The python interpreter path\n    interpreterPath: python\n    # Path to the conda dependencies file to use for this run. If a project\n    # contains multiple programs with different sets of dependencies, it may be\n    # convenient to manage those environments with separate files.\n    condaDependenciesFile: environment.yml\n    # The base conda environment used for incremental environment creation.\n    baseCondaEnvironment: AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\n  # Docker details\n  \n# History details.\nhistory:\n  # Enable history tracking -- this allows status, logs, metrics, and outputs\n  # to be collected for a run.\n  outputCollection: true\n  # Whether to take snapshots for history.\n  snapshotProject: true\n  # Directories to sync with FileWatcher.\n  directoriesToWatch:\n  - logs\n# data reference configuration details\ndataReferences: {}\n# The configuration details for data.\ndata: {}\n# Project share datastore reference.\nsourceDirectoryDataStore:\n<\/code><\/pre>\n<p>I already tried a few things like overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object or assigning a version number to pandas in the environment.yml, changing the location of the environment.yml. But I am at a loss at what else to try. the T01_Test_Task.py runs without issues on its own. But putting it into a pipeline just does not seem to work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1658922798813,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658924360076,
        "Question_score_count":0.0,
        "Question_view_count":51.0,
        "Poster_created_time":1544524371740,
        "Poster_location":"Cologne Germany",
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Answer_body":"<p>Okay I found the issue.\nI am unnecessarily using the ScriptRunConfig which overwrites the assigned environment with some default azureml environment. I was able to see that only in the Task description in the Azure Machine Learning Studio UI.<\/p>\n<p>I was able to just remove that part and now it works:<\/p>\n<pre><code>task_1_run_config = RunConfiguration.load(\n    os.path.join(WORKING_DIR + '\/pipeline\/task_runconfigs\/T01_Test_Task.yml')\n    ) \ntask_1_py_script_step = PythonScriptStep(\n    name='Task_1_Step',\n    script_name='T01_Test_Task.py',\n    source_directory=os.path.join(WORKING_DIR + '\/pipeline\/task_scripts'),\n    runconfig=task_1_run_config, \n    compute_target=compute_target\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1658937635368,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1544524371740,
        "Answerer_location":"Cologne Germany",
        "Answerer_reputation_count":21.0,
        "Answerer_view_count":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73137433",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"modulenotfounderror pipelin yml file base runconfigur environ yml run modulenotfounderror panda follow code orchestr pipelin load run config print load run config task run config runconfigur load path join work dir pipelin task runconfig test task yml task script run config scriptrunconfig sourc directori path join work dir pipelin task script run config task run config task script step pythonscriptstep task step script task script run config script sourc directori task script run config sourc directori comput target comput target pipelin run config pipelin workspac workspac step task script step task pipelin run experi workspac test run new pipelin submit pipelin run config pipelin run wait complet environ yml phinmo pipelin env depend python pip panda core sdk scipi scikit learn numpi pyyaml datetim azur channel conda forg load runconfigur test task yml look like script run script test task argument script file argument test fals date comput target us run comput target phinmo comput cluster framework execut insid allow valu python pyspark cntk tensorflow pytorch framework python maximum allow durat run maxrundurationsecond number node us run job nodecount environ detail environ environ phinmo pipelin env environ version version environ variabl set run environmentvari exampl env var exampl valu python detail python user manag depend true indic environmentwil user manag fals indic willmanag user environ usermanageddepend fals python interpret path interpreterpath python path conda depend file us run project contain multipl program differ set depend conveni manag environ separ file condadependenciesfil environ yml base conda environ increment environ creation basecondaenviron sklearn ubuntu cpu docker detail histori detail histori enabl histori track allow statu log metric output collect run outputcollect true snapshot histori snapshotproject true directori sync filewatch directoriestowatch log data refer configur detail datarefer configur detail data data project share datastor refer sourcedirectorydatastor tri thing like overwrit environ attribut runconfigur object environ python conda depend object assign version number panda environ yml chang locat environ yml loss try test task run issu put pipelin work",
        "Question_preprocessed_content":"pipelin yml file base runconfigur panda code orchestr pipelin load runconfigur like tri thing like overwrit environ runconfigur object object version number panda chang locat try run pipelin work",
        "Question_gpt_summary_original":"The user is encountering a ModuleNotFoundError for pandas while using Azure Machine Learning Pipeline with a yml file based RunConfiguration and environment.yml. The T01_Test_Task.py runs without issues on its own, but putting it into a pipeline is not working. The user has tried overwriting the environment attribute in the RunConfiguration object with a environment.python.conda_dependencies object and assigning a version number to pandas in the environment.yml, but the issue persists.",
        "Question_gpt_summary":"user encount modulenotfounderror panda pipelin yml file base runconfigur environ yml test task run issu put pipelin work user tri overwrit environ attribut runconfigur object environ python conda depend object assign version number panda environ yml issu persist"
    },
    {
        "Question_title":"Provisioning failed with error from CloudFormation when create aws sagemaker mlops project",
        "Question_body":"<p>I am testing out MLOps using SageMaker studio and creating a project using a template for MLOps provided by SageMaker: MLOps template for model building, training, deployment and monitoring<\/p>\n<p>I am getting this error when creating the project.(see picture 1)<\/p>\n<p>My relation and IAM role is picture 2 and 3.<\/p>\n<p>Could someone please help me what am I missing here?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2QZIf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2QZIf.png\" alt=\"picture1\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/D8wMG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/D8wMG.png\" alt=\"picture2\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/hVWfH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hVWfH.png\" alt=\"picture3\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1655371875283,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":54.0,
        "Poster_created_time":1499745972728,
        "Poster_location":null,
        "Poster_reputation_count":105.0,
        "Poster_view_count":31.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72643587",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"provis fail error cloudform creat mlop project test mlop studio creat project templat mlop provid mlop templat model build train deploy monitor get error creat project pictur relat iam role pictur help miss",
        "Question_preprocessed_content":"provis fail cloudform creat mlop project test mlop studio creat project templat mlop provid mlop templat model build train deploy monitor creat pictur relat iam role pictur help",
        "Question_gpt_summary_original":"The user is encountering an error while creating an MLOps project using a template provided by SageMaker in AWS. The error is related to provisioning and is accompanied by pictures of the user's relation and IAM role. The user is seeking help to identify the issue.",
        "Question_gpt_summary":"user encount error creat mlop project templat provid aw error relat provis accompani pictur user relat iam role user seek help identifi issu"
    },
    {
        "Question_title":"Not able to read variable in custom RStudio open-source application , Azure ML",
        "Question_body":"<p>In Azure ML we have created compute instance by using setup shell script and we also installed custom RStudio open-source application using below MS docs  <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=azure-studio\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=azure-studio<\/a><\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/247908-image.png?platform=QnA\" alt=\"compute instance\" \/><\/p>\n<p>In setup shell script we define environment variable<\/p>\n<h1 id=\"create-renviron-file\">Create .Renviron file<\/h1>\n<p>r_env_file=&quot;\/home\/${aml_user}\/.Renviron&quot;<\/p>\n<p>echo &quot;Creating R environment variables&quot;  <br \/>\nsudo tee &quot;${r_env_file}&quot; &gt; \/dev\/null &lt;&lt;EOF  <br \/>\nAZURE_TENANT_ID=${AZURE_TENANT_ID}  <br \/>\nAZURE_CLIENT_ID=${AZURE_CLIENT_ID}  <br \/>\nAZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}  <br \/>\nDEFAULT_ADLS_ACCOUNT=${DEFAULT_ADLS_ACCOUNT}  <br \/>\nDEFAULT_ADLS_CONTAINER=${DEFAULT_ADLS_CONTAINER}  <br \/>\nEOF<\/p>\n<p>sudo chown &quot;${aml_user}&quot; &quot;${r_env_file}&quot;  <br \/>\nsudo chmod 600 &quot;${r_env_file}&quot;<\/p>\n<p>In terminal we are able to read above variable  <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/247949-image.png?platform=QnA\" alt=\"treminal\" \/><\/p>\n<p>However, we are not able to read above variable in custom RStudio open-source application.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1665035652217,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1037115\/not-able-to-read-variable-in-custom-rstudio-open-s",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"abl read variabl custom rstudio open sourc applic creat comput instanc setup shell script instal custom rstudio open sourc applic doc http learn microsoft com azur machin learn creat manag comput instanc tab azur studio setup shell script defin environ variabl creat renviron file env file home aml user renviron echo creat environ variabl sudo tee env file dev null eof azur tenant azur tenant azur client azur client azur client secret azur client secret default adl account default adl account default adl contain default adl contain eof sudo chown aml user env file sudo chmod env file termin abl read variabl abl read variabl custom rstudio open sourc applic",
        "Question_preprocessed_content":"abl read variabl custom rstudio creat comput instanc setup script custom rstudio doc setup script defin environ variabl creat renviron file echo creat environ variabl sudo eof eof sudo chown sudo chmod termin abl read variabl abl read variabl custom rstudio",
        "Question_gpt_summary_original":"The user is unable to read environment variables defined in a setup shell script in a custom RStudio open-source application installed on an Azure ML compute instance, even though they are able to read the variables in the terminal.",
        "Question_gpt_summary":"user unabl read environ variabl defin setup shell script custom rstudio open sourc applic instal comput instanc abl read variabl termin"
    },
    {
        "Question_title":"How to Consume a trained model saved as web service",
        "Question_body":"<p>Hello all,   <\/p>\n<p>I trained a model which uses cross-validation. So I can not save it locally as trained model as far as I know. For this case, I tried to to deploy it as web service.  <\/p>\n<p>For example, I trained a model called X. Then, I created a new experiment called Y to consume this trained model X. I tried to drag and drop load trained model to use model X via web service. it does not work. Could you please guide me how I can consume my trained model that saved as web-services. Or Can you please let me know how I can save my model that using cross-validation locally?   <\/p>\n<p>-Best<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1612313789263,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/256246\/how-to-consume-a-trained-model-saved-as-web-servic",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"consum train model save web servic hello train model us cross valid save local train model far know case tri deploi web servic exampl train model call creat new experi call consum train model tri drag drop load train model us model web servic work guid consum train model save web servic let know save model cross valid local best",
        "Question_preprocessed_content":"consum train model save web servic train model us valid save train model far know case tri deploi web servic exampl train model creat new experi consum train model tri drag drop load train model us model web servic work guid consum train model save let know save model valid best",
        "Question_gpt_summary_original":"The user has trained a model using cross-validation and is unable to save it locally. They have attempted to deploy it as a web service but are facing issues in consuming the trained model. They are seeking guidance on how to consume the trained model saved as a web service or how to save the model locally.",
        "Question_gpt_summary":"user train model cross valid unabl save local attempt deploi web servic face issu consum train model seek guidanc consum train model save web servic save model local"
    },
    {
        "Question_title":"How do I give Vertex AI pipeline component permissions?",
        "Question_body":"<p>In a Vertex AI pipeline component,I try:<\/p>\n<pre><code>def my_comp(project_id: str, location: str, endpoint_id: str, endpoint: Output[Artifact]):\n    import google.cloud.aiplatform as aip\n    endpoints = aip.Endpoint.list()\n...\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>'aiplatform.endpoints.list' denied on resource '\/\/aiplatform.googleapis.com\/projects\/...\n<\/code><\/pre>\n<p>My service account has owner permissions, and it works outside of the component. What do I need to do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6.0,
        "Question_created_time":1662721061203,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":62.0,
        "Poster_created_time":1351154914716,
        "Poster_location":null,
        "Poster_reputation_count":2564.0,
        "Poster_view_count":451.0,
        "Answer_body":"<p>This permission denied on resource issue can be resolved by using import statement:<\/p>\n<pre><code>from google.cloud import aiplatform_v1 as aiplatform\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1663223342092,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1619177157943,
        "Answerer_location":null,
        "Answerer_reputation_count":835.0,
        "Answerer_view_count":516.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73661090",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pipelin compon permiss pipelin compon try def comp project str locat str endpoint str endpoint output artifact import googl cloud aiplatform aip endpoint aip endpoint list give aiplatform endpoint list deni resourc aiplatform googleapi com project servic account owner permiss work outsid compon need",
        "Question_preprocessed_content":"pipelin compon pipelin compon try give servic owner work outsid compon",
        "Question_gpt_summary_original":"The user is encountering a permission issue while trying to use a Vertex AI pipeline component. The error message indicates that the 'aiplatform.endpoints.list' permission is denied on a specific resource. The user's service account has owner permissions, but the issue persists. The user is seeking guidance on how to resolve this issue.",
        "Question_gpt_summary":"user encount permiss issu try us pipelin compon error messag indic aiplatform endpoint list permiss deni specif resourc user servic account owner permiss issu persist user seek guidanc resolv issu"
    },
    {
        "Question_title":"How to defined Serializer\/Deserializer while sagemaker create_endpoint using boto3 sdk",
        "Question_body":"<p>How can I defined serializer\/deserializer while using boto3 sdk client to create an sagemaker endpoint the same as passing them while using sagemaker sdk deploy() function like following?<\/p>\n<pre><code>from sagemaker.serializers import JSONSerializer\nfrom sagemaker.deserializers import JSONDeserializer\n\npredictor = estimator.deploy(\n    instance_type='ml.m4.xlarge',\n    initial_instance_count=1,\n    serializer=JSONSerializer(),\n    deserializer=JSONDeserializer(),\n)\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>    ......\n    endpoint_configs = sm_client.list_endpoint_configs(NameContains=endpoint_config_name)\n    if len(endpoint_configs['EndpointConfigs']) &gt; 0:\n        sm_client.delete_endpoint_config(EndpointConfigName = endpoint_config_name)\n    \n    create_endpoint_config_response = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n           {\n               &quot;InstanceType&quot;: &quot;ml.m4.xlarge&quot;,\n               &quot;InitialVariantWeight&quot;: 1,\n               &quot;InitialInstanceCount&quot;: 1,\n               &quot;ModelName&quot;: model_name,\n               &quot;VariantName&quot;: &quot;AllTraffic&quot;,\n           }\n        ]\n   )\n\n   endpoints = sm_client.list_endpoints(NameContains=endpoint_name)\n\n   if len(endpoints['Endpoints']) &gt; 0:\n       create_endpoint_response = sm_client.update_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n    else:\n        create_endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1657611857000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":34.0,
        "Poster_created_time":1352858984688,
        "Poster_location":null,
        "Poster_reputation_count":213.0,
        "Poster_view_count":25.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72948711",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"defin serial deseri creat endpoint boto sdk defin serial deseri boto sdk client creat endpoint pass sdk deploi function like follow serial import jsonseri deseri import jsondeseri predictor estim deploi instanc type xlarg initi instanc count serial jsonseri deseri jsondeseri code endpoint config client list endpoint config namecontain endpoint config len endpoint config endpointconfig client delet endpoint config endpointconfignam endpoint config creat endpoint config respons client creat endpoint config endpointconfignam endpoint config productionvari instancetyp xlarg initialvariantweight initialinstancecount modelnam model variantnam alltraff endpoint client list endpoint namecontain endpoint len endpoint endpoint creat endpoint respons client updat endpoint endpointnam endpoint endpointconfignam endpoint config creat endpoint respons client creat endpoint endpointnam endpoint endpointconfignam endpoint config",
        "Question_preprocessed_content":"defin boto sdk defin boto sdk client creat endpoint sdk deploi function like code",
        "Question_gpt_summary_original":"The user is seeking guidance on how to define serializer\/deserializer while using boto3 sdk client to create an sagemaker endpoint, similar to passing them while using sagemaker sdk deploy() function. The user has provided their code for creating an endpoint using boto3 sdk.",
        "Question_gpt_summary":"user seek guidanc defin serial deseri boto sdk client creat endpoint similar pass sdk deploi function user provid code creat endpoint boto sdk"
    },
    {
        "Question_title":"SageMaker and TensorFlow 2.0",
        "Question_body":"<p>What is the best way to run TensorFlow 2.0 with AWS Sagemeker?<\/p>\n\n<p>As of today (Aug 7th, 2019) AWS does not provide TensorFlow 2.0 <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"noreferrer\">SageMaker containers<\/a>, so my understanding is that I need to build my own.<\/p>\n\n<p>What is the best Base image to use? Example Dockerfile?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1.0,
        "Question_created_time":1565186451297,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1565186790803,
        "Question_score_count":13.0,
        "Question_view_count":4136.0,
        "Poster_created_time":1446859510543,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":3259.0,
        "Poster_view_count":233.0,
        "Answer_body":"<p>EDIT: <strong>Amazon SageMaker does now support TF 2.0 and higher.<\/strong><\/p>\n<ul>\n<li>SageMaker + TensorFlow docs: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/using_tf.html<\/a><\/li>\n<li>Supported Tensorflow versions (and Docker URIs): <a href=\"https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/releasenotes\/available-deep-learning-containers-images<\/a><\/li>\n<\/ul>\n<hr \/>\n<p><em>Original answer<\/em><\/p>\n<p>Here is an example Dockerfile that uses <a href=\"https:\/\/github.com\/aws\/sagemaker-containers\" rel=\"nofollow noreferrer\">the underlying SageMaker Containers library<\/a> (this is what is used in the official pre-built Docker images):<\/p>\n<pre><code>FROM tensorflow\/tensorflow:2.0.0b1\n\nRUN pip install sagemaker-containers\n\n# Copies the training code inside the container\nCOPY train.py \/opt\/ml\/code\/train.py\n\n# Defines train.py as script entrypoint\nENV SAGEMAKER_PROGRAM train.py\n<\/code><\/pre>\n<p>For more information on this approach, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-container-to-train-script-get-started.html<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Question_closed_time":1567635751827,
        "Answer_last_edit_time":1595008443227,
        "Answer_score_count":10.0,
        "Answerer_created_time":1512770138847,
        "Answerer_location":null,
        "Answerer_reputation_count":493.0,
        "Answerer_view_count":47.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57396212",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tensorflow best wai run tensorflow aw sagemek todai aug aw provid tensorflow contain understand need build best base imag us exampl dockerfil",
        "Question_preprocessed_content":"tensorflow best wai run tensorflow aw sagemek todai aw provid tensorflow contain understand build best base imag us exampl dockerfil",
        "Question_gpt_summary_original":"The user is seeking advice on the best way to run TensorFlow 2.0 with AWS SageMaker, as AWS does not currently provide SageMaker containers for TensorFlow 2.0. The user is looking for guidance on the best base image to use and an example Dockerfile.",
        "Question_gpt_summary":"user seek advic best wai run tensorflow aw current provid contain tensorflow user look guidanc best base imag us exampl dockerfil"
    },
    {
        "Question_title":"Clustering sharing between Spark kernel notebooks",
        "Question_body":"<p>I'm looking to understanding conceptually how several Jupyter notebooks running on Spark kernels (such as SparkMagic) can share a cluster of worker nodes.<\/p>\n<p>If User A persists or caches a large RDD (whether on disk or on memory) in a cell, and then goes away for the weekend but does not stop his\/her notebook, will this degrade other users' ability to run their jobs while User A's notebook is running?<\/p>\n<p>That is, all the Spark notebooks sharing the cluster will be able to submit jobs at the same time (do not have to run sequentially), but the resources will be divided up, right?<\/p>\n<p>This is a general question, but for us we're running on an AWS Sagemaker and EMR environment in an US region, in case it makes a difference.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1611569405597,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":143.0,
        "Poster_created_time":1564812605232,
        "Poster_location":null,
        "Poster_reputation_count":465.0,
        "Poster_view_count":48.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65882662",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"cluster share spark kernel notebook look understand conceptu jupyt notebook run spark kernel sparkmag share cluster worker node user persist cach larg rdd disk memori cell goe awai weekend stop notebook degrad user abil run job user notebook run spark notebook share cluster abl submit job time run sequenti resourc divid right gener question run emr environ region case make differ",
        "Question_preprocessed_content":"cluster share spark kernel understand jupyt spark kernel share cluster worker node user persist cach larg goe awai stop degrad user abil run job user spark share cluster abl submit job time resourc divid right gener question emr environ region case make",
        "Question_gpt_summary_original":"The user is seeking to understand how multiple Jupyter notebooks running on Spark kernels can share a cluster of worker nodes. They are concerned about whether a large RDD persisted or cached by one user will degrade the ability of other users to run their jobs while the first user's notebook is running. The user is running on an AWS Sagemaker and EMR environment in the US region.",
        "Question_gpt_summary":"user seek understand multipl jupyt notebook run spark kernel share cluster worker node concern larg rdd persist cach user degrad abil user run job user notebook run user run emr environ region"
    },
    {
        "Question_title":"Error when running ML Algorithms",
        "Question_body":"<p>Hello    <\/p>\n<p>I am trying to run some ML algorithms but I get the folliwing mistake and I do  not know how to proceed. Please let me know what I can do to fix this.    <\/p>\n<p> <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/143396-image.png?platform=QnA\" alt=\"143396-image.png\" \/>    <\/p>\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1635152541867,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/602539\/error-when-running-ml-algorithms",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error run algorithm hello try run algorithm folliw mistak know proce let know fix thank",
        "Question_preprocessed_content":"algorithm try run algorithm mistak know let know fix thank",
        "Question_gpt_summary_original":"The user is encountering an error while trying to run ML algorithms and is seeking assistance to fix the issue. A screenshot of the error message is provided.",
        "Question_gpt_summary":"user encount error try run algorithm seek assist fix issu screenshot error messag provid"
    },
    {
        "Question_title":"How to disable MLFlow temporarily?",
        "Question_body":"<p>Is it possible to disable MLFlow temporarily for debugging code or adding new features? If it is not disabled it saves a bunch of executions not actually usefull or unfinished executions.<\/p>\n\n<p>Or is the best strategy to use similar code that doesn't call <code>mlflow.start_run()<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1586290455120,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":653.0,
        "Poster_created_time":1472251053727,
        "Poster_location":null,
        "Poster_reputation_count":548.0,
        "Poster_view_count":81.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61088651",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"disabl temporarili possibl disabl temporarili debug code ad new featur disabl save bunch execut actual useful unfinish execut best strategi us similar code start run",
        "Question_preprocessed_content":"disabl temporarili disabl temporarili code new featur disabl save bunch execut unfinish execut best strategi us similar code",
        "Question_gpt_summary_original":"The user is seeking information on how to temporarily disable MLFlow for debugging or adding new features, as it saves unfinished or unnecessary executions. They are also asking if using similar code without calling mlflow.start_run() is a better strategy.",
        "Question_gpt_summary":"user seek inform temporarili disabl debug ad new featur save unfinish unnecessari execut ask similar code call start run better strategi"
    },
    {
        "Question_title":"Using SageMaker as a backend for a web app",
        "Question_body":"[Continued\/simplified from this post.](https:\/\/repost.aws\/questions\/QU-iZHXP55T_6BLKhHPET_0Q\/if-i-open-a-web-socket-connection-with-one-of-my-sage-makers-notebook-cells-how-long-will-the-connection-last)\n\nLet's say I have a web app UI and I want to connect it to a SageMaker notebook using the Churn Predictor model in SageMaker Sudio. Think of it like using that model hosted in SageMaker as a backend to my web app. \n\nI could create an API endpoint but my web app has a very simple solution for opening a web socket which I'd like to use. Question: **Whats the simplest\/best way to deploy the SageMaker notebook so that it keeps the kernel running and connection open?**\n\nThe websocket means I don't need to set up and configure individual API endpoints which is what all the documentation I can find suggests. \n\nThe desired workflow is:\n- Configure model in SageMaker notebook environment \n- Add a cell that establishes a websocket connection\n- Export notebook as a Python script (Maybe?)\n- Host & Run Python script on an AWS service\n\nWith the web socket I can simply establish the connection:\n\n`anvil.server.connect(app_key)`\n\nAnd then give each function that I want my web app to call a decorator. i.e:\n```\n@anvil.server.callable\ndef function(foo):\n  return foo\n```",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1645453626658,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668621505552,
        "Question_score_count":0.0,
        "Question_view_count":536.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUA06r5EiFR0ifBd3KixHHLA\/using-sagemaker-as-a-backend-for-a-web-app",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"backend web app continu simplifi post http repost aw question izhxpt blkhhpet open web socket connect sage maker notebook cell long connect let web app want connect notebook churn predictor model sudio think like model host backend web app creat api endpoint web app simpl solut open web socket like us question what simplest best wai deploi notebook keep kernel run connect open websocket mean need set configur individu api endpoint document suggest desir workflow configur model notebook environ add cell establish websocket connect export notebook python script mayb host run python script aw servic web socket simpli establish connect anvil server connect app kei function want web app decor anvil server callabl def function foo return foo",
        "Question_preprocessed_content":"backend web let web want churn predictor model sudio think like model host backend web creat api endpoint web simpl solut open web socket like us question what wai deploi kernel open websocket mean set configur individu api endpoint document desir workflow configur model environ establish websocket export python script host run python script aw servic web socket simpli establish function want web decor",
        "Question_gpt_summary_original":"The user wants to connect a web app UI to a SageMaker notebook using the Churn Predictor model in SageMaker Studio as a backend. They are looking for the simplest and best way to deploy the SageMaker notebook so that it keeps the kernel running and connection open, without having to set up and configure individual API endpoints. The user wants to establish a websocket connection and give each function that they want their web app to call a decorator.",
        "Question_gpt_summary":"user want connect web app notebook churn predictor model studio backend look simplest best wai deploi notebook keep kernel run connect open have set configur individu api endpoint user want establish websocket connect function want web app decor"
    },
    {
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1622232629793,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1622257491983,
        "Question_score_count":5.0,
        "Question_view_count":488.0,
        "Poster_created_time":1618255062696,
        "Poster_location":null,
        "Poster_reputation_count":75.0,
        "Poster_view_count":2.0,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1622257759208,
        "Answer_last_edit_time":1622503453296,
        "Answer_score_count":3.0,
        "Answerer_created_time":1305851487736,
        "Answerer_location":null,
        "Answerer_reputation_count":5993.0,
        "Answerer_view_count":457.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Tool":"DVC",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl check version file track metadata file exist remot storag pull file team set track dataset model gitlab repositori track code metadata file job dev gitlab pipelin run push merg request goal check sure develop rememb run push remot storag date right wai run pull gitlab runner fail error tell file new file latest version exist file downsid approach load entireti data store gitlab runner run memori issu mention lengthi run time download data path hash object store metadata file think inform need abl answer question remot storag date like statu similar ask compar cach workspac remot storag word requir file actual present filesystem make wai achiev goal laid inform develop need run push pull",
        "Question_preprocessed_content":"check version file track metadata file exist remot storag file team set track dataset model gitlab repositori track code metadata file job dev gitlab pipelin goal check sure develop rememb run remot storag right wai gitlab fail file downsid load entireti data store gitlab run mention lengthi run time download data path hash object store metadata file think inform abl answer question remot storag like similar ask compar cach workspac remot storag word requir file present filesystem make wai achiev goal laid",
        "Question_gpt_summary_original":"The user's team tracks datasets and models in DVC and has a GitLab repository for tracking code and DVC metadata files. They have a job in their dev GitLab pipeline that checks if the developer has run \"dvc push\" to keep DVC remote storage up-to-date. Currently, they run \"dvc pull\" on the GitLab runner, which loads the entirety of their data stored in DVC onto the runner, causing out-of-memory issues and lengthy run time. The user is looking for a way to check if the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file.",
        "Question_gpt_summary":"user team track dataset model gitlab repositori track code metadata file job dev gitlab pipelin check develop run push remot storag date current run pull gitlab runner load entireti data store runner caus memori issu lengthi run time user look wai check version file track metadata file exist remot storag pull file"
    },
    {
        "Question_title":"How do you create an azure jupyter in vscode?",
        "Question_body":"<p>Hey all,    <\/p>\n<p>I hope you are all doing well. I am wondering if you can use azure jupyter notebook in vscode if you have the azure extension and the jupyter notebook extension?     <\/p>\n<p>Thanks,    <br \/>\nKen<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1662435889603,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/995055\/how-do-you-create-an-azure-jupyter-in-vscode",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat azur jupyt vscode hei hope wonder us azur jupyt notebook vscode azur extens jupyt notebook extens thank ken",
        "Question_preprocessed_content":"creat azur jupyt vscode hei hope wonder us azur jupyt vscode azur extens jupyt extens thank ken",
        "Question_gpt_summary_original":"The user is seeking information on how to create an Azure Jupyter notebook in VSCode using the Azure and Jupyter Notebook extensions.",
        "Question_gpt_summary":"user seek inform creat azur jupyt notebook vscode azur jupyt notebook extens"
    },
    {
        "Question_title":"Speech to text API not returning long running operation results - is service impaired?",
        "Question_body":"The Speech to text API stopped returning results today. It was working properly yesterday evening (16 hours ago.\n\nI have tried it from python and node, both of which worked fine yesterday.\n\nI am using the same code and short audio (2 minutes) that I have been using.\n\nI am using the python speech_v1p1beta1 lib and have tried speech_v1 also.\n\nconfig = {\n    'encoding': 'FLAC',\n    'language_code': 'en-US',\n    'audio_channel_count': channel_count,\n    'sample_rate_hertz': sample_rate,\n    'enable_word_time_offsets': True,\n    'enable_separate_recognition_per_channel': False,\n    'enable_automatic_punctuation': True,\n    'use_enhanced': True,\n    'model': 'video'\n}\naudio = {\n    'uri': audio_uri\n}\nrequest = {\n    'config': config,\n    'audio': audio\n}\noperation = cls.client.long_running_recognize(request)\nresponse = operation.result(timeout=10000)\ntranscript = cls.build_transcript(response)\n\nI get the operation object but the results just timeout no matter what timeout I set\n\nThanks",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1677758700000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":96.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Speech-to-text-API-not-returning-long-running-operation-results\/td-p\/528170\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"speech text api return long run oper result servic impair speech text api stop return result todai work properli yesterdai even hour ago tri python node work fine yesterdai code short audio minut python speech vpbeta lib tri speech config encod flac languag code audio channel count channel count sampl rate hertz sampl rate enabl word time offset true enabl separ recognit channel fals enabl automat punctuat true us enhanc true model video audio uri audio uri request config config audio audio oper cl client long run recogn request respons oper result timeout transcript cl build transcript respons oper object result timeout matter timeout set thank",
        "Question_preprocessed_content":"text api return long oper result servic impair text api return result todai work properli yesterdai even python lib tri config audio request oper respons transcript oper object result timeout timeout set thank",
        "Question_gpt_summary_original":"The user is facing an issue with the Speech to text API as it has stopped returning results, even though it was working fine 16 hours ago. The user has tried using both python and node, but the results still timeout despite setting a timeout limit. The user is using the same code and a short audio file that was previously working.",
        "Question_gpt_summary":"user face issu speech text api stop return result work fine hour ago user tri python node result timeout despit set timeout limit user code short audio file previous work"
    },
    {
        "Question_title":"Why would I need to register a machine learning model with azure endpoint?",
        "Question_body":"<p>Hi,  <br \/>\nIt appears to me that the types of models that can be registered using the azure SDK should come from sklearn, Keras, etc and be converted into a pickle file and after registration, you can send the request to the endpoint.  <\/p>\n<p>I can think of a very limited number of cases where you just want to create a backend system that only returns the model prediction. In most cases, the backend of a front end application would take care of all the calculations and sometimes also decide which models to run and maybe combine the results before sending the output to the frontend. For this reason, you might want to implement the complex logic in rest API such as Flask. In this case, why would I call another endpoint from the flask application to run the model, instead of simply loading the pickle file in the flask project?  <\/p>\n<p>What are the cases where you are registering with Azure endpointst and actually using the endpoints to make a prediction in azure from you trained model? Are you limited to specific libraries to create the pickles? what if you need to process the input before making the predictions?   <\/p>\n<p>thanks  <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_created_time":1630340796540,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/533161\/why-would-i-need-to-register-a-machine-learning-mo",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"need regist machin learn model azur endpoint appear type model regist azur sdk come sklearn kera convert pickl file registr send request endpoint think limit number case want creat backend return model predict case backend end applic care calcul decid model run mayb combin result send output frontend reason want implement complex logic rest api flask case endpoint flask applic run model instead simpli load pickl file flask project case regist azur endpointst actual endpoint predict azur train model limit specif librari creat pickl need process input make predict thank",
        "Question_preprocessed_content":"regist machin learn model azur endpoint type model regist azur sdk come sklearn kera convert pickl file registr send request endpoint think limit number case want creat backend return model predict case backend end care calcul decid model run mayb combin result send output frontend reason want implement complex logic rest api flask case endpoint flask run model instead simpli load pickl file flask project case regist azur endpointst endpoint predict azur train model limit specif librari creat pickl input make predict thank",
        "Question_gpt_summary_original":"The user is questioning the need to register a machine learning model with Azure endpoint, especially when the model can be loaded as a pickle file in a Flask project. They are curious about the cases where registering with Azure endpoints is necessary and if there are limitations to the libraries used to create pickles. They also wonder how to process input before making predictions.",
        "Question_gpt_summary":"user question need regist machin learn model azur endpoint especi model load pickl file flask project curiou case regist azur endpoint necessari limit librari creat pickl wonder process input make predict"
    },
    {
        "Question_title":"Launch Jupyter Notebooks in AWS Sagemaker from a Custom Webapplication",
        "Question_body":"<p>We have a requirement where we are building a Webportal\/platform that will use services of AWS and Git as both will host certain content to allow users to search for certain artifacts.<\/p>\n\n<p>We also want to allow a user after they have searched for certain artifacts (lets say certain jupyter notebooks) to be able to launch these notebooks from our web-application. Note the notebooks are in different domain i.e AWS Console application host them.<\/p>\n\n<p>Now, When user click on the notebook links from the webportal search it should open up the  Jupyter notebook in a notebook instance in a new tab.<\/p>\n\n<p>We understand there is integration of AWS Sagemaker and GIT so some repos that will store notebooks can be configured. When user performs the search in webapp it will pick up the results from github API Call.<\/p>\n\n<p>The same repos can also be added in the sagemaker-github integration  through AWS Console. So when a user launches the notebook he will see the github repos as well.<\/p>\n\n<p>I understand we call Sagemaker API either through SDK or Rest API(not sure there is a rest api interface exploring on that). See a CLI call example -<\/p>\n\n<p>aws sagemaker create-presigned-notebook-instance-url --notebook-instance-name notebook-sagemaker-git<\/p>\n\n<p>this gives me a response url <code>\"AuthorizedUrl\": \"https:\/\/notebook-sagemaker-git.notebook.us-east-2.sagemaker.aws?authToken=eyJhbGciOiJIUzI1NiJ9.eyJmYXNDcmVkZW50aWFscyI6IkFZQURlQlR1NHBnZ2dlZGc3VTJNcjZKSmN3UUFYd0FCQUJWaGQzTXRZM0o1Y0hSdkxYQjFZbXhwWXkxclpYa0FSRUZvUVZadGMxSjFSVzV6V1hGVGJFWmphRXhWUTNwcVlucDZaR2x5ZDNGQ1RsZFplV1YyUkRoTGJubHRWRzVQT1dWM1RTdDBTR0p6TjJoYVdXeDJabnBrUVQwOUFBRUFCMkYzY3kxcmJYTUFTMkZ5YmpwaGQzTTZhMjF6T25WekxXVmhjM1F0TWpvMk5qZzJOek15TXpJMk5UUTZhMlY1THpObFlUaGxNMk14TFRSaU56a3RORGd4T0<\/code><\/p>\n\n<p>However, when i open this url it again asks me the aws console username and password. I feel in the webapp  when i logged in a user would already authenticate himself through AWS API as well as GIT API.<\/p>\n\n<p>So there should be no need to re-authenticate themselves when they connect to AWS-Console to access their notebooks. <\/p>\n\n<p>Is it something that can be circumvent using SIngle sign on etc.<\/p>\n\n<p>thanks,\nAakash<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1576342153710,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":954.0,
        "Poster_created_time":1346655643036,
        "Poster_location":null,
        "Poster_reputation_count":183.0,
        "Poster_view_count":47.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59337180",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"launch jupyt notebook custom webappl requir build webport platform us servic aw git host certain content allow user search certain artifact want allow user search certain artifact let certain jupyt notebook abl launch notebook web applic note notebook differ domain aw consol applic host user click notebook link webport search open jupyt notebook notebook instanc new tab understand integr git repo store notebook configur user perform search webapp pick result github api repo ad github integr aw consol user launch notebook github repo understand api sdk rest api sure rest api interfac explor cli exampl creat presign notebook instanc url notebook instanc notebook git give respons url authorizedurl http notebook git notebook east aw authtoken eyjhbgcioijiuzinij eyjmyxndcmvkzwawfscyiikfzqurlqlrnhbnzdlzgcvtjncjzksmnuufydfcqujwagqztxrzmoyhsdkxyqjfzbxhwwxkxclpyafsruzvuvzadgmxsjfsvzvvhgvgjfwmphrxhwutnwcvlucdzarxzdngqrszfplvyyukrotgjubhrwrzvqtdwmrtddbtrptjjoyvdxedjabnbruvqwoufbrufcmkyzykxcmjytuftmkzympwagqzttzhmjftwekxxvmhjmftwpvmkqzzjoektxpjmkuutzhmlythpobfluagxnmktfrsauarorgdt open url ask aw consol usernam password feel webapp log user authent aw api git api need authent connect aw consol access notebook circumv singl sign thank aakash",
        "Question_preprocessed_content":"launch jupyt custom requir build us servic aw git host certain content user search certain artifact want user search certain artifact abl launch note domain aw consol host user click link webport search open jupyt instanc new tab understand integr git repo store configur user perform search pick result github api repo github integr aw consol user launch github repo understand api sdk rest api cli exampl give respons url open url ask aw consol usernam user authent aw api git api circumv singl sign thank",
        "Question_gpt_summary_original":"The user is building a web portal that will use AWS and Git services to allow users to search for certain artifacts, including Jupyter notebooks hosted in AWS Console. They want users to be able to launch these notebooks from the web application without having to re-authenticate themselves when they connect to AWS Console. The user is exploring the possibility of using Single Sign-On to circumvent this issue.",
        "Question_gpt_summary":"user build web portal us aw git servic allow user search certain artifact includ jupyt notebook host aw consol want user abl launch notebook web applic have authent connect aw consol user explor possibl singl sign circumv issu"
    },
    {
        "Question_title":"SageMaker Estimator use_spot_instances causes Invalid MaxWaitTimeInSeconds",
        "Question_body":"<p><a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs<\/a> says:<\/p>\n<blockquote>\n<p>Setting it up is extremely simple, as it should be when working with a fully-managed service:<\/p>\n<ul>\n<li>If you\u2019re using the console, just switch the feature on.<\/li>\n<li>If you\u2019re working with the Amazon SageMaker SDK, <strong>just set the train_use_spot_instances to true<\/strong> in the Estimator constructor.<\/li>\n<\/ul>\n<\/blockquote>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.Estimator\" rel=\"nofollow noreferrer\">SageMaker SDK sagemaker.estimator.Estimator<\/a> says:<\/p>\n<blockquote>\n<ul>\n<li>use_spot_instances (bool) \u2013\nSpecifies whether to use SageMaker Managed Spot instances for training. If enabled then the max_wait arg should also be set.<\/li>\n<li>max_wait (int) \u2013\nTimeout in seconds waiting for spot training instances (default: None). After this amount of time Amazon SageMaker will stop waiting for Spot instances to become available (default: None).<\/li>\n<\/ul>\n<\/blockquote>\n<p>As per the documentations, run below.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\n\nestimator = TensorFlow(\n    entry_point=&quot;fashion_mnist_training.py&quot;,\n    source_dir=&quot;src&quot;,\n    metric_definitions=metric_definitions,\n    hyperparameters=hyperparameters,\n    role=role,\n    input_mode='File',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n    use_spot_instances=True,\n    max_wait= 23 * 60 * 60, \n    base_job_name=base_job_name,\n    checkpoint_s3_uri=checkpoint_s3_uri,\n    model_dir=False  # To avoid duplicate 'model_dir' command line argument\n)\n<\/code><\/pre>\n<p>However, error is caused.<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1630660877280,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":269.0,
        "Poster_created_time":1416648155470,
        "Poster_location":null,
        "Poster_reputation_count":14749.0,
        "Poster_view_count":968.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69042524",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"estim us spot instanc caus invalid maxwaittimeinsecond manag spot train save train job sai set extrem simpl work fulli manag servic your consol switch featur your work sdk set train us spot instanc true estim constructor sdk estim estim sai us spot instanc bool specifi us manag spot instanc train enabl max wait arg set max wait int timeout second wait spot train instanc default time stop wait spot instanc avail default document run tensorflow import tensorflow estim tensorflow entri point fashion mnist train sourc dir src metric definit metric definit hyperparamet hyperparamet role role input mode file framework version version instanc count instanc type xlarg us spot instanc true max wait base job base job checkpoint uri checkpoint uri model dir fals avoid duplic model dir command line argument error caus clienterror error occur validationexcept call createtrainingjob oper invalid maxwaittimeinsecond present greater equal maxruntimeinsecond",
        "Question_preprocessed_content":"estim caus invalid manag spot train save train job sai extrem simpl work manag servic your consol switch featur your work sdk set true estim constructor sdk sai specifi us manag spot instanc train enabl arg set timeout second wait spot train instanc time stop wait spot instanc avail document run caus",
        "Question_gpt_summary_original":"The user is encountering an error while using the SageMaker SDK to set up Managed Spot Training. Specifically, when setting the train_use_spot_instances to true in the Estimator constructor, the user is receiving a \"ValidationException\" error stating that the MaxWaitTimeInSeconds is invalid and must be present and greater than or equal to MaxRuntimeInSeconds.",
        "Question_gpt_summary":"user encount error sdk set manag spot train specif set train us spot instanc true estim constructor user receiv validationexcept error state maxwaittimeinsecond invalid present greater equal maxruntimeinsecond"
    },
    {
        "Question_title":"How to mock async operation in flink with completable future",
        "Question_body":"<p>I'm trying to test my <code>RichAsyncFunction<\/code> impl, but I have encountered the following issues. The main flow is to invoke an API call on the client, but I am getting a serialization error when I mock the response to be a completable future and test the step function with.<\/p>\n<pre><code>DataStream&lt;MyObject&gt; resultStream = AsyncDataStream.unorderedWait(stream,\n        new MyAsyncStep(), timeout, TimeUnit.MILLISECONDS, capacity);\n<\/code><\/pre>\n<p>I know the completable future itself is non-serializable, but <code>MyAsyncStep<\/code> works fine when I run it in the IDE or flink cluster and failed when I run the test case. In the <code>asyncInvoke<\/code> methods I have<\/p>\n<pre><code>List&lt;CompletableFuture&gt; completableFutureList = getCompletableFutureList(client, input);\n<\/code><\/pre>\n<p>Anyone knows how to mock the <code>getCompletableFutureList<\/code>? currently it gave me <code>getCompletableFutureList not serializable. The object probably contains or references non serializable fields.<\/code><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1635794717227,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":130.0,
        "Poster_created_time":1446933458500,
        "Poster_location":null,
        "Poster_reputation_count":190.0,
        "Poster_view_count":45.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69801846",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"mock async oper flink complet futur try test richasyncfunct impl encount follow issu main flow invok api client get serial error mock respons complet futur test step function datastream resultstream asyncdatastream unorderedwait stream new myasyncstep timeout timeunit millisecond capac know complet futur non serializ myasyncstep work fine run id flink cluster fail run test case asyncinvok method list completablefuturelist getcompletablefuturelist client input know mock getcompletablefuturelist current gave getcompletablefuturelist serializ object probabl contain refer non serializ field",
        "Question_preprocessed_content":"mock async oper flink complet futur try test impl encount main flow invok api client serial mock respons complet futur test step function know complet futur work fine run id flink cluster fail run test case method know mock gave",
        "Question_gpt_summary_original":"The user is trying to test their implementation of a RichAsyncFunction in Flink, but is encountering serialization errors when mocking a response to be a CompletableFuture. The issue seems to be with the non-serializable nature of the CompletableFuture, and the user is seeking advice on how to mock the getCompletableFutureList method.",
        "Question_gpt_summary":"user try test implement richasyncfunct flink encount serial error mock respons completablefutur issu non serializ natur completablefutur user seek advic mock getcompletablefuturelist method"
    },
    {
        "Question_title":"Can I build the environment in the computing cluster using pip?",
        "Question_body":"<p>I want to train AI model, and in the VM instance executing the command below worked well<\/p>\n<pre><code>pip install -r requirement.txt\npython ~\n<\/code><\/pre>\n<p>Then in order to train the Ai model in the same environment in the VM computing cluster, in the Python 3.8 - AzureML notebook I executed below (I'm sorry I couldn't attach the screenshot)<\/p>\n<pre><code>import azureml.core\nfrom azureml.core import Workspace\nimport os\nfrom azureml.core import ScriptRunConfig\nfrom azureml.core import Datastore\nfrom azureml.core import Experiment\nfrom azureml.core import Dataset\nfrom azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nfrom azureml.core import Environment\nimport datetime\n\ncluster_name = 'high-2x-v100-1'\ngpu_name = 'Standard_NC12s_v3'\nexperiment_name = 'training_agent_print'\nhyperparameters = [\n    '--max_train_time', '172800'\n]\nscript_folder = '.\/script_folder'\n\n# workspace\nws = Workspace.from_config()\nprint(ws.name, ws.location, ws.resource_group, sep='\\t')\n\n# compute cluster\ncompute_name = os.environ.get(&quot;AML_COMPUTE_CLUSTER_NAME&quot;, cluster_name)\ncompute_min_nodes = os.environ.get(&quot;AML_COMPUTE_CLUSTER_MIN_NODES&quot;, 0)\ncompute_max_nodes = os.environ.get(&quot;AML_COMPUTE_CLUSTER_MAX_NODES&quot;, 4)\nvm_size = os.environ.get(&quot;AML_COMPUTE_CLUSTER_SKU&quot;, gpu_name)\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n# environment\nenv = Environment.from_pip_requirements(name = &quot;m8-pip-training&quot;, file_path = &quot;.\/requirements.txt&quot;)\nexp = Experiment(workspace=ws,name=experiment_name)\n\n# run\nsrc = ScriptRunConfig(source_directory=script_folder,\n    script='main.py',\n    arguments=hyperparameters,\n    compute_target=compute_target,\n    environment=env\n)\nrun = exp.submit(config=src)\n<\/code><\/pre>\n<p>as a result, in the 20_image_build_log.txt file, I got the log as below<\/p>\n<pre><code>==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.9.2\n  latest version: 4.10.3\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\n\nPip subprocess error:\nERROR: Could not find a version that satisfies the requirement parlai==1.3.0 (from -r \/azureml-environment-setup\/condaenv.5svatkzc.requirements.txt (line 55)) (from versions: 0.1.20200409, 0.1.20200416, 0.1.20200610, 0.1.20200713, 0.1.20200716, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4)\nERROR: No matching distribution found for parlai==1.3.0 (from -r \/azureml-environment-setup\/condaenv.5svatkzc.requirements.txt (line 55))\n\n\nCondaEnvException: Pip failed\n\n [0mThe command '\/bin\/sh -c ldconfig \/usr\/local\/cuda\/lib64\/stubs &amp;&amp; conda env create -p \/azureml-envs\/azureml_ba289e67ead35c3dbaac125150111737 -f azureml-environment-setup\/mutated_conda_dependencies.yml &amp;&amp; rm -rf &quot;$HOME\/.cache\/pip&quot; &amp;&amp; conda clean -aqy &amp;&amp; CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; rm -rf &quot;$CONDA_ROOT_DIR\/pkgs&quot; &amp;&amp; find &quot;$CONDA_ROOT_DIR&quot; -type d -name __pycache__ -exec rm -rf {} + &amp;&amp; ldconfig' returned a non-zero code: 1\n2021\/08\/10 15:13:41 Container failed during run: acb_step_0. No retries remaining.\nfailed to run step ID: acb_step_0: exit status 1\n\nRun ID: caj failed after 2m24s. Error: failed during run, err: exit status 1\n<\/code><\/pre>\n<p>Ans the experiment failed. I have 3 questions  <\/p>\n<ol>\n<li> Why computing cluster is using conda to build image even though I export the file from pip?  <\/li>\n<li> Can I build the environment using pip?  <\/li>\n<li> As there is WARNING, if I can update the conda to latest version, the experiment might not faile. Can I update the conda in the computing cluster?<\/li>\n<\/ol>\n<p>Thank you so much<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1628609652357,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/508278\/can-i-build-the-environment-in-the-computing-clust",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"build environ comput cluster pip want train model instanc execut command work pip instal requir txt python order train model environ comput cluster python notebook execut sorri couldn attach screenshot import core core import workspac import core import scriptrunconfig core import datastor core import experi core import dataset core comput import amlcomput core comput import computetarget core import environ import datetim cluster high gpu standard nc experi train agent print hyperparamet max train time script folder script folder workspac workspac config print locat resourc group sep comput cluster comput environ aml comput cluster cluster comput min node environ aml comput cluster min node comput max node environ aml comput cluster max node size environ aml comput cluster sku gpu comput comput target comput target comput target comput comput target type comput target amlcomput print comput target us comput print creat new comput target provis config amlcomput provis configur size size min node comput min node max node comput max node comput target computetarget creat comput provis config environ env environ pip requir pip train file path requir txt exp experi workspac experi run src scriptrunconfig sourc directori script folder script main argument hyperparamet comput target comput target environ env run exp submit config src result imag build log txt file got log warn newer version conda exist current version latest version updat conda run conda updat base default conda pip subprocess error error version satisfi requir parlai environ setup condaenv svatkzc requir txt line version error match distribut parlai environ setup condaenv svatkzc requir txt line condaenvexcept pip fail mthe command bin ldconfig usr local cuda lib stub conda env creat env baeeadcdbaac environ setup mutat conda depend yml home cach pip conda clean aqi conda root dir conda info root conda root dir pkg conda root dir type pycach exec ldconfig return non zero code contain fail run acb step retri remain fail run step acb step exit statu run caj fail error fail run err exit statu an experi fail question comput cluster conda build imag export file pip build environ pip warn updat conda latest version experi fail updat conda comput cluster thank",
        "Question_preprocessed_content":"build environ comput cluster pip want train model instanc execut work order train model environ comput cluster python execut result file got log an experi fail question comput cluster conda build imag export file pip build environ pip warn updat conda latest version experi fail updat conda comput cluster thank",
        "Question_gpt_summary_original":"The user wants to train an AI model in a computing cluster using the same environment as in a VM instance. They tried to build the environment using pip, but the experiment failed due to the computing cluster using conda to build the image. The user has three questions: why the computing cluster is using conda instead of pip, if they can build the environment using pip, and if they can update conda in the computing cluster to avoid experiment failure.",
        "Question_gpt_summary":"user want train model comput cluster environ instanc tri build environ pip experi fail comput cluster conda build imag user question comput cluster conda instead pip build environ pip updat conda comput cluster avoid experi failur"
    },
    {
        "Question_title":"How to log custom Pytorch model with Mlflow?",
        "Question_body":"<p>I have been using Presumm <a href=\"https:\/\/github.com\/nlpyang\/PreSumm\" rel=\"nofollow noreferrer\">https:\/\/github.com\/nlpyang\/PreSumm<\/a> for text summarization.<\/p>\n\n<p>However, in <code>src\/train_abstractive.py<\/code>, the model learner <code>trainer<\/code> is not a <code>torch.nn.Module<\/code>. However, the input <code>AbsSummarizer<\/code> is an extension of the <code>torch.nn.Module<\/code> class.<\/p>\n\n<p>I want to use <code>mlflow.pytorch.log_model<\/code> to save the model as a native pytorch model. But <code>trainer<\/code> is not a <code>torch.nn.Module<\/code>. How do I go about this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1591717800773,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":495.0,
        "Poster_created_time":1494459143587,
        "Poster_location":null,
        "Poster_reputation_count":1319.0,
        "Poster_view_count":377.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62286596",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"log custom pytorch model presumm http github com nlpyang presumm text summar src train abstract model learner trainer torch modul input abssummar extens torch modul class want us pytorch log model save model nativ pytorch model trainer torch modul",
        "Question_preprocessed_content":"log custom pytorch model text model learner input extens want us save model nativ pytorch model",
        "Question_gpt_summary_original":"The user is facing a challenge in logging a custom Pytorch model with Mlflow while using Presumm for text summarization. The issue is that the model learner \"trainer\" is not a torch.nn.Module, which is required for using mlflow.pytorch.log_model to save the model as a native Pytorch model. The user is seeking guidance on how to resolve this issue.",
        "Question_gpt_summary":"user face challeng log custom pytorch model presumm text summar issu model learner trainer torch modul requir pytorch log model save model nativ pytorch model user seek guidanc resolv issu"
    },
    {
        "Question_title":"Vision API quota\/budget limit and API key help",
        "Question_body":"Hello, I have never used Vision API before but I recently found it very powerful for a project of mine. However I have two concerns regarding its budget limiting, in order to not get an unexpected bill:\n\nIs it possible to set a monthly cost limit? I have been used to Compute Engine which gives me an almost exact cost of the month but this seems not possible here. Since I will be using the API for labelling I have set the label detections requests per minute and per user to a specific amount, also to be sure I have set the global request per minute and per user to the same amount, all the other quotas to 0. If I have understood correctly, setting the max calls quota per minute to 4, for example, should provide a maximum of 178560 calls per month, right?\u00a0Should this limit my budget? Am I safe?\n\nThe API will be used as an API key in a mobile app. I have followed the code examples for iOS & Android and I have seen the key is written in the code. Is this safe? For a better security I have restricted the key to iOS\/Android apps bundle and to Cloud Vision API only. Would it be a safe enough option?\n\nThanks everyone for any help!",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1648805580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":106.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-API-quota-budget-limit-and-API-key-help\/td-p\/409566\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"vision api quota budget limit api kei help hello vision api recent power project concern budget limit order unexpect possibl set monthli cost limit comput engin give exact cost month possibl api label set label detect request minut user specif sure set global request minut user quota understood correctli set max call quota minut exampl provid maximum call month right limit budget safe api api kei mobil app follow code exampl io android seen kei written code safe better secur restrict kei io android app bundl cloud vision api safe option thank help",
        "Question_preprocessed_content":"vision api limit api kei help vision api recent power project concern budget limit order unexpect set monthli cost limit comput engin give exact cost month api set label detect request minut user specif sure set global request minut user quota max quota minut exampl provid maximum month right limit budget safe api api kei mobil code exampl io android kei code safe secur restrict kei bundl cloud vision api safe option thank help",
        "Question_gpt_summary_original":"The user is concerned about the budget limit of the Vision API and wants to know if it is possible to set a monthly cost limit. They have set specific quotas for label detection requests per minute and per user, and have set the global request per minute and per user to the same amount. The user is also concerned about the safety of using the API key in a mobile app and has restricted the key to iOS\/Android apps bundle and to Cloud Vision API only.",
        "Question_gpt_summary":"user concern budget limit vision api want know possibl set monthli cost limit set specif quota label detect request minut user set global request minut user user concern safeti api kei mobil app restrict kei io android app bundl cloud vision api"
    },
    {
        "Question_title":"Consume Azure ML Web Service with Postman: how to pass arguments?",
        "Question_body":"<p>Is it possible to pass parameters to an Azure ML Web Service with Postman? I created an R web service endpoint that runs in an Azure Container Instance. My run function has one argument (&quot;data&quot;). I can call the web service using Azure ML R SDK (using invoke_webservice()) and the input parameter is read successfully from the request content. The input is constructed like:  <\/p>\n<pre><code>toJSON(data.frame(data=&quot;This is my test string&quot;))\n<\/code><\/pre>\n<p>Result:  <\/p>\n<pre><code>[{&quot;data&quot;: &quot;This is my test string&quot;}]\n<\/code><\/pre>\n<p>If I create a Postman request and copy the input to the request body, the input parameter is not passed to the web service. The web service can return a static output to Postman but the variable data is always empty. Is this a property of the ML Web Service? If not, how can I set up the request body so that the argument is read successfully? I have tried many variations, but none have worked.  <\/p>\n<p>I have set content-type header to application\/json. I don't have authentication in the web service, since it is just a test instance.  <\/p>\n<p>Ultimately, we need to call the web service with C# from Azure Function. I know that we can use the C# template from documentation and it can probably pass the parameter to the web service, but it would be nice to be able to test the web service with Postman.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1603965300457,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Try this in postman.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/35960-postman.png?platform=QnA\" alt=\"35960-postman.png\" \/>    <\/p>\n",
        "Answer_comment_count":2.0,
        "Question_closed_time":1603976126077,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/144230\/consume-azure-ml-web-service-with-postman-how-to-p",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"consum web servic postman pass argument possibl pass paramet web servic postman creat web servic endpoint run azur contain instanc run function argument data web servic sdk invok webservic input paramet read successfulli request content input construct like tojson data frame data test string result data test string creat postman request copi input request bodi input paramet pass web servic web servic return static output postman variabl data properti web servic set request bodi argument read successfulli tri variat work set content type header applic json authent web servic test instanc ultim need web servic azur function know us templat document probabl pass paramet web servic nice abl test web servic postman",
        "Question_preprocessed_content":"consum web servic postman argument paramet web servic postman creat web servic endpoint run azur contain instanc run function argument web servic sdk input paramet read request content input construct like result creat postman request copi input request bodi input paramet web servic web servic return static output postman variabl data properti web servic set request bodi argument read tri variat work set header json authent web servic test instanc ultim web servic azur function know us templat document probabl paramet web servic nice abl test web servic postman",
        "Question_gpt_summary_original":"The user is facing challenges in passing parameters to an Azure ML Web Service with Postman. The user has created an R web service endpoint that runs in an Azure Container Instance and can call the web service using Azure ML R SDK. However, when creating a Postman request and copying the input to the request body, the input parameter is not passed to the web service. The user has tried many variations but none have worked. The user has set the content-type header to application\/json and does not have authentication in the web service. The user ultimately needs to call the web service with C# from Azure Function.",
        "Question_gpt_summary":"user face challeng pass paramet web servic postman user creat web servic endpoint run azur contain instanc web servic sdk creat postman request copi input request bodi input paramet pass web servic user tri variat work user set content type header applic json authent web servic user ultim need web servic azur function"
    },
    {
        "Question_title":"About the General Discussion category",
        "Question_body":"<p>This category is here for you to post any general questions or topics you\u2019d like to discuss with the community. Please observe community rules, and enjoy!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1633570856312,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":248.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.sigopt.com\/t\/about-the-general-discussion-category\/22",
        "Tool":"SigOpt",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"gener discuss categori categori post gener question topic youd like discuss commun observ commun rule enjoi",
        "Question_preprocessed_content":"gener categori categori post gener question topic youd like observ rule enjoi",
        "Question_gpt_summary_original":"The General Discussion category is available for users to post any general questions or topics they want to discuss with the community while following community rules.",
        "Question_gpt_summary":"gener discuss categori avail user post gener question topic want discuss commun follow commun rule"
    },
    {
        "Question_title":"Install gym atari in Amazon SageMaker",
        "Question_body":"<p>I am trying to install Gym Atari environments on a Amazon SageMaker instance. I run the following script in a jupyter notebook:<\/p>\n<pre><code>!python -m pip install --upgrade pip\n!pip install gym[atari]\nimport urllib.request\nurllib.request.urlretrieve('http:\/\/www.atarimania.com\/roms\/Roms.rar', 'Roms.rar')\nurllib.request.urlretrieve('https:\/\/www.rarlab.com\/rar\/rarlinux-x64-6.0.2.tar.gz', 'rarlinux-x64-6.0.2.tar.gz')\n!tar -xvzf rarlinux-x64-6.0.2.tar.gz\n%cd rar\n!make\n!sudo make install\n%cd ..\n!unrar x Roms.rar\n!mkdir rars\n!mv HC\\ ROMS.zip rars\n!mv ROMS.zip rars\n!python -m atari_py.import_roms rars\n<\/code><\/pre>\n<p>The script upgrades pip, install gym with the Atari dependencies, fetch the Roms, install unrar to extract the file Roms.rar and then run the command <code>python -m atari_py.import_roms rars<\/code>.<\/p>\n<p>An issue arise when I run <code>python -m atari_py.import_roms rars<\/code>. The output is the following:<\/p>\n<pre><code>rar\/\nrar\/unrar\nrar\/acknow.txt\nrar\/whatsnew.txt\nrar\/order.htm\nrar\/readme.txt\nrar\/rar.txt\nrar\/makefile\nrar\/default.sfx\nrar\/rar\nrar\/rarfiles.lst\nrar\/license.txt\n\/home\/ec2-user\/SageMaker\/rar\nmkdir -p \/usr\/local\/bin\nmkdir -p \/usr\/local\/lib\ncp rar unrar \/usr\/local\/bin\ncp: cannot create regular file \u2018\/usr\/local\/bin\/rar\u2019: Permission denied\ncp: cannot create regular file \u2018\/usr\/local\/bin\/unrar\u2019: Permission denied\nmake: *** [install] Error 1\nmkdir -p \/usr\/local\/bin\nmkdir -p \/usr\/local\/lib\ncp rar unrar \/usr\/local\/bin\ncp rarfiles.lst \/etc\ncp default.sfx \/usr\/local\/lib\n\/home\/ec2-user\/SageMaker\n\nUNRAR 6.02 freeware      Copyright (c) 1993-2021 Alexander Roshal\n\n\nExtracting from Roms.rar\n\nExtracting  HC ROMS.zip                                                 36  OK \nExtracting  ROMS.zip                                                    7 99  OK \nAll OK\ncopying adventure.bin from HC ROMS\/BY ALPHABET (PAL)\/A-G\/Adventure (PAL).bin to \/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/atari_roms\/adventure.bin\nTraceback (most recent call last):\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/runpy.py&quot;, line 193, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 93, in &lt;module&gt;\n    main()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 89, in main\n    import_roms(args.dirpath)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 78, in import_roms\n    _check_zipfile(f, save_if_matches)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 23, in _check_zipfile\n    process_f(innerf)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/amazonei_pytorch_latest_p36\/lib\/python3.6\/site-packages\/atari_py\/import_roms.py&quot;, line 67, in save_if_matches\n    f.seek(0)\nio.UnsupportedOperation: seek\n<\/code><\/pre>\n<p>I did some research but found nothing. I would appreciate some help, thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1627045779903,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":148.0,
        "Poster_created_time":1539639589447,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":8.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68499721",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"instal gym atari try instal gym atari environ instanc run follow script jupyt notebook python pip instal upgrad pip pip instal gym atari import urllib request urllib request urlretriev http www atarimania com rom rom rar rom rar urllib request urlretriev http www rarlab com rar rarlinux tar rarlinux tar tar xvzf rarlinux tar rar sudo instal unrar rom rar mkdir rar rom zip rar rom zip rar python atari import rom rar script upgrad pip instal gym atari depend fetch rom instal unrar extract file rom rar run command python atari import rom rar issu aris run python atari import rom rar output follow rar rar unrar rar acknow txt rar whatsnew txt rar order htm rar readm txt rar rar txt rar makefil rar default sfx rar rar rar rarfil lst rar licens txt home user rar mkdir usr local bin mkdir usr local lib rar unrar usr local bin creat regular file usr local bin rar permiss deni creat regular file usr local bin unrar permiss deni instal error mkdir usr local bin mkdir usr local lib rar unrar usr local bin rarfil lst default sfx usr local lib home user unrar freewar copyright alexand roshal extract rom rar extract rom zip extract rom zip copi adventur bin rom alphabet pal adventur pal bin home user anaconda env amazonei pytorch latest lib python site packag atari atari rom adventur bin traceback recent file home user anaconda env amazonei pytorch latest lib python runpi line run modul main main mod spec file home user anaconda env amazonei pytorch latest lib python runpi line run code exec code run global file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line main file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line main import rom arg dirpath file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line import rom check zipfil save match file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line check zipfil process innerf file home user anaconda env amazonei pytorch latest lib python site packag atari import rom line save match seek unsupportedoper seek research appreci help thank",
        "Question_preprocessed_content":"gym atari try gym atari environ instanc run script jupyt script upgrad pip gym atari depend fetch rom unrar extract file run aris run output research help thank",
        "Question_gpt_summary_original":"The user is trying to install Gym Atari environments on an Amazon SageMaker instance using a script that upgrades pip, installs gym with the Atari dependencies, fetches the Roms, installs unrar to extract the file Roms.rar, and then runs the command python -m atari_py.import_roms rars. However, the user encounters an issue when running the command, which results in a permission denied error and an io.UnsupportedOperation error. The user has researched the issue but has not found a solution.",
        "Question_gpt_summary":"user try instal gym atari environ instanc script upgrad pip instal gym atari depend fetch rom instal unrar extract file rom rar run command python atari import rom rar user encount issu run command result permiss deni error unsupportedoper error user research issu solut"
    },
    {
        "Question_title":"Sagemaker studio doesn't load and returns internal failure",
        "Question_body":"Trying to open sagemaker studio and keep getting the following error message: \n\nThe JupyterServer app default encountered a problem and was stopped. If you continue to experience issues, please contact Customer Service.\nDetails: InternalFailure\n\nHave restarted the app for couple of times and always the error message is the same with no progress. Any help would be appreciated.",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1648852294743,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668614502492,
        "Question_score_count":0.0,
        "Question_view_count":699.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQ_rnD4H_REKOnpPfMp8hMg\/sagemaker-studio-doesn-t-load-and-returns-internal-failure",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"studio load return intern failur try open studio get follow error messag jupyterserv app default encount problem stop continu experi issu contact custom servic detail internalfailur restart app coupl time error messag progress help appreci",
        "Question_preprocessed_content":"studio load return intern failur try open studio jupyterserv default encount problem continu experi contact custom servic detail internalfailur restart coupl time help",
        "Question_gpt_summary_original":"The user is unable to load Sagemaker studio and is receiving an error message stating that the JupyterServer app has encountered an internal failure. Despite restarting the app multiple times, the issue persists and the user is seeking assistance.",
        "Question_gpt_summary":"user unabl load studio receiv error messag state jupyterserv app encount intern failur despit restart app multipl time issu persist user seek assist"
    },
    {
        "Question_title":"AWS Sagemaker | How to debug docker image | What the parameter we pass",
        "Question_body":"<p>We want to upload a docker image which have our custom code for tensorflow, now we followed this standard code \n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_bring_your_own\/tensorflow_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_bring_your_own\/tensorflow_bring_your_own.ipynb<\/a><\/p>\n\n<p>We are able to upload docker there with our dependency but we are not able to pass the S3 location to their method, now we are not sure if the S3 location is passing to the container or not so added print which is not printing on sagemaker. Can someone please help how to debug the docker as the custom log is also not available on cloudwatch.<\/p>\n\n<pre><code>018-11-30 09:55:17 Uploading - Uploading generated training model\n2018-11-30 09:55:17 Failed - Training job failed\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-17-5fc1c1e7ed65&gt; in &lt;module&gt;()\n     11                       hyperparameters=hyperparameters)\n     12 \n---&gt; 13 estimator.fit(data_location)\n     14 \n     15 # predictor = estimator.deploy(1, instance_type)\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name)\n    232         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    233         if wait:\n--&gt; 234             self.latest_training_job.wait(logs=logs)\n    235 \n    236     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n    571     def wait(self, logs=True):\n    572         if logs:\n--&gt; 573             self.sagemaker_session.logs_for_job(self.job_name, wait=True)\n    574         else:\n    575             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll)\n   1121 \n   1122         if wait:\n-&gt; 1123             self._check_job_status(job_name, description, 'TrainingJobStatus')\n   1124             if dot:\n   1125                 print()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    821             reason = desc.get('FailureReason', '(No reason provided)')\n    822             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 823             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    824 \n    825     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Training job tensor-2018-11-30-09-52-12-964: Failed Reason: AlgorithmError: Exception during training: Return Code: 1, CMD: ['\/usr\/bin\/python', 'cifar10.py', '--model-dir', '\/opt\/ml\/model', '--train-steps', '100'], Err: b'\/usr\/local\/lib\/python3.5\/dist-packages\/h5py\/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\\n  from ._conv import register_converters as _register_converters\\nTraceback (most recent call last):\\n  File \"cifar10.py\", line 195, in &lt;module&gt;\\n    main()\\n  File \"cifar10.py\", line 188, in main\\n    interactions_processed, user_meta_processed, item_meta_processed, item_feats_set = process_data(interaction_data, interaction_cols, users_meta, users_meta_cols, items_meta, items_meta_cols, user_meta_filterlist=user_meta_list)\\n  File \"cifar10.py\", line 32, in process_data\\n    df=pd.read_csv(interaction_data, engine=\\'c\\', encoding=\\'latin1\\', usecols=interaction_cols).astype(str)\\n  File \"\/usr\/local\/lib\/python3.5\/d\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1543576936170,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":1095.0,
        "Poster_created_time":1501403168107,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53556558",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"debug docker imag paramet pass want upload docker imag custom code tensorflow follow standard code http github com awslab amazon exampl blob master advanc function tensorflow bring tensorflow bring ipynb abl upload docker depend abl pass locat method sure locat pass contain ad print print help debug docker custom log avail cloudwatch upload upload gener train model fail train job fail valueerror traceback recent hyperparamet hyperparamet estim fit data locat predictor estim deploi instanc type anaconda env tensorflow lib python site packag estim fit self input wait log job self latest train job trainingjob start new self input wait self latest train job wait log log def compil job self anaconda env tensorflow lib python site packag estim wait self log def wait self log true log self session log job self job wait true self session wait job self job anaconda env tensorflow lib python site packag session log job self job wait poll wait self check job statu job descript trainingjobstatu dot print anaconda env tensorflow lib python site packag session check job statu self job desc statu kei reason desc failurereason reason provid job type statu kei replac jobstatu job rais valueerror error reason format job type job statu reason def wait endpoint self endpoint poll valueerror error train job tensor fail reason algorithmerror except train return code cmd usr bin python cifar model dir opt model train step err usr local lib python dist packag hpy init futurewarn convers second argument issubdtyp float float deprec futur treat float dtype float type conv import regist convert regist convert ntraceback recent file cifar line main file cifar line main interact process user meta process item meta process item feat set process data interact data interact col user meta user meta col item meta item meta col user meta filterlist user meta list file cifar line process data read csv interact data engin encod latin usecol interact col astyp str file usr local lib python",
        "Question_preprocessed_content":"debug docker imag paramet want upload docker imag custom code tensorflow standard code abl upload docker depend abl locat method sure locat contain print print help debug docker custom log avail cloudwatch",
        "Question_gpt_summary_original":"The user is trying to upload a custom docker image with their code for TensorFlow on AWS Sagemaker, but they are unable to pass the S3 location to the method and are unsure if it is being passed to the container. They have added a print statement, but it is not printing on Sagemaker, and they are unable to debug the docker image as the custom log is not available on Cloudwatch. The training job has failed with a ValueError.",
        "Question_gpt_summary":"user try upload custom docker imag code tensorflow unabl pass locat method unsur pass contain ad print statement print unabl debug docker imag custom log avail cloudwatch train job fail valueerror"
    },
    {
        "Question_title":"Where to perform the saving of an nodeoutput in Kedro?",
        "Question_body":"<p>In Kedro, we can pipeline different nodes and partially run some nodes. When we are partially running some nodes, we need to save some inputs from the nodes somewhere so that when another node is run it can access the data that the previous node has generated. However, in which file do we write the code for this - pipeline.py, run.py or nodes.py?<\/p>\n\n<p>For instance, I am trying to save a dir path directly to the DataCatalog under a variable name 'model_path'. <\/p>\n\n<p>Snippet from pipeline.py:<\/p>\n\n<pre><code>    # A mapping from a pipeline name to a ``Pipeline`` object.\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\nio = DataCatalog(dict(\n    model_path=MemoryDataSet()\n))\n\nio.save('model_path', \"data\/06_models\/model_test\")\nprint('****', io.exists('model_path'))\n\npipeline = Pipeline([\n    node(\n        split_files,\n        [\"data_csv\", \"parameters\"],\n        [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\"],\n        name=\"splitting filenames\"\n    ),\n    # node(\n    #     create_and_train,\n    #     [\"train_filenames\", \"val_filenames\", \"train_labels\", \"val_labels\", \"parameters\"],\n    #     \"model_path\",\n    #     name=\"Create Dataset, Train and Save Model\"\n    # ),\n    node(\n        validate_model,\n        [\"val_filenames\", \"val_labels\", \"model_path\"],\n        None,\n        name=\"Validate Model\",\n    )\n\n]).decorate(decorators.log_time, decorators.mem_profile)\n\nreturn {\n    \"__default__\": pipeline\n}\n<\/code><\/pre>\n\n<p>However, I get the following error when I Kedro run:<\/p>\n\n<pre><code>ValueError: Pipeline input(s) {'model_path'} not found in the DataCatalog\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1571371429583,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1571671894436,
        "Question_score_count":4.0,
        "Question_view_count":1606.0,
        "Poster_created_time":1545311054088,
        "Poster_location":null,
        "Poster_reputation_count":170.0,
        "Poster_view_count":33.0,
        "Answer_body":"<p>Node inputs are automatically loaded by Kedro from the <code>DataCatalog<\/code> before being passed to the node function. Node outputs are consequently saved to the DataCatalog after the node successfully produces some data. DataCatalog configuration by default is taken from <code>conf\/base\/catalog.yml<\/code>. <\/p>\n\n<p>In your example <code>model_path<\/code> is produced by <code>Create Dataset, Train and Save Model<\/code> node and then consumed by <code>Validate Model<\/code>. If required dataset definition is not found in the <code>conf\/base\/catalog.yml<\/code>, Kedro will try to store this dataset in memory using <code>MemoryDataSet<\/code>. This will work if you run the pipeline that contains both <code>Create Dataset...<\/code> and <code>Validate Model<\/code> nodes (given no other issues arise). However, when you are trying to run <code>Validate Model<\/code> node alone, Kedro attempts to read <code>model_path<\/code> dataset from memory, which doesn't exist there.<\/p>\n\n<p>So, <strong>TLDR<\/strong>:<\/p>\n\n<p>To mitigate this, you need to:<\/p>\n\n<p>a) persist <code>model_path<\/code> by adding something like the following to your <code>conf\/base\/catalog.yml<\/code>:<\/p>\n\n<pre><code>model_path:\n  type: TextLocalDataSet\n  filepath: data\/02_intermediate\/model_path.txt\n<\/code><\/pre>\n\n<p>b) run <code>Create Dataset, Train and Save Model<\/code> node (and its dependencies) at least once<\/p>\n\n<p>After completing a) and b) you should be able to start running <code>Validate Model<\/code> separately.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1571671635460,
        "Answer_last_edit_time":1571672436532,
        "Answer_score_count":3.0,
        "Answerer_created_time":1393579668636,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation_count":1450.0,
        "Answerer_view_count":162.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58443788",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"perform save nodeoutput pipelin differ node partial run node partial run node need save input node node run access data previou node gener file write code pipelin run node instanc try save dir path directli datacatalog variabl model path snippet pipelin map pipelin pipelin object def creat pipelin kwarg dict str pipelin datacatalog dict model path memorydataset save model path data model model test print exist model path pipelin pipelin node split file data csv paramet train filenam val filenam train label val label split filenam node creat train train filenam val filenam train label val label paramet model path creat dataset train save model node valid model val filenam val label model path valid model decor decor log time decor mem profil return default pipelin follow error run valueerror pipelin input model path datacatalog",
        "Question_preprocessed_content":"perform save nodeoutput pipelin node run node node save input node node run data previou node gener file write code instanc try save dir path directli datacatalog variabl run",
        "Question_gpt_summary_original":"The user is facing a challenge in Kedro regarding where to save the output of a node when partially running some nodes. They are trying to save a directory path directly to the DataCatalog under a variable name 'model_path', but are encountering an error stating that the pipeline input 'model_path' is not found in the DataCatalog. The user is unsure whether to write the code for this in pipeline.py, run.py, or nodes.py.",
        "Question_gpt_summary":"user face challeng save output node partial run node try save directori path directli datacatalog variabl model path encount error state pipelin input model path datacatalog user unsur write code pipelin run node"
    },
    {
        "Question_title":"Can you create an azure aks cluster with ephemeral os disk using python sdk ?",
        "Question_body":"<p>I'm deploying some code with azure aks and think an ephemeral os disk would be beneficial but I can't see a way to include this setting with the python SDK. Is this possible at the moment?     <\/p>\n<p>Cheers    <br \/>\nRhys<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1664280949717,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1025025\/can-you-create-an-azure-aks-cluster-with-ephemeral",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat azur ak cluster ephemer disk python sdk deploi code azur ak think ephemer disk benefici wai includ set python sdk possibl moment cheer rhy",
        "Question_preprocessed_content":"creat azur ak cluster ephemer disk python sdk deploi code azur ak think ephemer disk benefici wai includ python sdk moment rhy",
        "Question_gpt_summary_original":"The user is trying to create an Azure AKS cluster with an ephemeral OS disk using Python SDK but is unable to find a way to include this setting. They are seeking help to determine if it is currently possible.",
        "Question_gpt_summary":"user try creat azur ak cluster ephemer disk python sdk unabl wai includ set seek help determin current possibl"
    },
    {
        "Question_title":"optuna.integration.lightGBM custom optimization metric",
        "Question_body":"<p>I am trying to optimize a lightGBM model using optuna.<\/p>\n<p>Reading the docs I noticed that there are two approaches that can be used, as mentioned here: <a href=\"https:\/\/medium.com\/optuna\/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258\" rel=\"nofollow noreferrer\">LightGBM Tuner: New Optuna Integration for Hyperparameter Optimization<\/a>.<\/p>\n<p>The first approach uses the &quot;standard&quot; way of optimizing with optuna (objective function + trials), the second one wrappes everything together with the <code>.train()<\/code> function. The first one basically tries combination of hyper-parameters values, while the second one optimizes following a step-wise approach on the hyperparameters.<\/p>\n<p>The two approaches are showed in the following code examples in the optuna github repository:<\/p>\n<ol>\n<li><a href=\"https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_simple.py\" rel=\"nofollow noreferrer\">First approach<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/optuna\/optuna-examples\/blob\/main\/lightgbm\/lightgbm_tuner_simple.py\" rel=\"nofollow noreferrer\">Second approach<\/a><\/li>\n<\/ol>\n<p>Both codes perform the exact same optimization on the same parameters (optimized parameters by the second approach are decribed <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.integration.lightgbm.LightGBMTuner.html\" rel=\"nofollow noreferrer\">here<\/a>), but in different ways (combinatorial vs step-wise).<\/p>\n<p><strong>My questions is:<\/strong><\/p>\n<ol>\n<li><p>Is it possible in the second approach to specify a custom evaluation metric? In the first one I can easily change the accuracy used inside the github examples with any custom metric.<br \/>\nAs an example I could write:<\/p>\n<pre><code> import lightgbm as lgb\n import numpy as np\n import sklearn.datasets\n import sklearn.metrics\n from sklearn.model_selection import train_test_split\n\n import optuna\n\n def my_eval_metric(valid_y, pred_labels):\n     # my custom metric\n     ..........\n     ..........\n\n     return my_metric\n\n def objective(trial):\n     data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n     train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n     dtrain = lgb.Dataset(train_x, label=train_y)\n\n     param = {\n         &quot;objective&quot;: &quot;binary&quot;,\n         &quot;metric&quot;: &quot;binary_logloss&quot;,\n         &quot;verbosity&quot;: -1,\n         &quot;boosting_type&quot;: &quot;gbdt&quot;,\n         &quot;lambda_l1&quot;: trial.suggest_float(&quot;lambda_l1&quot;, 1e-8, 10.0, log=True),\n         &quot;lambda_l2&quot;: trial.suggest_float(&quot;lambda_l2&quot;, 1e-8, 10.0, log=True),\n         &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256),\n         &quot;feature_fraction&quot;: trial.suggest_float(&quot;feature_fraction&quot;, 0.4, 1.0),\n         &quot;bagging_fraction&quot;: trial.suggest_float(&quot;bagging_fraction&quot;, 0.4, 1.0),\n         &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7),\n         &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100),\n     }\n\n     gbm = lgb.train(param, dtrain)\n     preds = gbm.predict(valid_x)\n     pred_labels = np.rint(preds)\n     my_eval_metric_value = my_eval_metric(valid_y, pred_labels)\n     return custom_metric_value \n\n\n if __name__ == &quot;__main__&quot;:\n     study = optuna.create_study(direction=&quot;maximize&quot;)\n     study.optimize(objective, n_trials=100)\n\n     print(&quot;Number of finished trials: {}&quot;.format(len(study.trials)))\n\n     print(&quot;Best trial:&quot;)\n     trial = study.best_trial\n\n     print(&quot;  Value: {}&quot;.format(trial.value))\n\n     print(&quot;  Params: &quot;)\n     for key, value in trial.params.items():\n         print(&quot;    {}: {}&quot;.format(key, value))\n<\/code><\/pre>\n<\/li>\n<\/ol>\n<p>This code will return the parameters of the lightGBM model that maximizes my custom metric. However in the second approach I haven't been able to specify my own custom metric.<\/p>\n<p><strong>UPDATE:<\/strong>\nI managed to define my own custom metric and its usage inside the second approach. A minimal reproducible code is the following (just pass the data using <code>train_test_split<\/code> by scikit):<\/p>\n<pre><code>from sklearn.metrics import average_precision_score\nimport optuna.integration.lightgbm as lgb_sequential\n\ndef tune_lightGBM_sequential(X_train, X_val, y_train, y_val):\n    \n    def calculate_ctr(gt):\n        positive = len([x for x in gt if x == 1])\n        ctr = positive\/float(len(gt))\n        return ctr\n\n    def compute_rce(preds, train_data):\n        gt = train_data.get_label()\n        cross_entropy = log_loss(gt, preds)\n        data_ctr = calculate_ctr(gt)\n        strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n        rce = (1.0 - cross_entropy\/strawman_cross_entropy)*100.0\n        return ('rce', rce, True)\n\n    def compute_avg_precision(preds, train_data):\n        gt = train_data.get_label()\n        avg_precision= average_precision_score(gt, preds)\n        return('avg_precision', avg_precision, True)\n    \n    params = {\n        &quot;objective&quot;: &quot;binary&quot;,\n        &quot;metric&quot;: 'custom',\n        &quot;boosting_type&quot;: &quot;gbdt&quot;,\n        &quot;verbose&quot; : 2\n    }\n    \n    dtrain = lgb_sequential.Dataset(X_train, label=y_train)\n    dval = lgb_sequential.Dataset(X_val, label=y_val)\n    \n    print('Starting training lightGBM sequential')\n    model = lgb_sequential.train(\n        params, dtrain, valid_sets=[dtrain, dval], verbose_eval=True,num_boost_round =2, early_stopping_rounds=100, feval = [compute_rce, compute_avg_precision]\n    )\n    \n    return model.params\n<\/code><\/pre>\n<p>However Optuna doesn't seem to be able to select the best trial based on my custom metrics, in fact, I get the following error:<\/p>\n<blockquote>\n<p>[W 2021-05-16 15:56:48,759] Trial 0 failed because of the following\nerror: KeyError('custom') Traceback (most recent call last): File\n&quot;C:\\Users\\Mattia\\anaconda3\\envs\\rec_sys_challenge\\lib\\site-packages\\optuna_optimize.py&quot;,\nline 217, in _run_trial value_or_values = func(trial) File\n&quot;C:\\Users\\Mattia\\anaconda3\\envs\\rec_sys_challenge\\lib\\site-packages\\optuna\\integration_lightgbm_tuner\\optimize.py&quot;,\nline 251, in call val_score = self._get_booster_best_score(booster)\nFile\n&quot;C:\\Users\\Mattia\\anaconda3\\envs\\rec_sys_challenge\\lib\\site-packages\\optuna\\integration_lightgbm_tuner\\optimize.py&quot;,\nline 118, in _get_booster_best_score val_score =\nbooster.best_score[valid_name][metric] KeyError: 'custom'<\/p>\n<\/blockquote>\n<p>It seems a issue with the library (you can find more here: <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/1351\" rel=\"nofollow noreferrer\">GitHub Issue<\/a>), I tried many proposed solutions, but none of them worked.<\/p>\n<p>Any help?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1621084691557,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1621176110623,
        "Question_score_count":0.0,
        "Question_view_count":1217.0,
        "Poster_created_time":1522782849496,
        "Poster_location":"Milano, MI, Italia",
        "Poster_reputation_count":1144.0,
        "Poster_view_count":305.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67547127",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"integr lightgbm custom optim metric try optim lightgbm model read doc notic approach mention lightgbm tuner new integr hyperparamet optim approach us standard wai optim object function trial second wrapp train function basic tri combin hyper paramet valu second optim follow step wise approach hyperparamet approach show follow code exampl github repositori approach second approach code perform exact optim paramet optim paramet second approach decrib differ wai combinatori step wise question possibl second approach specifi custom evalu metric easili chang accuraci insid github exampl custom metric exampl write import lightgbm lgb import numpi import sklearn dataset import sklearn metric sklearn model select import train test split import def eval metric valid pred label custom metric return metric def object trial data target sklearn dataset load breast cancer return true train valid train valid train test split data target test size dtrain lgb dataset train label train param object binari metric binari logloss verbos boost type gbdt lambda trial suggest float lambda log true lambda trial suggest float lambda log true num leav trial suggest int num leav featur fraction trial suggest float featur fraction bag fraction trial suggest float bag fraction bag freq trial suggest int bag freq min child sampl trial suggest int min child sampl gbm lgb train param dtrain pred gbm predict valid pred label rint pred eval metric valu eval metric valid pred label return custom metric valu main studi creat studi direct maxim studi optim object trial print number finish trial format len studi trial print best trial trial studi best trial print valu format trial valu print param kei valu trial param item print format kei valu code return paramet lightgbm model maxim custom metric second approach haven abl specifi custom metric updat manag defin custom metric usag insid second approach minim reproduc code follow pass data train test split scikit sklearn metric import averag precis score import integr lightgbm lgb sequenti def tune lightgbm sequenti train val train val def calcul ctr posit len ctr posit float len return ctr def comput rce pred train data train data label cross entropi log loss pred data ctr calcul ctr strawman cross entropi log loss data ctr rang len rce cross entropi strawman cross entropi return rce rce true def comput avg precis pred train data train data label avg precis averag precis score pred return avg precis avg precis true param object binari metric custom boost type gbdt verbos dtrain lgb sequenti dataset train label train dval lgb sequenti dataset val label val print start train lightgbm sequenti model lgb sequenti train param dtrain valid set dtrain dval verbos eval true num boost round earli stop round feval comput rce comput avg precis return model param abl select best trial base custom metric fact follow error trial fail follow error keyerror custom traceback recent file user mattia anaconda env rec sy challeng lib site packag optim line run trial valu valu func trial file user mattia anaconda env rec sy challeng lib site packag integr lightgbm tuner optim line val score self booster best score booster file user mattia anaconda env rec sy challeng lib site packag integr lightgbm tuner optim line booster best score val score booster best score valid metric keyerror custom issu librari github issu tri propos solut work help",
        "Question_preprocessed_content":"custom optim metric try optim lightgbm model read doc notic mention lightgbm tuner new integr hyperparamet optim us standard wai optim second function tri combin valu second optim hyperparamet show code exampl github repositori second code perform exact optim paramet wai question second specifi custom evalu metric easili chang insid github exampl custom metric exampl write code return paramet lightgbm model maxim custom metric second haven abl specifi custom metric updat manag defin custom metric usag insid second minim reproduc code abl select best trial base custom metric fact trial fail traceback file line func file line file line custom librari tri propos solut work help",
        "Question_gpt_summary_original":"The user is trying to optimize a lightGBM model using optuna and is exploring two approaches mentioned in the documentation. The first approach involves optimizing with optuna's objective function and trials, while the second approach wraps everything together with the .train() function. The user is trying to specify a custom evaluation metric in the second approach but is encountering an error. The user has tried various solutions proposed in the GitHub issue but none of them have worked.",
        "Question_gpt_summary":"user try optim lightgbm model explor approach mention document approach involv optim object function trial second approach wrap train function user try specifi custom evalu metric second approach encount error user tri solut propos github issu work"
    },
    {
        "Question_title":"Artifacts are not shown in UI",
        "Question_body":"I run mlflow server with Postgres backend and local directory as default artifact root. When I run my experiments\/runs I do see them in mlflow uI, however, artifacts are not shown. Artifacts are created in the local directory ( subdirectory from which I train model), but they are not showing in UI.",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1560965321000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":623.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/-WkRc1NoH5g",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"artifact shown run server postgr backend local directori default artifact root run experi run artifact shown artifact creat local directori subdirectori train model show",
        "Question_preprocessed_content":"artifact shown run server postgr backend local directori default artifact run artifact shown artifact creat local directori show",
        "Question_gpt_summary_original":"The user is running mlflow server with Postgres backend and local directory as default artifact root. Although the experiments\/runs are visible in the mlflow UI, the artifacts created in the local directory are not showing up in the UI.",
        "Question_gpt_summary":"user run server postgr backend local directori default artifact root experi run visibl artifact creat local directori show"
    },
    {
        "Question_title":"Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally.",
        "Question_body":"Hi,\n\nI have notebook like this\n\nWhen trying to start is I get error \"Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally.\"\n\nI upgraded from free trial, but it didn't help. Next I went to quota page and found no limit:\n\nWhat's the issue?\n\nThanks to all.",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1684641120000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":53.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Quota-GPUS-ALL-REGIONS-exceeded-Limit-0-0-globally\/td-p\/595266\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"quota gpu region exceed limit global notebook like try start error quota gpu region exceed limit global upgrad free trial help went quota page limit issu thank",
        "Question_preprocessed_content":"quota limit like try start quota limit upgrad trial help went quota page limit thank",
        "Question_gpt_summary_original":"The user is encountering an error message \"Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally\" while trying to start their notebook. They upgraded from the free trial but the issue persists. Upon checking the quota page, they found no limit. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error messag quota gpu region exceed limit global try start notebook upgrad free trial issu persist check quota page limit user seek help resolv issu"
    },
    {
        "Question_title":"MLFlow active run does not match environment run id",
        "Question_body":"<p>I am trying to perform an MLFlow run but stuck with the following error after trying a lot of things.<\/p>\n<pre><code>\nrun = mlflow.active_run()\nif run:\n    print(&quot;Active run_id: {}&quot;.format(run.info.run_id))\n    mlflow.end_run()\n\nmlflow.set_experiment('TNF_EXP') \nmlflow.set_tracking_uri(&quot;http:\/\/localhost:5000\/&quot;) # Actual Server URI instead of localhost\nexperiment = mlflow.get_experiment_by_name(&quot;TNF_EXP&quot;)\n\nwith mlflow.start_run(experiment_id=experiment.experiment_id) as run:\n...\n...\n\nmlflow.end_run()\n<\/code><\/pre>\n<p>Error -<\/p>\n<pre><code>File &quot;\/...\/ModelTrainer.py&quot;, line 108, in train\n    with mlflow.start_run(experiment_id=experiment.experiment_id) as run:\n  File &quot;\/usr\/local\/lib\/python3.6\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 207, in start_run\n    &quot;arguments&quot;.format(existing_run_id)\nmlflow.exceptions.MlflowException: Cannot start run with ID e9953eb5918845bb9be1xxxxxx because active run ID does not match environment run ID. Make sure --experiment-name or --experiment-id matches experiment set with set_experiment(), or just use command-line arguments\n2021\/02\/11 09:41:36 ERROR mlflow.cli: === Run (ID 'e9953eb5918845bb9be1xxxxxx') failed ===\n<\/code><\/pre>\n<p>I noticed I had an <code>active run<\/code> earlier so I included the first <code>if block<\/code> to end that run. The code ran successfully and I was able to log the data on MLFlow UI but now when I run it I start getting the same issue. There are no active runs found before starting a new run currently.<\/p>\n<blockquote>\n<p>FYI, I am running the code on Azure server with the respective tracking URI mentioned in the code.<\/p>\n<\/blockquote>\n<p>However the code runs fine if I include an argument <code>--experiment-name=&quot;TNF_EXP&quot;<\/code> in the <code>mlflow run<\/code> command on the CLI<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1613037319843,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":4094.0,
        "Poster_created_time":1559822816092,
        "Poster_location":null,
        "Poster_reputation_count":861.0,
        "Poster_view_count":149.0,
        "Answer_body":"<p>That is primarily because you have started a run with <code>default experiment name<\/code> and then you are trying to set the <code>experiment_name<\/code> as &quot;TNF_EXP&quot;.<\/p>\n<p>Will suggest you to make use of <code>mlflow.run(..., experiment_name=&quot;TNF_EXP&quot;)<\/code> python method then running it from the <code>CLI<\/code>.<\/p>\n<p>You can find more information <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.run\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1616657272012,
        "Answer_last_edit_time":null,
        "Answer_score_count":3.0,
        "Answerer_created_time":1608712064260,
        "Answerer_location":null,
        "Answerer_reputation_count":66.0,
        "Answerer_view_count":21.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66152375",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"activ run match environ run try perform run stuck follow error try lot thing run activ run run print activ run format run info run end run set experi tnf exp set track uri http localhost actual server uri instead localhost experi experi tnf exp start run experi experi experi run end run error file modeltrain line train start run experi experi experi run file usr local lib python site packag track fluent line start run argument format exist run except except start run eebbbbexxxxxx activ run match environ run sure experi experi match experi set set experi us command line argument error cli run eebbbbexxxxxx fail notic activ run earlier includ block end run code ran successfulli abl log data run start get issu activ run start new run current fyi run code azur server respect track uri mention code code run fine includ argument experi tnf exp run command cli",
        "Question_preprocessed_content":"activ run match environ run try perform run stuck try lot thing notic earlier includ end run code ran abl log data run start activ run start new run fyi code azur server respect track uri mention code code run fine includ argument cli",
        "Question_gpt_summary_original":"The user is encountering an error while performing an MLFlow run, where the active run ID does not match the environment run ID. The user has tried to end the active run and set the experiment ID, but the issue persists. The code runs fine with the argument \"--experiment-name\" in the MLFlow run command on the CLI. The user is running the code on an Azure server with the respective tracking URI mentioned in the code.",
        "Question_gpt_summary":"user encount error perform run activ run match environ run user tri end activ run set experi issu persist code run fine argument experi run command cli user run code azur server respect track uri mention code"
    },
    {
        "Question_title":"WANDB run initialization",
        "Question_body":"<p>I wanted to try using wandb to log runs of my ML experiments for a project; but I am not able to initialize the run itself.\nI tried:<\/p>\n<p><code>run = wandb.init(project=&quot;name&quot;,entity=&quot;username&quot;,name=&quot;classification&quot;)<\/code><\/p>\n<p>This results in:\nwandb: W&amp;B API key is configured (use <code>wandb login --relogin<\/code> to force relogin)<\/p>\n<p>wandb: Network error (ConnectTimeout), entering retry loop.<\/p>\n<p>wandb: Network error (ConnectTimeout), entering retry loop.<\/p>\n<p>What can I do to fix this? (I did login through the terminal before launching this cell idk what else to try)<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_created_time":1650311929900,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1650312801032,
        "Question_score_count":0.0,
        "Question_view_count":337.0,
        "Poster_created_time":1650311491996,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71916491",
        "Tool":"Weights & Biases",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"run initi want try log run experi project abl initi run tri run init project entiti usernam classif result api kei configur us login relogin forc relogin network error connecttimeout enter retri loop network error connecttimeout enter retri loop fix login termin launch cell idk try",
        "Question_preprocessed_content":"run initi want try log run experi project abl initi run tri result api kei configur network enter retri network enter retri fix",
        "Question_gpt_summary_original":"The user is facing issues while initializing the WANDB run for logging ML experiments. The error message \"Network error (ConnectTimeout)\" is displayed, and the user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user face issu initi run log experi error messag network error connecttimeout displai user seek help resolv issu"
    },
    {
        "Question_title":"Get notebook outline in sagemaker studio like in Visual Studio Code",
        "Question_body":"<p>When I use visual studio code with jupyter notebook, I have an &quot;outline&quot; tab in the left panels that display the Markdown section of my notebook for quick access.<\/p>\n<p>But in Sagemaker studio I don't have this and I would like to add it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1652353604940,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":164.0,
        "Poster_created_time":1576136255052,
        "Poster_location":null,
        "Poster_reputation_count":795.0,
        "Poster_view_count":37.0,
        "Answer_body":"<p>The 'Outline' tab (Table of Contents extension in Jupyter) is not available for Studio yet.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi.html\" rel=\"nofollow noreferrer\">SageMaker notebook instances<\/a> come with the extension prebuilt.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1652459529952,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1370505440848,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation_count":333.0,
        "Answerer_view_count":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72214443",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"notebook outlin studio like visual studio code us visual studio code jupyt notebook outlin tab left panel displai markdown section notebook quick access studio like add",
        "Question_preprocessed_content":"outlin studio like visual studio code us visual studio code jupyt outlin tab left panel displai markdown section quick studio like",
        "Question_gpt_summary_original":"The user is facing a challenge in Sagemaker studio as they are unable to find the \"outline\" tab in the left panel, which is available in Visual Studio Code while using Jupyter notebook. They are seeking a solution to add this feature in Sagemaker studio for quick access to the Markdown section of their notebook.",
        "Question_gpt_summary":"user face challeng studio unabl outlin tab left panel avail visual studio code jupyt notebook seek solut add featur studio quick access markdown section notebook"
    },
    {
        "Question_title":"Why jupyter notebook packages are not working in aws sagemaker notebook instances?",
        "Question_body":"<pre><code>fig, axes = plt.subplots(2,figsize=(15,10))\nsns.lineplot(agg_cases_death.index, agg_cases_death.cases, ax=axes[0]).set_title('Cases')\nsns.lineplot(agg_cases_death.index, agg_cases_death.deaths, ax=axes[1]).set_title('Deaths')\nplt.show()\n<\/code><\/pre>\n\n<p>The above used to work perfectly on jupyter notebook on my desktop but while I ran the same code in AWS Sagemaker Jupyter notebook it produces error. It says the module 'seaborn' has no attribute lineplot.<\/p>\n\n<p>Is there something we need to set up while using aws sagemaker jupyter?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1590715661283,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":840.0,
        "Poster_created_time":1547060529652,
        "Poster_location":"Cincinnati, OH, USA",
        "Poster_reputation_count":29.0,
        "Poster_view_count":61.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62077199",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"jupyt notebook packag work notebook instanc fig ax plt subplot figsiz sn lineplot agg case death index agg case death case ax set titl case sn lineplot agg case death index agg case death death ax set titl death plt work perfectli jupyt notebook desktop ran code jupyt notebook produc error sai modul seaborn attribut lineplot need set jupyt",
        "Question_preprocessed_content":"jupyt packag work instanc work perfectli jupyt desktop ran code jupyt produc sai modul seaborn lineplot set jupyt",
        "Question_gpt_summary_original":"The user is facing an issue while running a code on AWS Sagemaker Jupyter notebook that used to work perfectly on Jupyter notebook on their desktop. The error message says that the module 'seaborn' has no attribute lineplot. The user is seeking help to understand if there is any setup required while using AWS Sagemaker Jupyter.",
        "Question_gpt_summary":"user face issu run code jupyt notebook work perfectli jupyt notebook desktop error messag sai modul seaborn attribut lineplot user seek help understand setup requir jupyt"
    },
    {
        "Question_title":"Vertex AI Custom Training Job Container not finding my module: Error while finding module for '...'",
        "Question_body":"Hello,\n\nI have a PyTorch training job that I am packaging in a Python software distribution (.tar.gz file). I upload the sdist to a GCS bucket and run it in a container using the gcloud ai custom-jobs create CLI.\n\nUp until a couple of weeks ago this worked fine but in recent days my jobs consistently fail with messages like these appearing in their logs:\n\nRunning command: python3 -m MyPackage.MyModule --job-dir=gs:\/\/my-bucket\/my-job\/model --model-name=my-model\n\n\/opt\/conda\/bin\/python3: Error while finding module specification for 'MyPackage.MyModule' (ModuleNotFoundError: No module named 'MyPackage.MyModule')\n\n\u00a0\n\nMyPackage.MyModule is my module where my training code runs, naturally.\n\nAs I've mentioned above the same procedure worked until recently. There have not been any changes to it and I can clearly see that MyModule.py is located under MyPackage in my .tar.gz file.\n\nThe container image that I am using is us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-9:latest and from what I can tell it has not changed since the time I successfully used it before.\n\nWhy is the Vertex AI container not finding my training module? How can I further debug and fix this?",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1650854880000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":449.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Custom-Training-Job-Container-not-finding-my-module\/td-p\/416620\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"custom train job contain find modul error find modul hello pytorch train job packag python softwar distribut tar file upload sdist gc bucket run contain gcloud custom job creat cli coupl week ago work fine recent dai job consist fail messag like appear log run command python mypackag mymodul job dir bucket job model model model opt conda bin python error find modul specif mypackag mymodul modulenotfounderror modul name mypackag mymodul mypackag mymodul modul train code run natur mention procedur work recent chang clearli mymodul locat mypackag tar file contain imag docker pkg dev vertex train pytorch gpu latest tell chang time successfulli contain find train modul debug fix",
        "Question_preprocessed_content":"custom train job contain find modul find modul pytorch train job packag python softwar distribut upload sdist gc bucket run contain gcloud creat cli coupl ago work fine recent dai job consist fail like log python find modul specif modul train code run mention procedur work recent chang clearli locat mypackag file contain imag chang time contain find train modul debug fix",
        "Question_gpt_summary_original":"The user is facing an issue with the Vertex AI Custom Training Job Container as it is unable to find the module for their PyTorch training job, resulting in a \"ModuleNotFoundError\". The user has packaged their training job in a Python software distribution file and uploaded it to a GCS bucket. The same procedure worked previously, and there have been no changes to it. The user is using the us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-9:latest container image and is seeking help to debug and fix the issue.",
        "Question_gpt_summary":"user face issu custom train job contain unabl modul pytorch train job result modulenotfounderror user packag train job python softwar distribut file upload gc bucket procedur work previous chang user docker pkg dev vertex train pytorch gpu latest contain imag seek help debug fix issu"
    },
    {
        "Question_title":"Can't create dataset from csv file - error: 'Delimiter' is not specified or invalid.",
        "Question_body":"<p>I have got a problem. I can't add my csv files to datasets. Every time I have got error:  <\/p>\n<p>&quot;message&quot;: &quot;'Delimiter' is not specified or invalid.&quot;  <\/p>\n<p>Is there anyway to specified this while uploading csv file?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_created_time":1603103147810,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/131015\/cant-create-dataset-from-csv-file-error-delimiter",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat dataset csv file error delimit specifi invalid got problem add csv file dataset time got error messag delimit specifi invalid specifi upload csv file",
        "Question_preprocessed_content":"creat dataset csv file delimit specifi invalid got problem csv file dataset time got delimit specifi specifi upload csv file",
        "Question_gpt_summary_original":"The user is unable to create a dataset from a CSV file due to an error message stating that the delimiter is not specified or invalid. They are seeking a solution to specify the delimiter while uploading the CSV file.",
        "Question_gpt_summary":"user unabl creat dataset csv file error messag state delimit specifi invalid seek solut specifi delimit upload csv file"
    },
    {
        "Question_title":"Logistic regression in sagemaker",
        "Question_body":"<p>I am using the aws sagemaker for logistic regression. For validating the model on test data, the following code is used <\/p>\n\n<pre><code>runtime= boto3.client('runtime.sagemaker')\n\npayload = np2csv(test_X)\nresponse = runtime.invoke_endpoint(EndpointName=linear_endpoint,\n                                   ContentType='text\/csv',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\ntest_pred = np.array([r['score'] for r in result['predictions']])\n<\/code><\/pre>\n\n<p>The result contains the prediction values and the probability scores. \nI want to know how I can run a prediction model to predict the outcome based on two specific features. Eg. I have 30 features in the model and have trained model using those features. Now for my prediction, I want to know the outcome when feature1='x' and feature2='y'. But when I filter the data to those columns and pass that in the same code, I get the following error.<\/p>\n\n<pre><code>Customer Error: The feature dimension of the input: 4 does not match the feature dimension of the model: 30. Please fix the input and try again.\n<\/code><\/pre>\n\n<p>What is the equivalent of say glm.predict('feature1','feature2')in R in AWS Sagemaker implementation?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1533920085857,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":1557.0,
        "Poster_created_time":1490822961492,
        "Poster_location":null,
        "Poster_reputation_count":960.0,
        "Poster_view_count":122.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51790953",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"logist regress logist regress valid model test data follow code runtim boto client runtim payload npcsv test respons runtim invok endpoint endpointnam linear endpoint contenttyp text csv bodi payload result json load respons bodi read decod test pred arrai score result predict result contain predict valu probabl score want know run predict model predict outcom base specif featur featur model train model featur predict want know outcom featur featur filter data column pass code follow error custom error featur dimens input match featur dimens model fix input try equival glm predict featur featur implement",
        "Question_preprocessed_content":"logist logist valid model test data code result contain predict valu probabl score want know run predict model predict outcom base specif featur featur model train model featur predict want know outcom featur featur filter data column code equival implement",
        "Question_gpt_summary_original":"The user is facing an issue while trying to predict the outcome based on two specific features using logistic regression in AWS Sagemaker. They have trained the model using 30 features and want to know the outcome when feature1='x' and feature2='y'. However, when they filter the data to those columns and pass it in the code, they get an error stating that the feature dimension of the input does not match the feature dimension of the model. The user is seeking guidance on how to predict the outcome based on specific features in AWS Sagemaker.",
        "Question_gpt_summary":"user face issu try predict outcom base specif featur logist regress train model featur want know outcom featur featur filter data column pass code error state featur dimens input match featur dimens model user seek guidanc predict outcom base specif featur"
    },
    {
        "Question_title":"Using augmented manifest files for Tensorflow model evaluation in Sagemaker Pipelines",
        "Question_body":"<p>The <a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html\" rel=\"nofollow noreferrer\">Sagemaker Pipelines Guide<\/a> recommends that model evaluation be done in a ProcessingStep.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PkqGW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PkqGW.png\" alt=\"SageMaker Pipelines Guide Processing Step\" \/><\/a><\/p>\n<p>The Training step allows fast and convenient input with use of <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/augmented-manifest.html\" rel=\"nofollow noreferrer\">augmented manifest files<\/a> passed into a <code>TrainingInput<\/code> which can be used in conjunction with the Tensorflow <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-extensions\" rel=\"nofollow noreferrer\">PipeModeDataset<\/a>.<\/p>\n<p>However, a Processing step (and corresponding ProcessingInput) does not seem to support using an augmented manifest file. This means that a processing step cannot easily do a model evaluation.<\/p>\n<p>One workaround could be to use a TrainingStep for evaluation instead, however that brings along its own problems.<\/p>\n<p>Has anyone used augmented manifest files in a Tensorflow Sagemaker pipeline processing job? How do you handle the evaluation?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1633667032467,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1633676397440,
        "Question_score_count":0.0,
        "Question_view_count":84.0,
        "Poster_created_time":1462020664647,
        "Poster_location":null,
        "Poster_reputation_count":1053.0,
        "Poster_view_count":58.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69490472",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"augment manifest file tensorflow model evalu pipelin pipelin guid recommend model evalu processingstep train step allow fast conveni input us augment manifest file pass traininginput conjunct tensorflow pipemodedataset process step correspond processinginput support augment manifest file mean process step easili model evalu workaround us trainingstep evalu instead bring problem augment manifest file tensorflow pipelin process job handl evalu",
        "Question_preprocessed_content":"augment manifest file tensorflow model evalu pipelin pipelin guid model evalu train step fast conveni input us augment manifest file conjunct tensorflow pipemodedataset step augment manifest file mean step easili model evalu workaround us trainingstep evalu instead bring problem augment manifest file tensorflow pipelin job handl evalu",
        "Question_gpt_summary_original":"The user is facing a challenge in using augmented manifest files for model evaluation in Sagemaker Pipelines. While the Training step allows the use of augmented manifest files, the Processing step does not support it, making model evaluation difficult. The user is seeking advice on how to handle evaluation in this scenario.",
        "Question_gpt_summary":"user face challeng augment manifest file model evalu pipelin train step allow us augment manifest file process step support make model evalu difficult user seek advic handl evalu scenario"
    },
    {
        "Question_title":"Auto-resume for deep learning training is not working",
        "Question_body":"From slack\n\nI'm trying to run a training job and make it resume automatically whenever it is preempted or it encounters an issue.\nI'm using for this the \"termination\" and \"maxRetries\" field to restart the job.\nAfter a problem happens, the job is restarted automatically starting from where the problem has happened if I look at the logs. However, nothing is being saved to the artifacts and any call to tracking.log_metric doesn't seem to have an effect. If I look at the logs, the job then continues until it reaches the end. However instead of just ending, it just keeps restarting (from the point where the problem occurred) until all the \"maxRetries\" are used and fails with the warning \"Underlying job has an issue\" at the status page.\nAny idea what could cause such a problem and if there is anything I could do to avoid it?",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1649329501000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Polyaxon provides several strategies to restart, restrat with copy mode, and resumes jobs. The auto-resume behavior is enabled by default\n\nNote that resuming a job can only work if your code supports loading the last checkpoint.\n\nHere's a quick debugging logic to check that the resuming process works as expected:\n\nmain.py\ndef main():\n    tracking.init()\n    checkpoint_path = tracking.get_outputs_path(\"checkpoint.json\")\n    checkpoint_path_exists = os.path.exists(checkpoint_path)\n    print(\"[CHECKPOINT] path found: {}\".format(checkpoint_path_exists))\n    if checkpoint_path_exists:\n        with open(checkpoint_path, \"r\") as checkpoint_file:\n            checkpoint = json.loads(checkpoint_file.read())\n            print(\"[CHECKPOINT] last content: {}\".format(checkpoint))\n    else:\n      print(\"[CHECKPOINT] init ...\")\n      checkpoint = {\n        \"last_time\": time.time(),\n        \"last_index\": 0,\n        \"array\": [],\n      }\n    for i in range(checkpoint[\"last_index\"] + 1, 300):\n      print(\"[CHECKPOINT] step {}\".format(i))\n      tracking.log_progress((i + 1)\/300)\n      tracking.log_metric(name=\"index\", value=i, step=i)\n      checkpoint[\"array\"].append(i)\n      checkpoint[\"last_index\"] = i\n      checkpoint[\"last_time\"] = time.time()\n      if i in [10, 50]:\n        print(\"[CHECKPOINT] Saving last content ...\")\n        with open(checkpoint_path, \"w\") as checkpoint_file:\n          checkpoint_file.write(json.dumps(checkpoint))\n        raise ValueError(\"Error was raised at {}\".format(i))\n      time.sleep(1)\npolyaxonfile.yaml\nversion: 1.1\nkind: component\ntermination:\n  maxRetries: 3\nrun:\n  kind: job\n  container:\n    image: polyaxon\/polyaxon-examples:artifacts\n    workingDir: \"{{ globals.run_artifacts_path }}\/uploads\"\n    command: [\"\/bin\/bash\", -c]\n    args: [\"pip install -U polyaxon --no-cache && python3 main.py\"]\nLogged a dummy metric that resumes from last checkpoint and (apart from the warning regression that I mentioned) the job succeeds after after two failures (you can see the first chart where the x-axis is the time that there's gap time)",
        "Answer_comment_count":0.0,
        "Question_closed_time":1649329911000,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1474",
        "Tool":"Polyaxon",
        "Question_self_resolution":1.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"auto resum deep learn train work slack try run train job resum automat preempt encount issu termin maxretri field restart job problem happen job restart automat start problem happen look log save artifact track log metric effect look log job continu reach end instead end keep restart point problem occur maxretri fail warn underli job issu statu page idea caus problem avoid",
        "Question_preprocessed_content":"learn train work slack try run train job resum encount termin maxretri field restart job problem job restart start problem log save artifact log job continu reach end instead end restart maxretri fail warn underli job statu page idea caus problem avoid",
        "Question_gpt_summary_original":"The user is facing an issue with auto-resume for deep learning training. The job is restarting automatically after encountering an issue, but nothing is being saved to the artifacts and tracking.log_metric is not working. The job keeps restarting until all the \"maxRetries\" are used and fails with the warning \"Underlying job has an issue\" at the status page. The user is seeking advice on how to avoid this problem.",
        "Question_gpt_summary":"user face issu auto resum deep learn train job restart automat encount issu save artifact track log metric work job keep restart maxretri fail warn underli job issu statu page user seek advic avoid problem"
    },
    {
        "Question_title":"Separating Training From Testing",
        "Question_body":"<p>Hi!<\/p>\n<p>I\u2019ve got a sort of lopsided ML workflow where most of my time is spent producing plots and metrics, and my set of trained models is rarely updated. Is there a good way to separate my training code from all of my evaluation scripts?<\/p>\n<p>I\u2019d like my experiments to have the logging information from when the model was trained, but not to have to retrain it every time.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1630322356926,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":321.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/separating-training-from-testing\/353",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"separ train test iv got sort lopsid workflow time spent produc plot metric set train model rare updat good wai separ train code evalu script like experi log inform model train retrain time thank",
        "Question_preprocessed_content":"separ train test iv got sort lopsid workflow time spent produc plot metric set train model rare updat wai separ train code evalu script like experi inform model train retrain time thank",
        "Question_gpt_summary_original":"The user is facing a challenge in their ML workflow where they spend most of their time producing plots and metrics, and their set of trained models is rarely updated. They are looking for a way to separate their training code from evaluation scripts to avoid retraining the model every time while still having access to the logging information from the training phase.",
        "Question_gpt_summary":"user face challeng workflow spend time produc plot metric set train model rare updat look wai separ train code evalu script avoid retrain model time have access log inform train phase"
    },
    {
        "Question_title":"Can not pull public ECR image using AWS SageMaker Estimator",
        "Question_body":"<p>I would like to pass a public Docker Image on ECR through the image_uri parameter to the Estimator. However, the training job fails to find this image.\nI'm running my code using AWS SageMaker Notebook instance.<\/p>\n<pre><code>from sagemaker.estimator import Estimator\n\nbyoc_image_uri = 'public.ecr.aws\/v8x6t9d9\/train-image-classifier-firefly'\n\nestimator = Estimator(image_uri=byoc_image_uri,\n                      role=role,\n                      base_job_name='tf-custom-container-test-job',\n                      instance_count=1,\n                      hyperparameters=hyperparameters,\n                      output_path=output_path,\n                      instance_type='ml.p2.xlarge')\n<\/code><\/pre>\n<p>What I have tried:<\/p>\n<ul>\n<li>I first tried to use the same image on an ECR privet repo. This works<\/li>\n<li>Then pushed the same image to a public ECR repo that I had created. The image was successfully pushed and I could pull this image successfully.<\/li>\n<li>However, when I tried to use the same image url through Estimator image_uri parameter. I get the following error message<\/li>\n<\/ul>\n<pre><code>    ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: TrainingImageConfig with TrainingRepositoryAccessMode set to VPC must be provided when using a training image from a private Docker registry. Please provideTrainingImageConfig and TrainingRepositoryAccessMode set to VPC when using a training image from a private Docker registry.\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1631756389253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1632521317307,
        "Question_score_count":1.0,
        "Question_view_count":396.0,
        "Poster_created_time":1631755332636,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69201461",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pull public ecr imag estim like pass public docker imag ecr imag uri paramet estim train job fail imag run code notebook instanc estim import estim byoc imag uri public ecr aw vxtd train imag classifi firefli estim estim imag uri byoc imag uri role role base job custom contain test job instanc count hyperparamet hyperparamet output path output path instanc type xlarg tri tri us imag ecr privet repo work push imag public ecr repo creat imag successfulli push pull imag successfulli tri us imag url estim imag uri paramet follow error messag clienterror error occur validationexcept call createtrainingjob oper trainingimageconfig trainingrepositoryaccessmod set vpc provid train imag privat docker registri providetrainingimageconfig trainingrepositoryaccessmod set vpc train imag privat docker registri",
        "Question_preprocessed_content":"public ecr imag estim like public docker imag ecr paramet estim train job fail imag code instanc tri tri us imag ecr privet repo work push imag public ecr repo creat imag push imag tri us imag url estim paramet",
        "Question_gpt_summary_original":"The user is unable to pull a public Docker image from ECR using AWS SageMaker Estimator. The training job fails to find the image, and the user receives an error message stating that TrainingImageConfig with TrainingRepositoryAccessMode set to VPC must be provided when using a training image from a private Docker registry. The user has tried using the same image on a private ECR repo, which works, but encounters issues when using a public ECR repo.",
        "Question_gpt_summary":"user unabl pull public docker imag ecr estim train job fail imag user receiv error messag state trainingimageconfig trainingrepositoryaccessmod set vpc provid train imag privat docker registri user tri imag privat ecr repo work encount issu public ecr repo"
    },
    {
        "Question_title":"Scalars not getting saved",
        "Question_body":"<p>I\u2019m new to guild.ai and trying to use the guild.yml files to save all relevant information including scalars. The scalars are however not being saved.<\/p>\n<p>This is my code:<\/p>\n<pre><code>boston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nnamedf = ['X_train', 'X_test']\nnamenp = ['y_train', 'y_test']\n\nreg = Ridge(alpha=0.9)\nfitted = reg.fit(X, y)\n\nprint(\"score: %f\" % fitted.score(X, y))\n<\/code><\/pre>\n<p>and this is my guild.yml file:<\/p>\n<pre><code>ridge-regression:\n  description: fit ridge regression using boston data.\n  notebook: Checklist.ipynb\n  flags:\n    alpha:\n      description: alpha value in ridge regression\n      nb-replace: 'alpha=(\\d+)'\n  output-scalers:\n    score: 'score: (\\value)'\n<\/code><\/pre>\n<p>When I print out all information on the command line there are no scalars to be seen. Does anyone know why?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_created_time":1611248492978,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":411.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/scalars-not-getting-saved\/527",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"scalar get save new try us yml file save relev inform includ scalar scalar save code boston load boston datafram boston data column boston featur name boston target train test train test train test split test size namedf train test namenp train test reg ridg alpha fit reg fit print score fit score yml file ridg regress descript fit ridg regress boston data notebook checklist ipynb flag alpha descript alpha valu ridg regress replac alpha output scaler score score valu print inform command line scalar seen know",
        "Question_preprocessed_content":"scalar save new try us yml file save relev inform includ scalar scalar save code yml file print inform line scalar know",
        "Question_gpt_summary_original":"The user is having trouble saving scalars using guild.yml files in guild.ai. The scalars are not being saved despite the user's efforts. The user has shared their code and guild.yml file, but there are no scalars visible when printing out all information on the command line.",
        "Question_gpt_summary":"user have troubl save scalar yml file scalar save despit user effort user share code yml file scalar visibl print inform command line"
    },
    {
        "Question_title":"Google vertex AI support is terrible",
        "Question_body":"I usually don't post things like this but I have been trying to work with Google support for Vertex AI for a while. It has been a month on a P2 ticket and no help or support so far. I guess I don't pay the big bucks to get Google's attention. So just wanted to post a warning that is helpful hopefully. Use Vertex AI at your own risk. If something fails you are on your own.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1669150740000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":83.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-vertex-AI-support-is-terrible\/td-p\/492001\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"support terribl usual post thing like try work googl support month ticket help support far guess pai big buck googl attent want post warn help hopefulli us risk fail",
        "Question_preprocessed_content":" post thing like try work month ticket help far pai big buck want post warn help us risk fail",
        "Question_gpt_summary_original":"The user has been trying to get support from Google for Vertex AI for a month but has not received any help or support. The user warns others to use Vertex AI at their own risk as there is no guarantee of support if something fails.",
        "Question_gpt_summary":"user try support googl month receiv help support user warn us risk guarante support fail"
    },
    {
        "Question_title":"Passing arguments based on search parameters in sweep",
        "Question_body":"<p>I\u2019m running a grid-search sweep with a custom command of this sort:<\/p>\n<pre><code class=\"lang-auto\">project: my_project\nprogram: main.py\nname: grid_search\nmethod: grid\nmetric:\n  goal: maximize\n  name: eval_accuracy\nparameters:\n  learning_rate:\n    values: [1e-5, 5e-5, 1e-4]\n  batch_size:\n    values: [4, 8, 16]\ncommand:\n  - ${env}\n  - ${interpreter}\n  - ${program}\n  - \"--run_name\"\n  - \"${batch_size}_${learning_rate}\"\n  - ${args}\n<\/code><\/pre>\n<p>Note that when passing the <code>--run_name<\/code> argument I would like to condition it on the values of the search parameters <code>batch_size<\/code> &amp; <code>learning_rate<\/code>. I do not want to do this inside my code because the format might change between sweeps and I want my code to be generic.<\/p>\n<p>Is there a way to use the search parameters in other arguments? I tried using <code>${args_no_hyphens}<\/code> before my command so the variables would be defined but it didn\u2019t seem to work.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1649753616910,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":194.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/passing-arguments-based-on-search-parameters-in-sweep\/2221",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pass argument base search paramet sweep run grid search sweep custom command sort project project program main grid search method grid metric goal maxim eval accuraci paramet learn rate valu batch size valu command env interpret program run batch size learn rate arg note pass run argument like condit valu search paramet batch size learn rate want insid code format chang sweep want code gener wai us search paramet argument tri arg hyphen command variabl defin didnt work",
        "Question_preprocessed_content":"argument base search paramet custom sort note argument like condit valu search paramet want insid code format chang want code gener wai us search paramet argument tri variabl defin didnt work",
        "Question_gpt_summary_original":"The user is facing a challenge while passing arguments based on search parameters in a grid-search sweep with a custom command. They want to condition the <code>--run_name<\/code> argument on the values of the search parameters <code>batch_size<\/code> &amp; <code>learning_rate<\/code>, but they do not want to do this inside their code because the format might change between sweeps and they want their code to be generic. The user is looking for a way to use the search parameters in other arguments and has tried using <code>${args_no_hyphens}<\/code> before their command, but it did not work.",
        "Question_gpt_summary":"user face challeng pass argument base search paramet grid search sweep custom command want condit run argument valu search paramet batch size learn rate want insid code format chang sweep want code gener user look wai us search paramet argument tri arg hyphen command work"
    },
    {
        "Question_title":"Import problem for Facebook Prophet on AWS Sagemaker",
        "Question_body":"<p>I am trying to use Facebook Prophet on an AWS Sagemaker Jupyter notebook. I've tried installing fbprophet in two ways:<\/p>\n\n<p><code>!{sys.executable} -m pip install fbprophet<\/code><\/p>\n\n<p>and<\/p>\n\n<pre><code>!conda install -c conda-forge fbprophet --yes\n<\/code><\/pre>\n\n<p>(that last one comes from several answers I saw on other forums)<\/p>\n\n<p>However, none of them seem to work. In particular, the latest one seems to work for the installation, but the subsequent import of fbprophet results in an error, that seems related to matplotlib:<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-8-d9f3d4c04a60&gt; in &lt;module&gt;()\n      1 # Imports\n----&gt; 2 from fbprophet import Prophet\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/fbprophet\/__init__.py in &lt;module&gt;()\n      6 # of patent rights can be found in the PATENTS file in the same directory.\n      7 \n----&gt; 8 from fbprophet.forecaster import Prophet\n      9 \n     10 __version__ = '0.6'\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/fbprophet\/forecaster.py in &lt;module&gt;()\n     17 from fbprophet.make_holidays import get_holiday_names, make_holidays_df\n     18 from fbprophet.models import StanBackendEnum\n---&gt; 19 from fbprophet.plot import (plot, plot_components)\n     20 \n     21 logger = logging.getLogger('fbprophet')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/fbprophet\/plot.py in &lt;module&gt;()\n     19 \n     20 try:\n---&gt; 21     from matplotlib import pyplot as plt\n     22     from matplotlib.dates import (\n     23         MonthLocator,\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/pyplot.py in &lt;module&gt;()\n     30 from cycler import cycler\n     31 import matplotlib\n---&gt; 32 import matplotlib.colorbar\n     33 import matplotlib.image\n     34 from matplotlib import rcsetup, style\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/colorbar.py in &lt;module&gt;()\n     25 \n     26 import matplotlib as mpl\n---&gt; 27 import matplotlib.artist as martist\n     28 import matplotlib.cbook as cbook\n     29 import matplotlib.collections as collections\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/artist.py in &lt;module&gt;()\n     55 \n     56 \n---&gt; 57 class Artist(object):\n     58     \"\"\"\n     59     Abstract base class for objects that render into a FigureCanvas.\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/artist.py in Artist()\n     62     \"\"\"\n     63     @cbook.deprecated(\"3.1\")\n---&gt; 64     @property\n     65     def aname(self):\n     66         return 'Artist'\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/matplotlib\/cbook\/deprecation.py in deprecate(obj, message, name, alternative, pending, addendum)\n    180                 pass\n    181     \"\"\"\n--&gt; 182 \n    183     def deprecate(obj, message=message, name=name, alternative=alternative,\n    184                   pending=pending, obj_type=obj_type, addendum=addendum):\n\nAttributeError: 'property' object has no attribute '__name__'\n<\/code><\/pre>\n\n<p>Has anyone else run into this issue \/ managed to get fbprophet working on sagemaker? I'm thinking this might be an issue of versions of fbprophet and matplotlib, but couldn't find information on which ones to use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1584094777637,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":3052.0,
        "Poster_created_time":1369829834347,
        "Poster_location":null,
        "Poster_reputation_count":13.0,
        "Poster_view_count":2.0,
        "Answer_body":"<p>The following works for me  on a fresh notebook instance. I'm using the python3 environment.<\/p>\n\n<pre><code>sh-4.2$ source activate python3\n(python3) sh-4.2$ conda install -c conda-forge fbprophet --yes\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 4.5.12\n  latest version: 4.8.2\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\n\n\n## Package Plan ##\n\n  environment location: \/home\/ec2-user\/anaconda3\/envs\/python3\n\n  added \/ updated specs:\n    - fbprophet\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    pykerberos-1.2.1           |   py36h505690d_0          26 KB  conda-forge\n    python-3.6.7               |    h381d211_1004        34.5 MB  conda-forge\n    cryptography-2.8           |   py36h45558ae_2         628 KB  conda-forge\n    python_abi-3.6             |          1_cp36m           4 KB  conda-forge\n    binutils_impl_linux-64-2.33.1|       h53a641e_8         9.1 MB  conda-forge\n    certifi-2019.11.28         |   py36h9f0ad1d_1         149 KB  conda-forge\n    gcc_linux-64-7.3.0         |      h553295d_17          21 KB  conda-forge\n    gxx_linux-64-7.3.0         |      h553295d_17          21 KB  conda-forge\n    fbprophet-0.6              |   py36he1b5a44_0         642 KB  conda-forge\n    holidays-0.10.1            |             py_0          56 KB  conda-forge\n    tk-8.6.10                  |       hed695b0_0         3.2 MB  conda-forge\n    lunarcalendar-0.0.9        |             py_0          20 KB  conda-forge\n    convertdate-2.1.3          |          py_1000          30 KB  conda-forge\n    curl-7.68.0                |       hf8cf82a_0         137 KB  conda-forge\n    krb5-1.16.4                |       h2fd8d38_0         1.4 MB  conda-forge\n    libcurl-7.68.0             |       hda55be3_0         564 KB  conda-forge\n    expat-2.2.9                |       he1b5a44_2         191 KB  conda-forge\n    binutils_linux-64-2.33.1   |      h9595d00_17          21 KB  conda-forge\n    python-dateutil-2.8.0      |             py_0         219 KB  conda-forge\n    matplotlib-base-3.1.0      |   py36h5f35d83_0         6.7 MB  conda-forge\n    pycurl-7.43.0.5            |   py36h16ce93b_0          69 KB  conda-forge\n    ld_impl_linux-64-2.33.1    |       h53a641e_8         589 KB  conda-forge\n    pystan-2.17.1.0            |py36hf2d7682_1004        14.0 MB  conda-forge\n    ephem-3.7.7.1              |   py36h516909a_0         722 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        72.9 MB\n\nThe following NEW packages will be INSTALLED:\n\n    binutils_impl_linux-64: 2.33.1-h53a641e_8          conda-forge\n    binutils_linux-64:      2.33.1-h9595d00_17         conda-forge\n    convertdate:            2.1.3-py_1000              conda-forge\n    ephem:                  3.7.7.1-py36h516909a_0     conda-forge\n    fbprophet:              0.6-py36he1b5a44_0         conda-forge\n    gcc_impl_linux-64:      7.3.0-habb00fd_1\n    gcc_linux-64:           7.3.0-h553295d_17          conda-forge\n    gettext:                0.19.8.1-hc5be6a0_1002     conda-forge\n    gxx_impl_linux-64:      7.3.0-hdf63c60_1\n    gxx_linux-64:           7.3.0-h553295d_17          conda-forge\n    holidays:               0.10.1-py_0                conda-forge\n    ld_impl_linux-64:       2.33.1-h53a641e_8          conda-forge\n    lunarcalendar:          0.0.9-py_0                 conda-forge\n    matplotlib-base:        3.1.0-py36h5f35d83_0       conda-forge\n    pystan:                 2.17.1.0-py36hf2d7682_1004 conda-forge\n    python_abi:             3.6-1_cp36m                conda-forge\n\nThe following packages will be UPDATED:\n\n    ca-certificates:        2019.10.16-0                           --&gt; 2019.11.28-hecc5488_0     conda-forge\n    certifi:                2019.9.11-py36_0                       --&gt; 2019.11.28-py36h9f0ad1d_1 conda-forge\n    cryptography:           2.2.2-py36h14c3975_0                   --&gt; 2.8-py36h45558ae_2        conda-forge\n    curl:                   7.60.0-h84994c4_0                      --&gt; 7.68.0-hf8cf82a_0         conda-forge\n    expat:                  2.2.5-he0dffb1_0                       --&gt; 2.2.9-he1b5a44_2          conda-forge\n    glib:                   2.56.1-h000015b_0                      --&gt; 2.58.3-h6f030ca_1002      conda-forge\n    krb5:                   1.14.2-hcdc1b81_6                      --&gt; 1.16.4-h2fd8d38_0         conda-forge\n    libcurl:                7.60.0-h1ad7b7a_0                      --&gt; 7.68.0-hda55be3_0         conda-forge\n    libpng:                 1.6.34-hb9fc6fc_0                      --&gt; 1.6.37-hed695b0_0         conda-forge\n    libssh2:                1.8.0-h9cfc8f7_4                       --&gt; 1.8.2-h22169c7_2          conda-forge\n    openssl:                1.0.2t-h7b6447c_1                      --&gt; 1.1.1d-h516909a_0         conda-forge\n    pycurl:                 7.43.0.1-py36hb7f436b_0                --&gt; 7.43.0.5-py36h16ce93b_0   conda-forge\n    pykerberos:             1.2.1-py36h14c3975_0                   --&gt; 1.2.1-py36h505690d_0      conda-forge\n    python:                 3.6.5-hc3d631a_2                       --&gt; 3.6.7-h381d211_1004       conda-forge\n    python-dateutil:        2.7.3-py36_0                           --&gt; 2.8.0-py_0                conda-forge\n    qt:                     5.9.6-h52aff34_0                       --&gt; 5.9.7-h5867ecd_1\n    sqlite:                 3.23.1-he433501_0                      --&gt; 3.28.0-h8b20d00_0         conda-forge\n    tk:                     8.6.7-hc745277_3                       --&gt; 8.6.10-hed695b0_0         conda-forge\n\n\nDownloading and Extracting Packages\npykerberos-1.2.1     | 26 KB     | ################################################################################ | 100%\npython-3.6.7         | 34.5 MB   | ################################################################################ | 100%\ncryptography-2.8     | 628 KB    | ################################################################################ | 100%\npython_abi-3.6       | 4 KB      | ################################################################################ | 100%\nbinutils_impl_linux- | 9.1 MB    | ################################################################################ | 100%\ncertifi-2019.11.28   | 149 KB    | ################################################################################ | 100%\ngcc_linux-64-7.3.0   | 21 KB     | ################################################################################ | 100%\ngxx_linux-64-7.3.0   | 21 KB     | ################################################################################ | 100%\nfbprophet-0.6        | 642 KB    | ################################################################################ | 100%\nholidays-0.10.1      | 56 KB     | ################################################################################ | 100%\ntk-8.6.10            | 3.2 MB    | ################################################################################ | 100%\nlunarcalendar-0.0.9  | 20 KB     | ################################################################################ | 100%\nconvertdate-2.1.3    | 30 KB     | ################################################################################ | 100%\ncurl-7.68.0          | 137 KB    | ################################################################################ | 100%\nkrb5-1.16.4          | 1.4 MB    | ################################################################################ | 100%\nlibcurl-7.68.0       | 564 KB    | ################################################################################ | 100%\nexpat-2.2.9          | 191 KB    | ################################################################################ | 100%\nbinutils_linux-64-2. | 21 KB     | ################################################################################ | 100%\npython-dateutil-2.8. | 219 KB    | ################################################################################ | 100%\nmatplotlib-base-3.1. | 6.7 MB    | ################################################################################ | 100%\npycurl-7.43.0.5      | 69 KB     | ################################################################################ | 100%\nld_impl_linux-64-2.3 | 589 KB    | ################################################################################ | 100%\npystan-2.17.1.0      | 14.0 MB   | ################################################################################ | 100%\nephem-3.7.7.1        | 722 KB    | ################################################################################ | 100%\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n(python3) sh-4.2$\n(python3) sh-4.2$\n(python3) sh-4.2$\n(python3) sh-4.2$ python\nPython 3.6.7 | packaged by conda-forge | (default, Feb 28 2019, 09:07:38)\n[GCC 7.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import fbprophet\n&gt;&gt;&gt; \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1584358813960,
        "Answer_last_edit_time":null,
        "Answer_score_count":6.0,
        "Answerer_created_time":1426694564423,
        "Answerer_location":"Paris",
        "Answerer_reputation_count":2425.0,
        "Answerer_view_count":459.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60668635",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"import problem facebook prophet try us facebook prophet jupyt notebook tri instal fbprophet wai sy execut pip instal fbprophet conda instal conda forg fbprophet ye come answer saw forum work particular latest work instal subsequ import fbprophet result error relat matplotlib attributeerror traceback recent import fbprophet import prophet anaconda env python lib python site packag fbprophet init patent right patent file directori fbprophet forecast import prophet version anaconda env python lib python site packag fbprophet forecast fbprophet holidai import holidai name holidai fbprophet model import stanbackendenum fbprophet plot import plot plot compon logger log getlogg fbprophet anaconda env python lib python site packag fbprophet plot try matplotlib import pyplot plt matplotlib date import monthloc anaconda env python lib python site packag matplotlib pyplot cycler import cycler import matplotlib import matplotlib colorbar import matplotlib imag matplotlib import rcsetup style anaconda env python lib python site packag matplotlib colorbar import matplotlib mpl import matplotlib artist martist import matplotlib cbook cbook import matplotlib collect collect anaconda env python lib python site packag matplotlib artist class artist object abstract base class object render figurecanva anaconda env python lib python site packag matplotlib artist artist cbook deprec properti def anam self return artist anaconda env python lib python site packag matplotlib cbook deprec deprec obj messag altern pend addendum pass def deprec obj messag messag altern altern pend pend obj type obj type addendum addendum attributeerror properti object attribut run issu manag fbprophet work think issu version fbprophet matplotlib couldn inform on us",
        "Question_preprocessed_content":"import problem prophet try us prophet jupyt tri fbprophet wai come answer saw forum work particular latest work subsequ import fbprophet result relat matplotlib run manag fbprophet work think version fbprophet matplotlib couldn inform on us",
        "Question_gpt_summary_original":"The user is facing an issue while trying to install and import Facebook Prophet on an AWS Sagemaker Jupyter notebook. The user has tried two methods of installation, but none of them seem to work. The latest method seems to work for installation, but the subsequent import of fbprophet results in an error related to matplotlib. The user is seeking help to resolve the issue and is thinking that it might be an issue of versions of fbprophet and matplotlib.",
        "Question_gpt_summary":"user face issu try instal import facebook prophet jupyt notebook user tri method instal work latest method work instal subsequ import fbprophet result error relat matplotlib user seek help resolv issu think issu version fbprophet matplotlib"
    },
    {
        "Question_title":"Azure Machine Learning Monitoring",
        "Question_body":"<p>How can we setup monitor for Azure Machine Learning pipeline\/job failures ? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1656094570340,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/902953\/azure-machine-learning-monitoring",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"monitor setup monitor pipelin job failur",
        "Question_preprocessed_content":"monitor setup monitor failur",
        "Question_gpt_summary_original":"The user is seeking guidance on how to set up monitoring for Azure Machine Learning pipeline\/job failures.",
        "Question_gpt_summary":"user seek guidanc set monitor pipelin job failur"
    },
    {
        "Question_title":"Is it a good idea to store my dataset in my notebook instance in sagemaker?",
        "Question_body":"<p>I'm new to AWS and I am considering to use amazon sagemaker to train my deep learning model because I'm having memory issues due to the large dataset and neural network that I have to train. I'm confused whether to store my data in my notebook instance or in S3? If I store it in my s3 would I be able to access it to train on my notebook instance? I'm confused on the concepts. Can anyone explain the use of S3 in machine learning in AWS?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1619054238143,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":322.0,
        "Poster_created_time":1605938672327,
        "Poster_location":"Jakarta Selatan, South Jakarta City, Jakarta, Indonesia",
        "Poster_reputation_count":97.0,
        "Poster_view_count":34.0,
        "Answer_body":"<p>Yes you can use S3 as storage for your training datasets.<\/p>\n<p>Refer diagram in this link describing how everything works together: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html<\/a><\/p>\n<p>You may also want to checkout following blogs that details about File mode and Pipe mode, two mechanisms for transferring training data:<\/p>\n<ol>\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>In File mode, the training data is downloaded first to an encrypted EBS volume attached to the training instance prior to commencing the training. However, in Pipe mode the input data is streamed directly to the training algorithm while it is running.<\/p>\n<\/blockquote>\n<ol start=\"2\">\n<li><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/<\/a><\/li>\n<\/ol>\n<blockquote>\n<p>With Pipe input mode, your data is fed on-the-fly into the algorithm container without involving any disk I\/O. This approach shortens the lengthy download process and dramatically reduces startup time. It also offers generally better read throughput than File input mode. This is because your data is fetched from Amazon S3 by a highly optimized multi-threaded background process. It also allows you to train on datasets that are much larger than the 16 TB Amazon Elastic Block Store (EBS) volume size limit.<\/p>\n<\/blockquote>\n<p>The blog also contains python code snippets using Pipe input mode for reference.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1619062178832,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1336887489390,
        "Answerer_location":null,
        "Answerer_reputation_count":758.0,
        "Answerer_view_count":44.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67205469",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"good idea store dataset notebook instanc new aw consid us train deep learn model have memori issu larg dataset neural network train confus store data notebook instanc store abl access train notebook instanc confus concept explain us machin learn aw",
        "Question_preprocessed_content":"idea store dataset instanc new aw consid us train learn model have memori larg dataset neural network train confus store data instanc store abl train instanc confus concept explain us machin learn aw",
        "Question_gpt_summary_original":"The user is new to AWS and is considering using Amazon SageMaker to train a deep learning model due to memory issues caused by a large dataset and neural network. They are unsure whether to store their data in their notebook instance or in S3 and are seeking clarification on the use of S3 in machine learning in AWS.",
        "Question_gpt_summary":"user new aw consid train deep learn model memori issu caus larg dataset neural network unsur store data notebook instanc seek clarif us machin learn aw"
    },
    {
        "Question_title":"Timeout error",
        "Question_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>,<\/p>\n<p>I can\u2019t figure out why sometimes I get timeout error when I run experiments. Here is my running command:<\/p>\n<pre><code>guild run train tb_volatility_lookback=range[30:300:10]\n<\/code><\/pre>\n<p>it hangs after some time and when I click ctr + c to abort it the below error shows up.<\/p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\runpy.py\u201d, line 193, in _run_module_as_main<br>\nINFO: [numexpr.utils] NumExpr defaulting to 8 threads.<br>\n\u201c<strong>main<\/strong>\u201d, mod_spec)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\runpy.py\u201d, line 85, in _run_code<br>\nexec(code, run_globals)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_main.py\u201d, line 38, in <br>\nmain()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_main.py\u201d, line 26, in main<br>\nbatch_util.handle_trials(batch_run, trials)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_util.py\u201d, line 54, in handle_trials<br>\n_run_trials(batch_run, trials)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_util.py\u201d, line 79, in _run_trials<br>\n_start_trial_run(run, stage)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\batch_util.py\u201d, line 117, in _start_trial_run<br>\nrun_impl.run(restart=run.id, stage=stage)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1940, in run<br>\nmain(args)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1017, in main<br>\n_dispatch_op(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1101, in _dispatch_op<br>\n_dispatch_op_cmd(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1286, in _dispatch_op_cmd<br>\n_confirm_and_run(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1354, in _confirm_and_run<br>\n_run(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1544, in _run<br>\n_run_local(S)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1575, in _run_local<br>\n_run_op(op, S.args)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\commands\\run_impl.py\u201d, line 1683, in _run_op<br>\nextra_env=extra_env,<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 160, in run<br>\nexit_status = _run(run, op, quiet, stop_after, extra_env)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 195, in _run<br>\nexit_status = _op_wait_for_proc(op, proc, run, quiet, stop_after)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 230, in _op_wait_for_proc<br>\nreturn _op_watch_proc(op, proc, run, quiet, stop_after)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 238, in _op_watch_proc<br>\nreturn _proc_wait(proc, stop_after)<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op.py\u201d, line 259, in <strong>exit<\/strong><br>\nself._output.wait_and_close()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op_util_legacy.py\u201d, line 254, in wait_and_close<br>\nself.close()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op_util_legacy.py\u201d, line 219, in close<br>\nlock = self._acquire_output_lock()<br>\nFile \u201cC:\\ProgramData\\Anaconda3\\lib\\site-packages\\guild\\op_util_legacy.py\u201d, line 232, in _acquire_output_lock<br>\nraise RuntimeError(\u201ctimeout\u201d)<br>\nRuntimeError: timeout<\/p>\n<\/blockquote>",
        "Question_answer_count":9,
        "Question_comment_count":null,
        "Question_created_time":1592656643969,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":796.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/timeout-error\/196",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"timeout error garrett figur timeout error run experi run command run train volatil lookback rang hang time click ctr abort error show traceback recent file programdata anaconda lib runpi line run modul main info numexpr util numexpr default thread main mod spec file programdata anaconda lib runpi line run code exec code run global file programdata anaconda lib site packag batch main line main file programdata anaconda lib site packag batch main line main batch util handl trial batch run trial file programdata anaconda lib site packag batch util line handl trial run trial batch run trial file programdata anaconda lib site packag batch util line run trial start trial run run stage file programdata anaconda lib site packag batch util line start trial run run impl run restart run stage stage file programdata anaconda lib site packag command run impl line run main arg file programdata anaconda lib site packag command run impl line main dispatch file programdata anaconda lib site packag command run impl line dispatch dispatch cmd file programdata anaconda lib site packag command run impl line dispatch cmd confirm run file programdata anaconda lib site packag command run impl line confirm run run file programdata anaconda lib site packag command run impl line run run local file programdata anaconda lib site packag command run impl line run local run arg file programdata anaconda lib site packag command run impl line run extra env extra env file programdata anaconda lib site packag line run exit statu run run quiet stop extra env file programdata anaconda lib site packag line run exit statu wait proc proc run quiet stop file programdata anaconda lib site packag line wait proc return watch proc proc run quiet stop file programdata anaconda lib site packag line watch proc return proc wait proc stop file programdata anaconda lib site packag line exit self output wait close file programdata anaconda lib site packag util legaci line wait close self close file programdata anaconda lib site packag util legaci line close lock self acquir output lock file programdata anaconda lib site packag util legaci line acquir output lock rais runtimeerror timeout runtimeerror timeout",
        "Question_preprocessed_content":"timeout figur timeout run experi hang time click ctr abort show traceback file line info numexpr default thread main file line exec file line main file line main trial file line trial file line stage file line stage stage file line run main file line main file line file line file line file line file line file line file line run quiet file line proc run quiet file line return proc run quiet file line return file line exit file line file line close lock file line rais timeout",
        "Question_gpt_summary_original":"The user is encountering a timeout error while running experiments using the given command, which causes the program to hang and eventually display a runtime error.",
        "Question_gpt_summary":"user encount timeout error run experi given command caus program hang eventu displai runtim error"
    },
    {
        "Question_title":"Use an existing Gateway with Azure Machine Learning?",
        "Question_body":"<p>We want to access an onprem SQL database with an existing Gateway, is that possible in AML?  The tool only seems to allow creating new gateways.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1482505382510,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":39.0,
        "Poster_created_time":1296746642860,
        "Poster_location":null,
        "Poster_reputation_count":524.0,
        "Poster_view_count":48.0,
        "Answer_body":"<p>Confirmed that this is not possible, AML only allows use of AML-created gateways.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1488397699940,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1296746642860,
        "Answerer_location":null,
        "Answerer_reputation_count":524.0,
        "Answerer_view_count":48.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41303697",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us exist gatewai want access onprem sql databas exist gatewai possibl aml tool allow creat new gatewai",
        "Question_preprocessed_content":"us exist gatewai want onprem sql databas exist gatewai aml creat new gatewai",
        "Question_gpt_summary_original":"The user wants to know if it is possible to use an existing Gateway to access an on-premises SQL database in Azure Machine Learning, as the tool only seems to allow creating new gateways.",
        "Question_gpt_summary":"user want know possibl us exist gatewai access premis sql databas tool allow creat new gatewai"
    },
    {
        "Question_title":"Wandb sweeps running on Kaggle GPU or Colab GPU are much slower than on my local CPU",
        "Question_body":"<p>Hi there,<\/p>\n<p>i have run a few sweeps on my local computer and the same sweeps on Kaggle and Colab<\/p>\n<p>i have an i7 (10th gen) CPU in my home computer but no GPU<br>\ni measured around 50secs for 100 epochs (1 run)<\/p>\n<p>on Kaggle and Colab the same 100 epochs took 2mins 30secs (Colab) and ~3mins  (Kaggle) <em>using GPU<\/em><\/p>\n<p>how is that possible? am i doing something wrong?<\/p>\n<p>i observed this extreme slowdown only when using W&amp;B Sweeps<br>\nno slowdown when running single experiments<\/p>\n<p>please help, any idea appreciated!<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_created_time":1632736952286,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":432.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-sweeps-running-on-kaggle-gpu-or-colab-gpu-are-much-slower-than-on-my-local-cpu\/794",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sweep run kaggl gpu colab gpu slower local cpu run sweep local sweep kaggl colab gen cpu home gpu measur sec epoch run kaggl colab epoch took min sec colab min kaggl gpu possibl wrong observ extrem slowdown sweep slowdown run singl experi help idea appreci",
        "Question_preprocessed_content":"gpu colab gpu slower local cpu run local colab cpu home gpu measur sec epoch colab epoch min sec min gpu wrong observ extrem slowdown slowdown singl experi help idea",
        "Question_gpt_summary_original":"The user is experiencing a significant slowdown when running W&B sweeps on Kaggle and Colab GPUs compared to their local CPU. They have observed that 100 epochs took 2.5-3 minutes on Kaggle and Colab using GPU, while it took only 50 seconds on their local computer with no GPU. The user is seeking help to understand why this is happening and if they are doing something wrong.",
        "Question_gpt_summary":"user experienc signific slowdown run sweep kaggl colab gpu compar local cpu observ epoch took minut kaggl colab gpu took second local gpu user seek help understand happen wrong"
    },
    {
        "Question_title":"AWS Sagemaker: Jupyter Notebook kernel keeps dying",
        "Question_body":"<p>I get disconnect every now and then when running a piece of code in Jupyter Notebooks on Sagemaker. I usually just restart my notebook and run all the cells again. However, I want to know if there is a way to reconnect to my instance without having to lose my progress. At the minute, it shows that there is &quot;No Kernel&quot; at the bottom bar, but my file seems active in the kernel sessions tab. Can I recover my notebook's variables and contents? Also, is there a way to prevent future kernel disconnections?<\/p>\n<p>Note that I reverted back to tornado = 5.1.1, which seems to decrease the number of disconnections, but it still happens every now and then.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_created_time":1611775383920,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":1983.0,
        "Poster_created_time":1533753580750,
        "Poster_location":null,
        "Poster_reputation_count":344.0,
        "Poster_view_count":63.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65925975",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"jupyt notebook kernel keep dy disconnect run piec code jupyt notebook usual restart notebook run cell want know wai reconnect instanc have lose progress minut show kernel bar file activ kernel session tab recov notebook variabl content wai prevent futur kernel disconnect note revert tornado decreas number disconnect happen",
        "Question_preprocessed_content":"jupyt kernel dy piec code jupyt restart run want know wai instanc have lose minut show kernel bar file activ kernel tab recov variabl content wai prevent futur kernel note revert tornado decreas number",
        "Question_gpt_summary_original":"The user is facing issues with the Jupyter Notebook kernel disconnecting frequently while using AWS Sagemaker. They are looking for a way to reconnect to their instance without losing their progress and recover their notebook's variables and contents. The user also wants to know if there is a way to prevent future kernel disconnections. The user has tried reverting back to tornado = 5.1.1, which has decreased the number of disconnections but the issue still persists.",
        "Question_gpt_summary":"user face issu jupyt notebook kernel disconnect frequent look wai reconnect instanc lose progress recov notebook variabl content user want know wai prevent futur kernel disconnect user tri revert tornado decreas number disconnect issu persist"
    },
    {
        "Question_title":"Machine Learning models on AWS Sagemaker + Quicksight as a front end",
        "Question_body":"<p>Suppose I have a trained model into Sagemaker. It is not an AWS pre built model. This model will be accessed by users, who will pass input data to its inference server endpoint and the results must to be exhibited into a dashboard.<\/p>\n<p>First question: Quicksight are useful for that as a front end? Is it possible to create fields to be filled by the user? I know that some BI tools like Tableau does not allow inserting new data by users. Quicksight allows that?<\/p>\n<p>Second question: Some BI tool that allows that? Or this particular case is not covered by any? Do I need to go to front end development like, for instance, Flask?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1622065575813,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":226.0,
        "Poster_created_time":1522371976587,
        "Poster_location":"Brazil",
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67713111",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"machin learn model quicksight end suppos train model aw pre built model model access user pass input data infer server endpoint result exhibit dashboard question quicksight us end possibl creat field fill user know tool like tableau allow insert new data user quicksight allow second question tool allow particular case cover need end develop like instanc flask",
        "Question_preprocessed_content":"machin learn model quicksight end train model aw pre built model model user input data infer server endpoint result exhibit dashboard question quicksight us end creat field user know like tableau insert new data user quicksight second question particular case cover end develop like instanc flask",
        "Question_gpt_summary_original":"The user has a trained machine learning model on AWS Sagemaker that needs to be accessed by users who will pass input data to its inference server endpoint and the results must be exhibited into a dashboard. The user is wondering if Quicksight can be used as a front end and if it allows users to insert new data. The user is also asking if there are any other BI tools that allow this or if front end development is necessary.",
        "Question_gpt_summary":"user train machin learn model need access user pass input data infer server endpoint result exhibit dashboard user wonder quicksight end allow user insert new data user ask tool allow end develop necessari"
    },
    {
        "Question_title":"Error while loading DSTNEE i sagemaker terminal",
        "Question_body":"<p>I got this error can someone help?\nThis error I got while building docker image in the sagemaker terminal , i tried to resolve by changing ubuntu and the cuda version ut still error is persistent can someone tell the reason of error?<\/p>\n<pre><code>In file included from NcExcptionWrap.h:2:0,\n                 from NNNetwork.cpp:14:\n\/usr\/include\/ncException.h:26:7: note: candidate: netCDF::exceptions::NcException::NcException(const string&amp;, const string&amp;, const char*, int)\n       NcException(const std::string&amp; exceptionName,const std::string&amp; complaint,const char* fileName,int lineNumber);\n       ^\n\/usr\/include\/ncException.h:26:7: note:   candidate expects 4 arguments, 3 provided\n\/usr\/include\/ncException.h:24:11: note: candidate: netCDF::exceptions::NcException::NcException(const netCDF::exceptions::NcException&amp;)\n     class NcException : public std::exception {\n           ^\n\/usr\/include\/ncException.h:24:11: note:   candidate expects 1 argument, 3 provided\nmake[1]: *** [\/opt\/amazon\/dsstne\/build\/tmp\/engine\/cpp\/NNNetwork.o] Error 1\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1654666958430,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":12.0,
        "Poster_created_time":1654666320956,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72540598",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error load dstnee termin got error help error got build docker imag termin tri resolv chang ubuntu cuda version error persist tell reason error file includ ncexcptionwrap nnnetwork cpp usr includ ncexcept note candid netcdf except ncexcept ncexcept const string const string const char int ncexcept const std string exceptionnam const std string complaint const char filenam int linenumb usr includ ncexcept note candid expect argument provid usr includ ncexcept note candid netcdf except ncexcept ncexcept const netcdf except ncexcept class ncexcept public std except usr includ ncexcept note candid expect argument provid opt amazon dsstne build tmp engin cpp nnnetwork error",
        "Question_preprocessed_content":"load termin got help got build docker imag termin tri resolv chang ubuntu cuda version persist reason",
        "Question_gpt_summary_original":"The user encountered an error while building a docker image in the Sagemaker terminal. They tried to resolve the error by changing the Ubuntu and CUDA version, but the error persisted. The error message suggests a problem with the NcException function, with the candidate expecting 4 arguments but only receiving 3.",
        "Question_gpt_summary":"user encount error build docker imag termin tri resolv error chang ubuntu cuda version error persist error messag suggest problem ncexcept function candid expect argument receiv"
    },
    {
        "Question_title":"Can the Google Translate API v2 be used in the frontend?",
        "Question_body":"Hello? By any chance, can the Google Translate API v2 be used in the frontend?\n\nInitially, I thought the Google Translate API v2 couldn't be used in the frontend due to CORS issues. However, when I input the code below into the browser, the API is being called normally.\n\nI'm curious if it's okay to call the API directly from the browser from the beginning.\n\nconst url = 'https:\/\/translation.googleapis.com\/language\/translate\/v2?key={{API_KEY}}';\nconst headers = {\n  'Content-Type': 'application\/json; charset=utf-8',\n};\n\nfetch(url, {\n  method: 'POST',\n  headers: headers,\n  body: JSON.stringify({\n    \"q\": [\"Hello world\", \"My name is Jeff\"],\n    \"target\": \"de\"\n  }),\n})\n  .then((response) => response.json())\n  .then((data) => console.log(data))\n  .catch((error) => console.error(error));",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1685329260000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":15.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-the-Google-Translate-API-v2-be-used-in-the-frontend\/td-p\/597834\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"googl translat api frontend hello chanc googl translat api frontend initi thought googl translat api couldn frontend cor issu input code browser api call normal curiou okai api directli browser begin const url http translat googleapi com languag translat kei api kei const header content type applic json charset utf fetch url method post header header bodi json stringifi hello world jeff target respons respons json data consol log data catch error consol error error",
        "Question_preprocessed_content":"translat api frontend chanc translat api frontend thought translat api couldn frontend cor input code browser api curiou okai api directli browser const url const header fetch catch",
        "Question_gpt_summary_original":"The user is inquiring if the Google Translate API v2 can be used in the frontend, despite initial concerns about CORS issues. They have provided a code snippet that appears to call the API normally and are seeking confirmation if it is acceptable to call the API directly from the browser.",
        "Question_gpt_summary":"user inquir googl translat api frontend despit initi concern cor issu provid code snippet appear api normal seek confirm accept api directli browser"
    },
    {
        "Question_title":"Plot overlay obscures most of plot",
        "Question_body":"<p>See screenshot, when I mouse over the plot showing the value of a metric across multiple runs, the acutal shape of the plot is obscured by the detail view almost all of the time. This makes it almost impossible to visually compare runs while also seeing precise values at certain points.<\/p>\n<p>Additionally, assuming this gets fixed by moving the detail view to the lower-right portion of the plot (or even outside the bounds of the plot entirely), it would be really nice to see a horizontal guide in addition to a vertical guide, so I can visually tell  how the peaks of a given run compare to the peaks of other runs a different points in time (I\u2019m using <code>tfa.optimizers.CyclicalLearningRate<\/code> so my metrics have a rather spiky nature).<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2590f8667fb82a696da92b44e20ea000587efde7.png\" alt=\"Screen Shot 2022-09-01 at 2.24.01 pm\" data-base62-sha1=\"5mkgmk48PEjHXdWpVtiAwgVAwZN\" width=\"477\" height=\"277\"><\/p>\n<p>Screenshot showing spiky nature of metric plot:<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/f72adaf4477903b571813ced8457fdf9cdab5c34.png\" alt=\"Screen Shot 2022-09-01 at 2.31.11 pm\" data-base62-sha1=\"zgxKDAV5kmh1mT2P88Unv5CvThO\" width=\"422\" height=\"151\"><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1662006885138,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":83.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/plot-overlay-obscures-most-of-plot\/3044",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"plot overlai obscur plot screenshot mous plot show valu metric multipl run acut shape plot obscur view time make imposs visual compar run see precis valu certain point addition assum get fix move view lower right portion plot outsid bound plot entir nice horizont guid addit vertic guid visual tell peak given run compar peak run differ point time tfa optim cyclicallearningr metric spiki natur screenshot show spiki natur metric plot",
        "Question_preprocessed_content":"plot overlai obscur plot mous plot show valu metric multipl run acut shape plot obscur view time make compar run precis valu certain point get fix move view portion plot nice horizont guid vertic guid peak given run compar peak run point time show spiki natur metric plot",
        "Question_gpt_summary_original":"The user is facing an issue where the plot overlay is obscuring most of the plot, making it difficult to visually compare runs while also seeing precise values at certain points. The user suggests moving the detail view to the lower-right portion of the plot and adding a horizontal guide to visually compare the peaks of a given run to the peaks of other runs at different points in time. The user has provided screenshots to illustrate the spiky nature of the metric plot.",
        "Question_gpt_summary":"user face issu plot overlai obscur plot make difficult visual compar run see precis valu certain point user suggest move view lower right portion plot ad horizont guid visual compar peak given run peak run differ point time user provid screenshot illustr spiki natur metric plot"
    },
    {
        "Question_title":"how to access text file from s3 bucket into sagemaker for training a model?",
        "Question_body":"<p>I am trying to train chatbot model using tensorflow and seq to seq architecture using sagemaker also I have completed coding in spyder but when \nI am trying to access cornel movie corpus dataset from s3 bucket into sagemaker it says no such file or directory even granting access to s3 bucket<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1587795471797,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1587807784552,
        "Question_score_count":0.0,
        "Question_view_count":576.0,
        "Poster_created_time":1587794844532,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61421738",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"access text file bucket train model try train chatbot model tensorflow seq seq architectur complet code spyder try access cornel movi corpu dataset bucket sai file directori grant access bucket",
        "Question_preprocessed_content":"text file bucket train model try train chatbot model tensorflow seq seq architectur complet code spyder try cornel movi corpu dataset bucket sai file directori grant bucket",
        "Question_gpt_summary_original":"The user is facing challenges in accessing a text file from an S3 bucket into Sagemaker for training a chatbot model using TensorFlow and Seq to Seq architecture. Despite granting access to the S3 bucket, the user is receiving an error message stating that there is no such file or directory.",
        "Question_gpt_summary":"user face challeng access text file bucket train chatbot model tensorflow seq seq architectur despit grant access bucket user receiv error messag state file directori"
    },
    {
        "Question_title":"Error importing dvc.api",
        "Question_body":"<p>Trying follow the guide to read a file from DVC remote (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">open()<\/a>) but having issues import the DVC python API:<\/p>\n<p><code>import dvc.api<\/code><\/p>\n<p>Getting the error:<\/p>\n<pre><code class=\"lang-auto\">File c:\\...\\env\\lib\\site-packages\\dvc_objects\\fs\\__init__.py:5\n...\n--&gt; 157     ResourceEror = resource.error\n    159 _DEFAULT_BATCH_SIZE = 128\n    160 _NOFILES_DEFAULT_BATCH_SIZE = 1280\n\nAttributeError: module 'resource' has no attribute 'error'\n<\/code><\/pre>\n<p>Environment: Windows 11. Python 3.9.6<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1680251103867,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":61.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-importing-dvc-api\/1582",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error import api try follow guid read file remot open have issu import python api import api get error file env lib site packag object init resourceeror resourc error default batch size nofil default batch size attributeerror modul resourc attribut error environ window python",
        "Question_preprocessed_content":"import api try guid read file remot have import python api environ window python",
        "Question_gpt_summary_original":"The user is encountering an error while trying to import the DVC python API for reading a file from DVC remote. The error message indicates that the 'resource' module has no attribute 'error'. The user is using Windows 11 and Python 3.9.6.",
        "Question_gpt_summary":"user encount error try import python api read file remot error messag indic resourc modul attribut error user window python"
    },
    {
        "Question_title":"How to fix 'KeyError: dtype('float32')' in LDAviz",
        "Question_body":"<p>I use LDAvis library to visualize my LDA topics. It works fine before, but it gets me this error when I download the saved model files from Sagemaker to the local computer. I don't know why does this happen? Does that relate to Sagemaker?<\/p>\n\n<p>If I run from the local, and saved the model from local, and then run LDAviz library, it works fine.<\/p>\n\n<hr>\n\n<p>KeyError                                  Traceback (most recent call last)\n in ()<\/p>\n\n<p>~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py in prepare(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\n    116     See <code>pyLDAvis.prepare<\/code> for **kwargs.\n    117     \"\"\"\n--> 118     opts = fp.merge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n    119     return vis_prepare(**opts)<\/p>\n\n<p>~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\gensim.py in _extract_data(topic_model, corpus, dictionary, doc_topic_dists)\n     46           gamma = topic_model.inference(corpus)\n     47       else:\n---> 48           gamma, _ = topic_model.inference(corpus)\n     49       doc_topic_dists = gamma \/ gamma.sum(axis=1)[:, None]\n     50    else:<\/p>\n\n<p>~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py in inference(self, chunk, collect_sstats)\n    665             # phinorm is the normalizer.\n    666             # TODO treat zeros explicitly, instead of adding epsilon?\n--> 667             eps = DTYPE_TO_EPS[self.dtype]\n    668             phinorm = np.dot(expElogthetad, expElogbetad) + eps\n    669 <\/p>\n\n<p>KeyError: dtype('float32')<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1561573597343,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":577.0,
        "Poster_created_time":1524091215152,
        "Poster_location":"Hoboken, NJ, USA",
        "Poster_reputation_count":187.0,
        "Poster_view_count":28.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56779011",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"fix keyerror dtype float ldaviz us ldavi librari visual lda topic work fine get error download save model file local know happen relat run local save model local run ldaviz librari work fine keyerror traceback recent appdata local continuum anaconda lib site packag pyldavi gensim prepar topic model corpu dictionari doc topic dist kwarg pyldavi prepar kwarg opt merg extract data topic model corpu dictionari doc topic dist kwarg return vi prepar opt appdata local continuum anaconda lib site packag pyldavi gensim extract data topic model corpu dictionari doc topic dist gamma topic model infer corpu gamma topic model infer corpu doc topic dist gamma gamma sum axi appdata local continuum anaconda lib site packag gensim model ldamodel infer self chunk collect sstat phinorm normal todo treat zero explicitli instead ad epsilon ep dtype ep self dtype phinorm dot expelogthetad expelogbetad ep keyerror dtype float",
        "Question_preprocessed_content":"fix dtype ldaviz us ldavi librari visual lda topic work fine get download save model file local know relat run local save model local run ldaviz librari work fine traceback corpu dictionari kwarg kwarg opt corpu dictionari kwarg return corpu dictionari sum infer phinorm normal todo treat zero explicitli instead epsilon ep phinorm expelogbetad ep dtype",
        "Question_gpt_summary_original":"The user is encountering a 'KeyError: dtype('float32')' error while using the LDAvis library to visualize their LDA topics. The error occurs when they download saved model files from Sagemaker to their local computer. The user is unsure why this is happening and if it is related to Sagemaker. However, when they run the library from the local and save the model from there, it works fine.",
        "Question_gpt_summary":"user encount keyerror dtype float error ldavi librari visual lda topic error occur download save model file local user unsur happen relat run librari local save model work fine"
    },
    {
        "Question_title":"AzureML webservice deployment with custom Environment - \/var\/runit does not exist",
        "Question_body":"<p>I'm struggling to deploy a model with a custom environment through the azureml SDK.<\/p>\n<p>I have built a docker image locally and pushed it to azure container registry to use it for environment instantiating. This is how my dockerfile looks like:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04\nFROM python:3.9.12\n        \n# Keeps Python from generating .pyc files in the container\nENV PYTHONDONTWRITEBYTECODE=1\n        \n# Turns off buffering for easier container logging\nENV PYTHONUNBUFFERED=1\n        \n# Install requirement for deploying the service\nRUN apt-get update\nRUN apt-get install -y runit\n        \n# Install pip requirements\nRUN pip install --upgrade pip\nCOPY requirements.txt .\nRUN pip install azureml-defaults\nRUN pip install -r requirements.txt\n<\/code><\/pre>\n<p>I want to deploy the webservice locally for testing, so I am following the steps according to official documentation:<\/p>\n<pre><code>ws = Workspace(\n    subscription_id='mysub_id', \n    resource_group='myresource_group', \n    workspace_name='myworkspace'\n)\n        \nmodel = Model.register(\n    ws, \n    model_name='mymodel', \n    model_path='.\/Azure_Deployment\/mymodel_path'\n)\n        \ncontainer = ContainerRegistry()\ncontainer.address = 'myaddress'\nmyenv = Environment.from_docker_image('myenv_name', 'img\/img_name:v1', container)\n        \ninference_config = InferenceConfig(\n    environment=myenv, \n    source_directory='.\/Azure_Deployment', \n    entry_script='echo_score.py',\n)\n        \ndeployment_config = LocalWebservice.deploy_configuration(port=6789)\n        \nservice = Model.deploy(\n    ws, \n    &quot;myservice&quot;, \n    [model], \n    inference_config, \n    deployment_config, \n    overwrite=True,\n)\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>This is what I get from the logs:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/GLafe.png\" rel=\"nofollow noreferrer\">service container logs<\/a><\/p>\n<p>Checking into the resulting container for the service I can see indeed there is no \/runit folder inside \/var. There is also no other folders created for the service besides the azureml-app containing my model's files.<\/p>\n<p>I would really appreciate any insights to what's going on here as I have no clue at this point.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1648591923567,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":156.0,
        "Poster_created_time":1648585295487,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71669344",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"webservic deploy custom environ var runit exist struggl deploi model custom environ sdk built docker imag local push azur contain registri us environ instanti dockerfil look like mcr microsoft com openmpi ubuntu python keep python gener pyc file contain env pythondontwritebytecod turn buffer easier contain log env pythonunbuff instal requir deploi servic run apt updat run apt instal runit instal pip requir run pip instal upgrad pip copi requir txt run pip instal default run pip instal requir txt want deploi webservic local test follow step accord offici document workspac subscript mysub resourc group myresourc group workspac myworkspac model model regist model mymodel model path azur deploy mymodel path contain containerregistri contain address myaddress myenv environ docker imag myenv img img contain infer config inferenceconfig environ myenv sourc directori azur deploy entri script echo score deploy config localwebservic deploi configur port servic model deploi myservic model infer config deploy config overwrit true servic wait deploy output true log servic contain log check result contain servic runit folder insid var folder creat servic app contain model file appreci insight go clue point",
        "Question_preprocessed_content":"webservic deploy custom environ exist deploi model custom environ sdk built docker imag push azur contain registri us environ instanti dockerfil like want deploi webservic test step document log servic contain log check result contain servic runit folder insid var folder creat servic contain model file insight go clue point",
        "Question_gpt_summary_original":"The user is facing challenges while deploying a model with a custom environment through the AzureML SDK. The user has built a docker image locally and pushed it to Azure container registry to use it for environment instantiating. However, while deploying the webservice locally for testing, the user encountered an error message stating that the \/var\/runit folder does not exist. The resulting container for the service does not contain any other folders created for the service besides the azureml-app containing the model's files. The user is seeking insights into the issue.",
        "Question_gpt_summary":"user face challeng deploi model custom environ sdk user built docker imag local push azur contain registri us environ instanti deploi webservic local test user encount error messag state var runit folder exist result contain servic contain folder creat servic app contain model file user seek insight issu"
    },
    {
        "Question_title":"VSCode cannot see packages on a GCP VM",
        "Question_body":"<p>I have an issue where vscode when connected to a VM on GCP cannot see packages installed in <code>\/opt\/conda\/lib\/python3.7\/site-packages.<\/code> I have created the VM using Vertex AI. When I open the jupyter notebook through the UI in a the browser I can see all the packages via <code>pip3 list<\/code>. But when I am connected to the VM through SSH in vscode I cannot see the packages installed such as nltk, spacy etc. and when I try to load it gives me <code>ModuleNotFoundError<\/code>. This error does not show up when I use the jupyter notebook from the Vertex AI UI. The site-packages folder is in my system path and the python that I am using is <code>\/opt\/conda\/bin\/python3<\/code>.<\/p>\n<p>Any help is appreciated. Please do let me know if my question is clear.<\/p>\n<p>EDIT: I figured out that my packages are running on a container in the VM. Is there a way for me to access those packages via jupyter notebook in vscode?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_created_time":1638473538253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1638550660460,
        "Question_score_count":0.0,
        "Question_view_count":230.0,
        "Poster_created_time":1580840045043,
        "Poster_location":null,
        "Poster_reputation_count":85.0,
        "Poster_view_count":16.0,
        "Answer_body":"<p>Posting the answer as community wiki. As confirmed by @Abhishek, he was able to make it work by installing a docker extension on the VM then attach VS code to the container.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1638843453688,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1606724007903,
        "Answerer_location":null,
        "Answerer_reputation_count":5969.0,
        "Answerer_view_count":2590.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70205432",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"vscode packag gcp issu vscode connect gcp packag instal opt conda lib python site packag creat open jupyt notebook browser packag pip list connect ssh vscode packag instal nltk spaci try load give modulenotfounderror error us jupyt notebook site packag folder path python opt conda bin python help appreci let know question clear edit figur packag run contain wai access packag jupyt notebook vscode",
        "Question_preprocessed_content":"vscode packag gcp vscode gcp packag creat open jupyt browser packag vscode packag nltk spaci try load give us jupyt folder path python help let know question clear edit figur packag contain wai packag jupyt vscode",
        "Question_gpt_summary_original":"The user is facing an issue where VSCode, when connected to a VM on GCP, cannot see packages installed in \/opt\/conda\/lib\/python3.7\/site-packages. The packages are running on a container in the VM, and the user is looking for a way to access those packages via Jupyter Notebook in VSCode. The error message received is ModuleNotFoundError.",
        "Question_gpt_summary":"user face issu vscode connect gcp packag instal opt conda lib python site packag packag run contain user look wai access packag jupyt notebook vscode error messag receiv modulenotfounderror"
    },
    {
        "Question_title":"Shutdown AWS Sagemaker without using Python script",
        "Question_body":"<p>I am trying to find a solution to auto shutdown AWS Sagemaker instance after 1 hour of idleness. I found the below solution for this but at our environment the Sagemaker cant access any code from internet so wget will fail. Is there any way to achieve this result without using the script from below solution? Like defining everything in the bash shell and then run it using cron job?<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/on-start.sh<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1628279689717,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1628335111732,
        "Question_score_count":0.0,
        "Question_view_count":272.0,
        "Poster_created_time":1572868147876,
        "Poster_location":null,
        "Poster_reputation_count":553.0,
        "Poster_view_count":71.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68686881",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"shutdown python script try solut auto shutdown instanc hour idl solut environ access code internet wget fail wai achiev result script solut like defin bash shell run cron job http github com aw sampl amazon notebook instanc lifecycl config sampl blob master script auto stop idl start",
        "Question_preprocessed_content":"shutdown python script try solut auto shutdown instanc hour solut environ code internet wget fail wai achiev result script solut like defin bash run cron job",
        "Question_gpt_summary_original":"The user is trying to find a way to auto shutdown AWS Sagemaker instance after 1 hour of idleness without using the Python script provided in a solution due to the Sagemaker's inability to access any code from the internet. They are looking for an alternative solution, possibly defining everything in the bash shell and running it using a cron job.",
        "Question_gpt_summary":"user try wai auto shutdown instanc hour idl python script provid solut inabl access code internet look altern solut possibl defin bash shell run cron job"
    },
    {
        "Question_title":"Using HuggingFace in Sagemaker Studio as part of a project",
        "Question_body":"TLDR: if we are trying to use a HuggingFaceProcessor\/Estimator in a Sagemaker Studio project, what are the requirements for the `train.py` file in terms of how it refers to the assembled training data, and where it should store the results of the operations it performs( e.g. compiled model, datae etc.)\n\n-----------------------\nFULL DETAILS\n------------------------\n\nSo our high level goal is to be able to deploy some kind of non-XGB model from a sagemaker studio project, given that the templates provided are all XGB.  As outlined in [an earlier question](https:\/\/repost.aws\/questions\/QUdd2zOBY0Q4CEG1ZdbgNsgA\/using-transformers-module-with-sagemaker-studio-project-module-not-found-error-no-module-named-transformers) we'd started with TensorFlow, but since our TensorFlow model wraps a HuggingFace model we thought let's try something even simpler, just a HuggingFace model using the HuggingFaceProcessor.\n\nSo following docs on [HuggingFaceProcessor](https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job-frameworks-hugging-face.html) and a [HuggingFace Estimator](https:\/\/github.com\/huggingface\/notebooks\/blob\/main\/sagemaker\/02_getting_started_tensorflow\/sagemaker-notebook.ipynb) example we started to adjust the abalone (project template) pipeline.py to look like this (full code can be provided on request):\n\n```\n    # processing step for feature engineering\n    hf_processor = HuggingFaceProcessor(\n        role=role, \n        instance_count=processing_instance_count,\n        instance_type=processing_instance_type,\n        transformers_version='4.4.2',\n        pytorch_version='1.6.0', \n        base_job_name=f\"{base_job_prefix}\/frameworkprocessor-hf\",\n        sagemaker_session=pipeline_session,\n    )\n    step_args = hf_processor.run(\n        outputs=[\n            ProcessingOutput(output_name=\"train\", source=\"\/opt\/ml\/processing\/train\"),\n            ProcessingOutput(output_name=\"validation\", source=\"\/opt\/ml\/processing\/validation\"),\n            ProcessingOutput(output_name=\"test\", source=\"\/opt\/ml\/processing\/test\"),\n        ],\n        code=os.path.join(BASE_DIR, \"preprocess.py\"),\n        arguments=[\"--input-data\", input_data],\n    )\n    step_process = ProcessingStep(\n        name=\"PreprocessTopicData\",\n        step_args=step_args,\n    )\n\n    # training step for generating model artifacts\n    model_path = f\"s3:\/\/{sagemaker_session.default_bucket()}\/{base_job_prefix}\/TopicTrain\"\n\n    hf_train = HuggingFace(entry_point='train.py',\n                            source_dir=BASE_DIR,\n                            base_job_name='huggingface-sdk-extension',\n                            instance_type=processing_instance_type,\n                            instance_count=processing_instance_count,\n                            transformers_version='4.4',\n                            pytorch_version='1.6',\n                            py_version='py36',\n                            role=role,\n                          )\n  \n    hf_train.set_hyperparameters(\n       epochs=3,\n       train_batch_size=16,\n       learning_rate=1.0e-5,\n       model_name='distilbert-base-uncased',\n    )\n                           \n    step_args = hf_train.fit(\n        inputs={\n            \"train\": TrainingInput(\n                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                    \"train\"\n                ].S3Output.S3Uri,\n                content_type=\"text\/csv\",\n            ),\n            \"validation\": TrainingInput(\n                s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n                    \"validation\"\n                ].S3Output.S3Uri,\n                content_type=\"text\/csv\",\n            ),\n        },\n    )\n```\n\nFinding that pushing to master doesn't provide any feedback on issues arising from pipeline.py, we realised that trying to get the pipeline from a notebook was a better way of debugging these sorts of changes, assuming one remembered to restart the kernel each time to ensure changes to the pipeline.py file was available to the notebook.  \n\nSo using the following code in the notebook we worked through a series of issues trying to bash the code into shape such that it would compile:\n\n```\nfrom pipelines.topic.pipeline import get_pipeline\n\n\npipeline = get_pipeline(\n    region=region,\n    role=role,\n    default_bucket=default_bucket,\n    model_package_group_name=model_package_group_name,\n    pipeline_name=pipeline_name,\n)\n```\n\nWe needed to change the default processing and training instance types to avoid a \"cpu\" unsupported issue:\n\n```\n    processing_instance_type=\"ml.p3.xlarge\",\n    training_instance_type=\"ml.p3.xlarge\",\n```\n\nand add a train.py script:\n\n```\nfrom transformers import AutoTokenizer\nfrom transformers import TFAutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=18)\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    DistilBertTokenizerFast,\n    TFDistilBertForSequenceClassification,\n)\nDATA_COLUMN = 'text'\nLABEL_COLUMN = 'label'\nMAX_SEQUENCE_LENGTH = 512\nLEARNING_RATE = 5e-5\nBATCH_SIZE = 16\nNUM_EPOCHS = 3\nNUM_LABELS = 15\n\nif __name__ == \"__main__\":\n\n    # --------------------------------------------------------------------------------\n    # Tokenizer\n    # --------------------------------------------------------------------------------\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n    def tokenize(sentences, max_length=MAX_SEQUENCE_LENGTH, padding='max_length'):\n        \"\"\"Tokenize using the Huggingface tokenizer\n        Args:\n            sentences: String or list of string to tokenize\n            padding: Padding method ['do_not_pad'|'longest'|'max_length']\n        \"\"\"\n        return tokenizer(\n            sentences,\n            truncation=True,\n            padding=padding,\n            max_length=max_length,\n            return_tensors=\"tf\"\n        )\n    # --------------------------------------------------------------------------------\n    # Load data\n    # --------------------------------------------------------------------------------\n    from keras.utils import to_categorical\n    from sklearn.preprocessing import LabelEncoder\n    labelencoder_Y_1 = LabelEncoder()\n    yy = labelencoder_Y_1.fit_transform(train_data[LABEL_COLUMN].tolist())\n    yy = to_categorical(yy)\n    print(len(yy))\n    print(yy.shape)\n    train_dat, validation_dat, train_label, validation_label = train_test_split(\n        train_data[DATA_COLUMN].tolist(),\n        yy,\n        test_size=0.2,\n        shuffle=True\n    )\n    # --------------------------------------------------------------------------------\n    # Prepare TF dataset\n    # --------------------------------------------------------------------------------\n    train_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(tokenize(train_dat)),  # Convert BatchEncoding instance to dictionary\n        train_label\n    )).shuffle(1000).batch(BATCH_SIZE).prefetch(1)\n    validation_dataset = tf.data.Dataset.from_tensor_slices((\n        dict(tokenize(validation_dat)),\n        validation_label\n    )).batch(BATCH_SIZE).prefetch(1)\n    # --------------------------------------------------------------------------------\n    # training\n    # --------------------------------------------------------------------------------\n    model = TFDistilBertForSequenceClassification.from_pretrained(\n        'distilbert-base-uncased',\n        num_labels=NUM_LABELS\n    )\n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(\n        optimizer=optimizer,\n        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    )\n```\n\nHowever we are now stuck on this error when trying to get the pipeline from a notebook.\n\n```TypeError                                 Traceback (most recent call last)\n<ipython-input-3-be38b3dda75f> in <module>\n      7     default_bucket=default_bucket,\n      8     model_package_group_name=model_package_group_name,\n----> 9     pipeline_name=pipeline_name,\n     10 )\n     11 # !conda list\n\n~\/topic-models-no-monitoring-p-rboparx6tdeg\/sagemaker-topic-models-no-monitoring-p-rboparx6tdeg-modelbuild\/pipelines\/topic\/pipeline.py in get_pipeline(region, sagemaker_project_arn, role, default_bucket, model_package_group_name, pipeline_name, base_job_prefix, processing_instance_type, training_instance_type)\n    228                     \"validation\"\n    229                 ].S3Output.S3Uri,\n--> 230                 content_type=\"text\/csv\",\n    231             ),\n    232         },\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline_context.py in wrapper(*args, **kwargs)\n    246             return self_instance.sagemaker_session.context\n    247 \n--> 248         return run_func(*args, **kwargs)\n    249 \n    250     return wrapper\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name, experiment_config)\n   1059         self._prepare_for_training(job_name=job_name)\n   1060 \n-> 1061         self.latest_training_job = _TrainingJob.start_new(self, inputs, experiment_config)\n   1062         self.jobs.append(self.latest_training_job)\n   1063         if wait:\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in start_new(cls, estimator, inputs, experiment_config)\n   1956         train_args = cls._get_train_args(estimator, inputs, experiment_config)\n   1957 \n-> 1958         estimator.sagemaker_session.train(**train_args)\n   1959 \n   1960         return cls(estimator.sagemaker_session, estimator._current_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in train(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\n    611             self.sagemaker_client.create_training_job(**request)\n    612 \n--> 613         self._intercept_create_request(train_request, submit, self.train.__name__)\n    614 \n    615     def _get_train_request(  # noqa: C901\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in _intercept_create_request(self, request, create, func_name)\n   4303             func_name (str): the name of the function needed intercepting\n   4304         \"\"\"\n-> 4305         return create(request)\n   4306 \n   4307 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/session.py in submit(request)\n    608         def submit(request):\n    609             LOGGER.info(\"Creating training-job with name: %s\", job_name)\n--> 610             LOGGER.debug(\"train request: %s\", json.dumps(request, indent=4))\n    611             self.sagemaker_client.create_training_job(**request)\n    612 \n\n\/opt\/conda\/lib\/python3.7\/json\/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n    237         separators=separators, default=default, sort_keys=sort_keys,\n--> 238         **kw).encode(obj)\n    239 \n    240 \n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in encode(self, o)\n    199         chunks = self.iterencode(o, _one_shot=True)\n    200         if not isinstance(chunks, (list, tuple)):\n--> 201             chunks = list(chunks)\n    202         return ''.join(chunks)\n    203 \n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode(o, _current_indent_level)\n    429             yield from _iterencode_list(o, _current_indent_level)\n    430         elif isinstance(o, dict):\n--> 431             yield from _iterencode_dict(o, _current_indent_level)\n    432         else:\n    433             if markers is not None:\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode_dict(dct, _current_indent_level)\n    403                 else:\n    404                     chunks = _iterencode(value, _current_indent_level)\n--> 405                 yield from chunks\n    406         if newline_indent is not None:\n    407             _current_indent_level -= 1\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode_dict(dct, _current_indent_level)\n    403                 else:\n    404                     chunks = _iterencode(value, _current_indent_level)\n--> 405                 yield from chunks\n    406         if newline_indent is not None:\n    407             _current_indent_level -= 1\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in _iterencode(o, _current_indent_level)\n    436                     raise ValueError(\"Circular reference detected\")\n    437                 markers[markerid] = o\n--> 438             o = _default(o)\n    439             yield from _iterencode(o, _current_indent_level)\n    440             if markers is not None:\n\n\/opt\/conda\/lib\/python3.7\/json\/encoder.py in default(self, o)\n    177 \n    178         \"\"\"\n--> 179         raise TypeError(f'Object of type {o.__class__.__name__} '\n    180                         f'is not JSON serializable')\n    181 \n\nTypeError: Object of type ParameterInteger is not JSON serializable\n```\n\nWhich is telling us that some aspect of the training job (?) is not serializable, and it's not clear how to debug further.\n\nWhat would be enormously helpful is project templates for sagemaker studio showing the use of all the Processors, e.g. HuggingFace, TensorFlow and so on, but failing that we'd be most grateful is anyone could point us to documentation on what the requirements are for the `train.py` file that we need to specifiy for the HuggingFace Estimator.\n\nmany thanks in advance",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1664885918543,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668487418864,
        "Question_score_count":0.0,
        "Question_view_count":214.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6Ahf5zWZRZq63k1TOQ_48w\/using-huggingface-in-sagemaker-studio-as-part-of-a-project",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"huggingfac studio project tldr try us huggingfaceprocessor estim studio project requir train file term refer assembl train data store result oper perform compil model data detail high level goal abl deploi kind non xgb model studio project given templat provid xgb outlin earlier question http repost aw question quddzobyqcegzdbgnsga transform modul studio project modul error modul name transform start tensorflow tensorflow model wrap huggingfac model thought let try simpler huggingfac model huggingfaceprocessor follow doc huggingfaceprocessor http doc aw amazon com latest process job framework hug face html huggingfac estim http github com huggingfac notebook blob main get start tensorflow notebook ipynb exampl start adjust abalon project templat pipelin look like code provid request process step featur engin processor huggingfaceprocessor role role instanc count process instanc count instanc type process instanc type transform version pytorch version base job base job prefix frameworkprocessor session pipelin session step arg processor run output processingoutput output train sourc opt process train processingoutput output valid sourc opt process valid processingoutput output test sourc opt process test code path join base dir preprocess argument input data input data step process processingstep preprocesstopicdata step arg step arg train step gener model artifact model path session default bucket base job prefix topictrain train huggingfac entri point train sourc dir base dir base job huggingfac sdk extens instanc type process instanc type instanc count process instanc count transform version pytorch version version role role train set hyperparamet epoch train batch size learn rate model distilbert base uncas step arg train fit input train traininginput data step process properti processingoutputconfig output train soutput suri content type text csv valid traininginput data step process properti processingoutputconfig output valid soutput suri content type text csv find push master provid feedback issu aris pipelin realis try pipelin notebook better wai debug sort chang assum rememb restart kernel time ensur chang pipelin file avail notebook follow code notebook work seri issu try bash code shape compil pipelin topic pipelin import pipelin pipelin pipelin region region role role default bucket default bucket model packag group model packag group pipelin pipelin need chang default process train instanc type avoid cpu unsupport issu process instanc type xlarg train instanc type xlarg add train script transform import autotoken transform import tfautomodelforsequenceclassif token autotoken pretrain distilbert base uncas model tfautomodelforsequenceclassif pretrain distilbert base uncas num label import panda import tensorflow sklearn model select import train test split transform import distilberttokenizerfast tfdistilbertforsequenceclassif data column text label column label max sequenc length learn rate batch size num epoch num label main token token distilberttokenizerfast pretrain distilbert base uncas def token sentenc max length max sequenc length pad max length token huggingfac token arg sentenc string list string token pad pad method pad longest max length return token sentenc truncat true pad pad max length max length return tensor load data kera util import categor sklearn preprocess import labelencod labelencod labelencod labelencod fit transform train data label column tolist categor print len print shape train dat valid dat train label valid label train test split train data data column tolist test size shuffl true prepar dataset train dataset data dataset tensor slice dict token train dat convert batchencod instanc dictionari train label shuffl batch batch size prefetch valid dataset data dataset tensor slice dict token valid dat valid label batch batch size prefetch train model tfdistilbertforsequenceclassif pretrain distilbert base uncas num label num label optim kera optim adam learn rate learn rate model compil optim optim loss kera loss categoricalcrossentropi logit true stuck error try pipelin notebook typeerror traceback recent default bucket default bucket model packag group model packag group pipelin pipelin conda list topic model monitor rboparxtdeg topic model monitor rboparxtdeg modelbuild pipelin topic pipelin pipelin region project arn role default bucket model packag group pipelin base job prefix process instanc type train instanc type valid soutput suri content type text csv opt conda lib python site packag workflow pipelin context wrapper arg kwarg return self instanc session context return run func arg kwarg return wrapper opt conda lib python site packag estim fit self input wait log job experi config self prepar train job job self latest train job trainingjob start new self input experi config self job append self latest train job wait opt conda lib python site packag estim start new cl estim input experi config train arg cl train arg estim input experi config estim session train train arg return cl estim session estim current job opt conda lib python site packag session train self input mode input config role job output config resourc config vpc config hyperparamet stop condit tag metric definit enabl network isol imag uri algorithm arn encrypt inter contain traffic us spot instanc checkpoint uri checkpoint local path experi config debugg rule config debugg hook config tensorboard output config enabl metric profil rule config profil config environ retri strategi self client creat train job request self intercept creat request train request submit self train def train request noqa opt conda lib python site packag session intercept creat request self request creat func func str function need intercept return creat request opt conda lib python site packag session submit request def submit request logger info creat train job job logger debug train request json dump request indent self client creat train job request opt conda lib python json init dump obj skipkei ensur ascii check circular allow nan cl indent separ default sort kei check circular check circular allow nan allow nan indent indent separ separ default default sort kei sort kei encod obj opt conda lib python json encod encod self chunk self iterencod shot true isinst chunk list tupl chunk list chunk return join chunk opt conda lib python json encod iterencod current indent level yield iterencod list current indent level elif isinst dict yield iterencod dict current indent level marker opt conda lib python json encod iterencod dict dct current indent level chunk iterencod valu current indent level yield chunk newlin indent current indent level opt conda lib python json encod iterencod dict dct current indent level chunk iterencod valu current indent level yield chunk newlin indent current indent level opt conda lib python json encod iterencod current indent level rais valueerror circular refer detect marker markerid default yield iterencod current indent level marker opt conda lib python json encod default self rais typeerror object type class json serializ typeerror object type parameterinteg json serializ tell aspect train job serializ clear debug enorm help project templat studio show us processor huggingfac tensorflow fail grate point document requir train file need specifii huggingfac estim thank advanc",
        "Question_preprocessed_content":"studio project tldr try us estim studio project requir file term refer train data store result oper perform detail high level goal abl deploi kind model studio project given templat provid xgb outlin start tensorflow tensorflow model wrap model thought let try simpler model doc exampl start adjust abalon like find push master provid aris realis try pipelin wai sort chang rememb restart kernel time ensur chang file avail code work seri try bash code shape compil chang default train instanc type avoid cpu script stuck try pipelin aspect train job serializ clear debug enorm help project templat studio show us tensorflow fail grate point document requir file specifii estim thank advanc",
        "Question_gpt_summary_original":"The user is trying to deploy a non-XGB model from a Sagemaker Studio project, but the provided templates are all XGB. They attempted to use a HuggingFace model with the HuggingFaceProcessor\/Estimator, but encountered issues with the `train.py` file and where it should store the results of the operations it performs. They made adjustments to the code and encountered a TypeError when trying to get the pipeline from a notebook, which they are unsure how to debug further. The user is requesting documentation on the requirements for the `train.py` file for the HuggingFace Estimator or project templates for Sagemaker Studio showing the use of all the Processors.",
        "Question_gpt_summary":"user try deploi non xgb model studio project provid templat xgb attempt us huggingfac model huggingfaceprocessor estim encount issu train file store result oper perform adjust code encount typeerror try pipelin notebook unsur debug user request document requir train file huggingfac estim project templat studio show us processor"
    },
    {
        "Question_title":"I am not able to create a feature store in vertexAI using labels",
        "Question_body":"<p>I am passing the values of lables as below to create a featurestore with labels. But after creation of the featurestore, I do not see the featurestore created with labels. Is it still not supported in VertexAI<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    fs = aiplatform.Featurestore.create(\n        featurestore_id=featurestore_id,\n        labels=dict(project='retail', env='prod'),\n        online_store_fixed_node_count=online_store_fixed_node_count,\n        sync=sync\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/viOSu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/viOSu.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1651616413553,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1651623411107,
        "Question_score_count":0.0,
        "Question_view_count":83.0,
        "Poster_created_time":1530457174832,
        "Poster_location":null,
        "Poster_reputation_count":1043.0,
        "Poster_view_count":212.0,
        "Answer_body":"<p>As mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">featurestore documentation<\/a>:<\/p>\n<blockquote>\n<p>A <strong>featurestore<\/strong> is a top-level container for entity types, features,\nand feature values.<\/p>\n<\/blockquote>\n<p>With this, the GCP console UI &quot;labels&quot; are the &quot;labels&quot; at the <strong>Feature<\/strong> level.<\/p>\n<p>Once a <strong>featurestore<\/strong> is created, you will need to create an <strong>entity<\/strong> and then create a <strong>Feature<\/strong> that has the <em>labels<\/em> parameter as shown on the below sample python code.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform\n\ntest_label = {'key1' : 'value1'}\n\ndef create_feature_sample(\n    project: str,\n    location: str,\n    feature_id: str,\n    value_type: str,\n    entity_type_id: str,\n    featurestore_id: str,\n):\n\n    aiplatform.init(project=project, location=location)\n\n    my_feature = aiplatform.Feature.create(\n        feature_id=feature_id,\n        value_type=value_type,\n        entity_type_name=entity_type_id,\n        featurestore_id=featurestore_id,\n        labels=test_label,\n    )\n\n    my_feature.wait()\n\n    return my_feature\n\ncreate_feature_sample('your-project','us-central1','test_feature3','STRING','test_entity3','test_fs3')\n<\/code><\/pre>\n<p>Below is the screenshot of the GCP console which shows that <em>labels<\/em> for <strong>test_feature3<\/strong> feature has the values defined in the above sample python code.\n<a href=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-features#create-feature\" rel=\"nofollow noreferrer\">creation of feature documentation<\/a> using python for more details.<\/p>\n<p>On the other hand, you may still view the <em>labels<\/em> you defined for your featurestore using the REST API as shown on the below sample.<\/p>\n<pre><code>curl -X GET \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/&lt;your-location&gt;-aiplatform.googleapis.com\/v1\/projects\/&lt;your-project&gt;\/locations\/&lt;your-location&gt;\/featurestores&quot;\n<\/code><\/pre>\n<p>Below is the result of the REST API which also shows the value of the <em>labels<\/em> I defined for my &quot;test_fs3&quot; featurestore.\n<a href=\"https:\/\/i.stack.imgur.com\/gW45X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gW45X.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1651709813300,
        "Answer_last_edit_time":1651710305260,
        "Answer_score_count":0.0,
        "Answerer_created_time":1639972620503,
        "Answerer_location":null,
        "Answerer_reputation_count":1653.0,
        "Answerer_view_count":1212.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72106030",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"abl creat featur store vertexai label pass valu labl creat featurestor label creation featurestor featurestor creat label support vertexai aiplatform featurestor creat featurestor featurestor label dict project retail env prod onlin store fix node count onlin store fix node count sync sync",
        "Question_preprocessed_content":"abl creat featur store vertexai label valu labl creat featurestor label creation featurestor featurestor creat label vertexai",
        "Question_gpt_summary_original":"The user is unable to create a feature store in VertexAI using labels. They have passed the values of labels to create a feature store, but after creation, they are unable to see the feature store created with labels. The user is unsure if this feature is supported in VertexAI.",
        "Question_gpt_summary":"user unabl creat featur store vertexai label pass valu label creat featur store creation unabl featur store creat label user unsur featur support vertexai"
    },
    {
        "Question_title":"Is there some kind of persistent local storage in aws sagemaker model training?",
        "Question_body":"<p>I did some experimentation with aws sagemaker, and the download time of large data sets from S3 is very problematic, especially when the model is still in development, and you want some kind of initial feedback relatively fast<\/p>\n\n<p>Is there some kind of local storage or other way to speed things up?<\/p>\n\n<p><strong>EDIT<\/strong>\nI refer to the batch training service, that allows you to submit a job as a docker container.<\/p>\n\n<p>While this service is intended for already validated jobs that typically run for a long time (which makes the download time less significant) there's still a need for quick feedback<\/p>\n\n<ol>\n<li><p>There's no other way to do the \"integration\" testing of your job with the sagemaker infrastructure (configuration files, data files, etc.)<\/p><\/li>\n<li><p>When experimenting with different variations to the model, it's important to be able to get initial feedback relatively fast<\/p><\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_created_time":1516273838943,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1578086688792,
        "Question_score_count":6.0,
        "Question_view_count":5129.0,
        "Poster_created_time":1296032582892,
        "Poster_location":"Israel",
        "Poster_reputation_count":7829.0,
        "Poster_view_count":965.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48319893",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"kind persist local storag model train experiment download time larg data set problemat especi model develop want kind initi feedback rel fast kind local storag wai speed thing edit refer batch train servic allow submit job docker contain servic intend valid job typic run long time make download time signific need quick feedback wai integr test job infrastructur configur file data file experi differ variat model import abl initi feedback rel fast",
        "Question_preprocessed_content":"kind persist local storag model train experiment download time larg data set problemat model develop want kind initi rel fast kind local storag wai thing edit refer batch train servic submit job docker contain servic intend valid job run long time quick wai integr test job infrastructur experi variat model import abl initi rel fast",
        "Question_gpt_summary_original":"The user is facing challenges with the download time of large data sets from S3 while experimenting with AWS Sagemaker for model training. They are looking for a way to speed up the process and get initial feedback quickly. The user is specifically referring to the batch training service that allows submitting a job as a docker container. Although this service is intended for long-running jobs, the user still needs quick feedback for integration testing and experimenting with different model variations.",
        "Question_gpt_summary":"user face challeng download time larg data set experi model train look wai speed process initi feedback quickli user specif refer batch train servic allow submit job docker contain servic intend long run job user need quick feedback integr test experi differ model variat"
    },
    {
        "Question_title":"How to strip HTML from a text column in Azure ML Execute Python Script step",
        "Question_body":"<p>If I have a column of data of type string in an incoming Azure ML dataset that contains HTML tags screwing up my results, how can I remove those tags?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1484610622880,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":325.0,
        "Poster_created_time":1265234764768,
        "Poster_location":"Denver, CO",
        "Poster_reputation_count":30577.0,
        "Poster_view_count":6460.0,
        "Answer_body":"<p>Like this:<\/p>\n\n<pre><code>def azureml_main(dataframe1 = None, dataframe2 = None):\n  dataframe1[1] = dataframe1['text'].str.replace('&lt;[^&lt;]+?&gt;', ' ', case=False)\n  return dataframe1,\n<\/code><\/pre>\n\n<p>Remember to precede the <code>Execute Python Script<\/code> step with <code>Clean Missing Data<\/code> step and change the action to remove the entire row (if appropriate). This is important because the <code>Execute Python Script<\/code> step cannot return an empty <code>dataframe<\/code>. Only you know your data, in this case.<\/p>\n\n<p>Let me also point out that the <code>Preprocessing Text<\/code> step allows you to apply a Regular Expression. That is another alternative that might be right for your situation.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1484610622880,
        "Answer_last_edit_time":1484679092912,
        "Answer_score_count":1.0,
        "Answerer_created_time":1265234764768,
        "Answerer_location":"Denver, CO",
        "Answerer_reputation_count":30577.0,
        "Answerer_view_count":6460.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41686871",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"strip html text column execut python script step column data type string incom dataset contain html tag screw result remov tag",
        "Question_preprocessed_content":"strip html text column execut python script step column data type string incom dataset contain html tag screw result remov tag",
        "Question_gpt_summary_original":"The user is facing a challenge in removing HTML tags from a string column in an incoming Azure ML dataset to avoid interference with the results.",
        "Question_gpt_summary":"user face challeng remov html tag string column incom dataset avoid interfer result"
    },
    {
        "Question_title":"How to call azure endpoint rest api from model generated by visual studio 2019 ml builder?",
        "Question_body":"<p>I have tried visual studio 2019 model builder for object detection. I have followed a tutorial about stop sign image object detection which will use Azure for training. I then use the model generated to make inference. So far everything is working fine. I can genereate web api too which will use json input { &quot;ImageSource&quot;: &quot;path to local image&quot; } and this works too.  <\/p>\n<p>Now the problem is I am trying not to use my local cpu to do the inference. I want to use Azure to do the inference. And what I do is look for the experiment generated by model builder. Find the model and deploy the model to the endpoint.  <\/p>\n<p>Now when I go to the endpoint generated, there is a test tab there and I suppose that I need to supply the json for the inference. What is the json format needed since I try all of these and all of them not working:  <\/p>\n<ul>\n<li> just the url of the image file  <\/li>\n<li> use { &quot;url&quot; : &quot;url to the image&quot; }  <\/li>\n<li> use { &quot;imageSource&quot;: &quot;url to the image&quot; }  <\/li>\n<li> use { &quot;data&quot;: [ {&quot;url&quot; : &quot;url to the image&quot;} ] }  <\/li>\n<li> use { &quot;data&quot;: [ {&quot;imageSource&quot; : &quot;url to the image&quot;} ] }  <\/li>\n<\/ul>\n<p>And I can't find any documentation about the exact format of the json. And when I call rest api from postman\/insomnia it always says time out error.   <\/p>\n<p>Below is my deployment log when I try test.  <\/p>\n<p>Starting the inference  <br \/>\n\/azureml-envs\/azureml_a5cc75b048d996dfdd3ff5c7e66b85eb\/lib\/python3.7\/site-packages\/azureml\/contrib\/automl\/dnn\/vision\/common\/utils.py: since ignore_data_errors is True, file will be ignored.  <br \/>\nGot AutoMLVisionDataException as all images in the current batch are invalid. Skipping the batch.  <br \/>\nNumber of lines written to prediction file: 0  <br \/>\nTotal scoring time 0.0095 for 0 batches. Batch avg: 0.0000.   <br \/>\nMem stats scoring: {}.  <br \/>\nGPU stats scoring: {}{}.  <br \/>\nFinished inferencing.  <br \/>\n2021-07-08 04:15:27,849 | root | INFO | run() output is HTTP Response  <br \/>\n2021-07-08 04:15:27,849 | root | INFO | 200  <br \/>\n127.0.0.1 - - [08\/Jul\/2021:04:15:27 +0000] &quot;POST \/score?verbose=true HTTP\/1.0&quot; 200 0 &quot;-&quot; &quot;Go-http-client\/1.1&quot;  <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1625718765327,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/467346\/how-to-call-azure-endpoint-rest-api-from-model-gen",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur endpoint rest api model gener visual studio builder tri visual studio model builder object detect follow tutori stop sign imag object detect us azur train us model gener infer far work fine gener web api us json input imagesourc path local imag work problem try us local cpu infer want us azur infer look experi gener model builder model deploi model endpoint endpoint gener test tab suppos need suppli json infer json format need try work url imag file us url url imag us imagesourc url imag us data url url imag us data imagesourc url imag document exact format json rest api postman insomnia sai time error deploy log try test start infer env accbddfddffcebeb lib python site packag contrib automl dnn vision common util ignor data error true file ignor got automlvisiondataexcept imag current batch invalid skip batch number line written predict file total score time batch batch avg mem stat score gpu stat score finish inferenc root info run output http respons root info jul post score verbos true http http client",
        "Question_preprocessed_content":"azur endpoint rest api model gener visual studio builder tri visual studio model builder object detect tutori stop sign imag object detect us azur train us model gener infer far work fine gener web api us json input work problem try us local cpu infer want us azur infer experi gener model builder model deploi model endpoint endpoint gener test tab json infer json format try work url imag file us us us us document exact format json rest api sai time deploy log try test start infer true file ignor got automlvisiondataexcept imag batch invalid batch number line predict file total score time batch batch avg mem stat score gpu stat score finish inferenc info run output respons info post score verbos true",
        "Question_gpt_summary_original":"The user has generated a model using Visual Studio 2019 model builder for object detection and has successfully used it to generate a web API. However, they are facing issues while trying to use Azure to do the inference instead of their local CPU. The user is unable to find documentation about the exact format of the JSON needed to supply the inference and is receiving a timeout error when calling the REST API from Postman\/Insomnia. The deployment log shows an AutoMLVisionDataException and a time-out error.",
        "Question_gpt_summary":"user gener model visual studio model builder object detect successfulli gener web api face issu try us azur infer instead local cpu user unabl document exact format json need suppli infer receiv timeout error call rest api postman insomnia deploy log show automlvisiondataexcept time error"
    },
    {
        "Question_title":"Failed to write to mlflow after deleting experiment via mlflow UI",
        "Question_body":"<p>I'm using mlflow version 1.18.0<\/p>\n<p>When I delete experiment from <code>mlflow<\/code> <em>UI<\/em>, and than try to create and write a new experiment (with same name which I just deleted) I'm getting error on this line code:<\/p>\n<pre><code>mlflow.start_run(run_name=run_name)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>error mlflow.util.rest_util API resest to faild with code 500 != 200\n<\/code><\/pre>\n<p>If I change the experiment name, I have no problem to write new tests.<\/p>\n<ol>\n<li>Why is this happening ? (as I wrote, I delete the experiment name)<\/li>\n<li>Is there a way to solve it (without giving new experiment name) ?<\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1651576965597,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":45.0,
        "Poster_created_time":1400833595280,
        "Poster_location":null,
        "Poster_reputation_count":3934.0,
        "Poster_view_count":416.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72098661",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"fail write delet experi version delet experi try creat write new experi delet get error line code start run run run error error util rest util api resest faild code chang experi problem write new test happen wrote delet experi wai solv give new experi",
        "Question_preprocessed_content":"fail write delet experi version delet experi try creat write new experi line code chang experi problem write new test wai solv",
        "Question_gpt_summary_original":"The user is facing an issue while trying to create and write a new experiment with the same name after deleting an experiment from the mlflow UI. The error message \"error mlflow.util.rest_util API resest to faild with code 500 != 200\" is displayed when the user tries to run the code. The user is seeking answers to why this is happening and if there is a way to solve it without giving a new experiment name.",
        "Question_gpt_summary":"user face issu try creat write new experi delet experi error messag error util rest util api resest faild code displai user tri run code user seek answer happen wai solv give new experi"
    },
    {
        "Question_title":"What is the difference between deploying models in MLflow and Sagemaker?",
        "Question_body":"<p>I could do\n<code>mlflow model serve -m &lt;RUN_ID&gt; --p 1234 --no-conda<\/code><\/p>\n<p>and<\/p>\n<p><code>mlflow sagemaker run-local -m &lt;MODEL_PATH&gt; -p 1234<\/code><\/p>\n<p>Are they not the same anyway as both can do model serving so what's the hassle deploying it to Sagemaker?<\/p>\n<p>I'm a beginner at this so if anyone can help me out with my understanding that will be great. Thank you so much in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1663806620887,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":26.0,
        "Poster_created_time":1487831504143,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73808171",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"differ deploi model model serv conda run local model serv hassl deploi beginn help understand great thank advanc",
        "Question_preprocessed_content":"deploi model model serv deploi help understand great thank advanc",
        "Question_gpt_summary_original":"The user is seeking clarification on the difference between deploying models in MLflow and Sagemaker, as both can serve models. They are wondering if there is any benefit to deploying models in Sagemaker.",
        "Question_gpt_summary":"user seek clarif differ deploi model serv model wonder benefit deploi model"
    },
    {
        "Question_title":"Can't go to former version of dataset with `dvc checkout`",
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying out DVC for the first time for my deep learning development pipeline. As mentioned in the <a href=\"https:\/\/youtu.be\/kLKBcPonMYw\" rel=\"noopener nofollow ugc\">Version Data with DVC<\/a> tutorial, I created a folder named \u201cdataset\u201d in the main directory and then I added some data into it. And then  I used <code>dvc add dataset<\/code> to track the dataset. After that, I included \u201cdataset\u201d folder in the .gitignore to untrack the dataset with git.<\/p>\n<p>Then I setup Azure storage blob for my remote data repo. After adding some more data, I wanted to go back to the original version of the data so I checkout to that git commit and then used <code>dvc checkout<\/code> command. Then this error comes up.<\/p>\n<pre><code class=\"lang-auto\">+--------------------------------------------------+\n|                                                  |\n|        Update available 1.10.2 -&gt; 1.11.10        |\n|     Run `apt-get install --only-upgrade dvc`     |\n|                                                  |\n+--------------------------------------------------+\n\nWARNING: Cache 'HashInfo(name='md5', value='db4287e726604a63a231cf4462cb27df.dir', dir_info=None, size=497535926, nfiles=8818)' not found. File 'dataset' won't be created.\nERROR: Checkout failed for following targets:\ndataset\nIs your cache up to date?\n&lt;https:\/\/error.dvc.org\/missing-files&gt;\n<\/code><\/pre>\n<p>I tried searching for the solution in the given link but I could find it. Thanks in advance.<\/p>",
        "Question_answer_count":14,
        "Question_comment_count":null,
        "Question_created_time":1610450954470,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":2550.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/cant-go-to-former-version-of-dataset-with-dvc-checkout\/615",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"version dataset checkout try time deep learn develop pipelin mention version data tutori creat folder name dataset main directori ad data add dataset track dataset includ dataset folder gitignor untrack dataset git setup azur storag blob remot data repo ad data want origin version data checkout git commit checkout command error come updat avail run apt instal upgrad warn cach hashinfo valu dbeaacfcbdf dir dir info size nfile file dataset won creat error checkout fail follow target dataset cach date tri search solut given link thank advanc",
        "Question_preprocessed_content":"version dataset try time learn develop pipelin mention version data tutori creat folder name dataset main directori data track dataset includ dataset folder gitignor untrack dataset git setup azur storag blob remot data repo data want origin version data checkout git come tri search solut given link thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error while trying to go back to the original version of the data using the `dvc checkout` command. The error message suggests that the cache is not up to date and a cache file is missing, resulting in the failure of the checkout process. The user has tried to find a solution but was unsuccessful.",
        "Question_gpt_summary":"user encount error try origin version data checkout command error messag suggest cach date cach file miss result failur checkout process user tri solut unsuccess"
    },
    {
        "Question_title":"How does autoML actually calculate feature importance?",
        "Question_body":"<p>Do they use a sklearn feature selection method? I want to learn how they actually decide what features are more important than others (the ones shown in explain model). Going through the generated code doesn't reveal much to me.  <br \/>\nI see ExtraTreesRegressor, LGBMRegressor, PreFittedSoftVotingRegressor but I don't know if they're used for feature importance or something else.  <\/p>\n<p>Any advice really appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1645707636143,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/749298\/how-does-automl-actually-calculate-feature-importa",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"automl actual calcul featur import us sklearn featur select method want learn actual decid featur import on shown explain model go gener code reveal extratreesregressor lgbmregressor prefittedsoftvotingregressor know featur import advic appreci",
        "Question_preprocessed_content":"automl calcul featur import us sklearn featur select method want learn decid featur import go gener code reveal know featur import advic",
        "Question_gpt_summary_original":"The user is seeking information on how autoML calculates feature importance and is unsure if sklearn feature selection methods are used. They are also unsure if ExtraTreesRegressor, LGBMRegressor, and PreFittedSoftVotingRegressor are used for feature importance or something else.",
        "Question_gpt_summary":"user seek inform automl calcul featur import unsur sklearn featur select method unsur extratreesregressor lgbmregressor prefittedsoftvotingregressor featur import"
    },
    {
        "Question_title":"Machine Learning Code",
        "Question_body":"<p>Hey,   <\/p>\n<p>I am developing an ASP.Net application and I would like to integrate machine learning into my app. But let the result appear in a window of my app and not on the console. Can you tell me how to do it please?  <\/p>\n<p>Thank you in advance  <br \/>\nI look forward to your response  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1593980194230,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/43091\/machine-learning-code",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"machin learn code hei develop asp net applic like integr machin learn app let result appear window app consol tell thank advanc look forward respons",
        "Question_preprocessed_content":"machin learn code hei develop like integr machin learn let result window consol thank advanc forward respons",
        "Question_gpt_summary_original":"The user is developing an ASP.Net application and wants to integrate machine learning into it. They are seeking guidance on how to display the results in a window within their application instead of on the console.",
        "Question_gpt_summary":"user develop asp net applic want integr machin learn seek guidanc displai result window applic instead consol"
    },
    {
        "Question_title":"Read bytes file from AWS S3 into AWS SageMaker conda_python3",
        "Question_body":"<p>Good morning,\nYesterday I saved a file from SageMaker conda_python3 to S3 like this:<\/p>\n\n<pre><code>s3 = boto3.client(\n            's3',\n            aws_access_key_id='XXXX',\n            aws_secret_access_key='XXXX'\n        )\ny = pandas.DataFrame(df.tag_factor,index = df.index)\ns3.put_object(Body = y.values.tobytes(), Bucket='xxx', Key='xxx')\n<\/code><\/pre>\n\n<p>Today I am trying to open it with conda_python3 as a pandas.Series or as a numpy.array object, with this code:<\/p>\n\n<pre><code>s3 = boto3.client(\n            's3',\n            aws_access_key_id='XXX',\n            aws_secret_access_key='XXX'\n        )\ny_bytes = s3.get_object(Bucket='xxx', Key='xxx')\ny = numpy.load(io.BytesIO(y_bytes['Body'].read()))\n<\/code><\/pre>\n\n<p>but I am getting this error: OSError: Failed to interpret file &lt;_io.BytesIO >object at 0x7fcb0b403258> as a pickle<\/p>\n\n<p>I tried this:<\/p>\n\n<pre><code>y = numpy.fromfile(io.BytesIO(y_bytes['Body'].read()))\n<\/code><\/pre>\n\n<p>and I get this error:<\/p>\n\n<blockquote>\n  <p>UnsupportedOperation: fileno<\/p>\n<\/blockquote>\n\n<p>I tried this:<\/p>\n\n<pre><code>y = pd.read_csv(io.BytesIO(y_bytes['Body'].read()), sep=\" \", header=None)\n<\/code><\/pre>\n\n<p>and I get this error:<\/p>\n\n<blockquote>\n  <p>EmptyDataError: No columns to parse from file<\/p>\n<\/blockquote>\n\n<p>How can I read this file?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1544802647203,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1605707872200,
        "Question_score_count":1.0,
        "Question_view_count":1398.0,
        "Poster_created_time":1458416707412,
        "Poster_location":"Ecuador, Quito",
        "Poster_reputation_count":617.0,
        "Poster_view_count":117.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53782956",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"read byte file aw conda python good morn yesterdai save file conda python like boto client aw access kei xxxx aw secret access kei xxxx panda datafram tag factor index index object bodi valu tobyt bucket xxx kei xxx todai try open conda python panda seri numpi arrai object code boto client aw access kei xxx aw secret access kei xxx byte object bucket xxx kei xxx numpi load bytesio byte bodi read get error oserror fail interpret file object xfcbb pickl tri numpi fromfil bytesio byte bodi read error unsupportedoper fileno tri read csv bytesio byte bodi read sep header error emptydataerror column pars file read file",
        "Question_preprocessed_content":"read byte file aw morn yesterdai save file like todai try open object code fail interpret file object fcb pickl tri fileno tri column pars file read file",
        "Question_gpt_summary_original":"The user is facing challenges in reading a file from AWS S3 into AWS SageMaker conda_python3 as a pandas.Series or as a numpy.array object. The user has tried different methods such as numpy.fromfile and pd.read_csv but is encountering errors such as OSError, UnsupportedOperation, and EmptyDataError. The user is seeking guidance on how to read the file.",
        "Question_gpt_summary":"user face challeng read file aw conda python panda seri numpi arrai object user tri differ method numpi fromfil read csv encount error oserror unsupportedoper emptydataerror user seek guidanc read file"
    },
    {
        "Question_title":"How to train MLM model XLM Roberta large on google machine specs fastly with less memory",
        "Question_body":"<p>I am fine tuning masked language model from <code>XLM Roberta large<\/code> on google machine specs.\nI made couple of experiments and was strange to see few results.<\/p>\n<pre><code>&quot;a2-highgpu-4g&quot; ,accelerator_count=4, accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 4 Running ( 4 data*4 GPU=16 data points)\n&quot;a2-highgpu-4g&quot; ,accelerator_count=4 , accelerator_type=&quot;NVIDIA_TESLA_A100&quot;on 4,12,672 data batch size 8 failed\n &quot;a2-highgpu-4g&quot; ,accelerator_count=4, accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 16 failed\n&quot;a2-highgpu-4g&quot; ,accelerator_count=4.,accelerator_type=&quot;NVIDIA_TESLA_A100&quot; on 4,12,672 data batch size 32 failed\n<\/code><\/pre>\n<p>I was not able to train model with <code>batch size <\/code> more than 4 on # of GPU's. It stopped in mid-way.<\/p>\n<p>Here is the code I am using.<\/p>\n<pre><code>training_args = tr.TrainingArguments(\n#     disable_tqdm=True,\n    output_dir='\/home\/pc\/Bert_multilingual_exp_TCM\/results_mlm_exp2', \n    overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=4,\n#     per_device_train_batch_size\n#     per_gpu_train_batch_size\n    prediction_loss_only=True\n    ,save_strategy=&quot;no&quot;\n    ,run_name=&quot;MLM_Exp1&quot;\n    ,learning_rate=2e-5\n    ,logging_dir='\/home\/pc\/Bert_multilingual_exp_TCM\/logs_mlm_exp1'        # directory for storing logs\n    ,logging_steps=40000\n    ,logging_strategy='no'\n)\n\ntrainer = tr.Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data\n    \n)\n<\/code><\/pre>\n<p><strong>My Questions<\/strong><\/p>\n<p>How can I train with larger batch size on <code>a2-highgpu-4g<\/code> machine?<\/p>\n<p>Which parameters can I include in <code>TrainingArguments<\/code> so that training is fast and occupies small memory?<\/p>\n<p>Thanks in advance.<\/p>\n<h3>Versions<\/h3>\n<pre><code>torch==1.11.0+cu113 \n\ntorchvision==0.12.0+cu113  \n\ntorchaudio==0.11.0+cu113 \n\ntransformers==4.17.0\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1647445884443,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":210.0,
        "Poster_created_time":1528361086052,
        "Poster_location":null,
        "Poster_reputation_count":1127.0,
        "Poster_view_count":283.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71500193",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"train mlm model xlm roberta larg googl machin spec fastli memori fine tune mask languag model xlm roberta larg googl machin spec coupl experi strang result highgpu acceler count acceler type nvidia tesla data batch size run data gpu data point highgpu acceler count acceler type nvidia tesla data batch size fail highgpu acceler count acceler type nvidia tesla data batch size fail highgpu acceler count acceler type nvidia tesla data batch size fail abl train model batch size gpu stop mid wai code train arg trainingargu disabl tqdm true output dir home bert multilingu exp tcm result mlm exp overwrit output dir true num train epoch devic train batch size devic train batch size gpu train batch size predict loss true save strategi run mlm exp learn rate log dir home bert multilingu exp tcm log mlm exp directori store log log step log strategi trainer trainer model model arg train arg data collat data collat train dataset train data question train larger batch size highgpu machin paramet includ trainingargu train fast occupi small memori thank advanc version torch torchvis torchaudio transform",
        "Question_preprocessed_content":"train mlm model xlm roberta larg machin spec fastli memori fine tune mask languag model machin spec coupl experi strang result abl train model gpu code question train larger batch size machin paramet includ train fast memori thank advanc version",
        "Question_gpt_summary_original":"The user is trying to fine-tune a masked language model from XLM Roberta large on Google machine specs but is unable to train the model with a batch size larger than 4 on multiple GPUs. The user is seeking advice on how to train with a larger batch size and which parameters to include in TrainingArguments to make the training faster and occupy less memory. The user has provided the code and versions used for reference.",
        "Question_gpt_summary":"user try fine tune mask languag model xlm roberta larg googl machin spec unabl train model batch size larger multipl gpu user seek advic train larger batch size paramet includ trainingargu train faster occupi memori user provid code version refer"
    },
    {
        "Question_title":"Azure ML: Upload File to Step Run's Output - Authentication Error",
        "Question_body":"<p>During a PythonScriptStep in an Azure ML Pipeline, I'm saving a model as joblib pickle dump to a directory in a Blob Container in the Azure Blob Storage which I've created during the setup of the Azure ML Workspace. Afterwards I'm trying to upload this model file to the step run's output directory using<\/p>\n<pre><code>Run.upload_file (name, path_or_stream)\n<\/code><\/pre>\n<p>(for the function's documentation, see <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#upload-file-name--path-or-stream--datastore-name-none-\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#upload-file-name--path-or-stream--datastore-name-none-<\/a>)<\/p>\n<p>Some time ago when I created the script using the azureml-sdk version 1.18.0, everything worked fine. Now, I've updated the script's functionalities and upgraded the azureml-sdk to version 1.33.0 during the process and the upload function now runs into the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/upload.py&quot;, line 64, in upload_blob_from_stream\n    validate_content=True)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py&quot;, line 93, in execute_func_with_reset\n    return ClientBase._execute_func_internal(backoff, retries, module_logger, func, reset_func, *args, **kwargs)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py&quot;, line 367, in _execute_func_internal\n    left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py&quot;, line 399, in _handle_retry\n    raise error\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py&quot;, line 358, in _execute_func_internal\n    response = func(*args, **kwargs)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py&quot;, line 614, in create_blob_from_stream\n    initialization_vector=iv\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py&quot;, line 98, in _upload_blob_chunks\n    range_ids = [f.result() for f in futures]\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py&quot;, line 98, in &lt;listcomp&gt;\n    range_ids = [f.result() for f in futures]\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/concurrent\/futures\/_base.py&quot;, line 435, in result\n    return self.__get_result()\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/concurrent\/futures\/_base.py&quot;, line 384, in __get_result\n    raise self._exception\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py&quot;, line 210, in process_chunk\n    return self._upload_chunk_with_progress(chunk_offset, chunk_bytes)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py&quot;, line 224, in _upload_chunk_with_progress\n    range_id = self._upload_chunk(chunk_offset, chunk_data)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/_upload_chunking.py&quot;, line 269, in _upload_chunk\n    timeout=self.timeout,\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/blob\/blockblobservice.py&quot;, line 1013, in _put_block\n    self._perform_request(request)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py&quot;, line 432, in _perform_request\n    raise ex\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py&quot;, line 357, in _perform_request\n    raise ex\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/common\/storageclient.py&quot;, line 343, in _perform_request\n    HTTPError(response.status, response.message, response.headers, response.body))\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_vendor\/azure_storage\/common\/_error.py&quot;, line 115, in _http_error_handler\n    raise ex\nazure.common.AzureHttpError: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. ErrorCode: AuthenticationFailed\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;Error&gt;&lt;Code&gt;AuthenticationFailed&lt;\/Code&gt;&lt;Message&gt;Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\nRequestId:5d4e1b7e-c01e-0070-0d47-9bf8a0000000\nTime:2021-08-27T13:30:02.2685991Z&lt;\/Message&gt;&lt;AuthenticationErrorDetail&gt;Signature did not match. String to sign used was rcw\n2021-08-27T13:19:56Z\n2021-08-28T13:29:56Z\n\/blob\/mystorage\/azureml\/ExperimentRun\/dcid.98d11a7b-2aac-4bc0-bd64-bb4d72e0e0be\/outputs\/models\/Model.pkl\n\n2019-07-07\nb\n\n&lt;\/AuthenticationErrorDetail&gt;&lt;\/Error&gt;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/...\/azureml-setup\/context_manager_injector.py&quot;, line 243, in execute_with_context\n    runpy.run_path(sys.argv[0], globals(), run_name=&quot;__main__&quot;)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/runpy.py&quot;, line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/runpy.py&quot;, line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py&quot;, line 318, in &lt;module&gt;\n    main()\n  File &quot;401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py&quot;, line 286, in main\n    path_or_stream=model_path)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/core\/run.py&quot;, line 53, in wrapped\n    return func(self, *args, **kwargs)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/core\/run.py&quot;, line 1989, in upload_file\n    datastore_name=datastore_name)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py&quot;, line 114, in upload_artifact\n    return self.upload_artifact_from_path(artifact, *args, **kwargs)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py&quot;, line 107, in upload_artifact_from_path\n    return self.upload_artifact_from_stream(stream, *args, **kwargs)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py&quot;, line 99, in upload_artifact_from_stream\n    content_type=content_type, session=session)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py&quot;, line 88, in upload_stream_to_existing_artifact\n    timeout=TIMEOUT, backoff=BACKOFF_START, retries=RETRY_LIMIT)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/upload.py&quot;, line 71, in upload_blob_from_stream\n    raise AzureMLException._with_error(azureml_error, inner_exception=e)\nazureml._common.exceptions.AzureMLException: AzureMLException:\n    Message: Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\n    StorageAccount: mystorage\n    ContainerName: azureml\n    StatusCode: 403\n    InnerException Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. ErrorCode: AuthenticationFailed\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;Error&gt;&lt;Code&gt;AuthenticationFailed&lt;\/Code&gt;&lt;Message&gt;Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\nRequestId:5d4e1b7e-c01e-0070-0d47-9bf8a0000000\nTime:2021-08-27T13:30:02.2685991Z&lt;\/Message&gt;&lt;AuthenticationErrorDetail&gt;Signature did not match. String to sign used was rcw\n2021-08-27T13:19:56Z\n2021-08-28T13:29:56Z\n\/blob\/mystorage\/azureml\/ExperimentRun\/dcid.98d11a7b-2aac-4bc0-bd64-bb4d72e0e0be\/outputs\/models\/Model.pkl\n\n2019-07-07\nb\n\n&lt;\/AuthenticationErrorDetail&gt;&lt;\/Error&gt;\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\\n\\tStorageAccount: mystorage\\n\\tContainerName: azureml\\n\\tStatusCode: 403&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Auth&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;Authorization&quot;\n            }\n        }\n    }\n}\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py&quot;, line 318, in &lt;module&gt;\n    main()\n  File &quot;401_AML_Pipeline_Time_Series_Model_Training_Azure_ML_CPU.py&quot;, line 286, in main\n    path_or_stream=model_path)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/core\/run.py&quot;, line 53, in wrapped\n    return func(self, *args, **kwargs)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/core\/run.py&quot;, line 1989, in upload_file\n    datastore_name=datastore_name)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py&quot;, line 114, in upload_artifact\n    return self.upload_artifact_from_path(artifact, *args, **kwargs)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py&quot;, line 107, in upload_artifact_from_path\n    return self.upload_artifact_from_stream(stream, *args, **kwargs)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py&quot;, line 99, in upload_artifact_from_stream\n    content_type=content_type, session=session)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_restclient\/artifacts_client.py&quot;, line 88, in upload_stream_to_existing_artifact\n    timeout=TIMEOUT, backoff=BACKOFF_START, retries=RETRY_LIMIT)\n  File &quot;\/opt\/miniconda\/lib\/python3.7\/site-packages\/azureml\/_file_utils\/upload.py&quot;, line 71, in upload_blob_from_stream\n    raise AzureMLException._with_error(azureml_error, inner_exception=e)\nUserScriptException: UserScriptException:\n    Message: Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\n    StorageAccount: mystorage\n    ContainerName: azureml\n    StatusCode: 403\n    InnerException AzureMLException:\n    Message: Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\n    StorageAccount: mystorage\n    ContainerName: azureml\n    StatusCode: 403\n    InnerException Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. ErrorCode: AuthenticationFailed\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;&lt;Error&gt;&lt;Code&gt;AuthenticationFailed&lt;\/Code&gt;&lt;Message&gt;Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.\nRequestId:5d4e1b7e-c01e-0070-0d47-9bf8a0000000\nTime:2021-08-27T13:30:02.2685991Z&lt;\/Message&gt;&lt;AuthenticationErrorDetail&gt;Signature did not match. String to sign used was rcw\n2021-08-27T13:19:56Z\n2021-08-28T13:29:56Z\n\/blob\/mystorage\/azureml\/ExperimentRun\/dcid.98d11a7b-2aac-4bc0-bd64-bb4d72e0e0be\/outputs\/models\/Model.pkl\n\n2019-07-07\nb\n\n&lt;\/AuthenticationErrorDetail&gt;&lt;\/Error&gt;\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\\n\\tStorageAccount: verovisionstorage\\n\\tContainerName: azureml\\n\\tStatusCode: 403&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Auth&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;Authorization&quot;\n            }\n        }\n    }\n}\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Encountered authorization error while uploading to blob storage. Please check the storage account attached to your workspace. Make sure that the current user is authorized to access the storage account and that the request is not blocked by a firewall, virtual network, or other security setting.\\n\\tStorageAccount: mystorage\\n\\tContainerName: azureml\\n\\tStatusCode: 403&quot;\n    }\n}\n<\/code><\/pre>\n<p>As far as I can tell from the code of the azureml.core.Run class and the subsequent function calls, the Run object tries to upload the file to the step run's output directory using SAS-Token-Authentication (which fails). This documentation article is linked in the code (but I don't know if this relates to the issue): <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/create-service-sas#service-sas-example\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/rest\/api\/storageservices\/create-service-sas#service-sas-example<\/a><\/p>\n<p>Did anybody encounter this error as well and knows what causes it or how it can be resolved?<\/p>\n<p>Best,\nJonas<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1630074825080,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":215.0,
        "Poster_created_time":1594877973727,
        "Poster_location":"Germany",
        "Poster_reputation_count":51.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68955197",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"upload file step run output authent error pythonscriptstep pipelin save model joblib pickl dump directori blob contain azur blob storag creat setup workspac try upload model file step run output directori run upload file path stream function document http doc microsoft com python api core core run class view azur upload file path stream datastor time ago creat script sdk version work fine updat script function upgrad sdk version process upload function run follow error traceback recent file opt miniconda lib python site packag file util upload line upload blob stream valid content true file opt miniconda lib python site packag restclient clientbas line execut func reset return clientbas execut func intern backoff retri modul logger func reset func arg kwarg file opt miniconda lib python site packag restclient clientbas line execut func intern left retri cl handl retri left retri total retri error logger func file opt miniconda lib python site packag restclient clientbas line handl retri rais error file opt miniconda lib python site packag restclient clientbas line execut func intern respons func arg kwarg file opt miniconda lib python site packag vendor azur storag blob blockblobservic line creat blob stream initi vector file opt miniconda lib python site packag vendor azur storag blob upload chunk line upload blob chunk rang id result futur file opt miniconda lib python site packag vendor azur storag blob upload chunk line rang id result futur file opt miniconda lib python concurr futur base line result return self result file opt miniconda lib python concurr futur base line result rais self except file opt miniconda lib python concurr futur thread line run result self self arg self kwarg file opt miniconda lib python site packag vendor azur storag blob upload chunk line process chunk return self upload chunk progress chunk offset chunk byte file opt miniconda lib python site packag vendor azur storag blob upload chunk line upload chunk progress rang self upload chunk chunk offset chunk data file opt miniconda lib python site packag vendor azur storag blob upload chunk line upload chunk timeout self timeout file opt miniconda lib python site packag vendor azur storag blob blockblobservic line block self perform request request file opt miniconda lib python site packag vendor azur storag common storagecli line perform request rais file opt miniconda lib python site packag vendor azur storag common storagecli line perform request rais file opt miniconda lib python site packag vendor azur storag common storagecli line perform request httperror respons statu respons messag respons header respons bodi file opt miniconda lib python site packag vendor azur storag common error line http error handler rais azur common azurehttperror server fail authent request sure valu author header form correctli includ signatur errorcod authenticationfail authenticationfailedserv fail authent request sure valu author header form correctli includ signatur requestid debe bfa time zsignatur match string sign rcw blob mystorag experimentrun dcid dab aac bbdeeb output model model pkl handl except except occur traceback recent file mnt batch task share root job setup context manag injector line execut context runpi run path sy argv global run main file opt miniconda lib python runpi line run path pkg pkg script fname file opt miniconda lib python runpi line run modul code mod mod spec pkg script file opt miniconda lib python runpi line run code exec code run global file aml pipelin time seri model train azur cpu line main file aml pipelin time seri model train azur cpu line main path stream model path file opt miniconda lib python site packag core run line wrap return func self arg kwarg file opt miniconda lib python site packag core run line upload file datastor datastor file opt miniconda lib python site packag restclient artifact client line upload artifact return self upload artifact path artifact arg kwarg file opt miniconda lib python site packag restclient artifact client line upload artifact path return self upload artifact stream stream arg kwarg file opt miniconda lib python site packag restclient artifact client line upload artifact stream content type content type session session file opt miniconda lib python site packag restclient artifact client line upload stream exist artifact timeout timeout backoff backoff start retri retri limit file opt miniconda lib python site packag file util upload line upload blob stream rais except error error inner except common except except except messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set storageaccount mystorag containernam statuscod innerexcept server fail authent request sure valu author header form correctli includ signatur errorcod authenticationfail authenticationfailedserv fail authent request sure valu author header form correctli includ signatur requestid debe bfa time zsignatur match string sign rcw blob mystorag experimentrun dcid dab aac bbdeeb output model model pkl errorrespons error code usererror messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set tstorageaccount mystorag tcontainernam tstatuscod inner error code auth inner error code author handl except except occur traceback recent file aml pipelin time seri model train azur cpu line main file aml pipelin time seri model train azur cpu line main path stream model path file opt miniconda lib python site packag core run line wrap return func self arg kwarg file opt miniconda lib python site packag core run line upload file datastor datastor file opt miniconda lib python site packag restclient artifact client line upload artifact return self upload artifact path artifact arg kwarg file opt miniconda lib python site packag restclient artifact client line upload artifact path return self upload artifact stream stream arg kwarg file opt miniconda lib python site packag restclient artifact client line upload artifact stream content type content type session session file opt miniconda lib python site packag restclient artifact client line upload stream exist artifact timeout timeout backoff backoff start retri retri limit file opt miniconda lib python site packag file util upload line upload blob stream rais except error error inner except userscriptexcept userscriptexcept messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set storageaccount mystorag containernam statuscod innerexcept except messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set storageaccount mystorag containernam statuscod innerexcept server fail authent request sure valu author header form correctli includ signatur errorcod authenticationfail authenticationfailedserv fail authent request sure valu author header form correctli includ signatur requestid debe bfa time zsignatur match string sign rcw blob mystorag experimentrun dcid dab aac bbdeeb output model model pkl errorrespons error code usererror messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set tstorageaccount verovisionstorag tcontainernam tstatuscod inner error code auth inner error code author errorrespons error code usererror messag encount author error upload blob storag check storag account attach workspac sure current user author access storag account request block firewal virtual network secur set tstorageaccount mystorag tcontainernam tstatuscod far tell code core run class subsequ function call run object tri upload file step run output directori sa token authent fail document articl link code know relat issu http doc microsoft com rest api storageservic creat servic sa servic sa exampl anybodi encount error know caus resolv best jona",
        "Question_preprocessed_content":"upload file step run output authent pythonscriptstep pipelin save model joblib pickl dump directori blob contain azur blob storag creat setup workspac try upload model file step run output directori function document time ago creat script sdk version work fine updat script function upgrad sdk version upload function run far code subsequ function run object tri upload file step run output directori document articl link code anybodi encount know caus resolv best jona",
        "Question_gpt_summary_original":"the user encountered an authentication error while attempting to upload a file to the step run's output directory using run.upload_file, which was caused by a sas-token authentication failure.",
        "Question_gpt_summary":"user encount authent error attempt upload file step run output directori run upload file caus sa token authent failur"
    },
    {
        "Question_title":"how to fit learning rate with pruning?",
        "Question_body":"<p>The background for the question is optimizing hyper params of neural network training by running <code>study.optimize()<\/code> with default pruning enabled and learning rate as parameter to optimize (this question can be generalized to other hyperparams).<\/p>\n\n<p>high learning rates can have good results initially lowering the loss substantially every intermediate step at first but results in poor final result.<\/p>\n\n<p>low learning rates can have poor initial results but better end value.<\/p>\n\n<p>with pruning enabled wouldn't low learning rate be pruned because of poor intermediate results? which means the final best param would be incorrectly selected?<\/p>\n\n<p>how can this be fixed?\nwould using another pruner help?\nthanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1565506097393,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1565526490023,
        "Question_score_count":3.0,
        "Question_view_count":222.0,
        "Poster_created_time":1448461981527,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57447918",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"fit learn rate prune background question optim hyper param neural network train run studi optim default prune enabl learn rate paramet optim question gener hyperparam high learn rate good result initi lower loss substanti intermedi step result poor final result low learn rate poor initi result better end valu prune enabl wouldn low learn rate prune poor intermedi result mean final best param incorrectli select fix pruner help thank",
        "Question_preprocessed_content":"fit learn rate prune background question optim hyper param neural network train default prune enabl learn rate paramet optim high learn rate result lower intermedi step result final result low learn rate initi result end valu prune enabl wouldn low learn rate prune intermedi result mean final best param select fix pruner help thank",
        "Question_gpt_summary_original":"The user is trying to optimize hyperparameters of a neural network using default pruning and learning rate as a parameter to optimize. They are concerned that low learning rates may be pruned due to poor intermediate results, leading to incorrect final parameter selection. They are seeking advice on how to fix this issue, including the possibility of using another pruner.",
        "Question_gpt_summary":"user try optim hyperparamet neural network default prune learn rate paramet optim concern low learn rate prune poor intermedi result lead incorrect final paramet select seek advic fix issu includ possibl pruner"
    },
    {
        "Question_title":"Free tier usage Sagemaker",
        "Question_body":"Hi, I have a question about the free tier usage of AWS Sagemaker.   \nIt says on the free tier page:   \n  \nAmazon SageMaker  \n250 Hours  \nper month of t2.medium notebook usage for the first two months  \n  \nDoes this mean that you have to start using Sagemaker right away when you start your account or your two months will be void? Or does this mean that the first two months you're using Sagemaker, up to 250 hours per month of t2.medium notebook usage is free?  \n  \nI really just want to test something but I have to know if I can still do this for free. I've had my account since september 2018 so >2 months.   \n  \nThe FAQ page that's referred to about expiring offers FREE TRIAL (What's the difference between expiring and non-expiring offers? VIEW FAQs >> https:\/\/aws.amazon.com\/free\/faqs\/?ft=n) doesn't give an answer to my question.   \n  \nThanks in advance.",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_created_time":1548335489000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668454454976,
        "Question_score_count":0.0,
        "Question_view_count":326.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWzc5HuoeQZOuo79E8SQwxg\/free-tier-usage-sagemaker",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"free tier usag question free tier usag sai free tier page hour month medium notebook usag month mean start right awai start account month void mean month hour month medium notebook usag free want test know free account septemb month faq page refer expir offer free trial differ expir non expir offer view faq http aw amazon com free faq answer question thank advanc",
        "Question_preprocessed_content":"tier usag question tier usag sai tier page hour month usag month mean start right awai start month void mean month hour month usag want test know septemb month faq page expir trial answer question thank advanc",
        "Question_gpt_summary_original":"The user is inquiring about the free tier usage of AWS Sagemaker and is unsure if they can still use it for free after the first two months. They are seeking clarification on whether the 250 hours per month of t2.medium notebook usage is only applicable during the first two months of account creation or if it can be used anytime within the free tier period. The user has checked the FAQ page but did not find an answer to their question.",
        "Question_gpt_summary":"user inquir free tier usag unsur us free month seek clarif hour month medium notebook usag applic month account creation anytim free tier period user check faq page answer question"
    },
    {
        "Question_title":"AutoML MLFlow Batch Endpoint - Image build failed",
        "Question_body":"<p>When deploying an MLFlow model generated by Azure AutoML as a batch endpoint, using no-code option, the following &quot;Image build failed&quot; error occurs:<\/p>\n<pre><code>ERROR: Ignored the following versions that require a different python version: 0.7 Requires-Python &gt;=3.6, &lt;3.7; 0.8 Requires-Python &gt;=3.6, &lt;3.7; 1.19.0 Requires-Python &gt;=3.5,&lt;3.8; 1.20.0 Requires-Python &gt;=3.5,&lt;3.8; 1.20.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.21.0 Requires-Python &gt;=3.5,&lt;3.8; 1.21.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.22.0 Requires-Python &gt;=3.5,&lt;3.8; 1.22.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.22.0.post2 Requires-Python &gt;=3.5,&lt;3.8; 1.22.0.post2 Requires-Python &gt;=3.6,&lt;3.8; 1.23.0 Requires-Python &gt;=3.5,&lt;3.8; 1.23.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.24.0 Requires-Python &gt;=3.5,&lt;3.8; 1.25.0 Requires-Python &gt;=3.5,&lt;3.8; 1.25.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.26.0 Requires-Python &gt;=3.5,&lt;3.8; 1.26.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.27.0 Requires-Python &gt;=3.5,&lt;3.8; 1.27.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.27.0.post2 Requires-Python &gt;=3.5,&lt;3.8; 1.28.0 Requires-Python &gt;=3.5,&lt;3.8; 1.28.0.post1 Requires-Python &gt;=3.5,&lt;3.8; 1.28.0.post2 Requires-Python &gt;=3.5,&lt;3.8; 1.29.0 Requires-Python &gt;=3.6,&lt;3.8; 1.29.0.post1 Requires-Python &gt;=3.6,&lt;3.8; 1.30.0 Requires-Python &gt;=3.6,&lt;3.8; 1.31.0 Requires-Python &gt;=3.6,&lt;3.8; 1.32.0 Requires-Python &gt;=3.6,&lt;3.8; 1.33.0 Requires-Python &gt;=3.6,&lt;3.8; 1.33.1 Requires-Python &gt;=3.6,&lt;3.8; 1.33.1.post1 Requires-Python &gt;=3.6,&lt;3.8; 1.34.0 Requires-Python &gt;=3.6,&lt;3.8; 1.34.0.post1 Requires-Python &gt;=3.6,&lt;3.8; 1.34.1 Requires-Python &gt;=3.6,&lt;3.8; 1.34.1.post1 Requires-Python &gt;=3.6,&lt;3.8; 1.35.0 Requires-Python &gt;=3.6,&lt;3.8; 1.35.1 Requires-Python &gt;=3.6,&lt;3.8; 1.36.0 Requires-Python &gt;=3.6,&lt;3.8; 1.36.1 Requires-Python &gt;=3.6,&lt;3.8; 1.37.0 Requires-Python &gt;=3.6,&lt;3.8; 1.38.0 Requires-Python &gt;=3.6,&lt;3.8; 3.2.0.dev1 Requires-Python &gt;=3.9\nERROR: Could not find a version that satisfies the requirement en-core-web-sm==2.1.0 (from versions: none)\nERROR: No matching distribution found for en-core-web-sm==2.1.0\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1681910517490,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1248565\/automl-mlflow-batch-endpoint-image-build-failed",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"automl batch endpoint imag build fail deploi model gener azur automl batch endpoint code option follow imag build fail error occur error ignor follow version requir differ python version requir python error version satisfi requir core web version error match distribut core web",
        "Question_preprocessed_content":"automl batch endpoint imag build fail deploi model gener azur automl batch endpoint option imag build fail",
        "Question_gpt_summary_original":"The user is encountering an \"Image build failed\" error when deploying an MLFlow model generated by Azure AutoML as a batch endpoint, due to a version compatibility issue with Python and en-core-web-sm.",
        "Question_gpt_summary":"user encount imag build fail error deploi model gener azur automl batch endpoint version compat issu python core web"
    },
    {
        "Question_title":"I would like to configure Polyaxon in a way to avoid asking data-scientists to configure pre-emptible node-pools or request TPUs on their own",
        "Question_body":"From slack\n\nI am currently defining some machines configuration using machine-env1.yaml, machine-env2.yaml which basically contains node selectors and CPU, GPU, and TPU requests configuration, and then running:\n\npolyaxon run -f polyaxonfile.yaml -f machine-env1.yaml\n\nI have two problems with this approach:\n\nI need to copy the env files to all our git repos, which means if I make a change I need to perform several pull requests\nI need to tell the data-scientits to pull the last commit, sometimes that's not possible because they can not merge\/rebase the changes.\n\nBased on those two issues, in the end we tell data-scientists to just use:\n\nenvironment:\n  nodeSelector:\n    nodes: large-pool\n...\nrun:\n  ...\n  container:\n      resources:\n        limits:\n          cpu: 3000m\n          memory: 6000Mi\n        requests:\n          cpu: 2000m\n          memory: 4000Mi\n\nWhich is error prone and confusing for them, and make the files bigger and difficult to change.\n\nAny elegant way to abstract this type of configuration from the data-scientists?",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1649336966000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"We have already shared a resource on how to configure the environments in this guide\n\nif you are using multiple git repos and you do not want to replicate the yaml files in all repos you can register those files as presets:\n\nUsers will be able to use --presets machine1 or --presets=env1\n\nNote that in the example in that link, it shows that it defines a queue but you do not have to define a queue, a preset is just any YAML file that can be used with the override operator -f main.yaml -f override1.yaml -f override2.yaml in this case override1.yaml and override2.yaml it can be saved as organization presets using the UI.\n\nMore info from the intro section about presets and the UI section\n\nAlso, when you define presets you can use them directly on the operation or component\n\npresets: [preset1, preset2]\n\nThis is similar to the CLI command\n\npolyaxon run -f polyaxon.yaml --presets preset1,preset2",
        "Answer_comment_count":0.0,
        "Question_closed_time":1649337274000,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1484",
        "Tool":"Polyaxon",
        "Question_self_resolution":1.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"like configur wai avoid ask data scientist configur pre emptibl node pool request tpu slack current defin machin configur machin env yaml machin env yaml basic contain node selector cpu gpu tpu request configur run run file yaml machin env yaml problem approach need copi env file git repo mean chang need perform pull request need tell data scientit pull commit possibl merg rebas chang base issu end tell data scientist us environ nodeselector node larg pool run contain resourc limit cpu memori request cpu memori error prone confus file bigger difficult chang eleg wai abstract type configur data scientist",
        "Question_preprocessed_content":"like configur wai avoid ask configur request tpu slack defin machin configur contain node selector cpu gpu tpu request configur run problem copi env file git repo mean chang perform request chang base end us environ nodeselector node run contain resourc limit cpu memori request cpu memori prone confus file chang eleg wai abstract type configur",
        "Question_gpt_summary_original":"The user is facing challenges in configuring Polyaxon for their data scientists. They are currently using machine-env files to define node selectors and CPU, GPU, and TPU requests configuration, but this approach requires copying the files to all git repos and telling data scientists to pull the last commit. As a result, they are looking for a more elegant way to abstract this type of configuration from the data scientists.",
        "Question_gpt_summary":"user face challeng configur data scientist current machin env file defin node selector cpu gpu tpu request configur approach requir copi file git repo tell data scientist pull commit result look eleg wai abstract type configur data scientist"
    },
    {
        "Question_title":"Data import using local upload failed in Sagemaker canvas",
        "Question_body":"Hi, getting upload dialed error after multiple trials while uploading data using local upload in sagemaker canvas. Please advise. Thanks",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1680926716728,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1681273193271,
        "Question_score_count":0.0,
        "Question_view_count":67.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDTjxBMfPQ6OZe9pyHXks3A\/data-import-using-local-upload-failed-in-sagemaker-canvas",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"data import local upload fail canva get upload dial error multipl trial upload data local upload canva advis thank",
        "Question_preprocessed_content":"data import local upload fail canva upload dial multipl trial upload data local upload canva advis thank",
        "Question_gpt_summary_original":"The user is facing an issue while trying to upload data using local upload in Sagemaker canvas, as they are repeatedly encountering an upload dialed error. They are seeking advice to resolve the issue.",
        "Question_gpt_summary":"user face issu try upload data local upload canva repeatedli encount upload dial error seek advic resolv issu"
    },
    {
        "Question_title":"Run a Vertex AI model locally",
        "Question_body":"<p>Using the Vertex AI product at GCP training was very easy, I uploaded a data set and it returned a model which is saved in a gcp bucket, I downloaded the files and the tree has these files<\/p>\n<pre><code>\u251c\u2500\u2500 environment.json\n\u251c\u2500\u2500 feature_attributions.yaml\n\u251c\u2500\u2500 final_model_structure.pb\n\u251c\u2500\u2500 instance.yaml\n\u251c\u2500\u2500 predict\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 001\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 assets\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 PVC_vocab\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 assets.extra\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 tf_serving_warmup_requests\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 saved_model.pb\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 variables\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 variables.data-00000-of-00001\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 variables.index\n\u251c\u2500\u2500 prediction_schema.yaml\n\u251c\u2500\u2500 tables_server_metadata.pb\n\u2514\u2500\u2500 transformations.pb\n<\/code><\/pre>\n<p>I would like to serve this model locally from a dockerized python application, but I don't know enough TF to do this and I am very confused about which <code>.pb<\/code> file is the actual one that has the neural network I need.<\/p>\n<p>Thanks for any tips.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4.0,
        "Question_created_time":1649554815260,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":261.0,
        "Poster_created_time":1483057875592,
        "Poster_location":"St. Louis, MO, USA",
        "Poster_reputation_count":710.0,
        "Poster_view_count":175.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71813169",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"run model local product gcp train easi upload data set return model save gcp bucket download file tree file environ json featur attribut yaml final model structur instanc yaml predict asset pvc vocab asset extra serv warmup request save model variabl variabl data variabl index predict schema yaml tabl server metadata transform like serv model local docker python applic know confus file actual neural network need thank tip",
        "Question_preprocessed_content":"run model product gcp train easi upload data set return model save gcp bucket download file file like serv model docker python know confus file actual neural network thank tip",
        "Question_gpt_summary_original":"The user has successfully trained a model using Vertex AI product at GCP and downloaded the model files from a GCP bucket. However, the user is facing challenges in serving the model locally from a dockerized python application due to lack of knowledge in TensorFlow and confusion about which .pb file contains the required neural network. The user is seeking tips to overcome this challenge.",
        "Question_gpt_summary":"user successfulli train model product gcp download model file gcp bucket user face challeng serv model local docker python applic lack knowledg tensorflow confus file contain requir neural network user seek tip overcom challeng"
    },
    {
        "Question_title":"MLFlow tutorials",
        "Question_body":"Hello!\n\nWe would like to share with you our three tutorials:\n\nDeploying MLflow on the Google Cloud Platform using App Engine\u00a0\n\nIn the article we described how to:\n\n\ud83d\udd39Pre-configuring OAuth 2.0 Client\n\n\ud83d\udd39Build the docker image for MLflow on App Engine\n\n\ud83d\udd39Prepare the Terraform inputs\n\nLink: https:\/\/getindata.com\/blog\/deploying-mlflow-google-cloud-platform-using-app-engine\/?utm_source=mailing&utm_medium=mlflow&utm_campaign=grupy&utm_id=getindata\u00a0\n\nDeploying serverless MLFlow on Google Cloud Platform using Cloud Run\n\nIn this article you will find 7 steps to the successfully deployment:\n\n\u2733\ufe0f Step 1: Setting appropriate permissions\n\n\u2733\ufe0f Step 2: Pre-configuring #oauth 2.0 Client\n\n\u2733\ufe0f Step 3: Configuring Cloud SQL \/ Cloud Storage\n\n\u2733\ufe0f Step 4: Storing configuration in Secret Manager\n\n\u2733\ufe0f Step 5: Preparing MLFlow #Docker image\n\n\u2733\ufe0f Step 6: Deploying MLFlow with OAuth2-Proxy on Cloud Run\n\n\u2733\ufe0f Step 7: Finishing OAuth2.0 configuration\n\nLink: https:\/\/getindata.com\/blog\/deploying-serverless-mlflow-google-cloud-platform-using-cloud-run\/?utm_source=mailing&utm_medium=mlflow3&utm_campaign=grupy&utm_id=getindata\u00a0\n\nDeploying secure MLflow on AWS\n\nAfter reading our blog post about deploying serverless #MLflow on GCP read how we covered deployments on #AWS.\u00a0\n\nWhat are the main MLFlow infrastructure components? \ud83e\udd14\n\nThe architecture contains:\n\n\ud83d\udd39MLFlow Tracking Server, which exposes API for logging parameters, metrics, experiments, metadata and UI for visualizing the results.\n\n\ud83d\udd39Amazon Aurora Serverless used as the backend store\n\n\ud83d\udd39AWS S3 used as the artefact store\n\n\ud83d\udd39Oauth2-proxy protects MLFlow endpoints using OAuth2 compatible providers, e.g. Google.\n\nLink: https:\/\/getindata.com\/blog\/deploying-secure-mlfow-aws\/?utm_source=mailing&utm_medium=mlflow2&utm_campaign=grupy&utm_id=getindata\u00a0\n\n\nEnjoy and let us know what do you think!",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1671535390000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":34.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/3fIR6C7rJkw",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tutori hello like share tutori deploi googl cloud platform app engin articl describ pre configur oauth client build docker imag app engin prepar terraform input link http getindata com blog deploi googl cloud platform app engin utm sourc mail utm medium utm campaign grupi utm getindata deploi serverless googl cloud platform cloud run articl step successfulli deploy step set appropri permiss step pre configur oauth client step configur cloud sql cloud storag step store configur secret manag step prepar docker imag step deploi oauth proxi cloud run step finish oauth configur link http getindata com blog deploi serverless googl cloud platform cloud run utm sourc mail utm medium utm campaign grupi utm getindata deploi secur aw read blog post deploi serverless gcp read cover deploy aw main infrastructur compon architectur contain track server expos api log paramet metric experi metadata visual result amazon aurora serverless backend store aw artefact store oauth proxi protect endpoint oauth compat provid googl link http getindata com blog deploi secur mlfow aw utm sourc mail utm medium utm campaign grupi utm getindata enjoi let know think",
        "Question_preprocessed_content":"tutori like share tutori deploi cloud platform engin articl describ oauth client build docker imag engin prepar input link deploi cloud platform cloud run articl step deploy step step oauth client step configur cloud sql cloud storag step store configur secret manag step prepar docker imag step deploi cloud run step finish configur link deploi secur aw read blog post deploi gcp read cover deploy aw main infrastructur compon architectur contain track server expos api paramet metric experi metadata visual result amazon aurora backend store aw artefact store protect endpoint oauth compat provid link enjoi let know think",
        "Question_gpt_summary_original":"The user has shared three tutorials related to deploying MLflow on different cloud platforms, including Google Cloud Platform using App Engine, Google Cloud Platform using Cloud Run, and AWS. The tutorials cover various steps and components required for successful deployment, such as pre-configuring OAuth 2.0 Client, building Docker images, configuring Cloud SQL\/Cloud Storage, and using Oauth2-proxy to protect MLFlow endpoints.",
        "Question_gpt_summary":"user share tutori relat deploi differ cloud platform includ googl cloud platform app engin googl cloud platform cloud run aw tutori cover step compon requir success deploy pre configur oauth client build docker imag configur cloud sql cloud storag oauth proxi protect endpoint"
    },
    {
        "Question_title":"TypeError when creating Data Asset with Python",
        "Question_body":"<p>I'm trying to register data from a datastore programatically (via Python SDK v2). I need to do it this way since the final output will be a scheduled AzureML Pipeline that does preprocessing for an NLP module. Further, as time progresses, the source files will also be updated. I'm attempting to run the following script:<\/p>\n<pre><code class=\"lang-python\">from azure.ai.ml.entities  import Data\nfrom azure.ai.ml.constants import AssetTypes\n\ndata_path = &quot;azureml:\/\/\n\/datastores\/[name-of-datastore]\/paths\/[filename].csv\n&quot;\n\n\ndata_asset = Data(\n    path = data_path,\n    type = AssetTypes.URI_FILE,\n    description= 'description',\n    name = 'data_name'\n)\n\nml_client.data.create_or_update(asset)\n\n\n\n<\/code><\/pre>\n<p>However, running that prompts me with the error<\/p>\n<p>TypeError: Please refer to create_or_update docstring for valid input types.<\/p>\n<p>I'm not sure why I'm getting this error and would like to ask for all the help I can get. <\/p>\n<p>While the example shows an excel file, I have more files I wish to register this way, some being excel sheets.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_created_time":1685104254456,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1292188\/typeerror-when-creating-data-asset-with-python",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"typeerror creat data asset python try regist data datastor programat python sdk need wai final output schedul pipelin preprocess nlp modul time progress sourc file updat attempt run follow script azur entiti import data azur constant import assettyp data path datastor datastor path filenam csv data asset data path data path type assettyp uri file descript descript data client data creat updat asset run prompt error typeerror refer creat updat docstr valid input type sure get error like ask help exampl show excel file file wish regist wai excel sheet",
        "Question_preprocessed_content":"creat data python try regist data datastor wai final output schedul pipelin nlp modul time sourc file updat run script prompt refer docstr valid input type sure like ask help exampl show excel file file wish regist wai excel",
        "Question_gpt_summary_original":"The user is encountering a TypeError while trying to register data from a datastore programmatically using Python SDK v2. The user needs to do it this way since the final output will be a scheduled AzureML Pipeline that does preprocessing for an NLP module. The error message prompts the user to refer to create_or_update docstring for valid input types.",
        "Question_gpt_summary":"user encount typeerror try regist data datastor programmat python sdk user need wai final output schedul pipelin preprocess nlp modul error messag prompt user refer creat updat docstr valid input type"
    },
    {
        "Question_title":"Change AML script",
        "Question_body":"<p>How can we change the pipeline script OR the output setting of &quot;regenerate output&quot; over the AML pipeline description page, instead of submitting the pipeline again?    <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_created_time":1657574990310,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/922634\/change-aml-script",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"chang aml script chang pipelin script output set regener output aml pipelin descript page instead submit pipelin",
        "Question_preprocessed_content":"chang aml script chang pipelin script output regener output aml pipelin descript page instead pipelin",
        "Question_gpt_summary_original":"The user is seeking guidance on how to modify the pipeline script or output settings for \"regenerate output\" on the AML pipeline description page without having to resubmit the pipeline.",
        "Question_gpt_summary":"user seek guidanc modifi pipelin script output set regener output aml pipelin descript page have resubmit pipelin"
    },
    {
        "Question_title":"predicting the possibility of Active members becoming Inactive?",
        "Question_body":"<p>I have a database of members, some are active and some are inactive. <\/p>\n\n<p>I want to predict the possibility of Active members becoming Inactive?<\/p>\n\n<p>Should I run the AML on the inactive members (no splitting) and when I publish the model i pass in the active members?<\/p>\n\n<p>I have tried many AML datasets before however usually you will have a column that contains the values you want to predict (Active-Inactive) (True-False) (Red-Black-White) but i never tried having only one value to trina your model with.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1481090961200,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1481093503168,
        "Question_score_count":0.0,
        "Question_view_count":83.0,
        "Poster_created_time":1481090553287,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Answer_body":"<p>you will need to train your model with both active and inactive members.  I would split your dataset so there are examples of active and inactive members in both your training and your test set.  <\/p>\n\n<p>Let's discuss why we split the data.  Remember that with supervised learning, you need data with labeled examples.  For example, let\u2019s say I want to predict how much a house will cost based on its square footage and zip code.  To train my model, I need a dataset of existing houses with their square footage, zip codes, and prices, like this:<\/p>\n\n<p>SquareFootage ZipCode Price <br\/>\n2000          48075   200,000 <br\/>\n3000 48075 300,000 <br\/>\n4000 48075 400,000 <br\/>\n5000 48075 500,000 <br\/><\/p>\n\n<p>In this example, square footage and zip code are my features (things that influence the thing you want to predict) and price is my label (the thing that you want to predict).  I could train a model on some data like the above, and then use the trained model to predict prices, given only a square footage and zip code.  <\/p>\n\n<p>So, the reason I split the data is to provide most of the data to train the model (it will process the data to figure out correlations between the features and labels in the \u201ctrain model\u201d module), but we want to hold back some of that labeled data to test the model that we built.  Then, we can compare the price value that the trained model generates against the actual labeled price value in the test dataset (in the \u201cscore model\u201d module) to see how well the model is performing.  (We can\u2019t use the same data for both...the model is built using the training data, so it will perform pretty accurately with that; we hold back unused data to test.)  <\/p>\n\n<p>So, for your example, I would try a random split so there are examples of both active and inactive members (that is your label - inactive or active) and you will also need to provide relevant features that influence activity.  <\/p>",
        "Answer_comment_count":5.0,
        "Question_closed_time":1481093602247,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1342628508448,
        "Answerer_location":"United States",
        "Answerer_reputation_count":5147.0,
        "Answerer_view_count":1739.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41010551",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"predict possibl activ member inact databas member activ inact want predict possibl activ member inact run aml inact member split publish model pass activ member tri aml dataset usual column contain valu want predict activ inact true fals red black white tri have valu trina model",
        "Question_preprocessed_content":"predict activ member inact databas member activ inact want predict activ member inact run aml inact member publish model activ member tri aml dataset column contain valu want predict tri have valu trina model",
        "Question_gpt_summary_original":"The user wants to predict the likelihood of active members becoming inactive. They are unsure if they should run the AML on inactive members and pass in active members when publishing the model. The user has tried various AML datasets before, but they have never trained a model with only one value to predict.",
        "Question_gpt_summary":"user want predict likelihood activ member inact unsur run aml inact member pass activ member publish model user tri aml dataset train model valu predict"
    },
    {
        "Question_title":"Exporting Sagemaker model to local computer",
        "Question_body":"I hyper-tuned an XGBoost model, deployed the model and created an endpoint. Is there a way to export the model to my local computer? That way I can test the model locally.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1649274099076,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668613197516,
        "Question_score_count":0.0,
        "Question_view_count":894.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKgLZZhWVSg2d5XJWwbaTiA\/exporting-sagemaker-model-to-local-computer",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"export model local hyper tune xgboost model deploi model creat endpoint wai export model local wai test model local",
        "Question_preprocessed_content":"export model local model deploi model creat endpoint wai export model local wai test model",
        "Question_gpt_summary_original":"The user has hyper-tuned an XGBoost model, deployed it and created an endpoint. They are looking for a way to export the model to their local computer to test it locally.",
        "Question_gpt_summary":"user hyper tune xgboost model deploi creat endpoint look wai export model local test local"
    },
    {
        "Question_title":"How to deploy N models on N Greengrass devices with a unique Lambda for inference logic?",
        "Question_body":"Hi,\n\nLet's consider an ML edge inference use-case on Greengrass-managed device. The model is unique to each device, however its architecture and invocation logic are the same for all devices. In other words, the same invocation Lambda could be the same for all devices, only the model parameters would need to change across devices. We'd like to deploy a unique inference Lambda to all devices, and load device-specific artifact to each device.\n\nCan this be achieved with Greengrass ML Inference? It seems that GG MLI requires each model to be associated with a specific Lambda. \n\nOtherwise, is the recommended pattern to self-manage the inference in Lambda? E.g. by loading a specific model from S3 unique a local config file or some env variable?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1605018964000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1667926501335,
        "Question_score_count":0.0,
        "Question_view_count":51.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"In IoT Greengrass 1.x, the configuration is unique to each Greengrass Group. This includes Connectors, Lambdas and ML Resources.\n\nThe same Lambda can be referenced by multiple groups as a Greengrass function, which is likely what you want. This is similar to using one of the GG ML connectors (Object Detection or Image Classification).\n\nIn addition to your inference code, you'll also need to configure an ML Resource, which has a local name and a remote model. The local name would be the same for all Greengrass Groups, but in each group you will refer to a different remote object (the model) - either S3 or SageMaker job.\n\nEvery time a model changes, you will need to redeploy the corresponding Greengrass group for the changes to be deployed locally.",
        "Answer_comment_count":0.0,
        "Question_closed_time":1605020138000,
        "Answer_last_edit_time":1613588986552,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlVJHC1NaTTOquvDqs444oQ\/how-to-deploy-n-models-on-n-greengrass-devices-with-a-unique-lambda-for-inference-logic",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":1.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":1.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":1.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"deploi model greengrass devic uniqu lambda infer logic let consid edg infer us case greengrass manag devic model uniqu devic architectur invoc logic devic word invoc lambda devic model paramet need chang devic like deploi uniqu infer lambda devic load devic specif artifact devic achiev greengrass infer mli requir model associ specif lambda recommend pattern self manag infer lambda load specif model uniqu local config file env variabl",
        "Question_preprocessed_content":"deploi model devic uniqu lambda infer logic let consid edg infer manag devic model uniqu devic architectur invoc logic devic word invoc lambda devic model paramet chang devic like deploi uniqu infer lambda devic load artifact devic achiev infer mli requir model specif lambda infer lambda load specif model uniqu local config file env variabl",
        "Question_gpt_summary_original":"The user is facing a challenge in deploying N models on N Greengrass devices with a unique Lambda for inference logic. They are unsure if this can be achieved with Greengrass ML Inference as it requires each model to be associated with a specific Lambda. They are seeking recommendations on how to self-manage the inference in Lambda by loading a specific model from S3 unique a local config file or some env variable.",
        "Question_gpt_summary":"user face challeng deploi model greengrass devic uniqu lambda infer logic unsur achiev greengrass infer requir model associ specif lambda seek recommend self manag infer lambda load specif model uniqu local config file env variabl"
    },
    {
        "Question_title":"Loading Keras model-best.h5 saved with W&B run",
        "Question_body":"<p>Hi,<\/p>\n<p>While using <code>wandb.keras.WandbCallback()<\/code> I noticed that W&amp;B saves a \u201c<code>model-best.h5<\/code>\u201d file at every run. However, I run into errors while trying to load this model. In contrast, the model saved by <code>tf.keras<\/code>\u2019 <code>ModelCheckpoint<\/code> callback works fine.<\/p>\n<p>Could this be an error due to <code>keras<\/code> vs. <code>tf.keras<\/code> protocols or clashing between different <code>tf.keras<\/code> versions? Would love to get more insight in how <code>wandb.keras.WandbCallback()<\/code> saves <code>model-best.h5<\/code>.<\/p>\n<p><strong>Error traceback:<\/strong><\/p>\n<pre data-code-wrap=\"---------------------------------------------------------------------------\"><code class=\"lang-nohighlight\">OSError                                   Traceback (most recent call last)\n\/tmp\/ipykernel_25\/1740475024.py in &lt;module&gt;\n      1 model = tf.keras.models.load_model(MODEL_PATH, \n      2                                    custom_objects={'FixedDropout': PermaDropout, \n----&gt; 3                                                    'rmse_tf': rmse_tf})\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/tensorflow\/python\/keras\/saving\/save.py in load_model(filepath, custom_objects, compile, options)\n    205           (isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n    206         return hdf5_format.load_model_from_hdf5(filepath, custom_objects,\n--&gt; 207                                                 compile)\n    208 \n    209       filepath = path_to_string(filepath)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/tensorflow\/python\/keras\/saving\/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)\n    170   opened_new_file = not isinstance(filepath, h5py.File)\n    171   if opened_new_file:\n--&gt; 172     f = h5py.File(filepath, mode='r')\n    173   else:\n    174     f = filepath\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/h5py\/_hl\/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\n    406                 fid = make_fid(name, mode, userblock_size,\n    407                                fapl, fcpl=make_fcpl(track_order=track_order),\n--&gt; 408                                swmr=swmr)\n    409 \n    410             if isinstance(libver, tuple):\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/h5py\/_hl\/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)\n    171         if swmr and swmr_support:\n    172             flags |= h5f.ACC_SWMR_READ\n--&gt; 173         fid = h5f.open(name, flags, fapl=fapl)\n    174     elif mode == 'r+':\n    175         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n\nh5py\/_objects.pyx in h5py._objects.with_phil.wrapper()\n\nh5py\/_objects.pyx in h5py._objects.with_phil.wrapper()\n\nh5py\/h5f.pyx in h5py.h5f.open()\n\nOSError: Unable to open file (bad object header version number\n<\/code><\/pre>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_created_time":1632696333709,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":6.0,
        "Question_view_count":1164.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/loading-keras-model-best-h5-saved-with-w-b-run\/786",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load kera model best save run kera callback notic save model best file run run error try load model contrast model save kera modelcheckpoint callback work fine error kera kera protocol clash differ kera version love insight kera callback save model best error traceback oserror traceback recent tmp ipykernel model kera model load model model path custom object fixeddropout permadropout rmse rmse opt conda lib python site packag tensorflow python kera save save load model filepath custom object compil option isinst filepath hpy file hpy hdf filepath return hdf format load model hdf filepath custom object compil filepath path string filepath opt conda lib python site packag tensorflow python kera save hdf format load model hdf filepath custom object compil open new file isinst filepath hpy file open new file hpy file filepath mode filepath opt conda lib python site packag hpy file init self mode driver libver userblock size swmr rdcc nslot rdcc nbyte rdcc track order kwd fid fid mode userblock size fapl fcpl fcpl track order track order swmr swmr isinst libver tupl opt conda lib python site packag hpy file fid mode userblock size fapl fcpl swmr swmr swmr support flag acc swmr read fid open flag fapl fapl elif mode fid open acc rdwr fapl fapl hpy object pyx hpy object phil wrapper hpy object pyx hpy object phil wrapper hpy pyx hpy open oserror unabl open file bad object header version number",
        "Question_preprocessed_content":"load kera save run notic save file run run try load model contrast model save work fine protocol clash version love insight save traceback",
        "Question_gpt_summary_original":"The user encountered errors while trying to load the \"model-best.h5\" file saved by W&B run using wandb.keras.WandbCallback(), while the model saved by tf.keras' ModelCheckpoint callback works fine. The user is unsure if this is due to keras vs. tf.keras protocols or clashing between different tf.keras versions. The error traceback suggests an OSError due to a bad object header version number.",
        "Question_gpt_summary":"user encount error try load model best file save run kera callback model save kera modelcheckpoint callback work fine user unsur kera kera protocol clash differ kera version error traceback suggest oserror bad object header version number"
    },
    {
        "Question_title":"Jupyter Notebook: Git Files",
        "Question_body":"<p>I\u2019m trying to populate my Sagemaker EC2 instance with scripts\/modules from my git repo for running Jupyter notebooks.<\/p>\n<p>When I clone the git repo using access token, I get a repo.git file in my Jupyter directory where notebooks live.<\/p>\n<p>What do I do with this file and how do I Import the py files from the repo? My goal is to actually have the files in my local instance and navigate through them(vs a single flat .git file)<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_created_time":1615845518817,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":23.0,
        "Poster_created_time":1610051240692,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":24.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66646237",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"jupyt notebook git file try popul instanc script modul git repo run jupyt notebook clone git repo access token repo git file jupyt directori notebook live file import file repo goal actual file local instanc navig singl flat git file thank advanc",
        "Question_preprocessed_content":"jupyt git file try popul instanc git repo jupyt clone git repo token file jupyt directori live file import file repo goal file local instanc navig thank advanc",
        "Question_gpt_summary_original":"The user is trying to populate their Sagemaker EC2 instance with scripts\/modules from their git repo for running Jupyter notebooks. However, when they clone the git repo using an access token, they get a repo.git file in their Jupyter directory where notebooks live. The user is unsure of what to do with this file and how to import the py files from the repo. Their goal is to have the files in their local instance and navigate through them.",
        "Question_gpt_summary":"user try popul instanc script modul git repo run jupyt notebook clone git repo access token repo git file jupyt directori notebook live user unsur file import file repo goal file local instanc navig"
    },
    {
        "Question_title":"Timing test on azure ml",
        "Question_body":"<p>I have created data sets of various sizes say 1GB, 2GB, 3GB, 4GB (&lt; 10 GB) and executing various machine learning models on Azure ML. <\/p>\n\n<p>1) Can I know what is the server specifications (RAM, CPU) that is provided in the Azure ML service.<\/p>\n\n<p>2) Also at times the reader says \"Memory exhaust\" for >4GB of data.Though azure ml should be able to handle 10GB of data as per documentation.<\/p>\n\n<p>3) If I run multiple experiments(in different tabs of browser) in parallel, its taking more time.<\/p>\n\n<p>4) Is there any way to set the RAM, CPU cores in Azure ML<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4.0,
        "Question_created_time":1456387015953,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1457425395532,
        "Question_score_count":5.0,
        "Question_view_count":155.0,
        "Poster_created_time":1456386494000,
        "Poster_location":"Dubai - United Arab Emirates",
        "Poster_reputation_count":51.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35621424",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"time test creat data set size data abl handl data document run multipl experi differ tab browser parallel take time wai set ram cpu core",
        "Question_preprocessed_content":"time test creat data set size execut machin learn model know server specif provid servic time reader sai memori exhaust abl handl data document run multipl experi take time wai set ram cpu core",
        "Question_gpt_summary_original":"The user has encountered challenges while executing various machine learning models on Azure ML. They are unsure about the server specifications provided by Azure ML and have faced \"Memory exhaust\" errors while working with data sets larger than 4GB. The user has also observed that running multiple experiments in parallel takes more time and is looking for a way to set the RAM and CPU cores in Azure ML.",
        "Question_gpt_summary":"user encount challeng execut machin learn model unsur server specif provid face memori exhaust error work data set larger user observ run multipl experi parallel take time look wai set ram cpu core"
    },
    {
        "Question_title":"Is it possible to include the IP address of a specific Azure Machine Learning workspace in its storage account selected networks and get all functionality enabled?",
        "Question_body":"<p>We have secured the storage account of a Machine Learning workspace behind a vnet and have authorized a set of IPs to access the storage account. Since the workspace is not secured behind the vnet, a set of functions is disabled. Is there a way to get the IP of the workspace and include it in the list authorized networks for the storage account in order to have all workspace functionalities available? We know the official solution involves securing the workspace behind the vnet and enabling point-to-site, site-to-site or connecting through a VM, but these are not possible in our case. Thanks for the help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1626459319327,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/479209\/is-it-possible-to-include-the-ip-address-of-a-spec",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl includ address specif workspac storag account select network function enabl secur storag account machin learn workspac vnet author set ip access storag account workspac secur vnet set function disabl wai workspac includ list author network storag account order workspac function avail know offici solut involv secur workspac vnet enabl point site site site connect possibl case thank help",
        "Question_preprocessed_content":"includ specif workspac storag select network function enabl secur storag machin learn workspac vnet author set ip storag workspac secur vnet set function disabl wai workspac includ list author network storag order workspac function avail know solut involv secur workspac vnet enabl case thank help",
        "Question_gpt_summary_original":"The user has secured the storage account of an Azure Machine Learning workspace behind a vnet and authorized a set of IPs to access it. However, since the workspace is not secured behind the vnet, some functions are disabled. The user is looking for a way to include the IP of the workspace in the authorized networks list for the storage account to enable all workspace functionalities, but the official solutions are not possible in their case.",
        "Question_gpt_summary":"user secur storag account workspac vnet author set ip access workspac secur vnet function disabl user look wai includ workspac author network list storag account enabl workspac function offici solut possibl case"
    },
    {
        "Question_title":"unable to identify current timezone 'C'",
        "Question_body":"<p>all,<\/p>\n\n<p>I am using R on the Azure machine learning, and I have some problems.<\/p>\n\n<p>I want to use program R to calculate the difference between two date, for example, 2014\/11\/01 and 2014\/11\/03.<\/p>\n\n<p>I using the function \"strptime\" in R to do this thing, it can work on my own computer, but when I want to run the same code on Azure ml, it came out the error.<\/p>\n\n<p>The error is : <\/p>\n\n<pre><code>[ModuleOutput] 1: In strptime(x, format, tz = tz) :\n[ModuleOutput] \n[ModuleOutput]   unable to identify current timezone 'C':\n[ModuleOutput] \n[ModuleOutput] please set environment variable 'TZ'\n[ModuleOutput] \n[ModuleOutput] 2: In strptime(x, format, tz = tz) : unknown timezone 'localtime'\n<\/code><\/pre>\n\n<p>I think the problem is that it can't detect the timezone on Azure ml, but I'm not sure.<\/p>\n\n<p>Is there any way to solve this problem?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_created_time":1415262044470,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":1456.0,
        "Poster_created_time":1413344613203,
        "Poster_location":"Taipei City, Taiwan",
        "Poster_reputation_count":579.0,
        "Poster_view_count":73.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/26774724",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl identifi current timezon problem want us program calcul differ date exampl function strptime thing work want run code came error error moduleoutput strptime format moduleoutput moduleoutput unabl identifi current timezon moduleoutput moduleoutput set environ variabl moduleoutput moduleoutput strptime format unknown timezon localtim think problem detect timezon sure wai solv problem thank advanc",
        "Question_preprocessed_content":"unabl identifi timezon problem want us program calcul date exampl function strptime thing work want run code came think problem detect timezon sure wai solv problem thank advanc",
        "Question_gpt_summary_original":"The user is facing an issue while using the \"strptime\" function in R on Azure machine learning to calculate the difference between two dates. The error message suggests that the current timezone 'C' cannot be identified, and the user needs to set the environment variable 'TZ'. The user suspects that the problem is related to the inability to detect the timezone on Azure ml.",
        "Question_gpt_summary":"user face issu strptime function calcul differ date error messag suggest current timezon identifi user need set environ variabl user suspect problem relat inabl detect timezon"
    },
    {
        "Question_title":"MLflow 1.9.0 released!",
        "Question_body":"Hi all,\n\nWe are happy to announce the availability of MLflow 1.9.0! Some highlights from this release:\n\n\nlog_model and save_model APIs now support saving model signatures (the model's input and output schema) and example input along with the model itself \u00a0(#2698, #2775, @tomasatdatabricks). Model signatures are used to reorder and validate input fields when scoring\/serving models using the pyfunc flavor, mlflow models CLI commands, or mlflow.pyfunc.spark_udf (#2920, @tomasatdatabricks and @aarondav)\nIntroduce fastai model persistence and autologging APIs under mlflow.fastai (#2619, #2689 @antoniomdk)\nAdd pluggable mlflow.deployments API and CLI for deploying models to custom serving tools, e.g. RedisAI (#2327, @hhsecond)\nAdd plugin interface for executing MLflow projects against custom backends (#2566, @jdlesage)\nEnable viewing PDFs logged as artifacts from the runs UI \u00a0(#2859, @ankmathur96)\nSignificant performance and scalability improvements to metric comparison and scatter plots in the UI (#2447, @mjlbach)\nFor a comprehensive list of changes, see the\u00a0release change log (https:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v1.9.0), and check out the latest documentation on\u00a0mlflow.org.",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1592573520000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":10.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/M525psdvxds",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"releas happi announc avail highlight releas log model save model api support save model signatur model input output schema exampl input model tomasatdatabrick model signatur reorder valid input field score serv model pyfunc flavor model cli command pyfunc spark udf tomasatdatabrick aarondav introduc fastai model persist autolog api fastai antoniomdk add pluggabl deploy api cli deploi model custom serv tool redisai hhsecond add plugin interfac execut project custom backend jdlesag enabl view pdf log artifact run ankmathur signific perform scalabl improv metric comparison scatter plot mjlbach comprehens list chang thereleas chang log http github com releas tag check latest document org",
        "Question_preprocessed_content":"releas avail highlight releas api save model signatur exampl input model model signatur reorder valid input field model pyfunc flavor model cli introduc fastai model persist api fastai deploy api cli deploi model custom serv redisai plugin interfac execut project custom backend enabl view pdf artifact run signific perform scalabl improv metric comparison plot comprehens list chang thereleas chang log check latest document",
        "Question_gpt_summary_original":"MLflow 1.9.0 has been released with new features such as saving model signatures, fastai model persistence and autologging APIs, pluggable mlflow.deployments API and CLI for deploying models to custom serving tools, and a plugin interface for executing MLflow projects against custom backends. Additionally, there are significant performance and scalability improvements to metric comparison and scatter plots in the UI.",
        "Question_gpt_summary":"releas new featur save model signatur fastai model persist autolog api pluggabl deploy api cli deploi model custom serv tool plugin interfac execut project custom backend addition signific perform scalabl improv metric comparison scatter plot"
    },
    {
        "Question_title":"MLflow FileNotFound when calling spark_udf from pycharm with databricks-connect",
        "Question_body":"<p>I am saving a scikit-learn trained model to MLflow's model registry in my Windows laptop. I am using databricks-connect to connect to an Azure Databricks cluster and train models there from my local Pycharm, but for development I launch the model registry in my laptopt and save the trained models in it - to avoid having to set up accessing DBFS remotely.<\/p>\n<p>I am having trouble with the spark_udf function. <strong>I can read the model from my laptop's model registry but cannot use it as a UDF in my Databricks cluster<\/strong>:<\/p>\n<pre><code>model_udf = mlflow.pyfunc.spark_udf(spark, &quot;models:\/mymodel\/production&quot;)  # this works fine\nstruct_col = F.struct(*df.columns)\npredictions = df.withColumn(&quot;pred_spark&quot;, model_udf(struct_col))  \npredictions.show()                                        # throws an exception(see below)\n<\/code><\/pre>\n<p>pyspark.sql.utils.PythonException: An exception was thrown from a UDF: 'FileNotFoundError:<\/p>\n<p>[Errno 2] No such file or directory:<\/p>\n<p>'\/local_disk0\/spark-1fa39b20-9d2c-4697-957c-392d80326dee\/executor-57b039d8-7405-47c4-b072-612e9b87b3dd\/spark-e442241d-4007-4c6e-8acd-bf2a35b1a455\/isolatedSparkFiles\/044cd765-f5f7-46b3-9efb-0944cc91ef4d\/c:\\temp\\tmpsl4hpeyt.zip'<\/p>\n<p>The last part is weird as it mixes linux-style routes with Windows route (like a route in my Windows laptop's local dir). I thought that the driver would read the model from the model registry and broadcast it to the workers to call the UDF, but looks like the workers are trying to fetch it directly from the remote model registry, is this right? Is there a solution that does not require saving the model in the remote model registry - or at least, configuring the security to access DBFS?<\/p>\n<p><strong>EDIT:<\/strong> After having set up registering the model into the remote registry (<a href=\"https:\/\/docs.databricks.com\/applications\/mlflow\/access-hosted-tracking-server.html\" rel=\"nofollow noreferrer\">not that difficult!<\/a>) I can now download the registered model as a sklearn model and do predictions with it, but I cannot do the same via spark_udf. I get either<\/p>\n<ul>\n<li>The same FileNotFound error if I specify the model via runs:\/.... in function spark_udf<\/li>\n<li>An SSL error (unverified self-signed certificate) if I specify the model via model:\/... in the spark_udf, even though I have explicitly added a new line <code>insecure = True<\/code> in my .databrickscfg - and because that was not working, also added <code>os.environ[&quot;MLFLOW_TRACKING_INSECURE_TLS&quot;] = &quot;true&quot;<\/code> but has no effect. I guess it is doing something because at least I can download the model from the registry when it is not a spark_udf, but looks like &quot;the workers?&quot; are having issues when they try to do the same (that's my guess, no idea if it makes sense)<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1636295705043,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1636406015476,
        "Question_score_count":1.0,
        "Question_view_count":180.0,
        "Poster_created_time":1451376618088,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation_count":308.0,
        "Poster_view_count":75.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69873383",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"filenotfound call spark udf pycharm databrick connect save scikit learn train model model registri window laptop databrick connect connect azur databrick cluster train model local pycharm develop launch model registri laptopt save train model avoid have set access dbf remot have troubl spark udf function read model laptop model registri us udf databrick cluster model udf pyfunc spark udf spark model mymodel product work fine struct col struct column predict withcolumn pred spark model udf struct col predict throw except pyspark sql util pythonexcept except thrown udf filenotfounderror errno file directori local disk spark fab ddee executor ebbdd spark acd bfaba isolatedsparkfil efb ccefd temp tmpslhpeyt zip weird mix linux style rout window rout like rout window laptop local dir thought driver read model model registri broadcast worker udf look like worker try fetch directli remot model registri right solut requir save model remot model registri configur secur access dbf edit have set regist model remot registri difficult download regist model sklearn model predict spark udf filenotfound error specifi model run function spark udf ssl error unverifi self sign certif specifi model model spark udf explicitli ad new line insecur true databrickscfg work ad environ track insecur tl true effect guess download model registri spark udf look like worker have issu try guess idea make sens",
        "Question_preprocessed_content":"filenotfound pycharm save train model model registri window laptop azur databrick cluster train model local pycharm develop launch model registri laptopt save train model avoid have set dbf remot have troubl function read model laptop model registri us udf databrick cluster except thrown udf file directori weird mix rout window rout thought driver read model model registri broadcast worker udf like worker try fetch directli remot model registri right solut requir save model remot model registri configur secur dbf edit have set regist model remot registri download regist model sklearn model predict filenotfound specifi model function specifi model explicitli new line databrickscfg work download model registri like worker have try",
        "Question_gpt_summary_original":"The user is facing issues with using MLflow's spark_udf function to call a scikit-learn trained model saved in their laptop's MLflow model registry from a Databricks cluster using databricks-connect. The error message suggests that the workers are trying to fetch the model directly from the remote model registry, instead of the driver reading it and broadcasting it to the workers. The user has tried registering the model into the remote registry but still cannot use it via spark_udf, and is encountering either a FileNotFound error or an SSL error.",
        "Question_gpt_summary":"user face issu spark udf function scikit learn train model save laptop model registri databrick cluster databrick connect error messag suggest worker try fetch model directli remot model registri instead driver read broadcast worker user tri regist model remot registri us spark udf encount filenotfound error ssl error"
    },
    {
        "Question_title":"Run a flask app within AWS Sagemaker using a custom model",
        "Question_body":"<p>I've searched for solutions at AWS and Stackoverflow but haven't got a satisfying answer. I have a Sagemaker notebook instance where I built a custom model (not sklearn\/tf\/torch) which would calculated a result csv file and save it in the notebook.<\/p>\n<p>Now I need to serve predictions using the csv file. The input is id and output is the pre-calculated result from the csv. The sagemaker &quot;deploy custom model&quot; examples only include sklearn\/tf\/torch models.<\/p>\n<p>Is there a way to create an model endpoint serving pre-calculated predictions from csv files?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1633541485360,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":576.0,
        "Poster_created_time":1508194789703,
        "Poster_location":"SF, CA, United States",
        "Poster_reputation_count":609.0,
        "Poster_view_count":84.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69470223",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"run flask app custom model search solut aw stackoverflow haven got satisfi answer notebook instanc built custom model sklearn torch calcul result csv file save notebook need serv predict csv file input output pre calcul result csv deploi custom model exampl includ sklearn torch model wai creat model endpoint serv pre calcul predict csv file",
        "Question_preprocessed_content":"run flask custom model search solut aw stackoverflow haven got satisfi answer instanc built custom model calcul result csv file save serv predict csv file input output result csv deploi custom model exampl includ model wai creat model endpoint serv predict csv file",
        "Question_gpt_summary_original":"The user is facing challenges in running a Flask app within AWS Sagemaker using a custom model. They have built a custom model in a Sagemaker notebook instance that calculates a result CSV file and saves it in the notebook. However, they are unable to serve predictions using the CSV file as the Sagemaker \"deploy custom model\" examples only include sklearn\/tf\/torch models. The user is seeking a way to create a model endpoint that serves pre-calculated predictions from CSV files.",
        "Question_gpt_summary":"user face challeng run flask app custom model built custom model notebook instanc calcul result csv file save notebook unabl serv predict csv file deploi custom model exampl includ sklearn torch model user seek wai creat model endpoint serv pre calcul predict csv file"
    },
    {
        "Question_title":"ModuleNotFoundError: No module named 'ruamel' when excuting from azureml.core",
        "Question_body":"<p>I am trying to execute the Azure ml sdk from the local system using the Jupyter notebook. When I run the below code i am getting an error.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\n\nModuleNotFoundError: No module named 'ruamel' \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1622646368700,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":332.0,
        "Poster_created_time":1599816833352,
        "Poster_location":"New Delhi, Delhi, India",
        "Poster_reputation_count":329.0,
        "Poster_view_count":58.0,
        "Answer_body":"<p>You have to add pip 20.1.1<\/p>\n<p>Conda ruamel needs higher version of pip<\/p>\n<pre><code>conda install pip=20.1.1\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1626743937907,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1536318818623,
        "Answerer_location":"\u0130zmit, Kocaeli, T\u00fcrkiye",
        "Answerer_reputation_count":1033.0,
        "Answerer_view_count":55.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67807756",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"modulenotfounderror modul name ruamel excut core try execut sdk local jupyt notebook run code get error core import workspac datastor dataset modulenotfounderror modul name ruamel",
        "Question_preprocessed_content":"modul name ruamel excut core try execut sdk local jupyt run code",
        "Question_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when trying to execute the Azure ml sdk from their local system using Jupyter notebook. The error message specifically states that there is no module named 'ruamel'.",
        "Question_gpt_summary":"user encount modulenotfounderror try execut sdk local jupyt notebook error messag specif state modul name ruamel"
    },
    {
        "Question_title":"How do I resolve \"KeyError: 'brand'\" when running an experiment using Azure Automated Machine Learning?",
        "Question_body":"<p>I am using the Azure Automated Machine Learning SDK to train a machine learning model on my dataset. However, after the experiment, all my training iterations fail with a <code>KeyError: 'brand'<\/code> error even if the model training itself succeeded.<\/p>\n<p>How can I resolve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1594314866923,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":977.0,
        "Poster_created_time":1341461304023,
        "Poster_location":null,
        "Poster_reputation_count":716.0,
        "Poster_view_count":50.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62820211",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"resolv keyerror brand run experi azur autom machin learn azur autom machin learn sdk train machin learn model dataset experi train iter fail keyerror brand error model train succeed resolv",
        "Question_preprocessed_content":"resolv brand experi azur autom machin learn azur autom machin learn sdk train machin learn model dataset experi train iter fail model train resolv",
        "Question_gpt_summary_original":"The user is encountering a KeyError with the message 'brand' while running an experiment using Azure Automated Machine Learning SDK. All training iterations fail with this error even if the model training itself succeeded. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user encount keyerror messag brand run experi azur autom machin learn sdk train iter fail error model train succeed user seek help resolv issu"
    },
    {
        "Question_title":"Convert a datatable string from Azure ML WS to an actual Datatable C# Object?",
        "Question_body":"<p>Basically I'm receiving an output like this from my azure ws output:<\/p>\n\n<pre><code>{\n    'Results': {\n        'WSOutput': {\n            'type': 'table',\n            'value': {\n                'ColumnNames': ['ID', 'Start', 'Ask', 'Not', 'Passed', 'Suggest'],\n                'ColumnTypes': ['Int32', 'Int32', 'Int32', 'Double', 'Int64', 'Int32'],\n                'Values': [['13256025', '25000', '19000', '0.35', '1', '25000']]\n            }\n        }\n    }\n}\n<\/code><\/pre>\n\n<p>The string, as you can see, has the info to create a datatable object. Now, I can't seem to find an easy way to cast it to an actual datatable POCO. I'm able to manually code a parser with Newtonsoft.Json.Linq but there has to be an easier way. <\/p>\n\n<p>Does anybody know how? I can't seem to find anything on the net.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7.0,
        "Question_created_time":1519929782813,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1519930038023,
        "Question_score_count":2.0,
        "Question_view_count":83.0,
        "Poster_created_time":1324654920387,
        "Poster_location":"Waterloo, ON, Canada",
        "Poster_reputation_count":5211.0,
        "Poster_view_count":449.0,
        "Answer_body":"<p>Yes, there is a open source online gernator on the net (<a href=\"http:\/\/jsonutils.com\/\" rel=\"nofollow noreferrer\">http:\/\/jsonutils.com\/<\/a>). Copy paste your result will give you that:<\/p>\n\n<pre><code> public class Value\n    {\n        public IList&lt;string&gt; ColumnNames { get; set; }\n        public IList&lt;string&gt; ColumnTypes { get; set; }\n        public IList&lt;IList&lt;string&gt;&gt; Values { get; set; }\n    }\n\n    public class WSOutput\n    {\n        public string type { get; set; }\n        public Value value { get; set; }\n    }\n\n    public class Results\n    {\n        public WSOutput WSOutput { get; set; }\n    }\n\n    public class Example\n    {\n        public Results Results { get; set; }\n    }\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1519929928907,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1327234712912,
        "Answerer_location":"Germany",
        "Answerer_reputation_count":53015.0,
        "Answerer_view_count":3262.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49056593",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"convert datat string actual datat object basic receiv output like azur output result wsoutput type tabl valu columnnam start ask pass suggest columntyp int int int doubl int int valu string info creat datat object easi wai cast actual datat poco abl manual code parser newtonsoft json linq easier wai anybodi know net",
        "Question_preprocessed_content":"convert datat string actual datat object receiv output like azur output string info creat datat object easi wai cast actual datat poco abl code parser easier wai anybodi know net",
        "Question_gpt_summary_original":"The user is having trouble converting a datatable string received from Azure ML WS into an actual Datatable C# object. The string contains the necessary information to create a datatable object, but the user is unable to find an easy way to cast it. The user has tried manually coding a parser with Newtonsoft.Json.Linq but is looking for an easier solution.",
        "Question_gpt_summary":"user have troubl convert datat string receiv actual datat object string contain necessari inform creat datat object user unabl easi wai cast user tri manual code parser newtonsoft json linq look easier solut"
    },
    {
        "Question_title":"Undo 'dvc add' operation",
        "Question_body":"<p>I <code>dvc add<\/code>-ed a file I did not mean to add. I have not yet committed.<\/p>\n\n<p>How do I undo this operation? In Git, you would do <code>git rm --cached &lt;filename&gt;<\/code>.<\/p>\n\n<p>To be clear: I want to make DVC forget about the file, and I want the file to remain untouched in my working tree. This is the opposite of what <code>dvc remove<\/code> does.<\/p>\n\n<p>One <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1524\" rel=\"nofollow noreferrer\">issue<\/a> on the DVC issue tracker suggests that <code>dvc unprotect<\/code> is the right command. But reading the <a href=\"https:\/\/dvc.org\/doc\/commands-reference\/unprotect\" rel=\"nofollow noreferrer\">manual page<\/a> suggests otherwise.<\/p>\n\n<p>Is this possible with DVC?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1568689927047,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":1304.0,
        "Poster_created_time":1383611307000,
        "Poster_location":"New York",
        "Poster_reputation_count":10846.0,
        "Poster_view_count":984.0,
        "Answer_body":"<p>As per mroutis on the DVC Discord server:<\/p>\n\n<ol>\n<li><code>dvc unprotect<\/code> the file; this won't be necessary if you don't use <code>symlink<\/code> or <code>hardlink<\/code> caching, but it can't hurt.<\/li>\n<li>Remove the .dvc file<\/li>\n<li>If you need to delete the cache entry itself, run <code>dvc gc<\/code>, or look up the MD5 in <code>data.dvc<\/code> and manually remove it from <code>.dvc\/cache<\/code>.<\/li>\n<\/ol>\n\n<p><em>Edit<\/em> -- there is now an issue on their Github page to add this to the manual: <a href=\"https:\/\/github.com\/iterative\/dvc.org\/issues\/625\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc.org\/issues\/625<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Question_closed_time":1568693889196,
        "Answer_last_edit_time":1568725966083,
        "Answer_score_count":7.0,
        "Answerer_created_time":1383611307000,
        "Answerer_location":"New York",
        "Answerer_reputation_count":10846.0,
        "Answerer_view_count":984.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966851",
        "Tool":"DVC",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"undo add oper add file mean add commit undo oper git git cach clear want forget file want file remain untouch work tree opposit remov issu issu tracker suggest unprotect right command read manual page suggest possibl",
        "Question_preprocessed_content":"undo oper file mean undo oper git clear want forget file want file remain untouch work tracker right read manual page",
        "Question_gpt_summary_original":"The user accidentally added a file using the 'dvc add' command and wants to undo the operation without affecting the file in the working tree. They are seeking guidance on how to make DVC forget about the file, and are unsure if 'dvc unprotect' is the right command to use.",
        "Question_gpt_summary":"user accident ad file add command want undo oper affect file work tree seek guidanc forget file unsur unprotect right command us"
    },
    {
        "Question_title":"How to assign two or more time series identifier columns in Vertex AI Tabular Forecasting",
        "Question_body":"<p>I was wondering if it is possible to have more than one time series identifier column in the model? Let's assume I'd like to create a forecast at a product and store level (which the documentation suggests should be possible).<\/p>\n<p>If I select product as the series identifier, the only options I have left for store is either a covariate or an attribute and neither is applicable in this scenario.<\/p>\n<p>Would concatenating product and store and using the individual product and store code values for that concatenated ID as attributes be a solution? It doesn't feel right, but I can't see any other option - am I missing something?<\/p>\n<p>Note: I understand that this feature of Vertex AI is currently in preview and that because of that the options may be limited.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1632228068913,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":269.0,
        "Poster_created_time":1519630645620,
        "Poster_location":"Northampton, UK",
        "Poster_reputation_count":333.0,
        "Poster_view_count":46.0,
        "Answer_body":"<p>There isn't an alternate way to assign 2 or more <strong>Time Series Identifiers<\/strong> in the <strong>Forecasting Model<\/strong> on <strong>Vertex AI<\/strong>. The &quot;<strong>Forecasting model<\/strong>&quot; is in the &quot;<strong>Preview<\/strong>&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a>, as you are aware, with all consequences of that fact the options are limited. Please refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">doc<\/a> for more information about the best practices for data preparation to train the forecasting model.<\/p>\n<p>As a workaround, the two columns can be concatenated and assigned a Time Series Identifier on that concatenated column, as you have mentioned in the question. This way, the concatenated column carries more contextual information into the training of the model.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1632376096768,
        "Answer_last_edit_time":1632482492492,
        "Answer_score_count":2.0,
        "Answerer_created_time":1619174589310,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation_count":1200.0,
        "Answerer_view_count":804.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69269073",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level document suggest possibl select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_preprocessed_content":"time seri identifi column tabular forecast wonder time seri identifi column model let like creat forecast product store level select product seri identifi option left store covari scenario concaten product store individu product store code valu concaten solut right option note understand featur preview option limit",
        "Question_gpt_summary_original":"The user is facing a challenge in assigning two or more time series identifier columns in Vertex AI Tabular Forecasting. They want to create a forecast at a product and store level, but the only options available for store are either a covariate or an attribute, which are not applicable in this scenario. The user is considering concatenating product and store and using the individual product and store code values for that concatenated ID as attributes, but they are unsure if this is the right solution. The user acknowledges that this feature is currently in preview and the options may be limited.",
        "Question_gpt_summary":"user face challeng assign time seri identifi column tabular forecast want creat forecast product store level option avail store covari attribut applic scenario user consid concaten product store individu product store code valu concaten attribut unsur right solut user acknowledg featur current preview option limit"
    },
    {
        "Question_title":"DocumentAI \u2013 Internal Server Error when training on pre-labeled documents",
        "Question_body":"We want to automate training of our custom extraction processor by sending PDFs to the OCR processor, adding entities to the document for the values that we know of, uploading it to GCS as .json, and training the processor on our document.\u00a0\n\nOn our initial tries\u00a0we get the following error:\n\n\u00a0\n\n\u00a0\n\n{\n  \"name\": \"projects\/818666290880\/locations\/us\/operations\/8089109272130466059\",\n  \"done\": true,\n  \"result\": \"error\",\n  \"response\": {},\n  \"metadata\": {\n    \"@type\": \"type.googleapis.com\/google.cloud.documentai.uiv1beta3.ImportDocumentsMetadata\",\n    \"commonMetadata\": {\n      \"state\": \"FAILED\",\n      \"createTime\": \"2023-05-04T21:36:28.268708Z\",\n      \"updateTime\": \"2023-05-04T21:36:29.172396Z\",\n      \"resource\": \"projects\/818666290880\/locations\/us\/processors\/a57d00c6e1c2727\/dataset\"\n    },\n    \"individualImportStatuses\": [\n      {\n        \"inputGcsSource\": \"gs:\/\/mybucket.appspot.com\/path\/to\/00d45d9f-c6e3-4937-aa5c-59f395cfb5f0.json\",\n        \"status\": {\n          \"code\": 13,\n          \"message\": \"Internal error encountered.\"\n        }\n      }\n    ],\n    \"totalDocumentCount\": 1\n  },\n  \"error\": {\n    \"code\": 13,\n    \"message\": \"Internal error encountered.\",\n    \"details\": []\n  }\n}\n\n\u00a0\n\nThere is no log in Cloud Logging related to Document AI. The operation is\u00a0\n\nprojects\/818666290880\/locations\/us\/operations\/8089109272130466059.\n\u00a0\nIs there a way for us to determine what it causing the error? Thank you",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1683212580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":54.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/DocumentAI-Internal-Server-Error-when-training-on-pre-labeled\/td-p\/550466\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"documentai intern server error train pre label document want autom train custom extract processor send pdf ocr processor ad entiti document valu know upload gc json train processor document initi triesw follow error project locat oper true result error respons metadata type type googleapi com googl cloud documentai uivbeta importdocumentsmetadata commonmetadata state fail createtim updatetim resourc project locat processor adcec dataset individualimportstatus inputgcssourc mybucket appspot com path ddf aac fcfbf json statu code messag intern error encount totaldocumentcount error code messag intern error encount detail log cloud log relat document oper project locat oper wai determin caus error thank",
        "Question_preprocessed_content":"documentai intern server train document want autom train custom extract send pdf ocr entiti document valu know upload gc json train document initi triesw true result respons metadata individualimportstatus totaldocumentcount log cloud relat document oper wai determin caus thank",
        "Question_gpt_summary_original":"The user is encountering an internal server error while trying to automate the training of a custom extraction processor using pre-labeled PDF documents in DocumentAI. The error message shows that the operation has failed due to an internal error, and there are no logs available in Cloud Logging to determine the cause of the error. The user is seeking assistance in identifying the root cause of the issue.",
        "Question_gpt_summary":"user encount intern server error try autom train custom extract processor pre label pdf document documentai error messag show oper fail intern error log avail cloud log determin caus error user seek assist identifi root caus issu"
    },
    {
        "Question_title":"Why DVC not working with AWS role on AWS Batch?",
        "Question_body":"<p>I\u2019m using DVC on AWS Batch, and have configured the job definition with a role with correct S3 policies (I know its correct because it worked on github actions). But, when running the job with dvc pull I got this error:<\/p>\n<pre><code class=\"lang-auto\">dvc pull -R \/app\/data\/training\nERROR: unexpected error - Forbidden: An error occurred (403) when calling the HeadObject operation: Forbidden\n<\/code><\/pre>\n<p>The same job, if runned with the envs AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY configured, runs ok.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1646921366784,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":189.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/why-dvc-not-working-with-aws-role-on-aws-batch\/1114",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"work aw role aw batch aw batch configur job definit role correct polici know correct work github action run job pull got error pull app data train error unexpect error forbidden error occur call headobject oper forbidden job run env aw access kei aw secret access kei configur run",
        "Question_preprocessed_content":"work aw role aw batch aw batch configur job definit role polici job got job env configur run",
        "Question_gpt_summary_original":"The user is facing an issue with DVC not working with AWS role on AWS Batch. The job definition has been configured with a role with correct S3 policies, but when running the job with dvc pull, the user is getting a 403 Forbidden error. The job runs fine when run with the envs AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY configured.",
        "Question_gpt_summary":"user face issu work aw role aw batch job definit configur role correct polici run job pull user get forbidden error job run fine run env aw access kei aw secret access kei configur"
    },
    {
        "Question_title":"Manually deleted .dvc files -- these files still appear to be tracked by DVC",
        "Question_body":"<p>Hello,<\/p>\n<p>I have manually deleted .dvc files that were previously tracked (and the tracked files themselves), expecting this to remove them from tracking (I realise now I should have used dvc gc).<\/p>\n<p>My question is how do I now remove these from tracking? I cannot re-add then remove them properly as the original files are not there.<\/p>\n<p>I am syncing data in two machines, on the machine I deleted the files on it says everything is up to date but on my other machine I am getting cache errors for the now deleted items. I am confused as I have pushed the deletes of the .dvc files so I assumed this would prevent them being tracked on both machines. Are they tracked somewhere else? I do not have a dvc.yaml file.<\/p>\n<p>Thanks,<\/p>\n<p>Justin<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1611306516441,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":239.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/manually-deleted-dvc-files-these-files-still-appear-to-be-tracked-by-dvc\/634",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"manual delet file file appear track hello manual delet file previous track track file expect remov track realis question remov track add remov properli origin file sync data machin machin delet file sai date machin get cach error delet item confus push delet file assum prevent track machin track yaml file thank justin",
        "Question_preprocessed_content":"delet file file track delet file previous track expect remov track question remov track remov properli origin file sync data machin machin delet file sai date machin cach delet item confus push delet file prevent track machin track yaml file thank justin",
        "Question_gpt_summary_original":"The user manually deleted .dvc files that were previously tracked, but they still appear to be tracked by DVC. The user is unsure how to remove them from tracking and is experiencing cache errors on another machine. The user has pushed the deletes of the .dvc files but is confused as to why they are still being tracked.",
        "Question_gpt_summary":"user manual delet file previous track appear track user unsur remov track experienc cach error machin user push delet file confus track"
    },
    {
        "Question_title":"sagemaker xgboost output to be JSON",
        "Question_body":"<p>I am new to AWS sagemaker and trying to do a simple test, where I am trying to call the xgboost model.<\/p>\n<pre><code>xgboost_container = sagemaker.image_uris.retrieve(&quot;xgboost&quot;, 'us-east-1', &quot;1.2-1&quot;)\n<\/code><\/pre>\n<p>Creating the model and endpoint:<\/p>\n<pre><code>from sagemaker.serializers import JSONSerializer\nendp_name =&quot;myendpoint&quot;\nacc_model = sm_model.deploy(initial_instance_count=1, \n                instance_type='ml.m5.4xlarge',\n                endpoint_name=endp_name, \n                serializer=JSONSerializer(),\n                deserializer= sagemaker.deserializers.JSONDeserializer()\n                            \n               )\n<\/code><\/pre>\n<p>Creating the predictor instance:<\/p>\n<pre><code>from sagemaker.predictor import Predictor\nfrom sagemaker.serializers import CSVSerializer, JSONSerializer \n\nsess = sagemaker.Session()\n\npayload ={\n&quot;var1&quot;:1,\n&quot;var2&quot;:2,\n&quot;var3&quot;:3,\n&quot;var4&quot;:0,\n&quot;var5&quot;:4,\n&quot;var6&quot;:0,\n&quot;var7&quot;:5,\n&quot;var8&quot;:45,\n\n}\n\npredictor = Predictor(\n    endpoint_name=endp_name, sagemaker_session=sess, serializer=JSONSerializer(),\ndeserializer=JSONSerializer()  )\n<\/code><\/pre>\n<p>and then predicting:<\/p>\n<pre><code>predictor.predict(payload)\n<\/code><\/pre>\n<p>I want the output of the predictor.predict to be JSON format, however when I run this I get this.\n<a href=\"https:\/\/i.stack.imgur.com\/gnlxl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gnlxl.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What need to be done so that I can see the output as JSON?<\/p>\n<p>PS: If I remove the deserializer I get the output as byte:\n<a href=\"https:\/\/i.stack.imgur.com\/0ncvK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0ncvK.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>and if I change it to CSVDeserializer I get it out as:\n<a href=\"https:\/\/i.stack.imgur.com\/bv7L6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bv7L6.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1622515828260,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1622516341412,
        "Question_score_count":0.0,
        "Question_view_count":261.0,
        "Poster_created_time":1469929726907,
        "Poster_location":null,
        "Poster_reputation_count":101.0,
        "Poster_view_count":30.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67782131",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"xgboost output json new try simpl test try xgboost model xgboost contain imag uri retriev xgboost east creat model endpoint serial import jsonseri endp myendpoint acc model model deploi initi instanc count instanc type xlarg endpoint endp serial jsonseri deseri deseri jsondeseri creat predictor instanc predictor import predictor serial import csvserial jsonseri sess session payload var var var var var var var var predictor predictor endpoint endp session sess serial jsonseri deseri jsonseri predict predictor predict payload want output predictor predict json format run need output json remov deseri output byte chang csvdeseri",
        "Question_preprocessed_content":"output json new try simpl test try model creat model endpoint creat predictor instanc predict want output json format run output json remov deseri output byte chang csvdeseri",
        "Question_gpt_summary_original":"The user is trying to call an xgboost model in AWS Sagemaker and wants the output of the predictor to be in JSON format. They have created a model and endpoint, and a predictor instance with a JSON serializer and deserializer. However, when they run the predictor, they get the output in byte or CSV format. They are seeking guidance on how to get the output in JSON format.",
        "Question_gpt_summary":"user try xgboost model want output predictor json format creat model endpoint predictor instanc json serial deseri run predictor output byte csv format seek guidanc output json format"
    },
    {
        "Question_title":"Semi-transparent smoothing stopped working",
        "Question_body":"<p>Smoothing suddenly stopped making the original graph semi-transparent.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png\" data-download-href=\"\/uploads\/short-url\/6HJfxA7ERNAiH7FwWeL8MzvFDIi.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_512x500.png\" alt=\"image\" data-base62-sha1=\"6HJfxA7ERNAiH7FwWeL8MzvFDIi\" width=\"512\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_512x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2efe82c897ae30783052f01c9383e9582d1f229e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">517\u00d7504 48.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Any way your could rollback to the previous behavior? Some ETA for fixing this would be much appreciated, so our team could plan accordingly.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_created_time":1639019539043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":402.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/semi-transparent-smoothing-stopped-working\/1492",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"semi transpar smooth stop work smooth suddenli stop make origin graph semi transpar imag wai rollback previou behavior eta fix appreci team plan accordingli",
        "Question_preprocessed_content":"work make origin graph imag wai previou behavior eta fix team plan",
        "Question_gpt_summary_original":"The user is facing an issue where the semi-transparent smoothing feature has stopped working, resulting in the original graph not being semi-transparent anymore. They are requesting a rollback to the previous behavior and an ETA for fixing the issue.",
        "Question_gpt_summary":"user face issu semi transpar smooth featur stop work result origin graph semi transpar anymor request rollback previou behavior eta fix issu"
    },
    {
        "Question_title":"How to do \"Pnael Export\", I found no button of it",
        "Question_body":"<p>I want to export all log from cloud like this \" <a href=\"https:\/\/wandb.ai\/site\/articles\/export-data-from-wb\">Export Your Data from W&amp;B on Weights &amp; Biases (wandb.ai)<\/a>\". However, there  is no option for \u201cPanel Export\u201d now. How should I do?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_created_time":1661400785513,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":207.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-do-pnael-export-i-found-no-button-of-it\/2991",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pnael export button want export log cloud like export data option panel export",
        "Question_preprocessed_content":"pnael export want export log cloud like export data option panel export",
        "Question_gpt_summary_original":"The user is trying to export all logs from the cloud using \"Panel Export\" but cannot find the option for it. They are seeking guidance on how to proceed with the export.",
        "Question_gpt_summary":"user try export log cloud panel export option seek guidanc proce export"
    },
    {
        "Question_title":"Understanding Intermediate Values and Pruning in Optuna",
        "Question_body":"<p>I am just curious for more information on what an intermediate step actually is and how to use pruning if you're using a different ml library that isn't in the tutorial section eg) XGB, Pytorch etc.<\/p>\n<p>For example:<\/p>\n<pre><code>X, y = load_iris(return_X_y=True)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)\nclasses = np.unique(y)\nn_train_iter = 100\n\ndef objective(trial):\n    global num_pruned\n    alpha = trial.suggest_float(&quot;alpha&quot;, 0.0, 1.0)\n    clf = SGDClassifier(alpha=alpha)\n    for step in range(n_train_iter):\n        clf.partial_fit(X_train, y_train, classes=classes)\n\n        intermediate_value = clf.score(X_valid, y_valid)\n        trial.report(intermediate_value, step)\n\n        if trial.should_prune():\n            raise optuna.TrialPruned()\n\n    return clf.score(X_valid, y_valid)\n\n\nstudy = optuna.create_study(\n    direction=&quot;maximize&quot;,\n    pruner=optuna.pruners.HyperbandPruner(\n        min_resource=1, max_resource=n_train_iter, reduction_factor=3\n    ),\n)\nstudy.optimize(objective, n_trials=30)\n<\/code><\/pre>\n<p>What is the point of the <code>for step in range()<\/code> section? Doesn't doing this just make the optimisation take more time and won't you yield the same result for every step in the loop?<\/p>\n<p>I'm really trying to figure out the need for <code>for step in range()<\/code> and is it required every time you wish to use pruning?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1637069214753,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":841.0,
        "Poster_created_time":1482279816096,
        "Poster_location":null,
        "Poster_reputation_count":138.0,
        "Poster_view_count":28.0,
        "Answer_body":"<p>The basic model creation can be done by passing a complete training datasets once. But there are models that can still be improved (an increase in accuracy) by re-training again on the same training datasets.<\/p>\n<p>To see to it that we are not wasting resources here, we would check the accuracy after every step using the validation datasets via <code>intermediate_score<\/code> if accuracy improves, if not we prune the whole trial skipping other steps. Then we go for next trial asking another value of alpha - the hyperparameter that we are trying to determine to have the greatest accuracy on the validation datasets.<\/p>\n<p>For other libraries, it is just a matter of asking ourselves what do we want with our model, accuracy for sure is a good criteria to measure the model's competency. There can be others.<\/p>\n<p>Example optuna pruning, I want the model to continue re-training but only at my specific conditions. If intermediate value cannot defeat my best_accuracy and if steps are already more than half of my max iteration then prune this trial.<\/p>\n<pre><code>best_accuracy = 0.0\n\n\ndef objective(trial):\n    global best_accuracy\n\n    alpha = trial.suggest_float(&quot;alpha&quot;, 0.0, 1.0)\n    clf = SGDClassifier(alpha=alpha)\n\n    for step in range(n_train_iter):\n        clf.partial_fit(X_train, y_train, classes=classes)\n\n        if step &gt; n_train_iter\/\/2:\n            intermediate_value = clf.score(X_valid, y_valid)\n\n            if intermediate_value &lt; best_accuracy:\n                raise optuna.TrialPruned()\n\n    best_accuracy = clf.score(X_valid, y_valid)\n\n    return best_accuracy\n<\/code><\/pre>\n<p>Optuna has specialized pruners at <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/pruners.html<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1637119128723,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1634692867416,
        "Answerer_location":"Philippines",
        "Answerer_reputation_count":3105.0,
        "Answerer_view_count":290.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69990009",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"understand intermedi valu prune curiou inform intermedi step actual us prune differ librari isn tutori section xgb pytorch exampl load iri return true train valid train valid train test split class uniqu train iter def object trial global num prune alpha trial suggest float alpha clf sgdclassifi alpha alpha step rang train iter clf partial fit train train class class intermedi valu clf score valid valid trial report intermedi valu step trial prune rais trialprun return clf score valid valid studi creat studi direct maxim pruner pruner hyperbandprun min resourc max resourc train iter reduct factor studi optim object trial point step rang section optimis time won yield result step loop try figur need step rang requir time wish us prune",
        "Question_preprocessed_content":"understand intermedi valu prune curiou inform intermedi step us prune librari isn tutori section xgb pytorch exampl point section optimis time won yield result step try figur requir time wish us prune",
        "Question_gpt_summary_original":"The user is seeking clarification on intermediate values and pruning in Optuna. They are specifically curious about the purpose of the \"for step in range()\" section in the code and whether it is necessary for using pruning. They also want to know how to use pruning with ML libraries other than those in the tutorial section, such as XGB and Pytorch.",
        "Question_gpt_summary":"user seek clarif intermedi valu prune specif curiou purpos step rang section code necessari prune want know us prune librari tutori section xgb pytorch"
    },
    {
        "Question_title":"Failing to create multi-model endpoint",
        "Question_body":"I have been trying to create a multi-model endpoint with my own container, using the instructions here: <https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html>.  \n  \nFollowing the instructions here, I am able to successfully create a model and endpoint configuration:   <https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/create-multi-model-endpoint-sdk.html>   \n  \nHowever, when I try to create the endpoint itself, it shows the status of \"Creating\" for over 2 hours, before finally stopping with the status, \"Failed\". It gives no reason for the failure or any other help.  \n  \nDoes anyone have any ideas?  \n  \nThanks!",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1613660927000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668062163303,
        "Question_score_count":0.0,
        "Question_view_count":82.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcIYmAjUlRn-b_PqcIjk08A\/failing-to-create-multi-model-endpoint",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"fail creat multi model endpoint try creat multi model endpoint contain instruct follow instruct abl successfulli creat model endpoint configur try creat endpoint show statu creat hour final stop statu fail give reason failur help idea thank",
        "Question_preprocessed_content":"fail creat endpoint try creat endpoint contain instruct instruct abl creat model endpoint configur try creat endpoint show statu creat hour statu fail give reason failur help idea thank",
        "Question_gpt_summary_original":"The user is facing challenges in creating a multi-model endpoint with their own container on AWS SageMaker. They were able to create a model and endpoint configuration successfully, but the endpoint creation process failed after showing \"Creating\" status for over 2 hours without any reason or help. The user is seeking ideas to resolve this issue.",
        "Question_gpt_summary":"user face challeng creat multi model endpoint contain abl creat model endpoint configur successfulli endpoint creation process fail show creat statu hour reason help user seek idea resolv issu"
    },
    {
        "Question_title":"Deploying model from Azure ML to AKS inference cluster failing with 504 error",
        "Question_body":"<p>We are trying to deploy model from ML onto Inference cluster in AKS. While training is successful, during deployment it fails with 504 error. Not sure if this is due to an issue on ML end or on AKS<\/p>\n<p>WebserviceException:  <br \/>\nMessage: Received bad response from Model Management Service:  <br \/>\nResponse Code: 504  <br \/>\nHeaders: {'Server': 'nginx\/1.21.1', 'Date': 'Wed, 13 Oct 2021 08:13:26 GMT', 'Content-Type': 'text\/html', 'Content-Length': '160', 'Connection': 'keep-alive', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'x-request-time': '180.024'}  <br \/>\nContent: b'&lt;html&gt;\\r\\n&lt;head&gt;&lt;title&gt;504 Gateway Time-out&lt;\/title&gt;&lt;\/head&gt;\\r\\n&lt;body&gt;\\r\\n&lt;center&gt;&lt;h1&gt;504 Gateway Time-out&lt;\/h1&gt;&lt;\/center&gt;\\r\\n&lt;hr&gt;&lt;center&gt;nginx&lt;\/center&gt;\\r\\n&lt;\/body&gt;\\r\\n&lt;\/html&gt;\\r\\n'  <br \/>\nInnerException None  <br \/>\nErrorResponse  <br \/>\n{  <br \/>\n&quot;error&quot;: {  <br \/>\n&quot;message&quot;: &quot;Received bad response from Model Management Service:\\nResponse Code: 504\\nHeaders: {'Server': 'nginx\/1.21.1', 'Date': 'Wed, 13 Oct 2021 08:13:26 GMT', 'Content-Type': 'text\/html', 'Content-Length': '160', 'Connection': 'keep-alive', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains; preload', 'X-Content-Type-Options': 'nosniff', 'x-request-time': '180.024'}\\nContent: b'&lt;html&gt;\\r\\n&lt;head&gt;&lt;title&gt;504 Gateway Time-out&lt;\/title&gt;&lt;\/head&gt;\\r\\n&lt;body&gt;\\r\\n&lt;center&gt;&lt;h1&gt;504 Gateway Time-out&lt;\/h1&gt;&lt;\/center&gt;\\r\\n&lt;hr&gt;&lt;center&gt;nginx&lt;\/center&gt;\\r\\n&lt;\/body&gt;\\r\\n&lt;\/html&gt;\\r\\n'&quot;  <br \/>\n}  <br \/>\n}<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1634117582693,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/588825\/deploying-model-from-azure-ml-to-aks-inference-clu",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi model ak infer cluster fail error try deploi model infer cluster ak train success deploy fail error sure issu end ak webserviceexcept messag receiv bad respons model manag servic respons code header server nginx date wed oct gmt content type text html content length connect aliv strict transport secur max ag includesubdomain preload content type option nosniff request time content gatewai time gatewai time nnginx innerexcept errorrespons error messag receiv bad respons model manag servic nrespons code nheader server nginx date wed oct gmt content type text html content length connect aliv strict transport secur max ag includesubdomain preload content type option nosniff request time ncontent gatewai time gatewai time nnginx",
        "Question_preprocessed_content":"deploi model ak infer cluster fail try deploi model infer cluster ak train deploy fail sure end ak receiv bad respons model manag servic respons code header content gatewai gatewai gatewai gatewai",
        "Question_gpt_summary_original":"The user is encountering a 504 error while trying to deploy a model from Azure ML to an inference cluster in AKS, and is unsure if the issue is on the ML or AKS end. The error message indicates a bad response from the Model Management Service with a response code of 504.",
        "Question_gpt_summary":"user encount error try deploi model infer cluster ak unsur issu ak end error messag indic bad respons model manag servic respons code"
    },
    {
        "Question_title":"Problem with connecting python script with web service output",
        "Question_body":"<p>Azure ML studio got new apperance and some problems happen. I did pipeline like this:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/204312-1.png?platform=QnA\" alt=\"204312-1.png\" \/>    <\/p>\n<p>and then created inference pipeline:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/204256-2.png?platform=QnA\" alt=\"204256-2.png\" \/>    <\/p>\n<p>I would like to connect python script like this(in the previous version on azure it works) to Web Service Output:    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/204187-image.png?platform=QnA\" alt=\"204187-image.png\" \/>    <\/p>\n<p>But when i submit it now the connection is changing and execution is not apply.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/204304-image.png?platform=QnA\" alt=\"204304-image.png\" \/>    <\/p>\n<p>How to fix this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1653133927230,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/858550\/problem-with-connecting-python-script-with-web-ser",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"problem connect python script web servic output studio got new apper problem happen pipelin like creat infer pipelin like connect python script like previou version azur work web servic output submit connect chang execut appli fix issu",
        "Question_preprocessed_content":"problem python script web servic output studio got new problem pipelin like creat infer pipelin like python script like web servic output submit chang execut fix",
        "Question_gpt_summary_original":"The user is facing an issue with connecting a Python script to the Web Service Output in Azure ML Studio's new version. The connection is changing and the execution is not applying. The user is seeking a solution to fix this issue.",
        "Question_gpt_summary":"user face issu connect python script web servic output studio new version connect chang execut appli user seek solut fix issu"
    },
    {
        "Question_title":"Using Python visual in Power BI for calling ML Azure rest API works in desktop version but not when published",
        "Question_body":"<p>I created a dashboard in Power BI desktop. I have a trained model from ML Azure which I already deployed and has it's rest API. I need to call this rest API from the dashboard itself using measures I created (not from the query editor). I did it using the Python visual to send the input data and get back the output from the rest API and plotting the result (a number). This works perfectly in the desktop version. I need to publish this dashboard to share with other members of my organization but in the web version the script gives a runtime error. ![69200-capture.png][1] How to make it work? [1]: \/api\/attachments\/69200-capture.png?platform=QnA<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1613593697693,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/277404\/using-python-visual-in-power-bi-for-calling-ml-azu",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"python visual power call azur rest api work desktop version publish creat dashboard power desktop train model azur deploi rest api need rest api dashboard measur creat queri editor python visual send input data output rest api plot result number work perfectli desktop version need publish dashboard share member organ web version script give runtim error captur png work api attach captur png platform qna",
        "Question_preprocessed_content":"python visual power azur rest api work desktop version publish creat dashboard power desktop train model azur deploi rest api rest api dashboard measur creat python visual send input data output rest api result work perfectli desktop version publish dashboard share member organ web version script give runtim work",
        "Question_gpt_summary_original":"The user created a dashboard in Power BI desktop and used Python visual to call a trained model's rest API from ML Azure to get output and plot the result. It works fine in the desktop version but gives a runtime error when published on the web version. The user seeks a solution to make it work on the web version.",
        "Question_gpt_summary":"user creat dashboard power desktop python visual train model rest api azur output plot result work fine desktop version give runtim error publish web version user seek solut work web version"
    },
    {
        "Question_title":"Dvc.yaml file deletes",
        "Question_body":"<p>Hello all,<br>\nI am new here. I have been working on a project for sometime and it used data from devices for energy systems and then defines the best model for machine learning on that data. My question is whenever I use <code>dvc repro<\/code> and run the pipeline after the complete run which is after training stage, the dvc.yaml file deletes itself from the folder of the device which I manually created. Any help as to how I can prevent this file deleting by itself? Thank you.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1679476703857,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":35.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-yaml-file-deletes\/1573",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"yaml file delet hello new work project data devic energi system defin best model machin learn data question us repro run pipelin complet run train stage yaml file delet folder devic manual creat help prevent file delet thank",
        "Question_preprocessed_content":"yaml file delet new work project data devic energi system defin best model machin learn data question us run pipelin complet run train stage yaml file delet folder devic creat help prevent file delet thank",
        "Question_gpt_summary_original":"The user is facing an issue where the dvc.yaml file gets deleted from the device folder after running the pipeline using dvc repro. They are seeking help to prevent the file from deleting itself.",
        "Question_gpt_summary":"user face issu yaml file get delet devic folder run pipelin repro seek help prevent file delet"
    },
    {
        "Question_title":"AWS Sagemaker | region specific security credentials for endpoint",
        "Question_body":"<p>I am new to AWS infra and currently doing some POC\/Feasibility for new work.<\/p>\n\n<p>So I have created a S3 bucket in Ireland server, train and publish Sagemaker endpoint in Ireland server and its giving result in Jupyter notebook there. Now I want to use that endpoint in my browser javascript library to show some graphics. When I try to test my endpoint in Postman then its giving region specific error <\/p>\n\n<pre><code> {\n        \"message\": \"Credential should be scoped to a valid region, not 'us-east-1'. \nCredential should be scoped to correct service: 'sagemaker'. \"\n }\n<\/code><\/pre>\n\n<p>My AWS account is not yet enterprise managed so I am using as 'root user', Whenever I go to my profile>Security_Credential page and generate any security credential then it always create for 'us-east-1' region, As Sagemaker is region specific service, I am not able to find the way to create region specific security key for root user, can someone please help<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1526107432860,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":750.0,
        "Poster_created_time":1501403168107,
        "Poster_location":"Delhi, India",
        "Poster_reputation_count":1370.0,
        "Poster_view_count":125.0,
        "Answer_body":"<p>You should create an IAM role first that defines what should be permitted (mainly calling the invoke-endpoint API call for SageMaker runtime). Then you should create an IAM user, add the above role to that user, and then generate credentials that you can use in your Postman to call the service. Here you can find some details about the IAM role for SageMaker that you can use in this process: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/using-identity-based-policies.html<\/a><\/p>\n\n<p>A popular option to achieve external access to a SageMaker endpoint, is to create an API Gateway that calls a Lambda Function that is then calling the invoke-endpoint API. This chain is giving you various options such as different authentication options for the users and API keys as part of API-GW, processing the user input and inference output using API-GW and Lambda code, and giving the permission to call the SageMaker endpoint to the Lambda function. This chain removes the need for the credentials creation, update and distribution.  <\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1526179930343,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50303607",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"region specif secur credenti endpoint new aw infra current poc feasibl new work creat bucket ireland server train publish endpoint ireland server give result jupyt notebook want us endpoint browser javascript librari graphic try test endpoint postman give region specif error messag credenti scope valid region east credenti scope correct servic aw account enterpris manag root user profil secur credenti page gener secur credenti creat east region region specif servic abl wai creat region specif secur kei root user help",
        "Question_preprocessed_content":"region specif secur credenti endpoint new aw infra new work creat bucket ireland server train publish endpoint ireland server give result jupyt want us endpoint browser javascript librari graphic try test endpoint postman give region specif aw enterpris manag user page gener secur credenti creat region region specif servic abl wai creat region specif secur kei user help",
        "Question_gpt_summary_original":"The user is facing an issue with AWS Sagemaker where they are unable to create region-specific security credentials for their endpoint. They have created an S3 bucket and trained and published the Sagemaker endpoint in Ireland server, but when they try to test the endpoint in Postman, they receive a region-specific error. As the user is using a root user account, they are unable to generate region-specific security credentials for Sagemaker.",
        "Question_gpt_summary":"user face issu unabl creat region specif secur credenti endpoint creat bucket train publish endpoint ireland server try test endpoint postman receiv region specif error user root user account unabl gener region specif secur credenti"
    },
    {
        "Question_title":"Azure Auto ML forecasting - dependent variables",
        "Question_body":"<p>Hi, I am trying to use Azure auto ML for forecasting. The dataset has a date_time column, a target variable and other columns that affect the target variable. I have deployed the model as a web service. But I am finding it hard to use the service\/model for forecasting future frames. Let's say I need to forecast for the next 4 hours (data frequency is 5 minutes), but the model is asking for other column inputs as well.  Can you please help me to resolve this?   <br \/>\nTIA,  <br \/>\nRajesh<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1613448096113,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/274229\/azure-auto-ml-forecasting-dependent-variables",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur auto forecast depend variabl try us azur auto forecast dataset date time column target variabl column affect target variabl deploi model web servic find hard us servic model forecast futur frame let need forecast hour data frequenc minut model ask column input help resolv tia rajesh",
        "Question_preprocessed_content":"azur auto forecast depend variabl try us azur auto forecast dataset column target variabl column target variabl deploi model web servic find hard us forecast futur frame let forecast hour model ask column input help resolv tia rajesh",
        "Question_gpt_summary_original":"The user is facing challenges while using Azure Auto ML for forecasting as the model is asking for other column inputs along with the target variable and date_time column. The user needs help in resolving this issue to forecast for the next 4 hours with a data frequency of 5 minutes.",
        "Question_gpt_summary":"user face challeng azur auto forecast model ask column input target variabl date time column user need help resolv issu forecast hour data frequenc minut"
    },
    {
        "Question_title":"Support@wandb.com doesn't work",
        "Question_body":"<p>I emailed <a href=\"mailto:support@wandb.com\">support@wandb.com<\/a> following instructions from <a href=\"https:\/\/docs.wandb.ai\/company\/getting-help\">Support - Documentation<\/a><\/p>\n<p>However, my email doesn\u2019t go through and I get the following automated reply:<\/p>\n<blockquote>\n<p>We\u2019re writing to let you know that the group you tried to contact (support) may not exist, or you may not have permission to post messages to the group. A few more details on why you weren\u2019t able to post:<\/p>\n<ul>\n<li>You might have spelled or formatted the group name incorrectly.<\/li>\n<li>The owner of the group may have removed this group.<\/li>\n<li>You may need to join the group before receiving permission to post.<\/li>\n<li>This group may not be open to posting.<\/li>\n<\/ul>\n<\/blockquote>\n<p>I also can\u2019t seem to access the Zendesk chat widget, even with ad blockers and all extensions removed.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1663306668094,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":95.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/support-wandb-com-doesnt-work\/3122",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"support com work email support com follow instruct support document email doesnt follow autom repli write let know group tri contact support exist permiss post messag group detail werent abl post spell format group incorrectli owner group remov group need join group receiv permiss post group open post access zendesk chat widget blocker extens remov",
        "Question_preprocessed_content":"work email instruct document email doesnt autom repli write let know group tri contact exist post group detail werent abl post group owner group remov group join group receiv post group open post zendesk chat widget blocker extens remov",
        "Question_gpt_summary_original":"The user is facing challenges in reaching out to support@wandb.com as their email is not going through and they are receiving an automated reply stating that the group may not exist or they may not have permission to post messages. Additionally, the user is unable to access the Zendesk chat widget even after removing ad blockers and extensions.",
        "Question_gpt_summary":"user face challeng reach support com email go receiv autom repli state group exist permiss post messag addition user unabl access zendesk chat widget remov blocker extens"
    },
    {
        "Question_title":"Endpoints test inference gives error 415",
        "Question_body":"Hi,\n\nI deploy a abalone test model, and now I want to test it through Enpoint Test Inference, it gives me an error:\n\n```\nError invoking endpoint: Received client error (415) from primary with message \"application\/json is not an accepted ContentType: csv, libsvm, parquet, recordio-protobuf, text\/csv, text\/libsvm, text\/x-libsvm, application\/x-parquet, application\/x-recordio-protobuf.\". See https:\/\/ap-northeast-1.console.aws.amazon.com\/cloudwatch\/home?region=ap-northeast-1#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/abaloneTest in account 556267670448 for more information.\n```\nI understand the error contentType is not accepting value \"application\/json\", but I am not able to find out how can i update the default value from \"application\/json\" to any permitted value and test endpoint with test inference ui only?\nIt will be good if ui provides a sample request format, so that we can test it fast.",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1675495710193,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1675842228648,
        "Question_score_count":0.0,
        "Question_view_count":46.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5vZleUxqRTKF5GnF4d4FBQ\/endpoints-test-inference-gives-error-415",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"endpoint test infer give error deploi abalon test model want test enpoint test infer give error error invok endpoint receiv client error primari messag applic json accept contenttyp csv libsvm parquet recordio protobuf text csv text libsvm text libsvm applic parquet applic recordio protobuf http northeast consol aw amazon com cloudwatch home region northeast logeventview group aw endpoint abalonetest account inform understand error contenttyp accept valu applic json abl updat default valu applic json permit valu test endpoint test infer good provid sampl request format test fast",
        "Question_preprocessed_content":"endpoint test infer give deploi abalon test model want test enpoint test infer give understand valu abl updat default valu valu test endpoint test infer provid sampl request format test fast",
        "Question_gpt_summary_original":"The user is encountering an error while testing their deployed abalone test model through Endpoint Test Inference. The error message indicates that the content type \"application\/json\" is not accepted and suggests using one of the permitted values such as csv, libsvm, parquet, recordio-protobuf, text\/csv, text\/libsvm, text\/x-libsvm, application\/x-parquet, or application\/x-recordio-protobuf. The user is seeking guidance on how to update the default value and test the endpoint with test inference UI, and suggests that a sample request format in the UI would be helpful.",
        "Question_gpt_summary":"user encount error test deploi abalon test model endpoint test infer error messag indic content type applic json accept suggest permit valu csv libsvm parquet recordio protobuf text csv text libsvm text libsvm applic parquet applic recordio protobuf user seek guidanc updat default valu test endpoint test infer suggest sampl request format help"
    },
    {
        "Question_title":"Pipeline hyperparameter tunning with optuna",
        "Question_body":"<p>I'm trying to find the best parameter for the lasso polynomial especially : the degree of the polynomial, alpha and max_iteration,  using OPTUNA, however i'm getting this error message :<\/p>\n<pre><code>ValueError: Invalid parameter degree for estimator Pipeline(steps=[('poly', PolynomialFeatures()), ('linear', Lasso())]). Check the list of available parameters with `estimator.get_params().keys()`.\n<\/code><\/pre>\n<p>I used K-cross-validation and this is the code :<\/p>\n<pre><code>model = Pipeline([('poly', PolynomialFeatures()),('linear',Lasso())])```\n\n\n<\/code><\/pre>\n<p>def run(trial):\nscores =[]\nfor fold in range(5):\nparam_grid = {\n&quot;degree&quot;: trial.suggest_int(&quot;degree&quot;, 2, 5),\n'alpha': trial.suggest_float(&quot;alpha&quot;, 0.01, 0.3),\n'max_iter':trial.suggest_int('max_iter', 1000, 4000)<\/p>\n<pre><code>    }\n\n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    model.set_params(**param_grid) \n\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    rmse =  mean_squared_error(yvalid, preds_valid, squared=False)\n    scores.append(rmse)\nreturn np.mean(scores)\n<\/code><\/pre>\n<pre><code>study = optuna.create_study(direction=&quot;minimize&quot;)\nstudy.optimize(run, n_trials=100)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1662735310237,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":17.0,
        "Poster_created_time":1654208597272,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73664034",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pipelin hyperparamet tun try best paramet lasso polynomi especi degre polynomi alpha max iter get error messag valueerror invalid paramet degre estim pipelin step poli polynomialfeatur linear lasso check list avail paramet estim param kei cross valid code model pipelin poli polynomialfeatur linear lasso def run trial score fold rang param grid degre trial suggest int degre alpha trial suggest float alpha max iter trial suggest int max iter xtrain kfold fold reset index drop true xvalid kfold fold reset index drop true ytrain xtrain target yvalid xvalid target xtrain xtrain us featur xvalid xvalid us featur model set param param grid model fit xtrain ytrain pred valid model predict xvalid rmse mean squar error yvalid pred valid squar fals score append rmse return mean score studi creat studi direct minim studi optim run trial",
        "Question_preprocessed_content":"pipelin hyperparamet try best paramet polynomi polynomi alpha code def run score fold rang alpha",
        "Question_gpt_summary_original":"The user is attempting to use OPTUNA for pipeline hyperparameter tuning to find the best parameters for the Lasso polynomial, including degree, alpha, and max_iteration. However, they are encountering a ValueError stating that the degree parameter is invalid for the estimator Pipeline. The user is using K-cross-validation and has provided the relevant code.",
        "Question_gpt_summary":"user attempt us pipelin hyperparamet tune best paramet lasso polynomi includ degre alpha max iter encount valueerror state degre paramet invalid estim pipelin user cross valid provid relev code"
    },
    {
        "Question_title":"How can I save more metadata on an MLFlow model",
        "Question_body":"<p>I am trying to save a model to MLFlow, but as I have a custom prediction pipeline to retrieve data, I need to save extra metadata into the model.<\/p>\n<p>I tried using my custom signature class, which It does the job correctly and saves the model with the extra metadata in the MLModel file (YAML format). But when want to load the model from the MLFlow registry, the signature is not easy accesible.<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, signature = signature)\n<\/code><\/pre>\n<p>I've also tried to save an extra dictionary at the log_model function, but it saves it in the conda.yaml file:<\/p>\n<pre><code>mlflow.sklearn.log_model(model, &quot;model&quot;, {&quot;metadata1&quot;:&quot;value1&quot;, &quot;metadata2&quot;:&quot;value2&quot;})\n<\/code><\/pre>\n<p>Should I make my own flavour? Or my own Model inheritance? I've seen <a href=\"https:\/\/github1s.com\/mlflow\/mlflow\/blob\/HEAD\/mlflow\/pyfunc\/__init__.py\" rel=\"nofollow noreferrer\">here<\/a> that the PyFuncModel recieves some metadata class and an implementation to solve this, but I don't know where should I pass my own implementations to PyFuncModel on an experiment script. Here's a minimal example:<\/p>\n<pre><code>import mlflow\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nmetadata_dic = {&quot;metadata1&quot;: &quot;value1&quot;, \n                &quot;metadata2&quot;: &quot;value2&quot;}\n\nX = np.array([[-2, -1, 0, 1, 2, 1],[-2, -1, 0, 1, 2, 1]]).T\ny = np.array([0, 0, 1, 1, 1, 0])\n\nX = pd.DataFrame(X, columns=[&quot;X1&quot;, &quot;X2&quot;])\ny = pd.DataFrame(y, columns=[&quot;y&quot;])\n\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\n\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1638276045390,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":323.0,
        "Poster_created_time":1550233102176,
        "Poster_location":null,
        "Poster_reputation_count":621.0,
        "Poster_view_count":103.0,
        "Answer_body":"<p>Finally, I made a class that contains every metadata and saved it as an model argument:<\/p>\n<pre><code>model = LogisticRegression()\nmodel.fit(X, y)\nmodel.metadata = ModelMetadata(**metadata_dic)\nmlflow.sklearn.log_model(model, &quot;model&quot;)\n<\/code><\/pre>\n<p>Here I lost the customizable <code>predict<\/code> process, but after reading the <code>MLFlow<\/code> documentation is not very clear how to proceed.<\/p>\n<p>If anyone finds a good approach It would be very appreciated.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1638361888372,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1550233102176,
        "Answerer_location":null,
        "Answerer_reputation_count":621.0,
        "Answerer_view_count":103.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70169519",
        "Tool":"MLflow",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"save metadata model try save model custom predict pipelin retriev data need save extra metadata model tri custom signatur class job correctli save model extra metadata mlmodel file yaml format want load model registri signatur easi acces sklearn log model model model signatur signatur tri save extra dictionari log model function save conda yaml file sklearn log model model model metadata valu metadata valu flavour model inherit seen pyfuncmodel reciev metadata class implement solv know pass implement pyfuncmodel experi script minim exampl import import numpi import panda sklearn linear model import logisticregress metadata dic metadata valu metadata valu arrai arrai datafram column datafram column model logisticregress model fit sklearn log model model model",
        "Question_preprocessed_content":"save metadata model try save model custom predict pipelin retriev data save extra metadata model tri custom signatur job save model extra metadata mlmodel file want load model registri signatur easi tri save extra dictionari function save file flavour model inherit pyfuncmodel reciev metadata implement solv know implement pyfuncmodel experi script minim exampl",
        "Question_gpt_summary_original":"The user is trying to save a model to MLFlow with extra metadata using a custom signature class or a dictionary, but is unable to access the signature when loading the model from the MLFlow registry. The user is considering creating their own flavor or model inheritance and is seeking guidance on how to pass their own implementations to PyFuncModel on an experiment script.",
        "Question_gpt_summary":"user try save model extra metadata custom signatur class dictionari unabl access signatur load model registri user consid creat flavor model inherit seek guidanc pass implement pyfuncmodel experi script"
    },
    {
        "Question_title":"AWS AI\/ML integration with Power BI",
        "Question_body":"Customer wants to know if AWS AI\/ML services integrate with Power BI. The customer currently uses Power BI that integrates with Azure ML for sentiment analysis, opinion mining, etc. Customer is looking for a push button solution where the business analyst can do text analytics on the response from the model. Is there a way to do this on AWS or a marketplace solution?",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1607495476000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668614604512,
        "Question_score_count":0.0,
        "Question_view_count":360.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"PowerBI can connect to Amazon Redshift and leverage the [new SQL based ML capability in Redshift][1] that uses Sagemaker under the hood.  \n\nAs an alternative thought the customer can integrate Amazon Sagemaker Model with Amazon Quicksight to achieve functionality very similar to PowerBI with Azure ML. Quicksight does have some embedded ML capability like forecasting and anomaly detection but Opinion mining is not one of them yet.\n\nYou should be able to leverage [Blazing Text Algorithm][2] in Sagemaker or some market place solution like [Twinword sentiment model][3] in sagemaker for sentiment analysis for Text mining after the integration.\n\n\n\n  [1]: https:\/\/aws.amazon.com\/redshift\/features\/redshift-ml\/\n  [2]: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\n  [3]: https:\/\/aws.amazon.com\/marketplace\/pp\/Twinword-Inc-Sentiment-Analysis-Inference-Model\/prodview-wa74yx5z5qsvw",
        "Answer_comment_count":0.0,
        "Question_closed_time":1607528438000,
        "Answer_last_edit_time":1614011625614,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4VexAnfiSFi4Jf5i9RyO_A\/aws-ai-ml-integration-with-power-bi",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":1.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"aw integr power custom want know aw servic integr power custom current us power integr sentiment analysi opinion mine custom look push button solut busi analyst text analyt respons model wai aw marketplac solut",
        "Question_preprocessed_content":"aw integr power custom want know aw servic integr power custom us power integr sentiment analysi opinion mine custom push solut analyst text analyt respons model wai aw marketplac solut",
        "Question_gpt_summary_original":"The customer is looking for a way to integrate AWS AI\/ML services with Power BI for text analytics. They currently use Power BI with Azure ML for sentiment analysis and opinion mining, and are seeking a push button solution for business analysts to analyze model responses. They are wondering if there is a way to do this on AWS or through a marketplace solution.",
        "Question_gpt_summary":"custom look wai integr aw servic power text analyt current us power sentiment analysi opinion mine seek push button solut busi analyst analyz model respons wonder wai aw marketplac solut"
    },
    {
        "Question_title":"Track all experiments with a separate directory",
        "Question_body":"<p>I am reading about experiment management <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/experiment-management<\/a> and I am interested in exploring the directories organisation pattern where you can keep all your experiments that you have run recorded at the same time and not through git history.<\/p>\n<p>What is not clear to me is how to implement that pattern though? Anyone has a better idea of how this works?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1618393378968,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":258.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/track-all-experiments-with-a-separate-directory\/722",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"track experi separ directori read experi manag http org doc user guid experi manag interest explor directori organis pattern experi run record time git histori clear implement pattern better idea work",
        "Question_preprocessed_content":"track experi separ directori read experi manag interest explor directori organis experi run record time git histori clear implement idea work",
        "Question_gpt_summary_original":"The user is interested in exploring the directories organization pattern for experiment management, as described in a resource they found. However, they are unsure how to implement this pattern and are seeking advice.",
        "Question_gpt_summary":"user interest explor directori organ pattern experi manag describ resourc unsur implement pattern seek advic"
    },
    {
        "Question_title":"how to version step functions for ML?",
        "Question_body":"Hi,\nStep Functions can be used to create ML workflows. What is the best practice to version the code creating those workflows? boto3 code in CodeCommit? Something else?\n\nCheers\nOlivier",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1549456954000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668624402176,
        "Question_score_count":0.0,
        "Question_view_count":555.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"A Step Functions state machine usually doesn't come alone and typically relies on other resources such as Lambda, EC2, DynamoDB, etc. You might want to package these dependent artifacts\/resources altogether within a version otherwise you might have a state machine that doesn't fully work (eg, state machine version doesn't match Lambda version). I guess the simplest way to achieve this is to provision these resources together as code (eg, CDK or CloudFormation) and store them in a Git repo. You could then use Git tags for versioning.",
        "Answer_comment_count":0.0,
        "Question_closed_time":1549478317000,
        "Answer_last_edit_time":1601285800631,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdG1tanW-TXy-vY0YrCsVeg\/how-to-version-step-functions-for-ml",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":1.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":1.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"version step function step function creat workflow best practic version code creat workflow boto code codecommit cheer olivi",
        "Question_preprocessed_content":"version step function step function creat workflow best practic version code creat workflow boto code olivi",
        "Question_gpt_summary_original":"The user is seeking advice on the best practice for versioning the code used to create ML workflows using Step Functions, specifically whether to use boto3 code in CodeCommit or another method.",
        "Question_gpt_summary":"user seek advic best practic version code creat workflow step function specif us boto code codecommit method"
    },
    {
        "Question_title":"Google Vision API pricing",
        "Question_body":"Hello,\n\nI'm currently using the service of the Google Cloud Vision API. On the website it says that the first 1000 Request are for free every month.\u00a0\n\nBut for that I need a Billing account which is not for free if I understand correctly.\u00a0 So basically you can't use the Cloud Vision API completely for free.\u00a0\n\nAm I right or can you use the service without any costs?",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1654853700000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":348.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Vision-API-pricing\/td-p\/430454\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"googl vision api price hello current servic googl cloud vision api websit sai request free month need bill account free understand correctli basic us cloud vision api complet free right us servic cost",
        "Question_preprocessed_content":"vision api price servic cloud vision api websit sai request month understand us cloud vision api complet right us servic cost",
        "Question_gpt_summary_original":"The user is using Google Cloud Vision API and has found that the first 1000 requests are free every month, but a billing account is required which is not free. Therefore, the user cannot use the Cloud Vision API completely for free.",
        "Question_gpt_summary":"user googl cloud vision api request free month bill account requir free user us cloud vision api complet free"
    },
    {
        "Question_title":"ClearML multiple tasks in single script changes logged value names",
        "Question_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1613745436903,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1614004159640,
        "Question_score_count":1.0,
        "Question_view_count":279.0,
        "Poster_created_time":1604391794420,
        "Poster_location":null,
        "Poster_reputation_count":89.0,
        "Poster_view_count":4.0,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1613773903383,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1561143508792,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation_count":489.0,
        "Answerer_view_count":60.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Tool":"ClearML",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"multipl task singl script chang log valu name train multipl model differ configur custom hyperparamet search us pytorch lightn log tensorboardlogg run train script task init auto creat task connect logger output server log strain stage train val test follow scalar epoch loss acc iou multipl configur networka networkb train log valu loss acc iou second networkb loss networkb acc networkb iou make valu umcompar train loop task init look like name networka networkb name task task init project networkproject task train task close method train wrapper train pytorch ligtn code insid method hint properli us usag loop script completli separ task edit version issu fix main branch",
        "Question_preprocessed_content":"multipl task singl script chang valu name train multipl model configur custom hyperparamet search us train script task output server log strain stage scalar epoch multipl configur train log valu second make valu umcompar train task init like method train pytorch ligtn code insid method hint properli us usag script completli separ task edit version fix main branch",
        "Question_gpt_summary_original":"The user trained multiple models with different configurations for a custom hyperparameter search using pytorch_lightning and TensorboardLogger for logging. When running the training script, ClearML auto-creates a Task and connects the logger output to the server. However, when there are multiple configurations, the logged values have different names, making them incomparable. The user is seeking advice on how to properly use a loop in a script using completely separated tasks.",
        "Question_gpt_summary":"user train multipl model differ configur custom hyperparamet search pytorch lightn tensorboardlogg log run train script auto creat task connect logger output server multipl configur log valu differ name make incompar user seek advic properli us loop script complet separ task"
    },
    {
        "Question_title":"Both GPU usage and GPU TIme Spent Accessing Memory is low",
        "Question_body":"<p>What does it mean when both GPU Usage and GPU Time Spent Accessing Memory is low. I was under the impression that these two would normally add to 100. My GPU Usage is at about 20% and the GPU time spent accessing memory is at 4%. Could someone help me understand what might be causing this and potentially how  I might be able to solve this.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1685135386269,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":21.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/both-gpu-usage-and-gpu-time-spent-accessing-memory-is-low\/4476",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"gpu usag gpu time spent access memori low mean gpu usag gpu time spent access memori low impress normal add gpu usag gpu time spent access memori help understand caus potenti abl solv",
        "Question_preprocessed_content":"gpu usag gpu time spent memori low mean gpu usag gpu time spent memori low gpu usag gpu time spent memori help understand caus abl solv",
        "Question_gpt_summary_original":"The user is experiencing low GPU usage and low GPU time spent accessing memory, which is unusual as they are expected to add up to 100. They are seeking help to understand the cause of this issue and how to solve it.",
        "Question_gpt_summary":"user experienc low gpu usag low gpu time spent access memori unusu expect add seek help understand caus issu solv"
    },
    {
        "Question_title":"Use custom modules in Sagemaker MXNet",
        "Question_body":"<p>I\u2019ve been trying to use Sagemaker to run my custom MXNet training job. In all the examples I\u2019ve seen, the code sample looks like this<\/p>\n\n<pre><code>estimator = MXNet(\u2018train.py\u2019, role=role, other_params)\nestimator.fit(inputs)\n<\/code><\/pre>\n\n<p>\u2028\u2028What if my <code>train.py<\/code> relies on a custom module? Given a directory structure like so<\/p>\n\n<pre><code>.\n\u251c\u2500\u2500 awesome\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lib.py\n\u2514\u2500\u2500 train.py \n<\/code><\/pre>\n\n<p>With my <code>train.py<\/code> file importing from <code>awesome\/lib.py<\/code>, what\u2019s the best way for me to deploy this job on Sagemaker without going through the hassle of creating a Docker container. <\/p>\n\n<p>Note: all the code in the custom module is just regular mxnet code, organized across various files and helper methods<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1590078681800,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":139.0,
        "Poster_created_time":1590078591488,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61939275",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us custom modul mxnet iv try us run custom mxnet train job exampl iv seen code sampl look like estim mxnet train role role param estim fit input train reli custom modul given directori structur like awesom init lib train train file import awesom lib what best wai deploi job go hassl creat docker contain note code custom modul regular mxnet code organ file helper method",
        "Question_preprocessed_content":"us custom modul mxnet iv try us run custom mxnet train job exampl iv code sampl like reli custom modul given directori structur like file import what best wai deploi job go creat docker contain note code custom modul regular mxnet code organ file helper method",
        "Question_gpt_summary_original":"The user is facing a challenge while using Sagemaker to run a custom MXNet training job that relies on a custom module. The user is unsure about the best way to deploy the job on Sagemaker without creating a Docker container. The custom module contains regular MXNet code organized across various files and helper methods.",
        "Question_gpt_summary":"user face challeng run custom mxnet train job reli custom modul user unsur best wai deploi job creat docker contain custom modul contain regular mxnet code organ file helper method"
    },
    {
        "Question_title":"Artifact download link",
        "Question_body":"<p>Hello everyone,<\/p>\n<p>In <a href=\"http:\/\/wandb.ai\">wandb.ai<\/a> for every created project, there is an Artifacts page. There we have a toolbar with the options: \u201cOverview\u201d, \u201cMetadata\u201d, \u201cUsage\u201d, \u201cFiles\u201d and \u201cLineage\u201d. In the Files section, there are downloadable links for every file that make part of a given artifact.<br>\nIs it possible to get through wandb package this downloadable Artifact URL link?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_created_time":1673366790878,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":202.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/artifact-download-link\/3676",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"artifact download link hello creat project artifact page toolbar option overview metadata usag file lineag file section download link file given artifact possibl packag download artifact url link",
        "Question_preprocessed_content":"artifact download link creat project artifact page option overview metadata usag file lineag file section download link file given artifact packag download artifact url link",
        "Question_gpt_summary_original":"The user is asking if it is possible to obtain the downloadable Artifact URL link through the wandb package for every file that is part of a given artifact in the Files section of the Artifacts page in wandb.ai.",
        "Question_gpt_summary":"user ask possibl obtain download artifact url link packag file given artifact file section artifact page"
    },
    {
        "Question_title":"Is prediction speed of YOLOv5 recorded somewhere as part of the integration?",
        "Question_body":"<p>Hello!<br>\nI\u2019ve been training YOLOv5 models using wandb, and I\u2019ve been amazed at how much is natively built in with just a few command-line arguments. One thing I\u2019m lacking - is there any recording of the speed at which it makes predictions? I don\u2019t particularly care if it\u2019s on the training or test sets, I\u2019m just looking for any record of the speed at which the model makes predictions.<\/p>\n<p>Thanks!<br>\nIan<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":null,
        "Question_created_time":1652490183869,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":116.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-prediction-speed-of-yolov5-recorded-somewhere-as-part-of-the-integration\/2419",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"predict speed yolov record integr hello iv train yolov model iv amaz nativ built command line argument thing lack record speed make predict dont particularli care train test set look record speed model make predict thank ian",
        "Question_preprocessed_content":"predict yolov record integr iv train yolov model iv amaz nativ built line argument thing lack record make predict dont particularli care train test set record model make predict thank ian",
        "Question_gpt_summary_original":"The user is wondering if there is any way to record the prediction speed of YOLOv5 models during integration, as they have been training models using wandb and are impressed with the built-in features but lack the ability to record prediction speed.",
        "Question_gpt_summary":"user wonder wai record predict speed yolov model integr train model impress built featur lack abil record predict speed"
    },
    {
        "Question_title":"How to fix ModuleNotFoundError in azureml-sdk when installed inside conda environment",
        "Question_body":"<p>I'm setting up a conda environment on Windows 10 Pro x64 using Miniconda 4.5.12 and have done a pip install of azureml-sdk inside the environment but get a ModuleNotFoundError when attempting to execute the following code:<\/p>\n\n<pre><code>import azureml.core\nazureml.core.VERSION\n<\/code><\/pre>\n\n<p>This is the output:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\n  File \"D:\\Projects\\style-transfer\\azureml.py\", line 1, in &lt;module&gt;\n    import azureml.core\nModuleNotFoundError: No module named 'azureml.core'; 'azureml' is not a package\n<\/code><\/pre>\n\n<p>The code above has been run from the conda prompt, with the test environment active as well as in vscode with the same environment selected.<\/p>\n\n<p>I setup the conda environment as per the following:<\/p>\n\n<ol>\n<li>Created the conda environment <code>conda create -n test<\/code>.<\/li>\n<li>Activated the environment <code>activate test<\/code>.<\/li>\n<li>Installed pip <code>conda install pip<\/code>.<\/li>\n<li>Installed azureml-sdk <code>pip install azureml-sdk<\/code>.<\/li>\n<\/ol>\n\n<p>This results in the following packages being installed in the environment as per <code>conda list<\/code>:<\/p>\n\n<pre><code>adal                      1.2.0                     &lt;pip&gt;\nantlr4-python3-runtime    4.7.2                     &lt;pip&gt;\napplicationinsights       0.11.7                    &lt;pip&gt;\nargcomplete               1.9.4                     &lt;pip&gt;\nasn1crypto                0.24.0                    &lt;pip&gt;\nazure-cli-command-modules-nspkg 2.0.2                     &lt;pip&gt;\nazure-cli-core            2.0.54                    &lt;pip&gt;\nazure-cli-nspkg           3.0.3                     &lt;pip&gt;\nazure-cli-profile         2.1.2                     &lt;pip&gt;\nazure-cli-telemetry       1.0.0                     &lt;pip&gt;\nazure-common              1.1.16                    &lt;pip&gt;\nazure-graphrbac           0.53.0                    &lt;pip&gt;\nazure-mgmt-authorization  0.51.1                    &lt;pip&gt;\nazure-mgmt-containerregistry 2.5.0                     &lt;pip&gt;\nazure-mgmt-keyvault       1.1.0                     &lt;pip&gt;\nazure-mgmt-nspkg          3.0.2                     &lt;pip&gt;\nazure-mgmt-resource       2.0.0                     &lt;pip&gt;\nazure-mgmt-storage        3.1.0                     &lt;pip&gt;\nazure-nspkg               3.0.2                     &lt;pip&gt;\nazure-storage-blob        1.4.0                     &lt;pip&gt;\nazure-storage-common      1.4.0                     &lt;pip&gt;\nazure-storage-nspkg       3.1.0                     &lt;pip&gt;\nazureml-core              1.0.6                     &lt;pip&gt;\nazureml-pipeline          1.0.6                     &lt;pip&gt;\nazureml-pipeline-core     1.0.6                     &lt;pip&gt;\nazureml-pipeline-steps    1.0.6                     &lt;pip&gt;\nazureml-sdk               1.0.6                     &lt;pip&gt;\nazureml-telemetry         1.0.6                     &lt;pip&gt;\nazureml-train             1.0.6                     &lt;pip&gt;\nazureml-train-core        1.0.6                     &lt;pip&gt;\nazureml-train-restclients-hyperdrive 1.0.6                     &lt;pip&gt;\nbackports.tempfile        1.0                       &lt;pip&gt;\nbackports.weakref         1.0.post1                 &lt;pip&gt;\nbcrypt                    3.1.5                     &lt;pip&gt;\nca-certificates           2018.03.07                    0\ncertifi                   2018.11.29               py37_0\ncffi                      1.11.5                    &lt;pip&gt;\nchardet                   3.0.4                     &lt;pip&gt;\ncolorama                  0.4.1                     &lt;pip&gt;\ncontextlib2               0.5.5                     &lt;pip&gt;\ncryptography              2.4.2                     &lt;pip&gt;\ndocker                    3.6.0                     &lt;pip&gt;\ndocker-pycreds            0.4.0                     &lt;pip&gt;\nfutures                   3.1.1                     &lt;pip&gt;\nhumanfriendly             4.17                      &lt;pip&gt;\nidna                      2.8                       &lt;pip&gt;\nisodate                   0.6.0                     &lt;pip&gt;\njmespath                  0.9.3                     &lt;pip&gt;\njsonpickle                1.0                       &lt;pip&gt;\nknack                     0.5.1                     &lt;pip&gt;\nmsrest                    0.6.2                     &lt;pip&gt;\nmsrestazure               0.6.0                     &lt;pip&gt;\nndg-httpsclient           0.5.1                     &lt;pip&gt;\noauthlib                  2.1.0                     &lt;pip&gt;\nopenssl                   1.1.1a               he774522_0\nparamiko                  2.4.2                     &lt;pip&gt;\npathspec                  0.5.9                     &lt;pip&gt;\npip                       18.1                     py37_0\nportalocker               1.2.1                     &lt;pip&gt;\npyasn1                    0.4.4                     &lt;pip&gt;\npycparser                 2.19                      &lt;pip&gt;\nPygments                  2.3.1                     &lt;pip&gt;\nPyJWT                     1.7.1                     &lt;pip&gt;\nPyNaCl                    1.3.0                     &lt;pip&gt;\npyOpenSSL                 18.0.0                    &lt;pip&gt;\npypiwin32                 223                       &lt;pip&gt;\npyreadline                2.1                       &lt;pip&gt;\npython                    3.7.1                h8c8aaf0_6\npython-dateutil           2.7.5                     &lt;pip&gt;\npytz                      2018.7                    &lt;pip&gt;\npywin32                   224                       &lt;pip&gt;\nPyYAML                    3.13                      &lt;pip&gt;\nrequests                  2.21.0                    &lt;pip&gt;\nrequests-oauthlib         1.0.0                     &lt;pip&gt;\nruamel.yaml               0.15.51                   &lt;pip&gt;\nSecretStorage             2.3.1                     &lt;pip&gt;\nsetuptools                40.6.3                   py37_0\nsix                       1.12.0                    &lt;pip&gt;\nsqlite                    3.26.0               he774522_0\ntabulate                  0.8.2                     &lt;pip&gt;\nurllib3                   1.23                      &lt;pip&gt;\nvc                        14.1                 h0510ff6_4\nvs2015_runtime            14.15.26706          h3a45250_0\nwebsocket-client          0.54.0                    &lt;pip&gt;\nwheel                     0.32.3                   py37_0\nwheel                     0.30.0                    &lt;pip&gt;\nwincertstore              0.2                      py37_0\n<\/code><\/pre>\n\n<p>If I run <code>which pip<\/code>, I get the following output, which confirms that I used the pip inside the environment to install azureml-sdk, I think:<\/p>\n\n<pre><code>\/c\/Users\/allan\/Miniconda3\/envs\/test\/Scripts\/pip\n<\/code><\/pre>\n\n<p>I can also see that the azureml packages do in fact exist within the environment folder structure.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_created_time":1545617082117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":6343.0,
        "Poster_created_time":1460456204196,
        "Poster_location":"Australia",
        "Poster_reputation_count":140.0,
        "Poster_view_count":10.0,
        "Answer_body":"<p>It's probably because the name if your python file is the same as a module name you are trying import. In this case, rename the file to something other than <code>azureml.py<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1545633498167,
        "Answer_last_edit_time":null,
        "Answer_score_count":3.0,
        "Answerer_created_time":1401187183867,
        "Answerer_location":"Sweden",
        "Answerer_reputation_count":1709.0,
        "Answerer_view_count":112.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53908529",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"fix modulenotfounderror sdk instal insid conda environ set conda environ window pro miniconda pip instal sdk insid environ modulenotfounderror attempt execut follow code import core core version output traceback recent file line import core file project style transfer line import core modulenotfounderror modul name core packag code run conda prompt test environ activ vscode environ select setup conda environ follow creat conda environ conda creat test activ environ activ test instal pip conda instal pip instal sdk pip instal sdk result follow packag instal environ conda list adal antlr python runtim applicationinsight argcomplet asncrypto azur cli command modul nspkg azur cli core azur cli nspkg azur cli profil azur cli telemetri azur common azur graphrbac azur mgmt author azur mgmt containerregistri azur mgmt keyvault azur mgmt nspkg azur mgmt resourc azur mgmt storag azur nspkg azur storag blob azur storag common azur storag nspkg core pipelin pipelin core pipelin step sdk telemetri train train core train restclient hyperdr backport tempfil backport weakref post bcrypt certif certifi cffi chardet colorama contextlib cryptographi docker docker pycr futur humanfriendli idna isod jmespath jsonpickl knack msrest msrestazur ndg httpsclient oauthlib openssl paramiko pathspec pip portalock pyasn pycpars pygment pyjwt pynacl pyopenssl pypiwin pyreadlin python hcaaf python dateutil pytz pywin pyyaml request request oauthlib ruamel yaml secretstorag setuptool sqlite tabul urllib hff runtim websocket client wheel wheel wincertstor run pip follow output confirm pip insid environ instal sdk think user allan miniconda env test script pip packag fact exist environ folder structur",
        "Question_preprocessed_content":"fix sdk insid conda environ conda environ window pro miniconda pip sdk insid environ execut code output code run conda prompt test environ activ vscode environ select setup conda environ creat conda environ activ environ pip sdk result packag environ run output confirm pip insid environ sdk think packag fact exist environ folder structur",
        "Question_gpt_summary_original":"The user has installed azureml-sdk inside a conda environment on Windows 10 Pro x64 using Miniconda 4.5.12. However, when attempting to execute the code, the user gets a ModuleNotFoundError. The user has installed pip, azureml-sdk, and other packages inside the environment, and the azureml packages do exist within the environment folder structure.",
        "Question_gpt_summary":"user instal sdk insid conda environ window pro miniconda attempt execut code user get modulenotfounderror user instal pip sdk packag insid environ packag exist environ folder structur"
    },
    {
        "Question_title":"Problem with Sweep; how to use run.finish() and log without error + Question about defined metric",
        "Question_body":"<p>Hi all,<\/p>\n<p>Nice to meet you!<\/p>\n<p>Currently I\u2019m not understanding how to use run.finish() and wandb.init for logging correctly. I\u2019m constantly getting an error when the wandb.agent sweeps to another model configuration. It\u2019s successfully doing the K-fold split, I dont see any errors. But it\u2019s right after the K-fold split  when the now model configuration is applied by the sweep.<\/p>\n<p><strong>Information about my code:<\/strong><br>\nMy code is a bit messy. I think there is no way to use K cross validation from scikitlearn. I\u2019ve tried it many times, but my input and output are (with N = number of datasets):<\/p>\n<p>Input 1: N datasets of 1000 numbers (x-axis)<br>\nInput2 : N datasets of 1000 numbers (y-axis)<br>\nOutput1: 1 number for each N\u2019th dataset<br>\nOutput2: 1 number for each N\u2019th dataset<\/p>\n<p>Input 1 and 2 are concatenated to produce 2 outputs. Lets say N is 300 and split is 0.2 then:<br>\nOutput1.shape, Output2.shape, Output1_test.shape, Output2_test.shape, X.shape, Y.shape, X_test.shape, Y_test.shape<\/p>\n<p>In the same order, their shapes: ((240,), (240,), (60,), (60,), (240, 1000), (240, 1000), (60, 1000), (60, 1000))<br>\nI think there is just no way I can define the cross validation with sklearn with this type of data I think\u2026<\/p>\n<p><strong>Error<\/strong><br>\nI\u2019ve introduced to save to model each time it\u2019s configured. Then load the model in each for loop with zero weigths. This way may cross validation succeeds. However, I\u2019m not sure how to correctly log my files. This code is doing bad at producing the groups I want them to be in; it\u2019s just overwriting them. Also, and as I mentioned; every time when a new model is initiated by the sweep, I get and error:<\/p>\n<p><strong>display<\/strong><\/p>\n<pre><code class=\"lang-auto\">wandb: Sweep Agent: Waiting for job.\nwandb: Job received.\nwandb: While tearing down the service manager. The following error has occured: [WinError 10054] De externe host heeft een verbinding verbroken\nwandb: Agent Starting Run: ekc1s4gm with config:\nwandb: \tbatch_size: 6\nwandb: \tdense_units: 63.48472025895866\nwandb: \tdense_units2: 81.47931201263756\nwandb: \tlearning_rate: 0.0007466619646085462\nwandb: \tnum_layers: 10\nwandb: \toptimizer: Adam\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\n\nException in thread ChkStopThr: \n.....\nSome file information\n.....\n\nException in thread NetStatThr::\n.....\nSome file information\n.....\n\nEnding in: \n\nConnectionResetError: [WinError 10054] De externe host heeft een verbinding verbroken\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] De externe host heeft een verbinding verbroken\n<\/code><\/pre>\n<p>So when this error occurs, it just continues right after  it created the new model configuration. The last time this error occurs is when the last loop and last model gets evaluated.  Also this error occurs every loop<\/p>\n<blockquote>\n<p>wandb: WARNING Ignored wandb.init() arg project when running a sweep.<\/p>\n<\/blockquote>\n<p>I think it definitely has to do something with the Groups I want certain logs to be in and therefore also my run.finish() command (<strong>The error does not error without any wandb.init()!!! The error also occurs with just the wandb.init() in the for loop, also if I add this, each sweep is overwritten by the other sweep so it\u2019s not creating groups aswell!!<\/strong>.  I\u2019m unsure if I placed them correctly.  it sounded logical to me to define run just once and to have to others as just wandb.init()\u2026  (see me code). I just don\u2019t understand how to use it in this case\u2026  I hope you do ? <strong>How can I group my folds and my validation seperately without  using wandb.init()<\/strong> ? Any recommendations on this would be very welcome!<\/p>\n<p><strong>My code<\/strong><br>\nHere is my code\u2026 A little messy, sorry.<\/p>\n<pre><code class=\"lang-auto\">def seed_all(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">def build_model(config):\n  \n    activ = config.activation   \n    dense_units = config.dense_units  \n    dense_units2 = config.dense_units2\n    num_layers = config.num_layers\n    batch_size = config.batch_size\n    batch_norm = False\n    optimizer = config.optimizer\n    learning_rate = config.learning_rate\n    \n    x = tf.keras.layers.Concatenate()([input1, input2])\n                   \n        #input_layer = Input(shape=(len(norm_train_X .columns), len(norm_train_X.iloc[0][0]))\n    x = Dense(units=dense_units, activation=activ)(x)\n        \n    for _ in range(num_layers):\n        x = Dense(units=dense_units, activation=activ)(x)\n        \n        \n    x1 = Dense(units=dense_units2, activation=activ)(x)\n    \n        # Y1 output will be fed directly from the second dense\n    \n    y1_output = Dense(units='1', name='y1_output')(x1)\n\n    third_dense = Dense(units=dense_units2, activation=activ)(x1)\n\n         # Y2 output will come via the third dense\n    y2_output = Dense(units='1', name='y2_output')(third_dense)\n        \n    model = Model(inputs=[input1, input2], outputs=[y1_output, y2_output])\n    print(model.summary())   \n    model.save_weights('model.h5', overwrite = True)\n    return model\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">def train():\n    test_loss_sum =np.array([0])\n    Hp_loss_sum = np.array([0])\n    MDp_loss_sum = np.array([0])\n    Hp_rmse_sum = np.array([0])\n    MDp_rmse_sum = np.array([0])\n    loss_sum = np.array([0])\n    loss_sum_tot =0 \n    Hp_R2_append = []\n    test_loss_sum_tot =0\n    hp_append = []\n    MDp_append = []\n    hp_pred_append = []\n    MDp_R2_append = []\n    MDp_sum = 0\n    Hp_sum = 0\n    test_loss_mean = 0 \n    hp_pred_append = []\n    MDp_pred_append = []\n    Hp_loss_sum_tot = 0\n    MDp_loss_sum_tot =0\n    Hp_rmse_sum_tot =0\n    MDp_rmse_sum_tot=0\n    \n    X = np.vstack(np.asarray(norm1.numpy()[:]))\n    Y = np.vstack(np.asarray(norm2.numpy()[:]))\n    max_trials = 2\n    epochs = 100\n    test_loss_sum = np.array([0])\n    Hp = train_Y_1_t\n    MDp = train_Y_2_t\n    Hp_test = test_Y_1_t\n    MDp_test = test_Y_2_t\n    hyperparams = dict(\n        lr = 0.0001,\n        optimizer = 'Adam',\n        dense_units = 256,\n        batch_size = 64,\n        epochs = 1,        \n        ense_units2 = 64,\n        activation = 'relu',)\n    \n    cb_reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor = \"val_loss\",\n        mode = 'auto',\n        factor = 0.1,\n        patience = 20,\n        min_delta = 1e-04, #default\n        min_lr = 1e-07,\n        verbose = 1)\n\n    cb_earlystop= tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        mode=\"auto\",\n        min_delta = 0,\n        patience=25,\n        verbose=1)\n    \n    run = wandb.init(project=\"custom-charts\", config=hyperparams, reinit = True) #Note the Reinit here!\n    config = wandb.config\n    \n    Wandcalback = WandbCallback(monitor='val_loss')\n    \n    model =  build_model(config=config)\n    \n    LR = config.learning_rate     # 0.001\n    \n    if config.optimizer=='Adam':\n        optimizer = tf.keras.optimizers.Adam(lr = LR)\n    elif config.optimizer=='RMSprop':\n        optimizer = tf.keras.optimizers.RMSprop(lr=LR, rho=0.9, epsilon=1e-08, decay=0.0)\n    else: \n        raise\n    \n    # Compile the model\n    model.compile(optimizer=optimizer,\n    loss={'y1_output': 'mse', 'y2_output': 'mse'},\n    metrics={'y1_output': tf.keras.metrics.RootMeanSquaredError(),'y2_output': tf.keras.metrics.RootMeanSquaredError()})\n   \n    n_splits = 4\n    skf = KFold(n_splits, shuffle = True)\n    skf.get_n_splits(X, Y)\n    i=0\n    vall_loss = []\n    for train_index, test_index in skf.split(X, Y):\n        \n        wandb.init(project=\"custom-charts\", group = \"folds_experiment\", job_type = \"fold{}\".format(i)) \n        model.load_weights('model.h5')\n        \n        train_index = train_index.astype(int)\n        test_index = test_index.astype(int)\n        X = np.array(X)\n        Y = np.array(Y)\n        Hp = np.array(Hp)\n        MDp = np.array(MDp)\n        X_train, X_test = X[train_index], X[test_index]\n        Y_train, Y_test = Y[train_index], Y[test_index]\n        Hp_train, Hp_test = Hp[train_index], Hp[test_index]\n        MDp_train, MDp_test = MDp[train_index],MDp[test_index]      \n        \n        history = model.fit([tf.convert_to_tensor(X), tf.convert_to_tensor(Y)], [Hp, MDp], validation_data = ([tf.convert_to_tensor(X_test), tf.convert_to_tensor(Y_test)], [Hp_test, MDp_test]), \n                     batch_size=config.batch_size,    \n                     epochs=2,             \n                     callbacks=[Wandcalback,cb_earlystop,cb_reducelr],\n                     verbose=1)\n        \n        loss_sum = pd.DataFrame(history.history)['loss'].iloc[-1]  + loss_sum\n        test_loss_sum = pd.DataFrame(history.history)['val_loss'].iloc[-1]  + test_loss_sum\n        Hp_loss_sum = pd.DataFrame(history.history)['val_y1_output_loss'].iloc[-1]  + Hp_loss_sum\n        MDp_loss_sum = pd.DataFrame(history.history)['val_y2_output_loss'].iloc[-1]  + MDp_loss_sum\n        Hp_rmse_sum = pd.DataFrame(history.history)['val_y1_output_root_mean_squared_error'].iloc[-1]  + Hp_rmse_sum\n        MDp_rmse_sum = pd.DataFrame(history.history)['val_y2_output_root_mean_squared_error'].iloc[-1]  + MDp_rmse_sum\n            \n        loss_sum_tot = pd.DataFrame(history.history)['loss']  + loss_sum_tot\n        test_loss_sum_tot = pd.DataFrame(history.history)['val_loss']  + test_loss_sum_tot\n        Hp_loss_sum_tot = pd.DataFrame(history.history)['val_y1_output_loss'] +  Hp_loss_sum_tot\n        MDp_loss_sum_tot = pd.DataFrame(history.history)['val_y2_output_loss']  + MDp_loss_sum_tot\n        Hp_rmse_sum_tot = pd.DataFrame(history.history)['val_y1_output_root_mean_squared_error']  +  Hp_rmse_sum_tot\n        MDp_rmse_sum_tot = pd.DataFrame(history.history)['val_y2_output_root_mean_squared_error']  +  MDp_rmse_sum_tot   \n\n        Y_pred = model.predict([tf.convert_to_tensor(X_test), tf.convert_to_tensor(Y_test)])\n        metric = tfa.metrics.r_square.RSquare()\n            \n        metric.update_state(Hp_test, Y_pred[0].flatten())\n        result = metric.result()\n        R_2_Hp = result.numpy()\n        Hp_R2_append.append(R_2_Hp)\n           \n        metric.update_state(MDp_test, Y_pred[1].flatten())\n        result = metric.result()\n        R_2_MDp = result.numpy()\n        MDp_R2_append.append(R_2_MDp)\n        \n        hp_append.append(Hp_test)\n        MDp_append.append(MDp_test)\n        hp_pred_append.append(Y_pred[0])\n        MDp_pred_append.append(Y_pred[1])\n        MDp_sum = MDp_sum + Y_pred[1]\n        Hp_sum = Hp_sum + Y_pred[0]\n        i = i + 1 \n    \n    test_loss_mean = test_loss_sum\/n_splits\n    loss_sum_mean = loss_sum\/n_splits\n    test_loss_sum_mean =  test_loss_sum\/n_splits\n    Hp_loss_sum_mean = Hp_loss_sum\/n_splits\n    MDp_loss_sum_mean = MDp_loss_sum\/n_splits\n    Hp_rmse_sum_mean = Hp_rmse_sum\/n_splits\n    MDp_rmse_sum_mean = MDp_rmse_sum\/n_splits\n         \n    test_MDp_R2 = np.mean(MDp_R2_append)\n    test_Hp_R2 = np.mean(Hp_R2_append)\n                \n        \n    Hp_mean = Hp_sum\/n_splits\n    MDp_mean = MDp_sum\/n_splits    \n        # wandb.init(project= \"sweep &amp; optimalisation RandomSearch\", group=\"experimentfold{}\".format(i), job_type=\"validation\")\n    wandb.init(project=\"custom-charts\", group =\"folds_experiment\", job_type = \"validation\")\n    for val_los in range(len(test_loss_sum_tot)):\n        wandb.log({\"val_loss_mean\" : test_loss_sum_tot[val_los]\/n_splits})\n    for loss in range(len(loss_sum_tot)):\n        wandb.log({\"loss_mean\": loss_sum_tot[loss]\/n_splits})\n    for val_MDp_los in range(len(test_loss_sum_tot)):\n        wandb.log({\"val_MDp_loss_mean\" : test_loss_sum_tot[val_MDp_los]\/n_splits})\n    for val_hp_los in range(len( Hp_loss_sum_tot)):\n        wandb.log({\"val_hp_loss_mean\": Hp_loss_sum_tot[val_hp_los]\/n_splits})\n    for val_MDp_rmse in range(len(MDp_rmse_sum_tot)):\n        wandb.log({\"val_MDp_rmse_mean\" : MDp_loss_sum_tot[val_MDp_rmse]\/n_splits})\n    for val_hp_rmse in range(len(Hp_rmse_sum_tot)):\n        wandb.log({\"val_MDp_rmse_mean\": Hp_rmse_sum_tot[val_hp_rmse]\/n_splits})\n       \n    hp_append = np.concatenate(hp_append)\n    MDp_append = np.concatenate(MDp_append)\n    hp_pred_append = np.concatenate(hp_pred_append)\n    MDp_pred_append = np.concatenate(MDp_pred_append)\n            \n    Hp_score = np.sqrt(mean_squared_error(hp_pred_append,hp_append))\n    MDp_score = np.sqrt(mean_squared_error(MDp_pred_append,MDp_append))  \n    test_MDp_R2 = np.mean(MDp_R2_append)\n    test_Hp_R2 = np.mean(Hp_R2_append)\n        \n    wandb.log({\"R2_score_hp\":Hp_score, \"R2_score_MDp\":MDp_score, \"R2_hp\":test_Hp_R2, \"R2_MDp\":test_MDp_R2})\n    Hp = np.asarray(Hp_mean.flatten())\n    MDp = np.asarray(MDp_mean.flatten())\n    Hp_testt = np.asarray(Hp_test.flatten())\n    MDp_testt = np.asarray(MDp_test.flatten())\n        \n        \n    fd = pd.DataFrame({\"pred\": Hp,\"actual\":Hp_testt})\n    print(fd)\n    table = wandb.Table(dataframe=fd)\n    wandb.log({'scatter-plot1': wandb.plot.scatter(table, \"pred\", \"actual\")})\n        \n    fd2 = pd.DataFrame({\"pred\": MDp,\"actual\":MDp_testt})\n    print(fd2)\n    table2 = wandb.Table(dataframe=fd2)\n    wandb.log({'scatter-plot2': wandb.plot.scatter(table2, \"pred\", \"actual\")})\n        \n    predictions_h = [s for s in Hp_mean]\n    predictions_h\n    table2 = wandb.Table(data=predictions_h, columns=[\"h_predictions\"])\n    wandb.log({'my_histogramM': wandb.plot.histogram(table2, \"h_predictions\",\n    title=\"Prediction Score Distribution Hubble Parameter\")})\n        # hist = np.histogram(predictions_h)\n        # wandb.log({'Hubble parameter': wandb.plot.histogram(hist)})\n        \n        \n    predictions_hh = [ s for s in MDp_mean]\n    predictions_hh\n    table3 = wandb.Table(data=predictions_hh, columns=[\"h_predictions\"])\n    wandb.log({'my_histogram': wandb.plot.histogram(table3, \"h_predictions\",\n    title=\"Prediction Score Distribution Mass Density\")})\n        # hist = np.histogram(predictions_hh)\n        # wandb.log({'Mass Density parameter': wandb.plot.histogram(hist)})\n    run.finish()\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">sweep_config = {\n    'method': 'random',         \n    'metric': {\n        'name': 'test_loss_mean',     \n        'goal': 'minimize'      \n    },\n    'parameters': {\n        'dense_units': {\n            'distribution': 'log_uniform_values',\n            'min': 32,\n            'max': 256\n        },\n        'learning_rate': {\n            'distribution': 'log_uniform_values',\n            'min': 0.0000001,\n            'max': 0.1\n        },\n        'dense_units2': {\n            'distribution': 'log_uniform_values',\n            'min': 32,\n            'max': 256\n        },\n        'batch_size': {\n            #Integers between 32 and 256 \n            # with evenly distributed logarithms\n            'values': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n               \n        },\n        'optimizer': {\n            'values': ['Adam', 'RMSprop']\n        },\n        'num_layers': {\n            'values': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n        }\n\n    }\n}\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">sweep_id = wandb.sweep(sweep_config, entity=\"stijnvdbosch\", project=\"custom-charts\")\nwandb.agent(sweep_id, function=train, count=2, project=\"custom-charts\")\n<\/code><\/pre>\n<p>You can also see i\u2019m updating my own <strong>defined metric<\/strong> (not from model.fit) called <strong>test_loss_mean<\/strong>. I suppose I did that correct?<br>\nIf there is any more information you need to help me, then, please, send  me a message and I will reply in a blink. <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/blush.png?v=12\" title=\":blush:\" class=\"emoji\" alt=\":blush:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1665899098604,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":520.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/problem-with-sweep-how-to-use-run-finish-and-log-without-error-question-about-defined-metric\/3260",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"problem sweep us run finish log error question defin metric nice meet current understand us run finish init log correctli constantli get error agent sweep model configur successfulli fold split dont error right fold split model configur appli sweep inform code code bit messi think wai us cross valid scikitlearn iv tri time input output number dataset input dataset number axi input dataset number axi output number nth dataset output number nth dataset input concaten produc output let split output shape output shape output test shape output test shape shape shape test shape test shape order shape think wai defin cross valid sklearn type data think error iv introduc save model time configur load model loop zero weigth wai cross valid succe sure correctli log file code bad produc group want overwrit mention time new model initi sweep error displai sweep agent wait job job receiv tear servic manag follow error occur winerror extern host heeft een verbind verbroken agent start run ekcsgm config batch size dens unit dens unit learn rate num layer optim adam warn ignor init arg project run sweep except thread chkstopthr file inform except thread netstatthr file inform end connectionreseterror winerror extern host heeft een verbind verbroken sent self sock send data connectionreseterror winerror extern host heeft een verbind verbroken error occur continu right creat new model configur time error occur loop model get evalu error occur loop warn ignor init arg project run sweep think definit group want certain log run finish command error error init error occur init loop add sweep overwritten sweep creat group aswel unsur place correctli sound logic defin run init code dont understand us case hope group fold valid seper init recommend welcom code code littl messi sorri def seed seed random seed seed environ pythonhashse str seed random set seed seed def build model config activ config activ dens unit config dens unit dens unit config dens unit num layer config num layer batch size config batch size batch norm fals optim config optim learn rate config learn rate kera layer concaten input input input layer input shape len norm train column len norm train iloc dens unit dens unit activ activ rang num layer dens unit dens unit activ activ dens unit dens unit activ activ output fed directli second dens output dens unit output dens dens unit dens unit activ activ output come dens output dens unit output dens model model input input input output output output print model summari model save weight model overwrit true return model def train test loss sum arrai loss sum arrai mdp loss sum arrai rmse sum arrai mdp rmse sum arrai loss sum arrai loss sum tot append test loss sum tot append mdp append pred append mdp append mdp sum sum test loss mean pred append mdp pred append loss sum tot mdp loss sum tot rmse sum tot mdp rmse sum tot vstack asarrai norm numpi vstack asarrai norm numpi max trial epoch test loss sum arrai train mdp train test test mdp test test hyperparam dict optim adam dens unit batch size epoch ens unit activ relu reducelr kera callback reducelronplateau monitor val loss mode auto factor patienc min delta default min verbos earlystop kera callback earlystop monitor val loss mode auto min delta patienc verbos run init project custom chart config hyperparam reinit true note reinit config config wandcalback callback monitor val loss model build model config config config learn rate config optim adam optim kera optim adam elif config optim rmsprop optim kera optim rmsprop rho epsilon decai rais compil model model compil optim optim loss output mse output mse metric output kera metric rootmeansquarederror output kera metric rootmeansquarederror split skf kfold split shuffl true skf split vall loss train index test index skf split init project custom chart group fold experi job type fold format model load weight model train index train index astyp int test index test index astyp int arrai arrai arrai mdp arrai mdp train test train index test index train test train index test index train test train index test index mdp train mdp test mdp train index mdp test index histori model fit convert tensor convert tensor mdp valid data convert tensor test convert tensor test test mdp test batch size config batch size epoch callback wandcalback earlystop reducelr verbos loss sum datafram histori histori loss iloc loss sum test loss sum datafram histori histori val loss iloc test loss sum loss sum datafram histori histori val output loss iloc loss sum mdp loss sum datafram histori histori val output loss iloc mdp loss sum rmse sum datafram histori histori val output root mean squar error iloc rmse sum mdp rmse sum datafram histori histori val output root mean squar error iloc mdp rmse sum loss sum tot datafram histori histori loss loss sum tot test loss sum tot datafram histori histori val loss test loss sum tot loss sum tot datafram histori histori val output loss loss sum tot mdp loss sum tot datafram histori histori val output loss mdp loss sum tot rmse sum tot datafram histori histori val output root mean squar error rmse sum tot mdp rmse sum tot datafram histori histori val output root mean squar error mdp rmse sum tot pred model predict convert tensor test convert tensor test metric tfa metric squar rsquar metric updat state test pred flatten result metric result result numpi append append metric updat state mdp test pred flatten result metric result mdp result numpi mdp append append mdp append append test mdp append append mdp test pred append append pred mdp pred append append pred mdp sum mdp sum pred sum sum pred test loss mean test loss sum split loss sum mean loss sum split test loss sum mean test loss sum split loss sum mean loss sum split mdp loss sum mean mdp loss sum split rmse sum mean rmse sum split mdp rmse sum mean mdp rmse sum split test mdp mean mdp append test mean append mean sum split mdp mean mdp sum split init project sweep optimalis randomsearch group experimentfold format job type valid init project custom chart group fold experi job type valid val lo rang len test loss sum tot log val loss mean test loss sum tot val lo split loss rang len loss sum tot log loss mean loss sum tot loss split val mdp lo rang len test loss sum tot log val mdp loss mean test loss sum tot val mdp lo split val lo rang len loss sum tot log val loss mean loss sum tot val lo split val mdp rmse rang len mdp rmse sum tot log val mdp rmse mean mdp loss sum tot val mdp rmse split val rmse rang len rmse sum tot log val mdp rmse mean rmse sum tot val rmse split append concaten append mdp append concaten mdp append pred append concaten pred append mdp pred append concaten mdp pred append score sqrt mean squar error pred append append mdp score sqrt mean squar error mdp pred append mdp append test mdp mean mdp append test mean append log score score score mdp mdp score test mdp test mdp asarrai mean flatten mdp asarrai mdp mean flatten testt asarrai test flatten mdp testt asarrai mdp test flatten datafram pred actual testt print tabl tabl datafram log scatter plot plot scatter tabl pred actual datafram pred mdp actual mdp testt print tabl tabl datafram log scatter plot plot scatter tabl pred actual predict mean predict tabl tabl data predict column predict log histogramm plot histogram tabl predict titl predict score distribut hubbl paramet hist histogram predict log hubbl paramet plot histogram hist predict mdp mean predict tabl tabl data predict column predict log histogram plot histogram tabl predict titl predict score distribut mass densiti hist histogram predict log mass densiti paramet plot histogram hist run finish sweep config method random metric test loss mean goal minim paramet dens unit distribut log uniform valu min max learn rate distribut log uniform valu min max dens unit distribut log uniform valu min max batch size integ evenli distribut logarithm valu optim valu adam rmsprop num layer valu sweep sweep sweep config entiti stijnvdbosch project custom chart agent sweep function train count project custom chart updat defin metric model fit call test loss mean suppos correct inform need help send messag repli blink",
        "Question_preprocessed_content":"problem us log question defin metric nice understand us init constantli agent model configur split dont right split model configur inform code code bit think wai us valid scikitlearn iv tri time input output input dataset number input dataset number output number nth dataset output number nth dataset input concaten produc output let split order shape think wai defin valid sklearn type data think iv introduc save model time configur load model zero weigth wai valid sure log file code bad produc group want overwrit mention time new model initi displai continu right creat new model configur time model get evalu warn ignor init arg project think definit group want certain log init creat group unsur place sound logic defin run init dont understand us case hope group fold valid seper init welcom code code updat defin metric inform help send repli blink",
        "Question_gpt_summary_original":"The user is encountering an error when using wandb.agent sweeps to another model configuration. The error occurs after the K-fold split when the new model configuration is applied. The user's code is messy and they are unsure how to correctly log their files. They are also having trouble grouping their folds and validation separately without using wandb.init(). The user has defined their own metric called test_loss_mean and is unsure if they did it correctly. The error occurs every loop and the user is seeking recommendations on how to resolve these issues.",
        "Question_gpt_summary":"user encount error agent sweep model configur error occur fold split new model configur appli user code messi unsur correctli log file have troubl group fold valid separ init user defin metric call test loss mean unsur correctli error occur loop user seek recommend resolv issu"
    },
    {
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1600261190477,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1600855880503,
        "Question_score_count":0.0,
        "Question_view_count":405.0,
        "Poster_created_time":1600260166047,
        "Poster_location":null,
        "Poster_reputation_count":15.0,
        "Poster_view_count":4.0,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1600604920243,
        "Answer_last_edit_time":1600855957376,
        "Answer_score_count":0.0,
        "Answerer_created_time":1285219808283,
        "Answerer_location":"Perth WA, Australia",
        "Answerer_reputation_count":6770.0,
        "Answerer_view_count":1127.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"powerbi integr current try integr model current deploi webservic powerbi integr model requir addit schema file deploi webservic model view powerbi problem come us log model perform subsequ deploi select model webservic integr deploi unfortun option defin schema file model deploi result model avail powerbi lack requir schema file option workaround possibl work rest api model power queri rewrit deploy code handl webservic deploy step azur instead thought ask mayb miss workaround current code defin schema file deploi deploi",
        "Question_preprocessed_content":"powerbi integr try integr model deploi webservic powerbi integr model requir schema file deploi webservic model view powerbi problem come us log model perform subsequ deploi select model webservic integr unfortun option defin schema file model deploi result model avail powerbi lack requir schema file option workaround work rest api model power queri rewrit deploy code handl webservic deploy step azur instead thought ask mayb workaround code defin schema file deploi",
        "Question_gpt_summary_original":"The user is trying to integrate an ML model deployed as a webservice on AzureML with PowerBI, but the model requires a schema file to be viewed in PowerBI. The user uses MLflow to deploy the model onto AzureML, but MLflow's AzureML integration does not have the option to define a schema file before deployment, resulting in no model being available in PowerBI. The user is considering finding a workaround using the REST API or rewriting the deployment code to handle the webservice deployment steps in Azure instead of MLflow.",
        "Question_gpt_summary":"user try integr model deploi webservic powerbi model requir schema file view powerbi user us deploi model integr option defin schema file deploy result model avail powerbi user consid find workaround rest api rewrit deploy code handl webservic deploy step azur instead"
    },
    {
        "Question_title":"ERROR: Cannot uninstall 'ruamel-yaml' while creating docker image for azure ML ACI deployment",
        "Question_body":"<p><strong>I am trying to deploy machine learning model in azure ACI but i am getting following error while creating a docker image<\/strong><\/p>\n<pre><code>Pip subprocess error:\nERROR: Cannot uninstall 'ruamel-yaml'. It is a distutils installed project and thus we cannot \naccurately determine which files belong to it which would lead to only a partial uninstall.\n<\/code><\/pre>\n<p>Below is my yml file for pip dependencies<\/p>\n<pre><code>name: project_environment\ndependencies:\n# The python interpreter version.\n\n# Currently Azure ML only supports 3.5.2 and later.\n\n\n- pip:\n  # Required packages for AzureML execution, history, and data preparation.\n  - pandas\n  - azureml-defaults\n  - azureml-sdk\n  - azureml-widgets\n  - numpy\n  - tensorflow-gpu\n  - keras\n  - azureml-defaults\n  - torch==1.4.0\n  - scikit-learn==0.22.2.post1\n<\/code><\/pre>\n<p>and if i use conda instead of pip then i am getting following error<\/p>\n<pre><code>Step 11\/13 : RUN CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; if [ -n \n&quot;$AZUREML_CONDA_ENVIRONMENT_PATH&quot; ]; then conda env update -p \n&quot;$AZUREML_CONDA_ENVIRONMENT_PATH&quot; -f '\/var\/azureml-app\/binary_2.yml'; else \nconda env update -n base -f '\/var\/azureml-app\/binary_2.yml'; fi &amp;&amp; conda \nclean -aqy &amp;&amp; rm -rf \/root\/.cache\/pip &amp;&amp; rm -rf &quot;$CONDA_ROOT_DIR\/pkgs&quot; &amp;&amp; \nfind &quot;$CONDA_ROOT_DIR&quot; -type d -name __pycache__ -exec rm -rf {} +\n---&gt; Running in 9e6eb7278bfc  \n[91mUnable to install package for Conda.\n\nPlease double check and ensure you dependencies file has\nthe correct spelling.  You might also try installing the\nconda-env-Conda package to see if provides the required\ninstaller. \n[0mThe command '\/bin\/sh -c CONDA_ROOT_DIR=$(conda info --root) &amp;&amp; if [ -n \n&quot;$AZUREML_CONDA_ENVIRONMENT_PATH&quot; ]; then conda env update -p \n&quot;$AZUREML_CONDA_ENVIRONMENT_PATH&quot; -f '\/var\/azureml-app\/binary_2.yml'; else \n conda env update -n base -f '\/var\/azureml-app\/binary_2.yml'; fi &amp;&amp; conda \nclean \n-aqy &amp;&amp; rm -rf \/root\/.cache\/pip &amp;&amp; rm -rf &quot;$CONDA_ROOT_DIR\/pkgs&quot; &amp;&amp; find \n&quot;$CONDA_ROOT_DIR&quot; -type d -name __pycache__ -exec rm -rf {} +' returned a \nnon- \n zero code: 255\n 2020\/08\/12 19:36:09 Container failed during run: acb_step_0. No retries \n remaining.\n failed to run step ID: acb_step_0: exit status 255\n<\/code><\/pre>\n<p>**Can anyone please help me **<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":6.0,
        "Question_created_time":1597259959573,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597261729536,
        "Question_score_count":9.0,
        "Question_view_count":9743.0,
        "Poster_created_time":1426308318630,
        "Poster_location":null,
        "Poster_reputation_count":179.0,
        "Poster_view_count":44.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63383400",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error uninstal ruamel yaml creat docker imag aci deploy try deploi machin learn model azur aci get follow error creat docker imag pip subprocess error error uninstal ruamel yaml distutil instal project accur determin file belong lead partial uninstal yml file pip depend project environ depend python interpret version current support later pip requir packag execut histori data prepar panda default sdk widget numpi tensorflow gpu kera default torch scikit learn post us conda instead pip get follow error step run conda root dir conda info root conda environ path conda env updat conda environ path var app binari yml conda env updat base var app binari yml conda clean aqi root cach pip conda root dir pkg conda root dir type pycach exec run eebbfc munabl instal packag conda doubl check ensur depend file correct spell try instal conda env conda packag provid requir instal mthe command bin conda root dir conda info root conda environ path conda env updat conda environ path var app binari yml conda env updat base var app binari yml conda clean aqi root cach pip conda root dir pkg conda root dir type pycach exec return non zero code contain fail run acb step retri remain fail run step acb step exit statu help",
        "Question_preprocessed_content":"creat docker imag aci deploy try deploi machin learn model azur aci creat docker imag yml file pip depend us conda instead pip help",
        "Question_gpt_summary_original":"The user is encountering an error while creating a docker image for Azure ML ACI deployment. The error message states that 'ruamel-yaml' cannot be uninstalled as it is a distutils installed project. The user has provided a yml file for pip dependencies, but using conda instead of pip results in another error. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error creat docker imag aci deploy error messag state ruamel yaml uninstal distutil instal project user provid yml file pip depend conda instead pip result error user seek help resolv issu"
    },
    {
        "Question_title":"Kubeflow, passing Python dataframe across components?",
        "Question_body":"<p>I am writing a Kubeflow component which reads an input query and creates a <code>dataframe<\/code>, roughly as:<\/p>\n<pre><code>from kfp.v2.dsl import component \n\n@component(...)\ndef read_and_write():\n    # read the input query \n    # transform to dataframe \n    sql.to_dataframe()\n<\/code><\/pre>\n<p>I was wondering how I can pass this dataframe to the next operation in my Kubeflow pipeline.\nIs this possible? Or do I have to save the dataframe in a csv or other formats and then pass the output path of this?\nThank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1634041967303,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":879.0,
        "Poster_created_time":1624352292607,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69540618",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"kubeflow pass python datafram compon write kubeflow compon read input queri creat datafram roughli kfp dsl import compon compon def read write read input queri transform datafram sql datafram wonder pass datafram oper kubeflow pipelin possibl save datafram csv format pass output path thank",
        "Question_preprocessed_content":"kubeflow python datafram compon write kubeflow compon read input queri creat roughli wonder datafram oper kubeflow pipelin save datafram csv format output path thank",
        "Question_gpt_summary_original":"The user is developing a Kubeflow component that creates a dataframe from an input query. They are seeking guidance on how to pass this dataframe to the next operation in their Kubeflow pipeline, or if they need to save it in a different format first.",
        "Question_gpt_summary":"user develop kubeflow compon creat datafram input queri seek guidanc pass datafram oper kubeflow pipelin need save differ format"
    },
    {
        "Question_title":"Scan_history() is empty",
        "Question_body":"<p>Hi, when I run <code>run.history()<\/code>, I get a sampled version of the history as expected (although the number of samples fluctuates). But when I run <code>run.scan_history()<\/code>, I get an empty object (i.e. 0 rows).<\/p>\n<p>Any idea why this is happening or how it could be fixed?<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":null,
        "Question_created_time":1675419840337,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":236.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/scan-history-is-empty\/3811",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"scan histori run run histori sampl version histori expect number sampl fluctuat run run scan histori object row idea happen fix",
        "Question_preprocessed_content":" run sampl version histori expect run object idea fix",
        "Question_gpt_summary_original":"The user is facing an issue where the function run.scan_history() is returning an empty object with 0 rows, while run.history() is returning a sampled version of the history as expected. The user is seeking help to understand why this is happening and how it can be fixed.",
        "Question_gpt_summary":"user face issu function run scan histori return object row run histori return sampl version histori expect user seek help understand happen fix"
    },
    {
        "Question_title":"AWS Sagemaker Life Cycle Configuration - AutoStop",
        "Question_body":"<p>I have created 4 instances in my AWS sagemaker NOTEBOOKS tab.<\/p>\n<p>I want to create a life cycle configuration where the instance should stop every day at 9:00 PM.<\/p>\n<p>I have seen some examples but it is with IDLE TIME but not with the specific time<\/p>\n<pre><code>#!\/bin\/bash\nset -e\n\n# PARAMETERS\nIDLE_TIME=3600\n\necho &quot;Fetching the autostop script&quot;\nwget -O autostop.py https:\/\/raw.githubusercontent.com\/mariokostelac\/sagemaker-setup\/master\/scripts\/auto-stop-idle\/autostop.py\n\necho &quot;Starting the SageMaker autostop script in cron&quot;\n(crontab -l 2&gt;\/dev\/null; echo &quot;*\/5 * * * * \/bin\/bash -c '\/usr\/bin\/python3 $DIR\/autostop.py --time ${IDLE_TIME} | tee -a \/home\/ec2-user\/SageMaker\/auto-stop-idle.log'&quot;) | crontab -\n\necho &quot;Changing cloudwatch configuration&quot;\ncurl https:\/\/raw.githubusercontent.com\/mariokostelac\/sagemaker-setup\/master\/scripts\/publish-logs-to-cloudwatch\/on-start.sh | sudo bash -s auto-stop-idle \/home\/ec2-user\/SageMaker\/auto-stop-idle.log\n<\/code><\/pre>\n<p>Can anyone help me out on this one?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1645181373237,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1645181463240,
        "Question_score_count":1.0,
        "Question_view_count":357.0,
        "Poster_created_time":1608154572552,
        "Poster_location":"Islamabad, Pakistan",
        "Poster_reputation_count":75.0,
        "Poster_view_count":33.0,
        "Answer_body":"<p>Change the crontab syntax to <code>0 21 * * * shutdown.py<\/code><\/p>\n<p>Then create a shutdown.py which is reduced version of the <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/auto-stop-idle\/autostop.py\" rel=\"nofollow noreferrer\">autostop.py<\/a> and contains mainly:<\/p>\n<pre><code>...\nprint('Closing notebook')\nclient = boto3.client('sagemaker')\nclient.stop_notebook_instance(NotebookInstanceName=get_notebook_name())\n<\/code><\/pre>\n<p>BTW: triggering <code>shutdown now<\/code> directly from the crontab command didn't work for me, therefore calling the SageMaker API instead.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1645343727107,
        "Answer_last_edit_time":1645350630310,
        "Answer_score_count":1.0,
        "Answerer_created_time":1244808478036,
        "Answerer_location":"Israel",
        "Answerer_reputation_count":4932.0,
        "Answerer_view_count":405.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71172207",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"life cycl configur autostop creat instanc notebook tab want creat life cycl configur instanc stop dai seen exampl idl time specif time bin bash set paramet idl time echo fetch autostop script wget autostop http raw githubusercont com mariokostelac setup master script auto stop idl autostop echo start autostop script cron crontab dev null echo bin bash usr bin python dir autostop time idl time tee home user auto stop idl log crontab echo chang cloudwatch configur curl http raw githubusercont com mariokostelac setup master script publish log cloudwatch start sudo bash auto stop idl home user auto stop idl log help",
        "Question_preprocessed_content":"life cycl configur autostop creat instanc tab want creat life cycl configur instanc stop dai exampl idl time specif time help",
        "Question_gpt_summary_original":"The user has created 4 instances in AWS Sagemaker Notebooks and wants to create a life cycle configuration to stop the instance every day at 9:00 PM. The user has found examples with idle time but not with a specific time. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user creat instanc notebook want creat life cycl configur stop instanc dai user exampl idl time specif time user seek help resolv issu"
    },
    {
        "Question_title":"DVC Push KeyError fileSize",
        "Question_body":"<p>I've added a large list of CSV files to my dvc repository but when I try to do DVC push it complains with<\/p>\n<pre><code>ERROR: unexpected error - KeyError('fileSize')\n<\/code><\/pre>\n<p><strong>Edit<\/strong>\nSo searching around it seem that it might help to include the verbose log with regards to the error.<\/p>\n<pre><code>T11:27:08~\/documents\/*****\/data$ dvc push -v\n2022-02-01 11:32:13,186 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/config.local' to gitignore file.\n2022-02-01 11:32:13,199 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp' to gitignore file.\n2022-02-01 11:32:13,200 DEBUG: Adding '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to gitignore file.\n2022-02-01 11:32:14,102 DEBUG: Preparing to transfer data from '\/home\/jhylands\/Documents\/*****\/.dvc\/cache' to '*********'\n2022-02-01 11:32:14,102 DEBUG: Preparing to collect status from '********'\n2022-02-01 11:32:14,103 DEBUG: Collecting status from '*******'\n2022-02-01 11:32:14,439 DEBUG: GDrive remote auth with config '{'client_config_backend': 'settings', 'client_config_file': 'client_secrets.json', 'save_credentials': True, 'oauth_scope': ['https:\/\/www.googleapis.com\/auth\/drive', 'https:\/\/www.googleapis.com\/auth\/drive.appdata'], 'save_credentials_backend': 'file', 'save_credentials_file': '\/home\/jhylands\/Documents\/*****\/.dvc\/tmp\/gdrive-user-credentials.json', 'get_refresh_token': True, 'client_config': {'client_id': '*****.apps.googleusercontent.com', 'client_secret': '****************', 'auth_uri': 'https:\/\/accounts.google.com\/o\/oauth2\/auth', 'token_uri': 'https:\/\/oauth2.googleapis.com\/token', 'revoke_uri': 'https:\/\/oauth2.googleapis.com\/revoke', 'redirect_uri': ''}}'.\n2022-02-01 11:32:14,994 DEBUG: Estimated remote size: 256 files\n2022-02-01 11:32:14,995 DEBUG: Querying '316' hashes via traverse\n2022-02-01 11:32:15,325 ERROR: unexpected error - KeyError('fileSize')\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 226, in __getitem__\n    return dict.__getitem__(self, key)\nKeyError: 'fileSize'\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/main.py&quot;, line 55, in main\n    ret = cmd.do_run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/base.py&quot;, line 45, in do_run\n    return self.run()\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py&quot;, line 57, in run\n    processed_files_count = self.repo.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py&quot;, line 56, in push\n    pushed += self.cloud.push(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py&quot;, line 85, in push\n    return transfer(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py&quot;, line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 158, in compare_status\n    dest_exists, dest_missing = status(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py&quot;, line 131, in status\n    exists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 499, in hashes_exist\n    remote_hashes = set(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 334, in _list_hashes_traverse\n    yield from itertools.chain.from_iterable(in_remote)\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 611, in result_iterator\n    yield fs.pop().result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 439, in result\n    return self.__get_result()\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py&quot;, line 388, in __get_result\n    raise self._exception\n  File &quot;\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py&quot;, line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 324, in list_with_update\n    return list(\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 215, in _list_hashes\n    for path in self._list_paths(prefix, progress_callback):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py&quot;, line 195, in _list_paths\n    for file_info in self.fs.find(fs_path, prefix=prefix):\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py&quot;, line 107, in find\n    yield from self.fs.find(path)\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/fs\/spec.py&quot;, line 323, in find\n    &quot;size&quot;: int(item[&quot;fileSize&quot;]),\n  File &quot;\/home\/jhylands\/.local\/lib\/python3.8\/site-packages\/pydrive2\/files.py&quot;, line 229, in __getitem__\n    raise KeyError(e)\nKeyError: KeyError('fileSize')\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":5.0,
        "Question_created_time":1643714210580,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1643715642688,
        "Question_score_count":1.0,
        "Question_view_count":54.0,
        "Poster_created_time":1333908536528,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation_count":884.0,
        "Poster_view_count":59.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70939675",
        "Tool":"DVC",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"push keyerror files ad larg list csv file repositori try push complain error unexpect error keyerror files edit search help includ verbos log regard error document data push debug ad home jhyland document config local gitignor file debug ad home jhyland document tmp gitignor file debug ad home jhyland document cach gitignor file debug prepar transfer data home jhyland document cach debug prepar collect statu debug collect statu debug gdrive remot auth config client config backend set client config file client secret json save credenti true oauth scope http www googleapi com auth drive http www googleapi com auth drive appdata save credenti backend file save credenti file home jhyland document tmp gdrive user credenti json refresh token true client config client app googleusercont com client secret auth uri http account googl com oauth auth token uri http oauth googleapi com token revok uri http oauth googleapi com revok redirect uri debug estim remot size file debug queri hash travers error unexpect error keyerror files traceback recent file home jhyland local lib python site packag pydriv file line getitem return dict getitem self kei keyerror files handl except except occur traceback recent file home jhyland local lib python site packag main line main ret cmd run file home jhyland local lib python site packag command base line run return self run file home jhyland local lib python site packag command data sync line run process file count self repo push file home jhyland local lib python site packag repo init line wrapper return repo arg kwarg file home jhyland local lib python site packag repo push line push push self cloud push file home jhyland local lib python site packag data cloud line push return transfer file home jhyland local lib python site packag object transfer line transfer statu compar statu src dest obj id check delet fals kwarg file home jhyland local lib python site packag object statu line compar statu dest exist dest miss statu file home jhyland local lib python site packag object statu line statu exist updat odb hash exist hash odb path kwarg file home jhyland local lib python site packag object base line hash exist remot hash set file home jhyland local lib python site packag object base line list hash travers yield itertool chain iter remot file usr lib python concurr futur base line result iter yield pop result file usr lib python concurr futur base line result return self result file usr lib python concurr futur base line result rais self except file usr lib python concurr futur thread line run result self self arg self kwarg file home jhyland local lib python site packag object base line list updat return list file home jhyland local lib python site packag object base line list hash path self list path prefix progress callback file home jhyland local lib python site packag object base line list path file info self path prefix prefix file home jhyland local lib python site packag fsspec wrapper line yield self path file home jhyland local lib python site packag pydriv spec line size int item files file home jhyland local lib python site packag pydriv file line getitem rais keyerror keyerror keyerror files",
        "Question_preprocessed_content":"push files larg list csv file repositori try push complain edit search help includ verbos log regard",
        "Question_gpt_summary_original":"The user is encountering an error while trying to push a large list of CSV files to their DVC repository. The error message states \"KeyError('fileSize')\" and the user has tried including verbose logs to troubleshoot the issue.",
        "Question_gpt_summary":"user encount error try push larg list csv file repositori error messag state keyerror files user tri includ verbos log troubleshoot issu"
    },
    {
        "Question_title":"how to add autoscaling policy to an sagemaker endpoint via terraform?",
        "Question_body":"<p>based on sagemaker example, added an autoscaling policy ( sample code below) to an existing asynchronous sagemaker endpoint. however , looking into terraform documentation , to see if i can add autopolicy via terraform.Terraform documentation has all the resources need to create the asynchornous endpoint but not sure if it is possible to add auto scaling policy as well. there is one similar question but with untried solution in SO.<\/p>\n<pre><code>client = boto3.client(    &quot;application-autoscaling&quot;) \nresource_id = ( &quot;endpoint\/&quot; + endpoint_name + &quot;\/variant\/&quot; + &quot;variant1&quot;)  \nresponse = client.register_scalable_target(\n    ServiceNamespace=&quot;sagemaker&quot;,\n    ResourceId=resource_id,\n    ScalableDimension=&quot;sagemaker:variant:DesiredInstanceCount&quot;,\n    MinCapacity=0,\n    MaxCapacity=5,\n)\nresponse = client.put_scaling_policy(\n    PolicyName=&quot;Invocations-ScalingPolicy&quot;,\n    ServiceNamespace=&quot;sagemaker&quot;, \n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=&quot;sagemaker:variant:DesiredInstanceCount&quot;,  \n    PolicyType=&quot;TargetTrackingScaling&quot;,  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        &quot;TargetValue&quot;: 5.0,  \n         SageMakerVariantInvocationsPerInstance\n        &quot;CustomizedMetricSpecification&quot;: {\n            &quot;MetricName&quot;: &quot;ApproximateBacklogSizePerInstance&quot;,\n            &quot;Namespace&quot;: &quot;AWS\/SageMaker&quot;,\n            &quot;Dimensions&quot;: [{&quot;Name&quot;: &quot;EndpointName&quot;, &quot;Value&quot;: endpoint_name}],\n            &quot;Statistic&quot;: &quot;Average&quot;,\n        },\n        &quot;ScaleInCooldown&quot;: 600,\n   ....\n    },\n)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_created_time":1646877179737,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":172.0,
        "Poster_created_time":1584308275360,
        "Poster_location":null,
        "Poster_reputation_count":365.0,
        "Poster_view_count":94.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71417970",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"add autosc polici endpoint terraform base exampl ad autosc polici sampl code exist asynchron endpoint look terraform document add autopolici terraform terraform document resourc need creat asynchorn endpoint sure possibl add auto scale polici similar question untri solut client boto client applic autosc resourc endpoint endpoint variant variant respons client regist scalabl target servicenamespac resourceid resourc scalabledimens variant desiredinstancecount mincapac maxcapac respons client scale polici policynam invoc scalingpolici servicenamespac resourceid resourc endpoint scalabledimens variant desiredinstancecount policytyp targettrackingsc stepscal targettrackingsc targettrackingscalingpolicyconfigur targetvalu variantinvocationsperinst customizedmetricspecif metricnam approximatebacklogsizeperinst namespac aw dimens endpointnam valu endpoint statist averag scaleincooldown",
        "Question_preprocessed_content":"autosc polici endpoint base exampl autosc polici exist asynchron endpoint document autopolici document resourc creat asynchorn endpoint sure auto scale polici similar question untri solut",
        "Question_gpt_summary_original":"The user is trying to add an autoscaling policy to an existing asynchronous Sagemaker endpoint via Terraform. They have added a sample code for autoscaling policy, but they are unsure if it is possible to add an autoscaling policy via Terraform as the documentation only has resources to create the asynchronous endpoint. The user has found a similar question on SO but with an untried solution.",
        "Question_gpt_summary":"user try add autosc polici exist asynchron endpoint terraform ad sampl code autosc polici unsur possibl add autosc polici terraform document resourc creat asynchron endpoint user similar question untri solut"
    },
    {
        "Question_title":"Simple but frustrating error: Google.cloud module not found",
        "Question_body":"Have installed Google Cloud via pip and CLI installer, yet programs cannot seem to see import statements from Google.cloud, returning the following error:\n\nline 9, in <module>\nfrom google.cloud import vision\nModuleNotFoundError: No module named 'google.cloud'\n\nPlease advise and thank you for your time.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1672262100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":143.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Have you tried installing google cloud vision? You can also check what python version you are using, this package is only supported in python versions 3.7 and up.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Question_closed_time":"12-30-2022 09:41 AM",
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Simple-but-frustrating-error-Google-cloud-module-not-found\/td-p\/504285\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"simpl frustrat error googl cloud modul instal googl cloud pip cli instal program import statement googl cloud return follow error line googl cloud import vision modulenotfounderror modul name googl cloud advis thank time",
        "Question_preprocessed_content":"simpl frustrat cloud modul cloud pip cli program import statement cloud return line cloud import vision modul name advis thank time",
        "Question_gpt_summary_original":"The user has installed Google Cloud via pip and CLI installer, but is unable to import statements from Google.cloud. The error message \"ModuleNotFoundError: No module named 'google.cloud'\" is being returned. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user instal googl cloud pip cli instal unabl import statement googl cloud error messag modulenotfounderror modul name googl cloud return user seek advic resolv issu"
    },
    {
        "Question_title":"Regular expressions error when trying to deploy Sagemaker OD model",
        "Question_body":"<p>I am following this tutorial to try and deploy my own previously trained model, which I have artifacts for in a tar.gc file, stored in an S3 bucket (<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/object-detection-and-model-retraining-with-amazon-sagemaker-and-amazon-augmented-ai\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/object-detection-and-model-retraining-with-amazon-sagemaker-and-amazon-augmented-ai\/<\/a>)<\/p>\n<p>Everything seems to run fine until I get to this point in the code:<\/p>\n<pre><code># deploying the model into one ml.m4.xlarge instance\nobject_detector = model.deploy(initial_instance_count = 1,\n                               instance_type = 'ml.m4.xlarge',\n                               endpoint_name = endpoint_name)\n<\/code><\/pre>\n<p>Which spits out this error for me:<\/p>\n<p><code>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: 2 validation errors detected: Value 'model.tar.gz-2021-08-26-08-43-29-384' at 'modelName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])*; Value '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/object-detection:1' at 'primaryContainer.modelDataUrl' failed to satisfy constraint: Member must satisfy regular expression pattern: ^(https|s3):\/\/([^\/]+)\/?(.*)$<\/code><\/p>\n<p>I checked the attributes which it seems to refer to and they are:<\/p>\n<pre><code>model.model_data\n<\/code><\/pre>\n<p><code>'685385470294.dkr.ecr.eu-west-1.amazonaws.com\/object-detection:1'<\/code><\/p>\n<p>and<\/p>\n<pre><code>model.name\n<\/code><\/pre>\n<p><code>'model.tar.gz-2021-08-26-08-43-29-384'<\/code><\/p>\n<p>But I'm not sure what the issue is supposed to be. Earlier in the demo it seems to want to copy the tar.gc file from the output location to another, and I don't understand the significance of that. Can't it just stay where it is in \/output? I just copied it to one sub-directory below the folder it was deposited into after training, if that helps. e.g. S3\/folder\/output -&gt; S3\/folder\/output\/model\/<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1629968315473,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":730.0,
        "Poster_created_time":1507573254672,
        "Poster_location":"Gatineau, QC, Canada",
        "Poster_reputation_count":505.0,
        "Poster_view_count":86.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68935545",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"regular express error try deploi model follow tutori try deploi previous train model artifact tar file store bucket http aw amazon com blog machin learn object detect model retrain amazon amazon augment run fine point code deploi model xlarg instanc object detector model deploi initi instanc count instanc type xlarg endpoint endpoint spit error clienterror error occur validationexcept call createmodel oper valid error detect valu model tar modelnam fail satisfi constraint member satisfi regular express pattern valu dkr ecr west amazonaw com object detect primarycontain modeldataurl fail satisfi constraint member satisfi regular express pattern http check attribut refer model model data dkr ecr west amazonaw com object detect model model tar sure issu suppos earlier demo want copi tar file output locat understand signific stai output copi sub directori folder deposit train help folder output folder output model",
        "Question_preprocessed_content":"regular try deploi model tutori try deploi previous train model artifact file store bucket run fine point code spit check refer sure earlier demo want copi file output locat understand signific stai output copi folder deposit train help",
        "Question_gpt_summary_original":"The user is encountering a regular expression error while trying to deploy their previously trained model using Sagemaker OD. The error occurs when the code tries to deploy the model into one ml.m4.xlarge instance. The error message indicates that the values for 'modelName' and 'primaryContainer.modelDataUrl' do not satisfy the regular expression pattern. The user is unsure of the significance of copying the tar.gc file from the output location to another and is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount regular express error try deploi previous train model error occur code tri deploi model xlarg instanc error messag indic valu modelnam primarycontain modeldataurl satisfi regular express pattern user unsur signific copi tar file output locat seek help resolv issu"
    },
    {
        "Question_title":"Is it possible to write to synapse from ml studio using python SDK?",
        "Question_body":"<p>Hi Team,    <\/p>\n<p>I am trying to write to Azure SQL database from Azure ML studio. When I create a pipeline in designer, I can use Export Data Component to write data to a table in Azure SQL database. Our platform team has added a datastore with type as Azure SQL database in ML studio workspace. So, I can select the respective datastore, give table name and copy data using export data component in designer.    <\/p>\n<p>However, my intention is to write a Python script to create a pipeline. I don't find any step in ML Python SDK which can be used same as Export data component in designer. I want to write my results generated within the pipeline back to a table in Azure SQL database.    <\/p>\n<p>Solutions tried:    <br \/>\nStoring the result first in blob storage and then using datatransferstep() to copy that result to Azure SQL database. But this can't work for me because it requires ADF instance.    <\/p>\n<p>Is there any other way to directly write to Azure SQL databasein ML Python SDK instead of using datatransferstep and copying to blob storage?    <br \/>\nDue to restricted permissions, I can't create ADF instance to use for datatransferstep.    <\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1656484327123,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/907522\/is-it-possible-to-write-to-synapse-from-ml-studio",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl write synaps studio python sdk team try write azur sql databas studio creat pipelin design us export data compon write data tabl azur sql databas platform team ad datastor type azur sql databas studio workspac select respect datastor tabl copi data export data compon design intent write python script creat pipelin step python sdk export data compon design want write result gener pipelin tabl azur sql databas solut tri store result blob storag datatransferstep copi result azur sql databas work requir adf instanc wai directli write azur sql databasein python sdk instead datatransferstep copi blob storag restrict permiss creat adf instanc us datatransferstep thank",
        "Question_preprocessed_content":"write synaps studio python sdk team try write azur sql databas studio creat pipelin design us export data compon write data tabl azur sql databas platform team datastor type azur sql databas studio workspac select respect datastor tabl copi data export data compon design intent write python script creat pipelin step python sdk export data compon design want write result gener pipelin tabl azur sql databas solut tri store result blob storag datatransferstep copi result azur sql databas work requir adf instanc wai directli write azur sql databasein python sdk instead datatransferstep copi blob storag restrict creat adf instanc us datatransferstep thank",
        "Question_gpt_summary_original":"The user is trying to write to an Azure SQL database from Azure ML Studio using a Python script, but cannot find a step in the ML Python SDK that can be used to write data to the database directly. The user has tried storing the result in blob storage and then using datatransferstep() to copy the result to the database, but this requires an ADF instance which the user cannot create due to restricted permissions. The user is seeking an alternative solution to write data directly to the Azure SQL database using ML Python SDK.",
        "Question_gpt_summary":"user try write azur sql databas studio python script step python sdk write data databas directli user tri store result blob storag datatransferstep copi result databas requir adf instanc user creat restrict permiss user seek altern solut write data directli azur sql databas python sdk"
    },
    {
        "Question_title":"is it possible to create steps within the sagemaker pipeline via cloudformation?",
        "Question_body":"I am experimenting with sagemaker studio, while i was able to create the sagemaker studio domain and user profiles via cloudformation. I was wondering if, it was possible to create sagemaker projects and other resources like model registry group and steps for preprocessing , training ... via cloudformation? if is it possible , are there any samples, examples around this, if it is not supported or is not possible via cfn , may be help me on how can create link this to existing user and domain in studio  .",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1663606095949,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668618307228,
        "Question_score_count":0.0,
        "Question_view_count":73.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3aphi0KgRNyk6AcB9c9Spg\/is-it-possible-to-create-steps-within-the-sagemaker-pipeline-via-cloudformation",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"possibl creat step pipelin cloudform experi studio abl creat studio domain user profil cloudform wonder possibl creat project resourc like model registri group step preprocess train cloudform possibl sampl exampl support possibl cfn help creat link exist user domain studio",
        "Question_preprocessed_content":"creat step pipelin cloudform experi studio abl creat studio domain user profil cloudform wonder creat project resourc like model registri group step train cloudform sampl exampl cfn help creat link exist user domain studio",
        "Question_gpt_summary_original":"The user is trying to create SageMaker projects and resources like model registry group and steps for preprocessing, training, etc. via CloudFormation. They are seeking information on whether this is possible and if there are any examples available. They also need help linking this to an existing user and domain in SageMaker Studio.",
        "Question_gpt_summary":"user try creat project resourc like model registri group step preprocess train cloudform seek inform possibl exampl avail need help link exist user domain studio"
    },
    {
        "Question_title":"Unable to resolve the private dns name of a Sagemaker runtime VPC interface endpoint",
        "Question_body":"I deployed interface endpoints for multiple AWS services into a dedicated subnet in my VPC. Besides a Sagemaker runtime interface endpoint I also created endpoints for CloudWatch logs, KMS and more.\n\nThe resolving of the service domain name (e.g. kms.eu-central-1.amazonaws.com) works for all endpoints, except for the Sagemaker runtime (e.g. runtime.sagemaker.eu-central-1.amazonaws.com).The endpoint specific domain names of the Sagemaker runtime endpoint also work, only the resolving of runtime.sagemaker.eu-central-1.amazonaws.com fails with no answers from the DNS server.\n\nI tried multiple times to recreate the endpoint, but that didn't help either. The resolving of the domain works if I deploy the endpoint into another test VPC.\n\nAny ideas on what could be wrong? Thanks in advance!\nBert",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1657193804686,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668594825292,
        "Question_score_count":0.0,
        "Question_view_count":476.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUL78EiZhMTeq3v_FVsB-kXQ\/unable-to-resolve-the-private-dns-name-of-a-sagemaker-runtime-vpc-interface-endpoint",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl resolv privat dn runtim vpc interfac endpoint deploi interfac endpoint multipl aw servic dedic subnet vpc runtim interfac endpoint creat endpoint cloudwatch log km resolv servic domain km central amazonaw com work endpoint runtim runtim central amazonaw com endpoint specif domain name runtim endpoint work resolv runtim central amazonaw com fail answer dn server tri multipl time recreat endpoint help resolv domain work deploi endpoint test vpc idea wrong thank advanc bert",
        "Question_preprocessed_content":"unabl resolv privat dn runtim vpc interfac endpoint deploi interfac endpoint multipl aw servic dedic subnet vpc runtim interfac endpoint creat endpoint cloudwatch log km resolv servic domain work endpoint runtim endpoint specif domain name runtim endpoint work resolv fail answer dn server tri multipl time recreat endpoint help resolv domain work deploi endpoint test vpc idea wrong thank advanc bert",
        "Question_gpt_summary_original":"The user is unable to resolve the private DNS name of a Sagemaker runtime VPC interface endpoint, while the resolving of the service domain name works for all other endpoints. The user has tried recreating the endpoint multiple times, but the issue persists. The resolving of the domain works in another test VPC.",
        "Question_gpt_summary":"user unabl resolv privat dn runtim vpc interfac endpoint resolv servic domain work endpoint user tri recreat endpoint multipl time issu persist resolv domain work test vpc"
    },
    {
        "Question_title":"Best practice for python package dependency?",
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m figuring out how to structure my project and found your tutorials a very helpful introduction.<\/p>\n<p>In them, however, you only rely on a single script for each stage, for example:<\/p>\n<pre><code class=\"lang-auto\">dvc run -d code\/xml_to_tsv.py -d data\/Posts.xml -o data\/Posts.tsv \\\n          -f prepare.dvc \\\n          python code\/xml_to_tsv.py data\/Posts.xml data\/Posts.tsv\n<\/code><\/pre>\n<p><a href=\"https:\/\/dvc.org\/doc\/tutorials\/pipelines\" rel=\"nofollow noopener\">Source<\/a><\/p>\n<p>Now my understanding is the following:<br>\nIf inside of <code>xml_to_tsv.py<\/code> are imports of some other of my libraries (e.g., a <code>my_special_xml_importer.py<\/code>), and I make changes to <code>my_special_xml_importer.py<\/code>, these changes would not be picked up by dvc, since <code>my_special_xml_importer.py<\/code> is not an explicit dependency of the stage, correct?<\/p>\n<p>What\u2019s the best practice here for bigger projects, where each DVC stage is not just contained in a single script?<\/p>\n<p>Our use case will be the following: For each stage we\u2019ll be having a jupyter notebook, which will import some of our python packages. I\u2019m assuming I should create a stage like this:<\/p>\n<pre><code class=\"lang-auto\">dvc run -d my_notebook.ipynb -d code\/my_lib.py -d data\/Posts.xml -o data\/Posts.tsv\n  -f prepare.dvc\n   papermill my_notebook.ipynb my_notebook_out.ipynb\n<\/code><\/pre>\n<p>Is this a good way, are there other ways, how are people with bigger projects dealing with this issue?<\/p>\n<p>Thanks in advance,<br>\nFabi<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1587641178959,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":695.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-python-package-dependency\/358",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"best practic python packag depend figur structur project tutori help introduct reli singl script stage exampl run code xml tsv data post xml data post tsv prepar python code xml tsv data post xml data post tsv sourc understand follow insid xml tsv import librari special xml import chang special xml import chang pick special xml import explicit depend stage correct what best practic bigger project stage contain singl script us case follow stage have jupyt notebook import python packag assum creat stage like run notebook ipynb code lib data post xml data post tsv prepar papermil notebook ipynb notebook ipynb good wai wai peopl bigger project deal issu thank advanc fabi",
        "Question_preprocessed_content":"best practic python packag depend figur structur project tutori help introduct reli singl script stage exampl sourc understand insid import librari chang chang pick explicit depend stage what best practic project stage contain singl script us case stage have jupyt import python packag creat stage like wai wai peopl project deal thank advanc fabi",
        "Question_gpt_summary_original":"The user is seeking advice on the best practice for managing dependencies in a Python project using DVC. They are concerned that changes made to a library that is not an explicit dependency of a DVC stage will not be picked up by DVC. They are asking for suggestions on how to handle this issue in larger projects where each DVC stage is not contained in a single script. The user proposes creating a stage that includes all necessary dependencies, including Jupyter notebooks and Python packages. They are seeking feedback on this approach and other ways to manage dependencies in larger projects.",
        "Question_gpt_summary":"user seek advic best practic manag depend python project concern chang librari explicit depend stage pick ask suggest handl issu larger project stage contain singl script user propos creat stage includ necessari depend includ jupyt notebook python packag seek feedback approach wai manag depend larger project"
    },
    {
        "Question_title":"Why am I getting unexplained charges on my Microsoft Azure free account?",
        "Question_body":"<p>Hi, hope everyone is well. I opened an Azure free account on the 10th of January. I used the Azure portal for Azure Fundamentals and Azure AI Fundamentals. One day I noticed that I didn't have access to the AI Fundamentals on the portal only to find out that costs have been incurred on my free account. I am not sure where these costs are coming from, because when I checked my invoice, it is not giving me a clear explanation of where these costs are coming from only labeled them as &quot;other Microsoft services&quot;  <\/p>\n<p>Please if someone could provide me with some clarity as to why I am getting these charges. Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1678896697733,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hi,<\/p>\n<p>Please create a new billing support request.  Below is link to open new billing support request in portal:<\/p>\n<p><a href=\"https:\/\/portal.azure.com\/#view\/Microsoft_Azure_CostManagement\/Menu\/%7E\/support\/openedBy\/AzurePortal\">https:\/\/portal.azure.com\/#view\/Microsoft_Azure_CostManagement\/Menu\/~\/support\/openedBy\/AzurePortal<\/a><\/p>\n<p>It will look similar to below sample screenshot:<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/6882cb8e-c0c1-4aed-bffe-59c1e098af00?platform=QnA\" alt=\"azure new billing support request sample\" \/><\/p>\n<p>Enter summary, select Problem type, select Problem subtype, then click Next. After a delay, some potential solutions will be shown. Click <strong>Return to support request<\/strong> as shown below:<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/cfc06bb3-704f-44f5-af90-c32082ab5821?platform=QnA\" alt=\"azure support request solutions page\" \/><\/p>\n<p>This will return you to the recommended solution tab. Click <strong>Next<\/strong>, as shown below:<\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/e17dbf4b-713a-4c8c-aa0d-6a3f60edf3ea?platform=QnA\" alt=\"azure support request after solutions page\" \/><\/p>\n<p>Fill out the information on the Additional details tab making sure your email address\/phone are correct, click <strong>Next<\/strong>, then finally click <strong>Create<\/strong>. At this point your support request will be created and you should receive a Case email from Microsoft Support and will be contacted in near future by support engineer.<\/p>\n<p>If the above was helpful, please click <strong>Accept Answer<\/strong>.<\/p>\n<p>Thanks.<\/p>\n<p>-TP<\/p>\n",
        "Answer_comment_count":1.0,
        "Question_closed_time":1678897172116,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1189971\/why-am-i-getting-unexplained-charges-on-my-microso",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"get unexplain charg microsoft azur free account hope open azur free account januari azur portal azur fundament azur fundament dai notic access fundament portal cost incur free account sure cost come check invoic give clear explan cost come label microsoft servic provid clariti get charg thank",
        "Question_preprocessed_content":"unexplain charg microsoft azur hope open azur januari azur portal azur fundament azur fundament dai notic fundament portal cost sure cost come check invoic give clear explan cost come label microsoft servic provid clariti charg thank",
        "Question_gpt_summary_original":"The user opened a free Microsoft Azure account and used it for Azure Fundamentals and Azure AI Fundamentals. However, they noticed unexplained charges on their account labeled as \"other Microsoft services\" and are seeking clarity on where these charges are coming from.",
        "Question_gpt_summary":"user open free microsoft azur account azur fundament azur fundament notic unexplain charg account label microsoft servic seek clariti charg come"
    },
    {
        "Question_title":"Inaccurate column separation on string data containing commas in CSV file in Azure Machine Learning Studio",
        "Question_body":"<p>I'm using first 100 of lines from <a href=\"https:\/\/www.kaggle.com\/wcukierski\/enron-email-dataset\/home\" rel=\"nofollow noreferrer\">The Enron Email Dataset<\/a> for my experiment in Azure ML Studio, however the Saved Dataset object is being populated with odd 4.8K lines instead of 100. That must be due to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/convert-to-csv#inaccurate-column-separation-on-string-data-containing-commas\" rel=\"nofollow noreferrer\">\"Inaccurate column separation on string data containing commas\"<\/a> issue, which I understand. <\/p>\n\n<p>However, using same dataset in the Python project locally and\/or in Azure ML Jupyter notebook (same imported dataset from ML Studio - not separately imported to Jupyter notebook) the number of lines is being read correctly and the further logic does also work fine. <\/p>\n\n<p>Jupyter example:<\/p>\n\n<pre><code>from azureml import Workspace\nws = Workspace()\nds = ws.datasets['The Enron Email Dataset (Minimal)']\nemails_df = ds.to_dataframe()\n<\/code><\/pre>\n\n<p>Local example: <\/p>\n\n<pre><code>import pandas as pd\nemails_df = pd.read_csv('C:\/enron-email-dataset\/emails.csv', nrows=100)\n<\/code><\/pre>\n\n<p>And here is how dataset visualisation looks like in Azure ML Studio \n<a href=\"https:\/\/i.stack.imgur.com\/zBBZY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zBBZY.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>It's clear that it get's messed up after it gets moved from saved datasets to an experiment, but my question is - what would be the best way to work around it? Calling dataset from Azure BLOB Storage inside my Python code perhaps? <\/p>\n\n<p><strong>EDIT 1:<\/strong> Removing commas from CSV file didn't helped either. The dataset is working fine in Jupyter and locally, but it appears messed up when added to the experiment. <\/p>\n\n<p><strong>EDIT 2:<\/strong> Removing carriage returns resulted in dataset being visualised correctly in the experiment, however that broke the further text pre-processing logic in Python code (<a href=\"https:\/\/docs.python.org\/3\/library\/email.parser.html\" rel=\"nofollow noreferrer\">email.parser<\/a>). Which I could amend making it to work across different environments. That's probably as good as it can get.\n<a href=\"https:\/\/i.stack.imgur.com\/mDMq8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mDMq8.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1543931016747,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1544089884492,
        "Question_score_count":3.0,
        "Question_view_count":307.0,
        "Poster_created_time":1323736477696,
        "Poster_location":"Nottingham, United Kingdom",
        "Poster_reputation_count":277.0,
        "Poster_view_count":58.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53614301",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"inaccur column separ string data contain comma csv file studio line enron email dataset experi studio save dataset object popul odd line instead inaccur column separ string data contain comma issu understand dataset python project local jupyt notebook import dataset studio separ import jupyt notebook number line read correctli logic work fine jupyt exampl import workspac workspac dataset enron email dataset minim email datafram local exampl import panda email read csv enron email dataset email csv nrow dataset visualis look like studio clear mess get move save dataset experi question best wai work call dataset azur blob storag insid python code edit remov comma csv file help dataset work fine jupyt local appear mess ad experi edit remov carriag return result dataset visualis correctli experi broke text pre process logic python code email parser amend make work differ environ probabl good",
        "Question_preprocessed_content":"column separ string data contain csv file studio line enron email dataset experi studio save dataset object popul line instead column separ string data contain understand dataset python project jupyt number line read logic work fine jupyt exampl local exampl dataset visualis like studio clear get move save dataset experi question best wai work dataset azur blob storag insid python code edit remov csv file help dataset work fine jupyt experi edit remov return result dataset visualis experi broke text logic python code amend make work environ probabl",
        "Question_gpt_summary_original":"The user is facing an issue with inaccurate column separation on string data containing commas in a CSV file in Azure Machine Learning Studio. The saved dataset object is being populated with 4.8K lines instead of 100. However, the same dataset is working fine in Jupyter and locally. The user is looking for a workaround, such as calling the dataset from Azure BLOB Storage inside their Python code. Removing carriage returns resulted in the dataset being visualized correctly in the experiment, but it broke the further text pre-processing logic in Python code.",
        "Question_gpt_summary":"user face issu inaccur column separ string data contain comma csv file studio save dataset object popul line instead dataset work fine jupyt local user look workaround call dataset azur blob storag insid python code remov carriag return result dataset visual correctli experi broke text pre process logic python code"
    },
    {
        "Question_title":"AzureMLCompute job failed with `FailedLoginToImageRegistry`",
        "Question_body":"<p>I've been trying to send a train job through azure ml python sdk with:<\/p>\n<pre><code>from azureml.core import Workspace, Experiment, ScriptRunConfig \n\nif __name__ == &quot;__main__&quot;:\n    ws = Workspace.from_config()\n    experiment = Experiment(workspace=ws, name='ConstructionTopicsModel')\n\n    config = ScriptRunConfig(source_directory='.\/',\n                         script='src\/azureml\/train.py',\n                         arguments=None,\n                         compute_target='ComputeTargetName',\n                         )\n\n    env = ws.environments['test-env']\n    config.run_config.environment = env\n    run = experiment.submit(config)\n    \n    run.wait_for_completion(show_output=True)\n\n    aml_url = run.get_portal_url()\n    print(aml_url)\n<\/code><\/pre>\n<p>But I was getting the <code>ServiceError<\/code> message:<\/p>\n<pre><code>AzureMLCompute job failed. FailedLoginToImageRegistry: Unable to login to docker image repo\nReason: Failed to login to the docker registry\nerror: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nserviceURL: 7ac86b04d6564d36aa80ae2ad090582c.azurecr.io\nReason: WARNING! Using --password via the CLI is insecure. Use --password-stdin. Error saving credentials: error storing credentials - err: exit status 1, out: `Cannot autolaunch D-Bus without X11 $DISPLAY`\n\nInfo: Failed to setup runtime for job execution: Job environment preparation failed on 10.0.0.5 with err exit status 1.\n<\/code><\/pre>\n<p>I also tried using the azure cli without success, same error message<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1643645330913,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":202.0,
        "Poster_created_time":1589293508567,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Answer_body":"<p>The only way I've found so far to make this work, was to run it on a terminal of the compute-target itself. That's how the docker error goes away. Trying to run the experiment from a terminal of a different compute instance raises the exception.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1643645330912,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70929123",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"comput job fail failedlogintoimageregistri try send train job python sdk core import workspac experi scriptrunconfig main workspac config experi experi workspac constructiontopicsmodel config scriptrunconfig sourc directori script src train argument comput target computetargetnam env environ test env config run config environ env run experi submit config run wait complet output true aml url run portal url print aml url get serviceerror messag comput job fail failedlogintoimageregistri unabl login docker imag repo reason fail login docker registri error warn password cli insecur us password stdin error save credenti error store credenti err exit statu autolaunch bu displai serviceurl acbddaaaeadc azurecr reason warn password cli insecur us password stdin error save credenti error store credenti err exit statu autolaunch bu displai info fail setup runtim job execut job environ prepar fail err exit statu tri azur cli success error messag",
        "Question_preprocessed_content":"comput job fail try send train job python sdk tri azur cli",
        "Question_gpt_summary_original":"The user encountered an issue while trying to send a train job through Azure ML Python SDK. The job failed with a \"FailedLoginToImageRegistry\" error message, indicating that the user was unable to login to the docker image repo. The error message also suggested using \"--password-stdin\" instead of \"--password\" via the CLI. The user also tried using the Azure CLI but encountered the same error message.",
        "Question_gpt_summary":"user encount issu try send train job python sdk job fail failedlogintoimageregistri error messag indic user unabl login docker imag repo error messag suggest password stdin instead password cli user tri azur cli encount error messag"
    },
    {
        "Question_title":"mlflow gui - displaying csv artifact.",
        "Question_body":"Hi all,\n First of all great job with mlflow, it is an amazing tool.\nI have a question\/request though, is there a way to display a csv file (i.e. confusion matrix data) saved in mlflow artifacts repo as a table or graph? \nIf not,  it would be nice if we could add some plugin into ML Flow to display CSV data in a table view.  \n\nCheers",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1544526691000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":23.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/OgApYabrvPE",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"gui displai csv artifact great job amaz tool question request wai displai csv file confus matrix data save artifact repo tabl graph nice add plugin flow displai csv data tabl view cheer",
        "Question_preprocessed_content":"gui displai csv artifact great job amaz wai displai csv file save artifact repo tabl graph nice plugin flow displai csv data tabl view",
        "Question_gpt_summary_original":"The user is requesting a way to display a CSV file saved in mlflow artifacts repo as a table or graph and suggests adding a plugin to ML Flow to display CSV data in a table view.",
        "Question_gpt_summary":"user request wai displai csv file save artifact repo tabl graph suggest ad plugin flow displai csv data tabl view"
    },
    {
        "Question_title":"Library is not installed on PATH - How can I install on path?",
        "Question_body":"<p>I am running this notebook in my managed notebooks environment on Google Cloud and I'm getting the following error when trying to install the packages: &quot;WARNING: The script google-oauthlib-tool is installed in '\/home\/jupyter\/.local\/bin' which is not on PATH.\nConsider adding this directory to PATH.&quot;<\/p>\n<p>Here is the python code that I'm trying to run for reference. <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/main\/notebooks\/official\/model_monitoring\/model_monitoring.ipynb<\/a><\/p>\n<p>Any suggestions on how I can update the package installation so it is on path and resolve the error? I'm currently working on GCP user-managed notebooks on a Mac.<\/p>\n<p>Thanks so much for any tips!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1658518447040,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":52.0,
        "Poster_created_time":1621620820567,
        "Poster_location":null,
        "Poster_reputation_count":29.0,
        "Poster_view_count":16.0,
        "Answer_body":"<p>Open up your shell config file (likely .zshrc because the default shell on Mac is now zsh and that's the name of the zsh config file) located at your home directory in a text editor (TextEdit, etc) and add the path to the  executable.\nLike this:\nOpen the file:\n<code>open -e ~\/.zshrc<\/code>\nEdit the file:\nAdd this line at the top (may vary, check the documentation):\n<code>export PATH=&quot;\/home\/jupyter\/.local\/bin&quot;<\/code>\nThat may not work, try this:\n<code>export PATH=&quot;$PATH:\/home\/jupyter\/.local\/bin&quot;<\/code>\nYour best bet is to read the package documentation.<\/p>\n<p>After saving the config file, run <code>source ~\/.zshrc<\/code> and replace .zshrc with the config file name if it's different OR open a new terminal tab.<\/p>\n<p>What this does is tells the shell that the command exists and where to find it.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1658596861407,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1622117284230,
        "Answerer_location":null,
        "Answerer_reputation_count":316.0,
        "Answerer_view_count":40.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73085293",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"librari instal path instal path run notebook manag notebook environ googl cloud get follow error try instal packag warn script googl oauthlib tool instal home jupyt local bin path consid ad directori path python code try run refer http github com googlecloudplatform vertex sampl blob main notebook offici model monitor model monitor ipynb suggest updat packag instal path resolv error current work gcp user manag notebook mac thank tip",
        "Question_preprocessed_content":"librari path path manag environ cloud try packag warn script path consid directori python code try run refer updat packag path resolv work gcp mac thank tip",
        "Question_gpt_summary_original":"The user is encountering an error while trying to install packages in their managed notebooks environment on Google Cloud. The error message suggests that the library is not installed on PATH and recommends adding the directory to PATH. The user is seeking suggestions on how to update the package installation to resolve the error.",
        "Question_gpt_summary":"user encount error try instal packag manag notebook environ googl cloud error messag suggest librari instal path recommend ad directori path user seek suggest updat packag instal resolv error"
    },
    {
        "Question_title":"Unable To Run AzureML Experiment with SDK - Failed to Build Wheel for pynacl \/ Exit status:1",
        "Question_body":"<p>I am trying to run a AzureML Experiment using sdk (following a Udemy course). When I try to use the Experiment.submit function the experiment prepares and then fails with the following error messages:<\/p>\n<pre><code>ERROR: Command errored out with exit status 1 \n\nERROR: Failed building wheel for pynacl\nERROR: Could not build wheels for pynacl which use PEP 517 and cannot be installed directly\n<\/code><\/pre>\n<p>The Azure env as created within my anaconda navigator for a short period of time and then gets removed.<\/p>\n<p>Does anyone know how I can get around this? Any help would be really appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1649943158360,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1650357451940,
        "Question_score_count":0.0,
        "Question_view_count":73.0,
        "Poster_created_time":1603536549848,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":7.0,
        "Answer_body":"<p>To resolve <code>ERROR: Could not build wheels for pynacl which use PEP 517 and cannot be installed directly<\/code> this error, try either of the following ways:<\/p>\n<ol>\n<li><p>Install missing dependencies:<\/p>\n<pre><code>sudo apt install libpython3-dev build-essential\n<\/code><\/pre>\n<\/li>\n<li><p>Upgrade pip:<\/p>\n<pre><code>pip3 install --upgrade pip\n<\/code><\/pre>\n<\/li>\n<li><p>Upgrade pip with setuptools wheel:<\/p>\n<pre><code>pip3 install --upgrade pip setuptools wheel\n<\/code><\/pre>\n<\/li>\n<li><p>Reinstall PEP517:<\/p>\n<pre><code>pip3 install p5py\npip3 install PEP517\n<\/code><\/pre>\n<\/li>\n<\/ol>\n<p>You can refer to  <a href=\"https:\/\/stackoverflow.com\/questions\/61365790\/error-could-not-build-wheels-for-scipy-which-use-pep-517-and-cannot-be-installe\">ERROR: Could not build wheels for scipy which use PEP 517 and cannot be installed directly<\/a>, <a href=\"https:\/\/stackoverflow.com\/questions\/64038673\/could-not-build-wheels-for-which-use-pep-517-and-cannot-be-installed-directly\">Could not build wheels for _ which use PEP 517 and cannot be installed directly - Easy Solution<\/a> and <a href=\"https:\/\/github.com\/martomi\/chiadog\/issues\/44\" rel=\"nofollow noreferrer\">failed building wheel for pynacl<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1650257274672,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1517548787092,
        "Answerer_location":null,
        "Answerer_reputation_count":1925.0,
        "Answerer_view_count":3530.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71872506",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"unabl run experi sdk fail build wheel pynacl exit statu try run experi sdk follow udemi cours try us experi submit function experi prepar fail follow error messag error command error exit statu error fail build wheel pynacl error build wheel pynacl us pep instal directli azur env creat anaconda navig short period time get remov know help appreci",
        "Question_preprocessed_content":"unabl run experi sdk fail build pynacl exit statu try run experi sdk try us function experi prepar fail azur env creat anaconda navig short period time get remov know help",
        "Question_gpt_summary_original":"The user is unable to run an AzureML experiment using SDK and is encountering an error message \"Failed building wheel for pynacl\". The Azure environment created within Anaconda Navigator is getting removed after a short period of time. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user unabl run experi sdk encount error messag fail build wheel pynacl azur environ creat anaconda navig get remov short period time user seek help resolv issu"
    },
    {
        "Question_title":"Early Stopping and Callbacks with Keras when using SageMaker",
        "Question_body":"<p>I am using sagemaker to train a keras model. I need to implement early stoping approach when training the model. <\/p>\n\n<p>Is there a way to pass callbacks such as EarlyStopping, Histories..etc. <\/p>\n\n<p>In traditional way, we used to pass this as a parameter to keras's fit function:<\/p>\n\n<pre><code>results = model.fit(train_x_trim, train_y_trim, \n                    validation_data=(test_x, test_y), \n                    epochs=FLAGS.epoch,  \n                    verbose=0, \n                    callbacks=[tboard, checkpointer, early_stopping, history])\n<\/code><\/pre>\n\n<p>However, if using SageMaker, we need to call SageMaker's fit function instead which doesn't support callbacks. <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow \niris_estimator = TensorFlow(entry_point='training_code.py', \n                            role=role, output_path=model_location, \n                            code_location=custom_code_upload_location, \n                            train_instance_count=1, \n                            train_instance_type='ml.c4.xlarge', \n                            training_steps=1000, \n                            evaluation_steps=100)\n<\/code><\/pre>\n\n<p>Any idea how to implement callbacks in SageMaker ? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1543252902240,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1543256081430,
        "Question_score_count":3.0,
        "Question_view_count":557.0,
        "Poster_created_time":1485886879543,
        "Poster_location":"Canada",
        "Poster_reputation_count":71.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53486118",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"earli stop callback kera train kera model need implement earli stope approach train model wai pass callback earlystop histori tradit wai pass paramet kera fit function result model fit train trim train trim valid data test test epoch flag epoch verbos callback tboard checkpoint earli stop histori need fit function instead support callback tensorflow import tensorflow iri estim tensorflow entri point train code role role output path model locat code locat custom code upload locat train instanc count train instanc type xlarg train step evalu step idea implement callback",
        "Question_preprocessed_content":"earli kera train kera model implement earli stope train model wai tradit wai paramet kera fit function fit function instead idea implement",
        "Question_gpt_summary_original":"The user is facing a challenge while using SageMaker to train a Keras model as SageMaker's fit function does not support callbacks such as EarlyStopping and Histories. The user is seeking a solution to implement callbacks in SageMaker.",
        "Question_gpt_summary":"user face challeng train kera model fit function support callback earlystop histori user seek solut implement callback"
    },
    {
        "Question_title":"Error when loading ML model from the remote MLflow instance",
        "Question_body":"<p>I tried to load a model from the remote MLflow instance, using <code>load_model<\/code> function:<\/p>\n<pre><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;http:\/\/remote_IP_address:5000\/runs:\/&lt;run_id&gt;\/model&quot;)\n<\/code><\/pre>\n<p>I found the run_id by using the REST API:<\/p>\n<pre><code>import requests\n\nrequests.get(&quot;http:\/\/remote_IP_address:5000\/api\/2.0\/preview\/mlflow\/runs\/search&quot;,params={&quot;experiment_ids&quot;:[0,1]})\n<\/code><\/pre>\n<p>But I am receiving an error:<\/p>\n<pre><code>ValueError: not enough values to unpack (expected 2, got 1)\n<\/code><\/pre>\n<p>I suppose the error is in the URI that I am using. Can you tell me the correct way to access the remote Mlflow instance and load the model?<\/p>\n<p>p.s.\nI also tried:<\/p>\n<pre><code>mlflow.pyfunc.load_model(&quot;http:\/\/remote_Ip_address:5000\/models:\/&lt;model_name&gt;\/production&quot;)\n<\/code><\/pre>\n<p>but I received the same error.<\/p>\n<p>Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1649414682147,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1649415164790,
        "Question_score_count":0.0,
        "Question_view_count":286.0,
        "Poster_created_time":1528365488027,
        "Poster_location":null,
        "Poster_reputation_count":499.0,
        "Poster_view_count":59.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71795643",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error load model remot instanc tri load model remot instanc load model function import model pyfunc load model http remot address run model run rest api import request request http remot address api preview run search param experi id receiv error valueerror valu unpack expect got suppos error uri tell correct wai access remot instanc load model tri pyfunc load model http remot address model product receiv error thank advanc",
        "Question_preprocessed_content":"load model remot instanc tri load model remot instanc function rest api receiv uri wai remot instanc load model tri receiv thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error while trying to load a machine learning model from a remote MLflow instance using the 'load_model' function. The user has tried to find the 'run_id' using the REST API but is receiving a 'ValueError' stating that there are not enough values to unpack. The user suspects that the error is in the URI used to access the remote MLflow instance and is seeking guidance on the correct way to load the model.",
        "Question_gpt_summary":"user encount error try load machin learn model remot instanc load model function user tri run rest api receiv valueerror state valu unpack user suspect error uri access remot instanc seek guidanc correct wai load model"
    },
    {
        "Question_title":"Missing delimiter error when importing html text",
        "Question_body":"<p>Playing with Azure Machine Learning using the Designer and am getting a &quot;Delimiter not found&quot; error when importing my data.<\/p>\n<p>I originally started with a few hundred html files stored as azure blobs. Each file would be considered a single row of text, however, I had no luck importing these files for further text analytics.<\/p>\n<p>I created a Data Factory job that imported each file, stripped all the tabs, quotes, cr\/lf from the text, added a column for the file name and stored it all as a combined tab-delimited file. In notepad++ I can confirm that the format is <code>FileName tab HtmlText<\/code>. This is the file I'm trying to import into ML and getting the missing delimiter message as I'm trying to define the import module.<\/p>\n<p>Here is the error when I try and create a dataset:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>{\n  &quot;message&quot;: &quot;'Delimiter' is not specified or invalid.&quot;\n}\n<\/code><\/pre>\n<p>Question 1: Is there a better way to do text analytics on a large collection of html files?<\/p>\n<p>Question 2: Is there a format I need to use in my combined .tsv file that works?<\/p>\n<p>Question 3: Is there maybe a max length to the string column? My html can be 10's of thousands of characters long.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1598396437553,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1598498523072,
        "Question_score_count":0.0,
        "Question_view_count":209.0,
        "Poster_created_time":1221591235460,
        "Poster_location":null,
        "Poster_reputation_count":936.0,
        "Poster_view_count":105.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63588251",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"miss delimit error import html text plai design get delimit error import data origin start html file store azur blob file consid singl row text luck import file text analyt creat data factori job import file strip tab quot text ad column file store combin tab delimit file notepad confirm format filenam tab htmltext file try import get miss delimit messag try defin import modul error try creat dataset messag delimit specifi invalid question better wai text analyt larg collect html file question format need us combin tsv file work question mayb max length string column html thousand charact long",
        "Question_preprocessed_content":"delimit import html text plai design delimit import data start html file store azur blob file consid singl row text luck import file text analyt creat data factori job import file tab quot text column file store combin file notepad confirm format file try import delimit try defin import modul try creat dataset question wai text analyt larg html file question format us combin tsv file work question mayb max length string column html thousand charact long",
        "Question_gpt_summary_original":"The user is encountering a \"Delimiter not found\" error when trying to import a combined tab-delimited file containing html text into Azure Machine Learning. They had previously imported hundreds of html files stored as azure blobs, but were unable to use them for text analytics. The user is seeking advice on a better way to perform text analytics on a large collection of html files, the correct format for the combined .tsv file, and whether there is a maximum length for the string column.",
        "Question_gpt_summary":"user encount delimit error try import combin tab delimit file contain html text previous import hundr html file store azur blob unabl us text analyt user seek advic better wai perform text analyt larg collect html file correct format combin tsv file maximum length string column"
    },
    {
        "Question_title":"install java dependency when building my own processing container",
        "Question_body":"A python package I'm using requires java dependency. I put the following lines in my docker file when building processing container, and got apt-add-repository: command not found error. What is the correct commands to put in a dockerfile to install java? \n\nRUN add-apt-repository ppa:openjdk-r\/ppa && \\\n    apt-get update && \\\n    apt-get install -y openjdk-7-jdk && \\\n    apt-get install -y ant && \\\n    apt-get clean;\n    \n\nRUN apt-get update && \\\n    apt-get install ca-certificates-java && \\\n    apt-get clean && \\\n    update-ca-certificates -f;\n\nENV JAVA_HOME \/usr\/lib\/jvm\/java-8-openjdk-amd64\/\nRUN export JAVA_HOME",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1684544519306,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1684891019388,
        "Question_score_count":0.0,
        "Question_view_count":52.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjfdp5BKmTvSdXirILemjoQ\/install-java-dependency-when-building-my-own-processing-container",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"instal java depend build process contain python packag requir java depend follow line docker file build process contain got apt add repositori command error correct command dockerfil instal java run add apt repositori ppa openjdk ppa apt updat apt instal openjdk jdk apt instal ant apt clean run apt updat apt instal certif java apt clean updat certif env java home usr lib jvm java openjdk amd run export java home",
        "Question_preprocessed_content":"java depend build contain python packag requir java depend line docker file build contain got dockerfil java run updat ant clean run updat clean env run export",
        "Question_gpt_summary_original":"The user is facing an issue while building a processing container as a python package requires a java dependency. The user is trying to install java using a dockerfile but is encountering an error \"apt-add-repository: command not found\". The user is seeking help to know the correct commands to install java in a dockerfile.",
        "Question_gpt_summary":"user face issu build process contain python packag requir java depend user try instal java dockerfil encount error apt add repositori command user seek help know correct command instal java dockerfil"
    },
    {
        "Question_title":"Load .asc file into azure machine learning",
        "Question_body":"<p>For my Azure Machine Learning experiment I want to load a .asc file into an Execute R script in my experiment. It is in fact a tab delimited file with some comments on the first couple of rows. Can anyone tell me how to do this?<\/p>\n\n<p>A csv goes well, but with this file I get an error.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_created_time":1457552318890,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1457561625260,
        "Question_score_count":0.0,
        "Question_view_count":69.0,
        "Poster_created_time":1387445364536,
        "Poster_location":"Netherlands",
        "Poster_reputation_count":81.0,
        "Poster_view_count":9.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35901044",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load asc file experi want load asc file execut script experi fact tab delimit file comment coupl row tell csv goe file error",
        "Question_preprocessed_content":"load asc file experi want load asc file execut script experi fact tab delimit file coupl row csv goe file",
        "Question_gpt_summary_original":"The user is facing challenges in loading a .asc file into an Execute R script in their Azure Machine Learning experiment. The file is tab-delimited with comments on the first few rows, and the user is seeking guidance on how to accomplish this task. They have tried using a csv file but encountered an error.",
        "Question_gpt_summary":"user face challeng load asc file execut script experi file tab delimit comment row user seek guidanc accomplish task tri csv file encount error"
    },
    {
        "Question_title":"OCR in Sage Maker",
        "Question_body":"<p>Guys i am trying to build infrastructure on aws for getting help from others on annotation. currently we uses label-studio for text annotation. as might know you can label text by selecting through polygon and than writing what does selected area mean. ex: if polygon is made around english word than what writing out label  of it to annotate that given english word. for more see image below.<a href=\"https:\/\/i.stack.imgur.com\/8jvaj.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8jvaj.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>How can i do this in <strong>SageMaker Ground Truth<\/strong>. as far as i have gone i think it can just label pre defined words. you cant create custom label in it by selecting any given area using polygon in image am i right ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1644428834663,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":67.0,
        "Poster_created_time":1482044943532,
        "Poster_location":"Gurgaon, Haryana, India",
        "Poster_reputation_count":549.0,
        "Poster_view_count":50.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71054340",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"ocr sage maker gui try build infrastructur aw get help annot current us label studio text annot know label text select polygon write select area mean polygon english word write label annot given english word imag ground truth far gone think label pre defin word creat custom label select given area polygon imag right",
        "Question_preprocessed_content":"ocr sage maker gui try build infrastructur aw help us text know label text select polygon write select area mean polygon english word write label given english word imag ground truth far gone think label pre defin word creat custom label select given area polygon imag right",
        "Question_gpt_summary_original":"The user is trying to build infrastructure on AWS for text annotation using SageMaker Ground Truth. They are currently using Label-Studio for text annotation, which allows them to label text by selecting through a polygon and writing the label for the selected area. The user is wondering if they can do the same in SageMaker Ground Truth, but they believe it can only label pre-defined words and cannot create custom labels by selecting any given area using a polygon in an image.",
        "Question_gpt_summary":"user try build infrastructur aw text annot ground truth current label studio text annot allow label text select polygon write label select area user wonder ground truth believ label pre defin word creat custom label select given area polygon imag"
    },
    {
        "Question_title":"Custom sagemaker container for training, write forecast to AWS RDS, on a daily basis",
        "Question_body":"<p>I have 3 main process to perform using Amazon SageMaker.<\/p>\n\n<ol>\n<li>Using own training python script, (not using sagemaker container, inbuilt algorithm) [Train.py]<\/li>\n<\/ol>\n\n<p>-> For this, I have referred to this link:<br>\n<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/train-and-host-scikit-learn-models-in-amazon-sagemaker-by-building-a-scikit-docker-container\/\" rel=\"nofollow noreferrer\">Bring own algorithm to AWS sagemaker<\/a>\nand it seems that we can bring our own training script to sagemaker managed training setup, and model artifacts can be uploaded to s3 etc.\nNote: I am using Light GBM model for training.<\/p>\n\n<ol start=\"2\">\n<li>Writing forecast to AWS RDS DB:<\/li>\n<\/ol>\n\n<p>-> There is no need to deploy model and create endpoint, because training will happen everyday, and will create forecast as soon as training completes. (Need to generate forecast in train.py itself)<\/p>\n\n<p>-> <strong>Challenge is how can I write forecast in AWS RDS DB from train.py script. (Given that script is running in Private VPC)<\/strong><\/p>\n\n<ol start=\"3\">\n<li>Scheduling this process as daily job:<\/li>\n<\/ol>\n\n<p>--> I have gone through AWS step functions and seems to be the way to trigger daily training and write forecast to RDS.<\/p>\n\n<p>--> <strong>Challenge is how to use step function for time based trigger and not event based.<\/strong><\/p>\n\n<p>Any suggestions on how to do this? Any best practices to follow? Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1566547602673,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1566550066240,
        "Question_score_count":0.0,
        "Question_view_count":466.0,
        "Poster_created_time":1469183608840,
        "Poster_location":"Gurugram, Haryana, India",
        "Poster_reputation_count":624.0,
        "Poster_view_count":67.0,
        "Answer_body":"<p>The way to trigger Step Functions on schedule is by using CloudWatch Events (sort of cron). Check out this tutorial: <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/tutorial-cloudwatch-events-target.html<\/a><\/p>\n\n<p>Don't write to the RDS from your Python code! It is better to write the output to S3 and then \"copy\" the files from S3 into the RDS. Decoupling these batches will make a more reliable and scalable process. You can trigger the bulk copy into the RDS when the files are written to S3 or to a later time when your DB is not too busy. <\/p>",
        "Answer_comment_count":5.0,
        "Question_closed_time":1566713256896,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1253986272627,
        "Answerer_location":null,
        "Answerer_reputation_count":11930.0,
        "Answerer_view_count":2649.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57622122",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"custom contain train write forecast aw rd daili basi main process perform train python script contain inbuilt algorithm train refer link bring algorithm bring train script manag train setup model artifact upload note light gbm model train write forecast aw rd need deploi model creat endpoint train happen everydai creat forecast soon train complet need gener forecast train challeng write forecast aw rd train script given script run privat vpc schedul process daili job gone aw step function wai trigger daili train write forecast rd challeng us step function time base trigger event base suggest best practic follow thank advanc",
        "Question_preprocessed_content":"custom contain train write forecast aw rd daili basi main perform train python script link bring algorithm bring train script manag train setup model artifact upload note light gbm model train write forecast aw rd deploi model creat endpoint train everydai creat forecast train complet write forecast aw rd script schedul daili job gone aw step function wai daili train write forecast rd us step function time base event base best practic thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in performing three main processes using Amazon SageMaker. The first process involves using their own training python script, not using the sagemaker container, and uploading model artifacts to S3. The second process involves writing forecast to AWS RDS DB from the train.py script running in a private VPC. The third process involves scheduling this process as a daily job using AWS step functions for time-based trigger instead of event-based. The user is seeking suggestions and best practices to overcome these challenges.",
        "Question_gpt_summary":"user face challeng perform main process process involv train python script contain upload model artifact second process involv write forecast aw rd train script run privat vpc process involv schedul process daili job aw step function time base trigger instead event base user seek suggest best practic overcom challeng"
    },
    {
        "Question_title":"Conditional sweep config",
        "Question_body":"<p>I guess the answer to this is \u201cnot supported\u201d when looking at similar requests, but at least\u2026 let me add a +1 to the feature request. Simplified, presume I have two approaches to training a model, each with its own sub-approaches. Is there a way to define a sweep config in a way that I can do a search over sub-approaches that depend on the main approach, i.e., how to build something like:<\/p>\n<pre><code class=\"lang-bash\">python3 train.py --approach a --sub_approach a1\npython3 train.py --approach a --sub_approach a2\npython3 train.py --approach b --sub_approach b1\npython3 train.py --approach b --sub_approach b2\n<\/code><\/pre>\n<p>I could\u2026<\/p>\n<ul>\n<li>do 2 sweeps for a anb b in that toy example, but I think I cannot combine the results of different sweeps into a single graph, right?<\/li>\n<li>Or I could check in <code>train.py<\/code> and abort the program for invalid combinations; in this case I think I need to tell wandb to continue with the sweep since there would be a number of \u201ccrashes\u201d<\/li>\n<li>Or use a different hyper param optimizer and integrate into wandb (but would like to avoid that).<\/li>\n<\/ul>\n<p>Any other options? If different hyper param optimizer, any recommendation because of better\/easier integration with wandb? Thanks for your input\/ideas.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1678194773379,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":105.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/conditional-sweep-config\/4017",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"condit sweep config guess answer support look similar request let add featur request simplifi presum approach train model sub approach wai defin sweep config wai search sub approach depend main approach build like python train approach sub approach python train approach sub approach python train approach sub approach python train approach sub approach sweep anb toi exampl think combin result differ sweep singl graph right check train abort program invalid combin case think need tell continu sweep number crash us differ hyper param optim integr like avoid option differ hyper param optim recommend better easier integr thank input idea",
        "Question_preprocessed_content":"condit config answer similar request let featur request simplifi presum train model wai defin config wai search depend main build like anb toi exampl think combin result singl graph right check abort program invalid combin case think continu number crash us hyper param optim integr option hyper param optim easier integr thank",
        "Question_gpt_summary_original":"The user is requesting a feature to define a sweep config in a way that allows for a search over sub-approaches that depend on the main approach. They are considering different options such as doing separate sweeps for each approach or checking in the train.py file for invalid combinations. They are also seeking recommendations for a different hyper param optimizer that integrates better with wandb.",
        "Question_gpt_summary":"user request featur defin sweep config wai allow search sub approach depend main approach consid differ option separ sweep approach check train file invalid combin seek recommend differ hyper param optim integr better"
    },
    {
        "Question_title":"Azure ML Studio: How to change input value with Python before it goes through data process",
        "Question_body":"<p>I am currently attempting to change the value of input as it goes through data process in Azure ML. However, I cannot find a clue about how to access to the input data with python.<\/p>\n\n<p>For example, if you were to use python, you can access to the column of data with<\/p>\n\n<pre><code>print(dataframe1[\"Hello World\"])\n<\/code><\/pre>\n\n<p>I tried to change the name of Web Service Input and tried to do it like how I did for other dataframe (e.g. sample)<\/p>\n\n<pre><code>print(dataframe[\"sample\"])\n<\/code><\/pre>\n\n<p>But it returns an error with no luck, and from what I read from an error, it's not compatible to dataframe:<\/p>\n\n<pre><code>object of type 'NoneType' has no len()\n<\/code><\/pre>\n\n<p>I tried to look up a solution with Nonetype error, but there is no good solution.\nThe whole error message:<\/p>\n\n<pre><code>requestId = 1f0f621f1d8841baa7862d5c05154942 errorComponent=Module. taskStatusCode=400. {\"Exception\":{\"ErrorId\":\"FailedToEvaluateScript\",\"ErrorCode\":\"0085\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 0085: The following error occurred during script evaluation, please view the output log for more information:\\r\\n---------- Start of error message from Python interpreter ----------\\r\\nCaught exception while executing function: Traceback (most recent call last):\\r\\n File \\\"C:\\\\server\\\\invokepy.py\\\", line 211, in batch\\r\\n xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True)\\r\\n File \\\"C:\\\\server\\\\XDRReader\\\\xdrutils.py\\\", line 51, in DataFrameToRFile\\r\\n attributes = XDRBridge.DataFrameToRObject(dataframe)\\r\\n File \\\"C:\\\\server\\\\XDRReader\\\\xdrbridge.py\\\", line 40, in DataFrameToRObject\\r\\n if (len(dataframe) == 1 and type(dataframe[0]) is pd.DataFrame):\\r\\nTypeError: object of type 'NoneType' has no len()\\r\\nProcess returned with non-zero exit code 1\\r\\n\\r\\n---------- End of error message from Python interpreter ----------\"}}Error: Error 0085: The following error occurred during script evaluation, please view the output log for more information:---------- Start of error message from Python interpreter ----------Caught exception while executing function: Traceback (most recent call last): File \"C:\\server\\invokepy.py\", line 211, in batch xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True) File \"C:\\server\\XDRReader\\xdrutils.py\", line 51, in DataFrameToRFile attributes = XDRBridge.DataFrameToRObject(dataframe) File \"C:\\server\\XDRReader\\xdrbridge.py\", line 40, in DataFrameToRObject if (len(dataframe) == 1 and type(dataframe[0]) is pd.DataFrame):TypeError: object of type 'NoneType' has no len()Process returned with non-zero exit code 1---------- End of error message from Python interpreter ---------- Process exited with error code -2\n<\/code><\/pre>\n\n<p>I have also tried to <a href=\"https:\/\/i.stack.imgur.com\/DWZK6.png\" rel=\"nofollow noreferrer\">a way to pass python script in data<\/a>, but it is not able to make any change to Web Service Input value as I want it to be.<\/p>\n\n<p>I have tried to look on forums like msdn or SO, but it's been difficult to find any information about it. Please let me know if you need any more information if needed. I would greatly appreciate your help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1533163130393,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":309.0,
        "Poster_created_time":1508281047960,
        "Poster_location":null,
        "Poster_reputation_count":25.0,
        "Poster_view_count":22.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51643168",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"studio chang input valu python goe data process current attempt chang valu input goe data process clue access input data python exampl us python access column data print datafram hello world tri chang web servic input tri like datafram sampl print datafram sampl return error luck read error compat datafram object type nonetyp len tri look solut nonetyp error good solut error messag requestid fffdbaadc errorcompon modul taskstatuscod except errorid failedtoevaluatescript errorcod exceptiontyp moduleexcept messag error follow error occur script evalu view output log inform start error messag python interpret ncaught except execut function traceback recent file server invokepi line batch xdrutil xdrutil dataframetorfil outlist outfil true file server xdrreader xdrutil line dataframetorfil attribut xdrbridg dataframetorobject datafram file server xdrreader xdrbridg line dataframetorobject len datafram type datafram datafram ntypeerror object type nonetyp len nprocess return non zero exit code end error messag python interpret error error follow error occur script evalu view output log inform start error messag python interpret caught except execut function traceback recent file server invokepi line batch xdrutil xdrutil dataframetorfil outlist outfil true file server xdrreader xdrutil line dataframetorfil attribut xdrbridg dataframetorobject datafram file server xdrreader xdrbridg line dataframetorobject len datafram type datafram datafram typeerror object type nonetyp len process return non zero exit code end error messag python interpret process exit error code tri wai pass python script data abl chang web servic input valu want tri look forum like msdn difficult inform let know need inform need greatli appreci help",
        "Question_preprocessed_content":"studio chang input valu python goe data chang valu input goe data clue input data python exampl us python column data tri chang web servic input tri like datafram return luck read compat datafram tri solut nonetyp solut tri wai python script data abl chang web servic input valu want tri forum like msdn inform let know inform greatli help",
        "Question_gpt_summary_original":"The user is trying to change the value of input data as it goes through data processing in Azure ML Studio using Python, but is unable to access the input data with Python. The user has tried to change the name of the Web Service Input and has encountered a 'NoneType' error. The user has also tried to pass a Python script in data, but it did not make any changes to the Web Service Input value. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user try chang valu input data goe data process studio python unabl access input data python user tri chang web servic input encount nonetyp error user tri pass python script data chang web servic input valu user seek help resolv issu"
    },
    {
        "Question_title":"Issues with prediction time not proportional w.r.t. number of trees in RF",
        "Question_body":"<p>My estimation strategy consists on using a set of Random Forest models, each one concerns some\nsubset of data (ex : RF_A if feature == A). This has been said seek of completeness as I dont think this affects my issue.<\/p>\n\n<p>My deployment strategy:<\/p>\n\n<ul>\n<li>Fit: return a pickle that contains a dictionary of fitted sklearn Random\nForest models<\/li>\n<li>Deploy: load these dictionaries in memory.<\/li>\n<li>Inference:\n1) maps each observation to the correct model in the already loaded dictionary\n2) for each observation, computes predictions given by each tree in order to allow for elementary confidence interval computation\n<a href=\"http:\/\/blog.datadive.net\/prediction-intervals-for-random-forests\/\" rel=\"nofollow noreferrer\">http:\/\/blog.datadive.net\/prediction-intervals-for-random-forests\/<\/a>\nNote that this last operation is the most time consuming in the inference and the time is proportional to the number of trees in my RF (loop w.r.t. trees).<\/li>\n<\/ul>\n\n<p>My code (my custom code in lib) :<\/p>\n\n<pre><code>import argparse\nimport os\nimport sys\nimport pandas as pd\nfrom sklearn.externals import joblib\nmodule_path = os.path.abspath('\/opt\/ml\/code')\nif module_path not in sys.path:\n    sys.path.append(module_path)\nfrom lib import training, prediction\nfrom data.transactions import raw\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    args = parser.parse_args()\n    grid_models_dict =\\\n        training.train_models_in_dict(raw_training_data=raw)\n    joblib.dump(grid_models_dict, os.path.join(args.model_dir, \"model\"))\ndef model_fn(model_dir):\n    grid_models_dict = joblib.load(os.path.join(model_dir, \"model\"))\n    return grid_models_dict\ndef predict_fn(input_data, model):\n    predicted = prediction.predict(input_data, model)\n    return predicted\n<\/code><\/pre>\n\n<p>My problem :<\/p>\n\n<p>Say I have two deployments scenarios : one with 100 trees\/RF and one with 300 trees\/RF.<\/p>\n\n<p>Fit is performed without issues. On S3 : compressed 100 trees\/RF pickle is 261 Mo and compressed 300 trees\/RF is 784 Mo.<\/p>\n\n<p>Deploy is done with some issues : some timeout with some workers with the 300 trees\/RF already reported for example awslabs\/amazon-sagemaker-examples#556, but it deploy at the end.<\/p>\n\n<p>Prediction is performed :<\/p>\n\n<ul>\n<li>with the 100 trees\/RF in around 500 ms, always, with the same observation<\/li>\n<li><p>with the 300 trees\/RF: in paper, with the same observation, due my prediction nature which is a for loop w.r.t. trees, I am supposed to predict in less than 1 second.<\/p><\/li>\n<li><p>with the 300 trees\/RF, in practice, with the same observation :\n1) sometimes (33% of cases) in 700 ms,\n2) sometimes (33% of cases) in 40 to 50 seconds,\n3) and sometimes (33% of cases) I have a timeout error (inference timeout is limited to 60 seconds)<\/p><\/li>\n<\/ul>\n\n<p>This behavior remains when I deploy in a bigger\/recent machine. (ml.t2.xlarge to ml.c5.4xlarge)<\/p>\n\n<p>My guess is that there is a memory swapping mechanism or that the container's memory is not fully privately allocated to me after some threshold.<\/p>\n\n<p>Is there any solution to predict consistently with more than 100trees\/RF ?<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Issue also reported here :\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/681\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/681<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1551707140937,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1551779654840,
        "Question_score_count":1.0,
        "Question_view_count":64.0,
        "Poster_created_time":1486387674052,
        "Poster_location":null,
        "Poster_reputation_count":190.0,
        "Poster_view_count":13.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54984594",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"issu predict time proport number tree estim strategi consist set random forest model concern subset data featur said seek complet dont think affect issu deploy strategi fit return pickl contain dictionari fit sklearn random forest model deploi load dictionari memori infer map observ correct model load dictionari observ comput predict given tree order allow elementari confid interv comput http blog datad net predict interv random forest note oper time consum infer time proport number tree loop tree code custom code lib import argpars import import sy import panda sklearn extern import joblib modul path path abspath opt code modul path sy path sy path append modul path lib import train predict data transact import raw main parser argpars argumentpars parser add argument output data dir type str default environ output data dir parser add argument model dir type str default environ model dir arg parser pars arg grid model dict train train model dict raw train data raw joblib dump grid model dict path join arg model dir model def model model dir grid model dict joblib load path join model dir model return grid model dict def predict input data model predict predict predict input data model return predict problem deploy scenario tree tree fit perform issu compress tree pickl compress tree deploi issu timeout worker tree report exampl awslab amazon exampl deploi end predict perform tree observ tree paper observ predict natur loop tree suppos predict second tree practic observ case case second case timeout error infer timeout limit second behavior remain deploi bigger recent machin xlarg xlarg guess memori swap mechan contain memori fulli privat alloc threshold solut predict consist tree thank advanc issu report http github com aw python sdk issu",
        "Question_preprocessed_content":"predict time proport number estim strategi consist set random forest model concern subset data said dont think deploy strategi fit return pickl contain dictionari sklearn random forest model deploi load dictionari memori infer map observ model load dictionari observ comput predict given order elementari confid interv comput note oper time consum infer time proport number code problem deploy scenario fit perform pickl deploi timeout worker report exampl deploi end predict perform observ paper observ predict natur predict second practic observ second timeout behavior remain deploi recent machin memori mechan contain memori privat threshold solut predict consist thank advanc report",
        "Question_gpt_summary_original":"The user is facing issues with prediction time not being proportional to the number of trees in Random Forest (RF) models. The user's deployment strategy involves fitting a set of RF models and returning a pickle containing a dictionary of fitted models, which are then loaded in memory for inference. The most time-consuming operation in inference is computing predictions for each tree, which is proportional to the number of trees in the RF. The user has observed inconsistent prediction times with 300 trees\/RF, sometimes taking up to 50 seconds or resulting in a timeout error. The user suspects a memory swapping mechanism or insufficient memory allocation as the cause of the issue.",
        "Question_gpt_summary":"user face issu predict time proport number tree random forest model user deploy strategi involv fit set model return pickl contain dictionari fit model load memori infer time consum oper infer comput predict tree proport number tree user observ inconsist predict time tree take second result timeout error user suspect memori swap mechan insuffici memori alloc caus issu"
    },
    {
        "Question_title":"How do I store a sweep_id in a cli environment variable so that it runs in a wandb agent later?",
        "Question_body":"<p>how do I run a wandb sweep in a cli\/terminal\/bash and store the sweep id in a env variable to use it later in the wandb agent cli command?<\/p>\n<p>Is this really correct\/recommended way?<\/p>\n<pre><code class=\"lang-auto\">export SWEEP_ID=$(wandb sweep sweep.yaml --project &lt;your-project-name&gt; | awk '\/ID:\/{print $2}')\nwandb agent --count $N $SWEEP_ID\n<\/code><\/pre>",
        "Question_answer_count":7,
        "Question_comment_count":null,
        "Question_created_time":1680241948958,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":183.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-i-store-a-sweep-id-in-a-cli-environment-variable-so-that-it-runs-in-a-wandb-agent-later\/4157",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"store sweep cli environ variabl run agent later run sweep cli termin bash store sweep env variabl us later agent cli command correct recommend wai export sweep sweep sweep yaml project awk print agent count sweep",
        "Question_preprocessed_content":"store cli environ variabl run agent later run store env variabl us later agent cli wai",
        "Question_gpt_summary_original":"The user is seeking guidance on how to store a sweep_id in a cli environment variable to run it later in a wandb agent command. They have provided a code snippet that exports the sweep_id using the wandb sweep command and awk, and then uses it in the wandb agent command. The user is questioning if this is the correct and recommended way to do it.",
        "Question_gpt_summary":"user seek guidanc store sweep cli environ variabl run later agent command provid code snippet export sweep sweep command awk us agent command user question correct recommend wai"
    },
    {
        "Question_title":"Fastai error running experiment on Azure ML",
        "Question_body":"<p>Hello everybody,  <br \/>\nI\u2019m working with Fastai (V. 2.1.7) on Azure Machine Learning (Azure ML) and I\u2019m having an issue.  <\/p>\n<p>If I train a model directly in the notebook, everything looks ok.  <br \/>\nWhen I try to run exactly the same python code into an experiment I get the following error.  <\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;train.py&quot;, line 75, in &lt;module&gt;\n    learn.fit_one_cycle(8, 3e-3)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/callback\/schedule.py&quot;, line 112, in fit_one_cycle\n    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/learner.py&quot;, line 205, in fit\n    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/learner.py&quot;, line 154, in _with_events\n    try:       self(f'before_{event_type}')       ;f()\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/learner.py&quot;, line 196, in _do_fit\n    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/learner.py&quot;, line 154, in _with_events\n    try:       self(f'before_{event_type}')       ;f()\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/learner.py&quot;, line 190, in _do_epoch\n    self._do_epoch_train()\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/learner.py&quot;, line 182, in _do_epoch_train\n    self._with_events(self.all_batches, 'train', CancelTrainException)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/learner.py&quot;, line 154, in _with_events\n    try:       self(f'before_{event_type}')       ;f()\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/learner.py&quot;, line 160, in all_batches\n    for o in enumerate(self.dl): self.one_batch(*o)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/data\/load.py&quot;, line 103, in __iter__\n    yield self.after_batch(b)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastcore\/transform.py&quot;, line 198, in __call__\n    def __call__(self, o): return compose_tfms(o, tfms=self.fs, split_idx=self.split_idx)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastcore\/transform.py&quot;, line 150, in compose_tfms\n    x = f(x, **kwargs)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/vision\/augment.py&quot;, line 34, in __call__\n    self.before_call(b, split_idx=split_idx)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/vision\/augment.py&quot;, line 377, in before_call\n    self.do,self.mat = True,self._get_affine_mat(b)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/vision\/augment.py&quot;, line 388, in _get_affine_mat\n    aff_m = _init_mat(x)\n  File &quot;\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/fastai\/vision\/augment.py&quot;, line 286, in _init_mat\n    mat = torch.eye(3, device=x.device).float()\nAttributeError: 'list' object has no attribute 'device'\n<\/code><\/pre>\n<p>Have you ever experienced the same issue?  <br \/>\nDo you have any idea about it?  <br \/>\nThanks a lot  <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1613753712050,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/280802\/fastai-error-running-experiment-on-azure-ml",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"fastai error run experi hello everybodi work fastai have issu train model directli notebook look try run exactli python code experi follow error traceback recent file train line learn fit cycl file anaconda env lib python site packag fastai callback schedul line fit cycl self fit epoch cb paramschedul sched cb reset opt reset opt file anaconda env lib python site packag fastai learner line fit self event self fit fit cancelfitexcept self end cleanup file anaconda env lib python site packag fastai learner line event try self event type file anaconda env lib python site packag fastai learner line fit self event self epoch epoch cancelepochexcept file anaconda env lib python site packag fastai learner line event try self event type file anaconda env lib python site packag fastai learner line epoch self epoch train file anaconda env lib python site packag fastai learner line epoch train self event self batch train canceltrainexcept file anaconda env lib python site packag fastai learner line event try self event type file anaconda env lib python site packag fastai learner line batch enumer self self batch file anaconda env lib python site packag fastai data load line iter yield self batch file anaconda env lib python site packag fastcor transform line def self return compos tfm tfm self split idx self split idx file anaconda env lib python site packag fastcor transform line compos tfm kwarg file anaconda env lib python site packag fastai vision augment line self split idx split idx file anaconda env lib python site packag fastai vision augment line self self mat true self affin mat file anaconda env lib python site packag fastai vision augment line affin mat aff init mat file anaconda env lib python site packag fastai vision augment line init mat mat torch ey devic devic float attributeerror list object attribut devic experienc issu idea thank lot",
        "Question_preprocessed_content":"fastai experi everybodi work fastai have train model directli try run exactli python code experi experienc idea thank lot",
        "Question_gpt_summary_original":"The user is facing an error while running a Fastai experiment on Azure ML. The error occurs when the user tries to run the same Python code used for training a model directly in the notebook. The error traceback suggests that the issue is related to a 'list' object not having the attribute 'device'. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user face error run fastai experi error occur user tri run python code train model directli notebook error traceback suggest issu relat list object have attribut devic user seek help resolv issu"
    },
    {
        "Question_title":"SSML to read Date in German Language not working",
        "Question_body":"Hi - I am working on converting Text to Speech using SSML via Google Speech API. Below is the request to the API. This perfectly works when language Code is En-US , however for code de-DE and to hear in German voice, the output is totally random. Please help me in checking this issue,\u00a0\n\n\u00a0\n\nTTS Request JSON :: {\"voice\":{\"ssmlGender\":\"MALE\",\"name\":\"de-DE-Wavenet-E\",\"languageCode\":\"de-DE\"},\"input\":{\"ssml\":\"<speak><say-as interpret-as=\\\"date\\\" format=\\\"yyyymmdd\\\"> 20220506<\\\/say-as><\\\/speak>\"},\"audioConfig\":{\"sampleRateHertz\":8000,\"volumeGainDb\":0,\"speakingRate\":1,\"audioEncoding\":\"LINEAR16\",\"pitch\":0,\"effectsProfileId\":[\"telephony-class-application\"]}}\n\n\u00a0\n\nThe same request when changeing the name and Language code works perfectly.",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1650551940000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":124.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/SSML-to-read-Date-in-German-Language-not-working\/td-p\/415908\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"ssml read date german languag work work convert text speech ssml googl speech api request api perfectli work languag code code hear german voic output total random help check issu tt request json voic ssmlgender male wavenet languagecod input ssml audioconfig sampleratehertz volumegaindb speakingr audioencod linear pitch effectsprofileid telephoni class applic request chang languag code work perfectli",
        "Question_preprocessed_content":"read date german languag work work convert text api request api perfectli work languag code code hear german voic output random help check request json input request chang languag code work perfectli",
        "Question_gpt_summary_original":"The user is facing an issue with SSML not reading the date in German language when using Google Speech API. The request works fine for language code En-US, but for code de-DE, the output is random. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user face issu ssml read date german languag googl speech api request work fine languag code code output random user seek help resolv issu"
    },
    {
        "Question_title":"Save long tensors using wandb.save OR wandb.log?",
        "Question_body":"<p>Hi all,<br>\nIn my experimentation, I want to save some tensor_10k_size for a dataset which has 90k samples.  so that I can compare how the tensors look for different datasets. It\u2019s part of my thesis.<br>\nWhich is a better option to save tensors in my scripts using wandb.save(\u2018filename\u2019)<br>\nor<br>\nwandb.log(tensor)<\/p>\n<p>Thanks,<br>\nPrachi<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1657564024663,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":264.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/save-long-tensors-using-wandb-save-or-wandb-log\/2737",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"save long tensor save log experiment want save tensor size dataset sampl compar tensor look differ dataset thesi better option save tensor script save filenam log tensor thank prachi",
        "Question_preprocessed_content":"save long tensor save log experiment want save dataset sampl compar tensor dataset thesi option save tensor script save log thank prachi",
        "Question_gpt_summary_original":"The user is conducting an experiment and wants to save long tensors for a dataset with 90k samples to compare how the tensors look for different datasets. They are seeking advice on whether to use wandb.save('filename') or wandb.log(tensor) to save the tensors in their scripts.",
        "Question_gpt_summary":"user conduct experi want save long tensor dataset sampl compar tensor look differ dataset seek advic us save filenam log tensor save tensor script"
    },
    {
        "Question_title":"Dvc get git@github.com:*.git folder giving ERROR: unexpected error - Response payload is not completed",
        "Question_body":"<p>I have successfully added a folder with 2gb of files using DVC to an s3 bucket. After that I tried to get the folder using the dvc get command. It downloaded half the folder correctly and after that the download stopped and gave me this message: ERROR: Unexpected error - Response payload not completed<br>\nAny suggestions on how I can resolve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1634238227604,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":481.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-get-git-github-com-git-folder-giving-error-unexpected-error-response-payload-is-not-completed\/919",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"git github com git folder give error unexpect error respons payload complet successfulli ad folder file bucket tri folder command download half folder correctli download stop gave messag error unexpect error respons payload complet suggest resolv issu",
        "Question_preprocessed_content":"folder give unexpect respons payload complet folder file bucket tri folder download half folder download gave unexpect respons payload complet resolv",
        "Question_gpt_summary_original":"The user encountered an issue while using DVC to download a folder from an s3 bucket. The download stopped halfway and gave an error message stating \"ERROR: Unexpected error - Response payload not completed\". The user is seeking suggestions to resolve this issue.",
        "Question_gpt_summary":"user encount issu download folder bucket download stop halfwai gave error messag state error unexpect error respons payload complet user seek suggest resolv issu"
    },
    {
        "Question_title":"Azure ML Text Classification Import Text Files",
        "Question_body":"<p>Hi,    <\/p>\n<p>I'm trying to create an Azure Machine Learning model to classify text files. I have hundreds of text files that have been organized into a subfolder named its correct label. Similar to how you train Image classification.     <\/p>\n<p>How would I get this data into a data set. I have been trying to use the python sdk since I was able to successfully get the Image classification to work.     <\/p>\n<p>Thanks,    <br \/>\nKyle<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1659040614790,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p><a href=\"\/users\/na\/?userid=8a26106b-c601-41ff-b38c-3aa0f07e4ea0\">@Domsohn, Kyle  <\/a> Thanks for the question. Here is the sample to import text files and explore azure ml text classification.    <\/p>\n<p><a href=\"https:\/\/github.com\/microsoft\/nlp-recipes\/blob\/master\/examples\/text_classification\/tc_bert_azureml.ipynb\">https:\/\/github.com\/microsoft\/nlp-recipes\/blob\/master\/examples\/text_classification\/tc_bert_azureml.ipynb<\/a>    <\/p>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1659586063577,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/946690\/azure-ml-text-classification-import-text-files",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"text classif import text file try creat model classifi text file hundr text file organ subfold name correct label similar train imag classif data data set try us python sdk abl successfulli imag classif work thank kyle",
        "Question_preprocessed_content":"text import text file try creat model text file hundr text file organ subfold name label similar train imag data data set try us python sdk abl imag work thank kyle",
        "Question_gpt_summary_original":"The user is trying to create an Azure Machine Learning model to classify text files, but is facing challenges in importing the data into a dataset using the Python SDK. The user has successfully used the SDK for image classification.",
        "Question_gpt_summary":"user try creat model classifi text file face challeng import data dataset python sdk user successfulli sdk imag classif"
    },
    {
        "Question_title":"CPU + memory usage missing from SM Studio notebook toolbar",
        "Question_body":"I work in SM Studio, and I do not understand why CPU and memory usage do not appear in the notebook toolbar. These metrics should be there, at least given this description:\n\nhttps:\/\/docs.amazonaws.cn\/en_us\/sagemaker\/latest\/dg\/notebooks-menu.html\n\nWhen I open a notebook in SM Studio, I see the same toolbar but without CPU and memory usage listed. Moreover, I see 'cluster' before the kernel's name in my toolbar.\n\nHas anyone experienced sth similar? I assume an alternative for me would be to use CloudWatch.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1652797037698,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668563929559,
        "Question_score_count":0.0,
        "Question_view_count":614.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Hi, you should be able to see your CPU and Memory on the bottom toolbar, looks like `Kernel: Idle | Instance MEM`. You can click on that text to show the kernel and instance usage metrics.",
        "Answer_comment_count":0.0,
        "Question_closed_time":1652798353413,
        "Answer_last_edit_time":1652798353412,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGQfGnTgqQcyNbWVb3U9V8Q\/cpu-memory-usage-missing-from-sm-studio-notebook-toolbar",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"cpu memori usag miss studio notebook toolbar work studio understand cpu memori usag appear notebook toolbar metric given descript http doc amazonaw latest notebook menu html open notebook studio toolbar cpu memori usag list cluster kernel toolbar experienc sth similar assum altern us cloudwatch",
        "Question_preprocessed_content":"cpu memori usag studio work studio understand cpu memori usag metric given descript open studio cpu memori usag list cluster kernel experienc sth similar altern us cloudwatch",
        "Question_gpt_summary_original":"The user is facing an issue in SM Studio where CPU and memory usage are missing from the notebook toolbar, despite being listed in the documentation. The user also notes that 'cluster' appears before the kernel's name in the toolbar. They are seeking advice from others who may have experienced a similar issue and suggest using CloudWatch as an alternative.",
        "Question_gpt_summary":"user face issu studio cpu memori usag miss notebook toolbar despit list document user note cluster appear kernel toolbar seek advic experienc similar issu suggest cloudwatch altern"
    },
    {
        "Question_title":"cannot load locally pretrained model that I deployed via sagemaker notebook",
        "Question_body":"<p>Trying to deploy locally pretrained model via sagemaker to make a endpoint and use it\nI have been learning AI\/ML field lately, and I am such a noob.\nNeed a help.<\/p>\n<h1>I deployed a model<\/h1>\n<pre><code>from sagemaker.pytorch import PyTorchModel\n\npytorch_model = PyTorchModel(model_data='model.tar.gz',\n                             role=role,\n                             entry_point='inference.py',\n                             framework_version=&quot;1.11.0&quot;,\n                             py_version=&quot;py38&quot;)\n\npredictor = \npytorch_model.deploy(instance_type='ml.g4dn.xlarge', \ninitial_instance_count=1)\n<\/code><\/pre>\n<h1>and predict data<\/h1>\n<pre><code>from PIL import Image \ndata = Image.open('.\/samples\/inputs\/1.jpg')\nresult = predictor.predict(data)\nimg = Image.open(result)\nimg.show()\n<\/code><\/pre>\n<p>as a result I got an error that I cannot load my model<\/p>\n<p>you can see the error log as well in <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/1813\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1661149956383,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1661394688063,
        "Question_score_count":0.0,
        "Question_view_count":51.0,
        "Poster_created_time":1650602380563,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73441001",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load local pretrain model deploi notebook try deploi local pretrain model endpoint us learn field late noob need help deploi model pytorch import pytorchmodel pytorch model pytorchmodel model data model tar role role entri point infer framework version version predictor pytorch model deploi instanc type gdn xlarg initi instanc count predict data pil import imag data imag open sampl input jpg result predictor predict data img imag open result img result got error load model error log",
        "Question_preprocessed_content":"load pretrain model deploi try deploi pretrain model endpoint us learn field late help deploi model predict data result got load model log",
        "Question_gpt_summary_original":"The user is trying to deploy a locally pretrained model via Sagemaker to make an endpoint and use it for predicting data. However, the user encountered an error while trying to load the model and is seeking help.",
        "Question_gpt_summary":"user try deploi local pretrain model endpoint us predict data user encount error try load model seek help"
    },
    {
        "Question_title":"Sagemaker Ground Truth completes with labelling errors",
        "Question_body":"Hi all,\n\nI set a Sagemaker Ground Truth labelling task. All the labelled objects results with \"Failed\" status, though it is possible retrieve the labels from S3. Eventually, nothing apart from the job status is wrong - but I am worried that I could and I would like to resolve it. There are no logs in cloudwatch with anything FAILED. Would you know how to get rid of this issue? For start, it would be good to see a specific issue in cloudwatch and then try to resolve the problem with failed jobs.\n\nThe logs show the following (I filtered out repeating events for different input objects):\n\n```\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"INPUT_REQUEST_VALIDATED\",\n    \"event-log-message\": \"Input request validated.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"CREATED_WORKER_INTERFACE\",\n    \"event-log-message\": \"Human task UI created.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"INPUT_MANIFEST_PARSED\",\n    \"event-log-message\": \"Input manifest parsed successfully.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"PRE_HUMAN_LAMBDA_INVOKED\",\n    \"event-log-message\": \"Pre-human task Lambda invoked for line 6.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"PRE_HUMAN_LAMBDA_SUCCEEDED\",\n    \"event-log-message\": \"Pre-human task Lambda returned successfully for line 6.\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"HUMAN_TASK_CREATED\",\n    \"event-log-message\": \"Human task created successfully for line 6.\"\n}\n{\n    \"event-name\": \"HUMAN_TASK_COMPLETED\",\n    \"event-log-message\": \"Human task completed successfully for line 6.\",\n    \"labeling-job-name\": \"test-job51\"\n}\n{\n    \"labeling-job-name\": \"test-job51\",\n    \"event-name\": \"LABELING_JOB_PROGRESS\",\n    \"event-log-message\": \"Labeling job progress for batch number:1 | #Total data set objects:2 | #Data set objects completed:0 | #Data set objects waiting for humans:0 | #Data set objects failed:2 | #Data set objects expired:0 | #Data set objects cancelled:0\"\n}\n```\n\n\nThe setup goes as follows\n- using VPC\n- using Private Labelling team\n- CORS set on S3 bucket (per suggestion from `https:\/\/repost.aws\/knowledge-center\/sagemaker-ground-truth-errors`)\n- the execution role has permission to use cloudwatch (per suggestion from https:\/\/repost.aws\/knowledge-center\/sagemaker-ground-truth-errors)\n- Cognito App callback url is `https:\/\/XXX.labeling.us-east-1.sagemaker.aws\/oauth2\/idpresponse` and logout URL is `https:\/\/XXX.labeling.us-east-1.sagemaker.aws\/logout` (I noticed that when the labelling job\/workforce is created from Console, there is an additional allowed callback `https:\/\/XXX.cloudfront.net`; I did not add it)",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1680685751465,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1681033088987,
        "Question_score_count":0.0,
        "Question_view_count":52.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUh8WSJpuNTVqQ4-gW6_e6LQ\/sagemaker-ground-truth-completes-with-labelling-errors",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"ground truth complet label error set ground truth label task label object result fail statu possibl retriev label eventu apart job statu wrong worri like resolv log cloudwatch fail know rid issu start good specif issu cloudwatch try resolv problem fail job log follow filter repeat event differ input object label job test job event input request valid event log messag input request valid label job test job event creat worker interfac event log messag human task creat label job test job event input manifest pars event log messag input manifest pars successfulli label job test job event pre human lambda invok event log messag pre human task lambda invok line label job test job event pre human lambda succeed event log messag pre human task lambda return successfulli line label job test job event human task creat event log messag human task creat successfulli line event human task complet event log messag human task complet successfulli line label job test job label job test job event label job progress event log messag label job progress batch number total data set object data set object complet data set object wait human data set object fail data set object expir data set object cancel setup goe follow vpc privat label team cor set bucket suggest http repost aw knowledg center ground truth error execut role permiss us cloudwatch suggest http repost aw knowledg center ground truth error cognito app callback url http xxx label east aw oauth idprespons logout url http xxx label east aw logout notic label job workforc creat consol addit allow callback http xxx cloudfront net add",
        "Question_preprocessed_content":"ground truth complet set ground truth task object result fail statu retriev label apart job statu wrong like resolv log cloudwatch fail know rid start specif cloudwatch try resolv problem fail job log setup goe vpc privat team cor set bucket execut role us cloudwatch cognito url logout url",
        "Question_gpt_summary_original":"The user has encountered an issue with Sagemaker Ground Truth labelling task where all the labelled objects result in a \"Failed\" status, despite being able to retrieve the labels from S3. There are no logs in cloudwatch with anything FAILED, and the user is seeking help to resolve the problem with failed jobs. The setup includes using VPC, Private Labelling team, CORS set on S3 bucket, and the execution role has permission to use cloudwatch.",
        "Question_gpt_summary":"user encount issu ground truth label task label object result fail statu despit abl retriev label log cloudwatch fail user seek help resolv problem fail job setup includ vpc privat label team cor set bucket execut role permiss us cloudwatch"
    },
    {
        "Question_title":"Connecting sagemaker using java sdk",
        "Question_body":"<p>Can anyone tell me how to connect to sagemaker using aws java sdk and invoke a endpoint which is arleady created using jupyter notebook?<\/p>\n\n<p>Link -<a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemaker\/AmazonSageMaker.html#createNotebookInstance-com.amazonaws.services.sagemaker.model.CreateNotebookInstanceRequest-\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaSDK\/latest\/javadoc\/com\/amazonaws\/services\/sagemaker\/AmazonSageMaker.html#createNotebookInstance-com.amazonaws.services.sagemaker.model.CreateNotebookInstanceRequest-<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1560072156127,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":1112.0,
        "Poster_created_time":1539596007860,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":39.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56513409",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"connect java sdk tell connect aw java sdk invok endpoint arleadi creat jupyt notebook link http doc aw amazon com awsjavasdk latest javadoc com amazonaw servic amazon html createnotebookinst com amazonaw servic model createnotebookinstancerequest",
        "Question_preprocessed_content":"java sdk aw java sdk invok endpoint arleadi creat jupyt link",
        "Question_gpt_summary_original":"The user is seeking guidance on how to connect to Sagemaker using AWS Java SDK and invoke an endpoint that was created using Jupyter Notebook. They have provided a link to the AWS Java SDK documentation for creating a Notebook Instance.",
        "Question_gpt_summary":"user seek guidanc connect aw java sdk invok endpoint creat jupyt notebook provid link aw java sdk document creat notebook instanc"
    },
    {
        "Question_title":"MLflow Artifacts Storing But Not Listing In UI",
        "Question_body":"<p>I've run into an issue using MLflow server. When I first ran the command to start an mlflow server on an ec2 instance, everything worked fine. Now, although logs and artifacts are being stored to postgres and s3, the UI is not listing the artifacts. Instead, the artifact section of the UI shows:<\/p>\n\n<pre><code>Loading Artifacts Failed\nUnable to list artifacts stored under &lt;s3-location&gt; for the current run. Please contact your tracking server administrator to notify them of this error, which can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n\n<p>But when I check in s3, I see the artifact in the s3 location that the error shows. What could possibly have started causing this as it used to work not too long ago and nothing was changed on the ec2 that is hosting mlflow?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1589810994053,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":9.0,
        "Question_view_count":3949.0,
        "Poster_created_time":1417012835812,
        "Poster_location":null,
        "Poster_reputation_count":945.0,
        "Poster_view_count":148.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61871515",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"artifact store list run issu server ran command start server instanc work fine log artifact store postgr list artifact instead artifact section show load artifact fail unabl list artifact store current run contact track server administr notifi error happen track server lack permiss list artifact current run root artifact directori check artifact locat error show possibl start caus work long ago chang host",
        "Question_preprocessed_content":"artifact store list run server ran start server instanc work fine log artifact store postgr list artifact instead artifact section show check artifact locat show start caus work long ago chang host",
        "Question_gpt_summary_original":"MLflow server is storing logs and artifacts to postgres and s3, but the UI is not listing the artifacts. The UI shows an error message stating that it is unable to list artifacts stored under the s3 location for the current run. However, the user can see the artifact in the s3 location that the error shows. The user is unsure why this issue has started occurring as nothing was changed on the ec2 that is hosting MLflow.",
        "Question_gpt_summary":"server store log artifact postgr list artifact show error messag state unabl list artifact store locat current run user artifact locat error show user unsur issu start occur chang host"
    },
    {
        "Question_title":"Key Error in wandb.config when using wandb.sweep in pytorch",
        "Question_body":"<p>I am trying to run this codebase <a href=\"https:\/\/github.com\/devzhk\/LMCTS\" rel=\"nofollow noreferrer\">https:\/\/github.com\/devzhk\/LMCTS<\/a>. To run hyperparameter sweep, I ran the following:<\/p>\n<pre><code>wandb sweep sweep\/simulation\/linear-lmcts.yaml\nwandb agent [agent id]\n<\/code><\/pre>\n<p><a href=\"https:\/\/github.com\/devzhk\/LMCTS\/blob\/master\/sweep\/simulation\/linear-lmcts.yaml\" rel=\"nofollow noreferrer\">The config<\/a> file seems to have the same style as the sweep tutorial in W&amp;B website has. However, when I run sweep command as above, I get the following error<\/p>\n<pre><code>2022-08-10 02:12:55,113 - wandb.wandb_agent - INFO - Running runs: []\n2022-08-10 02:12:55,352 - wandb.wandb_agent - INFO - Agent received command: run\n2022-08-10 02:12:55,362 - wandb.wandb_agent - INFO - Agent starting run with config:\n    T: 10000\n    algo: LMCTS\n    beta_inv: 0.0001\n    datapath: data\/gaussian50-20-1-1.pt\n    dim_context: 20\n    func: linear\n    lr: 0.1\n    model: linear\n    num_arm: 50\n    num_iter: 70\n    sigma: 0.5\n2022-08-10 02:12:55,372 - wandb.wandb_agent - INFO - About to run command: \/usr\/bin\/env python3 run_simulation.py\n2022-08-10 02:13:00,385 - wandb.wandb_agent - INFO - Running runs: ['ngxmksza']\n\/home\/mila\/i\/ishfaqha\/code\/LMCTS\/LCMTS\/lib\/python3.7\/site-packages\/sklearn\/feature_extraction\/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https:\/\/numpy.org\/devdocs\/release\/1.20.0-notes.html#deprecations\n  dtype=np.int):\nRandom seed: 720\nTraceback (most recent call last):\n  File &quot;run_simulation.py&quot;, line 107, in &lt;module&gt;\n    run(config, args)\n  File &quot;run_simulation.py&quot;, line 41, in run\n    data = torch.load(config['datapath'])\nKeyError: 'datapath'\n<\/code><\/pre>\n<p>Shouldn't it automatically parse the config?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1660196943067,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":52.0,
        "Poster_created_time":1398814181992,
        "Poster_location":null,
        "Poster_reputation_count":1975.0,
        "Poster_view_count":188.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73315823",
        "Tool":"Weights & Biases",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"kei error config sweep pytorch try run codebas http github com devzhk lmct run hyperparamet sweep ran follow sweep sweep simul linear lmct yaml agent agent config file style sweep tutori websit run sweep command follow error agent info run run agent info agent receiv command run agent info agent start run config algo lmct beta inv datapath data gaussian dim context func linear model linear num arm num iter sigma agent info run command usr bin env python run simul agent info run run ngxmksza home mila ishfaqha code lmct lcmt lib python site packag sklearn featur extract imag deprecationwarn int deprec alia builtin int silenc warn us int modifi behavior safe replac int wish us int int specifi precis wish review current us check releas note link addit inform deprec numpi detail guidanc http numpi org devdoc releas note html deprec dtype int random seed traceback recent file run simul line run config arg file run simul line run data torch load config datapath keyerror datapath shouldn automat pars config",
        "Question_preprocessed_content":"kei config pytorch try run codebas run hyperparamet ran config file style tutori websit run shouldn pars config",
        "Question_gpt_summary_original":"The user encountered a KeyError in wandb.config while using wandb.sweep in PyTorch. The user was trying to run a hyperparameter sweep using a config file similar to the one in the W&B website tutorial. However, when the user ran the sweep command, they received a KeyError for 'datapath'. The user is unsure why the config file was not parsed automatically.",
        "Question_gpt_summary":"user encount keyerror config sweep pytorch user try run hyperparamet sweep config file similar websit tutori user ran sweep command receiv keyerror datapath user unsur config file pars automat"
    },
    {
        "Question_title":"Crowd HTML Element for NER does not allow for overlapping annotations like the current Ground Truth allows",
        "Question_body":"I am able to make an A2I UI that allows for NER annotation and classification (based off of https:\/\/github.com\/aws-samples\/amazon-sagemaker-ground-truth-task-uis\/blob\/master\/text\/named-entity-recognition-with-additional-classification.liquid.html).\nHowever the NER interface doesn't allow for overlapping annotations like Sagemaker Ground Truth's NER interface does. Is there a way to get overlapping annotations in A2I for NER?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1671738370745,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1672084173368,
        "Question_score_count":0.0,
        "Question_view_count":33.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3uEu19m5TYyg7HZiknXqxw\/crowd-html-element-for-ner-does-not-allow-for-overlapping-annotations-like-the-current-ground-truth-allows",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"crowd html element ner allow overlap annot like current ground truth allow abl allow ner annot classif base http github com aw sampl amazon ground truth task ui blob master text name entiti recognit addit classif liquid html ner interfac allow overlap annot like ground truth ner interfac wai overlap annot ner",
        "Question_preprocessed_content":"crowd html element ner like ground truth abl ner ner interfac like ground truth ner interfac wai ner",
        "Question_gpt_summary_original":"The user is facing a challenge with the Crowd HTML Element for NER as it does not allow for overlapping annotations like the current Ground Truth allows. They are seeking a solution to get overlapping annotations in A2I for NER.",
        "Question_gpt_summary":"user face challeng crowd html element ner allow overlap annot like current ground truth allow seek solut overlap annot ner"
    },
    {
        "Question_title":"Shape error while converting Image to JSON file",
        "Question_body":"<p>I am trying to convert image to JSON file and POST it with REST API by using MLFLow. Below you can see my code. I got an error like &quot;cannot reshape array of size 535500 into shape (1,4096)&quot;. Can you please help me. Thank you in advance.<\/p>\n<pre><code>import json\nimport cv2\nimport requests\nimport base64\nimport numpy as np\nfrom PIL import Image\n\n\ndata = np.asarray(Image.open('Dataset\/test2\/dog_PNG50348.png').convert('LA'))\ndata = data.reshape((1, 64*64))\ncolumns = [f&quot;col_{c}&quot; for c in range(0, data[0].shape[0])]\ndct = {&quot;columns&quot;: columns, &quot;data&quot;: [data[0].tolist()]}\nprint(json.dumps(dct, indent=2) + &quot;\\n&quot;)\n\n#print(data)\nheaders = {'Content-Type': 'application\/json'}\nrequest_uri = 'http:\/\/127.0.0.1:5000\/invocations'\n\nif __name__ == '__main__':\n    try:\n        response = requests.post(request_uri, data=json.dumps(dct,indent=2)+&quot;\\n&quot;, headers=headers)\n        print(response.content)\n        print('done!!!')\n    except Exception as ex:\n        raise (ex)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_created_time":1628172503093,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":70.0,
        "Poster_created_time":1460657116247,
        "Poster_location":null,
        "Poster_reputation_count":98.0,
        "Poster_view_count":19.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68668190",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"shape error convert imag json file try convert imag json file post rest api code got error like reshap arrai size shape help thank advanc import json import import request import base import numpi pil import imag data asarrai imag open dataset test dog png png convert data data reshap column col rang data shape dct column column data data tolist print json dump dct indent print data header content type applic json request uri http invoc main try respons request post request uri data json dump dct indent header header print respons content print except rais",
        "Question_preprocessed_content":"shape convert imag json file try convert imag json file post rest api code got like reshap size shape help thank advanc",
        "Question_gpt_summary_original":"The user is trying to convert an image to a JSON file and POST it with REST API using MLFlow, but encountered an error \"cannot reshape array of size 535500 into shape (1,4096)\". The user has provided the code used for the conversion and is seeking help to resolve the issue.",
        "Question_gpt_summary":"user try convert imag json file post rest api encount error reshap arrai size shape user provid code convers seek help resolv issu"
    },
    {
        "Question_title":"Cannot install azureml-sdk without dependency conflicts",
        "Question_body":"<p>I'm trying to install the latest azureml-sdk (1.34.0) inside a new conda env (with python 3.7) but the installation ends with the following error:    <\/p>\n<p>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.    <br \/>\nknack 0.7.2 requires argcomplete, which is not installed.    <br \/>\nknack 0.7.2 requires colorama, which is not installed.    <br \/>\nknack 0.7.2 requires pygments, which is not installed.    <br \/>\nazure-functions-devops-build 0.0.22 requires jinja2, which is not installed.    <br \/>\nazure-cli-core 2.10.0 requires argcomplete~=1.8, which is not installed.    <br \/>\nazure-cli-core 2.10.0 requires colorama~=0.4.1, which is not installed.    <br \/>\nazure-cli 2.10.0 requires azure-mgmt-keyvault~=2.2.0, but you have azure-mgmt-keyvault 9.1.0 which is incompatible.    <br \/>\nazure-cli 2.10.0 requires cryptography&lt;3.0.0,&gt;=2.3.1, but you have cryptography 3.4.8 which is incompatible.    <br \/>\nazure-cli-core 2.10.0 requires azure-mgmt-core==1.0.0, but you have azure-mgmt-core 1.3.0 which is incompatible.    <br \/>\nazure-cli-core 2.10.0 requires msal~=1.0.0, but you have msal 1.14.0 which is incompatible.    <br \/>\nazure-cli-core 2.10.0 requires msal-extensions~=0.1.3, but you have msal-extensions 0.2.2 which is incompatible.    <\/p>\n<p>How can I solve it?    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130764-image.png?platform=QnA\" alt=\"130764-image.png\" \/>    <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2.0,
        "Question_created_time":1631182445263,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/545669\/cannot-install-azureml-sdk-without-dependency-conf",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"instal sdk depend conflict try instal latest sdk insid new conda env python instal end follow error error pip depend resolv current account packag instal behaviour sourc follow depend conflict knack requir argcomplet instal knack requir colorama instal knack requir pygment instal azur function devop build requir jinja instal azur cli core requir argcomplet instal azur cli core requir colorama instal azur cli requir azur mgmt keyvault azur mgmt keyvault incompat azur cli requir cryptographi cryptographi incompat azur cli core requir azur mgmt core azur mgmt core incompat azur cli core requir msal msal incompat azur cli core requir msal extens msal extens incompat solv",
        "Question_preprocessed_content":"sdk depend conflict try latest sdk insid new conda env end pip depend resolv packag behaviour sourc depend conflict knack requir argcomplet knack requir colorama knack requir pygment requir jinja requir requir requir incompat requir cryptographi incompat requir incompat requir msal incompat requir incompat solv",
        "Question_gpt_summary_original":"The user is facing dependency conflicts while trying to install the latest version of azureml-sdk (1.34.0) in a new conda env with Python 3.7. The installation fails with an error message that lists several missing packages and incompatible versions. The user is seeking a solution to resolve the issue.",
        "Question_gpt_summary":"user face depend conflict try instal latest version sdk new conda env python instal fail error messag list miss packag incompat version user seek solut resolv issu"
    },
    {
        "Question_title":"Azure ML operations : workspace authentication error",
        "Question_body":"<p>I need to connect with Azure ML Workspace during deployment over container instance.<\/p>\n<pre><code>ws = Workspace(subscription_id=&quot;your-sub-id&quot;,\n              resource_group=&quot;your-resource-group-id&quot;,\n              workspace_name=&quot;your-workspace-name&quot;\n              )\n<\/code><\/pre>\n<p>Interactive Authentication to the ML Workspace prompts to login and then fails with below error message.<\/p>\n<pre><code>AttributeError: 'BasicTokenAuthentication' object has no attribute 'get_token'\n<\/code><\/pre>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication#interactive-authentication\" rel=\"nofollow noreferrer\">i have been following this Azure Authentication document.<\/a><\/p>\n<p>Any help is much appreciated.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3.0,
        "Question_created_time":1600761544463,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":706.0,
        "Poster_created_time":1530258066240,
        "Poster_location":null,
        "Poster_reputation_count":125.0,
        "Poster_view_count":33.0,
        "Answer_body":"<p>For me this was fixed by updating azureml-core from 1.13.0 to 1.14.0.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1600775316867,
        "Answer_last_edit_time":1600781478488,
        "Answer_score_count":1.0,
        "Answerer_created_time":1508836189288,
        "Answerer_location":"Finland",
        "Answerer_reputation_count":36.0,
        "Answerer_view_count":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64005433",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"oper workspac authent error need connect workspac deploy contain instanc workspac subscript sub resourc group resourc group workspac workspac interact authent workspac prompt login fail error messag attributeerror basictokenauthent object attribut token follow azur authent document help appreci",
        "Question_preprocessed_content":"oper workspac authent workspac deploy contain instanc interact authent workspac prompt login fail azur authent document help",
        "Question_gpt_summary_original":"The user is encountering an authentication error while trying to connect with Azure ML Workspace during deployment over container instance. The error message \"AttributeError: 'BasicTokenAuthentication' object has no attribute 'get_token'\" is displayed while attempting interactive authentication to the ML Workspace. The user has been following the Azure Authentication document for help.",
        "Question_gpt_summary":"user encount authent error try connect workspac deploy contain instanc error messag attributeerror basictokenauthent object attribut token displai attempt interact authent workspac user follow azur authent document help"
    },
    {
        "Question_title":"Guild view not working in jupyterhub",
        "Question_body":"<p>I am using GuildAI in a Jupyter Lab server provisioned in Jupyterhub. To view the UI, I run the command <code>guild view --host=localhost --port=5000<\/code> but am getting the error \u201cFailed to load resource: the server responded with a status of 404 (Not Found)\u201d. On the other hand, <code>guild tensorboard --host=localhost --port=5000<\/code> works fine for me. I\u2019m not sure what\u2019s the issue or how to debug it.<\/p>\n<p>Packages:<br>\nguildai==0.8.1<br>\njupyterhub==1.5.0<br>\njupyterlab==3.4.7<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1664820465043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":153.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-view-not-working-in-jupyterhub\/922",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"view work jupyterhub jupyt lab server provis jupyterhub view run command view host localhost port get error fail load resourc server respond statu hand tensorboard host localhost port work fine sure what issu debug packag jupyterhub jupyterlab",
        "Question_preprocessed_content":"view work jupyterhub jupyt lab server provis jupyterhub view run fail load resourc server respond statu hand work fine sure what debug packag",
        "Question_gpt_summary_original":"The user is encountering an issue with GuildAI in a Jupyter Lab server provisioned in Jupyterhub. The command \"guild view\" is not working and is giving an error \"Failed to load resource: the server responded with a status of 404 (Not Found)\". However, \"guild tensorboard\" is working fine. The user is unsure about the issue and how to debug it. The packages used are guildai==0.8.1, jupyterhub==1.5.0, and jupyterlab==3.4.7.",
        "Question_gpt_summary":"user encount issu jupyt lab server provis jupyterhub command view work give error fail load resourc server respond statu tensorboard work fine user unsur issu debug packag jupyterhub jupyterlab"
    },
    {
        "Question_title":"Azure ML Web Service request not working in C#",
        "Question_body":"<p>I have created an Azure ML Web service which outputs JSON response on request, and the structure of the sample request is as following:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"gender\",\n        \"age\",\n        \"income\"\n      ],\n      \"Values\": [\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ],\n        [\n          \"value\",\n          \"0\",\n          \"0\"\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>And the input parameters are supposedly like this:<\/p>\n\n<p>gender  String<br>\nage Numeric<br>\nincome  Numeric     <\/p>\n\n<p>My Post method looks like this:<\/p>\n\n<pre><code>    [HttpPost]\n        public ActionResult GetPredictionFromWebService()\n        {\n            var gender = Request.Form[\"gender\"];\n            var age = Request.Form[\"age\"];\n\n\n            if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n            {\n                var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = Int32.Parse(result[0, 2])\n                    };\n                }\n            }\n\n\n\n\n            return RedirectToAction(\"index\");\n        }\n<\/code><\/pre>\n\n<p>But for whatever reason; the Azure ML Webservice doesn\u2019t seem to respond anything to my request.\nDoes anyone know what the reason might be? I see no error or anything, just an empty response.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1456313293347,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1456839908307,
        "Question_score_count":0.0,
        "Question_view_count":480.0,
        "Poster_created_time":1456309738852,
        "Poster_location":null,
        "Poster_reputation_count":39.0,
        "Poster_view_count":6.0,
        "Answer_body":"<p>The answer to your problem is that the \u201cNumeric\u201d datatype which is written in the input parameters in Azure ML is in fact a float and not an integer for your income measure. So when trying to request a response from Azure ML, you are not providing it the \u201cadequate\u201d information needed in the right format for it to respond correctly, resulting in it not giving you any response.<\/p>\n\n<p>I believe your model would look something similar to this based on your input parameters:<\/p>\n\n<pre><code>public class Person\n    {\n        public string Gender { get; set; }\n        public int Age { get; set; }\n        public int Income { get; set; }\n\n\n        public override string ToString()\n        {\n            return Gender + \",\" + Age + \",\" + Income;\n        }\n    }\n<\/code><\/pre>\n\n<p>You would have to change your Income datatype into float like so:<\/p>\n\n<pre><code>public class Person\n{\n    public string Gender { get; set; }\n    public int Age { get; set; }\n    public float Income { get; set; }\n\n    public override string ToString()\n    {\n        return Gender + \",\" + Age + \",\" + Income;\n    }\n}\n<\/code><\/pre>\n\n<p>And then your post-method would look something like this:<\/p>\n\n<pre><code>    [HttpPost]\n    public ActionResult GetPredictionFromWebService()\n    {\n        var gender = Request.Form[\"gender\"];\n        var age = Request.Form[\"age\"];\n\n        if (!string.IsNullOrEmpty(gender) &amp;&amp; !string.IsNullOrEmpty(age))\n        {\n            var resultResponse = _incomeWebService.InvokeRequestResponseService&lt;ResultOutcome&gt;(gender, age).Result;\n\n                if (resultResponse != null)\n                {\n                    var result = resultResponse.Results.Output1.Value.Values;\n                    PersonResult = new Person\n                    {\n                        Gender = result[0, 0],\n                        Age = Int32.Parse(result[0, 1]),\n                        Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n                };\n            }\n        }\n\n        ViewBag.myData = PersonResult.Income.ToString();\n        return View(\"Index\");\n    }\n<\/code><\/pre>\n\n<p>The key here is simply:<\/p>\n\n<pre><code>Income = float.Parse(result[0, 3], CultureInfo.InvariantCulture.NumberFormat)\n<\/code><\/pre>\n\n<p>Rather than your legacy <\/p>\n\n<pre><code>Income = Int32.Parse(result[0, 2])\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1456314151448,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1446841002932,
        "Answerer_location":"Trondheim, Norway",
        "Answerer_reputation_count":226.0,
        "Answerer_view_count":60.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35600907",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"web servic request work creat web servic output json respons request structur sampl request follow input input columnnam gender ag incom valu valu valu globalparamet input paramet supposedli like gender string ag numer incom numer post method look like httppost public actionresult getpredictionfromwebservic var gender request form gender var ag request form ag string isnullorempti gender string isnullorempti ag var resultrespons incomewebservic invokerequestresponseservic gender ag result resultrespons null var result resultrespons result output valu valu personresult new person gender result ag int pars result incom int pars result return redirecttoact index reason webservic doesnt respond request know reason error respons",
        "Question_preprocessed_content":"web servic request work creat web servic output json respons request structur sampl request input paramet like gender string ag numer incom numer post method like reason webservic doesnt respond request know reason respons",
        "Question_gpt_summary_original":"The user has created an Azure ML Web service that outputs a JSON response on request. The user has created a post method to get predictions from the web service, but the web service is not responding to the request, and the user is not receiving any error messages.",
        "Question_gpt_summary":"user creat web servic output json respons request user creat post method predict web servic web servic respond request user receiv error messag"
    },
    {
        "Question_title":"Create endpoint from Python",
        "Question_body":"Hello,  \n  \nI have trained my model on sagemaker. I have deleted the endpoint, but I am keeping the model and the endpoint configuration which points to the model.  \n  \nFrom the sagemaker dashboard I am able to recreate the endpoint using the existing endpoint configuration. However I don't want to keep the endpoint on all the time, as I will use it only once a day for a few minutes.  \n  \nIs it possible to create in on demand from a Python script? I would assume that it is possible, but can't find how. Can someone point me in the right direction?  \n  \nRegards.",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1625083671000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668599351248,
        "Question_score_count":0.0,
        "Question_view_count":125.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Hello hugoflores,   \n  \nYou can use SageMaker APIs - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DeleteEndpoint.html to delete the endpoint and https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateEndpoint.html. to create an endpoint. This an be automated either using SageMaker Pipelines or a Lambda function.  \n  \nHere are a few resources towards that:  \n  \nhttps:\/\/awsfeed.com\/whats-new\/machine-learning\/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services  \nhttps:\/\/github.com\/aws-samples\/aws-lambda-layer-create-script  \nhttps:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1200  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/  \nhttps:\/\/www.sagemakerworkshop.com\/step\/deploymodel\/  \n  \nHTH,   \n  \nChaitanya",
        "Answer_comment_count":0.0,
        "Question_closed_time":1626971777000,
        "Answer_last_edit_time":1626971777000,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUTyUMHH4QRDaMa6L24rhOMg\/create-endpoint-from-python",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"creat endpoint python hello train model delet endpoint keep model endpoint configur point model dashboard abl recreat endpoint exist endpoint configur want endpoint time us dai minut possibl creat demand python script assum possibl point right direct regard",
        "Question_preprocessed_content":"creat endpoint python train model delet endpoint model endpoint configur point model dashboard abl recreat endpoint exist endpoint configur want endpoint time us dai minut creat demand python script point right direct regard",
        "Question_gpt_summary_original":"The user has trained a model on sagemaker and deleted the endpoint, but kept the model and endpoint configuration. They want to recreate the endpoint on demand from a Python script instead of keeping it on all the time, but are unsure how to do so and are seeking guidance.",
        "Question_gpt_summary":"user train model delet endpoint kept model endpoint configur want recreat endpoint demand python script instead keep time unsur seek guidanc"
    },
    {
        "Question_title":"Azure Subscription Cost",
        "Question_body":"<p>my azure subscription cost is decreasing everyday. Knowing that i have deleted everything from my workspace and in my azureml workspace don't have any cluster, I don't know why it is still decreasing.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/172214-image.png?platform=QnA\" alt=\"172214-image.png\" \/>    <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1644315390737,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>If you want to review your costs and what resources are being charged, then the <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/cost-management-billing\/cost-management-billing-overview#understand-cost-management\">Cost Analysis blade<\/a> will allow you to drill down work this out. Please let us know if this helps    <\/p>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1644316889220,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/726898\/azure-subscription-cost",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur subscript cost azur subscript cost decreas everydai know delet workspac workspac cluster know decreas",
        "Question_preprocessed_content":"azur subscript cost azur subscript cost decreas everydai know delet workspac workspac cluster know decreas",
        "Question_gpt_summary_original":"The user's Azure subscription cost is decreasing every day, even though they have deleted everything from their workspace and do not have any cluster in their AzureML workspace. The user is unsure why this is happening.",
        "Question_gpt_summary":"user azur subscript cost decreas dai delet workspac cluster workspac user unsur happen"
    },
    {
        "Question_title":"Loading Pretrained Keras to Sagemaker - local classification works but sagemaker classification changes",
        "Question_body":"<p>EDIT: Found a solution, see bottom of post.<\/p>\n<p>I have a pre-trained keras model (model.h5) which is a CNN for image classification. My goal is to deploy the model on sagemaker and use a lambda function to interface with the sagemaker endpoint and make predictions. When I predict with the model on my local machine using the following code, I get results I would expect:<\/p>\n<pre><code>model = load_model(r'model.h5')\nphoto_fp = r'\/path\/to\/photo.jpg'\n\nimg = Image.open(photo_fp).resize((128,128))\nimage_array = np.array(img) \/ 255.\nimg_batch = np.expand_dims(image_array, axis=0)\n\nprint(model.predict(img_batch))\n# [[9.9984562e-01 1.5430539e-04 2.2775747e-14 9.5851349e-16]]\n<\/code><\/pre>\n<p>However, when I deploy the model as an endpoint on sagemaker, I get different results. Below is my code to deploy the model as an endpoint:<\/p>\n<pre><code>model = load_model(r'model.h5')\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sagemaker\nimport boto3, re\nfrom sagemaker import get_execution_role\ndef convert_h5_to_aws(loaded_model):\n    # Interpreted from 'Data Liam'\n    from tensorflow.python.saved_model import builder\n    from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n    from tensorflow.python.saved_model import tag_constants\n    \n    model_version = '1'\n    export_dir = 'export\/Servo\/' + model_version\n    \n    # Build the Protocol Buffer SavedModel at 'export_dir'\n    builder = builder.SavedModelBuilder(export_dir)\n    \n    # Create prediction signature to be used by TensorFlow Serving Predict API\n    signature = predict_signature_def(\n        inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})\n\n    with tf.compat.v1.Session() as sess:\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        # Save the meta graph and variables\n        builder.add_meta_graph_and_variables(\n            sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n        builder.save()\n    \n    #create a tarball\/tar file and zip it\n    import tarfile\n    with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n        archive.add('export', recursive=True)\n        \nconvert_h5_to_aws(model)\n\nsagemaker_session = sagemaker.Session()\ninputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\n\n!touch train.py # from notebook\n# the (default) IAM role\nrole = get_execution_role()\nframework_version = tf.__version__\n\n# Create Sagemaker model\nfrom sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = framework_version,\n                                  entry_point = 'train.py')\n\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>This deploys fine and saves as an endpoint. Then, I invoke the endpoint:<\/p>\n<pre><code>runtime = boto3.client('runtime.sagemaker')\nendpoint_name = 'endpoint-name-for-stackoverflow'\n\nimg = Image.open(photo_fp).resize((128,128))\nimage_array = np.array(img) \/ 255.\nimg_batch = np.expand_dims(image_array, axis=0)\npredictor = TensorFlowPredictor(endpoint_name)\nresult = predictor.predict(data=img_batch)\nprint(result)\n# {'predictions': [[0.199595317, 0.322404563, 0.209394112, 0.268606]]}\n<\/code><\/pre>\n<p>As you can see, the classifier is predicting all of the outputs as nearly equal probabilities, which is not what was predicted on the local machine. This leads me to believe that something is going wrong in my deployment.<\/p>\n<p>I have tried loading the model weights and json model structure to sagemaker rather than the entire h5 model but that yielded the same results. I also used invoke endpoint instead of the predictor API with the following code:<\/p>\n<pre><code>payload = json.dumps(img_batch.tolist())\nresponse = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                                   ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nprint(result)\n# {'predictions': [[0.199595317, 0.322404563, 0.209394112, 0.268606]]}\n<\/code><\/pre>\n<p>But yet again, the same results.<\/p>\n<p>Any ideas why I'm getting different results with the sagemaker than on my local machine with the same model?\nThanks!<\/p>\n<p>EDIT: Found a solution. The problem was with the TensorflowModel framework version argument. I changed the framework_version to '1.12' and installed version 1.12 in the Sagemaker Jupyter instance and retrained my model locally using TF 1.12. I'm not totally sure why this works but all of the blogs I found (e.g. <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">this one<\/a>) used 1.12. Hope this helps.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1620830223953,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1621607684923,
        "Question_score_count":0.0,
        "Question_view_count":169.0,
        "Poster_created_time":1606275044956,
        "Poster_location":null,
        "Poster_reputation_count":59.0,
        "Poster_view_count":8.0,
        "Answer_body":"<p>For the benefit of community providing solution in answer section<\/p>\n<blockquote>\n<p>The problem was with the <code>TensorflowModel<\/code> framework version argument. After\nchanging the <code>framework_version<\/code> to <code>1.12<\/code> and installed version <code>TF 1.12<\/code> in\nthe <code>Sagemaker Jupyter<\/code> instance and retrained model locally using <code>TF 1.12<\/code> got same results. (paraphrased from Peter Van Katwyk)<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1621926225083,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1600296752470,
        "Answerer_location":null,
        "Answerer_reputation_count":3586.0,
        "Answerer_view_count":684.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67505781",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load pretrain kera local classif work classif chang edit solut post pre train kera model model cnn imag classif goal deploi model us lambda function interfac endpoint predict predict model local machin follow code result expect model load model model photo path photo jpg img imag open photo resiz imag arrai arrai img img batch expand dim imag arrai axi print model predict img batch deploi model endpoint differ result code deploi model endpoint model load model model import tensorflow tensorflow import kera import import boto import execut role def convert aw load model interpret data liam tensorflow python save model import builder tensorflow python save model signatur def util import predict signatur def tensorflow python save model import tag constant model version export dir export servo model version build protocol buffer savedmodel export dir builder builder savedmodelbuild export dir creat predict signatur tensorflow serv predict api signatur predict signatur def input input load model input output score load model output compat session sess init global variabl initi sess run init save meta graph variabl builder add meta graph variabl sess sess tag tag constant serv signatur def map serv default signatur builder save creat tarbal tar file zip import tarfil tarfil open model tar mode archiv archiv add export recurs true convert aw model session session input session upload data path model tar kei prefix model touch train notebook default iam role role execut role framework version version creat model tensorflow model import tensorflowmodel model tensorflowmodel model data session default bucket model model tar role role framework version framework version entri point train predictor model deploi initi instanc count instanc type xlarg deploi fine save endpoint invok endpoint runtim boto client runtim endpoint endpoint stackoverflow img imag open photo resiz imag arrai arrai img img batch expand dim imag arrai axi predictor tensorflowpredictor endpoint result predictor predict data img batch print result predict classifi predict output nearli equal probabl predict local machin lead believ go wrong deploy tri load model weight json model structur entir model yield result invok endpoint instead predictor api follow code payload json dump img batch tolist respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload result json load respons bodi read decod print result predict result idea get differ result local machin model thank edit solut problem tensorflowmodel framework version argument chang framework version instal version jupyt instanc retrain model local total sure work blog hope help",
        "Question_preprocessed_content":"load pretrain kera local work chang edit solut post kera model imag goal deploi model us lambda function interfac endpoint predict predict model local machin code result expect deploi model endpoint result code deploi model endpoint deploi fine save endpoint invok endpoint predict output nearli equal probabl predict local machin lead believ go wrong deploy tri load model weight json model structur entir model yield result invok endpoint instead predictor api code result idea result local machin model thank edit solut problem tensorflowmodel framework version argument chang version jupyt instanc retrain model sure work blog hope help",
        "Question_gpt_summary_original":"The user has a pre-trained Keras model for image classification and wants to deploy it on Sagemaker to make predictions using a lambda function. While the local classification works fine, the Sagemaker classification produces different results, with all outputs having nearly equal probabilities. The user has tried loading the model weights and JSON model structure to Sagemaker, but it yielded the same results. The user is seeking help to understand why the Sagemaker classification is different from the local classification.",
        "Question_gpt_summary":"user pre train kera model imag classif want deploi predict lambda function local classif work fine classif produc differ result output have nearli equal probabl user tri load model weight json model structur yield result user seek help understand classif differ local classif"
    },
    {
        "Question_title":"Azure AutoML Model - Test Interface",
        "Question_body":"<p>Hi Guys. New in the Azure AutoML space.     <br \/>\nI followed through the steps and successfully deployed a model on web:     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/153843-image.png?platform=QnA\" alt=\"153843-image.png\" \/>    <\/p>\n<p>When I go into &quot;Test&quot; tab, the interface with dialog boxes is missing, and it's displaying raw code:     <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/153790-image.png?platform=QnA\" alt=\"153790-image.png\" \/>    <\/p>\n<p>Just wanted to check if anyone knew how I could get the dialog boxes or UI ? <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4.0,
        "Question_created_time":1638313645353,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/646763\/azure-automl-model-test-interface",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur automl model test interfac gui new azur automl space follow step successfulli deploi model web test tab interfac dialog box miss displai raw code want check knew dialog box",
        "Question_preprocessed_content":"azur automl model test interfac gui new azur automl space step deploi model web test tab interfac dialog box displai raw code want check knew dialog box",
        "Question_gpt_summary_original":"The user has successfully deployed an Azure AutoML model on the web but is facing an issue with the \"Test\" tab. The interface with dialog boxes is missing, and raw code is being displayed instead. The user is seeking help to get the dialog boxes or UI back.",
        "Question_gpt_summary":"user successfulli deploi azur automl model web face issu test tab interfac dialog box miss raw code displai instead user seek help dialog box"
    },
    {
        "Question_title":"Deployment failed - InternalServerError",
        "Question_body":"<p>Hello,  <\/p>\n<p>I just created a free Azure account. I want to create an an Azure Machine Learning workspace (create a resource), but it seems that the the deployment fails. I receive the following error:  <\/p>\n<p>&quot;code&quot;: &quot;InternalServerError&quot;,  <br \/>\n&quot;message&quot;: &quot;Received 400 from a service request&quot;  <\/p>\n<p>What should I do?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1630015001840,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/529757\/deployment-failed-internalservererror",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploy fail internalservererror hello creat free azur account want creat workspac creat resourc deploy fail receiv follow error code internalservererror messag receiv servic request",
        "Question_preprocessed_content":"deploy fail creat azur want creat workspac deploy fail receiv code receiv servic request",
        "Question_gpt_summary_original":"The user encountered an issue while trying to create an Azure Machine Learning workspace using their newly created free Azure account. The deployment failed with an InternalServerError and a message stating that they received 400 from a service request. The user is seeking guidance on what to do next.",
        "Question_gpt_summary":"user encount issu try creat workspac newli creat free azur account deploy fail internalservererror messag state receiv servic request user seek guidanc"
    },
    {
        "Question_title":"No option to select Local machine for compute in Azure ML",
        "Question_body":"<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/25233-capture2.png?platform=QnA\" alt=\"25233-capture2.png\" \/>Hi,    <\/p>\n<p>I have created ML space for experimenting ML on azure. However to execute my ML work I need to select compute target and as per Azure docs I can select local machine or remote compute but I don't see option to select local machine. I only have options to create compute instance, compute clusters.    <\/p>\n<p>Can someone please help me why I don't have the option.    <\/p>\n<p>reference on doc: <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-azure-machine-learning-architecture#computes\">concept-azure-machine-learning-architecture<\/a>    <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1600253902967,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/96915\/no-option-to-select-local-machine-for-compute-in-a",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"option select local machin comput creat space experi azur execut work need select comput target azur doc select local machin remot comput option select local machin option creat comput instanc comput cluster help option refer doc concept azur machin learn architectur",
        "Question_preprocessed_content":"option select local machin comput creat space experi azur execut work select comput target azur doc select local machin remot comput option select local machin option creat comput instanc comput cluster help option refer doc",
        "Question_gpt_summary_original":"The user is unable to select local machine as a compute target for their ML work in Azure ML, despite it being listed as an option in Azure documentation. They only have the options to create compute instance or compute clusters. The user is seeking help to understand why they don't have the option to select local machine.",
        "Question_gpt_summary":"user unabl select local machin comput target work despit list option azur document option creat comput instanc comput cluster user seek help understand option select local machin"
    },
    {
        "Question_title":"Does Azure Cognitive Service provide a way to detect depression (or mental health) from a text?",
        "Question_body":"<p>I'm doing research that aims to detect if social media users are depressed or\/and have mental health issues. Does Cognitive Service and its sentiment analysis models provide an API for this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1668695329173,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hello <a href=\"\/users\/na\/?userid=7518a830-4fa1-4aa7-867e-fad36ac3156a\">@xiriro  <\/a>     <\/p>\n<p>Thanks for using Microsoft Q&amp;A platform. Sentimnt Analysis of Azure Cognitive Service does not support Depression Detection at this moment, it can only returen positive, neutral and negative labels, which may not enough for depression from my personal experience.    <\/p>\n<p>But I do see Microsoft Research group is working on this case - <a href=\"https:\/\/www.microsoft.com\/en-us\/research\/project\/technology-for-mental-health-and-well-being-interventions\/\">https:\/\/www.microsoft.com\/en-us\/research\/project\/technology-for-mental-health-and-well-being-interventions\/<\/a>    <\/p>\n<p>And also I see some external Microsoft resource about this topic you may be interested in - <a href=\"https:\/\/www.youtube.com\/watch?v=HzlOkaGHZSg&amp;t=1432s\">https:\/\/www.youtube.com\/watch?v=HzlOkaGHZSg&amp;t=1432s<\/a>    <\/p>\n<p>I hope this helps and thank you for your product feedback, I will bring this feature to product team for future considerations.    <\/p>\n<p>Regards,    <br \/>\nYutong    <\/p>\n<p>-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.<\/p>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1668715953530,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1093496\/does-azure-cognitive-service-provide-a-way-to-dete",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur cognit servic provid wai detect depress mental health text research aim detect social media user depress mental health issu cognit servic sentiment analysi model provid api",
        "Question_preprocessed_content":"azur cognit servic provid wai detect text research aim detect social media user mental health cognit servic sentiment analysi model provid api",
        "Question_gpt_summary_original":"The user is conducting research to detect depression and mental health issues in social media users and is wondering if Azure Cognitive Service provides an API for sentiment analysis to help with this task.",
        "Question_gpt_summary":"user conduct research detect depress mental health issu social media user wonder azur cognit servic provid api sentiment analysi help task"
    },
    {
        "Question_title":"How to add more photos to AWS Sagemaker Labelling job?",
        "Question_body":"<p>We wish to add more photos to the AWS Sagemaker Labelling job. We tried adding the photo's directly to the s3 path and editing the input manifest file but they don't seem to update on the Labelling task. Is there any workaround for it?<\/p>\n<p>Reference used: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-input-data-input-manifest.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-input-data-input-manifest.html<\/a><\/p>\n<p>On a side note: Is there any reference for chaining the jobs, since the documentation on Amazon is confusing and there are no practical examples out there either.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1631557077603,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1632136492540,
        "Question_score_count":1.0,
        "Question_view_count":51.0,
        "Poster_created_time":1631533148667,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69167374",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"add photo label job wish add photo label job tri ad photo directli path edit input manifest file updat label task workaround refer http doc aw amazon com latest sm input data input manifest html note refer chain job document amazon confus practic exampl",
        "Question_preprocessed_content":"photo job wish photo job tri photo directli path edit input manifest file updat task workaround refer note refer chain job document amazon confus practic exampl",
        "Question_gpt_summary_original":"The user is facing challenges in adding more photos to their AWS Sagemaker Labelling job. They have tried adding the photos directly to the s3 path and editing the input manifest file, but the Labelling task does not update. They are looking for a workaround. Additionally, they are seeking references for chaining jobs, as the documentation on Amazon is confusing and lacks practical examples.",
        "Question_gpt_summary":"user face challeng ad photo label job tri ad photo directli path edit input manifest file label task updat look workaround addition seek refer chain job document amazon confus lack practic exampl"
    },
    {
        "Question_title":"Import of exported Vertex-AI AutoML model in production fails",
        "Question_body":"<p>I want to deploy a Vertex-AI model in a production project which has been trained in a training project.<\/p>\n<pre><code>----TRAINING PRJ-----     --------PRODUCTION PRJ---------\nTrain &gt; test &gt; export  &gt;  import &gt; deploy | batch predict\n<\/code><\/pre>\n<p>I follow <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-registry\/import-model#automl-tabular-container\" rel=\"nofollow noreferrer\">these instructions<\/a> and get a success email:<\/p>\n<blockquote>\n<p>Vertex AI finished uploading model &quot;xxxxxx&quot;.<\/p>\n<p>Operation State: Succeeded<\/p>\n<\/blockquote>\n<p>but when I try to test the model with a Batch prediction I always get a failed message:<\/p>\n<blockquote>\n<p>Due to an error, Vertex AI was unable to batch predict using model &quot;TEST&quot;.<\/p>\n<p>Operation State: Failed with errors Error<\/p>\n<p>Messages: INTERNAL<\/p>\n<\/blockquote>\n<p>Please note deploying the model to an endpoint and testing with a JSON request it <strong>does provide the expected response<\/strong>.<\/p>\n<p>I tried several container types besides the one suggested <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/model-registry\/import-model#automl-tabular-container\" rel=\"nofollow noreferrer\">here<\/a>, included the one stated in the exported model's <code>environment.json<\/code> <code>container_uri<\/code>: <strong>the batch prediction always fails with message INTERNAL<\/strong><\/p>\n<p>Any clue?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/EXrc0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EXrc0.png\" alt=\"Vertex-AI screenshot\" \/><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/t6wWJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t6wWJ.png\" alt=\"Model details\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6.0,
        "Question_created_time":1651748590180,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1651828025243,
        "Question_score_count":0.0,
        "Question_view_count":164.0,
        "Poster_created_time":1270548910492,
        "Poster_location":"Milan, Italy",
        "Poster_reputation_count":10832.0,
        "Poster_view_count":761.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72126124",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"import export vertex automl model product fail want deploi vertex model product project train train project train prj product prj train test export import deploi batch predict follow instruct success email finish upload model xxxxxx oper state succeed try test model batch predict fail messag error unabl batch predict model test oper state fail error error messag intern note deploi model endpoint test json request provid expect respons tri contain type suggest includ state export model environ json contain uri batch predict fail messag intern clue",
        "Question_preprocessed_content":"import export automl model product fail want deploi model product project train train project instruct email finish upload model oper state try test model batch predict fail unabl batch predict model test oper state fail intern note deploi model endpoint test json request provid expect respons tri contain type includ state export model batch predict fail intern clue",
        "Question_gpt_summary_original":"The user is facing challenges while trying to import an exported Vertex-AI AutoML model in production. Although the model has been successfully uploaded, the batch prediction always fails with an \"INTERNAL\" error message. The user has tried different container types but has not been successful. However, deploying the model to an endpoint and testing with a JSON request provides the expected response.",
        "Question_gpt_summary":"user face challeng try import export vertex automl model product model successfulli upload batch predict fail intern error messag user tri differ contain type success deploi model endpoint test json request provid expect respons"
    },
    {
        "Question_title":"1 minute Service timeout for AMLS models deployed on ACI or AKS",
        "Question_body":"<p>We have created an image scoring model on Machine learning Service and deployed using AMLS portal on ACI and AKS both.\nThough it runs on smaller images , for larger images it gets timed-out after exactly 1 minute on both ACI and AKS.\nIt is expected that an image scoring can take few minutes.<\/p>\n\n<p>Wanted to know , if it\u2019s a limitation on using AMLS deployment,  or on ACI and AKS that they timeout the deployed webservice after 60 seconds??\nAny workaround would be welcomed<\/p>\n\n<p>ACI Error :-\n Post <a href=\"http:\/\/localhost:5001\/score\" rel=\"nofollow noreferrer\">http:\/\/localhost:5001\/score<\/a>: net\/http: request canceled (Client.Timeout exceeded while awaiting headers)<\/p>\n\n<p>AKS Error :-\n Replica closed connection before replying<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_created_time":1569251056157,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":1976.0,
        "Poster_created_time":1508663110972,
        "Poster_location":null,
        "Poster_reputation_count":55.0,
        "Poster_view_count":16.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58065258",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"minut servic timeout aml model deploi aci ak creat imag score model machin learn servic deploi aml portal aci ak run smaller imag larger imag get time exactli minut aci ak expect imag score minut want know limit aml deploy aci ak timeout deploi webservic second workaround welcom aci error post http localhost score net http request cancel client timeout exceed await header ak error replica close connect repli",
        "Question_preprocessed_content":"minut servic timeout aml model deploi aci ak creat imag score model machin learn servic deploi aml portal aci ak run imag larger imag get exactli minut aci ak expect imag score minut want know limit aml deploy aci ak timeout deploi webservic second workaround welcom aci post request cancel ak replica close repli",
        "Question_gpt_summary_original":"The user has created an image scoring model on Machine Learning Service and deployed it using AMLS portal on ACI and AKS. However, the model gets timed-out after exactly 1 minute on both ACI and AKS when running on larger images. The user is unsure if this is a limitation on using AMLS deployment or on ACI and AKS that they timeout the deployed webservice after 60 seconds. The user is seeking a workaround for this issue.",
        "Question_gpt_summary":"user creat imag score model machin learn servic deploi aml portal aci ak model get time exactli minut aci ak run larger imag user unsur limit aml deploy aci ak timeout deploi webservic second user seek workaround issu"
    },
    {
        "Question_title":"How to store my artifcats in the my mlflow tracking server's artifact-root?",
        "Question_body":"<pre><code>mlflow server --backend-store-uri sqlite:\/\/\/mlflow.db --default-artifact-root $(pwd)\/artifacts --host 0.0.0.0 --port 8000\n<\/code><\/pre>\n<p>I ran this command, so i thought that my default artifact root is tracking server's execution location.<\/p>\n<pre><code>#\/bin\/bash \nexport MLFLOW_TRACKING_URI=&quot;http:\/\/localhost:8000&quot;\nmlflow run mlflow_docker\/ \\\n    --experiment-name AIT.DL.YOLOv5\/yolov5 \\\n    -A gpus=all \\\n    -A volume=&quot;\/home\/chaejin:\/data\/yolov5&quot; \\\n    -A v=&quot;${PWD}:\/usr\/src\/app&quot; \\\n    -A network=host\n<\/code><\/pre>\n<p>And I ran my MLproject in a docker env using shell script..\nThis location was different from tracking server execution location.<\/p>\n<p>As a result, my artifacts was stored in the Shell script location Not my default-artifacts-root.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/80efI.png\" rel=\"nofollow noreferrer\">my MLproject execution location<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bLpjG.png\" rel=\"nofollow noreferrer\">my Tracking server location<\/a><\/p>\n<p>I wanna storing my default artifacts location.. Please help me<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1646195615057,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":283.0,
        "Poster_created_time":1646195180140,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71317506",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"store artifcat track server artifact root server backend store uri sqlite default artifact root pwd artifact host port ran command thought default artifact root track server execut locat bin bash export track uri http localhost run docker experi ait yolov yolov gpu volum home chaejin data yolov pwd usr src app network host ran mlproject docker env shell script locat differ track server execut locat result artifact store shell script locat default artifact root mlproject execut locat track server locat wanna store default artifact locat help",
        "Question_preprocessed_content":"store artifcat track server ran thought default artifact track server execut locat ran mlproject docker env locat track server execut locat result artifact store script locat mlproject execut locat track server locat store default artifact help",
        "Question_gpt_summary_original":"The user ran a command to set the default artifact root for their mlflow tracking server, but when they ran their MLproject in a docker environment using a shell script, the artifacts were stored in the shell script location instead of the default-artifacts-root. The user is seeking help to store their artifacts in the default location.",
        "Question_gpt_summary":"user ran command set default artifact root track server ran mlproject docker environ shell script artifact store shell script locat instead default artifact root user seek help store artifact default locat"
    },
    {
        "Question_title":"How to move DVC files",
        "Question_body":"<p>Search for this topic, I found <a href=\"https:\/\/discuss.dvc.org\/t\/dvc-heartbeat-discord-gems\/159\/2\">this summary post<\/a> but I cannot open the original question (discord tells me it cannot find it) so I\u2019m creating a new topic. Apologies in advance.<\/p>\n<p>I tried following the instructions given i.e. I ran<\/p>\n<pre><code class=\"lang-auto\">mv my_file.dvc dvc_links\/my_file.dvc\n<\/code><\/pre>\n<p>and edited the working directory in the moved file.<br>\nHowever running <code>dvc status<\/code> gives<\/p>\n<pre><code class=\"lang-auto\">dvc_links\/my_file.dvc:\n        changed checksum\n<\/code><\/pre>\n<p>How do I get dvc to update the checksum in the relocated file? I even tried removing the md5: xx line at the top of the new file, calculating the md5 sum and reinserting it but this didn\u2019t work.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1566355790691,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":1341.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-move-dvc-files\/205",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"file search topic summari post open origin question discord tell creat new topic apolog advanc tri follow instruct given ran file link file edit work directori move file run statu give link file chang checksum updat checksum reloc file tri remov line new file calcul sum reinsert didnt work",
        "Question_preprocessed_content":"file search topic post open origin question creat new topic apolog advanc tri instruct given ran edit work directori move file give updat checksum reloc file tri remov line new file calcul sum reinsert didnt work",
        "Question_gpt_summary_original":"The user is trying to move DVC files by following the instructions given in a summary post. However, after moving the file and editing the working directory, running \"dvc status\" gives an error message indicating a changed checksum. The user is seeking help on how to update the checksum in the relocated file.",
        "Question_gpt_summary":"user try file follow instruct given summari post move file edit work directori run statu give error messag indic chang checksum user seek help updat checksum reloc file"
    },
    {
        "Question_title":"AzureML pass data between pipeline without saving it",
        "Question_body":"<p>I have made two scripts using PythonScriptStep where data_prep.py prepares a dataset by doing some data transformation which is thereafter sent to train.py for training an ML model in AzureML.<\/p>\n<p>It is possible passing data between pipeline steps using PipelineData and OutputFileDatasetConfig, however these seem to save the data in azure blob.<\/p>\n<p>Q: How can I send the data between the steps <strong>without<\/strong> saving the data anywhere?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1629355183933,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":100.0,
        "Poster_created_time":1629354819276,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68843120",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pass data pipelin save script pythonscriptstep data prep prepar dataset data transform sent train train model possibl pass data pipelin step pipelinedata outputfiledatasetconfig save data azur blob send data step save data",
        "Question_preprocessed_content":"data pipelin save script pythonscriptstep prepar dataset data transform sent train model data pipelin step pipelinedata outputfiledatasetconfig save data azur blob send data step save data",
        "Question_gpt_summary_original":"The user is trying to pass data between two PythonScriptSteps in AzureML without saving it in Azure Blob. They have tried using PipelineData and OutputFileDatasetConfig, but these options save the data in Azure Blob. The user is seeking a solution to pass data between pipeline steps without saving it anywhere.",
        "Question_gpt_summary":"user try pass data pythonscriptstep save azur blob tri pipelinedata outputfiledatasetconfig option save data azur blob user seek solut pass data pipelin step save"
    },
    {
        "Question_title":"How to speed up batch delete of files & artifacts?",
        "Question_body":"<p>I have been concerned with trying to stay within the 100GB limit on files and artifact storage imposed by Wandb, so I have the idea to delete files &amp; artifacts on old runs.<\/p>\n<p>However, I do not want to delete all files on those runs! It is definitely useful to be able to see the progression of generated files over time. I don\u2019t need to see all 50,000 or so logged steps on each run, but I\u2019ll just keep 100 of them evenly spaced in time. so I programmed a script to do that by indexing all my files on Wandb using the Python API, grouping them, sorting them, and selecting files to delete.<\/p>\n<p>My issue comes with how slow the current API seems to be to delete files &amp; artifacts: Using <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/file#delete\">File.delete<\/a>, it takes around 2s per file. With hundreds of runs and tens of thousands of files per run, I am then looking at weeks of time needed to delete the files I need to delete.<\/p>\n<p>I then tried to refactor my code into parallel workers, thinking I could increase that speed several fold, but I quickly ran into the 200 call\/minute <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/limits#rate-limits\">rate limit<\/a>. It even started to affect my ongoing runs.<\/p>\n<p>Is there any better way I could prune the files &amp; artifacts so that I could have the process complete faster?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_created_time":1682048482710,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":166.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-speed-up-batch-delete-of-files-artifacts\/4251",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"speed batch delet file artifact concern try stai limit file artifact storag impos idea delet file artifact old run want delet file run definit us abl progress gener file time dont need log step run ill evenli space time program script index file python api group sort select file delet issu come slow current api delet file artifact file delet take file hundr run ten thousand file run look week time need delet file need delet tri refactor code parallel worker think increas speed fold quickli ran minut rate limit start affect ongo run better wai prune file artifact process complet faster",
        "Question_preprocessed_content":"batch delet file artifact concern try stai limit file artifact storag impos idea delet file artifact old run want delet file run definit us abl gener file time dont step run evenli space time script index file python api group sort select file delet come slow api delet file artifact take file hundr run ten thousand file run time delet file delet tri refactor code worker think increas fold quickli ran minut rate limit start ongo run wai prune file artifact complet faster",
        "Question_gpt_summary_original":"The user is trying to stay within the 100GB limit on files and artifact storage imposed by Wandb and wants to delete old runs' files and artifacts. However, the current API is slow, taking around 2s per file, and with hundreds of runs and tens of thousands of files per run, it would take weeks to delete the necessary files. The user tried to refactor the code into parallel workers, but it ran into the 200 call\/minute rate limit. The user is looking for a better way to prune the files and artifacts to complete the process faster.",
        "Question_gpt_summary":"user try stai limit file artifact storag impos want delet old run file artifact current api slow take file hundr run ten thousand file run week delet necessari file user tri refactor code parallel worker ran minut rate limit user look better wai prune file artifact complet process faster"
    },
    {
        "Question_title":"Translating streaming audio into text",
        "Question_body":"Hi, I'm using\u00a0@Google-cloud\/media-translation in node with express js server. I want to translate media file (\".wav\" format) with media-translation. At first, i got an error because of authentication and I fixed it with env variable as specified in documentation, I followed each and every step exactly told in the documentation but I'm getting no response from server. When i looked into APIs & Services tab it only recorded my failed auth attempts no other API calls are recorded. Please help because there is no help available online about this product and it doesn't even send error responses so i can debug.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1649687580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":50.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Translating-streaming-audio-into-text\/td-p\/412679\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"translat stream audio text googl cloud media translat node express server want translat media file wav format media translat got error authent fix env variabl specifi document follow step exactli told document get respons server look api servic tab record fail auth attempt api call record help help avail onlin product send error respons debug",
        "Question_preprocessed_content":"translat stream audio text node server want translat media file got authent fix env variabl specifi document step exactli told document respons server api servic tab record fail auth api record help help avail onlin product send respons debug",
        "Question_gpt_summary_original":"The user is facing challenges in translating a media file in \".wav\" format into text using Google Cloud's media translation service in node with express js server. Although the user has fixed the authentication error, they are not receiving any response from the server and no API calls are being recorded. The user is seeking help as there is no online support available for this product and there are no error responses to debug the issue.",
        "Question_gpt_summary":"user face challeng translat media file wav format text googl cloud media translat servic node express server user fix authent error receiv respons server api call record user seek help onlin support avail product error respons debug issu"
    },
    {
        "Question_title":"Do Training Jobs Run in Parallel? (VERTEX AI)",
        "Question_body":"I am wondering if training jobs on vertex AI run in parallel, based on my tests it seems they do but wondering if anyone can confirm this is true as the number of concurrent jobs grows past say 1000.\n\n\u00a0\n\nThanks!",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1668498960000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":154.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Yes training jobs run in parallel but the concurrency is subject to quota. See Vertex AI quota document.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Question_closed_time":"11-15-2022 11:57 AM",
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Do-Training-Jobs-Run-in-Parallel-VERTEX-AI\/td-p\/489639\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"train job run parallel wonder train job run parallel base test wonder confirm true number concurr job grow past thank",
        "Question_preprocessed_content":"train job run wonder train job run base test wonder confirm true number job grow past thank",
        "Question_gpt_summary_original":"The user is questioning whether training jobs on Vertex AI run in parallel, as their tests suggest they do. They are seeking confirmation if this is true, particularly when the number of concurrent jobs exceeds 1000.",
        "Question_gpt_summary":"user question train job run parallel test suggest seek confirm true particularli number concurr job exce"
    },
    {
        "Question_title":"Document AI fails for one particular image, else works great",
        "Question_body":"We are delivering a platform to a customer based on Document AI. The use case it to send a lottery ticket via API and return the structure information using Document AI. We tried for several hundred images and the Document AI OCR worked great (95%+ times captured right string, only errors were line feeds and Q turning into O etc. that we could resolve using a post-processor). But for one set of images (from DC), the OCR fails miserably.\u00a0 This is a corner case that seems to throw the Document AI engine off the mark.I will appreciate greatly if anyone can help explain it.\n\nSee one particular image which is the most problematic.",
        "Question_answer_count":7,
        "Question_comment_count":null,
        "Question_created_time":1651497900000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":139.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Document-AI-fails-for-one-particular-image-else-works-great\/td-p\/419233\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"document fail particular imag work great deliv platform custom base document us case send lotteri ticket api return structur inform document tri imag document ocr work great time captur right string error line feed turn resolv post processor set imag ocr fail miser corner case throw document engin mark appreci greatli help explain particular imag problemat",
        "Question_preprocessed_content":"document fail particular imag work great deliv platform custom base document us case send ticket api return structur inform document tri imag document ocr work great set imag ocr fail miser corner case throw document engin greatli help explain particular imag problemat",
        "Question_gpt_summary_original":"The user is facing an issue with Document AI where it fails to capture the correct information for a set of images from DC, despite working well for several hundred other images. The OCR fails miserably for this particular set of images, which seems to be a corner case that throws off the Document AI engine. The user is seeking help to understand and resolve this issue.",
        "Question_gpt_summary":"user face issu document fail captur correct inform set imag despit work imag ocr fail miser particular set imag corner case throw document engin user seek help understand resolv issu"
    },
    {
        "Question_title":"Optuna pdf uses dtrain = lgb.Dataset(train_x, label=train_y) - but what is the equivalent for xgb",
        "Question_body":"<p>I really got a lot out of the Optuna documentation pdf <a href=\"https:\/\/buildmedia.readthedocs.org\/media\/pdf\/optuna\/stable\/optuna.pdf\" rel=\"nofollow noreferrer\">https:\/\/buildmedia.readthedocs.org\/media\/pdf\/optuna\/stable\/optuna.pdf<\/a> and followed it closely for the lightgbm training on the breast cancer data set.  I tried to follow the same procedure using xgboost on antoher data set but xgboost does not seem to have a similar command to the lightgbm.Dataset(X,y) command.  Does anyone know what the parallel command would be for xgboost?  Thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4.0,
        "Question_created_time":1635808680633,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":55.0,
        "Poster_created_time":1362066118236,
        "Poster_location":null,
        "Poster_reputation_count":143.0,
        "Poster_view_count":42.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69803770",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pdf us dtrain lgb dataset train label train equival xgb got lot document pdf http buildmedia readthedoc org media pdf stabl pdf follow close lightgbm train breast cancer data set tri follow procedur xgboost antoh data set xgboost similar command lightgbm dataset command know parallel command xgboost thank",
        "Question_preprocessed_content":"pdf us dtrain equival xgb got lot document pdf close lightgbm train breast cancer data set tri procedur antoh data set similar know thank",
        "Question_gpt_summary_original":"The user is seeking help in finding the equivalent command for xgboost to replace the lightgbm.Dataset() command used in the Optuna documentation pdf for training on a different dataset.",
        "Question_gpt_summary":"user seek help find equival command xgboost replac lightgbm dataset command document pdf train differ dataset"
    },
    {
        "Question_title":"Action on error in Azure Machine Learning pipeline",
        "Question_body":"<p>I have a published and scheduled pipeline running at regular intervals. Some times, the pipeline may fail (for example if the datastore is offline for maintenance). Is there a way to specify the scheduled pipeline to perform a certain action if the pipeline fails for any reason? Actions could be to send me an email, try to run again in a few hours later or invoke a webhook. As it is now, I have to manually check the status of our production pipeline at regular intervals, and this is sub-optimal for obvious reasons. I could of course instruct every script in my pipeline to perform certain actions if they fail for whatever reason, but it would be cleaner and easier to specify it globally for the pipeline schedule (or the pipeline itself).<\/p>\n<p>Possible sub-optimal solutions could be:<\/p>\n<ul>\n<li>Setting up an Azure Logic App to invoke the pipeline<\/li>\n<li>Setting a cron job or Azure Scheduler<\/li>\n<li>Setting up a second Azure Machine Learning pipeline on a schedule that triggers the pipeline, monitors the output and performs relevant actions if errors are encountered<\/li>\n<\/ul>\n<p>All the solutions above suffers from being convoluted and not very clean - surely there must exist a simple, clean solution for this problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1607593531447,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":357.0,
        "Poster_created_time":1307442430756,
        "Poster_location":null,
        "Poster_reputation_count":2042.0,
        "Poster_view_count":80.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65232059",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"action error pipelin publish schedul pipelin run regular interv time pipelin fail exampl datastor offlin mainten wai specifi schedul pipelin perform certain action pipelin fail reason action send email try run hour later invok webhook manual check statu product pipelin regular interv sub optim obviou reason cours instruct script pipelin perform certain action fail reason cleaner easier specifi global pipelin schedul pipelin possibl sub optim solut set azur logic app invok pipelin set cron job azur schedul set second pipelin schedul trigger pipelin monitor output perform relev action error encount solut suffer convolut clean sure exist simpl clean solut problem",
        "Question_preprocessed_content":"action pipelin publish schedul pipelin regular interv time pipelin fail wai specifi schedul pipelin perform certain action pipelin fail reason action send email try run hour later invok check statu product pipelin regular interv obviou reason cours instruct script pipelin perform certain action fail reason cleaner easier specifi pipelin schedul solut azur logic invok pipelin cron job azur schedul second pipelin schedul pipelin monitor output perform relev action encount solut convolut clean sure exist simpl clean solut problem",
        "Question_gpt_summary_original":"The user has a published and scheduled pipeline in Azure Machine Learning that sometimes fails due to reasons such as offline data store maintenance. The user is looking for a way to specify an action to be performed if the pipeline fails, such as sending an email, retrying after a few hours, or invoking a webhook. The user is currently manually checking the pipeline status, which is sub-optimal. The user is looking for a global solution for the pipeline schedule or the pipeline itself, rather than instructing every script in the pipeline to perform certain actions if they fail. The user has considered sub-optimal solutions such as setting up an Azure Logic App, a cron job, or a second pipeline to monitor and perform actions if errors are encountered.",
        "Question_gpt_summary":"user publish schedul pipelin fail reason offlin data store mainten user look wai specifi action perform pipelin fail send email retri hour invok webhook user current manual check pipelin statu sub optim user look global solut pipelin schedul pipelin instruct script pipelin perform certain action fail user consid sub optim solut set azur logic app cron job second pipelin monitor perform action error encount"
    },
    {
        "Question_title":"MLFlow UI issue when running in docker",
        "Question_body":"I installed mlflow and started the ui with no issues on my windows 10 machine in an anaconda python 3.7 environment and am able to access the UI via http:\/\/localhost:5000\nHowever, when doing the exact same thing within the anacanda3 docker container the UI doesn't appear to be rendering\/responding.\nMy docker run command includes the proper port and I am able to exec into the container, install mlflow and start the ui (>mlflow ui) without any errors.\n\n\ndocker run --name conda3 -d -t -v \/\/c\/\/develop:\/develop -p 5000:5000 continuumio\/anaconda3\\\n\nWhen I try to access\u00a0http:\/\/localhost:5000\u00a0(or\u00a0http:\/\/127.0.0.1:5000\/), the response in the browser is \"ERR_EMPTY_RESPONSE\".\nI tried to access the UI within the contain via lynx, to confirm that it is running, however, the response is just a warning that this site is javascript which can't be rendered in lynx.\n\nI also tried the above docker approach using python 3.6 and also utilizing the docker container I normally develop with but in both cases I still get \"ERR_EMPTY_RESPONSE\".\n\nI am pretty much stuck at this point so any suggestions will be appreciated. Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1575115725000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":867.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/7cezJGzfnic",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"issu run docker instal start issu window machin anaconda python environ abl access http localhost exact thing anacanda docker contain appear render respond docker run command includ proper port abl exec contain instal start error docker run conda develop develop continuumio anaconda try accesshttp localhost orhttp respons browser err respons tri access contain lynx confirm run respons warn site javascript render lynx tri docker approach python util docker contain normal develop case err respons pretti stuck point suggest appreci thank",
        "Question_preprocessed_content":"docker start window machin anaconda python environ abl exact thing anacanda docker contain docker run includ proper port abl exec contain start docker run conda anaconda try respons browser tri contain lynx confirm respons warn site javascript render lynx tri docker python util docker contain develop case stuck point thank",
        "Question_gpt_summary_original":"The user is facing an issue with the MLFlow UI not rendering\/responding when running in a docker container. The user has tried accessing the UI via the proper port but is receiving an \"ERR_EMPTY_RESPONSE\" message in the browser. The user has also tried accessing the UI within the container via lynx but is unable to render the javascript. The user has tried different docker approaches but is still facing the same issue. The user is seeking suggestions to resolve the issue.",
        "Question_gpt_summary":"user face issu render respond run docker contain user tri access proper port receiv err respons messag browser user tri access contain lynx unabl render javascript user tri differ docker approach face issu user seek suggest resolv issu"
    },
    {
        "Question_title":"Can I add Component Tags via Input Params?",
        "Question_body":"I have no idea if this is possible, but I found myself wanting to do the following.\n\nIn words, I want to add component tags based on matrix parameters. The following yaml doesn't work, but I think it illustrates what I want.\n\nversion: 1.1\nkind: operation\nmatrix:\n  kind: grid\n  params:\n    sota:\n      kind: choice\n      value:\n        - model: ModelA\n          max_epochs: 200\n          dataset_hash: ae43ff\n        - model: ModelB\n          max_epochs: 150\n          dataset_hash: 33fba2\ncomponent:\n  name: model-trainer\n  tags:\n    - dataset_update_retrain\n    - \"{{ sota.model }}\"  #<- I want to set this dynamically based on the job input\n  inputs:\n    - name: sota\n      type: dict\n      isOptional: false\n  run:\n    kind: job\n    container:\n      args: >\n        set -x;\n       train \\\n          hydra.run.dir={{ globals.run_outputs_path }} \\\n          model={{ sota.model }} \\\n            model.datamodule.dataset_hash={{ sota.dataset_hash }}\/ \\\n            model.datamodule.data_directory=\/data\/{{ sota.model }}\/ \\\n          trainer=gpu \\\n            trainer.max_epochs={{ sota.max_epochs }} \\\n      ...\n\nAs the above pattern doesn't work, I am either doing it wrong or it isn't possible in the config. Could I potentially set ENV vars that could get picked up by the tracking client?\n\nThanks a ton for the help!",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1667585195000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1518",
        "Tool":"Polyaxon",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"add compon tag input param idea possibl want follow word want add compon tag base matrix paramet follow yaml work think illustr want version kind oper matrix kind grid param sota kind choic valu model modela max epoch dataset hash aeff model modelb max epoch dataset hash fba compon model trainer tag dataset updat retrain sota model set train hydra run dir global run output path model sota model model datamodul dataset hash sota dataset hash model datamodul data directori data sota model trainer gpu trainer max epoch sota max epoch pattern work wrong isn possibl config potenti set env var pick track client thank ton help",
        "Question_preprocessed_content":"compon tag input param idea want word want compon tag base matrix paramet yaml work think want version kind oper matrix kind grid param sota kind choic valu model modela model modelb fba compon tag want set base job input input sota type dict isopt fals run kind job contain arg set train model trainer gpu work wrong isn config set env var pick track client thank ton help",
        "Question_gpt_summary_original":"The user is unsure if it is possible to add component tags based on matrix parameters in their YAML configuration file. They have attempted to do so but it did not work, and they are seeking advice on whether it is possible or if there is an alternative solution such as setting environment variables.",
        "Question_gpt_summary":"user unsur possibl add compon tag base matrix paramet yaml configur file attempt work seek advic possibl altern solut set environ variabl"
    },
    {
        "Question_title":"Building an autoencoder with Keras and Kedro",
        "Question_body":"<p>I'm trying to build an autoencoder, which I'm sure I'm doing something wrong. I tried separating the creation of the model from the actual training but this is not really working out for me and is giving me the following error.<\/p>\n<pre><code>AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 310), dtype=tf.float32, name=None), name='dense_7\/Sigmoid:0', description=&quot;created by layer 'dense_7'&quot;)\n<\/code><\/pre>\n<p>I'm doing this all using the Kedro framework. I have a pipeline.py file with the pipeline definition and a nodes.py with the functions that I want to use. So far, this is my project structure:<\/p>\n<p>pipelines.py:<\/p>\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes.autoencoder_nodes import *\n\ndef train_autoencoder_pipeline():\n    return Pipeline([\n        # Build neural network\n        node(\n            build_models, \n            inputs=[\n                &quot;train_x&quot;, \n                &quot;params:autoencoder_n_hidden_layers&quot;,\n                &quot;params:autoencoder_latent_space_size&quot;,\n                &quot;params:autoencoder_regularization_strength&quot;,\n                &quot;params:seed&quot;\n                ],\n            outputs=dict(\n                pre_train_autoencoder=&quot;pre_train_autoencoder&quot;,\n                pre_train_encoder=&quot;pre_train_encoder&quot;,\n                pre_train_decoder=&quot;pre_train_decoder&quot;\n            ), name=&quot;autoencoder-create-models&quot;\n        ),\n        # Scale features\n        node(fit_scaler, inputs=&quot;train_x&quot;, outputs=&quot;autoencoder_scaler&quot;, name=&quot;autoencoder-fit-scaler&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;train_x&quot;], outputs=&quot;autoencoder_scaled_train_x&quot;, name=&quot;autoencoder-scale-train&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;test_x&quot;], outputs=&quot;autoencoder_scaled_test_x&quot;, name=&quot;autoencoder-scale-test&quot;),\n\n        # Train autoencoder\n        node(\n            train_autoencoder, \n            inputs=[\n                &quot;autoencoder_scaled_train_x&quot;,\n                &quot;autoencoder_scaled_test_x&quot;,\n                &quot;pre_train_autoencoder&quot;, \n                &quot;pre_train_encoder&quot;, \n                &quot;pre_train_decoder&quot;,\n                &quot;params:autoencoder_epochs&quot;,\n                &quot;params:autoencoder_batch_size&quot;,\n                &quot;params:seed&quot;\n            ],\n            outputs= dict(\n                autoencoder=&quot;autoencoder&quot;,\n                encoder=&quot;encoder&quot;,\n                decoder=&quot;decoder&quot;,\n                autoencoder_history=&quot;autoencoder_history&quot;,\n            ),\n            name=&quot;autoencoder-train-model&quot;\n        )])\n<\/code><\/pre>\n<p>nodes.py:<\/p>\n<pre><code>from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom typing import Dict, Any, Tuple\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport logging\n\n\ndef build_models(data: pd.DataFrame, n_hidden_layers: int, latent_space_size: int, retularization_stregth: float, seed: int) -&gt; Tuple[keras.Model, keras.Model, keras.Model]:\n    assert n_hidden_layers &gt;= 1, &quot;There must be at least 1 hidden layer for the autoencoder&quot;\n    \n    n_features = data.shape[1]\n    tf.random.set_seed(seed)\n    input_layer = keras.Input(shape=(n_features,))\n    \n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(input_layer)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n    encoded = keras.layers.Dense(latent_space_size, activation=&quot;sigmoid&quot;)(hidden)\n\n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(encoded)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n\n    decoded = keras.layers.Dense(n_features, activation=&quot;sigmoid&quot;)(hidden)\n\n    # Defines the neural networks\n    autoencoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    encoder = keras.models.Model(inputs=input_layer, outputs=encoded)\n    decoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    autoencoder.compile(optimizer=&quot;adam&quot;, loss=&quot;mean_absolute_error&quot;)\n\n    return dict(\n        pre_train_autoencoder=autoencoder,\n        pre_train_encoder=encoder,\n        pre_train_decoder=decoder\n    )\n\ndef fit_scaler(data: pd.DataFrame) -&gt; MinMaxScaler:\n    scaler = MinMaxScaler()\n    scaler.fit(data)\n    return scaler\n\ndef tranform_scaler(scaler: MinMaxScaler, data: pd.DataFrame) -&gt; np.array:\n    return scaler.transform(data)\n\ndef train_autoencoder(\n    train_x: pd.DataFrame, test_x: pd.DataFrame, \n    autoencoder: keras.Model, encoder: keras.Model, decoder: keras.Model, \n    epochs: int, batch_size: int, seed: int) -&gt; Dict[str, Any]:\n\n    tf.random.set_seed(seed)\n    callbacks = [\n        keras.callbacks.History(),\n        keras.callbacks.EarlyStopping(patience=3)\n    ]\n    logging.info(train_x.shape)\n    logging.info(test_x.shape)\n\n    history = autoencoder.fit(\n        train_x, train_x,\n        validation_data=(test_x, test_x),\n        callbacks=callbacks, \n        epochs=epochs,\n        batch_size=batch_size\n    )\n\n    return dict(\n        autoencoder=autoencoder,\n        encoder=encoder,\n        decoder=decoder,\n        autoencoder_history=history,\n    )\n<\/code><\/pre>\n<p>catalog.yaml:<\/p>\n<pre><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n\nautoencoder_train_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_train_x.csv\n\nautoencoder_test_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_test_x.csv\n<\/code><\/pre>\n<p>And finally parameters.yaml:<\/p>\n<pre><code>seed: 200\n# Autoencoder\nautoencoder_n_hidden_layers: 3\nautoencoder_latent_space_size: 15\nautoencoder_epochs: 100\nautoencoder_batch_size: 32\nautoencoder_regularization_strength: 0.001\n<\/code><\/pre>\n<p>I believe that Keras is not seeing the whole graph since they will be out of the scope for the buld_models function, but I'm not sure whether this is the case, or how to fix it. Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1629078707297,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":245.0,
        "Poster_created_time":1423164285360,
        "Poster_location":"Itabira, Brazil",
        "Poster_reputation_count":856.0,
        "Poster_view_count":106.0,
        "Answer_body":"<p>I was able to set up your project locally and reproduce the error. To fix it, I had to add the <code>pre_train_*<\/code> outputs to the catalog as well. Therefore, it's my <code>catalog.yaml<\/code> file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\npre_train_autoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_autoencoder.h5\n\npre_train_encoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_encoder.h5\n\npre_train_decoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_decoder.h5\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n<\/code><\/pre>\n<p>Also, I changed the return of <code>train_autoencoder<\/code> node to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>return dict(\n    autoencoder=autoencoder,\n    autoencoder_history=history.history,\n)\n<\/code><\/pre>\n<p>Note that I changed the <code>autoencoder_history<\/code> to return <code>history.history<\/code> since <code>MemoryDataset<\/code> can't pickle the object <code>history<\/code> by itself. The <code>history.history<\/code> is a dictionary with losses of train and validation sets.<\/p>\n<p>You can find the complete code <a href=\"https:\/\/gist.github.com\/arnaldog12\/a3f7801fe3910c02f4bcea8be61b910c\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":4.0,
        "Question_closed_time":1629132605447,
        "Answer_last_edit_time":1629217805312,
        "Answer_score_count":1.0,
        "Answerer_created_time":1341842709088,
        "Answerer_location":"Jo\u00e3o Pessoa - PB, Brasil",
        "Answerer_reputation_count":314.0,
        "Answerer_view_count":43.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68796641",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"build autoencod kera try build autoencod sure wrong tri separ creation model actual train work give follow error assertionerror comput output kerastensor type spec tensorspec shape dtype float dens sigmoid descript creat layer dens framework pipelin file pipelin definit node function want us far project structur pipelin pipelin import pipelin node node autoencod node import def train autoencod pipelin return pipelin build neural network node build model input train param autoencod hidden layer param autoencod latent space size param autoencod regular strength param seed output dict pre train autoencod pre train autoencod pre train encod pre train encod pre train decod pre train decod autoencod creat model scale featur node fit scaler input train output autoencod scaler autoencod fit scaler node tranform scaler input autoencod scaler train output autoencod scale train autoencod scale train node tranform scaler input autoencod scaler test output autoencod scale test autoencod scale test train autoencod node train autoencod input autoencod scale train autoencod scale test pre train autoencod pre train encod pre train decod param autoencod epoch param autoencod batch size param seed output dict autoencod autoencod encod encod decod decod autoencod histori autoencod histori autoencod train model node sklearn preprocess import minmaxscal tensorflow import kera import tensorflow type import dict tupl import matplotlib pyplot plt import panda import numpi import log def build model data datafram hidden layer int latent space size int retular stregth float seed int tupl kera model kera model kera model assert hidden layer hidden layer autoencod featur data shape random set seed seed input layer kera input shape featur hidden kera layer dens featur kernel regular kera regular retular stregth input layer hidden kera layer leakyrelu hidden rang hidden layer hidden kera layer dens featur kernel regular kera regular retular stregth hidden hidden kera layer leakyrelu hidden encod kera layer dens latent space size activ sigmoid hidden hidden kera layer dens featur kernel regular kera regular retular stregth encod hidden kera layer leakyrelu hidden rang hidden layer hidden kera layer dens featur kernel regular kera regular retular stregth hidden hidden kera layer leakyrelu hidden decod kera layer dens featur activ sigmoid hidden defin neural network autoencod kera model model input input layer output decod encod kera model model input input layer output encod decod kera model model input input layer output decod autoencod compil optim adam loss mean absolut error return dict pre train autoencod autoencod pre train encod encod pre train decod decod def fit scaler data datafram minmaxscal scaler minmaxscal scaler fit data return scaler def tranform scaler scaler minmaxscal data datafram arrai return scaler transform data def train autoencod train datafram test datafram autoencod kera model encod kera model decod kera model epoch int batch size int seed int dict str random set seed seed callback kera callback histori kera callback earlystop patienc log info train shape log info test shape histori autoencod fit train train valid data test test callback callback epoch epoch batch size batch size return dict autoencod autoencod encod encod decod decod autoencod histori histori catalog yaml autoencod scaler type pickl pickledataset filepath data model autoencod scaler pkl autoencod type extra dataset tensorflow tensorflowmodeldataset filepath data model autoencod encod type extra dataset tensorflow tensorflowmodeldataset filepath data model encod decod type extra dataset tensorflow tensorflowmodeldataset filepath data model decod autoencod train type panda csvdataset filepath data featur autoencod train csv autoencod test type panda csvdataset filepath data featur autoencod test csv final paramet yaml seed autoencod autoencod hidden layer autoencod latent space size autoencod epoch autoencod batch size autoencod regular strength believ kera see graph scope buld model function sure case fix help appreci",
        "Question_preprocessed_content":"build autoencod kera try build autoencod sure wrong tri separ creation model actual train work give framework file pipelin definit function want us far project structur believ kera graph scope function sure case fix help",
        "Question_gpt_summary_original":"The user is encountering an error while building an autoencoder using Keras and Kedro framework. They have defined the pipeline and functions in separate files and are receiving an AssertionError related to the output of a KerasTensor. The user suspects that Keras is not seeing the whole graph due to the scope of the build_models function. They are seeking help to fix the issue.",
        "Question_gpt_summary":"user encount error build autoencod kera framework defin pipelin function separ file receiv assertionerror relat output kerastensor user suspect kera see graph scope build model function seek help fix issu"
    },
    {
        "Question_title":"Error notification not working in background job Neptune Software",
        "Question_body":"<p>For a particular server script, we are adding a background job.\nIn that ,it has an option to add error notification emails ,which is not working.  There is error in my script, which I can see in job log but not getting any notifications on email.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1639300561957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1649766671956,
        "Question_score_count":0.0,
        "Question_view_count":36.0,
        "Poster_created_time":1583493572323,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":9.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70322291",
        "Tool":"Neptune",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error notif work background job softwar particular server script ad background job option add error notif email work error script job log get notif email",
        "Question_preprocessed_content":"notif work background job softwar particular server script background job option notif email work script job log notif email",
        "Question_gpt_summary_original":"The user is facing an issue with error notification emails not working in a background job for a server script. Although there is an error in the script, the user is not receiving any notifications on email.",
        "Question_gpt_summary":"user face issu error notif email work background job server script error script user receiv notif email"
    },
    {
        "Question_title":"Error on DeleteEndpoint operation: Cannot update in-progress endpoint",
        "Question_body":"Hi,\n\nI have created and endpoint in sagemaker using boto3 but never finishes creation, is stuck in Creating status for few days now. I have tried to delete it using the aws cli api but i get the message:\n\n*An error occurred (ValidationException) when calling the DeleteEndpoint operation: Cannot update in-progress endpoint*\n\nUsually endpoint fails after some time and can deleted but this time doesn't fail. Is there any way to force deletion?",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1652103997301,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668502656311,
        "Question_score_count":0.0,
        "Question_view_count":62.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2NawO4aWQvmytekX8xJJNQ\/error-on-deleteendpoint-operation-cannot-update-in-progress-endpoint",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error deleteendpoint oper updat progress endpoint creat endpoint boto finish creation stuck creat statu dai tri delet aw cli api messag error occur validationexcept call deleteendpoint oper updat progress endpoint usual endpoint fail time delet time fail wai forc delet",
        "Question_preprocessed_content":"oper updat endpoint creat endpoint boto finish creation stuck creat statu dai tri delet aw cli api oper updat endpoint endpoint fail time delet time fail wai forc delet",
        "Question_gpt_summary_original":"The user is unable to delete an endpoint in SageMaker using the AWS CLI API due to an error message stating \"Cannot update in-progress endpoint\". The endpoint has been stuck in the \"Creating\" status for several days and has not failed like it usually does. The user is seeking a solution to force the deletion of the endpoint.",
        "Question_gpt_summary":"user unabl delet endpoint aw cli api error messag state updat progress endpoint endpoint stuck creat statu dai fail like usual user seek solut forc delet endpoint"
    },
    {
        "Question_title":"Can't use Keras CSVLogger callbacks in Sagemaker script mode. It fails to write the log file on S3 ( error - No such file or directory )",
        "Question_body":"<p>I have this script where I want to get the callbacks to a separate CSV file in sagemaker custom script docker container. But when I try to run in local mode, it fails giving the following error. I have a hyper-parameter tuning job(HPO) to run and this keeps giving me errors. I need to get this local mode run correctly before doing the HPO. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/de522.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/de522.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In the notebook I use the following code.<\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='lstm_model.py', \n                          role=role,\n                          code_location=custom_code_upload_location,\n                          output_path=model_artifact_location+'\/',\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1},\n                          base_job_name='hpo-lstm-local-test'\n                         )\n\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n\n<p>In my <strong>lstm_model.py<\/strong> script the following code is used.<\/p>\n\n<pre><code>lgdir = os.path.join(model_dir, 'callbacks_log.csv')\ncsv_logger = CSVLogger(lgdir, append=True)\n\nregressor.fit(x_train, y_train, batch_size=batch_size,\n              validation_data=(x_val, y_val), \n              epochs=epochs,\n              verbose=2,\n              callbacks=[csv_logger]\n              )\n<\/code><\/pre>\n\n<p>I tried creating a file before hand like shown below using tensorflow backend. But it doesn't create a file. ( K : tensorflow Backend, tf: tensorflow )<\/p>\n\n<pre><code>filename = tf.Variable(lgdir , tf.string)\ncontent = tf.Variable(\"\", tf.string)\nsess = K.get_session()\ntf.io.write_file(filename, content)\n<\/code><\/pre>\n\n<p>I can't use any other packages like pandas to create the file as the TensorFlow docker container in SageMaker for custom scripts doesn't provide them. They give only a limited amount of packages. <\/p>\n\n<p>Is there a way I can write the csv file to the S3 bucket location, before the fit method try to write the callback. Or is that the solution to the problem? I am not sure. <\/p>\n\n<p>If you can even suggest other suggestions to get callbacks, I would even accept that answer. But it should be worth the effort. <\/p>\n\n<p>This docker image is really narrowing the scope. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1580725861867,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":412.0,
        "Poster_created_time":1517147266416,
        "Poster_location":null,
        "Poster_reputation_count":65.0,
        "Poster_view_count":18.0,
        "Answer_body":"<p>Well for starters, you can always make your own docker image using the Tensorflow image as a base. I work in Tensorflow 2.0 so this will be slightly different for you but here is an example of my image pattern:<\/p>\n\n<pre><code># Downloads the TensorFlow library used to run the Python script\nFROM tensorflow\/tensorflow:2.0.0a0 # you would use the equivalent for your TF version\n\n# Contains the common functionality necessary to create a container compatible with Amazon SageMaker\nRUN pip install sagemaker-containers -q \n\n# Wandb allows us to customize and centralize logging while maintaining open-source agility\nRUN pip install wandb -q # here you would install pandas\n\n# Copies the training code inside the container to the design pattern created by the Tensorflow estimator\n# here you could copy over a callbacks csv\nCOPY mnist-2.py \/opt\/ml\/code\/mnist-2.py \nCOPY callbacks.py \/opt\/ml\/code\/callbacks.py \nCOPY wandb_setup.sh \/opt\/ml\/code\/wandb_setup.sh\n\n# Set the login script as the entry point\nENV SAGEMAKER_PROGRAM wandb_setup.sh # here you would instead launch lstm_model.py\n<\/code><\/pre>\n\n<p>I believe you are looking for a pattern similar to this, but I prefer to log all of my model data using <a href=\"https:\/\/www.wandb.com\/\" rel=\"nofollow noreferrer\">Weights and Biases<\/a>. They're a little out of data on their SageMaker integration but I'm actually in the midst of writing an updated tutorial for them. It should certainly be finished this month and include logging and comparing runs from hyperparameter tuning jobs<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1583860547412,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1539831335196,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation_count":137.0,
        "Answerer_view_count":56.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60037376",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us kera csvlogger callback script mode fail write log file error file directori script want callback separ csv file custom script docker contain try run local mode fail give follow error hyper paramet tune job hpo run keep give error need local mode run correctli hpo notebook us follow code tensorflow import tensorflow estim tensorflow entri point lstm model role role code locat custom code upload locat output path model artifact locat train instanc count train instanc type local framework version version script mode true hyperparamet epoch base job hpo lstm local test estim fit train train input path valid valid input path lstm model script follow code lgdir path join model dir callback log csv csv logger csvlogger lgdir append true regressor fit train train batch size batch size valid data val val epoch epoch verbos callback csv logger tri creat file hand like shown tensorflow backend creat file tensorflow backend tensorflow filenam variabl lgdir string content variabl string sess session write file filenam content us packag like panda creat file tensorflow docker contain custom script provid limit packag wai write csv file bucket locat fit method try write callback solut problem sure suggest suggest callback accept answer worth effort docker imag narrow scope",
        "Question_preprocessed_content":"us kera script mode fail write log file script want separ csv file custom script docker contain try run local mode fail give tune job run give local mode run hpo us code script code tri creat file hand like shown tensorflow backend creat file us packag like panda creat file tensorflow docker contain custom script provid limit packag wai write csv file bucket locat fit method try write solut problem sure answer worth docker imag scope",
        "Question_gpt_summary_original":"The user is facing an issue while trying to use Keras CSVLogger callbacks in Sagemaker script mode. The log file fails to write on S3, resulting in a \"No such file or directory\" error. The user has tried creating a file beforehand using TensorFlow backend, but it doesn't work. The user is looking for a solution to write the CSV file to the S3 bucket location before the fit method tries to write the callback. The user is open to other suggestions to get callbacks.",
        "Question_gpt_summary":"user face issu try us kera csvlogger callback script mode log file fail write result file directori error user tri creat file tensorflow backend work user look solut write csv file bucket locat fit method tri write callback user open suggest callback"
    },
    {
        "Question_title":"What is AML_MODEL_URI - PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Question_body":"<p>Hi all,     <\/p>\n<p>I am following the steps on this tutorial:     <br \/>\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\">https:\/\/learn.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool<\/a>      <\/p>\n<p>I don't know what is the AML_MODEL_URI. I thought it was the REST endpoint or the Swagger URI from the endpoint.     <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/150362-image.png?platform=QnA\" alt=\"150362-image.png\" \/>    <\/p>\n<p>But it is not working. I am getting this error on Synapse: &quot;RuntimeError: Load model failed    <br \/>\nTraceback (most recent call last):&quot;    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/150308-image.png?platform=QnA\" alt=\"150308-image.png\" \/>    <\/p>\n<p>I appreciate you help.    <\/p>\n<p>Kind regards,     <br \/>\nAnaid     <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1637180271803,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hello <a href=\"\/users\/na\/?userid=4bb27b25-616e-491c-b986-136b5bf96f77\">@Anaid  <\/a>,    <\/p>\n<p>Thanks for the question and using MS Q&amp;A platform.    <\/p>\n<blockquote>\n<p>AML_MODEL_URL is the same name of the model in the ML workspace with (follow the format of <code>aml:\/\/<\/code> + Name of the Model).    <\/p>\n<\/blockquote>\n<p>Example: <code>aml:\/\/sklearn_regression_model:1<\/code> (follow the format of <code>aml:\/\/<\/code> + Name of the Model).    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/153599-image.png?platform=QnA\" alt=\"153599-image.png\" \/>    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Answer_comment_count":0.0,
        "Question_closed_time":1637667545273,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"aml model uri predict serverless apach spark pool synaps aml follow step tutori tutori score machin learn model predict serverless apach spark pool http learn microsoft com azur synaps analyt machin learn tutori score model predict spark pool know aml model uri thought rest endpoint swagger uri endpoint work get error synaps runtimeerror load model fail traceback recent appreci help kind regard anaid",
        "Question_preprocessed_content":"predict apach spark step tutori tutori score machin learn model predict apach spark know thought rest endpoint uri endpoint work synaps load model fail traceback help kind regard anaid",
        "Question_gpt_summary_original":"The user is following a tutorial on scoring machine learning models with PREDICT in serverless Apache Spark pools but is encountering an error related to the AML_MODEL_URI. The user is unsure of what the AML_MODEL_URI is and has tried using the REST endpoint and Swagger URI but it is not working. The error message received is \"RuntimeError: Load model failed\".",
        "Question_gpt_summary":"user follow tutori score machin learn model predict serverless apach spark pool encount error relat aml model uri user unsur aml model uri tri rest endpoint swagger uri work error messag receiv runtimeerror load model fail"
    },
    {
        "Question_title":"how to hook up CI\/CD with sagemaker pipelines  in sagemaker sudio?",
        "Question_body":"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb, I have created various pipeline step and tie those together and create a pipeline at the end. once these pipelines are created , how do i link this pipeline with steps for data preprocessing, training with other pipelines, say one for evaluation , another for deployment. is there anything inbuild in sagemaker studio to do this , or do we have to write our own code for this ? how can i link\/feed output\/artifact from one pipeline to another? \n\n```\nfrom sagemaker.workflow.steps import TrainingStep\nfrom sagemaker.workflow.steps import ProcessingStep\nfrom sagemaker.workflow.pipeline import Pipeline\n\nprocessor_args = sklearn_processor.run(    inputs=  ...  ,    outputs=...  ,    code=\"code\/preprocessing.py\",)\n\nmy_processing_step = ProcessingStep(name=\"my_processing_step \", step_args=processor_args)\n\n.....\nmy_training_train = TrainingStep(    name=\"my_training_train \",    step_args=train_args,)\n...\npipeline_name = f\"experiment_Pipeline\"\npipeline = Pipeline(   name=pipeline_name,\n    parameters= .., \n    steps=[my_processing_step , my_training_train  ...],\n)\n\n```",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1672250354190,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1672598727232,
        "Question_score_count":0.0,
        "Question_view_count":101.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzTH1AL_WToem_Gt47y83Ug\/how-to-hook-up-ci-cd-with-sagemaker-pipelines-in-sagemaker-sudio",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"hook pipelin sudio http github com aw amazon exampl blob main pipelin tabular abalon build train deploi pipelin preprocess train evalu batch transform ipynb creat pipelin step tie creat pipelin end pipelin creat link pipelin step data preprocess train pipelin evalu deploy inbuild studio write code link feed output artifact pipelin workflow step import trainingstep workflow step import processingstep workflow pipelin import pipelin processor arg sklearn processor run input output code code preprocess process step processingstep process step step arg processor arg train train trainingstep train train step arg train arg pipelin experi pipelin pipelin pipelin pipelin paramet step process step train train",
        "Question_preprocessed_content":"pipelin sudio creat pipelin step tie creat pipelin end pipelin creat link pipelin step data train pipelin evalu deploy inbuild studio write code pipelin",
        "Question_gpt_summary_original":"The user has created various pipeline steps in Sagemaker Studio and tied them together to create a pipeline. However, they are unsure how to link this pipeline with other pipelines for data preprocessing, training, evaluation, and deployment. They are asking if there is an inbuilt feature in Sagemaker Studio to do this or if they need to write their own code. They also want to know how to link the output\/artifact from one pipeline to another.",
        "Question_gpt_summary":"user creat pipelin step studio ti creat pipelin unsur link pipelin pipelin data preprocess train evalu deploy ask inbuilt featur studio need write code want know link output artifact pipelin"
    },
    {
        "Question_title":"How to use a PySpark UDF in a Scala Spark project?",
        "Question_body":"<p>Several people (<a href=\"https:\/\/stackoverflow.com\/questions\/41780141\/using-a-scala-udf-in-pyspark\">1<\/a>, <a href=\"https:\/\/medium.com\/wbaa\/using-scala-udfs-in-pyspark-b70033dd69b9\" rel=\"nofollow noreferrer\">2<\/a>, <a href=\"https:\/\/github.com\/amesar\/spark-python-scala-udf\" rel=\"nofollow noreferrer\">3<\/a>) have discussed using a Scala UDF in a PySpark application, usually for performance reasons.  I am interested in the opposite - using a python UDF in a Scala Spark project.<\/p>\n\n<p>I am particularly interested in building a model using sklearn (and <a href=\"https:\/\/databricks.com\/blog\/2018\/06\/05\/introducing-mlflow-an-open-source-machine-learning-platform.html\" rel=\"nofollow noreferrer\">MLFlow<\/a>) then efficiently applying that to records in a Spark streaming job.  I know I could also host the python model behind a REST API and <a href=\"https:\/\/stackoverflow.com\/questions\/41799578\/restapi-service-call-from-spark-streaming\">make calls to that API in the Spark streaming application<\/a> in <a href=\"https:\/\/spark.apache.org\/docs\/2.3.0\/api\/scala\/index.html#org.apache.spark.sql.Dataset@mapPartitions[U](f:org.apache.spark.api.java.function.MapPartitionsFunction[T,U],encoder:org.apache.spark.sql.Encoder[U]):org.apache.spark.sql.Dataset[U]\" rel=\"nofollow noreferrer\"><code>mapPartitions<\/code><\/a>, but managing concurrency for that task and setting up the API for hosted model isn't something I'm super excited about.<\/p>\n\n<p>Is this possible without too much custom development with something like Py4J? Is this just a bad idea?<\/p>\n\n\n\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":10.0,
        "Question_created_time":1534609811997,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1575115357760,
        "Question_score_count":11.0,
        "Question_view_count":1100.0,
        "Poster_created_time":1307578819472,
        "Poster_location":"Atlanta, GA, United States",
        "Poster_reputation_count":8549.0,
        "Poster_view_count":393.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51910607",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"us pyspark udf scala spark project peopl discuss scala udf pyspark applic usual perform reason interest opposit python udf scala spark project particularli interest build model sklearn effici appli record spark stream job know host python model rest api call api spark stream applic mappartit manag concurr task set api host model isn super excit possibl custom develop like pyj bad idea thank",
        "Question_preprocessed_content":"us pyspark udf scala spark project peopl scala udf pyspark perform reason interest python udf scala spark project particularli interest build model sklearn record spark stream job know host python model rest api api spark stream manag task api host model isn super excit custom develop like bad idea thank",
        "Question_gpt_summary_original":"The user is seeking guidance on how to use a Python UDF in a Scala Spark project, specifically for building a model using sklearn and MLFlow, and efficiently applying it to records in a Spark streaming job. The user is hesitant to host the Python model behind a REST API and make calls to it in the Spark streaming application due to concurrency management and API setup concerns. The user is wondering if this is possible without significant custom development using Py4J and if it is a good idea.",
        "Question_gpt_summary":"user seek guidanc us python udf scala spark project specif build model sklearn effici appli record spark stream job user hesit host python model rest api call spark stream applic concurr manag api setup concern user wonder possibl signific custom develop pyj good idea"
    },
    {
        "Question_title":"Where does Azure Machine Learning Service cache data?",
        "Question_body":"<p>I am looking to use Azure Machine Learning Services (the one with the new drag and drop feature; still in preview) in a new data science project. <\/p>\n\n<p>I have realised that I can preview the data when I connect a data set; I am able to do this using the option 'Dataset output' which is available as part of the dataset.<\/p>\n\n<p>To be able to see this data, the data needs to be cached some where. <\/p>\n\n<p>Can someone advise where this is cached? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1582250022010,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":397.0,
        "Poster_created_time":1456485566000,
        "Poster_location":"Canberra, Australian Capital Territory, Australia",
        "Poster_reputation_count":214.0,
        "Poster_view_count":54.0,
        "Answer_body":"<p>Data is cached by default in a storage account that is created along with the the ML service workspace. It has the same name as the workspace plus some numbers. Inside the account there is a blobstore called <code>azureml-blobstore-{GUID}<\/code> Inside of that container your data is cached,  organized by runs.<\/p>\n\n<p>This data is made available to ML service as a <code>Datastore<\/code> that you can navigate to in the UI by clicking \"Datastores\" in the blade on the left-hand of the Studio.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YVwPl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YVwPl.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1582256294052,
        "Answer_last_edit_time":null,
        "Answer_score_count":3.0,
        "Answerer_created_time":1405457120427,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation_count":3359.0,
        "Answerer_view_count":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60331084",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"servic cach data look us servic new drag drop featur preview new data scienc project realis preview data connect data set abl option dataset output avail dataset abl data data need cach advis cach",
        "Question_preprocessed_content":"servic cach data us servic new data scienc project realis preview data data set abl option dataset output avail dataset abl data data cach advis cach",
        "Question_gpt_summary_original":"The user is seeking information on where Azure Machine Learning Service caches data when previewing a dataset using the \"Dataset output\" option.",
        "Question_gpt_summary":"user seek inform servic cach data preview dataset dataset output option"
    },
    {
        "Question_title":"AZURE MAchine Learning  - Resource exhausted",
        "Question_body":"<p>Currently we are facing issues with resource exhausted with several experiments (see table below).<\/p>\n<p>We are using <a href=\"https:\/\/images.nvidia.com\/content\/tesla\/pdf\/188417-Tesla-M60-DS-A4-fnl-Web.pdf%22https:\/\/images.nvidia.com\/content\/tesla\/pdf\/188417-tesla-m60-ds-a4-fnl-web.pdf%22\">GPU - 1 x NVIDIA Tesla M60<\/a>, Standard_NV6 (6 cores, 56 GB RAM, 380 GB disk)<\/p>\n<table>\n<thead>\n<tr>\n<th>Image size<\/th>\n<th>Batch size<\/th>\n<th>status<\/th>\n<th>Remarks<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>256x256<\/td>\n<td>32<\/td>\n<td>failed<\/td>\n<td>Resource exhausted<\/td>\n<\/tr>\n<tr>\n<td>256x256<\/td>\n<td>16<\/td>\n<td>working<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>512x512<\/td>\n<td>16<\/td>\n<td>failed<\/td>\n<td>Resource exhausted<\/td>\n<\/tr>\n<tr>\n<td>512x512<\/td>\n<td>8<\/td>\n<td>failed<\/td>\n<td>Resource exhausted<\/td>\n<\/tr>\n<tr>\n<td>512x512<\/td>\n<td>1<\/td>\n<td>working<\/td>\n<td><\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<p>How to decide which GPU machine is to be purchased to train the deep learning task semantic segmentation. Specifications: model with 31 million \u00a0parameters image size ( minimum 1024x1024), batch size 256 framework -tensorflow keras 2.7 Total image samples 30,000, Label Mask samples 30,000<\/p>\n<p>\u00a0<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4.0,
        "Question_created_time":1678104336393,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1186849\/azure-machine-learning-resource-exhausted",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"resourc exhaust current face issu resourc exhaust experi tabl gpu nvidia tesla standard core ram disk imag size batch size statu remark fail resourc exhaust work fail resourc exhaust fail resourc exhaust work decid gpu machin purchas train deep learn task semant segment specif model million paramet imag size minimum batch size framework tensorflow kera total imag sampl label mask sampl",
        "Question_preprocessed_content":"resourc exhaust face resourc exhaust experi gpu nvidia tesla imag size batch size statu remark fail resourc exhaust work fail resourc exhaust fail resourc exhaust work decid gpu machin purchas train learn task semant segment specif model paramet imag size batch size framework tensorflow kera total imag sampl label mask sampl",
        "Question_gpt_summary_original":"The user is facing resource exhaustion issues with several experiments while using Azure Machine Learning with a GPU of 1 x NVIDIA Tesla M60 and Standard_NV6 (6 cores, 56 GB RAM, 380 GB disk). The user is unsure which GPU machine to purchase to train a deep learning task of semantic segmentation with a model having 31 million parameters, minimum image size of 1024x1024, batch size of 256, and framework of TensorFlow Keras 2.7. The total image samples and label mask samples are 30,000 each.",
        "Question_gpt_summary":"user face resourc exhaust issu experi gpu nvidia tesla standard core ram disk user unsur gpu machin purchas train deep learn task semant segment model have million paramet minimum imag size batch size framework tensorflow kera total imag sampl label mask sampl"
    },
    {
        "Question_title":"Instance history on sagemaker?",
        "Question_body":"<p>I have a Sagemaker instance that was changed from large to medium by someone, is there any way to check history and reset it back to the previous instance type?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1641221404327,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":171.0,
        "Poster_created_time":1498498558510,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70567516",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"instanc histori instanc chang larg medium wai check histori reset previou instanc type",
        "Question_preprocessed_content":"instanc histori instanc chang larg medium wai check histori reset previou instanc type",
        "Question_gpt_summary_original":"The user is looking for a way to check the history of changes made to their Sagemaker instance and revert it back to its previous instance type after someone changed it from large to medium.",
        "Question_gpt_summary":"user look wai check histori chang instanc revert previou instanc type chang larg medium"
    },
    {
        "Question_title":"AzurePipeline failing with ResourceExhausted error",
        "Question_body":"<p>Hi,    <br \/>\nI am using Microsoft Azure Machine Learning Studio to train LSTM model on 600K rows of data for binary classification.    <br \/>\nWhile doing that I am getting this error in 5th\/6th Epoch. Could you please let me know which setting in my Azure should I change to overcome this error.    <br \/>\nLet me know in case you need more information.    <\/p>\n<p>Virtual machine size=&gt; Standard_F8s_v2 (8 cores, 16 GB RAM, 64 GB disk)    <br \/>\nProcessing unit=&gt;CPU - Compute-optimized<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/263096-pipelineerror.jpg?platform=QnA\" alt=\"263096-pipelineerror.jpg\" \/>    <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1669128558063,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1099870\/azurepipeline-failing-with-resourceexhausted-error",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azurepipelin fail resourceexhaust error studio train lstm model row data binari classif get error epoch let know set azur chang overcom error let know case need inform virtual machin size standard core ram disk process unit cpu comput optim",
        "Question_preprocessed_content":"azurepipelin fail studio train lstm model row data binari epoch let know azur chang overcom let know case inform virtual machin size unit cpu",
        "Question_gpt_summary_original":"The user is encountering a ResourceExhausted error while training an LSTM model on 600K rows of data for binary classification using Microsoft Azure Machine Learning Studio. The error occurs in the 5th\/6th Epoch and the user is seeking advice on which setting in Azure to change to overcome the error. The virtual machine size is Standard_F8s_v2 with 8 cores, 16 GB RAM, and 64 GB disk, and the processing unit is CPU - Compute-optimized.",
        "Question_gpt_summary":"user encount resourceexhaust error train lstm model row data binari classif studio error occur epoch user seek advic set azur chang overcom error virtual machin size standard core ram disk process unit cpu comput optim"
    },
    {
        "Question_title":"How should I customize my own component and how should I use it.",
        "Question_body":"<p>Hello team, I see there is a custom component module in Designer. I am curious about it. Does it mean I can customize my own component? Any reference I can read?<\/p>\n<p>Can you let me know what is custom component and how to define it? <\/p>\n<p>It\u2019s known that designer has very less flexibility so we are always considering SDK. How could it help? <\/p>\n<p>Meanwhile I am very surprised by the new release, anywhere we should look for those releasing news? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1677550674846,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hello <a href=\"https:\/\/learn.microsoft.com\/en-us\/users\/na\/?userid=0d974fba-0e6f-4b9f-a796-c7257f76884f\">kamala dey<\/a><\/p>\n<p>Thanks for reaching out to us. Yes, this helps. Designer supports two type of components, classic prebuilt components and <strong>custom components.<\/strong> These two types of components are not compatible.<\/p>\n<p>Classic prebuilt components provides prebuilt components majorly for data processing and traditional machine learning tasks like regression and classification. This type of component continues to be supported but will not have any new components added.<\/p>\n<p><strong>Custom components allow you to provide your own code as a component. It supports sharing across workspaces and seamless authoring across Studio, CLI, and SDK interfaces.<\/strong><\/p>\n<p>To build pipeline using components in UI, you need to register components to your workspace first. You can use CLI or SDK to register components to your workspace, so that you can share and reuse the component within the workspace. Registered components support automatic versioning so you can update the component but assure that pipelines that require an older version will continue to work.<\/p>\n<p>In the example below take using CLI for example. If you want to learn more about how to build a component, see <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-component-pipelines-cli\">Create and run pipelines using components with CLI<\/a>.<\/p>\n<ol>\n<li> From the <code>cli\/jobs\/pipelines-with-components\/basics<\/code> directory of the <a href=\"https:\/\/github.com\/Azure\/azureml-examples\"><code>azureml-examples<\/code> repository<\/a>, navigate to the <code>1b_e2e_registered_components<\/code> subdirectory.<\/li>\n<li> Register the components to Azure Machine Learning workspace using following commands. Learn more about <a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/concept-component\">ML components<\/a>.\n    CLICopy<\/li>\n<\/ol>\n<pre><code>    az ml component create --file train.yml\naz ml component create --file score.yml\naz ml component create --file eval.yml\n    ```\n    \n    \n1. After register component successfully, you can see your component in the studio UI.\n    \n[![Screenshot showing registered component in component page.](https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/media\/how-to-create-component-pipelines-ui\/component-page.png)\n\n](https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/media\/how-to-create-component-pipelines-ui\/component-page.png#lightbox)Please refer to this guidance for how to leverage your custom components - [https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-component-pipelines-ui#create-pipeline-using-registered-component](https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-component-pipelines-ui#create-pipeline-using-registered-component)\n\nCurrently registered components and the designer built-in components cannot be used together.\n\n\nI hope this helps, please have a try and let me know if you have any questions. \n\nRegards,\n\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.\n\n<\/code><\/pre>\n",
        "Answer_comment_count":2.0,
        "Question_closed_time":1677573962083,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1184883\/how-should-i-customize-my-own-component-and-how-sh",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"custom compon us hello team custom compon modul design curiou mean custom compon refer read let know custom compon defin known design flexibl consid sdk help surpris new releas look releas new",
        "Question_preprocessed_content":"custom compon us team custom compon modul design curiou mean custom compon refer read let know custom compon defin known design flexibl consid sdk help surpris new releas releas new",
        "Question_gpt_summary_original":"The user is seeking information on how to customize their own component using the custom component module in Designer. They are also asking for references on defining custom components and how the SDK can help. Additionally, they express surprise about a new release and ask where to find news about it.",
        "Question_gpt_summary":"user seek inform custom compon custom compon modul design ask refer defin custom compon sdk help addition express surpris new releas ask new"
    },
    {
        "Question_title":"How cache is maintained for big data size locally",
        "Question_body":"<p>Hi,<br>\nI have a VM on which my local git repo exists. On top of that i have installed dvc on same machine. Now when i add data to dvc it will be in dvc cache and on git push, commit same data will go to git repo as well. Is my understanding correct? If yes then their will be two copies of data and size will keep increasing as data grows. I am not using any remote repo for data.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_created_time":1568253347446,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":930.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-cache-is-maintained-for-big-data-size-locally\/217",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"cach maintain big data size local local git repo exist instal machin add data cach git push commit data git repo understand correct ye copi data size increas data grow remot repo data",
        "Question_preprocessed_content":"cach maintain big data size local git repo exist machin data cach git push data git repo understand ye copi data size increas data grow remot repo data",
        "Question_gpt_summary_original":"The user is concerned about the maintenance of cache for big data size locally. They have a VM with a local git repo and have installed dvc on the same machine. They are worried that adding data to dvc will result in two copies of the data, increasing the size as the data grows, as they are not using any remote repo for data.",
        "Question_gpt_summary":"user concern mainten cach big data size local local git repo instal machin worri ad data result copi data increas size data grow remot repo data"
    },
    {
        "Question_title":"Sweep track loss from Tensorboard",
        "Question_body":"<p>Hi,<br>\nIs it possible to use sweep with a metric that is only visible in Tensorboard?<br>\nIt does show up on WandB when  sync_tensorboard=True<\/p>\n<p>But sweep does not seem to work properly when I use the name showing up in the GUI to be tracked and minimized.<\/p>\n<p>Thanks<br>\nBen<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1636052247640,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":227.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-track-loss-from-tensorboard\/1226",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sweep track loss tensorboard possibl us sweep metric visibl tensorboard sync tensorboard true sweep work properli us show gui track minim thank ben",
        "Question_preprocessed_content":"track tensorboard us metric visibl tensorboard work properli us show gui track minim thank ben",
        "Question_gpt_summary_original":"The user is facing challenges in using sweep with a metric that is only visible in Tensorboard. The metric shows up on WandB when sync_tensorboard=True, but sweep does not work properly when using the name of the metric shown in the GUI to be tracked and minimized.",
        "Question_gpt_summary":"user face challeng sweep metric visibl tensorboard metric show sync tensorboard true sweep work properli metric shown gui track minim"
    },
    {
        "Question_title":"Deep Learning VM Config to connect to Google Colab",
        "Question_body":"Morning to all\n\nIm trying to connect a google colab file to a Google Deep Learning VM with any results. My guess is that I need to configure something inside the VM or the google console but not sure how to do so.\n\nI get the error that you will find in image 1 that says:\n\n\"The VM requested does not exist. Check out our guide to set up GCE VMs in Colab\"\n\nand has a the next link in which theres not much info on how to solve the situation:\u00a0 https:\/\/research.google.com\/colaboratory\/marketplace.html\n\nOn image 2 and 3 you will find the info that I add to the colab file that is the same as the VM configuration that you will fins on image 3.\n\nWhat I\u00b4m doing wrong? Do I need to asing special permits to the VM?\u00a0\n\nAny comments or advice is more than appreciated\n\nImage 1\n\nImage 2\nImage 3",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1652954760000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":548.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Deep-Learning-VM-Config-to-connect-to-Google-Colab\/td-p\/424603\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deep learn config connect googl colab morn try connect googl colab file googl deep learn result guess need configur insid googl consol sure error imag sai request exist check guid set gce vm colab link there info solv situat http research googl com colaboratori marketplac html imag info add colab file configur fin imag wrong need as special permit comment advic appreci imag imag imag",
        "Question_preprocessed_content":"learn config colab morn try colab file learn result configur insid consol sure imag sai request exist check guid set gce vm colab link there info solv situat imag info colab file configur fin imag wrong as special permit advic imag imag imag",
        "Question_gpt_summary_original":"The user is trying to connect a Google Colab file to a Google Deep Learning VM but is encountering an error message stating that the VM requested does not exist. The user suspects that they need to configure something inside the VM or the Google console but is unsure how to do so. The user has provided images of the error message and the information they added to the Colab file and VM configuration. The user is seeking advice on what they may be doing wrong and if they need to assign special permits to the VM.",
        "Question_gpt_summary":"user try connect googl colab file googl deep learn encount error messag state request exist user suspect need configur insid googl consol unsur user provid imag error messag inform ad colab file configur user seek advic wrong need assign special permit"
    },
    {
        "Question_title":"Why does Featuretools slows down when I increase the number of Dask workers?",
        "Question_body":"<p>I'm using an Amazon SageMaker Notebook that has 72 cores and 144 GB RAM, and I carried out 2 tests with a sample of the whole data to check if the Dask cluster was working.<\/p>\n\n<p>The sample has 4500 rows and 735 columns from 5 different \"assets\" (I mean 147 columns for each asset). The code is filtering the columns and creating a feature matrix for each filtered Dataframe.<\/p>\n\n<p>First, I initialized the cluster as follows, I received 72 workers, and got 17 minutes of running. (I assume I created 72 workers with one core each.)<\/p>\n\n<pre><code>    from dask.distributed import Client, LocalCluster\n    cluster = LocalCluster(processes=True,n_workers=72,threads_per_worker=72)\n\n    def main():\n      import featuretools as ft\n      list_columns = list(df_concat_02.columns)\n\n      list_df_features=[]\n      from tqdm.notebook import tqdm\n\n      for asset in tqdm(list_columns,total=len(list_columns)):\n        dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n        es = ft.EntitySet()  \n        es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                      index = 'index', \n                                      time_index = 'Date')\n        fm, features = ft.dfs(entityset=es, \n                              target_entity='MARKET',\n                              trans_primitives = ['divide_numeric'],\n                              agg_primitives = [],\n                              max_depth=1,\n                              verbose=True,\n                              dask_kwargs={'cluster': client.scheduler.address}\n\n                              )\n        list_df_features.append(fm)\n      return list_df_features\n\n    if __name__ == \"__main__\":\n        list_df = main()\n<\/code><\/pre>\n\n<p>Second, I initialized the cluster as follows, I received 9 workers, and got 3,5 minutes of running. (I assume I created 9 workers with 8 cores each.)<\/p>\n\n<pre><code>from dask.distributed import Client, LocalCluster\ncluster = LocalCluster(processes=True)\n\ndef main():\n  import featuretools as ft\n  list_columns = list(df_concat_02.columns)\n\n  list_df_features=[]\n  from tqdm.notebook import tqdm\n\n  for asset in tqdm(list_columns,total=len(list_columns)):\n    dataframe = df_sma.filter(regex=\"^\"+asset, axis=1).reset_index()\n\n    es = ft.EntitySet()  \n    es = es.entity_from_dataframe(entity_id = 'MARKET', dataframe =dataframe, \n                                  index = 'index', \n                                  time_index = 'Date')\n    fm, features = ft.dfs(entityset=es, \n                          target_entity='MARKET',\n                          trans_primitives = ['divide_numeric'],\n                          agg_primitives = [],\n                          max_depth=1,\n                          verbose=True,\n                          dask_kwargs={'cluster': client.scheduler.address}\n\n                          )\n    list_df_features.append(fm)\n  return list_df_features\n\nif __name__ == \"__main__\":\n    list_df = main()\n<\/code><\/pre>\n\n<p>For me, it's mind-blowing because I thought that 72 workers could carry the work out faster! Once I'm not a specialist neither in Dask nor in FeatureTools I guess that I'm setting something wrong.<\/p>\n\n<p>I would appreciate any kind of help and advice!<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1583540635953,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1583589065656,
        "Question_score_count":0.0,
        "Question_view_count":192.0,
        "Poster_created_time":1509012479112,
        "Poster_location":"Belo Horizonte, MG, Brasil",
        "Poster_reputation_count":97.0,
        "Poster_view_count":25.0,
        "Answer_body":"<p>You are correctly setting <code>dask_kwargs<\/code> in DFS. I think the slow down happens as a result of additional overhead and less cores in each worker. The more workers there are, the more overhead exists from transmitting data. Additionally, 8 cores from 1 worker can be leveraged to make computations run faster than 1 core from 8 workers.<\/p>",
        "Answer_comment_count":4.0,
        "Question_closed_time":1583793190592,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1443482042487,
        "Answerer_location":"United States",
        "Answerer_reputation_count":1995.0,
        "Answerer_view_count":150.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60573260",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"featuretool slow increas number dask worker notebook core ram carri test sampl data check dask cluster work sampl row column differ asset mean column asset code filter column creat featur matrix filter datafram initi cluster follow receiv worker got minut run assum creat worker core dask distribut import client localclust cluster localclust process true worker thread worker def main import featuretool list column list concat column list featur tqdm notebook import tqdm asset tqdm list column total len list column datafram sma filter regex asset axi reset index entityset entiti datafram entiti market datafram datafram index index time index date featur df entityset target entiti market tran primit divid numer agg primit max depth verbos true dask kwarg cluster client schedul address list featur append return list featur main list main second initi cluster follow receiv worker got minut run assum creat worker core dask distribut import client localclust cluster localclust process true def main import featuretool list column list concat column list featur tqdm notebook import tqdm asset tqdm list column total len list column datafram sma filter regex asset axi reset index entityset entiti datafram entiti market datafram datafram index index time index date featur df entityset target entiti market tran primit divid numer agg primit max depth verbos true dask kwarg cluster client schedul address list featur append return list featur main list main mind blow thought worker carri work faster specialist dask featuretool guess set wrong appreci kind help advic thank",
        "Question_preprocessed_content":"slow increas number dask worker core ram test sampl data check dask cluster work sampl row column code filter column creat featur matrix filter datafram initi cluster receiv worker got minut second initi cluster receiv worker got minut thought worker work faster specialist dask wrong kind help advic thank",
        "Question_gpt_summary_original":"The user is facing a challenge with Featuretools slowing down when increasing the number of Dask workers. They conducted two tests with a sample of the whole data and found that initializing the cluster with 9 workers with 8 cores each resulted in faster running time (3.5 minutes) compared to initializing the cluster with 72 workers with one core each (17 minutes). The user is seeking advice on how to optimize the process.",
        "Question_gpt_summary":"user face challeng featuretool slow increas number dask worker conduct test sampl data initi cluster worker core result faster run time minut compar initi cluster worker core minut user seek advic optim process"
    },
    {
        "Question_title":"How to override existing statistics in plot when resuming from a checkpoint?",
        "Question_body":"<p>Hello all! I\u2019m trying to set up proper training checkpointing and resuming for my code and thus far I\u2019ve gotten things to work but there is still one thing I am trying to figure out, which is how to get the logs in wandb to get overwritten\/replaced after I load a checkpoint.<\/p>\n<p>For instance, right now in my code if I save a checkpoint at 5000 timesteps, let training run for a few more thousand timesteps, cancel it, and then load and resume training from that 5000 step checkpoint, a training plot will look like this:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/0\/0fd7c64199202e092516ae6671d3a7ffad419c7f.png\" alt=\"image\" data-base62-sha1=\"2g9sjoDhJ11TQUKPD8Ox6R0RONx\" width=\"481\" height=\"393\"><\/p>\n<p>This is because the built-in Wandb Step value didn\u2019t also reset back to 5k for when I restarted training from the 5k checkpoint, it just kept going. What I would instead like to have happen is that the Step value is synced with when I save the checkpoint so that when I resume, the existing plot is <strong>overridden<\/strong>, rather than continued. Is it possible to do this? Thanks in advance!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1674473603626,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":61.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-override-existing-statistics-in-plot-when-resuming-from-a-checkpoint\/3737",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"overrid exist statist plot resum checkpoint hello try set proper train checkpoint resum code far iv gotten thing work thing try figur log overwritten replac load checkpoint instanc right code save checkpoint timestep let train run thousand timestep cancel load resum train step checkpoint train plot look like built step valu didnt reset restart train checkpoint kept go instead like happen step valu sync save checkpoint resum exist plot overridden continu possibl thank advanc",
        "Question_preprocessed_content":"exist statist plot resum checkpoint try set proper train checkpoint resum code far iv thing work thing try figur log replac load checkpoint instanc right code save checkpoint timestep let train run thousand timestep cancel load resum train step checkpoint train plot like step valu didnt reset restart train checkpoint kept go instead like step valu sync save checkpoint resum exist plot continu thank advanc",
        "Question_gpt_summary_original":"The user is trying to set up proper training checkpointing and resuming for their code. They have successfully saved a checkpoint at 5000 timesteps, but when they load and resume training from that checkpoint, the Wandb Step value doesn't reset back to 5k, resulting in the existing plot being continued instead of overridden. The user is seeking a solution to override the existing statistics in the plot when resuming from a checkpoint.",
        "Question_gpt_summary":"user try set proper train checkpoint resum code successfulli save checkpoint timestep load resum train checkpoint step valu reset result exist plot continu instead overridden user seek solut overrid exist statist plot resum checkpoint"
    },
    {
        "Question_title":"How to organize inference results by model ID that produced it?",
        "Question_body":"<p>I am creating a deep ensemble, and I need to keep track of which inference results came from which neural network. However, enabling versioning via the Kedro catalog only organizes my inference results by datetime, with each file having an identical name.<\/p>\n<p>What is the best practice for organizing results by model ID? For example, is it possible to customize the versioned folder names, or the versioned file? E.g. <code>data\/07_model_output\/model_predictions.json\/2020-08-09T20.55.57.237Z-Model-04\/model_predictions.json<\/code> or <code>...\/2020-08-09T20.55.57.237Z\/model_predictions-Model-04.json<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1597007197307,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597058346200,
        "Question_score_count":0.0,
        "Question_view_count":55.0,
        "Poster_created_time":1415053264667,
        "Poster_location":"USA",
        "Poster_reputation_count":11166.0,
        "Poster_view_count":653.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63331325",
        "Tool":"Kedro",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"organ infer result model produc creat deep ensembl need track infer result came neural network enabl version catalog organ infer result datetim file have ident best practic organ result model exampl possibl custom version folder name version file data model output model predict json model model predict json model predict model json",
        "Question_preprocessed_content":"organ infer result model produc creat ensembl track infer result came neural network enabl version catalog organ infer result datetim file have ident best practic organ result model exampl custom version folder name version file",
        "Question_gpt_summary_original":"The user is creating a deep ensemble and needs to organize inference results by the model ID that produced it. However, enabling versioning via the Kedro catalog only organizes the results by datetime with identical file names. The user is seeking advice on the best practice for organizing results by model ID and customizing versioned folder names or files.",
        "Question_gpt_summary":"user creat deep ensembl need organ infer result model produc enabl version catalog organ result datetim ident file name user seek advic best practic organ result model custom version folder name file"
    },
    {
        "Question_title":"AzureML No Module Found Error on Deployment of Inference model: Xgboost",
        "Question_body":"<p>We're getting in an error in one environment deploying a ML endpoint with stating that xgboost cannot be found although it's included in the Dockerfile. We do not see this issue in 3 other environments and the model is able to deploy fine without this package error.<\/p>\n<p>Dockerfile:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.0-cudnn7-ubuntu16.04:20210220.v1\nUSER root\nRUN mkdir -p \/etc\/OpenCL\/vendors &amp;&amp; echo &quot;libnvidia-opencl.so.1&quot; &gt; \/etc\/OpenCL\/vendors\/nvidia.icd\nRUN apt-get update &amp;&amp; echo 'success updated apt-get!'\nRUN apt-get install -y --no-install-recommends cmake libboost-dev libboost-system-dev libboost-filesystem-dev\nRUN conda create -n gpuexp python=3.6.2 -y\n\n###############################\n# Pre-Build LightGBM\n###############################\nRUN cd \/usr\/local\/src &amp;&amp; mkdir lightgbm &amp;&amp; cd lightgbm &amp;&amp; \\\n    git clone --recursive --branch v2.3.0 --depth 1 https:\/\/github.com\/microsoft\/LightGBM &amp;&amp; \\\n cd LightGBM &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; \\\n cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ .. &amp;&amp; \\\n make -j4\n\n###############################\n# Install GPU LightGBM and XgBoost\n###############################\nRUN \/bin\/bash -c &quot;source activate gpuexp &amp;&amp; \\\n    cd \/usr\/local\/src\/lightgbm\/LightGBM\/python-package &amp;&amp; python setup.py install --precompile &amp;&amp; \\\n pip install --upgrade --force-reinstall xgboost==1.1.1 &amp;&amp; \\ \n source deactivate&quot;\n<\/code><\/pre>\n<p>Conda:<\/p>\n<pre><code>channels:\n  - anaconda\n  - conda-forge\n  - pytorch\ndependencies:\n  - python=3.6.2\n  - pip=20.2.4\n  - pip:\n      - azureml-core==1.27.0\n      - azureml-pipeline-core==1.27.0\n      - azureml-telemetry==1.27.0\n      - azureml-defaults==1.27.0\n      - azureml-interpret==1.27.0\n      - azureml-automl-core==1.27.0\n      - azureml-automl-runtime==1.27.0.post2\n      - azureml-train-automl-client==1.27.0\n      - azureml-train-automl-runtime==1.27.0.post1\n      - azureml-dataset-runtime==1.27.0\n      - azureml-mlflow==1.27.0\n      - inference-schema\n      - py-cpuinfo==5.0.0\n      - boto3==1.15.18\n      - botocore==1.18.18\n      - azure-storage-file-datalake\n      - azure-identity&lt;1.5.0\n      - azure-keyvault\n      - azure-servicebus\n  - numpy~=1.18.0\n  - scikit-learn==0.22.1\n  - pandas~=0.25.0\n  - fbprophet==0.5\n  - holidays==0.9.11\n  - setuptools-git\n  - 'psutil&gt;5.0.0,&lt;6.0.0'\n<\/code><\/pre>\n<p>I haven't included the name in the conda file intentionally.<\/p>\n<p>Is there something we're missing in the container set up for this issue that could cause it to fail for one environment and not the other?<\/p>\n<p>We are able to see the model within our endpoints section in the Azure Machine Learning Studio, but this error is visible on the deployment logs and the endpoint is a Failed state.<\/p>\n<p>In our three other environments, the endpoint is visible and in a healthy state.<\/p>\n<p>Full error message:<\/p>\n<pre><code>    2022-01-11T19:46:08,279016451+00:00 - rsyslog\/run \n    2022-01-11T19:46:08,277445539+00:00 - gunicorn\/run \n    2022-01-11T19:46:08,280042359+00:00 - iot-server\/run \n    \/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n    2022-01-11T19:46:08,285741101+00:00 - nginx\/run \n    \/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n    \/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n    \/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n    \/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n    rsyslogd: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libuuid.so.1: no version information available (required by rsyslogd)\n    EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n    2022-01-11T19:46:08,407862719+00:00 - iot-server\/finish 1 0\n    2022-01-11T19:46:08,409832434+00:00 - Exit code 1 is normal. Not restarting iot-server.\n    Starting gunicorn 19.9.0\n    Listening at: http:\/\/127.0.0.1:31311 (11)\n    Using worker: sync\n    worker timeout is set to 300\n    Booting worker with pid: 37\n    SPARK_HOME not set. Skipping PySpark Initialization.\n    Generating new fontManager, this may take some time...\n    Initializing logger\n    2022-01-11 19:46:09,674 | root | INFO | Starting up app insights client\n    2022-01-11 19:46:09,675 | root | INFO | Starting up request id generator\n    2022-01-11 19:46:09,675 | root | INFO | Starting up app insight hooks\n    2022-01-11 19:46:09,675 | root | INFO | Invoking user's init function\n    Loading model from path.\n    2022-01-11 19:46:11,728 | azureml.core | WARNING | Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception cannot import name 'RunType'.\n    Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception cannot import name 'RunType'.\n    2022-01-11 19:46:12,132 | root | ERROR | User's init function failed\n    2022-01-11 19:46:12,133 | root | ERROR | Encountered Exception Traceback (most recent call last):\n      File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 182, in register\n        main.init()\n      File &quot;\/var\/azureml-app\/main.py&quot;, line 35, in init\n        driver_module.init()\n      File &quot;\/structure\/azureml-app\/scripts\/inference\/score.py&quot;, line 67, in init\n        raise e\n      File &quot;\/structure\/azureml-app\/scripts\/inference\/score.py&quot;, line 64, in init\n        model = joblib.load(model_path)\n      File &quot;\/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/python3.6\/site-packages\/joblib\/numpy_pickle.py&quot;, line 605, in load\n        obj = _unpickle(fobj, filename, mmap_mode)\n      File &quot;\/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/python3.6\/site-packages\/joblib\/numpy_pickle.py&quot;, line 529, in _unpickle\n        obj = unpickler.load()\n      File &quot;\/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/python3.6\/pickle.py&quot;, line 1050, in load\n        dispatch[key[0]](self)\n      File &quot;\/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/python3.6\/pickle.py&quot;, line 1347, in load_stack_global\n        self.append(self.find_class(module, name))\n      File &quot;\/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/python3.6\/pickle.py&quot;, line 1388, in find_class\n        __import__(module, level=0)\n    ModuleNotFoundError: No module named 'xgboost'\n\n    2022-01-11 19:46:12,134 | root | INFO | Waiting for logs to be sent to Application Insights before exit.\n    2022-01-11 19:46:12,137 | root | INFO | Waiting 30 seconds for upload.\n    Worker exiting (pid: 37)\n    Shutting down: Master\n    Reason: Worker failed to boot.\n    2022-01-11T19:46:42,562394399+00:00 - gunicorn\/finish 3 0\n    2022-01-11T19:46:42,563843910+00:00 - Exit code 3 is not normal. Killing image.\n<\/code><\/pre>\n<p>Partial deployment logs for a successfully deployed endpoint using the same pkl file:<\/p>\n<pre><code>2022-01-10T20:02:28,608154878+00:00 - rsyslog\/run \n2022-01-10T20:02:28,608160978+00:00 - iot-server\/run \n2022-01-10T20:02:28,609567614+00:00 - gunicorn\/run \n2022-01-10T20:02:28,619823782+00:00 - nginx\/run \n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libcrypto.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\n\/usr\/sbin\/nginx: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libssl.so.1.0.0: no version information available (required by \/usr\/sbin\/nginx)\nrsyslogd: \/azureml-envs\/azureml_5ea1391fd04105b52a0d9fc3d6d367ac\/lib\/libuuid.so.1: no version information available (required by rsyslogd)\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-10T20:02:28,789369303+00:00 - iot-server\/finish 1 0\n2022-01-10T20:02:28,791654562+00:00 - Exit code 1 is normal. Not restarting iot-server.\nStarting gunicorn 19.9.0\nListening at: http:\/\/127.0.0.1:31311 (14)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 40\nSPARK_HOME not set. Skipping PySpark Initialization.\nGenerating new fontManager, this may take some time...\nInitializing logger\n2022-01-10 20:02:30,434 | root | INFO | Starting up app insights client\n2022-01-10 20:02:30,435 | root | INFO | Starting up request id generator\n2022-01-10 20:02:30,435 | root | INFO | Starting up app insight hooks\n2022-01-10 20:02:30,435 | root | INFO | Invoking user's init function\nLoading model from path.\n2022-01-10 20:02:32,892 | azureml.core | WARNING | Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception cannot import name 'RunType'.\nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception cannot import name 'RunType'.\nModel loaded succesfully.\nManagedIdentityCredential will use IMDS\n<\/code><\/pre>\n<p>I have tried utilizing py-xgboost in the Conda File and updating packages, however, I get the following error message:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 182, in register\n    main.init()\n  File &quot;\/var\/azureml-app\/main.py&quot;, line 35, in init\n    driver_module.init()\n  File &quot;\/structure\/azureml-app\/scripts\/inference\/score.py&quot;, line 67, in init\n    raise e\n  File &quot;\/structure\/azureml-app\/scripts\/inference\/score.py&quot;, line 64, in init\n    model = joblib.load(model_path)\n  File &quot;\/azureml-envs\/azureml_a6a4caa8ade8fc5dac7282e2e275c022\/lib\/python3.6\/site-packages\/joblib\/numpy_pickle.py&quot;, line 605, in load\n    obj = _unpickle(fobj, filename, mmap_mode)\n  File &quot;\/azureml-envs\/azureml_a6a4caa8ade8fc5dac7282e2e275c022\/lib\/python3.6\/site-packages\/joblib\/numpy_pickle.py&quot;, line 529, in _unpickle\n    obj = unpickler.load()\n  File &quot;\/azureml-envs\/azureml_a6a4caa8ade8fc5dac7282e2e275c022\/lib\/python3.6\/pickle.py&quot;, line 1050, in load\n    dispatch[key[0]](self)\n  File &quot;\/azureml-envs\/azureml_a6a4caa8ade8fc5dac7282e2e275c022\/lib\/python3.6\/pickle.py&quot;, line 1347, in load_stack_global\n    self.append(self.find_class(module, name))\n  File &quot;\/azureml-envs\/azureml_a6a4caa8ade8fc5dac7282e2e275c022\/lib\/python3.6\/pickle.py&quot;, line 1390, in find_class\n    return _getattribute(sys.modules[module], name)[0]\n  File &quot;\/azureml-envs\/azureml_a6a4caa8ade8fc5dac7282e2e275c022\/lib\/python3.6\/pickle.py&quot;, line 272, in _getattribute\n    .format(name, obj))\nAttributeError: Can't get attribute 'XGBoostLabelEncoder' on &lt;module 'xgboost.compat' from '\/azureml-envs\/azureml_a6a4caa8ade8fc5dac7282e2e275c022\/lib\/python3.6\/site-packages\/xgboost\/compat.py'&gt;\n<\/code><\/pre>\n<p>The hyper parameters within the model created by Azure Auto ML include a XGBoost package from Azure ML:<\/p>\n<pre><code>{\n    &quot;spec_class&quot;: &quot;sklearn&quot;,\n    &quot;class_name&quot;: &quot;XGBoostClassifier&quot;,\n    &quot;module&quot;: &quot;automl.client.core.common.model_wrappers&quot;,\n    &quot;param_args&quot;: [],\n    &quot;param_kwargs&quot;: {\n        &quot;tree_method&quot;: &quot;auto&quot;\n    },\n    &quot;prepared_kwargs&quot;: {}\n}\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":7.0,
        "Question_created_time":1641912514007,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/690970\/azureml-no-module-found-error-on-deployment-of-inf",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"modul error deploy infer model xgboost get error environ deploi endpoint state xgboost includ dockerfil issu environ model abl deploi fine packag error dockerfil mcr microsoft com openmpi cuda cudnn ubuntu user root run mkdir opencl vendor echo libnvidia opencl opencl vendor nvidia icd run apt updat echo success updat apt run apt instal instal recommend cmake libboost dev libboost dev libboost filesystem dev run conda creat gpuexp python pre build lightgbm run usr local src mkdir lightgbm lightgbm git clone recurs branch depth http github com microsoft lightgbm lightgbm mkdir build build cmake duse gpu dopencl librari usr local cuda lib libopencl dopencl includ dir usr local cuda includ instal gpu lightgbm xgboost run bin bash sourc activ gpuexp usr local src lightgbm lightgbm python packag python setup instal precompil pip instal upgrad forc reinstal xgboost sourc deactiv conda channel anaconda conda forg pytorch depend python pip pip core pipelin core telemetri default interpret automl core automl runtim post train automl client train automl runtim post dataset runtim infer schema cpuinfo boto botocor azur storag file datalak azur ident hyper paramet model creat azur auto includ xgboost packag spec class sklearn class xgboostclassifi modul automl client core common model wrapper param arg param kwarg tree method auto prepar kwarg",
        "Question_preprocessed_content":"modul deploy infer model environ deploi endpoint state includ dockerfil environ model abl deploi fine packag dockerfil conda haven includ conda file contain set caus fail environ abl model endpoint section studio visibl deploy log endpoint fail state environ endpoint visibl healthi state partial deploy log deploi endpoint pkl file tri util conda file updat packag hyper paramet model creat azur auto includ packag",
        "Question_gpt_summary_original":"The user is facing several challenges related to connecting an API to an AutoML model. They are seeking guidance on which Google tool to use to connect Google Vertex to the API, which module\/section of Vertex AI to connect the API to in order to receive live data, where to upload the Google Colab notebook with the feature calculations, which tool to use to connect the AutoML model to the feature data, and which tool to use to export the predictions to an API.",
        "Question_gpt_summary":"user face challeng relat connect api automl model seek guidanc googl tool us connect googl vertex api modul section connect api order receiv live data upload googl colab notebook featur calcul tool us connect automl model featur data tool us export predict api"
    },
    {
        "Question_title":"What DVC does when git merge is executed?",
        "Question_body":"<p>I have two git branches. DVC maps a data folder in both of them. When I go into master and merging with develop is correct that DVC does not add any new file inside the data folder created in the develop branch but leaves the folder as it is unchanged?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_created_time":1662026528437,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":139.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/what-dvc-does-when-git-merge-is-executed\/1316",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"git merg execut git branch map data folder master merg develop correct add new file insid data folder creat develop branch leav folder unchang",
        "Question_preprocessed_content":"git merg execut git branch map data folder master merg develop new file insid data folder creat develop branch leav folder unchang",
        "Question_gpt_summary_original":"The user has two git branches with DVC mapping a data folder in both. When merging the master branch with the develop branch, DVC does not add any new files inside the data folder created in the develop branch and leaves the folder unchanged.",
        "Question_gpt_summary":"user git branch map data folder merg master branch develop branch add new file insid data folder creat develop branch leav folder unchang"
    },
    {
        "Question_title":"Seaborn kdeplot works in local - gives error in amazon sagemaker",
        "Question_body":"<p>I am running the following straightforward code:<\/p>\n<pre><code>sns.kdeplot(data=subset_data['VaribleA'],fill=True,  color = &quot;#FF0000&quot; , linewidth=1, bw_method = 0.25)\n<\/code><\/pre>\n<p>When I run it on my local machine, I am able to run it without any issues.<\/p>\n<p>But when I run it on an Amazon Sagemaker Jupyter Notebook, but installing and initializing all the same libraries, I get the following error.<\/p>\n<pre><code>AttributeError: 'Line2D' object has no property 'fill'\n<\/code><\/pre>\n<p>Has anyone else faced this issue? Any recommendations on how to fix it?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1648694340420,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1648834156916,
        "Question_score_count":1.0,
        "Question_view_count":72.0,
        "Poster_created_time":1469047536060,
        "Poster_location":null,
        "Poster_reputation_count":49.0,
        "Poster_view_count":21.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71686302",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"seaborn kdeplot work local give error run follow straightforward code sn kdeplot data subset data variblea true color linewidth method run local machin abl run issu run jupyt notebook instal initi librari follow error attributeerror line object properti face issu recommend fix",
        "Question_preprocessed_content":"seaborn kdeplot work local give straightforward code run local machin abl run run jupyt initi librari face fix",
        "Question_gpt_summary_original":"The user is facing an issue with Seaborn kdeplot while running it on Amazon Sagemaker Jupyter Notebook. The code works fine on their local machine but gives an error on Sagemaker. The error message says that 'Line2D' object has no property 'fill'. The user is seeking recommendations to fix this issue.",
        "Question_gpt_summary":"user face issu seaborn kdeplot run jupyt notebook code work fine local machin give error error messag sai line object properti user seek recommend fix issu"
    },
    {
        "Question_title":"Output model metrics to Cloudwatch",
        "Question_body":"<p>I am following the mnist-2 guide from the aws github documentation to implement my own training job <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving<\/a>. I have wrote my code using a similar structure, but I would like to visualise the training and validation metrics from Cloudwatch while the job is running. Do I need to manually specify the metrics I am trying to observe? The AWS guide states &quot;<em>SageMaker automatically parses the logs for metrics that built-in algorithms emit and sends those metrics to CloudWatch.<\/em>&quot; I am only using Tensorflow's training and validation accuracy and loss metrics, which I am not sure if they are built-in, or if I need to call them manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1616769637850,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":475.0,
        "Poster_created_time":1533753580750,
        "Poster_location":null,
        "Poster_reputation_count":344.0,
        "Poster_view_count":63.0,
        "Answer_body":"<p>If you are not using a built-in algorithm, like in the example you linked, you have to define your metrics when you create the training job. You have to define regex expressions to grab from the logs the metric values, then cloudwatch will plot for you. The x axis will be the timestamp, you cannot change it.\nBasically just run your traning job and observe how the metrics are outputted, then you can build the appropriate regex. For example, since I am using coco metrics in tensorflow which periodically produce this:<\/p>\n<pre><code>INFO:tensorflow:Saving dict for global step 1109: DetectionBoxes_Precision\/mAP = 0.111895345, DetectionBoxes_Precision\/mAP (large) = 0.12102994, DetectionBoxes_Precision\/mAP (medium) = 0.050807837, DetectionBoxes_Precision\/mAP (small) = -1.0, DetectionBoxes_Precision\/mAP@.50IOU = 0.33130914, DetectionBoxes_Precision\/mAP@.75IOU = 0.03787096, DetectionBoxes_Recall\/AR@1 = 0.18493989, DetectionBoxes_Recall\/AR@10 = 0.36792925, DetectionBoxes_Recall\/AR@100 = 0.48543888, DetectionBoxes_Recall\/AR@100 (large) = 0.5131599, DetectionBoxes_Recall\/AR@100 (medium) = 0.21598063, DetectionBoxes_Recall\/AR@100 (small) = -1.0, Loss\/classification_loss = 0.8041124, Loss\/localization_loss = 0.35313264, Loss\/regularization_loss = 0.15211834, Loss\/total_loss = 1.30936, global_step = 1109, learning_rate = 0.28119853, loss = 1.30936\n<\/code><\/pre>\n<p>I use to grab the total loss for example:<\/p>\n<pre><code>INFO.*Loss\\\/total_loss = ([0-9\\.]+) \n<\/code><\/pre>\n<p>That's it, cloudwatch automatically plot the total_loss in time.<\/p>\n<p>You can define metrics either in the console or in the notebook, like this (just an example from my code):<\/p>\n<pre><code>metrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n<\/code><\/pre>\n<p>In order to test your regex, you can use a tool like <a href=\"https:\/\/regex101.com\/\" rel=\"nofollow noreferrer\">this<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1616771138300,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1416346350292,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation_count":2302.0,
        "Answerer_view_count":227.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66819026",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"output model metric cloudwatch follow mnist guid aw github document implement train job http github com aw amazon exampl tree master python sdk tensorflow script mode train serv wrote code similar structur like visualis train valid metric cloudwatch job run need manual specifi metric try observ aw guid state automat pars log metric built algorithm emit send metric cloudwatch tensorflow train valid accuraci loss metric sure built need manual",
        "Question_preprocessed_content":"output model metric cloudwatch guid aw github document implement train job wrote code similar structur like visualis train valid metric cloudwatch job specifi metric try observ aw guid state pars log metric algorithm emit send metric tensorflow train valid metric sure",
        "Question_gpt_summary_original":"The user is following the mnist-2 guide from AWS Github documentation to implement their own training job. They want to visualize the training and validation metrics from Cloudwatch while the job is running. They are unsure if they need to manually specify the metrics they want to observe or if SageMaker automatically parses the logs for metrics that built-in algorithms emit and sends those metrics to CloudWatch. They are only using Tensorflow's training and validation accuracy and loss metrics, which they are not sure if they are built-in or if they need to call them manually.",
        "Question_gpt_summary":"user follow mnist guid aw github document implement train job want visual train valid metric cloudwatch job run unsur need manual specifi metric want observ automat pars log metric built algorithm emit send metric cloudwatch tensorflow train valid accuraci loss metric sure built need manual"
    },
    {
        "Question_title":"Load a Picked or Joblib Pre trained ML Model to Sagemaker and host as endpoint",
        "Question_body":"<p>If I have a trained model in Using pickle, or Joblib.\nLets say its Logistic regression or XGBoost.<\/p>\n<p>I would like to host that model in AWS Sagemaker as endpoint without running a training job.\nHow to achieve that.<\/p>\n<pre><code>#Lets Say myBucketName contains model.pkl\nmodel = joblib.load('filename.pkl')  \n# X_test = Numpy Array \nmodel.predict(X_test)  \n<\/code><\/pre>\n<p>I am not interested to <code>sklearn_estimator.fit('S3 Train, S3 Validate' )<\/code> , I have the trained model<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1599661148083,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":1622.0,
        "Poster_created_time":1370509408500,
        "Poster_location":null,
        "Poster_reputation_count":1573.0,
        "Poster_view_count":194.0,
        "Answer_body":"<p>For Scikit Learn for example, you can get inspiration from this public demo <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>\n<p>Step 1: Save your artifact (eg the joblib) compressed in S3 at <code>s3:\/\/&lt;your path&gt;\/model.tar.gz<\/code><\/p>\n<p>Step 2: Create an inference script with the deserialization function <code>model_fn<\/code>. (Note that you could also add custom inference functions <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code> but for scikit the defaults function work fine)<\/p>\n<pre><code>%%writefile inference_script.py. # Jupiter command to create file in case you're in Jupiter\n\nimport joblib\nimport os\n\ndef model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>Step 3: Create a model associating the artifact with the right container<\/p>\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/&lt;your path&gt;\/model.tar.gz',\n    role='&lt;your role&gt;',\n    entry_point='inference_script.py',\n    framework_version='0.23-1')\n<\/code><\/pre>\n<p>Step 4: Deploy!<\/p>\n<pre><code>model.deploy(\n    instance_type='ml.c5.large',  # choose the right instance type\n    initial_instance_count=1)\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1599722577176,
        "Answer_last_edit_time":null,
        "Answer_score_count":5.0,
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63813624",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"load pick joblib pre train model host endpoint train model pickl joblib let logist regress xgboost like host model endpoint run train job achiev let mybucketnam contain model pkl model joblib load filenam pkl test numpi arrai model predict test interest sklearn estim fit train valid train model",
        "Question_preprocessed_content":"load pick joblib pre train model host endpoint train model pickl joblib let logist like host model endpoint train job achiev interest train model",
        "Question_gpt_summary_original":"The user wants to know how to host a pre-trained machine learning model, specifically a Logistic Regression or XGBoost model, on AWS Sagemaker as an endpoint without running a training job. They have the model saved using pickle or Joblib and do not want to use the <code>sklearn_estimator.fit('S3 Train, S3 Validate')<\/code> method.",
        "Question_gpt_summary":"user want know host pre train machin learn model specif logist regress xgboost model endpoint run train job model save pickl joblib want us sklearn estim fit train valid method"
    },
    {
        "Question_title":"Artifacts (local) caching - how does it really work?",
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m trying to figure out how does the caching  of artifacts work. Let\u2019s say I want to download a model artifact to run some evaluation on. I don\u2019t need the file on disk to persist rather I just want to load it into memory. What I do right now in my evaluation script is:<\/p>\n<pre><code class=\"lang-auto\">import tempfile\nimport wandb\n\nartifact = wandb.use_artifact(model_weights_uri)\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    artifact.download(tmpdirname)\n    model_weights = load_pickle(os.path.join(tmpdirname, \"model_weights.pickle\"))\n<\/code><\/pre>\n<p>And from that point on I use the <code>model_weights<\/code> as it was loaded into memory.<\/p>\n<p>My first question is: if I run the code twice (on the same machine), <strong>will the model-weights be downloaded again<\/strong> or are they cached somewhere? assuming the logged artifact wasn\u2019t changed of course. And if they are cached, where are they cached?<br>\nI\u2019m also not clear about the <code>artifact<\/code> directory (which is used if I run <code>artifact.download()<\/code> without any argument). Does that directory serve as cache? if so, what does the <code>.cache<\/code> directory used for?<\/p>\n<p>I would appreciate answers to my questions and perhaps a  general explanation of the artifact caching mechanism &amp; best practices.<\/p>\n<p>Thanks!<br>\nRan<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1650203452610,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":861.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ranshadmi-nexite\">@ranshadmi-nexite<\/a>,<\/p>\n<p>Thank you for your question. You are right, all Artifacts are cached on your system under <code>~\/.cache\/wandb\/artifacts<\/code> and organized by their checksum. So if you try to download a file with checksum <code>x<\/code> and that file has been logged in an Artifact from your machine or downloaded to your machine as part of an artifact before, we just pull it from the cache by checking if there is a cached Artifact file with checksum <code>x<\/code>.<\/p>\n<p>So, if you run the same code twice, assuming the version of the artifact you are trying to download has not changed, the artifact can simply be pickked up from your cache directory.<\/p>\n<p>Also, when calling <code>artifact.download()<\/code> without any arguments, the artifact is saved in the directory in which the code is running. This, however,  is not the directory that serves as a cache, that still remains <code>.cache<\/code> which acts as a central location to look for artifacts before fetching it.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1650312955393,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/artifacts-local-caching-how-does-it-really-work\/2255",
        "Tool":"Weights & Biases",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"artifact local cach work try figur cach artifact work let want download model artifact run evalu dont need file disk persist want load memori right evalu script import tempfil import artifact us artifact model weight uri tempfil temporarydirectori tmpdirnam artifact download tmpdirnam model weight load pickl path join tmpdirnam model weight pickl point us model weight load memori question run code twice machin model weight download cach assum log artifact wasnt chang cours cach cach clear artifact directori run artifact download argument directori serv cach cach directori appreci answer question gener explan artifact cach mechan best practic thank ran",
        "Question_preprocessed_content":"artifact cach work try figur cach artifact work let want download model artifact run evalu dont file disk persist want load memori right evalu script point us load memori question run code twice download cach artifact wasnt chang cours cach cach clear directori directori serv cach directori answer question gener explan artifact cach mechan best practic thank ran",
        "Question_gpt_summary_original":"The user is trying to understand how the caching of artifacts works in their evaluation script. They want to know if the model-weights will be downloaded again if they run the code twice on the same machine and where the cached files are stored. They also have questions about the artifact directory and the .cache directory. The user is seeking answers and best practices for artifact caching.",
        "Question_gpt_summary":"user try understand cach artifact work evalu script want know model weight download run code twice machin cach file store question artifact directori cach directori user seek answer best practic artifact cach"
    },
    {
        "Question_title":"Specify AWS profile when adding external data from S3",
        "Question_body":"<p>Hi all,<\/p>\n<p>how can I make DVC use a specific AWS-profile when adding external data?<\/p>\n<p>If my credentials are stored in the default-profile I can add external data as follows (just following the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">S3-Example for Managing External Data<\/a>)<\/p>\n<pre><code class=\"lang-auto\">$ git init\n$ dvc init\n$ dvc remote add s3cache s3:my-bucket\/cache\n$ dvc config cache.s3 s3cache\n$ dvc add --external s3:\/\/my-bucket\/remote-data.txt\n<\/code><\/pre>\n<p>But I want to use another profile (<code>MyProfile<\/code>) that I can add to <code>s3cache<\/code>:<\/p>\n<pre><code class=\"lang-auto\">$ dvc remote modify s3cache profile MyProfile\n<\/code><\/pre>\n<p>If the default profile is removed or changed from the <code>.aws\/credentials<\/code> the following gives a <code>Bad Request<\/code>:<\/p>\n<pre><code class=\"lang-auto\">$ dvc add --external s3:\/\/my-bucket\/remote-data.txt\n# ERROR: unexpected error - [Errno 22] Bad Request: An error occurred (400) when calling the HeadObject operation: Bad Request\n<\/code><\/pre>\n<p>How can I provide the profile information to the add?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_created_time":1647607974097,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":1159.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/specify-aws-profile-when-adding-external-data-from-s3\/1126",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"specifi aw profil ad extern data us specif aw profil ad extern data credenti store default profil add extern data follow follow exampl manag extern data git init init remot add scach bucket cach config cach scach add extern bucket remot data txt want us profil myprofil add scach remot modifi scach profil myprofil default profil remov chang aw credenti follow give bad request add extern bucket remot data txt error unexpect error errno bad request error occur call headobject oper bad request provid profil inform add",
        "Question_preprocessed_content":"specifi aw profil extern data us specif extern data credenti store extern data want us profil default profil remov chang give provid profil inform",
        "Question_gpt_summary_original":"The user is trying to add external data from S3 using DVC but is facing issues in specifying a specific AWS profile. The user has tried modifying the S3 cache profile but is still encountering errors when the default profile is removed or changed from the .aws\/credentials file. The user is seeking guidance on how to provide profile information to the add command.",
        "Question_gpt_summary":"user try add extern data face issu specifi specif aw profil user tri modifi cach profil encount error default profil remov chang aw credenti file user seek guidanc provid profil inform add command"
    },
    {
        "Question_title":"Confusion on multistep operations, restarting substeps, and copied files?",
        "Question_body":"<p>I have a guild operation <code>main<\/code> that runs 3 steps which are other operations: <code>impute<\/code>, <code>evaluate<\/code>, and <code>predict<\/code>. The latter two require on the <code>impute<\/code> operation (specifically a model checkpoint and some data output).<br>\n(<a href=\"https:\/\/github.com\/davzaman\/autopopulus\/blob\/dev\/guild.yml\" rel=\"noopener nofollow ugc\">guild.yml<\/a> file for reference)<\/p>\n<ol>\n<li>When one of the steps fails (e.g. <code>evaluate<\/code>), the <code>main<\/code> op shows error and so does <code>evaluate<\/code>. If I fix the error in the code and restart the run with something like <code>for hash in $(guild select --operation evaluate --error --all); do guild run -y --background --restart $hash --force-sourcecode; done<\/code>,  then the <code>evaluate<\/code> op fixes to completed, but the <code>main<\/code> operation does not. It doesn\u2019t seem very possible to update it, but it is slightly unclean and annoying to keep track of what broke and what is fixed. I end up with something like:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\">[71:ec03c916]   evaluate  2023-02-20 14:43:57  completed  dvae myexperiment\n[72:957ecb30]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n[73:19493e6b]   evaluate  2023-02-20 14:43:56  completed  dvae myexperiment\n...\n[127:fe72a7ff]  predict   2023-02-18 20:58:56  completed  dvae \n[128:617bc8fd]  impute    2023-02-18 20:26:16  completed  dvae myexperiment\n[129:2b155ff0]  main      2023-02-18 20:26:14  error      dvae \n[130:39125144]  predict   2023-02-18 20:21:08  completed  dvae \n[131:5c4ed46a]  impute    2023-02-18 19:45:25  completed  dvae myexperiment\n[132:c542fcbe]  main      2023-02-18 19:45:24  error      dvae \n<\/code><\/pre>\n<p>It said <code>error<\/code> for <code>main<\/code> but it\u2019s really been fixed sine the <code>evaluate<\/code> op was fixed.<br>\nAnother issue is also what files are stored under each op which leads me to the next point, where ill use <code>run 132<\/code> as an example:<\/p>\n<ol start=\"2\">\n<li>If I look at what is stored under the <code>main<\/code> op I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-shell\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ ls\nevaluate  impute  options.yml  predict\n<\/code><\/pre>\n<p>If I drill into the directories I see:<\/p>\n<pre><code class=\"lang-plaintext\">me@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a$ cd evaluate\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/c542fcbe24ab4a86b1ea0e33fabd839a\/evaluate$ ls\nF.O.  options.yml  serialized_models\n<\/code><\/pre>\n<p>If <code>evaluate<\/code> fails and I rerun it, does that mean that the <code>evaluate<\/code>folder will be updated too (is it a symlink)? There seems to be some redundancy too which leads me to:<\/p>\n<ol start=\"3\">\n<li>If I look at the output of the substeps <code>impute<\/code> and <code>predict<\/code> I see:<\/li>\n<\/ol>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a\/serialized_models$ ls\nAEDitto_STATIC.pt  imputed_data.pkl  STATIC_test_dataloader.pt\n<\/code><\/pre>\n<p>I also see<\/p>\n<pre><code class=\"lang-plaintext\"># impute op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/5c4ed46a12e145158b2351621ee81345$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n# predict op\nme@machine:~\/mambaforge\/envs\/ap\/.guild\/runs\/391251446fe041048d93d80deda6ac8a$ ls F.O.\/0.33\/MNAR\\(G\\)\/dvae\/lightning_logs\/version_0\/\nevents.out.tfevents.1676780435.lambda2.6521.0\nevents.out.tfevents.1676780445.lambda2.6521.1\nevents.out.tfevents.1676780453.lambda2.6521.2\nhparams.yaml\n<\/code><\/pre>\n<p>It looks like it copies over everything from the <code>impute<\/code> op top the parents: <code>main<\/code>, and dependent steps: <code>predict<\/code>, and <code>evaluate<\/code>. This is a lot of redundancy especially for expensive\/large models and artifacts. This is making me run out of space on my machine.<\/p>\n<p>My questions are<br>\na) How do I avoid redundancy in stored artifacts between parent and child steps like <code>main<\/code> having substeps.<br>\nb) How do I avoid redundancy amongst sibling runs where one may be dependent on another? While <code>evaluate<\/code> relies on the artifacts from <code>impute<\/code> I don\u2019t want it to store all the artifacts all over again (including the model checkpoints, data, and the logging files), I just want <code>evaluate<\/code> to use the checkpointed data and model. <a href=\"https:\/\/my.guild.ai\/t\/guild-file-cheatsheet\/192#required-operation-files-14\">I know there\u2019s a <code>select:<\/code> option<\/a> but it seems to be regex, making it complicated to select the checkpointed model AND data. Also even if that solves excluding the logged files, I don\u2019t want to copy over the files it relies on to the final logged artifacts.<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":null,
        "Question_created_time":1676938696104,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":128.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/davzaman\">@davzaman<\/a> It looks like there was a regression and Guild is indeed <em>copying<\/em> resolved operation dependency files. This is not the intended behavior and we\u2019ll fix that ASAP.<\/p>\n<p>As a workaround, avoid copying files by adding <code>target-type<\/code> to your dependency def like this:<\/p>\n<pre><code class=\"lang-yaml\">upstream: {}\n\ndownstream:\n  requires:\n    - operation: upstream\n      target-type: link  # tells Guild to link to the resolved files, not copy\n<\/code><\/pre>\n<p>The <code>downstream<\/code> operation is any operation that requires an upstream run.<\/p>\n<p>Sorry about that! This will make a big difference in disk space for you. We\u2019ll post here when the fix is applied, after which you can remove the explicit <code>target-type<\/code> in your dependencies.<\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1678831800198,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/confusion-on-multistep-operations-restarting-substeps-and-copied-files\/998",
        "Tool":"Guild AI",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"confus multistep oper restart substep copi file oper main run step oper imput evalu predict requir imput oper specif model checkpoint data output yml file refer step fail evalu main show error evalu fix error code restart run like hash select oper evalu error run background restart hash forc sourcecod evalu fix complet main oper doesnt possibl updat slightli unclean annoi track broke fix end like ecc evalu complet dvae myexperi ecb evalu complet dvae myexperi evalu complet dvae myexperi feaff predict complet dvae bcfd imput complet dvae myexperi bff main error dvae predict complet dvae ceda imput complet dvae myexperi cfcbe main error dvae said error main fix sine evalu fix issu file store lead point ill us run exampl look store main machin mambaforg env run cfcbeababeaefabda evalu imput option yml predict drill directori machin mambaforg env run cfcbeababeaefabda evalu machin mambaforg env run cfcbeababeaefabda evalu option yml serial model evalu fail rerun mean evaluatefold updat symlink redund lead look output substep imput predict imput machin mambaforg env run cedaebe serial model aeditto static imput data pkl static test dataload predict machin mambaforg env run fedddedaaca serial model aeditto static imput data pkl static test dataload imput machin mambaforg env run cedaebe mnar dvae lightn log version event tfevent lambda event tfevent lambda event tfevent lambda hparam yaml predict machin mambaforg env run fedddedaaca mnar dvae lightn log version event tfevent lambda event tfevent lambda event tfevent lambda hparam yaml look like copi imput parent main depend step predict evalu lot redund especi expens larg model artifact make run space machin question avoid redund store artifact parent child step like main have substep avoid redund sibl run depend evalu reli artifact imput dont want store artifact includ model checkpoint data log file want evalu us checkpoint data model know there select option regex make complic select checkpoint model data solv exclud log file dont want copi file reli final log artifact",
        "Question_preprocessed_content":"confus multistep oper restart substep copi file oper run step oper requir oper yml file refer step fail show fix code restart run like fix complet oper doesnt updat slightli unclean track broke fix end like said fix sine fix file store lead point us exampl store directori fail rerun mean folder updat redund lead output substep like copi parent depend step lot redund model artifact make run space machin question avoid redund store artifact parent child step like have substep avoid redund sibl run depend reli artifact dont want store artifact want us checkpoint data model know there option regex make complic select checkpoint model data solv exclud file dont want copi file reli final artifact",
        "Question_gpt_summary_original":"The user is facing confusion regarding multistep operations, restarting substeps, and copied files. When one of the steps fails, the main operation shows an error and fixing the error in the code and restarting the run does not update the main operation. The user is also facing redundancy in stored artifacts between parent and child steps and amongst sibling runs, which is making them run out of space on their machine. They are seeking solutions to avoid redundancy in stored artifacts and to avoid copying over files that the final logged artifacts rely on.",
        "Question_gpt_summary":"user face confus multistep oper restart substep copi file step fail main oper show error fix error code restart run updat main oper user face redund store artifact parent child step sibl run make run space machin seek solut avoid redund store artifact avoid copi file final log artifact reli"
    },
    {
        "Question_title":"Wandb Agent - some runs fail",
        "Question_body":"<p>I\u2019ve been using the Wandb agent to do some hyperparameter optimization with sweep config. Somehow approx. 70% of my runs fails.  This message is persisted:<\/p>\n<p>wandb: ERROR Run kyg8jl2m errored: InternalError()<\/p>\n<p>Is this a known issue?<\/p>\n<p>edit: I just did another 20 runs<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6fa137232597a0d819fa8d93bcea31af5fc491c8.png\" alt=\"image\" data-base62-sha1=\"fVwoTFqCbnNIdB63Atkf4ye1nm0\" width=\"585\" height=\"211\"><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1635197682205,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":318.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-agent-some-runs-fail\/1094",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"agent run fail iv agent hyperparamet optim sweep config approx run fail messag persist error run kygjlm error internalerror known issu edit run",
        "Question_preprocessed_content":"agent run fail iv agent hyperparamet optim config run fail persist run kyg known edit run",
        "Question_gpt_summary_original":"The user is facing issues with the Wandb agent while doing hyperparameter optimization with sweep config, as around 70% of the runs are failing with an InternalError() message. The user is seeking to know if this is a known issue.",
        "Question_gpt_summary":"user face issu agent hyperparamet optim sweep config run fail internalerror messag user seek know known issu"
    },
    {
        "Question_title":"Vertex AI not training custom jobs in batches larger than 20?",
        "Question_body":"Whenever I try to train more than 20 jobs (custom containers) in parallel it seems like the jobs enter a queue such that only 20 run at one time, my quotas should allow me to do much more (cpu and vertex ai api way under limit, same with all my compute stuff).\n\nIs there any way to see why those jobs aren't starting instantly or to increase the number of custom jobs running at one time? I don't even see them in pending but when I submit them all using sync=False in the workbench they get submitted with no error instantly.\n\nI also tried to throttle the start of each training job incase its a per min request limit thats causing the limit of 20 parallel jobs but that didnt solve it.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1675596180000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":248.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-not-training-custom-jobs-in-batches-larger-than-20\/td-p\/518322\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"train custom job batch larger try train job custom contain parallel like job enter queue run time quota allow cpu api wai limit comput stuff wai job aren start instantli increas number custom job run time pend submit sync fals workbench submit error instantli tri throttl start train job incas min request limit that caus limit parallel job didnt solv",
        "Question_preprocessed_content":"train custom job batch larger try train job like job enter queue run time quota wai job aren start instantli increas number custom job time pend submit sync fals workbench instantli tri start train job incas min request limit that caus limit job didnt solv",
        "Question_gpt_summary_original":"The user is facing an issue with Vertex AI where custom jobs are not being trained in batches larger than 20. Even though the user's quotas allow for more jobs to be trained, only 20 jobs run at one time, and the jobs seem to enter a queue. The user is looking for a way to increase the number of custom jobs running at one time or to see why the jobs aren't starting instantly. The user has tried throttling the start of each training job, but it did not solve the issue.",
        "Question_gpt_summary":"user face issu custom job train batch larger user quota allow job train job run time job enter queue user look wai increas number custom job run time job aren start instantli user tri throttl start train job solv issu"
    },
    {
        "Question_title":"Sagemaker: How to debug Model monitoring(data quality and model quality)?",
        "Question_body":"<p>I have created a Data Quality monitoring from Sagemaker Studio UI and also created using sagemaker SDK <a href=\"https:\/\/stackoverflow.com\/q\/69179914\/11844406\">code<\/a>, I referred to create model Data Quality monitoring job.<\/p>\n<p><strong>Errors:<\/strong><\/p>\n<ol>\n<li>when there is no captured data (this is expected)<\/li>\n<\/ol>\n<blockquote>\n<p>Monitoring job failure reason:<\/p>\n<p>Job inputs had no data<\/p>\n<\/blockquote>\n<ol start=\"2\">\n<li>From logs, I can see that it is using <code>Java<\/code> in background. Not sure how to debug?<\/li>\n<\/ol>\n<blockquote>\n<p>org.json4s.package$MappingException: Do not know how to convert\nJObject(List(0,JDouble(38.0))) into class java.lang.String.<\/p>\n<\/blockquote>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3da3i.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3da3i.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Once we create the DataQuality monitoring job using Sagemaker Studio UI or Sagemkaer python sdk, it is taking a hour to start. I would like to know is there a way to debug monitoring job without waiting for a hour every time we get a error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1643780042067,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":88.0,
        "Poster_created_time":1564208933767,
        "Poster_location":null,
        "Poster_reputation_count":491.0,
        "Poster_view_count":59.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70950914",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"debug model monitor data qualiti model qualiti creat data qualiti monitor studio creat sdk code refer creat model data qualiti monitor job error captur data expect monitor job failur reason job input data log java background sure debug org json packag mappingexcept know convert jobject list jdoubl class java lang string creat dataqu monitor job studio sagemka python sdk take hour start like know wai debug monitor job wait hour time error",
        "Question_preprocessed_content":"debug model monitor creat data qualiti monitor studio creat sdk code creat model data qualiti monitor job captur data monitor job failur reason job input data log background sure debug know convert creat dataqu monitor job studio sagemka python sdk take hour start like know wai debug monitor job wait hour time",
        "Question_gpt_summary_original":"The user has encountered issues with debugging model monitoring for data quality and model quality in Sagemaker. They have created a Data Quality monitoring job using Sagemaker Studio UI and SDK, but are facing errors when there is no captured data and are unsure how to debug the job as it uses Java in the background. Additionally, the job takes an hour to start, and the user is looking for a way to debug it without waiting for an hour every time an error occurs.",
        "Question_gpt_summary":"user encount issu debug model monitor data qualiti model qualiti creat data qualiti monitor job studio sdk face error captur data unsur debug job us java background addition job take hour start user look wai debug wait hour time error occur"
    },
    {
        "Question_title":"AzureML Model Register",
        "Question_body":"<p>I was trying to register a model using the <code>Run<\/code> Class like this:<\/p>\n<pre><code>model = run.register_model(\n    model_name=model_name,\n    model_path=model_path)\n<\/code><\/pre>\n<p>Errors with message: <code>Could not locate the provided model_path ... in the set of files uploaded to the run...<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1643643837027,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":319.0,
        "Poster_created_time":1589293508567,
        "Poster_location":null,
        "Poster_reputation_count":833.0,
        "Poster_view_count":55.0,
        "Answer_body":"<p>The only way I found to fix the issue was to use the <code>Model<\/code> Class instead:<\/p>\n<pre><code>        model = Model.register(\n            workspace=ws,\n            model_name=model_name,\n            model_path=model_path,\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version=sklearn.__version__,\n            description='Model Deescription',\n            tags={'Name' : 'ModelName', 'Type' : 'Production'},\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version='1.0'\n            )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1643643837027,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1589293508567,
        "Answerer_location":null,
        "Answerer_reputation_count":833.0,
        "Answerer_view_count":55.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70928761",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"model regist try regist model run class like model run regist model model model model path model path error messag locat provid model path set file upload run",
        "Question_preprocessed_content":"model regist try regist model like",
        "Question_gpt_summary_original":"The user encountered an error while trying to register a model using the Run Class in AzureML. The error message stated that the provided model_path could not be located in the uploaded files.",
        "Question_gpt_summary":"user encount error try regist model run class error messag state provid model path locat upload file"
    },
    {
        "Question_title":"MLflow 2.2.2 released!",
        "Question_body":"Hi all,\n\n\nWe are happy to announce the availability of\u00a0MLflow 2.2.2!\n\nMLflow 2.2.2 is a patch release containing the following bug fixes:\n\n[Model Registry] Allow\u00a0source\u00a0to be a local path within a run\u2019s artifact directory if a\u00a0run_id\u00a0is specified (#7993, @harupy)\n[Model Registry] Fix a bug where a windows UNC path is considered a local path (#7988, @WeichenXu123)\n[Model Registry] Disallow\u00a0name\u00a0to be a file path in\u00a0FileStore.get_registered_model\u00a0(#7965, @harupy)\n\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1678755504000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":9.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/TFMjnuoc-Hg",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"releas happi announc avail patch releas contain follow bug fix model registri allowsourceto local path run artifact directori arun idi specifi harupi model registri fix bug window unc path consid local path weichenxu model registri disallownameto file path infilestor regist model harupi comprehens list chang thereleas chang log check latest document org",
        "Question_preprocessed_content":"releas avail patch releas contain bug fix model registri local path run artifact directori specifi model registri fix bug window unc path consid local path model registri file path comprehens list chang thereleas chang log check latest document",
        "Question_gpt_summary_original":"The user has encountered no issue post, but rather an announcement of the availability of MLflow 2.2.2, which is a patch release containing bug fixes related to Model Registry. The fixes include allowing the source to be a local path within a run's artifact directory, fixing a bug where a windows UNC path is considered a local path, and disallowing name to be a file path in FileStore.get_registered_model.",
        "Question_gpt_summary":"user encount issu post announc avail patch releas contain bug fix relat model registri fix includ allow sourc local path run artifact directori fix bug window unc path consid local path disallow file path filestor regist model"
    },
    {
        "Question_title":"Experiment added into collection becomes a deleted item",
        "Question_body":"<p>I cannot add these experiments into my collections. Whenever I add them, and then refresh my page to see if they are in the collections, they become deleted items. Is this some sort of bug?<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/15905-screen-shot-2020-08-05-at-81447-pm.png?platform=QnA\" alt=\"15905-screen-shot-2020-08-05-at-81447-pm.png\" \/><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/15947-screen-shot-2020-08-05-at-81541-pm.png?platform=QnA\" alt=\"15947-screen-shot-2020-08-05-at-81541-pm.png\" \/>    <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_created_time":1596676751863,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/60252\/experiment-added-into-collection-becomes-a-deleted",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"experi ad collect delet item add experi collect add refresh page collect delet item sort bug",
        "Question_preprocessed_content":"experi delet item experi refresh page delet item sort bug",
        "Question_gpt_summary_original":"The user is unable to add experiments to their collections as they become deleted items after being added and refreshed, indicating a possible bug.",
        "Question_gpt_summary":"user unabl add experi collect delet item ad refresh indic possibl bug"
    },
    {
        "Question_title":"MemoryError: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
        "Question_body":"<p>I'm trying to deploy a model using Azure Machine Learning. In the init() method of the score.py (the entry script), I try to load in a Google Word2Vec model (<a href=\"https:\/\/mccormickml.com\/2016\/04\/12\/googles-pretrained-word2vec-model-in-python\/\">https:\/\/mccormickml.com\/2016\/04\/12\/googles-pretrained-word2vec-model-in-python\/<\/a>). When trying to create an endpoint, the following exception is thrown:   <\/p>\n<p>MemoryError: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32  <\/p>\n<p>I'm using a Standard_DS12_v2 compute instance, from which I would expect that the specified ram and storage should be sufficient to handle the 3.35 GiB.   <\/p>\n<p>Any suggested solutions? Thanks a lot<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1645539137690,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/745993\/memoryerror-unable-to-allocate-3-35-gib-for-an-arr",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"memoryerror unabl alloc gib arrai shape data type float try deploi model init method score entri script try load googl wordvec model http mccormickml com googl pretrain wordvec model python try creat endpoint follow except thrown memoryerror unabl alloc gib arrai shape data type float standard comput instanc expect specifi ram storag suffici handl gib suggest solut thank lot",
        "Question_preprocessed_content":"unabl gib shape data type float try deploi model init method try load word vec model try creat endpoint except thrown unabl gib shape data type float comput instanc expect specifi ram storag handl gib solut thank lot",
        "Question_gpt_summary_original":"The user is encountering a MemoryError while trying to deploy a model using Azure Machine Learning. The error occurs when trying to load a Google Word2Vec model in the init() method of the score.py file. The error message indicates that the system is unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32. The user is using a Standard_DS12_v2 compute instance, which should have sufficient RAM and storage to handle the model. The user is seeking suggestions for a solution.",
        "Question_gpt_summary":"user encount memoryerror try deploi model error occur try load googl wordvec model init method score file error messag indic unabl alloc gib arrai shape data type float user standard comput instanc suffici ram storag handl model user seek suggest solut"
    },
    {
        "Question_title":"Is there any equivalent of hyperopts lognormal in Optuna?",
        "Question_body":"<p>I am trying to use Optuna for hyperparameter tuning of my model.<\/p>\n<p>I am stuck in a place where I want to define a search space having lognormal\/normal distribution. It is possible in <code>hyperopt<\/code> using <code>hp.lognormal<\/code>. Is it possible to define such a space using a combination of the existing <code>suggest_<\/code> api of <code>Optuna<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_created_time":1610971789293,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1610981620636,
        "Question_score_count":0.0,
        "Question_view_count":140.0,
        "Poster_created_time":1445250318392,
        "Poster_location":null,
        "Poster_reputation_count":2164.0,
        "Poster_view_count":298.0,
        "Answer_body":"<p>You could perhaps make use of inverse transforms from <code>suggest_float(..., 0, 1)<\/code> (i.e. U(0, 1)) since Optuna currently doesn't provide <code>suggest_<\/code> variants for those two distributions directly. This example might be a starting point <a href=\"https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d\" rel=\"nofollow noreferrer\">https:\/\/gist.github.com\/hvy\/4ef02ee2945fe50718c71953e1d6381d<\/a>\nPlease find the code below<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import erfcinv\n\nimport optuna\n\n\ndef objective(trial):\n    # Suggest from U(0, 1) with Optuna.\n    x = trial.suggest_float(&quot;x&quot;, 0, 1)\n\n    # Inverse transform into normal.\n    y0 = norm.ppf(x, loc=0, scale=1)\n\n    # Inverse transform into lognormal.\n    y1 = np.exp(-np.sqrt(2) * erfcinv(2 * x))\n\n    return y0, y1\n\n\nif __name__ == &quot;__main__&quot;:\n    n_objectives = 2  # Normal and lognormal.\n\n    study = optuna.create_study(\n        sampler=optuna.samplers.RandomSampler(),\n        # Could be &quot;maximize&quot;. Does not matter for this demonstration.\n        directions=[&quot;minimize&quot;] * n_objectives,\n    )\n    study.optimize(objective, n_trials=10000)\n\n    fig, axs = plt.subplots(n_objectives)\n    for i in range(n_objectives):\n        axs[i].hist(list(t.values[i] for t in study.trials), bins=100)\n    plt.show()\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1611382397500,
        "Answer_last_edit_time":1611565726960,
        "Answer_score_count":1.0,
        "Answerer_created_time":1556182989007,
        "Answerer_location":null,
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":11.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65774253",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"equival hyperopt lognorm try us hyperparamet tune model stuck place want defin search space have lognorm normal distribut possibl hyperopt lognorm possibl defin space combin exist suggest api",
        "Question_preprocessed_content":"equival hyperopt lognorm try us hyperparamet tune model stuck place want defin search space have distribut defin space combin exist api",
        "Question_gpt_summary_original":"The user is trying to use Optuna for hyperparameter tuning of their model but is stuck on defining a search space with lognormal\/normal distribution, which is possible in hyperopt using hp.lognormal. The user is asking if it is possible to define such a space using a combination of the existing suggest_ api of Optuna.",
        "Question_gpt_summary":"user try us hyperparamet tune model stuck defin search space lognorm normal distribut possibl hyperopt lognorm user ask possibl defin space combin exist suggest api"
    },
    {
        "Question_title":"Change pointsizes, background, etc. in Object3D objects",
        "Question_body":"<p>I am logging pointclouds using numpy arrays following the shape [x,y,z,class], all integers.<br>\nLogging works, but the points are tiny and barely visible. Is it possible to make them larger?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1631616541633,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":293.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/change-pointsizes-background-etc-in-object3d-objects\/564",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"chang pointsiz background objectd object log pointcloud numpi arrai follow shape class integ log work point tini bare visibl possibl larger",
        "Question_preprocessed_content":"chang pointsiz background object object pointcloud numpi shape integ work point tini bare visibl larger",
        "Question_gpt_summary_original":"The user is logging pointclouds using numpy arrays with [x,y,z,class] shape, but the points are too small and barely visible. They are looking for a way to increase the size of the points.",
        "Question_gpt_summary":"user log pointcloud numpi arrai class shape point small bare visibl look wai increas size point"
    },
    {
        "Question_title":"SageMaker notebook instance in VPC failed to connect to local database",
        "Question_body":"hi there,  \n  \nI am setting up a jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. But it shows the following error messages.  \n\": com.microsoft.sqlserver.jdbc.SQLServerException: The TCP\/IP connection to the host xxx.xxx.xxx.xxx, port 21000 has failed. Error: \"connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP\/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\"  \n  \nI used the exactly the same VPC, subnet, security group as what i used in a glue job to extract the data from the local db. While the glue job works but the SageMaker notebook failed. I am sure the firewalls are opened.  \nCould anyone tell me how to solve it?  \nI also came across the following articles, but i am not sure if it is the root cause.  \nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1557130614000,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668629494374,
        "Question_score_count":0.0,
        "Question_view_count":547.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Hi,  \nThe principle here is that there much be network connectivity between the Notebook Instance and the DB Instance, and the security groups on the DB Instance should allow in-bound traffic from the Notebook Instance  \n  \nOne example of such as setup is  \n1. RDS DB Instance is VPC vpc-a and Subnet subnet-b.  \n2. SageMaker Notebook is launched in VPC vpc-a, Subnet subnet-b, with Security Group sg-c with DirectIntenetAccess \"Disabled\"  \n3. In the RDS DB Instance's Security Group rules, you can add an Inbound Rule to allow inbound traffic from the SageMaker Notebook security group \"sg-c\"  \n_-- Type - Protocol - Port Range - Source_  \n_-- MYSQL\/Aurora - TCP - 3306 - sg-c_  \n  \n  \n-----  \nSample Code:\n\n```\n! pip install mysql-connector\r\n\r\nimport mysql.connector\r\nmydb = mysql.connector.connect(\r\nhost=\"$RDS_ENDPOINT\",\r\nuser=\"$RDS_USERNAME\",\r\npasswd=\"$RDS_PASSWORD\"\r\n)\r\ncursor = mydb.cursor()\r\ncursor.execute(\"SHOW DATABASES\") \n```\n\nThanks for using Amazon SageMaker and let us know if there's anything else we can help with!  \n  \nEdited by: JaipreetS-AWS on May 6, 2019 3:04 PM",
        "Answer_comment_count":0.0,
        "Question_closed_time":1557180251000,
        "Answer_last_edit_time":1557180305000,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUUm8LxKZTzixOCI_IovK1A\/sagemaker-notebook-instance-in-vpc-failed-to-connect-to-local-database",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":0.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"notebook instanc vpc fail connect local databas set jupyt notebook vpc jdbc jar connect local databas show follow error messag com microsoft sqlserver jdbc sqlserverexcept tcp connect host xxx xxx xxx xxx port fail error connect time verifi connect properti sure instanc sql server run host accept tcp connect port sure tcp connect port block firewal exactli vpc subnet secur group glue job extract data local glue job work notebook fail sure firewal open tell solv came follow articl sure root caus http aw amazon com blog machin learn understand amazon notebook instanc network configur advanc rout option",
        "Question_preprocessed_content":"instanc vpc fail local databas jupyt vpc jdbc jar local databas show host port fail time verifi properti sure instanc sql server host port sure tcp port block exactli vpc subnet secur group glue job extract data local glue job work fail sure open solv came articl sure caus",
        "Question_gpt_summary_original":"The user is facing an issue while setting up a Jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. The error message shows that the TCP\/IP connection to the host has failed, and the user is unsure about the root cause. The user has tried using the same VPC, subnet, and security group as used in a glue job, which works, but the SageMaker notebook failed. The user is seeking help to solve the issue.",
        "Question_gpt_summary":"user face issu set jupyt notebook vpc jdbc jar connect local databas error messag show tcp connect host fail user unsur root caus user tri vpc subnet secur group glue job work notebook fail user seek help solv issu"
    },
    {
        "Question_title":"Error- Warning message: Annotation label is deduped",
        "Question_body":"I have uploaded a csv for image classification. I have checked the csv for any errors or duplicate values. But when I import the csv, i get the error that annotation is deduped and only a few images out of all the images are getting imported.\n\nWhat could be the possible cause for this?",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1676248200000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":204.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Error-Warning-message-Annotation-label-is-deduped\/td-p\/521402\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error warn messag annot label dedup upload csv imag classif check csv error duplic valu import csv error annot dedup imag imag get import possibl caus",
        "Question_preprocessed_content":"warn label dedup upload csv imag check csv duplic valu import csv dedup imag imag import caus",
        "Question_gpt_summary_original":"The user is encountering an error message stating that the annotation label is deduped while importing a CSV file for image classification. Despite checking for errors and duplicate values in the CSV, only a few images are getting imported. The cause of this issue is unclear.",
        "Question_gpt_summary":"user encount error messag state annot label dedup import csv file imag classif despit check error duplic valu csv imag get import caus issu unclear"
    },
    {
        "Question_title":"AWS SageMaker: CapacityError: Unable to provision requested ML compute capacity.",
        "Question_body":"<p>We were running two TrainingJob instances of type (1) <em>ml.p3.8xlarge<\/em> and (2) <em>ml.p3.2xlarge<\/em> . <\/p>\n\n<p>Each training job is running a custom algorithm with Tensorflow plus a Keras backend.<\/p>\n\n<p>The instance (1) is running ok, while the instance (2) after a reported time of training of 1 hour, with any logging in CloudWatch (any text tow log), exits with this error:<\/p>\n\n<pre><code>Failure reason\nCapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\n<\/code><\/pre>\n\n<p>I'm not sure what this message mean.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1544027022437,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":3875.0,
        "Poster_created_time":1305708350447,
        "Poster_location":"Bologna, Italy",
        "Poster_reputation_count":14823.0,
        "Poster_view_count":1847.0,
        "Answer_body":"<p>This message mean SageMaker tried to launch the instance but EC2 was not having enough capacity of this instance hence after waiting for some time(in this case 1 hour) SageMaker gave up and failed the training job.<\/p>\n\n<p>For more information about capacity issue from ec2, please visit: \n<a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/troubleshooting-launch.html#troubleshooting-launch-capacity\" rel=\"noreferrer\">troubleshooting-launch-capacity<\/a><\/p>\n\n<p>To solve this, you can either try running jobs with different instance type as suggested in failure reason or wait a few minutes and then submit your request again as suggested by EC2.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1544569877892,
        "Answer_last_edit_time":1544571385672,
        "Answer_score_count":5.0,
        "Answerer_created_time":1340284487940,
        "Answerer_location":null,
        "Answerer_reputation_count":76.0,
        "Answerer_view_count":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53636589",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"capacityerror unabl provis request comput capac run trainingjob instanc type xlarg xlarg train job run custom algorithm tensorflow plu kera backend instanc run instanc report time train hour log cloudwatch text tow log exit error failur reason capacityerror unabl provis request comput capac retri differ instanc type sure messag mean",
        "Question_preprocessed_content":"unabl provis request comput capac trainingjob instanc type train job custom algorithm tensorflow plu kera backend instanc instanc report time train hour cloudwatch exit sure mean",
        "Question_gpt_summary_original":"The user encountered a CapacityError while running two TrainingJob instances on AWS SageMaker, with one instance running fine and the other exiting with the error message \"CapacityError: Unable to provision requested ML compute capacity. Please retry using a different ML instance type.\" The user is unsure about the meaning of this error message.",
        "Question_gpt_summary":"user encount capacityerror run trainingjob instanc instanc run fine exit error messag capacityerror unabl provis request comput capac retri differ instanc type user unsur mean error messag"
    },
    {
        "Question_title":"Vertex Workbench Managed Notebook vs Quotas",
        "Question_body":"Hi to all\n\nA little help need it... Im trying to create a Managed Notebook with the next configuration\u00a0for what I request a quota increase as you can see in the following images\n\na2-highgpu-1g (Accelerator Optimized: 1 NVIDIA Tesla A100 GPU, 12 vCPUs, 85GB Ram)\u00a0\n\nThe problem is that although I got the quota increase I have not been able to create the notebook in any us regions as:\u00a0 US Central (Iowa1), Us West (Oregon), always get the same error:\u00a0\n\nCould not create instance: Quota limit 'GPUsA100PerProjectPerRegion' has been exceeded. Limit: 0 in region us-central1.\n\nWhat Im doing wrong? How much quota do I need to get so I can use this type of machine?",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1659516120000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":210.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-Workbench-Managed-Notebook-vs-Quotas\/td-p\/450055\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"vertex workbench manag notebook quota littl help need try creat manag notebook configurationfor request quota increas follow imag highgpu acceler optim nvidia tesla gpu vcpu ram problem got quota increas abl creat notebook region central iowa west oregon error creat instanc quota limit gpusaperprojectperregion exceed limit region central wrong quota need us type machin",
        "Question_preprocessed_content":"vertex workbench manag quota help try creat manag configurationfor request quota increas imag problem got quota increas abl creat region central west creat instanc quota limit gpusa limit region wrong quota us type machin",
        "Question_gpt_summary_original":"The user is trying to create a Managed Notebook with a specific configuration that requires a quota increase. Although the quota increase was granted, the user is unable to create the notebook in any US regions due to a \"Quota limit\" error. The error message indicates that the user has exceeded the limit for \"GPUsA100PerProjectPerRegion\" in the specified region. The user is seeking help to understand how much quota is needed to use this type of machine and what they might be doing wrong.",
        "Question_gpt_summary":"user try creat manag notebook specif configur requir quota increas quota increas grant user unabl creat notebook region quota limit error error messag indic user exceed limit gpusaperprojectperregion specifi region user seek help understand quota need us type machin wrong"
    },
    {
        "Question_title":"Sagemaker endpoint servving doesnt work for multiple inputs (mulit input-output LSTM)",
        "Question_body":"<p>I have a LSTM network that has 3 inputs and 3 outputs(built with <a href=\"https:\/\/keras.io\/guides\/functional_api\/\" rel=\"nofollow noreferrer\">functional api<\/a> in Tf.keras) , that I am trying to deploy as sagemaker endpoint. I have input shape of (None,10,1) for each input\/feature, which means 10 timesteps.(I later concatenate the embeddings, but its irrelevant here)<\/p>\n<p>Everything works fine on training time on sagemaker training jobs as well and training completes and artifacts are made successfully. But at time of invocation, endpoint is not working to predict <code>1 example, having 10 timesteps with 3 inputs<\/code> , I have tried multiple things but cant provide three inputs for prediction(input_1,input_2,input_1).<\/p>\n<p>As I said that each input has 10 timesteps, so have shape (10,1). Endpoint only returns the output if I format my payload as below, but by doing so, it treats each time-step as separate example\/instance and return 10 predictions for each output<\/p>\n<pre><code>{'inputs':{\n           'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],\n           'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\n           'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}\n          } # gives len(pred['output_1&quot;])) == 10\n<\/code><\/pre>\n<p>This is expected as it consider this request as 10 examples, but in my case it is one example with 10-timesteps for each feature (1,10,1). So I tried different things from the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/deploying_tensorflow_serving.html\" rel=\"nofollow noreferrer\">documentation<\/a>. Like using instances.<\/p>\n<pre><code>{'instances': [\n                {\n                      'input_1': [[0],[0], [0],[0],[2],[12],[11], [7], [7], [2]],\n                      'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\n                      'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]\n                }\n              ]\n}\n<\/code><\/pre>\n<p>But it gives this error.<\/p>\n<blockquote>\n<p>transpose expects a vector of size 4. But input(1) is a vector of\nsize 3\\n\\t [[{{node transpose_1}}]]\\n\\t\n[[functional_1\/lstm\/PartitionedCall]]\\n\\t\n[[StatefulPartitionedCall\/StatefulPartitionedCall]]&quot;\\n}&quot;}&quot;<\/p>\n<\/blockquote>\n<p>Document also gives example and says<\/p>\n<blockquote>\n<p>for models with multiple named inputs, just include all the keys in the input dict<\/p>\n<\/blockquote>\n<p>but when I use this, I get error saying <code>Missing 'inputs' or 'instances' key\\&quot;\\n}&quot;<\/code><\/p>\n<pre><code>{'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],\n 'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],\n 'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}\n<\/code><\/pre>\n<p>My invocation code is below.<\/p>\n<pre><code>import boto3\nimport json\n\nsm = boto3.client('sagemaker-runtime')    \nendpoint_name = &quot;tensorflow--------------------4&quot;\nresponse = sm.invoke_endpoint(EndpointName=endpoint_name, \n                              Body=json.dumps(payload),\n                              ContentType='application\/json')\n<\/code><\/pre>\n<p>I am not sure how to solve this issue, looking forward for help<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1610719108907,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1610720123127,
        "Question_score_count":1.0,
        "Question_view_count":163.0,
        "Poster_created_time":1401212205023,
        "Poster_location":"London, Uk",
        "Poster_reputation_count":19335.0,
        "Poster_view_count":1053.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65737401",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"endpoint serv doesnt work multipl input mulit input output lstm lstm network input output built function api kera try deploi endpoint input shape input featur mean timestep later concaten embed irrelev work fine train time train job train complet artifact successfulli time invoc endpoint work predict exampl have timestep input tri multipl thing provid input predict input input input said input timestep shape endpoint return output format payload treat time step separ exampl instanc return predict output input input input input give len pred output expect consid request exampl case exampl timestep featur tri differ thing document like instanc instanc input input input give error transpos expect vector size input vector size node transpos function lstm partitionedcal statefulpartitionedcal statefulpartitionedcal document give exampl sai model multipl name input includ kei input dict us error sai miss input instanc kei input input input invoc code import boto import json boto client runtim endpoint tensorflow respons invok endpoint endpointnam endpoint bodi json dump payload contenttyp applic json sure solv issu look forward help",
        "Question_preprocessed_content":"endpoint doesnt work multipl input lstm network input output try deploi endpoint input shape mean later concaten work fine train time train job train complet artifact time invoc endpoint work predict tri multipl thing provid input said input timestep shape endpoint return output format payload treat separ return predict output expect consid request exampl case exampl featur tri thing document like instanc give transpos expect vector size input vector size document give exampl sai model multipl name input includ kei input dict us sai invoc code sure solv forward help",
        "Question_gpt_summary_original":"The user has built a LSTM network with 3 inputs and 3 outputs using functional API in Tf.keras. The input shape is (None,10,1) for each input\/feature. The training job completes successfully on Sagemaker, but at the time of invocation, the endpoint is not working to predict 1 example having 10 timesteps with 3 inputs. The endpoint only returns the output if the payload is formatted in a way that treats each time-step as a separate example\/instance. The user has tried different things from the documentation, but it gives an error. The user is looking for help to solve this issue.",
        "Question_gpt_summary":"user built lstm network input output function api kera input shape input featur train job complet successfulli time invoc endpoint work predict exampl have timestep input endpoint return output payload format wai treat time step separ exampl instanc user tri differ thing document give error user look help solv issu"
    },
    {
        "Question_title":"Remote stop not working",
        "Question_body":"<p>After running <code>guild runs stop X -r server<\/code>, the processes are still running on the remote and GPU memory has not been released even though guild reports the run as terminated.<\/p>\n<p>I think it may be pytorch\u2019s data loader worker processes that are still running but I\u2019m not sure. I think I saw something about this subject before but I couldn\u2019t find it here or on github. Has anyone else experienced this issue?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1617118654840,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":333.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/remote-stop-not-working\/584",
        "Tool":"Guild AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"remot stop work run run stop server process run remot gpu memori releas report run termin think pytorch data loader worker process run sure think saw subject github experienc issu",
        "Question_preprocessed_content":"remot stop work remot gpu memori releas report run termin think pytorch data loader worker sure think saw subject github experienc",
        "Question_gpt_summary_original":"The user is facing an issue where the remote stop command is not working and the processes are still running on the remote, causing GPU memory to not be released. The user suspects that it may be related to pytorch's data loader worker processes and is seeking help from others who may have experienced the same issue.",
        "Question_gpt_summary":"user face issu remot stop command work process run remot caus gpu memori releas user suspect relat pytorch data loader worker process seek help experienc issu"
    },
    {
        "Question_title":"Access .log files acc\\loss after training model (using .fit_generator) with multiple pauses",
        "Question_body":"<p>I'm using Amazon's SageMaker Studio Lab to train a model using a certain dataset.<br>\nThe code is as follow (which saves the History object in history variable):<\/p>\n<pre><code>model = tf.keras.models.load_model('best_model.hdf5')  # Every run after runtime end, use the last saved model\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n<\/code><\/pre>\n<p>I had to make several pauses due to ending runtime, and with each pause I saved the .log file. Now after appending those .log files, I'm trying to access them using the standard accuracy and loss plotting methods:<\/p>\n<pre><code>def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n<\/code><\/pre>\n<p>But the issue is I can't seem to manage to receive a working History object file.<br>\nAmong the things I tried:<br>\nI read about this possible method, of re-loading the model and trying to get it's history, but it didn't work, &quot;'NoneType' object has no attribute 'history'&quot;<\/p>\n<pre><code>model = tf.keras.models.load_model('best_model.hdf5')\nhistory = model.history\n<\/code><\/pre>\n<p>Another try was using pandas package and loading the file, which generated an error &quot;'DataFrame' object has no attribute 'history'&quot;:<\/p>\n<pre><code>history = pd.read_csv('history.log', sep=',', engine='python')\n<\/code><\/pre>\n<p>And this try generated a CSVLogger object, &quot;'CSVLogger' object has no attribute 'history'&quot;:<\/p>\n<pre><code>history = CSVLogger('history.log')\n<\/code><\/pre>\n<p>Appreciate any help on how to recover the History object, so I can plot those results (if it's even possible?)...<br>\nThanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1662213868913,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":54.0,
        "Poster_created_time":1540226093150,
        "Poster_location":null,
        "Poster_reputation_count":119.0,
        "Poster_view_count":32.0,
        "Answer_body":"<p>Instead of recreating the History object, what I did was read the .log file using pandas package, <code>read_csv<\/code> method, and create a DataFrame data structure with the wanted columns and plot it. Code below:<\/p>\n<pre><code>history = pd.read_csv('history.log')\nhistory_acc = pd.DataFrame(history, columns=[&quot;accuracy&quot;, &quot;val_accuracy&quot;])\nhistory_loss = pd.DataFrame(history, columns=[&quot;loss&quot;, &quot;val_loss&quot;])\nplot_accuracy(history_acc,'plot title...')\nplot_loss(history_loss,'plot title...')\n<\/code><\/pre>\n<br>\n<pre><code>def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n<\/code><\/pre>\n<p>Hope this helps someone having the same issue as I did in the future.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1662234715187,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":1540226093150,
        "Answerer_location":null,
        "Answerer_reputation_count":119.0,
        "Answerer_view_count":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73592834",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"access log file acc loss train model fit gener multipl paus amazon studio lab train model certain dataset code follow save histori object histori variabl model kera model load model best model hdf run runtim end us save model model compil optim sgd momentum loss categor crossentropi metric accuraci checkpoint modelcheckpoint filepath best model hdf verbos save best true csv logger csvlogger histori log histori model fit gener train gener step epoch train sampl batch size valid data valid gener valid step valid sampl batch size epoch verbos callback csv logger checkpoint paus end runtim paus save log file append log file try access standard accuraci loss plot method def plot accuraci histori titl plt titl titl plt plot histori histori accuraci plt plot histori histori val accuraci plt ylabel accuraci plt xlabel epoch plt legend train accuraci valid accuraci loc best plt def plot loss histori titl plt titl titl plt plot histori histori loss plt plot histori histori val loss plt ylabel loss plt xlabel epoch plt legend train loss valid loss loc best plt issu manag receiv work histori object file thing tri read possibl method load model try histori work nonetyp object attribut histori model kera model load model best model hdf histori model histori try panda packag load file gener error datafram object attribut histori histori read csv histori log sep engin python try gener csvlogger object csvlogger object attribut histori histori csvlogger histori log appreci help recov histori object plot result possibl thank",
        "Question_preprocessed_content":"log file train model multipl paus amazon studio lab train model certain dataset code paus end runtim paus save log file log file try standard method manag receiv work histori object file thing tri read method model try histori work nonetyp object histori try panda packag load file gener datafram object histori try gener object object histori help recov histori object plot result thank",
        "Question_gpt_summary_original":"The user is facing an issue while trying to access the .log files after training a model using .fit_generator with multiple pauses. The user has tried different methods to access the History object file, including re-loading the model and trying to get its history, using pandas package to load the file, and generating a CSVLogger object, but none of them worked. The user is seeking help to recover the History object to plot the results.",
        "Question_gpt_summary":"user face issu try access log file train model fit gener multipl paus user tri differ method access histori object file includ load model try histori panda packag load file gener csvlogger object work user seek help recov histori object plot result"
    },
    {
        "Question_title":"Feature request: adding zip files to reports",
        "Question_body":"<p>I often want to upload zip files with a bunch of info from experiments to wandb but currently the web UI does not respond to this. May we have this as a feature?<\/p>\n<hr>\n<p>originally asked as a help question but realized it was likely a feature request or tech support question. Original question: <a href=\"https:\/\/community.wandb.ai\/t\/how-do-i-upload-artifacts-e-g-zip-files-manually-in-the-website-gui\/840\/2\" class=\"inline-onebox\">How do I upload artifacts (e.g. zip files) manually in the website GUI? - #2 by brando<\/a><\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_created_time":1633555566071,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":319.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/feature-request-adding-zip-files-to-reports\/877",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"featur request ad zip file report want upload zip file bunch info experi current web respond featur origin ask help question realiz like featur request tech support question origin question upload artifact zip file manual websit gui brando",
        "Question_preprocessed_content":"featur request zip file report want upload zip file bunch info experi web respond featur ask help question realiz like featur request tech question origin question upload artifact websit gui brando",
        "Question_gpt_summary_original":"The user is requesting a feature to be added to the web UI of wandb that allows for the uploading of zip files containing experiment information. Currently, the web UI does not support this feature.",
        "Question_gpt_summary":"user request featur ad web allow upload zip file contain experi inform current web support featur"
    },
    {
        "Question_title":"Access file in a storage account residing in different tenant to ML Service in another tenant via IP based SAS restricted access",
        "Question_body":"<p>Hi,  <\/p>\n<p>I have a storage account residing in tenant-A and machine learning service in tenant-B. When I try to read file from storage account in tenant-A via SAS (with IP restriction) in the jupyter notebook running on compute in ML service in tenant-B, it is not accessible and failing with 403 (Forbidden).  <\/p>\n<p>But when I try to access the file without IP restriction, I am able to read it in the notebook.  <br \/>\nCan you please help in understanding why it is happening and possible fix for the problem?  <\/p>\n<p>Please note, the public IP of ML compute is being used for whitelisting in SAS.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1645593594070,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/747081\/access-file-in-a-storage-account-residing-in-diffe",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"access file storag account resid differ tenant servic tenant base sa restrict access storag account resid tenant machin learn servic tenant try read file storag account tenant sa restrict jupyt notebook run comput servic tenant access fail forbidden try access file restrict abl read notebook help understand happen possibl fix problem note public comput whitelist sa",
        "Question_preprocessed_content":"file storag resid tenant servic tenant base sa restrict storag resid machin learn servic try read file storag sa jupyt comput servic fail try file restrict abl read help understand fix problem note public comput whitelist sa",
        "Question_gpt_summary_original":"The user is facing challenges accessing a file in a storage account located in a different tenant from their machine learning service. They are using an IP-based SAS with restrictions, but the file is not accessible and returns a 403 error. However, they are able to access the file without IP restrictions. The user is seeking help to understand why this is happening and how to fix the issue.",
        "Question_gpt_summary":"user face challeng access file storag account locat differ tenant machin learn servic base sa restrict file access return error abl access file restrict user seek help understand happen fix issu"
    },
    {
        "Question_title":"About the end of Machine Learning Studio (classic)",
        "Question_body":"<p>hello.  <\/p>\n<p>I am currently using Machine Learning Studio (classic).  <\/p>\n<p>'From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) experiments and web services. Beginning 1 December 2021, new creation of Machine Learning Studio (classic) resources will not be available.'  <\/p>\n<p>As mentioned above, what do resources mean?  <br \/>\nIs it possible to continue creating experiments and web sales, and using APIs from outside until 2024?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1635690920907,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hello <a href=\"\/users\/na\/?userid=6fde402b-b49f-41d7-8640-5316dd9a5cd2\">@gatsby53  <\/a> ,    <\/p>\n<p>Thanks for reaching out to us here. There are several important dates.    <\/p>\n<p>Beginning 1 December 2021, you will <strong>not be able to create new<\/strong> Machine Learning Studio (classic) resources. You can still work on your <strong>existing resource<\/strong> from 1 December 2021 to 31 August 2024.    <\/p>\n<p>Support for Machine Learning Studio (classic) will end on 31 August 2024. We recommend you transition to Azure Machine Learning by that date.    <\/p>\n<p>Please refer to this guidance for how to migrate your project for better experience.    <br \/>\n<a href=\"https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\">https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview<\/a>    <\/p>\n<p>Hope this will help. Please let us know if any further queries.    <\/p>\n<p>------------------------------    <\/p>\n<ul>\n<li> Please don't forget to click on <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130616-image.png?platform=QnA\" alt=\"130616-image.png\" \/> or upvote <img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/130671-image.png?platform=QnA\" alt=\"130671-image.png\" \/> button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/25904\/accepted-answers.html\">how<\/a>    <\/li>\n<li> Want a reminder to come back and check responses? Here is how to subscribe to a <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/articles\/67444\/email-notifications.html\">notification<\/a>    <\/li>\n<li> If you are interested in joining the VM program and help shape the future of Q&amp;A: Here is how you can be part of <a href=\"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/543261\/index.html\">Q&amp;A Volunteer Moderators<\/a>    <\/li>\n<\/ul>\n",
        "Answer_comment_count":1.0,
        "Question_closed_time":1635733766197,
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/610432\/about-the-end-of-machine-learning-studio-(classic)",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"end machin learn studio classic hello current machin learn studio classic august continu us exist machin learn studio classic experi web servic begin decemb new creation machin learn studio classic resourc avail mention resourc mean possibl continu creat experi web sale api outsid",
        "Question_preprocessed_content":"end machin learn studio machin learn studio august continu us exist machin learn studio experi web servic decemb new creation machin learn studio resourc mention resourc mean continu creat experi web sale api outsid",
        "Question_gpt_summary_original":"The user is currently using Machine Learning Studio (classic) and has learned that new creation of resources will not be available starting December 1, 2021. The user is seeking clarification on what \"resources\" means and if they can continue creating experiments, web services, and using APIs until 2024.",
        "Question_gpt_summary":"user current machin learn studio classic learn new creation resourc avail start decemb user seek clarif resourc mean continu creat experi web servic api"
    },
    {
        "Question_title":"How to deploy mlflow model with data preprocessing(text data)",
        "Question_body":"<p>I have developed keras text classification model. I have preprocessed data(tokenization). I have logged trained model successfully(mlflow.keras.log_model). I have served model using mlflow serve. Now while doing prediction on text data I need to do preprocessing using same tokenizer object used for training.\nHow to preprocess test data and get predictions from served model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1584090517703,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":1996.0,
        "Poster_created_time":1498470936987,
        "Poster_location":null,
        "Poster_reputation_count":33.0,
        "Poster_view_count":6.0,
        "Answer_body":"<p>You can log a custom python model: \n<a href=\"https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/models.html#custom-python-models<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1584552358667,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":1579718832727,
        "Answerer_location":null,
        "Answerer_reputation_count":149.0,
        "Answerer_view_count":10.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60667610",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi model data preprocess text data develop kera text classif model preprocess data token log train model successfulli kera log model serv model serv predict text data need preprocess token object train preprocess test data predict serv model",
        "Question_preprocessed_content":"deploi model data develop kera text model data train model serv model serv predict text data token object train test data predict serv model",
        "Question_gpt_summary_original":"The user has developed a text classification model using Keras and has preprocessed the data using tokenization. They have successfully logged the trained model using mlflow.keras.log_model and served the model using mlflow serve. However, they are facing challenges in preprocessing the test data using the same tokenizer object used for training and getting predictions from the served model.",
        "Question_gpt_summary":"user develop text classif model kera preprocess data token successfulli log train model kera log model serv model serv face challeng preprocess test data token object train get predict serv model"
    },
    {
        "Question_title":"ERROR: failed to pull data from the cloud - CI-CD pipeline",
        "Question_body":"<p>Hi,<br>\nI am getting this error \u201cERROR: failed to pull data from the cloud - Checkout failed for following targets:\u201d  with some files. The details are below<\/p>\n<ul>\n<li>Creating CI-CD pipeline for ML Model<br>\ncommands in git workflow yaml  file are below<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\">  - name: Dvc pull updated files\n      run: | \n         dvc pull -v\n    - name: Run pipeline\n      run: |\n         dvc repro -f \n<\/code><\/pre>\n<pre><code class=\"lang-auto\">2023-02-21 17:58:37,096 ERROR: failed to pull data from the cloud - Checkout failed for following targets:\n\/home\/runner\/work\/MLOTest\/MLOTest\/data\/processed\/churn_train.csv\n\/home\/runner\/work\/MLOTest\/MLOTest\/data\/processed\/churn_test.csv\n\/home\/runner\/work\/MLOTest\/MLOTest\/models\/model.joblib\n\/home\/runner\/work\/MLOTest\/MLOTest\/data\/external\/train.csv\n\/home\/runner\/work\/MLOTest\/MLOTest\/data\/raw\/train.csv\n<\/code><\/pre>\n<p>dvc version: 2.45.1<\/p>\n<p>Please give your inputs to resolve the issue<\/p>\n<p>Thanks,<br>\nVeerendra<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1677004566284,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":95.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-failed-to-pull-data-from-the-cloud-ci-cd-pipeline\/1535",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"error fail pull data cloud pipelin get error error fail pull data cloud checkout fail follow target file detail creat pipelin model command git workflow yaml file pull updat file run pull run pipelin run repro error fail pull data cloud checkout fail follow target home runner work mlotest mlotest data process churn train csv home runner work mlotest mlotest data process churn test csv home runner work mlotest mlotest model model joblib home runner work mlotest mlotest data extern train csv home runner work mlotest mlotest data raw train csv version input resolv issu thank veerendra",
        "Question_preprocessed_content":"fail data cloud pipelin fail data cloud checkout fail target file detail creat pipelin model git workflow yaml file version input resolv thank",
        "Question_gpt_summary_original":"The user is encountering an error while creating a CI-CD pipeline for an ML model. The error message states that there was a failure to pull data from the cloud, specifically for certain targets such as processed and raw data files and the model file. The user has provided the relevant commands and the version of dvc being used. The user is seeking input to resolve the issue.",
        "Question_gpt_summary":"user encount error creat pipelin model error messag state failur pull data cloud specif certain target process raw data file model file user provid relev command version user seek input resolv issu"
    },
    {
        "Question_title":"Clean Up Azure Machine Learning Blob Storage",
        "Question_body":"<p>I manage a frequently used Azure Machine Learning workspace. With several Experiments and active pipelines. Everything is working good so far. My problem is to get rid of old data from runs, experiments and pipelines. Over the last year the blob storage grew to enourmus size, because every pipeline data is stored.<\/p>\n<p>I have deleted older runs from experimnents by using the gui, but the actual pipeline data on the blob store is not deleted. Is there a smart way to clean up data on the blob store from runs which have been deleted ?<\/p>\n<p>On one of the countless Microsoft support pages, I found the following not very helpfull post:<\/p>\n<p>*Azure does not automatically delete intermediate data written with OutputFileDatasetConfig. To avoid storage charges for large amounts of unneeded data, you should either:<\/p>\n<ol>\n<li>Programmatically delete intermediate data at the end of a pipeline\nrun, when it is no longer needed<\/li>\n<li>Use blob storage with a short-term storage policy for intermediate data (see Optimize costs by automating Azure Blob Storage access tiers)<\/li>\n<li>Regularly review and delete no-longer-needed data*<\/li>\n<\/ol>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-move-data-in-out-of-pipelines#delete-outputfiledatasetconfig-contents-when-no-longer-needed<\/a><\/p>\n<p>Any idea is welcome.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1642771426213,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":4.0,
        "Question_view_count":368.0,
        "Poster_created_time":1635428968927,
        "Poster_location":"Germany",
        "Poster_reputation_count":61.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70802006",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"clean blob storag manag frequent workspac experi activ pipelin work good far problem rid old data run experi pipelin year blob storag grew enourmu size pipelin data store delet older run experimn gui actual pipelin data blob store delet smart wai clean data blob store run delet countless microsoft support page follow helpful post azur automat delet intermedi data written outputfiledatasetconfig avoid storag charg larg amount unneed data programmat delet intermedi data end pipelin run longer need us blob storag short term storag polici intermedi data optim cost autom azur blob storag access tier regularli review delet longer need data http doc microsoft com azur machin learn data pipelin delet outputfiledatasetconfig content longer need idea welcom",
        "Question_preprocessed_content":"clean blob storag manag frequent workspac experi activ pipelin work far problem rid old data run experi pipelin year blob storag grew enourmu size pipelin data store delet older run experimn gui actual pipelin data blob store delet smart wai clean data blob store run delet microsoft page post azur delet intermedi data outputfiledatasetconfig avoid storag charg larg amount data delet intermedi data end pipelin run longer us blob storag storag polici intermedi data regularli review delet data idea welcom",
        "Question_gpt_summary_original":"The user manages an Azure Machine Learning workspace with several experiments and active pipelines. The blob storage has grown to an enormous size due to the storage of pipeline data. The user has deleted older runs from experiments using the GUI, but the actual pipeline data on the blob store is not deleted. The user is looking for a smart way to clean up data on the blob store from deleted runs. The Microsoft support page suggests programmatically deleting intermediate data, using blob storage with a short-term storage policy, or regularly reviewing and deleting no-longer-needed data. The user is seeking any ideas to resolve the issue.",
        "Question_gpt_summary":"user manag workspac experi activ pipelin blob storag grown enorm size storag pipelin data user delet older run experi gui actual pipelin data blob store delet user look smart wai clean data blob store delet run microsoft support page suggest programmat delet intermedi data blob storag short term storag polici regularli review delet longer need data user seek idea resolv issu"
    },
    {
        "Question_title":"Best parameters of an Optuna multi-objective optimization",
        "Question_body":"<p>When performing a single-objective optimization with <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Optuna<\/a>, the best parameters of the study are accessible using:<\/p>\n<pre><code>import optuna\ndef objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    return (x - 2) ** 2\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nstudy.best_params  # E.g. {'x': 2.002108042}\n<\/code><\/pre>\n<p>If I want to perform a multi-objective optimization, this would be become for example :<\/p>\n<pre><code>import optuna\ndef multi_objective(trial):\n    x = trial.suggest_uniform('x', -10, 10)\n    f1 = (x - 2) ** 2\n    f2 = -f1\n    return f1, f2\n\nstudy = optuna.create_study(directions=['minimize', 'maximize'])\nstudy.optimize(multi_objective, n_trials=100)\n<\/code><\/pre>\n<p>This works, but the command <code>study.best_params<\/code> fails with <code>RuntimeError: The best trial of a 'study' is only supported for single-objective optimization.<\/code><\/p>\n<p>How can I get the best parameters for a multi-objective optimization ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1611255257910,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":2571.0,
        "Poster_created_time":1588846413276,
        "Poster_location":null,
        "Poster_reputation_count":108.0,
        "Poster_view_count":25.0,
        "Answer_body":"<p>In multi-objective optimization, you often end up with more than one best trial, but rather a set of trials. This set if often referred to as the Pareto front. You can get this Pareto front, or the list of trials, via <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.study.Study.html#optuna.study.Study.best_trials\" rel=\"noreferrer\"><code>study.best_trials<\/code><\/a>, then look at the parameters from each individual trial i.e. <code>study.best_trials[some_index].params<\/code>.<\/p>\n<p>For instance, given your directions of minimizing <code>f1<\/code> and maximizing <code>f2<\/code>, you might end up with a trial that has a small value for <code>f1<\/code> (good) but at the same time small value for <code>f2<\/code> (bad) while another trial might have a large value for both <code>f1<\/code> (bad) and <code>f2<\/code> (good). Both of these trials could be returned from <code>study.best_trials<\/code>.<\/p>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1611273369707,
        "Answer_last_edit_time":null,
        "Answer_score_count":6.0,
        "Answerer_created_time":1556182989007,
        "Answerer_location":null,
        "Answerer_reputation_count":181.0,
        "Answerer_view_count":11.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65833998",
        "Tool":"Optuna",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"best paramet multi object optim perform singl object optim best paramet studi access import def object trial trial suggest uniform return studi creat studi direct minim studi optim object trial studi best param want perform multi object optim exampl import def multi object trial trial suggest uniform return studi creat studi direct minim maxim studi optim multi object trial work command studi best param fail runtimeerror best trial studi support singl object optim best paramet multi object optim",
        "Question_preprocessed_content":"best paramet optim perform optim best paramet studi want perform optim exampl work fail best paramet optim",
        "Question_gpt_summary_original":"The user is facing a challenge in obtaining the best parameters for a multi-objective optimization using Optuna. While the best parameters can be easily accessed for a single-objective optimization, the command fails for multi-objective optimization, resulting in a runtime error. The user is seeking a solution to obtain the best parameters for multi-objective optimization.",
        "Question_gpt_summary":"user face challeng obtain best paramet multi object optim best paramet easili access singl object optim command fail multi object optim result runtim error user seek solut obtain best paramet multi object optim"
    },
    {
        "Question_title":"Trouble modifying and saving dvc data file which lives outside the repo",
        "Question_body":"<p>I have a dvc setup where the git\/dvc repo, the data-file directory and the dvc-cache directory are all peers.<br>\ne.g.<br>\nmy-test-repo<br>\ntest-files<br>\nmy-test-dvc-cache<br>\nare all directories at the same level<\/p>\n<p>All the files under test-files were added using dvc add test-files<\/p>\n<p>my-test-repo\/test-files.dvc contains this line:<br>\npath: \u2026\/test-files<\/p>\n<p>.\/dvc\/config contains this line<br>\n[cache]<br>\ndir = \u2026\/\u2026\/my-test-dvc-cache<\/p>\n<p>I did a dvc push and all the file went to the specified remote.<br>\nI did a git clone and a dvc pull (on several different machines) and all the files came down in the directory structure specified above.<br>\nCode which runs from the repo and uses the files works.<\/p>\n<p>However, when I modify a data  file, I am having trouble saving it.<br>\nI changed a file in the test-files directory and dvc status shows that test-files is modified.<br>\n$ dvc status<br>\nunit_test_input.dvc:<br>\nchanged outs:<br>\nmodified:           \u2026\\test-files<br>\nchanged checksum<\/p>\n<p>When I do dvc commit it gives this error message:<br>\n$ dvc commit<br>\nERROR: failed to commit - unable to commit changed stage: \u2018test-files.dvc\u2019. Use <code>-f|--force<\/code> to force.<\/p>\n<p>So I entered dvc commit -f and it complains about files outside of the repo.<br>\nI read that  it is ok to have files outside the repo and the original push and pull operations worked fine.<\/p>\n<p>ERROR: unexpected error - Cmd(\u2018git\u2019) failed due to: exit code(128)<br>\ncmdline: git ls-files C:\\test-files<br>\nstderr: \u2018fatal: C:\\test-files: \u2018C:\\test-files\u2019 is outside repository at \u2018C:\\test-files\/my-test-repo\u2019\u2019<\/p>\n<p>It seems like dvc doesn\u2019t care that the data files are outside the repo, but the commit command is try to perform git commands on those files and git doesn\u2019t like them being outside the repo.<\/p>\n<p>Is that what\u2019s going on?<br>\nIs there something I can do about it?<\/p>",
        "Question_answer_count":22,
        "Question_comment_count":null,
        "Question_created_time":1594430779787,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":2347.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/trouble-modifying-and-saving-dvc-data-file-which-lives-outside-the-repo\/435",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"troubl modifi save data file live outsid repo setup git repo data file directori cach directori peer test repo test file test cach directori level file test file ad add test file test repo test file contain line path test file config contain line cach dir test cach push file went specifi remot git clone pull differ machin file came directori structur specifi code run repo us file work modifi data file have troubl save chang file test file directori statu show test file modifi statu unit test input chang out modifi test file chang checksum commit give error messag commit error fail commit unabl commit chang stage test file us forc forc enter commit complain file outsid repo read file outsid repo origin push pull oper work fine error unexpect error cmd git fail exit code cmdline git file test file stderr fatal test file test file outsid repositori test file test repo like doesnt care data file outsid repo commit command try perform git command file git doesnt like outsid repo what go",
        "Question_preprocessed_content":"troubl modifi save data file live outsid repo setup git repo directori cach directori directori level file contain line path contain line cach dir push file went specifi remot git clone file came directori structur specifi code run repo us file work modifi data file have troubl save chang file directori statu show modifi statu chang out modifi chang checksum give fail unabl chang stage us forc enter complain file outsid repo read file outsid repo origin push oper work fine unexpect cmd fail exit code cmdline git fatal outsid repositori like doesnt care data file outsid repo try perform git file git doesnt like outsid repo what go",
        "Question_gpt_summary_original":"The user is having trouble modifying and saving a dvc data file that is located outside the repo. The files were added using dvc add and were pushed to a remote successfully. However, when the user modifies a data file and tries to commit it, they receive an error message stating that the file is outside the repo. Although dvc doesn't seem to care about the files being outside the repo, the commit command is trying to perform git commands on those files, which git doesn't allow. The user is seeking a solution to this issue.",
        "Question_gpt_summary":"user have troubl modifi save data file locat outsid repo file ad add push remot successfulli user modifi data file tri commit receiv error messag state file outsid repo care file outsid repo commit command try perform git command file git allow user seek solut issu"
    },
    {
        "Question_title":"Without ECR being enabled in AWS account at Organization Level, what's the impact to SageMaker Studio?",
        "Question_body":"I have a customer already standardized Artifactory as the centralized image registration. They disabled ECR service at Org level. Now we want to understand the potential impact on customer's day-2-day use of SageMaker Studio as a platform to support their full ML lifecycle. \n\n(If customer only use built-in SageMaker algorithm or framework and use prebuilt SageMaker container images)\n\nEspecially when customer trying to deploy the model to endpoint, does that need ECR service to be enabled in customer account?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1649820151027,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1668444527276,
        "Question_score_count":0.0,
        "Question_view_count":183.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Hi, yes, if you're restricting the user to only built-in algorithms and frameworks, and prebuilt images for Studio, you should be able to use it seamlessly (to deploy endpoints as well). That said, it severely restricts the data scientist from using custom images that could be built to their needs and packages, or bringing their own container for machine learning.",
        "Answer_comment_count":0.0,
        "Question_closed_time":1650300385539,
        "Answer_last_edit_time":1650300385539,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUXZB0Oki3QamlQ5ijtVSuzQ\/without-ecr-being-enabled-in-aws-account-at-organization-level-what-s-the-impact-to-sagemaker-studio",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":1.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":1.0,
        "Answerer_isModerator":0.0,
        "Answerer_isExpert":0.0,
        "Answerer_isCse":0.0,
        "Question_original_content":"ecr enabl aw account organ level impact studio custom standard artifactori central imag registr disabl ecr servic org level want understand potenti impact custom dai dai us studio platform support lifecycl custom us built algorithm framework us prebuilt contain imag especi custom try deploi model endpoint need ecr servic enabl custom account",
        "Question_preprocessed_content":"ecr enabl aw organ level impact studio custom standard artifactori central imag registr disabl ecr servic org level want understand potenti impact custom us studio platform lifecycl custom us algorithm framework us prebuilt contain imag custom try deploi model endpoint ecr servic enabl custom",
        "Question_gpt_summary_original":"The user is inquiring about the potential impact on their use of SageMaker Studio if ECR service is disabled at the organization level. They are specifically concerned about deploying models to endpoints and whether ECR service needs to be enabled for this. The user mentions that they use built-in SageMaker algorithms and prebuilt container images.",
        "Question_gpt_summary":"user inquir potenti impact us studio ecr servic disabl organ level specif concern deploi model endpoint ecr servic need enabl user mention us built algorithm prebuilt contain imag"
    },
    {
        "Question_title":"DVC with external data is very slow",
        "Question_body":"<p>Hi there,<\/p>\n<p>I think my question has two parts: firstly, is the use of external data (as described <a href=\"https:\/\/dvc.org\/doc\/user-guide\/managing-external-data\" rel=\"noopener nofollow ugc\">here<\/a>) the best way in my case and, if so, why is it so slow to add files?<\/p>\n<p>The context: my ML code + git repo live on my local machine. I develop this codebase using pycharm,  and use pycharm\u2019s remote ssh deployment to run my code on a remote machine where the data lives. The dataset is a large imaging dataset, &gt;200GB with 1000s of files, and will not fit on my local machine. When I run my code, pycharm copies it to the remote machine to run it, but it does not copy the git files so there is no git repo on my remote machine.<\/p>\n<p>As my git repo and data live on different machines, it seemed like adding the external dataset to DVC using ssh would be a good way to go about things. However the dvc add command is taking a long time to run. In fact I tried to run it on a 6GB subset of my data and it errored out:<\/p>\n<p><code>ERROR: too many open files, please visit &lt;https:\/\/error.dvc.org\/many-files&gt; to see how to handle this problem<\/code><\/p>\n<p>So my questions are:<\/p>\n<ol>\n<li>Is this a sensible way to be using DVC?<\/li>\n<li>If so, how can I speed things up? It seems like everything is getting downloaded to my local machine, even though I have set up a remote cache over ssh. Is there any way I can get it to compute the hashes on the remote machine?<\/li>\n<\/ol>\n<p>Thanks for your help,<br>\nMark<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1647375655860,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":215.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-with-external-data-is-very-slow\/1121",
        "Tool":"DVC",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"extern data slow think question part firstli us extern data describ best wai case slow add file context code git repo live local machin develop codebas pycharm us pycharm remot ssh deploy run code remot machin data live dataset larg imag dataset file fit local machin run code pycharm copi remot machin run copi git file git repo remot machin git repo data live differ machin like ad extern dataset ssh good wai thing add command take long time run fact tri run subset data error error open file visit handl problem question sensibl wai speed thing like get download local machin set remot cach ssh wai comput hash remot machin thank help mark",
        "Question_preprocessed_content":"extern data slow think question part firstli us extern data best wai case slow file context code git repo live local machin develop codebas pycharm us pycharm remot deploy run code remot machin data live dataset larg imag dataset file fit local machin run code pycharm copi remot machin run copi git file git repo remot machin git repo data live machin like extern dataset wai thing take long time run fact tri run subset data question sensibl wai thing like download local machin set remot cach wai comput hash remot machin thank help mark",
        "Question_gpt_summary_original":"The user is facing slow performance while adding external data to DVC using ssh. The dataset is a large imaging dataset of over 200GB with thousands of files, and the user's git repo and data live on different machines. The user is seeking advice on whether this is a sensible way to use DVC and how to speed up the process. The dvc add command is taking a long time to run, and the user is getting an error message when trying to run it on a 6GB subset of the data.",
        "Question_gpt_summary":"user face slow perform ad extern data ssh dataset larg imag dataset thousand file user git repo data live differ machin user seek advic sensibl wai us speed process add command take long time run user get error messag try run subset data"
    },
    {
        "Question_title":"Can I update NGINX version in Azure AKS Cluster?",
        "Question_body":"<p>Hopefully, I'm posting this in the right place. We have a ML team using an Azure AKS cluster which was built by me. Because this is all built around ML Studio I figured this might be the best place to ask for a dev viewpoint.<\/p>\n<p>A recent security scan identified several open ports on the nodes and workloads which identify themselves as runnning NGINX v1.10.3:<\/p>\n<pre><code>[root ~]# curl 10.210.100.62:32570 -ik\nHTTP\/1.1 200 OK\nServer: nginx\/1.10.3 (Ubuntu)\nDate: Wed, 09 Mar 2022 14:19:55 GMT\nContent-Type: text\/html; charset=utf-8\nContent-Length: 7\nConnection: keep-alive\n<\/code><\/pre>\n<p>The cluster is strictly used to host ML Studio inference endpoints.<\/p>\n<p>The open ports running NGINX are:<\/p>\n<pre><code>5001\n31366\n31419\n32570\n<\/code><\/pre>\n<p>I'm pretty sure 5001 is the listening port on all the inference endpoints, so I imagine it might have something to do with the ML Studio and how it deploys the inference endpoints. The other ports are probably some control ports on the Kubernetes nodes?<\/p>\n<p>I tried updating the kubernetes version on the control plain and nodes. But this didn't make any difference to the running NGINX version, even on the nodes. I connected to one of the hosts in a root shell, but the environment is really stripped and I didn't get very far in trying to identify where NGINX is running from or if it is even possible to update. I suspect attempting to do so in a shell can only break things.<\/p>\n<p>Does anybody know if it is even possible to update this in anyway?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1646838919210,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":278.0,
        "Poster_created_time":1504185478296,
        "Poster_location":null,
        "Poster_reputation_count":473.0,
        "Poster_view_count":66.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71411665",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"updat nginx version azur ak cluster hopefulli post right place team azur ak cluster built built studio figur best place ask dev viewpoint recent secur scan identifi open port node workload identifi runn nginx root curl http server nginx ubuntu date wed mar gmt content type text html charset utf content length connect aliv cluster strictli host studio infer endpoint open port run nginx pretti sure listen port infer endpoint imagin studio deploi infer endpoint port probabl control port kubernet node tri updat kubernet version control plain node differ run nginx version node connect host root shell environ strip far try identifi nginx run possibl updat suspect attempt shell break thing anybodi know possibl updat",
        "Question_preprocessed_content":"updat nginx version azur ak cluster post right place team azur ak cluster built built studio figur best place ask dev viewpoint recent secur scan identifi open port node workload identifi nginx cluster strictli host studio infer endpoint open port nginx sure listen port infer endpoint imagin studio deploi infer endpoint port probabl control port kubernet node tri updat kubernet version control plain node nginx version node host environ far try identifi nginx updat suspect break thing anybodi know updat",
        "Question_gpt_summary_original":"The user is seeking advice on updating the NGINX version in an Azure AKS cluster used for hosting ML Studio inference endpoints. A recent security scan revealed open ports running NGINX v1.10.3, and the user suspects that these ports are related to the ML Studio deployment of inference endpoints. The user attempted to update the Kubernetes version on the control plane and nodes, but it did not affect the NGINX version. The user is unsure if it is possible to update NGINX and is seeking advice.",
        "Question_gpt_summary":"user seek advic updat nginx version azur ak cluster host studio infer endpoint recent secur scan reveal open port run nginx user suspect port relat studio deploy infer endpoint user attempt updat kubernet version control plane node affect nginx version user unsur possibl updat nginx seek advic"
    },
    {
        "Question_title":"Integrate Sklearn Processing Step in Inference Pipeline",
        "Question_body":"Hello,\n\nI am facing a problem as I not able to integrate a fitted sklearn processor\/estimator in my sagemaker pipeline. I am defining the different steps in different functions as the follows:\n```\ndef _get_step_preprocess(\n    pipeline_session: PipelineSession,\n    processing_instance_count: ParameterInteger,\n    role: str,\n    # input_data_uri: ParameterString,\n    subnet_id: str,\n    security_group_id: str,\n) -> ProcessingStep:\n    \"\"\"\n    Step 1\n    This Step is preprocessing the data as a first step of the pipeline.\n    Args:\n        processing_instance_count (ParameterInteger): Number of instances\n        role (str): Sagemaker Execution Role\n\n    Returns:\n        ProcessingStep: Defined PreprocessingStep\n    \"\"\"\n\n    network_config = NetworkConfig(\n        enable_network_isolation=False,\n        security_group_ids=[security_group_id],\n        subnets=[subnet_id],\n        encrypt_inter_container_traffic=True,\n    )\n\n    sklearn_processor = FrameworkProcessor(\n        estimator_cls=SKLearn,\n        framework_version=\"1.0-1\",\n        instance_count=processing_instance_count,\n        instance_type=\"ml.m5.xlarge\",\n        sagemaker_session=pipeline_session,\n        base_job_name=\"name\",\n        role=role,\n        network_config=network_config,\n    )\n\n    processor_args = sklearn_processor.run(\n        inputs=[],\n        outputs=[\n            ProcessingOutput(output_name=\"train\", source=\"\/opt\/ml\/processing\/train\"),\n            ProcessingOutput(output_name=\"validation\", source=\"\/opt\/ml\/processing\/validation\"),\n            ProcessingOutput(output_name=\"test\", source=\"\/opt\/ml\/processing\/test\"),\n            ProcessingOutput(output_name=\"encoder\", source=\"\/opt\/ml\/processing\/encoder\"),\n        ],\n        code=\"main.py\",\n        source_dir=\"..\/sagemaker\/step_preprocess\",\n    )\n\n    step_preprocess = ProcessingStep(name=\"BankingSecondaryRejectionPreprocess\", step_args=processor_args)\n\n    return step_preprocess\n```\nIf seen in different examples that I am not only able to execute a script like in the given example but also fit a sklearn preprocessor which can be integrated in my final pipeline model and so in the whole inference endpoint. An example i came across was this:\n https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.html\n\nNevertheless, I am not able to integrate the sklearn estimator from the example into my whole preprocessing step defined above. How is it done the right way? Is it even possible? The ProcessingStep seems not to be able to take a fitted estimator as an argument.\n\nThanks in advance",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1673967894306,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1674314919148,
        "Question_score_count":0.0,
        "Question_view_count":36.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU1q1ppjUGQMK-uw0qbKCJag\/integrate-sklearn-processing-step-in-inference-pipeline",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"integr sklearn process step infer pipelin hello face problem abl integr fit sklearn processor estim pipelin defin differ step differ function follow def step preprocess pipelin session pipelinesess process instanc count parameterinteg role str input data uri parameterstr subnet str secur group str processingstep step step preprocess data step pipelin arg process instanc count parameterinteg number instanc role str execut role return processingstep defin preprocessingstep network config networkconfig enabl network isol fals secur group id secur group subnet subnet encrypt inter contain traffic true sklearn processor frameworkprocessor estim cl sklearn framework version instanc count process instanc count instanc type xlarg session pipelin session base job role role network config network config processor arg sklearn processor run input output processingoutput output train sourc opt process train processingoutput output valid sourc opt process valid processingoutput output test sourc opt process test processingoutput output encod sourc opt process encod code main sourc dir step preprocess step preprocess processingstep bankingsecondaryrejectionpreprocess step arg processor arg return step preprocess seen differ exampl abl execut script like given exampl fit sklearn preprocessor integr final pipelin model infer endpoint exampl came http exampl readthedoc latest python sdk scikit learn infer pipelin infer pipelin scikit learn linear learner html abl integr sklearn estim exampl preprocess step defin right wai possibl processingstep abl fit estim argument thank advanc",
        "Question_preprocessed_content":"integr sklearn step infer pipelin face problem abl integr sklearn estim pipelin defin step function exampl abl execut script like given exampl fit sklearn integr final pipelin model infer endpoint exampl came abl integr sklearn estim exampl step defin right wai abl estim argument thank advanc",
        "Question_gpt_summary_original":"The user is facing difficulty integrating a fitted sklearn processor\/estimator in their sagemaker pipeline. They have defined different steps in different functions and are not able to integrate the sklearn estimator from an example into their preprocessing step. They are seeking guidance on how to integrate a fitted estimator into their pipeline model and inference endpoint.",
        "Question_gpt_summary":"user face difficulti integr fit sklearn processor estim pipelin defin differ step differ function abl integr sklearn estim exampl preprocess step seek guidanc integr fit estim pipelin model infer endpoint"
    },
    {
        "Question_title":"Not able to archieve specific enviornment version in azure ml",
        "Question_body":"<p>I was trying to archive a specific environment version in azure machine learning using the &quot;az ml environment archive &quot; but getting error that the &quot;Version is already registered and can not be changed&quot;.      <br \/>\nAs per the Microsoft documentation at <a href=\"https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml\/environment?view=azure-cli-latest#az-ml-environment-archive\">https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml\/environment?view=azure-cli-latest#az-ml-environment-archive<\/a> it is possible to archive a specific version without archiving entire environment container.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/245922-image-1.png?platform=QnA\" alt=\"245922-image-1.png\" \/>    <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1664433909617,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1028195\/not-able-to-archieve-specific-enviornment-version",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"abl archiev specif enviorn version try archiv specif environ version environ archiv get error version regist chang microsoft document http learn microsoft com cli azur environ view azur cli latest environ archiv possibl archiv specif version archiv entir environ contain",
        "Question_preprocessed_content":"abl archiev specif enviorn version try archiv specif environ version environ archiv version regist chang microsoft document archiv specif version archiv entir environ contain",
        "Question_gpt_summary_original":"The user is facing an issue while trying to archive a specific environment version in Azure Machine Learning using \"az ml environment archive\". The error message states that the version is already registered and cannot be changed. The user is referring to Microsoft documentation, which suggests that it is possible to archive a specific version without archiving the entire environment container.",
        "Question_gpt_summary":"user face issu try archiv specif environ version environ archiv error messag state version regist chang user refer microsoft document suggest possibl archiv specif version archiv entir environ contain"
    },
    {
        "Question_title":"Making predictions with Azure Machine learning with new data that contains headers (like pd.Dataframe)",
        "Question_body":"<p>My question is somehow related to <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a> - however, the provided solution does not seem to work.<\/p>\n<p>I am constructing a simple model with heart-disease dataset but I wrap it into Pipeline as I use some featurization steps (scaling, encoding etc.) The full script below:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport pickle\n\n# data input\ndf = pd.read_csv('heart.csv')\n\n# numerical variables\nnum_cols = ['age',\n            'trestbps',\n            'chol',\n            'thalach',\n            'oldpeak'\n]\n\n# categorical variables\ncat_cols = ['sex',\n            'cp',\n            'fbs',\n            'restecg',\n            'exang',\n            'slope',\n            'ca',\n            'thal']\n\n# changing format of the categorical variables\ndf[cat_cols] = df[cat_cols].apply(lambda x: x.astype('object'))\n\n# target variable\ny = df['target']\n\n# features\nX = df.drop(['target'], axis=1)\n\n# data split:\n\n# random seed\nnp.random.seed(42)\n\n# splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2,\n                                                    stratify=y)\n\n# double check\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n# pipeline for numerical data\nnum_preprocessing = Pipeline([('num_imputer', SimpleImputer(strategy='mean')), # imputing with mean\n                                                   ('minmaxscaler', MinMaxScaler())]) # scaling\n\n# pipeline for categorical data\ncat_preprocessing = Pipeline([('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # filling missing values\n                                                ('onehot', OneHotEncoder(drop='first', handle_unknown='error'))]) # One Hot Encoding\n\n# preprocessor - combining pipelines\npreprocessor = ColumnTransformer([\n                                  ('categorical', cat_preprocessing, cat_cols),\n                                  ('numerical', num_preprocessing, num_cols)\n                                                           ])\n\n# initial model parameters\nlog_ini_params = {'penalty': 'l2', \n                  'tol': 0.0073559740277086005, \n                  'C': 1.1592424247511928, \n                  'fit_intercept': True, \n                  'solver': 'liblinear'}\n\n# model - Pipeline\nlog_clf = Pipeline([('preprocessor', preprocessor),\n                  ('clf', LogisticRegression(**log_ini_params))])\n\nlog_clf.fit(X_train, y_train)\n\n# dumping the model\nf = 'model\/log.pkl'\nwith open(f, 'wb') as file:\n    pickle.dump(log_clf, file)\n\n# loading it\nloaded_model = joblib.load(f)\n\n# double check on a single datapoint\nnew_data = pd.DataFrame({'age': 71,\n                         'sex': 0,\n                         'cp': 0,\n                         'trestbps': 112,\n                         'chol': 203,\n                         'fbs': 0,\n                         'restecg': 1,\n                         'thalach': 185,\n                         'exang': 0,\n                         'oldpeak': 0.1,\n                         'slope': 2,\n                         'ca': 0,\n                          'thal': 2}, index=[0])\n\nloaded_model.predict(new_data)\n\n<\/code><\/pre>\n<p>...and it works just fine.  Then I deploy the model to the Azure Web Service using these steps:<\/p>\n<ol>\n<li>I create the score.py file<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport json\n\ndef init():\n    global model\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n\ndef run(data):\n    try:\n        data = json.loads(data)\n        result = model.predict(data['data'])\n        # any data type, as long as it is JSON serializable.\n        return {'data' : result.tolist() , 'message' : 'Successfully classified heart diseases'}\n    except Exception as e:\n        error = str(e)\n        return {'data' : error , 'message' : 'Failed to classify heart diseases'}\n<\/code><\/pre>\n<ol>\n<li>I deploy the model:<\/li>\n<\/ol>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nws = Workspace.from_config()\n\nmodel = Model.register(workspace = ws,\n              model_path ='model\/log.pkl',\n              model_name = 'log',\n              tags = {'version': '1'},\n              description = 'Heart disease classification',\n              )\n\n# to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0'], conda_packages = ['scikit-learn==0.23.2'])\nenv.python.conda_dependencies = cd\n\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n\nmyenv = Environment.get(workspace=ws, name='env')\n\nmyenv.save_to_directory('.\/environ', overwrite=True)\n\naciconfig = AciWebservice.deploy_configuration(\n            cpu_cores=1,\n            memory_gb=1,\n            tags={'data':'heart disease classifier'},\n            description='Classification of heart diseases',\n            )\n\ninference_config = InferenceConfig(entry_script='score.py', environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                name='hd-model-log',\n                models=[model],\n                inference_config=inference_config,\n                deployment_config=aciconfig, \n                overwrite = True)\n\nservice.wait_for_deployment(show_output=True)\nurl = service.scoring_uri\nprint(url)\n<\/code><\/pre>\n<p>The deployment is fine:<\/p>\n<blockquote>\n<p>Succeeded\nACI service creation operation finished, operation &quot;Succeeded&quot;<\/p>\n<\/blockquote>\n<p>But I can not make any predictions with the new data. I try to use:<\/p>\n<pre><code>import pandas as pd\n\nnew_data = pd.DataFrame([[71, 0, 0, 112, 203, 0, 1, 185, 0, 0.1, 2, 0, 2],\n                         [80, 0, 0, 115, 203, 0, 1, 185, 0, 0.1, 2, 0, 0]],\n                         columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])\n<\/code><\/pre>\n<p>Following the answer from this topic (<a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a>) I transform the data:<\/p>\n<pre><code>test_sample = json.dumps({'data': new_data.to_dict(orient='records')})\n<\/code><\/pre>\n<p>And try to make some predictions:<\/p>\n<pre><code>import json\nimport requests\ndata = test_sample\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, data=data, headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>However, I encounter an error:<\/p>\n<blockquote>\n<p>200\n{'data': &quot;Expected 2D array, got 1D array instead:\\narray=[{'age': 71, 'sex': 0, 'cp': 0, 'trestbps': 112, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': &gt; 2}\\n {'age': 80, 'sex': 0, 'cp': 0, 'trestbps': 115, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': 0}].\\nReshape your data either using array.reshape(-1, &gt; 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&quot;, 'message': 'Failed to classify heart diseases'}<\/p>\n<\/blockquote>\n<p>How is it possible to adjust the input data to this form of predictions and add other output like predict_proba so I could store them in a separate output dataset?<\/p>\n<p>I know this error is somehow related either with the &quot;run&quot; part of the score.py file or the last code cell that calls the webservice, but I'm unable to find it.<\/p>\n<p>Would really appreciate some help.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1653476427160,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1653505948283,
        "Question_score_count":1.0,
        "Question_view_count":280.0,
        "Poster_created_time":1513074641863,
        "Poster_location":"Antarctica",
        "Poster_reputation_count":155.0,
        "Poster_view_count":13.0,
        "Answer_body":"<p>I believe I managed to solve the problem - even though I encountered some serious issues. :)<\/p>\n<ol>\n<li>As described here <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a> - I edited the <code>score.py<\/code> script:<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport numpy as np\nimport json\nimport pandas as pd\nimport numpy as np\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\nfrom inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n    \ndata_sample = PandasParameterType(pd.DataFrame({'age': pd.Series([0], dtype='int64'),\n                                                'sex': pd.Series(['example_value'], dtype='object'),\n                                                'cp': pd.Series(['example_value'], dtype='object'),\n                                                'trestbps': pd.Series([0], dtype='int64'),\n                                                'chol': pd.Series([0], dtype='int64'),\n                                                'fbs': pd.Series(['example_value'], dtype='object'),\n                                                'restecg': pd.Series(['example_value'], dtype='object'),\n                                                'thalach': pd.Series([0], dtype='int64'),\n                                                'exang': pd.Series(['example_value'], dtype='object'),\n                                                'oldpeak': pd.Series([0.0], dtype='float64'),\n                                                'slope': pd.Series(['example_value'], dtype='object'),\n                                                'ca': pd.Series(['example_value'], dtype='object'),\n                                                'thal': pd.Series(['example_value'], dtype='object')}))\n\ninput_sample = StandardPythonParameterType({'data': data_sample})\nresult_sample = NumpyParameterType(np.array([0]))\noutput_sample = StandardPythonParameterType({'Results':result_sample})\n\ndef init():\n    global model\n    # Example when the model is a file\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n@input_schema('Inputs', input_sample)\n@output_schema(output_sample)\ndef run(Inputs):\n    try:\n        data = Inputs['data']\n        result = model.predict_proba(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<ol start=\"2\">\n<li>In the deployment step I adjusted the <code>CondaDependencies<\/code>:<\/li>\n<\/ol>\n<pre><code># to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0', 'inference-schema==1.3.0'], conda_packages = ['scikit-learn==0.22.2.post1'])\nenv.python.conda_dependencies = cd\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n<\/code><\/pre>\n<p>as<\/p>\n<p>a) It is necessary to include <code>inference-schema<\/code> in the <code>Dependencies<\/code> file\nb) I downgraded <code>scikit-learn<\/code> to <code>scikit-learn==0.22.2.post1<\/code> version because of <a href=\"https:\/\/github.com\/hyperopt\/hyperopt\/issues\/668\" rel=\"nofollow noreferrer\">this issue<\/a><\/p>\n<p>Now, when I feed the model with new data:<\/p>\n<pre><code>new_data = {\n  &quot;Inputs&quot;: {\n    &quot;data&quot;: [\n      {\n        &quot;age&quot;: 71,\n        &quot;sex&quot;: &quot;0&quot;,\n        &quot;cp&quot;: &quot;0&quot;,\n        &quot;trestbps&quot;: 112,\n        &quot;chol&quot;: 203,\n        &quot;fbs&quot;: &quot;0&quot;,\n        &quot;restecg&quot;: &quot;1&quot;,\n        &quot;thalach&quot;: 185,\n        &quot;exang&quot;: &quot;0&quot;,\n        &quot;oldpeak&quot;: 0.1,\n        &quot;slope&quot;: &quot;2&quot;,\n        &quot;ca&quot;: &quot;0&quot;,\n        &quot;thal&quot;: &quot;2&quot;\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>And use it for prediction:<\/p>\n<pre><code>import json\nimport requests\ndata = new_data\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, str.encode(json.dumps(data)), headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>I get:<\/p>\n<p><code>200 [[0.02325369841858338, 0.9767463015814166]]<\/code><\/p>\n<p>Uff! Maybe someone will benefit from my painful learning path! :)<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1653558488392,
        "Answer_last_edit_time":1653559436590,
        "Answer_score_count":0.0,
        "Answerer_created_time":1513074641863,
        "Answerer_location":"Antarctica",
        "Answerer_reputation_count":155.0,
        "Answerer_view_count":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72376401",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"make predict new data contain header like datafram question relat http doc microsoft com answer question data input format servic azur html provid solut work construct simpl model heart diseas dataset wrap pipelin us featur step scale encod script import panda import numpi sklearn preprocess import minmaxscal sklearn compos import columntransform sklearn preprocess import onehotencod sklearn pipelin import pipelin sklearn model select import train test split sklearn imput import simpleimput sklearn linear model import logisticregress matplotlib inlin import matplotlib pyplot plt import seaborn sn import joblib import pickl data input read csv heart csv numer variabl num col ag trestbp chol thalach oldpeak categor variabl cat col sex fb restecg exang slope thal chang format categor variabl cat col cat col appli lambda astyp object target variabl target featur drop target axi data split random seed random seed split data train test train test train test split test size stratifi doubl check train shape test shape train shape test shape pipelin numer data num preprocess pipelin num imput simpleimput strategi mean imput mean minmaxscal minmaxscal scale pipelin categor data cat preprocess pipelin cat imput simpleimput strategi constant valu miss fill miss valu onehot onehotencod drop handl unknown error hot encod preprocessor combin pipelin preprocessor columntransform categor cat preprocess cat col numer num preprocess num col initi model paramet log ini param penalti tol fit intercept true solver liblinear model pipelin log clf pipelin preprocessor preprocessor clf logisticregress log ini param log clf fit train train dump model model log pkl open file pickl dump log clf file load load model joblib load doubl check singl datapoint new data datafram ag sex trestbp chol fb restecg thalach exang oldpeak slope thal index load model predict new data work fine deploi model azur web servic step creat score file import joblib core model import model import json def init global model model path model model path log logist print model path model path model joblib load model path def run data try data json load data result model predict data data data type long json serializ return data result tolist messag successfulli classifi heart diseas except error str return data error messag fail classifi heart diseas deploi model core import workspac core webservic import aciwebservic core webservic import webservic core model import inferenceconfig core environ import environ core import workspac core model import model core conda depend import condadepend workspac config model model regist workspac model path model log pkl model log tag version descript heart diseas classif instal requir packag env environ env condadepend creat pip packag panda default joblib conda packag scikit learn env python conda depend regist environ us later env regist workspac print regist environ myenv environ workspac env myenv save directori environ overwrit true aciconfig aciwebservic deploi configur cpu core memori tag data heart diseas classifi descript classif heart diseas infer config inferenceconfig entri script score environ myenv servic model deploi workspac model log model model infer config infer config deploy config aciconfig overwrit true servic wait deploy output true url servic score uri print url deploy fine succeed aci servic creation oper finish oper succeed predict new data try us import panda new data datafram column ag sex trestbp chol fb restecg thalach exang oldpeak slope thal follow answer topic http doc microsoft com answer question data input format servic azur html transform data test sampl json dump data new data dict orient record try predict import json import request data test sampl header content type applic json request post url data data header header print statu code print json encount error data expect arrai got arrai instead narrai ag sex trestbp chol fb restecg thalach exang oldpeak slope thal ag sex trestbp chol fb restecg thalach exang oldpeak slope thal nreshap data arrai reshap data singl featur arrai reshap contain singl sampl messag fail classifi heart diseas possibl adjust input data form predict add output like predict proba store separ output dataset know error relat run score file code cell call webservic unabl appreci help",
        "Question_preprocessed_content":"make predict new data contain header question relat provid solut work construct simpl model dataset wrap pipelin us featur step script work fine deploi model azur web servic step creat file deploi model deploy fine aci servic creation oper finish oper predict new data try us answer topic transform data try predict encount data expect got sex trestbp chol fb restecg thalach exang oldpeak slope thal data data singl featur reshap contain singl fail heart diseas adjust input data form predict output like store separ output dataset know relat run file code webservic unabl help",
        "Question_gpt_summary_original":"The user is facing challenges in making predictions with Azure Machine Learning using new data that contains headers. The user has constructed a simple model with heart-disease dataset and wrapped it into Pipeline as they use some featurization steps. The model is deployed to the Azure Web Service, but the user is unable to make any predictions with the new data. The user is encountering an error related to the input data format and is seeking help to adjust the input data to this form of predictions and add other output like predict_proba so they could store them in a separate output dataset.",
        "Question_gpt_summary":"user face challeng make predict new data contain header user construct simpl model heart diseas dataset wrap pipelin us featur step model deploi azur web servic user unabl predict new data user encount error relat input data format seek help adjust input data form predict add output like predict proba store separ output dataset"
    },
    {
        "Question_title":"Mlflow licensed version",
        "Question_body":"Hi ,\n\n\nIs there a licensed version of mlflow? If not, how is everyone who is leveraging mlflow able to incorporate permissions to access the underlying models?\n\n\nThank you,\u00a0\nPuja",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1616612526000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":296.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/3DIZAVPgqeI",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"licens version licens version leverag abl incorpor permiss access underli model thank puja",
        "Question_preprocessed_content":"licens version licens version leverag abl incorpor underli model thank puja",
        "Question_gpt_summary_original":"The user is inquiring about the availability of a licensed version of mlflow and how users are able to incorporate permissions to access the underlying models.",
        "Question_gpt_summary":"user inquir avail licens version user abl incorpor permiss access underli model"
    },
    {
        "Question_title":"Issue with using system functions in parameter presets",
        "Question_body":"Hi there,\n\nI'm trying to dynamically append values to a list parameter using the 'Parameter Preset' box when certain conditions are met for a transitional route.\u00a0\n\nThe list parameter is defined as follows:\u00a0\n\nThen, during the route I use the following system function:\n\n\u00a0\n\n$sys.func.APPEND($session.params.negative_products, credit score)\n\n\u00a0\n\n\u2003However, when the condition is met during the conversation, the updated value in $session.params.negative_products is not: [\" \", credit score], but prints out the whole text in the parameter preset box, e.g.,\u00a0$sys.func.APPEND($session.params.negative_products, credit score).\u00a0\n\nWhy is this the case? I thought system functions were able to be used to dynamically change values in parameter presets and I have no idea why it just keeps on printing out the function as a string!\u00a0\n\nAny help would be much appreciated,\n\nVicky",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1674629580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":145.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Hey!\n\nWhen referencing the parameter in my fulfilment I use:\u00a0\n\n$session.params.negative_products\n\n\u00a0I did figure a workaround yesterday. I realised when I define $session.params.negative_products within the 'Entry Fulfilment', 'Parameter Presets' section of the page the appending works:\u00a0\n\n1. Define empty list parameter in presets of the Entry Fulfilment, i.e., before I want to append to the list:\u00a0\n\n\u20032. During a conditional route, append to the empty list using system functions in the parameter presets with the following command:\u00a0\n\n$sys.func.APPEND($session.params.negative_products, \"credit score\")\n\n3. This then prints out the correct output when referencing $session.params.negative_products:\u00a0\n\n\" , credit score\"\n\nI think my original issue was that when trying to define $session.params.negative_products as a 'isList' parameter, it wasn't actually generating a list per se. So, when I was trying to call it in $sys.func.APPEND(), the function wasn't reading the $session.params.negative_products as a list, and as a result, just printed out the system command.\u00a0\n\nHope this helps other users who get faced with a similar issue!\n\nVicky\n\nView solution in original post",
        "Answer_comment_count":null,
        "Question_closed_time":"01-27-2023 02:03 AM",
        "Answer_last_edit_time":null,
        "Answer_score_count":0.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issue-with-using-system-functions-in-parameter-presets\/td-p\/513851\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":1.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"issu function paramet preset try dynam append valu list paramet paramet preset box certain condit met transit rout list paramet defin follow rout us follow function sy func append session param neg product credit score condit met convers updat valu session param neg product credit score print text paramet preset box sy func append session param neg product credit score case thought function abl dynam chang valu paramet preset idea keep print function string help appreci vicki",
        "Question_preprocessed_content":"function paramet preset try valu list paramet paramet preset box certain condit met transit rout list paramet defin rout us function credit score condit met convers updat valu print text paramet preset box credit score case thought function abl chang valu paramet preset idea print function string help vicki",
        "Question_gpt_summary_original":"The user is facing an issue while trying to append values to a list parameter using the 'Parameter Preset' box when certain conditions are met for a transitional route. The system function used to append values is not working as expected and instead of updating the value in $session.params.negative_products, it prints out the whole text in the parameter preset box. The user is seeking help to understand why this is happening.",
        "Question_gpt_summary":"user face issu try append valu list paramet paramet preset box certain condit met transit rout function append valu work expect instead updat valu session param neg product print text paramet preset box user seek help understand happen"
    },
    {
        "Question_title":"Available framework version for AWS SageMaker SKLearn",
        "Question_body":"<p>I'm using SageMaker Python SDK's SKLearn class which has a parameter called framework version. Its default is 0.20.0.\nI want to use a different version of Scikit-learn (0.21.0 or higher). It is possible? How do I know which versions are supported?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1592749422133,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":1436.0,
        "Poster_created_time":1515413576887,
        "Poster_location":null,
        "Poster_reputation_count":459.0,
        "Poster_view_count":58.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62499878",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"avail framework version sklearn python sdk sklearn class paramet call framework version default want us differ version scikit learn higher possibl know version support",
        "Question_preprocessed_content":"avail framework version sklearn python sdk sklearn paramet framework version default want us version know version",
        "Question_gpt_summary_original":"The user is using the SKLearn class in AWS SageMaker Python SDK and wants to know if it is possible to use a different version of Scikit-learn (0.21.0 or higher) and how to know which versions are supported.",
        "Question_gpt_summary":"user sklearn class python sdk want know possibl us differ version scikit learn higher know version support"
    },
    {
        "Question_title":"Azure ML Web Service - Slow response time",
        "Question_body":"<p>I have developed a Machine learning model (Random Forest classification model) in Azure Machine Learning Studio &amp; deployed the same to Azure Container Instance (ACI) as a web service.<\/p>\n\n<p>I then test the performance of the Webservice from within a notebook running in Azure ML studio with a 4 core, 8 GB ram back-end Azure compute. Using the <code>%%timeit<\/code> magic command in Jupyter notebook, I get an average speed of around 1.2 seconds\/loop (for running inference on 1000 data points).<\/p>\n\n<p>However, when I test the same webservice outside of Azure, in my local machine, the performance drops to ~5 seconds\/loop (more than 4 times slower)<\/p>\n\n<p>I am fairly new to web service deployment, so I am really not sure how to go about troubleshooting this (and could not find any helpful info on googling either).\nDo let me know if there are any specific configuration or environment details required to answer this (I have mostly just followed the azure documentation for deployment,e.g. the tutorial <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance\" rel=\"nofollow noreferrer\">here<\/a>) <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1590264116550,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":328.0,
        "Poster_created_time":1433054608167,
        "Poster_location":null,
        "Poster_reputation_count":1409.0,
        "Poster_view_count":70.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61977905",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"web servic slow respons time develop machin learn model random forest classif model studio deploi azur contain instanc aci web servic test perform webservic notebook run studio core ram end azur comput timeit magic command jupyt notebook averag speed second loop run infer data point test webservic outsid azur local machin perform drop second loop time slower fairli new web servic deploy sure troubleshoot help info googl let know specif configur environ detail requir answer follow azur document deploy tutori",
        "Question_preprocessed_content":"web servic slow respons time develop machin learn model studio deploi azur contain instanc web servic test perform webservic studio core ram azur comput magic jupyt averag test webservic outsid azur local machin perform drop fairli new web servic deploy sure let know specif configur environ detail requir answer",
        "Question_gpt_summary_original":"The user has developed a machine learning model in Azure Machine Learning Studio and deployed it to Azure Container Instance as a web service. When testing the performance of the web service from within Azure, the average speed is around 1.2 seconds\/loop, but when testing outside of Azure on their local machine, the performance drops to ~5 seconds\/loop. The user is seeking help in troubleshooting this issue.",
        "Question_gpt_summary":"user develop machin learn model studio deploi azur contain instanc web servic test perform web servic azur averag speed second loop test outsid azur local machin perform drop second loop user seek help troubleshoot issu"
    },
    {
        "Question_title":"How do I get the version of an artifact?",
        "Question_body":"<p>Hey,<br>\nI\u2019m trying to get the version of an artifact directly after logging my model (encoder) as an artifact to WandB.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code class=\"lang-auto\">artifact = wandb.Artifact('my_artifact_name', type='model')\nartifact.add_file('\/home\/dezzardhd\/encoder.pth')\nwandb.log_artifact(artifact)\nversion = artifact.version\n<\/code><\/pre>\n<p>Logging works so far, but\u2026<br>\nwhen trying to access the version of the artifact I get an error.<br>\n<strong>Error:<\/strong><\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/main.py\", line 48, in &lt;module&gt;\n    train_setups.start_training_sessions(project=project)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/train_setups.py\", line 18, in start_training_sessions\n    model_pipeline(config, project=project)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/learning.py\", line 84, in model_pipeline\n    save_model(model_ae=model, model_encoder=model_encoder, model_decoder=model_decoder)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/learning.py\", line 124, in save_model\n    version = artifact_enc.version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_artifacts.py\", line 191, in version\n    return self._logged_artifact.version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 2899, in version\n    return self._assert_instance().version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 2871, in _assert_instance\n    raise ValueError(\nValueError: Must call wait() before accessing logged artifact properties\n<\/code><\/pre>\n<p>What should I do now?<\/p>\n<p>For context:<br>\nI want to print out the version number with some other parameters so that I can easier start my evaluation process for certain runs.<\/p>\n<p>Best regards<br>\nDezzardHD<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_created_time":1646690111212,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":411.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dezzardhd\">@dezzardhd<\/a>,<\/p>\n<p>Could you try running your code as the following?<\/p>\n<pre><code class=\"lang-python\">artifact = wandb.Artifact('my_artifact_name', type='model')\nartifact.add_file('\/home\/dezzardhd\/encoder.pth')\nwandb.log_artifact(artifact).wait()\nversion = artifact.version\n<\/code><\/pre>\n<p>Calling <code>wait()<\/code> after <code>log_artifact()<\/code> should resolve this for you.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Question_closed_time":1646696888233,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-i-get-the-version-of-an-artifact\/2035",
        "Tool":"Weights & Biases",
        "Question_self_resolution":0.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"version artifact hei try version artifact directli log model encod artifact code artifact artifact artifact type model artifact add file home dezzardhd encod pth log artifact artifact version artifact version log work far try access version artifact error error traceback recent file home moritz pycharmproject bachelorarbeit main line train setup start train session project project file home moritz pycharmproject bachelorarbeit train setup line start train session model pipelin config project project file home moritz pycharmproject bachelorarbeit learn line model pipelin save model model model model encod model encod model decod model decod file home moritz pycharmproject bachelorarbeit learn line save model version artifact enc version file home moritz anaconda env bachelorarbeit lib python site packag sdk artifact line version return self log artifact version file home moritz anaconda env bachelorarbeit lib python site packag sdk run line version return self assert instanc version file home moritz anaconda env bachelorarbeit lib python site packag sdk run line assert instanc rais valueerror valueerror wait access log artifact properti context want print version number paramet easier start evalu process certain run best regard dezzardhd",
        "Question_preprocessed_content":"version artifact hei try version artifact directli model artifact code work far try version artifact context want print version number paramet easier start evalu certain run best regard",
        "Question_gpt_summary_original":"The user is trying to get the version of an artifact after logging their model as an artifact to WandB. Although logging works, the user encounters an error when trying to access the version of the artifact. The error message suggests that the user must call wait() before accessing logged artifact properties. The user is seeking advice on what to do next as they want to print out the version number with some other parameters to start their evaluation process for certain runs.",
        "Question_gpt_summary":"user try version artifact log model artifact log work user encount error try access version artifact error messag suggest user wait access log artifact properti user seek advic want print version number paramet start evalu process certain run"
    },
    {
        "Question_title":"Azure Machine Learning Python SDK Error \"RunIDConflict\"",
        "Question_body":"<p>Hello everybody,  <\/p>\n<p>I am currently learning Azure Machine Learning using the learning paths and labs. The lab scripts give me the following error I could not find anywhere yet.  <\/p>\n<p>&quot;Run IDs must be unique within a workspace and can only be used once. Ensure multiple runs with the same ID are not submitted simultaneously.&quot;  <\/p>\n<p>I am running the scripts locally. I have tried both Jupyter Notebook and VS Code. The error occurs exactly after 65 seconds when running experiment scripts. Both <a href=\"https:\/\/github.com\/MicrosoftDocs\/mslearn-aml-labs\/blob\/master\/01-Getting_Started_with_Azure_ML.ipynb\">lab 1<\/a> with script config and <a href=\"https:\/\/github.com\/MicrosoftDocs\/mslearn-aml-labs\/blob\/master\/02-Training_Models.ipynb\">lab 2<\/a> with an estimator give me the error. Running experiments directly within the IPython script works and I can see the results in the Azure web GUI. My SDK version is 1.21. When I did this in May 2020 with SDK version 1.15 I did not receive this error.  <\/p>\n<p>The respective code blocks that throw the errors copied from the lab scripts:  <\/p>\n<p>Lab 1:  <\/p>\n<pre><code>import os\nimport sys\nfrom azureml.core import Experiment, ScriptRunConfig\nfrom azureml.widgets import RunDetails\n\n\n# Create a script config\nscript_config = ScriptRunConfig(source_directory=experiment_folder, \n                      script='diabetes_experiment.py') \n\n# submit the experiment\nexperiment = Experiment(workspace = ws, name = 'diabetes-experiment')\nrun = experiment.submit(config=script_config)\nRunDetails(run).show()\nrun.wait_for_completion()\n<\/code><\/pre>\n<p>Lab 2:  <\/p>\n<pre><code>from azureml.train.estimator import Estimator\nfrom azureml.core import Experiment\n\n# Create an estimator\nestimator = Estimator(source_directory=training_folder,\n                      entry_script='diabetes_training.py',\n                      compute_target='local',\n                      conda_packages=['scikit-learn']\n                      )\n\n# Create an experiment\nexperiment_name = 'diabetes-training'\nexperiment = Experiment(workspace = ws, name = experiment_name)\n\n# Run the experiment based on the estimator\nrun = experiment.submit(config=estimator)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>Can you tell me how to fix this error? Thanks in advance.  <\/p>\n<p>Regards  <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":7.0,
        "Question_created_time":1612451511860,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/259049\/azure-machine-learning-python-sdk-error-runidconfl",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"python sdk error runidconflict hello everybodi current learn learn path lab lab script follow error run id uniqu workspac ensur multipl run submit simultan run script local tri jupyt notebook code error occur exactli second run experi script lab script config lab estim error run experi directli ipython script work result azur web gui sdk version sdk version receiv error respect code block throw error copi lab script lab import import sy core import experi scriptrunconfig widget import rundetail creat script config script config scriptrunconfig sourc directori experi folder script diabet experi submit experi experi experi workspac diabet experi run experi submit config script config rundetail run run wait complet lab train estim import estim core import experi creat estim estim estim sourc directori train folder entri script diabet train comput target local conda packag scikit learn creat experi experi diabet train experi experi workspac experi run experi base estim run experi submit config estim run wait complet output true tell fix error thank advanc regard",
        "Question_preprocessed_content":"python sdk runidconflict everybodi learn learn path lab lab script run id uniqu workspac ensur multipl run script tri jupyt code exactli second experi script lab script config lab estim experi directli ipython script work result azur web gui sdk version sdk version receiv respect code block throw copi lab script lab lab fix thank advanc regard",
        "Question_gpt_summary_original":"The user is encountering a \"RunIDConflict\" error while learning Azure Machine Learning using lab scripts. The error occurs after 65 seconds when running experiment scripts locally in both Jupyter Notebook and VS Code. The error message states that run IDs must be unique within a workspace and can only be used once. The user is using SDK version 1.21 and did not receive this error when using version 1.15 in May 2020. The user has provided code blocks from lab 1 and lab 2 that throw the error.",
        "Question_gpt_summary":"user encount runidconflict error learn lab script error occur second run experi script local jupyt notebook code error messag state run id uniqu workspac user sdk version receiv error version user provid code block lab lab throw error"
    },
    {
        "Question_title":"Import custom modules in Amazon Sagemaker Jupyter notebook",
        "Question_body":"<p>I want to import a custom module in my jupyter notebook in Sagemaker. Trying the import from Untitled1.ipynb I have tried two different structures. The first one is:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tJb5l.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tJb5l.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Inside \"package folder\" there were the files \"cross_validation.py\" and \"<strong>init<\/strong>.py\". The followings commands have been tried:<\/p>\n\n<pre><code>from package import cross_validation\nimport package.cross_validation\n<\/code><\/pre>\n\n<p>The second one is<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/eplmc.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eplmc.png\" alt=\"emak\"><\/a><\/p>\n\n<p>and I have coded  <code>import cross_validation<\/code><\/p>\n\n<p>In both cases I get no error at all when importing, but I can't use the class inside the module because I receive the error name <code>Class_X is not defined<\/code><\/p>\n\n<p>I also have restarted the notebook, just in case and it still not working. How could I make it?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_created_time":1558540323460,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":5.0,
        "Question_view_count":6033.0,
        "Poster_created_time":1523298968403,
        "Poster_location":null,
        "Poster_reputation_count":1754.0,
        "Poster_view_count":197.0,
        "Answer_body":"<p>You can add a <code>__init__.py<\/code> file to your <code>package<\/code> directory to make it a Python package. Then you will be import the modules from the package inside your Jupyter notebook<\/p>\n\n<pre><code>\/home\/ec2-user\/SageMaker\n    -- Notebook.ipynb \n    -- mypackage\n        -- __init__.py\n        -- mymodule.py\n<\/code><\/pre>\n\n<p>Contents of Notebook.ipynb<\/p>\n\n<pre><code>from mypackage.mymodule import SomeClass, SomeOtherClass\n<\/code><\/pre>\n\n<p>For more details, see <a href=\"https:\/\/docs.python.org\/3\/tutorial\/modules.html#packages\" rel=\"nofollow noreferrer\">https:\/\/docs.python.org\/3\/tutorial\/modules.html#packages<\/a><\/p>\n\n<p>Thanks for using Amazon SageMaker!<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1558571220172,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1510046220943,
        "Answerer_location":null,
        "Answerer_reputation_count":292.0,
        "Answerer_view_count":43.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56260720",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"import custom modul jupyt notebook want import custom modul jupyt notebook try import untitl ipynb tri differ structur insid packag folder file cross valid init follow command tri packag import cross valid import packag cross valid second code import cross valid case error import us class insid modul receiv error class defin restart notebook case work",
        "Question_preprocessed_content":"import custom modul jupyt want import custom modul jupyt try import tri structur insid packag folder file tri second code case import us insid modul receiv restart case work",
        "Question_gpt_summary_original":"The user is facing challenges while trying to import a custom module in their Jupyter notebook in Amazon Sagemaker. They have tried two different structures to import the module, but they are unable to use the class inside the module as they receive an error stating that the class is not defined. The user has also restarted the notebook, but the issue persists.",
        "Question_gpt_summary":"user face challeng try import custom modul jupyt notebook tri differ structur import modul unabl us class insid modul receiv error state class defin user restart notebook issu persist"
    },
    {
        "Question_title":"Extract feature importance from a mlflow 1.9 PyFuncModel model",
        "Question_body":"<p><strong>Top line<\/strong>: How can I extract feature importance from an xgboost model that has been saved in mlflow as a PyFuncModel?<\/p>\n<p>Details:<\/p>\n<ul>\n<li>I've picked up model update responsibilities from a data scientist who has just left. They used mlflow to tune hyperparameters. I need to understand the feature importance from the model they have built. My attempts of using <code>shap<\/code> have failed (my code below).<\/li>\n<li>I'm using mlflow 1.9.1. I can see that <code>mlflow.shap<\/code> exists in the current version, but this does not seem to appear in the version I'm on.<\/li>\n<\/ul>\n<pre><code>import mlflow\nimport shap\n\nmodel = mlflow.pyfunc.load_model(model_load_details)  \nprint(f&quot;model {type(model)})&quot;) \n# model &lt;class 'mlflow.pyfunc.PyFuncModel'&gt;)\n\nexplainer = shap.Explainer(model)\n<\/code><\/pre>\n<p>... which returns the error message &quot;Exception: The passed model is not callable and cannot be analyzed directly with the given masker! Model: mlflow.pyfunc.loaded_model:&quot;<\/p>\n<p><strong>My own thinking<\/strong>: Extract the parameter settings for the best model from mlflow, use these to retrain fresh xgboost model, then save as an xgboost flavor: From <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/python_api\/mlflow.xgboost.html\" rel=\"nofollow noreferrer\">here<\/a>, then use <code>mlflow.xgboost.save_model()<\/code>. But, is there a better way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1642078578843,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":417.0,
        "Poster_created_time":1369061791847,
        "Poster_location":null,
        "Poster_reputation_count":607.0,
        "Poster_view_count":60.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70696986",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"extract featur import pyfuncmodel model line extract featur import xgboost model save pyfuncmodel detail pick model updat respons data scientist left tune hyperparamet need understand featur import model built attempt shap fail code shap exist current version appear version import import shap model pyfunc load model model load detail print model type model model explain shap explain model return error messag except pass model callabl analyz directli given masker model pyfunc load model think extract paramet set best model us retrain fresh xgboost model save xgboost flavor us xgboost save model better wai",
        "Question_preprocessed_content":"extract featur import pyfuncmodel model line extract featur import model save pyfuncmodel detail pick model updat respons data scientist left tune hyperparamet understand featur import model built fail exist version version return except model analyz directli given masker model think extract paramet best model us retrain fresh model save flavor us wai",
        "Question_gpt_summary_original":"The user is trying to extract feature importance from an xgboost model saved in mlflow as a PyFuncModel. They have attempted to use shap but encountered an error message. They are using mlflow 1.9.1 and cannot find mlflow.shap. The user is considering retraining a fresh xgboost model using parameter settings from mlflow and saving it as an xgboost flavor.",
        "Question_gpt_summary":"user try extract featur import xgboost model save pyfuncmodel attempt us shap encount error messag shap user consid retrain fresh xgboost model paramet set save xgboost flavor"
    },
    {
        "Question_title":"Issues with importing aiplatform",
        "Question_body":"Hi, I am following this tutorial on model deployment (https:\/\/codelabs.developers.google.com\/vertex-image-deploy#6), but I ran into a issue when importing the aiplatform library.\n\nWhen running \"from google.cloud import aiplatform\", I get the following error message:\n\nImportError                               Traceback (most recent call last)\n\/tmp\/ipykernel_22080\/3236611779.py in <module>\n      4 #!python #3.7.12\n      5 \n----> 6 from google.cloud import aiplatform\n      7 \n      8 import tensorflow as tf\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/__init__.py in <module>\n     22 \n     23 \n---> 24 from google.cloud.aiplatform import initializer\n     25 \n     26 from google.cloud.aiplatform.datasets import (\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/cloud\/aiplatform\/initializer.py in <module>\n     24 \n     25 from google.api_core import client_options\n---> 26 from google.api_core import gapic_v1\n     27 import google.auth\n     28 from google.auth import credentials as auth_credentials\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/__init__.py in <module>\n     17 from google.api_core.gapic_v1 import config_async\n     18 from google.api_core.gapic_v1 import method\n---> 19 from google.api_core.gapic_v1 import method_async\n     20 from google.api_core.gapic_v1 import routing_header\n     21 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/gapic_v1\/method_async.py in <module>\n     20 import functools\n     21 \n---> 22 from google.api_core import grpc_helpers_async\n     23 from google.api_core.gapic_v1 import client_info\n     24 from google.api_core.gapic_v1.method import _GapicCallable\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/google\/api_core\/grpc_helpers_async.py in <module>\n     23 \n     24 import grpc\n---> 25 from grpc import aio\n     26 \n     27 from google.api_core import exceptions, grpc_helpers\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/aio\/__init__.py in <module>\n     21 \n     22 import grpc\n---> 23 from grpc._cython.cygrpc import (init_grpc_aio, shutdown_grpc_aio, EOF,     24                                  AbortError, BaseError, InternalError,\n     25                                  UsageError)\n\nImportError: cannot import name 'shutdown_grpc_aio' from 'grpc._cython.cygrpc' (\/opt\/conda\/lib\/python3.7\/site-packages\/grpc\/_cython\/cygrpc.cpython-37m-x86_64-linux-gnu.so)\n\nThe versions of the concerned libraries are shown below.\n\ngoogle-api-core                       2.10.1\ngoogle-api-python-client              2.55.0\ngoogle-cloud-aiplatform               1.17.0\n\ngrpcio                                1.33.1\ngrpcio-gcp                            0.2.2\ngrpcio-status                         1.47.0\n\nI have tried grpcio versions 1.26, 1.27.2, and even the latest 1.50, but all of them had import errors (concerning importing of aio module for 1.26 and 127.2 and AbortError module for 1.50). Are there any additional steps or libraries that I need to take to avoid these import errors?\n\nThank you!",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_created_time":1668388200000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":0.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"Hi, thank you for your reply. I am running the code on Vertex AI.\n\nI realised I had to restart the kernel to refresh the package after updating grpcio, and I could then import aiplatform without any issues as shown below:\n\nfrom google.cloud import aiplatform\nprint(\"aiplatform version: \", aiplatform.__version__)\n\naiplatform version:  1.17.0\n\nThanks again for your help!\n\nView solution in original post",
        "Answer_comment_count":null,
        "Question_closed_time":"11-14-2022 05:15 PM",
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Issues-with-importing-aiplatform\/td-p\/489087\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":1.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"issu import aiplatform follow tutori model deploy http codelab develop googl com vertex imag deploi ran issu import aiplatform librari run googl cloud import aiplatform follow error messag importerror traceback recent tmp ipykernel python googl cloud import aiplatform import tensorflow opt conda lib python site packag googl cloud aiplatform init googl cloud aiplatform import initi googl cloud aiplatform dataset import opt conda lib python site packag googl cloud aiplatform initi googl api core import client option googl api core import gapic import googl auth googl auth import credenti auth credenti opt conda lib python site packag googl api core gapic init googl api core gapic import config async googl api core gapic import method googl api core gapic import method async googl api core gapic import rout header opt conda lib python site packag googl api core gapic method async import functool googl api core import grpc helper async googl api core gapic import client info googl api core gapic method import gapiccal opt conda lib python site packag googl api core grpc helper async import grpc grpc import aio googl api core import except grpc helper opt conda lib python site packag grpc aio init import grpc grpc cython cygrpc import init grpc aio shutdown grpc aio eof aborterror baseerror internalerror usageerror importerror import shutdown grpc aio grpc cython cygrpc opt conda lib python site packag grpc cython cygrpc cpython linux gnu version concern librari shown googl api core googl api python client googl cloud aiplatform grpcio grpcio gcp grpcio statu tri grpcio version latest import error concern import aio modul aborterror modul addit step librari need avoid import error thank",
        "Question_preprocessed_content":"import aiplatform tutori model deploy ran import aiplatform librari cloud import aiplatform traceback python cloud import aiplatform import tensorflow import initi import import version concern librari shown grpcio tri grpcio version latest import step librari avoid import thank",
        "Question_gpt_summary_original":"The user is facing an issue while importing the aiplatform library while following a tutorial on model deployment. The error message shows an ImportError and the concerned libraries' versions are also mentioned. The user has tried different versions of grpcio but still faces import errors. The user is seeking additional steps or libraries to avoid these errors.",
        "Question_gpt_summary":"user face issu import aiplatform librari follow tutori model deploy error messag show importerror concern librari version mention user tri differ version grpcio face import error user seek addit step librari avoid error"
    },
    {
        "Question_title":"Add model description when registering model after hyperdrive successful run",
        "Question_body":"<p>I have successfully trained a model on Azure Machine Learning Service using Hyperdrive that has now yielded a hyperdrive run instance<\/p>\n\n<pre><code>hyperdrive_run = exp.submit(config=hypertune_config)\nhyperdrive_run\nbest_run = hyperdrive_run.get_best_run_by_primary_metric()\n<\/code><\/pre>\n\n<p>As a next step, I would like to register a model while adding a description to the model.:<\/p>\n\n<pre><code>pumps_rf = best_run.register_model(model_name='pumps_rf', model_path='outputs\/rf.pkl')\n<\/code><\/pre>\n\n<p>There is a <code>description<\/code> column in the Models section of my AML Workspace on Azure portal but the <code>register_model<\/code> method does not seem to have a <code>description<\/code> flag. So how do I go about adding a description to the model so I see it in Azure Portal?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_created_time":1550539380057,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":453.0,
        "Poster_created_time":1408574571227,
        "Poster_location":"Toronto, Canada",
        "Poster_reputation_count":2754.0,
        "Poster_view_count":124.0,
        "Answer_body":"<p>Good question :).<\/p>\n\n<p>Looking at the current version of the API, it doesn't look like you can add the description using <code>Run.register_model<\/code>, as confirmed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#register-model-model-name--model-path-none--tags-none--properties-none----kwargs-\" rel=\"nofollow noreferrer\">by the docs<\/a>. <\/p>\n\n<p>You can go around this however by registering the model using the <code>Model.register<\/code> method which, fortunately, includes an argument for <code>description<\/code> as detailed <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">here<\/a>. In your case, you also need to <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run(class)?view=azure-ml-py#download-file-name--output-file-path-none-\" rel=\"nofollow noreferrer\">download the files<\/a> first.<\/p>\n\n<p>In short, use something like:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>best_run.download_file('outputs\/rf.pkl', output_file_path='.\/rf.pkl')\n\nModel.register(workspace=ws, model_path='.\/rf.pkl', model_name=\"pumps_rf\", description=\"There are many models like it, but this one is mine.\")\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Question_closed_time":1550668517447,
        "Answer_last_edit_time":1550686459150,
        "Answer_score_count":1.0,
        "Answerer_created_time":1250158552416,
        "Answerer_location":"Romania",
        "Answerer_reputation_count":7916.0,
        "Answerer_view_count":801.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54757598",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"add model descript regist model hyperdr success run successfulli train model servic hyperdr yield hyperdr run instanc hyperdr run exp submit config hypertun config hyperdr run best run hyperdr run best run primari metric step like regist model ad descript model pump best run regist model model pump model path output pkl descript column model section aml workspac azur portal regist model method descript flag ad descript model azur portal",
        "Question_preprocessed_content":"model descript regist model hyperdr run train model servic hyperdr yield hyperdr run instanc step like regist model descript column model section aml workspac azur portal method flag descript model azur portal",
        "Question_gpt_summary_original":"The user has successfully trained a model on Azure Machine Learning Service using Hyperdrive and wants to register the model while adding a description to it. However, the register_model method does not seem to have a description flag, and the user is unsure how to add a description to the model so that it appears in the Azure Portal.",
        "Question_gpt_summary":"user successfulli train model servic hyperdr want regist model ad descript regist model method descript flag user unsur add descript model appear azur portal"
    },
    {
        "Question_title":"Why do I encounter \"INVALID_PARAMETER VALUE\" error when opening \"Models\" tab in MLFlow UI?",
        "Question_body":"<p>I installed mlflow via pip and opened it in the browser through the terminal. The tab Experiments displays normally; however, when I switch to Models tab, the app crashes due to the following error:<\/p>\n<pre><code>INVALID_PARAMETER_VALUE: Model registry functionality is unavailable; \ngot unsupported URI '.\/mlruns' for model registry data storage. \nSupported URI schemes are: ['postgresql', 'mysql', 'sqlite', 'mssql']. \nSee https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#storage for how to run \nan MLflow server against one of the supported backend storage locations.\n<\/code><\/pre>\n<p>I would like to log model data locally without connecting to external servers or databases. I thank you in advance for any help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1659957616140,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1660218063030,
        "Question_score_count":1.0,
        "Question_view_count":39.0,
        "Poster_created_time":1652711404248,
        "Poster_location":null,
        "Poster_reputation_count":51.0,
        "Poster_view_count":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73277043",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"encount invalid paramet valu error open model tab instal pip open browser termin tab experi displai normal switch model tab app crash follow error invalid paramet valu model registri function unavail got unsupport uri mlrun model registri data storag support uri scheme postgresql mysql sqlite mssql http www org doc latest track html storag run server support backend storag locat like log model data local connect extern server databas thank advanc help",
        "Question_preprocessed_content":"encount valu open model tab pip open browser termin tab experi displai switch model tab crash like log model data extern server databas thank advanc help",
        "Question_gpt_summary_original":"The user encounters an \"INVALID_PARAMETER_VALUE\" error when trying to open the \"Models\" tab in MLFlow UI. The error message suggests that the model registry functionality is unavailable due to an unsupported URI for model registry data storage. The user wants to log model data locally without connecting to external servers or databases.",
        "Question_gpt_summary":"user encount invalid paramet valu error try open model tab error messag suggest model registri function unavail unsupport uri model registri data storag user want log model data local connect extern server databas"
    },
    {
        "Question_title":"I am trying to create a job in azure ML and after giving the correct inputs the job is failing with the following error:",
        "Question_body":"<p><code>Invalid config parameters<\/code>\n<code>error[Errno 2] No such file or directory: '\/opt\/ml\/input\/config\/hyperparameters.json'<\/code><\/p>\n<p>How can I solve this issue? I have tried to use python sdk to upload the config.json file according to the client request but I am not sure how to achieve that since there's very less material online. Please help me with this issue.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1654064121797,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1654456534080,
        "Question_score_count":0.0,
        "Question_view_count":71.0,
        "Poster_created_time":1654063693230,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72457208",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"try creat job give correct input job fail follow error invalid config paramet error errno file directori opt input config hyperparamet json solv issu tri us python sdk upload config json file accord client request sure achiev materi onlin help issu",
        "Question_preprocessed_content":"try creat job give input job fail solv tri us python sdk upload file client request sure achiev materi onlin help",
        "Question_gpt_summary_original":"The user is facing an issue while creating a job in Azure ML. The job is failing with an error message \"Invalid config parameters\" and \"No such file or directory: '\/opt\/ml\/input\/config\/hyperparameters.json'\". The user has tried to upload the config.json file using Python SDK but is unsure how to achieve it as there is limited material available online. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face issu creat job job fail error messag invalid config paramet file directori opt input config hyperparamet json user tri upload config json file python sdk unsur achiev limit materi avail onlin user seek help resolv issu"
    },
    {
        "Question_title":"Serving models from mlflow registry to sagemaker",
        "Question_body":"<p>I have an mlflow server running locally and being exposed at port 80. I also have a model in the mlflow registry and I want to deploy it using the <code>mlflow sagemaker run-local<\/code> because after testing this locally, I am going to deploy everything to AWS and Sagemaker. My problem is that when I run:<\/p>\n<pre><code>export MODEL_PATH=models:\/churn-lgb-test\/2\nexport LOCAL_PORT=8000\nmlflow sagemaker run-local -m $MODEL_PATH -p $LOCAL_PORT -f python_function -i splicemachine\/mlflow-pyfunc:1.6.0\n<\/code><\/pre>\n<p>it starts the container and I immediately get this error:<\/p>\n<pre><code>2020-07-27 13:02:13 +0000] [827] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [828] [INFO] Worker exiting (pid: 828)\n[2020-07-27 13:02:13 +0000] [827] [INFO] Worker exiting (pid: 827)\n[2020-07-27 13:02:13 +0000] [829] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [829] [INFO] Worker exiting (pid: 829)\n[2020-07-27 13:02:13 +0000] [830] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [830] [INFO] Worker exiting (pid: 830)\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 209, in run\n    self.sleep()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 357, in sleep\n    ready = select.select([self.PIPE[0]], [], [], 1.0)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/bin\/gunicorn&quot;, line 8, in &lt;module&gt;\n    sys.exit(run())\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in run\n    WSGIApplication(&quot;%(prog)s [OPTIONS] [APP_MODULE]&quot;).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 228, in run\n    super().run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 72, in run\n    Arbiter(self).run()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 229, in run\n    self.halt(reason=inst.reason, exit_status=inst.exit_status)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 342, in halt\n    self.stop()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 393, in stop\n    time.sleep(0.1)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 242, in handle_chld\n    self.reap_workers()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 525, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: &lt;HaltServer 'Worker failed to boot.' 3&gt;\ncreating and activating custom environment\nGot sigterm signal, exiting.\n[2020-07-27 13:02:13 +0000] [831] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:13 +0000] [831] [INFO] Worker exiting (pid: 831)\n[2020-07-27 13:02:14 +0000] [833] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [833] [INFO] Worker exiting (pid: 833)\n[2020-07-27 13:02:14 +0000] [832] [ERROR] Exception in worker process\nTraceback (most recent call last):\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker\n    worker.init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/ggevent.py&quot;, line 162, in init_process\n    super().init_process()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 119, in init_process\n    self.load_wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 144, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 49, in load\n    return self.load_wsgiapp()\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 39, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 358, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/models\/container\/scoring_server\/wsgi.py&quot;, line 3, in &lt;module&gt;\n    app = scoring_server.init(pyfunc.load_model(&quot;\/opt\/ml\/model\/&quot;))\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/pyfunc\/__init__.py&quot;, line 292, in load_model\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 219, in _load_pyfunc\n    return _load_model_from_local_file(path)\n  File &quot;\/miniconda\/envs\/custom_env\/lib\/python3.7\/site-packages\/mlflow\/sklearn.py&quot;, line 206, in _load_model_from_local_file\n    with open(path, &quot;rb&quot;) as f:\nIsADirectoryError: [Errno 21] Is a directory: '\/opt\/ml\/model'\n[2020-07-27 13:02:14 +0000] [832] [INFO] Worker exiting (pid: 832)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1595856379060,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":429.0,
        "Poster_created_time":1591385263856,
        "Poster_location":null,
        "Poster_reputation_count":21.0,
        "Poster_view_count":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63116338",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"serv model registri server run local expos port model registri want deploi run local test local go deploi aw problem run export model path model churn lgb test export local port run local model path local port python function splicemachin pyfunc start contain immedi error error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid info worker exit pid error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid traceback recent file miniconda env custom env lib python site packag gunicorn arbit line run self sleep file miniconda env custom env lib python site packag gunicorn arbit line sleep readi select select self pipe file miniconda env custom env lib python site packag gunicorn arbit line handl chld self reap worker file miniconda env custom env lib python site packag gunicorn arbit line reap worker rais haltserv reason self worker boot error gunicorn error haltserv handl except except occur traceback recent file miniconda env custom env bin gunicorn line sy exit run file miniconda env custom env lib python site packag gunicorn app wsgiapp line run wsgiapplic prog option app modul run file miniconda env custom env lib python site packag gunicorn app base line run super run file miniconda env custom env lib python site packag gunicorn app base line run arbit self run file miniconda env custom env lib python site packag gunicorn arbit line run self halt reason inst reason exit statu inst exit statu file miniconda env custom env lib python site packag gunicorn arbit line halt self stop file miniconda env custom env lib python site packag gunicorn arbit line stop time sleep file miniconda env custom env lib python site packag gunicorn arbit line handl chld self reap worker file miniconda env custom env lib python site packag gunicorn arbit line reap worker rais haltserv reason self worker boot error gunicorn error haltserv creat activ custom environ got sigterm signal exit error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid error except worker process traceback recent file miniconda env custom env lib python site packag gunicorn arbit line spawn worker worker init process file miniconda env custom env lib python site packag gunicorn worker ggevent line init process super init process file miniconda env custom env lib python site packag gunicorn worker base line init process self load wsgi file miniconda env custom env lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file miniconda env custom env lib python site packag gunicorn app base line wsgi self callabl self load file miniconda env custom env lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file miniconda env custom env lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file miniconda env custom env lib python site packag gunicorn util line import app mod importlib import modul modul file miniconda env custom env lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file miniconda env custom env lib python site packag model contain score server wsgi line app score server init pyfunc load model opt model file miniconda env custom env lib python site packag pyfunc init line load model return importlib import modul conf main load pyfunc data path file miniconda env custom env lib python site packag sklearn line load pyfunc return load model local file path file miniconda env custom env lib python site packag sklearn line load model local file open path isadirectoryerror errno directori opt model info worker exit pid",
        "Question_preprocessed_content":"serv model registri server expos port model registri want deploi test go deploi aw problem run start contain",
        "Question_gpt_summary_original":"the user encountered a challenge of serving models from a registry to , resulting in an \"isadirectoryerror\" when attempting to run the container.",
        "Question_gpt_summary":"user encount challeng serv model registri result isadirectoryerror attempt run contain"
    },
    {
        "Question_title":"Deploy model on AKS ModuleNotFoundError: No module named 'main'",
        "Question_body":"<p>When trying to deploy a model on AKS an error occurs.  <br \/>\nThe traceback is the following:  <br \/>\nException in worker process  <br \/>\nTraceback (most recent call last):  <br \/>\nFile &quot;\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 583, in spawn_worker  <br \/>\nworker.init_process()  <br \/>\nFile &quot;\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 129, in init_process  <br \/>\nself.load_wsgi()  <br \/>\nFile &quot;\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 138, in load_wsgi  <br \/>\nself.wsgi = self.app.wsgi()  <br \/>\nFile &quot;\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi  <br \/>\nself.callable = self.load()  <br \/>\nFile &quot;\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 52, in load  <br \/>\nreturn self.load_wsgiapp()  <br \/>\nFile &quot;\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 41, in load_wsgiapp  <br \/>\nreturn util.import_app(self.app_uri)  <br \/>\nFile &quot;\/azureml-envs\/azureml_828a57093157f4d4d37377511c7dd8a0\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 350, in import_app  <br \/>\n<strong>import<\/strong>(module)  <br \/>\nFile &quot;\/var\/azureml-server\/entry.py&quot;, line 1, in &lt;module&gt;  <br \/>\nimport create_app  <br \/>\nFile &quot;\/var\/azureml-server\/create_app.py&quot;, line 4, in &lt;module&gt;  <br \/>\nfrom routes_common import main  <br \/>\nFile &quot;\/var\/azureml-server\/routes_common.py&quot;, line 32, in &lt;module&gt;  <br \/>\nfrom aml_blueprint import AMLBlueprint  <br \/>\nFile &quot;\/var\/azureml-server\/aml_blueprint.py&quot;, line 29, in &lt;module&gt;  <br \/>\nimport main  <br \/>\nModuleNotFoundError: No module named 'main'<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_created_time":1624613802117,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/451760\/deploy-model-on-aks-modulenotfounderror-no-module",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"deploi model ak modulenotfounderror modul name main try deploi model ak error occur traceback follow except worker process traceback recent file env afddcdda lib python site packag gunicorn arbit line spawn worker worker init process file env afddcdda lib python site packag gunicorn worker base line init process self load wsgi file env afddcdda lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file env afddcdda lib python site packag gunicorn app base line wsgi self callabl self load file env afddcdda lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file env afddcdda lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file env afddcdda lib python site packag gunicorn util line import app import modul file var server entri line import creat app file var server creat app line rout common import main file var server rout common line aml blueprint import amlblueprint file var server aml blueprint line import main modulenotfounderror modul name main",
        "Question_preprocessed_content":"deploi model ak modul name main try deploi model ak traceback except worker traceback file line file line file line file line wsgi file line load return file line return file line import file line import file line import main file line import amlblueprint file line import main modul name main",
        "Question_gpt_summary_original":"The user encountered an error while trying to deploy a model on AKS. The error message indicates that there is no module named 'main', which is required for the deployment process.",
        "Question_gpt_summary":"user encount error try deploi model ak error messag indic modul name main requir deploy process"
    },
    {
        "Question_title":"AzureML Classic Workspace New Webservices : New-AzMlWebService Powershell module",
        "Question_body":"<p>All,    <br \/>\nDoes anyone know how to use this powershell module to deploy a new webservice in the azure ml classic studio? I am able to deploy an existing experiment to a new webservice from the portal. I am trying to achieve the same using powershell.    <br \/>\nTried <a href=\"https:\/\/learn.microsoft.com\/en-us\/powershell\/module\/az.machinelearning\/new-azmlwebservice?view=azps-5.1.0\">this<\/a> cmdlet but couldnt succeed.    <br \/>\n<img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42969-image.png?platform=QnA\" alt=\"42969-image.png\" \/>    <\/p>\n<p>All I want is to deploy an experiment to a &quot;new&quot; webservice and choose a webservice plan using powershell.(like the following image action in the portal.    <\/p>\n<p><img src=\"https:\/\/learn-attachment.microsoft.com\/api\/attachments\/42995-image.png?platform=QnA\" alt=\"42995-image.png\" \/>    <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1606389917980,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/176959\/azureml-classic-workspace-new-webservices-new-azml",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"classic workspac new webservic new azmlwebservic powershel modul know us powershel modul deploi new webservic classic studio abl deploi exist experi new webservic portal try achiev powershel tri cmdlet succe want deploi experi new webservic choos webservic plan powershel like follow imag action portal",
        "Question_preprocessed_content":"workspac new webservic modul know us modul deploi new webservic studio abl deploi exist experi new webservic portal try achiev tri cmdlet want deploi experi new webservic webservic plan like imag action portal",
        "Question_gpt_summary_original":"The user is seeking help in using the New-AzMlWebService Powershell module to deploy a new webservice in Azure ML Classic Studio. They have tried using a cmdlet but were unsuccessful and are looking for guidance on how to choose a webservice plan using Powershell.",
        "Question_gpt_summary":"user seek help new azmlwebservic powershel modul deploi new webservic classic studio tri cmdlet unsuccess look guidanc choos webservic plan powershel"
    },
    {
        "Question_title":"Anyone know why sagemaker would not be able to reach the files.fast.ai server",
        "Question_body":"<p>I have setup fast.ai sagemaker jupyter notebooks. The notebook environment works fine however its not able to download files from files.fast.ai server. Its able to reach fast.ai server. (attach pings from terminal)<\/p>\n\n<p>Anyone have a clue? <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ltOdy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ltOdy.png\" alt=\"not able to reach files.fast.ai server\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ER3jn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ER3jn.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1585762381243,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":49.0,
        "Poster_created_time":1310476336900,
        "Poster_location":"Middletown, CT, United States",
        "Poster_reputation_count":2262.0,
        "Poster_view_count":83.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60976891",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"know abl reach file fast server setup fast jupyt notebook notebook environ work fine abl download file file fast server abl reach fast server attach ping termin clue",
        "Question_preprocessed_content":"know abl reach server setup jupyt environ work fine abl download file server abl reach server clue",
        "Question_gpt_summary_original":"The user is facing an issue with their fast.ai sagemaker jupyter notebooks as it is unable to download files from files.fast.ai server, although it is able to reach fast.ai server. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face issu fast jupyt notebook unabl download file file fast server abl reach fast server user seek help resolv issu"
    },
    {
        "Question_title":"Xgboost Amazon Sagemaker predict_proba",
        "Question_body":"<p>I have trained an Xgboost multi-class classification algo for 5 classes using the Sagemaker framework. I then saved the model on S3. Now, when I load it, I only have the option of predict, that will return me the classes and not the probabilities of each class.<\/p>\n\n<pre><code>pred = xgb_uploaded_model.predict(new_data)\n<\/code><\/pre>\n\n<p>The uploaded model has no option of predict_proba, that would have came in handy. \nAny ideas on hot to get probabilities from this a model saved like this? \nP.S. I don't want to use Endpoints.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_created_time":1528383788867,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":6.0,
        "Question_view_count":869.0,
        "Poster_created_time":1432680790120,
        "Poster_location":null,
        "Poster_reputation_count":455.0,
        "Poster_view_count":58.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50744503",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"xgboost predict proba train xgboost multi class classif algo class framework save model load option predict return class probabl class pred xgb upload model predict new data upload model option predict proba came handi idea hot probabl model save like want us endpoint",
        "Question_preprocessed_content":" train algo framework save model load option predict return probabl upload model option came handi idea hot probabl model save like want us endpoint",
        "Question_gpt_summary_original":"The user has trained an Xgboost multi-class classification algorithm for 5 classes using the Sagemaker framework and saved the model on S3. However, when the user loads the model, they only have the option of predict, which returns the classes and not the probabilities of each class. The uploaded model has no option of predict_proba, and the user is seeking ideas on how to get probabilities from this model without using Endpoints.",
        "Question_gpt_summary":"user train xgboost multi class classif algorithm class framework save model user load model option predict return class probabl class upload model option predict proba user seek idea probabl model endpoint"
    },
    {
        "Question_title":"How to return name of AWS Sagemaker notebook instance within python script?",
        "Question_body":"<p>Bit of an edge case ask, basically I'm trying to write a general purpose script that can be set up as a cronjob on AWS Sagemaker that will notify users if an instance is still active. <strong>If possible, I'd like to be able to call name of the AWS Sagemaker instance (different from the name of the active jupyter notebook) within the cronjob.<\/strong><\/p>\n\n<p>Haven't been able to find anything in the boto3 or sagemaker documentation specific to this. I know that the instance name is contained within the URL path, but similarly haven't found a way to reference back to this. I'm expecting this isn't an intended functionality so will probably just go ahead with setting values manually in a config file, but if anyone has any creative solutions I'd appreciate the input!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1580241080940,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1580320624603,
        "Question_score_count":0.0,
        "Question_view_count":926.0,
        "Poster_created_time":1456986606312,
        "Poster_location":null,
        "Poster_reputation_count":757.0,
        "Poster_view_count":80.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59956042",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"return notebook instanc python script bit edg case ask basic try write gener purpos script set cronjob notifi user instanc activ possibl like abl instanc differ activ jupyt notebook cronjob haven abl boto document specif know instanc contain url path similarli haven wai refer expect isn intend function probabl ahead set valu manual config file creativ solut appreci input",
        "Question_preprocessed_content":"return instanc python script bit edg case ask try write gener purpos script set cronjob notifi user instanc activ like abl instanc cronjob haven abl boto document specif know instanc contain url path similarli haven wai refer expect isn intend function probabl ahead valu config file creativ solut input",
        "Question_gpt_summary_original":"The user is trying to write a script that can notify users if an AWS Sagemaker instance is still active. They are looking for a way to call the name of the instance within the script, but have not found any specific documentation on how to do so. They are considering setting values manually in a config file as a workaround.",
        "Question_gpt_summary":"user try write script notifi user instanc activ look wai instanc script specif document consid set valu manual config file workaround"
    },
    {
        "Question_title":"Docker image not found during local deployment (\"no such image\")",
        "Question_body":"<p>I want to test my service and to do so I deploy it locally and until now everything worked fine. However, for some unrelated reason I was forced to delete all my docker images and since then I'm unable to deploy the service locally. Upon deployment I receive the following error:<\/p>\n\n<blockquote>\n  <p>404 Client Error: Not Found for url:\n  http+docker:\/\/localnpipe\/v1.39\/images\/471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\/json<\/p>\n<\/blockquote>\n\n<p>And also:<\/p>\n\n<blockquote>\n  <p>ImageNotFound: 404 Client Error: Not Found (\"no such image: \n  471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814: No\n  such image:\n  sha256:471b7320d98e95ad137228efff17267535936b632a749f817dbee3e9d03cd814\")<\/p>\n<\/blockquote>\n\n<p>What I did to deploy the model:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.webservice import LocalWebservice\nfrom azureml.core.model import InferenceConfig\n\nws = Workspace.from_config(\"config.json\")\n\ndeployment_config = LocalWebservice.deploy_configuration(port=8890)\n\ninference_config = InferenceConfig(runtime= \"python\", \n                               entry_script=\"score.py\",\n                               conda_file=\"env.yml\")\n\nmodel_box = Model(ws, \"box\")\nmodel_view = Model(ws, \"view_crop\")\nmodel_damage = Model(ws, \"damage_crop\")\n\nservice = Model.deploy(ws, \"test-service\", [model_box, model_view, model_damage], inference_config, deployment_config)\n\nservice.wait_for_deployment(True)\n<\/code><\/pre>\n\n<p>I understand why there is no image present, but I would expect that it is downloaded in that case.<\/p>\n\n<p>Is there a way to force the build process to re-download the docker base image?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1559042269360,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":3685.0,
        "Poster_created_time":1408356046196,
        "Poster_location":"Bonn, Deutschland",
        "Poster_reputation_count":594.0,
        "Poster_view_count":55.0,
        "Answer_body":"<p>I just found the problem and corresponding solution:<\/p>\n\n<p>I deleted all images but there where still some containers based on deleted images present. Deleting the corresponding container had the desired effect that the docker image is reloaded from the server.<\/p>\n\n<p>You can delete all containers with <code>docker kill $(docker ps -q)<\/code>.<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1559043123528,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1408356046196,
        "Answerer_location":"Bonn, Deutschland",
        "Answerer_reputation_count":594.0,
        "Answerer_view_count":55.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56341012",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":1.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"docker imag local deploy imag want test servic deploi local work fine unrel reason forc delet docker imag unabl deploi servic local deploy receiv follow error client error url http docker localnpip imag bdeadefffbafdbeeedcd json imagenotfound client error imag bdeadefffbafdbeeedcd imag sha bdeadefffbafdbeeedcd deploi model core model import model core import workspac core webservic import localwebservic core model import inferenceconfig workspac config config json deploy config localwebservic deploi configur port infer config inferenceconfig runtim python entri script score conda file env yml model box model box model view model view crop model damag model damag crop servic model deploi test servic model box model view model damag infer config deploy config servic wait deploy true understand imag present expect download case wai forc build process download docker base imag thank advanc",
        "Question_preprocessed_content":"docker imag local deploy want test servic deploi work fine unrel reason forc delet docker imag unabl deploi servic deploy receiv client url imagenotfound client deploi model understand imag present expect download case wai forc build docker base imag thank advanc",
        "Question_gpt_summary_original":"The user is unable to deploy their service locally after deleting all their docker images. They receive a \"no such image\" error and a 404 client error when attempting to deploy the model. The user is looking for a way to force the build process to re-download the docker base image.",
        "Question_gpt_summary":"user unabl deploi servic local delet docker imag receiv imag error client error attempt deploi model user look wai forc build process download docker base imag"
    },
    {
        "Question_title":"Issues accessing a FileDataset created from HTTP URIs in a PythonScriptStep",
        "Question_body":"<p>I\u2019m having some issues trying to access a FileDataset created from two http URIs in an Azure ML Pipeline PythonScriptStep.<\/p>\n<p>In the step, I\u2019m only getting a single file named <code>['https%3A\u2019]<\/code> when doing an <code>os.listdir()<\/code> on my mount point. I would have expected two files, with their actual names instead. This happens both when sending the dataset <code>as_upload<\/code> and <code>as_mount<\/code>. Even happens when I send the dataset reference to the pipeline step and mount it directly from the step.<\/p>\n<p>The dataset is registered in a notebook, the same notebook that creates and invokes the pipeline, as seen below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>tempFileData = Dataset.File.from_files(\n        ['https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg',\n        'https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg'])\ntempFileData.register(ws, name='FileData', create_new_version=True)\n\n#...\n\nread_datasets_step = PythonScriptStep(\n    name='The Dataset Reader',\n    script_name='read-datasets.py',\n    inputs=[fileData.as_named_input('Files'), fileData.as_named_input('Files_mount').as_mount(), fileData.as_named_input('Files_download').as_download()],\n    compute_target=compute_target,\n    source_directory='.\/dataset-reader',\n    allow_reuse=False,\n)\n\n<\/code><\/pre>\n<p>The <code>FileDataset<\/code> seems to be registered properly, if I examine it within the notebook I get the following result:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [\n    &quot;https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot;,\n    &quot;https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;\n  ],\n  &quot;definition&quot;: [\n    &quot;GetFiles&quot;\n  ],\n  &quot;registration&quot;: {\n    &quot;id&quot;: &quot;...&quot;,\n    &quot;name&quot;: &quot;FileData&quot;,\n    &quot;version&quot;: 4,\n    &quot;workspace&quot;: &quot;Workspace.create(...)&quot;\n  }\n}\n<\/code><\/pre>\n<p>For reference, the machine running the notebook is using AML SDK v1.24, whereas the node running the pipeline steps is running v1.25.<\/p>\n<p>Has anybody encountered anything like this? Is there a way to make it work?<\/p>\n<p>Note that I'm specifically looking at file datasets created from web uris, and not necessarily interested in getting a <code>FileDataset<\/code> to work with blob storage or similar.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1618832324140,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1618849094430,
        "Question_score_count":1.0,
        "Question_view_count":91.0,
        "Poster_created_time":1250158552416,
        "Poster_location":"Romania",
        "Poster_reputation_count":7916.0,
        "Poster_view_count":801.0,
        "Answer_body":"<p>The files should've been mounted at path &quot;https%3A\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot; and &quot;https%3A\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;.<\/p>\n<p>We retain the directory structure following the url structure to avoid potential conflicts.<\/p>",
        "Answer_comment_count":0.0,
        "Question_closed_time":1618855169223,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1577353307072,
        "Answerer_location":null,
        "Answerer_reputation_count":491.0,
        "Answerer_view_count":49.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67161293",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"issu access filedataset creat http uri pythonscriptstep have issu try access filedataset creat http uri pipelin pythonscriptstep step get singl file name http listdir mount point expect file actual name instead happen send dataset upload mount happen send dataset refer pipelin step mount directli step dataset regist notebook notebook creat invok pipelin seen tempfiledata dataset file file http vladiliescu net imag deploi model azur pipelin jpg http vladiliescu net imag revers engin autom jpg tempfiledata regist filedata creat new version true read dataset step pythonscriptstep dataset reader script read dataset input filedata name input file filedata name input file mount mount filedata name input file download download comput target comput target sourc directori dataset reader allow reus fals filedataset regist properli examin notebook follow result sourc http vladiliescu net imag deploi model azur pipelin jpg http vladiliescu net imag revers engin autom jpg definit getfil registr filedata version workspac workspac creat refer machin run notebook aml sdk node run pipelin step run anybodi encount like wai work note specif look file dataset creat web uri necessarili interest get filedataset work blob storag similar",
        "Question_preprocessed_content":"filedataset creat uri pythonscriptstep have try filedataset creat uri pipelin pythonscriptstep step singl file name mount point expect file actual name instead send dataset send dataset refer pipelin step mount directli step dataset regist creat invok pipelin regist properli examin result refer machin aml sdk node pipelin step anybodi encount like wai work note file dataset creat web uri interest work blob storag similar",
        "Question_gpt_summary_original":"The user is facing issues while accessing a FileDataset created from two HTTP URIs in an Azure ML Pipeline PythonScriptStep. The user is only getting a single file named ['https%3A'] when doing an os.listdir() on the mount point, instead of two files with their actual names. This happens even when sending the dataset as_upload and as_mount. The FileDataset is registered properly, but the issue persists. The user is specifically looking at file datasets created from web URIs and not interested in getting a FileDataset to work with blob storage or similar.",
        "Question_gpt_summary":"user face issu access filedataset creat http uri pipelin pythonscriptstep user get singl file name http listdir mount point instead file actual name happen send dataset upload mount filedataset regist properli issu persist user specif look file dataset creat web uri interest get filedataset work blob storag similar"
    },
    {
        "Question_title":"MLFlow Projects throw JSONDecode error when run",
        "Question_body":"<p>I'm trying to get MLFlow Projects to run using the MLFlow CLI and its following the tutorial leads to an error.  For any project I try to run from the CLI, I get the following error<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"\/home\/rbc\/.local\/bin\/mlflow\", line 11, in &lt;module&gt;\n    sys.exit(cli())\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/click\/core.py\", line 764, in __call__\n    return self.main(*args, **kwargs)\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/click\/core.py\", line 717, in main\n    rv = self.invoke(ctx)\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/click\/core.py\", line 1137, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/click\/core.py\", line 956, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/click\/core.py\", line 555, in invoke\n    return callback(*args, **kwargs)\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/mlflow\/cli.py\", line 139, in run\n    run_id=run_id,\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/mlflow\/projects\/__init__.py\", line 230, in run\n    storage_dir=storage_dir, block=block, run_id=run_id)\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/mlflow\/projects\/__init__.py\", line 88, in _run\n    active_run = _create_run(uri, experiment_id, work_dir, entry_point)\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/mlflow\/projects\/__init__.py\", line 579, in _create_run\n    active_run = tracking.MlflowClient().create_run(experiment_id=experiment_id, tags=tags)\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/mlflow\/tracking\/client.py\", line 101, in create_run\n    source_version=source_version\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/mlflow\/store\/rest_store.py\", line 156, in create_run\n    response_proto = self._call_endpoint(CreateRun, req_body)\n  File \"\/home\/rbc\/.local\/lib\/python3.6\/site-packages\/mlflow\/store\/rest_store.py\", line 66, in _call_endpoint\n    js_dict = json.loads(response.text)\n  File \"\/usr\/lib\/python3.6\/json\/__init__.py\", line 354, in loads\n    return _default_decoder.decode(s)\n  File \"\/usr\/lib\/python3.6\/json\/decoder.py\", line 339, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"\/usr\/lib\/python3.6\/json\/decoder.py\", line 357, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n<\/code><\/pre>\n\n<p>Here's an example of the type of command I'm using to start the run, which comes directly from the tutorial <\/p>\n\n<pre><code>mlflow run https:\/\/github.com\/mlflow\/mlflow#examples\/sklearn_elasticnet_wine -m databricks -c cluster-spec.json --experiment-id 72647065958042 -P alpha=2.0 -P l1_ratio=0.5\n<\/code><\/pre>\n\n<p>I've traced the error to something involving MLFLow returning empty when it tries to start a run but I can successfully run MLFlow experiments using the Databricks environment I'm connecting to so I'm not sure where the problem is, I'm running MLFlow 0.9.1 on Ubuntu 18.04<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_created_time":1557754667253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":3.0,
        "Question_view_count":386.0,
        "Poster_created_time":1509471438900,
        "Poster_location":"Orlando, FL, United States",
        "Poster_reputation_count":31.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56113569",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"project throw jsondecod error run try project run cli follow tutori lead error project try run cli follow error traceback recent file home rbc local bin line sy exit cli file home rbc local lib python site packag click core line return self main arg kwarg file home rbc local lib python site packag click core line main self invok ctx file home rbc local lib python site packag click core line invok return process result sub ctx command invok sub ctx file home rbc local lib python site packag click core line invok return ctx invok self callback ctx param file home rbc local lib python site packag click core line invok return callback arg kwarg file home rbc local lib python site packag cli line run run run file home rbc local lib python site packag project init line run storag dir storag dir block block run run file home rbc local lib python site packag project init line run activ run creat run uri experi work dir entri point file home rbc local lib python site packag project init line creat run activ run track client creat run experi experi tag tag file home rbc local lib python site packag track client line creat run sourc version sourc version file home rbc local lib python site packag store rest store line creat run respons proto self endpoint createrun req bodi file home rbc local lib python site packag store rest store line endpoint dict json load respons text file usr lib python json init line load return default decod decod file usr lib python json decod line decod obj end self raw decod idx end file usr lib python json decod line raw decod rais jsondecodeerror expect valu err valu json decod jsondecodeerror expect valu line column char exampl type command start run come directli tutori run http github com exampl sklearn elasticnet wine databrick cluster spec json experi alpha ratio trace error involv return tri start run successfulli run experi databrick environ connect sure problem run ubuntu",
        "Question_preprocessed_content":"project throw jsondecod run try project run cli tutori lead project try run cli exampl type start run come directli tutori trace involv return tri start run run experi databrick environ sure problem ubuntu",
        "Question_gpt_summary_original":"The user is encountering a JSONDecode error when trying to run MLFlow Projects using the MLFlow CLI. The error occurs when MLFlow returns empty while trying to start a run. The user has traced the error to MLFlow and is unsure of the problem. The user is running MLFlow 0.9.1 on Ubuntu 18.04 and can successfully run MLFlow experiments using the Databricks environment.",
        "Question_gpt_summary":"user encount jsondecod error try run project cli error occur return try start run user trace error unsur problem user run ubuntu successfulli run experi databrick environ"
    },
    {
        "Question_title":"mlflow scaling issue",
        "Question_body":"Hi all,\nwhen running below script, we notice a visible performance degradation as number of experiments increases.\nimport mlflow\n  \nfor i in range(5000):\n\n    mlflow.set_experiment('exp%d' % i)\n\n    for j in range(5):\n\n        with mlflow.start_run() as run:\n\n            mlflow.log_param('n', '%d-%d' % (i, j))\n            mlflow.log_metric('fscore', 0.9242)\n            mlflow.set_tag('some tag', '%d-%d' % (i, j))\n            with open('test.json', 'w') as f:\n                f.write('{ \"i\": %d, \"j\": %d }' % (i, j))\n            mlflow.log_artifact('test.json')\n            print(mlflow.active_run())\n\nabove script creates 5000 experiments, with 5 dummy runs in each experiment.  After about 18 hours, it has only created 2500 experiments.  At this point, it is taking about a minute to create a new experiment, and 5 seconds to create a run.\n\nwhat is the bottleneck. \nThanks in advance ..",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1549379818000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":32.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/MfgmfilrJsI",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"scale issu run script notic visibl perform degrad number experi increas import rang set experi exp rang start run run log param log metric fscore set tag tag open test json write log artifact test json print activ run script creat experi dummi run experi hour creat experi point take minut creat new experi second creat run bottleneck thank advanc",
        "Question_preprocessed_content":"scale script notic visibl perform degrad number experi increas import rang rang run tag script creat experi run experi hour creat experi point take minut creat new experi second creat run thank advanc",
        "Question_gpt_summary_original":"The user is facing a performance degradation issue while running a script that creates 5000 experiments with 5 dummy runs in each experiment using mlflow. After 18 hours, only 2500 experiments have been created, and it takes about a minute to create a new experiment and 5 seconds to create a run. The user is seeking help to identify the bottleneck causing this issue.",
        "Question_gpt_summary":"user face perform degrad issu run script creat experi dummi run experi hour experi creat take minut creat new experi second creat run user seek help identifi bottleneck caus issu"
    },
    {
        "Question_title":"Does wandb have a limit on how long it can be run and deadlocks?",
        "Question_body":"<p>I find that my scripts seem to halt on their own but they seem to deadlock or don\u2019t throw an error e.g. I was running a training script on my laptop but cuz it was on debug mode I was able to pause and it seemed to be stuck with some multiprocessing things and it seemed it was related to wandb\u2026<\/p>\n<pre><code class=\"lang-auto\">epoch_num=95: train_loss=1.861583555999555, train_acc=0.4987407624721527\nepoch_num=95: val_loss=tensor(7.3504), val_acc=tensor(0.)\n 16% (96 of 600) | | Elapsed Time: 7:47:13 | ETA:  1 day, 16:52:54 | 175.7 s\/it\nepoch_num=96: train_loss=1.8501708821246499, train_acc=0.5018503069877625\nepoch_num=96: val_loss=tensor(6.9187), val_acc=tensor(0.)\nException ignored in: &lt;Finalize object, dead&gt;\nTraceback (most recent call last):\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/multiprocessing\/util.py\", line 224, in __call__\n    res = self._callback(*self._args, **self._kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/multiprocessing\/synchronize.py\", line 88, in _cleanup\n    unregister(name, \"semaphore\")\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/multiprocessing\/resource_tracker.py\", line 151, in unregister\n    self._send('UNREGISTER', name, rtype)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/multiprocessing\/resource_tracker.py\", line 154, in _send\n    self.ensure_running()\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/multiprocessing\/resource_tracker.py\", line 75, in ensure_running\n    with self._lock:\nKeyboardInterrupt: \n<\/code><\/pre>\n<p>does wandb have some deadlock bug if it is ran for too long for a reallllyyyyyy long experiment?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7d5f38d6a7cf69ebb63dc36b3191889903bf273a.jpeg\" data-download-href=\"\/uploads\/short-url\/hT5H9DiWLr9feqG3ZUqUHMsO3lE.jpeg?dl=1\" title=\"Screen Shot 2022-02-15 at 1.51.43 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7d5f38d6a7cf69ebb63dc36b3191889903bf273a_2_387x500.jpeg\" alt=\"Screen Shot 2022-02-15 at 1.51.43 PM\" data-base62-sha1=\"hT5H9DiWLr9feqG3ZUqUHMsO3lE\" width=\"387\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7d5f38d6a7cf69ebb63dc36b3191889903bf273a_2_387x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7d5f38d6a7cf69ebb63dc36b3191889903bf273a_2_580x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7d5f38d6a7cf69ebb63dc36b3191889903bf273a_2_774x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7d5f38d6a7cf69ebb63dc36b3191889903bf273a_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-02-15 at 1.51.43 PM<\/span><span class=\"informations\">1032\u00d71333 251 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_created_time":1644954641495,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":154.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/does-wandb-have-a-limit-on-how-long-it-can-be-run-and-deadlocks\/1933",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"limit long run deadlock script halt deadlock dont throw error run train script laptop cuz debug mode abl paus stuck multiprocess thing relat epoch num train loss train acc epoch num val loss tensor val acc tensor elaps time eta dai epoch num train loss train acc epoch num val loss tensor val acc tensor except ignor traceback recent file user brandomiranda opt anaconda env meta learn lib python multiprocess util line re self callback self arg self kwarg file user brandomiranda opt anaconda env meta learn lib python multiprocess synchron line cleanup unregist semaphor file user brandomiranda opt anaconda env meta learn lib python multiprocess resourc tracker line unregist self send unregist rtype file user brandomiranda opt anaconda env meta learn lib python multiprocess resourc tracker line send self ensur run file user brandomiranda opt anaconda env meta learn lib python multiprocess resourc tracker line ensur run self lock keyboardinterrupt deadlock bug ran long reallllyyyyyi long experi screen shot",
        "Question_preprocessed_content":"limit long run deadlock script halt deadlock dont throw train script laptop cuz debug mode abl paus stuck thing relat deadlock bug ran long long experi shot",
        "Question_gpt_summary_original":"The user is experiencing issues with their training script halting without throwing an error, and suspects that it may be related to wandb. They are wondering if wandb has a limit on how long it can be run and if it has a deadlock bug for long experiments.",
        "Question_gpt_summary":"user experienc issu train script halt throw error suspect relat wonder limit long run deadlock bug long experi"
    },
    {
        "Question_title":"in Azure Machine Learning service, how to update python pickle file 's ML model parameter?",
        "Question_body":"<p>after using Azure Auto ML or Designer, model creates Pickle file.  <\/p>\n<p>i tried to develop model in python pickle file by tunning some parameter by set_params method  <\/p>\n<p>as you know this pickle file has 'Pipeline', 'y_transformer', 'y_transformer_name' key.  <\/p>\n<p>and Pipeline has parameter setting  <\/p>\n<p>but as i wrote,  <\/p>\n<p>a.pipeline.set_params (memory = 'n')  <\/p>\n<p>this code is work.  <\/p>\n<p>memory parameter is changed.  <\/p>\n<p>but  <\/p>\n<p>a.pipeline.set_params (XGBoostClassifier__base_score = 0.6)  <\/p>\n<p>this code is not work.  <\/p>\n<p>please let me know how to change model's hyperparameter<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_created_time":1592180368837,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/35933\/in-azure-machine-learning-service-how-to-update-py",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"servic updat python pickl file model paramet azur auto design model creat pickl file tri develop model python pickl file tun paramet set param method know pickl file pipelin transform transform kei pipelin paramet set wrote pipelin set param memori code work memori paramet chang pipelin set param xgboostclassifi base score code work let know chang model hyperparamet",
        "Question_preprocessed_content":"servic updat python pickl file model paramet azur auto design model creat pickl file tri develop model python pickl file paramet method know pickl file pipelin kei pipelin paramet wrote code work memori paramet chang code work let know chang model hyperparamet",
        "Question_gpt_summary_original":"The user is trying to update the hyperparameters of a Python pickle file containing a machine learning model created using Azure Auto ML or Designer. The user was able to change the 'memory' parameter using the 'set_params' method, but is unable to change the 'XGBoostClassifier__base_score' parameter. The user is seeking guidance on how to change the model's hyperparameters.",
        "Question_gpt_summary":"user try updat hyperparamet python pickl file contain machin learn model creat azur auto design user abl chang memori paramet set param method unabl chang xgboostclassifi base score paramet user seek guidanc chang model hyperparamet"
    },
    {
        "Question_title":"AWS Sagemaker using Batch Transfrom on AWS?",
        "Question_body":"<p>I have a Sagemaker model trained and deployed and looking to run a Batch Transform on multiple files. \nI have a lambda function configured to run when a new file is uploaded to S3.\nCurrently, I have only seen ways to use the Invoke Endpoint function on lambda\ni.e<\/p>\n\n<pre><code>runtime= boto3.client('runtime.sagemaker')\nmessage = [\"On Wed Sep PDT Brent Welch said Hacksaw said W\"]\nresponse = runtime.invoke_endpoint(EndpointName=\"sagemaker-scikit-learn-2020-02-05-13-44-45-011\",\n                                       ContentType='text\/plain',\n                                       Body=message)\njson.loads(response['Body'].read())\n<\/code><\/pre>\n\n<p>However, i have multiple files that need to processed by the Sagemaker model.<\/p>\n\n<p>I have code to create a transformer and run batch transform on multiple files<\/p>\n\n<pre><code>import sagemaker\ntraining_job = sagemaker.estimator.Estimator.attach(\"{model_name\")\ntransformer = training_job.transformer(instance_count=1,instance_type='ml.m4.xlarge',strategy='MultiRecord',assemble_with='Line')\n\n#batch_input_s3\ntransformer.transform(batch_input_s3)\nprint('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\ntransformer.wait()\n<\/code><\/pre>\n\n<p>however, im unable to use this transformer code in a lambda since it requires the sagemaker library, which is over the size limit for zip file is 50mb<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_created_time":1580929631577,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":931.0,
        "Poster_created_time":1285379271580,
        "Poster_location":null,
        "Poster_reputation_count":625.0,
        "Poster_view_count":110.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60082556",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"batch transfrom aw model train deploi look run batch transform multipl file lambda function configur run new file upload current seen wai us invok endpoint function lambda runtim boto client runtim messag wed sep pdt brent welch said hacksaw said respons runtim invok endpoint endpointnam scikit learn contenttyp text plain bodi messag json load respons bodi read multipl file need process model code creat transform run batch transform multipl file import train job estim estim attach model transform train job transform instanc count instanc type xlarg strategi multirecord assembl line batch input transform transform batch input print wait transform job transform latest transform job job transform wait unabl us transform code lambda requir librari size limit zip file",
        "Question_preprocessed_content":"batch transfrom aw model train deploi run batch transform multipl file lambda function configur run new file upload wai us invok endpoint function lambda multipl file model code creat transform run batch transform multipl file unabl us transform code lambda requir librari size limit zip file",
        "Question_gpt_summary_original":"The user has a Sagemaker model trained and deployed and wants to run a Batch Transform on multiple files using a lambda function. They have code to create a transformer and run batch transform on multiple files, but they are unable to use this transformer code in a lambda since it requires the Sagemaker library, which is over the size limit for a zip file of 50mb.",
        "Question_gpt_summary":"user model train deploi want run batch transform multipl file lambda function code creat transform run batch transform multipl file unabl us transform code lambda requir librari size limit zip file"
    },
    {
        "Question_title":"Trying to invoke an AWS sagemaker endpoint for a locally trained Object detection model (with .pb file). Facing this issue while invoking",
        "Question_body":"<p>I have created a multi model endpoint and facing issue while invoking the object detection model alone. The OD model has only .pb file and no variables folder in the saved model dir. While trying to invoke the endpoint with a sample image, I am getting this error:\n<a href=\"https:\/\/i.stack.imgur.com\/H4I3v.png\" rel=\"nofollow noreferrer\">Error in Cloudwatch:<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1631791432093,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":32.0,
        "Poster_created_time":1605887877220,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69207550",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"try invok endpoint local train object detect model file face issu invok creat multi model endpoint face issu invok object detect model model file variabl folder save model dir try invok endpoint sampl imag get error error cloudwatch",
        "Question_preprocessed_content":"try invok endpoint train object detect model face invok creat multi model endpoint face invok object detect model model file variabl folder save model dir try invok endpoint sampl imag cloudwatch",
        "Question_gpt_summary_original":"The user is facing an issue while trying to invoke an AWS Sagemaker endpoint for a locally trained object detection model with only a .pb file and no variables folder in the saved model directory. The user is receiving an error while trying to invoke the endpoint with a sample image.",
        "Question_gpt_summary":"user face issu try invok endpoint local train object detect model file variabl folder save model directori user receiv error try invok endpoint sampl imag"
    },
    {
        "Question_title":"ML.NET Detect anomalies with gaps\/missings in data",
        "Question_body":"<p>I have done some tests with <a href=\"https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/microsoft.ml.timeseriescatalog.detectiidspike?view=ml-dotnet\" rel=\"nofollow noreferrer\">DetectSpike<\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/microsoft.ml.timeseriescatalog.detectanomalybysrcnn?view=ml-dotnet\" rel=\"nofollow noreferrer\">DetectAnomaly<\/a> but despite differences and whole pipeline building - both methods require input of 1-dimensional float[]<\/p>\n<p>Its fine to find anomalies and spikes but I was asked to find anomalies &quot;normal&quot;, and anomalies with gaps. (this can be one scenario no need to separate this by kind)<\/p>\n<p>Example 1: WebService receives about 30-100 requests\/min, then theres break\/gap and no requests for few minutes.<\/p>\n<p>Example 2: I am receiving one bill invoice with random value each month, and theres two months without invoices.\n<a href=\"https:\/\/i.stack.imgur.com\/8DJ8n.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8DJ8n.png\" alt=\"enter image description here\" \/><\/a>\nI guess, that I am not able to do this without Date\/Time dimension\/column.\nAny ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_created_time":1658833883307,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":38.0,
        "Poster_created_time":1353149902688,
        "Poster_location":"Cracow, Poland",
        "Poster_reputation_count":887.0,
        "Poster_view_count":111.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73122328",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"net detect anomali gap miss data test detectspik detectanomali despit differ pipelin build method requir input dimension float fine anomali spike ask anomali normal anomali gap scenario need separ kind exampl webservic receiv request min there break gap request minut exampl receiv invoic random valu month there month invoic guess abl date time dimens column idea",
        "Question_preprocessed_content":"detect anomali data test detectspik detectanomali despit pipelin build method requir input float fine anomali spike ask anomali normal anomali gap exampl webservic receiv there request minut exampl receiv invoic random valu month there month invoic abl idea",
        "Question_gpt_summary_original":"The user has tested DetectSpike and DetectAnomaly to find anomalies and spikes in data, but both methods require input of 1-dimensional float[]. The user needs to find anomalies with gaps\/missings in data, such as a scenario where there are no requests for a few minutes after receiving 30-100 requests\/min or when there are two months without invoices. The user believes that a Date\/Time dimension\/column is necessary to accomplish this task and is seeking ideas.",
        "Question_gpt_summary":"user test detectspik detectanomali anomali spike data method requir input dimension float user need anomali gap miss data scenario request minut receiv request min month invoic user believ date time dimens column necessari accomplish task seek idea"
    },
    {
        "Question_title":"How to rerun successful AzureML Pipeline",
        "Question_body":"<p>I have a successful pipeline that writes files to Blob storage. If I delete the files in blob storage and try to rerun the pipeline, the pipeline run is cached so the pipeline doesn't actually rerun. Is there a way to force rerun the pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1683842694076,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/1282954\/how-to-rerun-successful-azureml-pipeline",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"rerun success pipelin success pipelin write file blob storag delet file blob storag try rerun pipelin pipelin run cach pipelin actual rerun wai forc rerun pipelin",
        "Question_preprocessed_content":"rerun pipelin pipelin write file blob storag delet file blob storag try rerun pipelin pipelin run cach pipelin rerun wai forc rerun pipelin",
        "Question_gpt_summary_original":"The user has a successful AzureML pipeline that writes files to Blob storage. However, when they delete the files and try to rerun the pipeline, the pipeline run is cached and does not actually rerun. The user is seeking a way to force the pipeline to rerun.",
        "Question_gpt_summary":"user success pipelin write file blob storag delet file try rerun pipelin pipelin run cach actual rerun user seek wai forc pipelin rerun"
    },
    {
        "Question_title":"How to persist custom job runs table",
        "Question_body":"From slack\n\nHi everyone. Is there a way to save display preferences in the job runs search UI? My main interest is in saving (1) which columns are displayed, and (2) the order in which they are displayed. Thank you.",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1650636682000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":"There\u2019s a save button next to the query search, it persist everything configured in the dashboard: https:\/\/polyaxon.com\/docs\/management\/organizations\/searches\/",
        "Answer_comment_count":0.0,
        "Question_closed_time":1650636762000,
        "Answer_last_edit_time":null,
        "Answer_score_count":1.0,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1499",
        "Tool":"Polyaxon",
        "Question_self_resolution":1.0,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"persist custom job run tabl slack wai save displai prefer job run search main save column displai order displai thank",
        "Question_preprocessed_content":"persist custom job run tabl slack wai save displai prefer job run search main save column displai order displai thank",
        "Question_gpt_summary_original":"The user is looking for a way to save display preferences in the job runs search UI, specifically which columns are displayed and the order in which they are displayed.",
        "Question_gpt_summary":"user look wai save displai prefer job run search specif column displai order displai"
    },
    {
        "Question_title":"MLflow project error",
        "Question_body":"Hi Team, there is an error although all my tries, I could not resolve it. I also posted the problem in StackOverflow and our slack channel.\n\nThe description of the error:\n\u00a0\nI want to implement the MLFlow project on my own ML model. However, I am getting \"Could not find main among entry points\"\n\nThe full problem with source files published on StackOverflow.\n\nhttps:\/\/stackoverflow.com\/questions\/69479488\/hi-i-am-very-new-to-mlflow-and-want-to-implement-mlflow-project-on-my-own-ml-m\n\nAny comments are more than welcome.\nBest Regards,\nNihad Shukur",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_created_time":1633590226000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":null,
        "Question_view_count":26.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/9yeEpSWsoFc",
        "Tool":"MLflow",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"project error team error tri resolv post problem stackoverflow slack channel descript error want implement project model get main entri point problem sourc file publish stackoverflow http stackoverflow com question new want implement project comment welcom best regard nihad shukur",
        "Question_preprocessed_content":"project team tri resolv post problem stackoverflow slack descript want implement project model main entri point problem sourc file publish stackoverflow welcom best regard nihad shukur",
        "Question_gpt_summary_original":"The user is encountering an error while trying to implement an MLFlow project on their own ML model. The error message states \"Could not find main among entry points\". The user has tried to resolve the issue but has been unsuccessful and has posted the problem on StackOverflow and their slack channel for assistance.",
        "Question_gpt_summary":"user encount error try implement project model error messag state main entri point user tri resolv issu unsuccess post problem stackoverflow slack channel assist"
    },
    {
        "Question_title":"Custom Vision for student",
        "Question_body":"<p>I registered Microsoft Azure with student account for senior project in my university and I wonder how long that I can use Custom Vision (1month or 1 year)?, I would like to know the algorithm inside the Custom Vision for object detection?, which domain is good for the big model for object detection, and can I export the project to share with my friend to label pictures in project?  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1630134876857,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/531378\/custom-vision-for-student",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"custom vision student regist microsoft azur student account senior project univers wonder long us custom vision month year like know algorithm insid custom vision object detect domain good big model object detect export project share friend label pictur project",
        "Question_preprocessed_content":"custom vision student regist microsoft azur student senior project univers wonder long us custom vision like know algorithm insid custom vision object detect domain big model object detect export project share friend label pictur project",
        "Question_gpt_summary_original":"The user has registered for Microsoft Azure with a student account for their senior project and is unsure of how long they can use Custom Vision. They also have questions about the algorithm used for object detection, the best domain for a big model for object detection, and whether they can export the project to share with friends for labeling pictures.",
        "Question_gpt_summary":"user regist microsoft azur student account senior project unsur long us custom vision question algorithm object detect best domain big model object detect export project share friend label pictur"
    },
    {
        "Question_title":"Preprocessing data for Sagemaker Inference Pipeline with Blazingtext",
        "Question_body":"<p>I'm trying to figure out the best way to preprocess my input data for my inference endpoint for AWS Sagemaker. I'm using the BlazingText algorithm.<\/p>\n\n<p>I'm not really sure the best way forward and I would be thankful for any pointers.<\/p>\n\n<p>I currently train my model using a Jupyter notebook in Sagemaker and that works wonderfully, but the problem is that I use NLTK to clean my data (Swedish stopwords and stemming etc):<\/p>\n\n<pre><code>import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n<\/code><\/pre>\n\n<p>So the question is really, how do I get the same  pre-processing logic to the inference endpoint  ?<\/p>\n\n<p>I have a couple of thoughts about how to proceed:<\/p>\n\n<ul>\n<li><p>Build a docker container with the python libs &amp; data installed with the sole purpose of pre-processing the data. Then use this container in the inference pipeline. <\/p><\/li>\n<li><p>Supply the Python libs and Script to an existing container in the same way you can do for external lib an notebook <\/p><\/li>\n<li><p>Build a custom fastText container with the libs I need and run it outside of Sagemaker.<\/p><\/li>\n<li><p>Will probably work, but feels like a \"hack\": Build a Lambda function that has the proper Python libs&amp;data installed and calls the Sagemaker Endpoint. I'm worried about cold start delays as the prediction traffic volume will be low. <\/p><\/li>\n<\/ul>\n\n<p>I would like to go with the first option, but I'm struggling a bit to understand if there is a docker image that I could build from, and add my dependencies to, or if I need to build something from the ground up. For instance, would the image sagemaker-sparkml-serving:2.2 be a good candidate? <\/p>\n\n<p>But maybe there is a better way all around? <\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_created_time":1586251113373,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":848.0,
        "Poster_created_time":1420497448900,
        "Poster_location":null,
        "Poster_reputation_count":56.0,
        "Poster_view_count":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61076649",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"preprocess data infer pipelin blazingtext try figur best wai preprocess input data infer endpoint blazingtext algorithm sure best wai forward thank pointer current train model jupyt notebook work wonderfulli problem us nltk clean data swedish stopword stem import nltk nltk download punkt nltk download stopword question pre process logic infer endpoint coupl thought proce build docker contain python lib data instal sole purpos pre process data us contain infer pipelin suppli python lib script exist contain wai extern lib notebook build custom fasttext contain lib need run outsid probabl work feel like hack build lambda function proper python lib data instal call endpoint worri cold start delai predict traffic volum low like option struggl bit understand docker imag build add depend need build ground instanc imag sparkml serv good candid mayb better wai",
        "Question_preprocessed_content":"data infer pipelin blazingtext try figur best wai input data infer endpoint blazingtext algorithm sure best wai forward thank pointer train model jupyt work problem us nltk clean data question logic infer endpoint coupl thought build docker contain python lib data sole purpos data us contain infer pipelin python lib script exist contain wai extern lib build custom contain lib run outsid probabl work like hack build lambda function proper python lib data endpoint cold start delai predict volum low like option bit understand docker imag build depend build ground instanc imag candid mayb wai",
        "Question_gpt_summary_original":"The user is trying to preprocess input data for an inference endpoint for AWS Sagemaker using the BlazingText algorithm. They currently use NLTK to clean their data during training, but are unsure how to apply the same pre-processing logic to the inference endpoint. The user is considering several options, including building a docker container with the necessary Python libraries and data, supplying the Python libs and script to an existing container, building a custom fastText container, or building a Lambda function. The user is leaning towards the first option but is unsure if there is a suitable docker image available or if they need to build one from scratch.",
        "Question_gpt_summary":"user try preprocess input data infer endpoint blazingtext algorithm current us nltk clean data train unsur appli pre process logic infer endpoint user consid option includ build docker contain necessari python librari data suppli python lib script exist contain build custom fasttext contain build lambda function user lean option unsur suitabl docker imag avail need build scratch"
    },
    {
        "Question_title":"Sagemaker Tensorflow Kernel Launch Issue",
        "Question_body":"Hello,\n\nRegardless of the instance type I can't launch a Tensorflow 2.11.0 CPU\/GPU kernel on the Frankfurt datacenter, getting this error:\n\nStarting notebook kernel...\n\nHad error starting kernel 1 times.\n\nResponded with error: Failed to launch app [tensorflow-2-11-gpu-ml-g4dn-xlarge-d54c3ba7f07d45a5aba6519fd6c2]. InternalFailure (Context: RequestId: 1374ab56-eed6-405d-bb43-e5fe279674ff, TimeStamp: 1680809971.070621, Date: Thu Apr 6 19:39:31 2023)",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_created_time":1680810606414,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1681157672860,
        "Question_score_count":1.0,
        "Question_view_count":44.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdxDnRhIGR7ClXORuhRX_LQ\/sagemaker-tensorflow-kernel-launch-issue",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"tensorflow kernel launch issu hello regardless instanc type launch tensorflow cpu gpu kernel frankfurt datacent get error start notebook kernel error start kernel time respond error fail launch app tensorflow gpu gdn xlarg dcbafdaabafdc internalfailur context requestid eed efeff timestamp date thu apr",
        "Question_preprocessed_content":"tensorflow kernel launch instanc type launch tensorflow kernel frankfurt datacent start start kernel time respond fail launch internalfailur",
        "Question_gpt_summary_original":"The user is unable to launch a Tensorflow 2.11.0 CPU\/GPU kernel on the Frankfurt datacenter, regardless of the instance type, and is receiving an error message stating that the app failed to launch due to an InternalFailure.",
        "Question_gpt_summary":"user unabl launch tensorflow cpu gpu kernel frankfurt datacent regardless instanc type receiv error messag state app fail launch internalfailur"
    },
    {
        "Question_title":"Does Vertex AI support labels for counting?",
        "Question_body":"I have an image where I have to do a multi-label classification and additionally count the number of a specific item in each image. I'm trying to setup a labeling task so I can enter a continuous number (0-100 for example), but there doesn't seem to be support for it.\u00a0\n\n\u00a0\n\nAdditionally, does the labeling have capabilities to pre-choose a \"default\"\u00a0 value?\n\n\u00a0\n\nDoes anyone have an idea?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_created_time":1635151620000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":449.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Does-Vertex-AI-support-labels-for-counting\/td-p\/173840\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"support label count imag multi label classif addition count number specif item imag try setup label task enter continu number exampl support addition label capabl pre choos default valu idea",
        "Question_preprocessed_content":"label count imag count number specif item imag try setup label task enter continu number label capabl default valu idea",
        "Question_gpt_summary_original":"The user is trying to set up a labeling task in Vertex AI for multi-label classification and counting of a specific item in each image. However, there seems to be no support for entering a continuous number for counting. The user is also looking for the capability to pre-choose a default value.",
        "Question_gpt_summary":"user try set label task multi label classif count specif item imag support enter continu number count user look capabl pre choos default valu"
    },
    {
        "Question_title":"Logging Artifacts from MlFlow on GCS Bucket",
        "Question_body":"<p>I have a running MlFlow server on GCS VM instance. I have created a bucket to log the artifacts.\nThis is the command I'm running to start the server and for specifying bucket path-<\/p>\n\n<pre><code>mlflow server --default-artifact-root gs:\/\/gcs_bucket\/artifacts --host x.x.x.x\n<\/code><\/pre>\n\n<p>But facing this error:<\/p>\n\n<pre><code>TypeError: stat: path should be string, bytes, os.PathLike or integer, not ElasticNet\n<\/code><\/pre>\n\n<p>Note- The mlflow server is running fine with the specified host alone. The problem is in the way when I'm specifying the storage bucket path.\nI have given permission of storage api by using these commands:<\/p>\n\n<pre><code>gcloud auth application-default login\ngcloud auth login\n<\/code><\/pre>\n\n<p>Also, on printing the artifact URI, this is what I'm getting:<\/p>\n\n<pre><code>mlflow.get_artifact_uri()\n<\/code><\/pre>\n\n<p>Output:<\/p>\n\n<pre><code>gs:\/\/gcs_bucket\/artifacts\/0\/122481bf990xxxxxxxxxxxxxxxxxxxxx\/artifacts\n<\/code><\/pre>\n\n<p>So in the above path from where this is coming <code>0\/122481bf990xxxxxxxxxxxxxxxxxxxxx\/artifacts<\/code> and why it's not getting auto-created at <code>gs:\/\/gcs_bucket\/artifacts<\/code><\/p>\n\n<p>After debugging more, why it's not able to get the local path from VM:\n<a href=\"https:\/\/i.stack.imgur.com\/ubDU0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ubDU0.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And this error I'm getting on VM:<\/p>\n\n<pre><code>ARNING:root:Malformed experiment 'mlruns'. Detailed error Yaml file '.\/mlruns\/mlruns\/meta.yaml' does not exist.\nTraceback (most recent call last):\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/store\/tracking\/file_store.py\", line 197, in list_experiments\n   experiment = self._get_experiment(exp_id, view_type)\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/store\/tracking\/file_store.py\", line 256, in _get_experiment\n   meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n File \"\/usr\/local\/lib\/python3.6\/dist-packages\/mlflow\/utils\/file_utils.py\", line 160, in read_yaml\n   raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\nmlflow.exceptions.MissingConfigException: Yaml file '.\/mlruns\/mlruns\/meta.yaml' does not exist.\n<\/code><\/pre>\n\n<p>Can I get a solution to this and what I'm missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7.0,
        "Question_created_time":1583840420677,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1583922371776,
        "Question_score_count":4.0,
        "Question_view_count":2153.0,
        "Poster_created_time":1451124057623,
        "Poster_location":"India",
        "Poster_reputation_count":736.0,
        "Poster_view_count":234.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60616879",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"log artifact gc bucket run server gc instanc creat bucket log artifact command run start server specifi bucket path server default artifact root gc bucket artifact host face error typeerror stat path string byte pathlik integ elasticnet note server run fine specifi host problem wai specifi storag bucket path given permiss storag api command gcloud auth applic default login gcloud auth login print artifact uri get artifact uri output gc bucket artifact bfxxxxxxxxxxxxxxxxxxxxx artifact path come bfxxxxxxxxxxxxxxxxxxxxx artifact get auto creat gc bucket artifact debug abl local path error get arn root malform experi mlrun detail error yaml file mlrun mlrun meta yaml exist traceback recent file usr local lib python dist packag store track file store line list experi experi self experi exp view type file usr local lib python dist packag store track file store line experi meta read yaml experi dir filestor meta data file file usr local lib python dist packag util file util line read yaml rais missingconfigexcept yaml file exist file path except missingconfigexcept yaml file mlrun mlrun meta yaml exist solut miss",
        "Question_preprocessed_content":"artifact gc bucket server gc instanc creat bucket log artifact start server specifi bucket path face note server fine specifi host problem wai specifi storag bucket path given storag api print artifact uri output path come abl local path solut",
        "Question_gpt_summary_original":"The user is encountering an error while trying to log artifacts from MlFlow on a GCS bucket. The error message states that the path should be a string, bytes, os.PathLike or integer, not ElasticNet. The user has given permission of storage API and is getting an output on printing the artifact URI. However, the path is not getting auto-created at the specified location. The user is also getting an error on the VM and is looking for a solution to the problem.",
        "Question_gpt_summary":"user encount error try log artifact gc bucket error messag state path string byte pathlik integ elasticnet user given permiss storag api get output print artifact uri path get auto creat specifi locat user get error look solut problem"
    },
    {
        "Question_title":"Why does Azure AutoML TargetLag featurization use so much memory? Is this a bug?",
        "Question_body":"<p>I am trying to train a time series forecast model using Azure AutoML. My data set has three series each with two years of hourly data. The whole csv file of training data is less than 2MB. My compute cluster is using D2V2 machines with 7GB of ram. If I configure the training with three target lags, the training run fails to get under way. It seems like there may be a bug. Can someone take a look at the stack trace below and confirm?<\/p>\n<p>This is the error in the log:<\/p>\n<pre><code>2020-12-04 00:06:04,486|azureml.WorkerPool|ERROR|&lt;class '__main__.UserScriptException'&gt;: UserScriptException:\n    Message: There is not enough memory on the machine to do the requested operation. Please try running the experiment on a VM with higher memory.\n    InnerException ResourceException:\n    Message: There is not enough memory on the machine to do the requested operation. Please try running the experiment on a VM with higher memory.\n    InnerException: MemoryError: cannot allocate memory for array\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;There is not enough memory on the machine to do the requested operation. Please try running the experiment on a VM with higher memory.&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/azurevmsizes&quot;,\n        &quot;target&quot;: &quot;Skipping setup\/featurization run split. Beginning full featurization logic.&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;ResourceExhausted&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;Memory&quot;\n            }\n        },\n        &quot;reference_code&quot;: &quot;791a2bd3-4812-407f-91f7-1f8a5a0b0f5a&quot;\n    }\n}\n<\/code><\/pre>\n<p>I found the error stack trace in another log:<\/p>\n<pre><code>raceback:\n  File &quot;telemetry_activity_logger.py&quot;, line 57, in _log_activity\n    yield\n  File &quot;utilities.py&quot;, line 177, in _transform_and_validate_input_data\n    feature_sweeping_config=feature_sweeping_config,\n  File &quot;contextlib.py&quot;, line 99, in __exit__\n    self.gen.throw(type, value, traceback)\n  File &quot;telemetry_activity_logger.py&quot;, line 81, in _log_activity\n    raise e.with_traceback(e.__traceback__)\n  File &quot;telemetry_activity_logger.py&quot;, line 57, in _log_activity\n    yield\n  File &quot;utilities.py&quot;, line 177, in _transform_and_validate_input_data\n    feature_sweeping_config=feature_sweeping_config,\n  File &quot;data_transformation.py&quot;, line 529, in complete_featurization\n    fault_verifier=verifier\n  File &quot;data_transformation.py&quot;, line 1048, in _get_ts_transformer_x\n    x_transform = tst.fit_transform(x, y)\n  File &quot;logging_utilities.py&quot;, line 300, in debug_log_wrapped\n    r = f(self, *args, **kwargs)\n  File &quot;timeseries_transformer.py&quot;, line 1475, in fit_transform\n    transformed = self.transform(X, y)\n  File &quot;logging_utilities.py&quot;, line 300, in debug_log_wrapped\n    r = f(self, *args, **kwargs)\n  File &quot;timeseries_transformer.py&quot;, line 1374, in transform\n    transformed_data = self._fit_columns_order()\n  File &quot;timeseries_transformer.py&quot;, line 1260, in _fit_columns_order\n    self._known_train_part, self._known_train_part.ts_value.values)\n  File &quot;forecasting_pipeline.py&quot;, line 390, in fit_transform\n    X, y, **fit_params)\n  File &quot;forecasting_pipeline.py&quot;, line 284, in execute_pipeline_op\n    Xt, fit_params = self.__execute_pipeline__preprocess_fit(X, y, **fit_params)\n  File &quot;forecasting_pipeline.py&quot;, line 320, in __execute_pipeline__preprocess_fit\n    return self._pipeline._fit(X, y, **fit_params)\n  File &quot;pipeline.py&quot;, line 315, in _fit\n    **fit_params_steps[name])\n  File &quot;memory.py&quot;, line 355, in __call__\n    return self.func(*args, **kwargs)\n  File &quot;pipeline.py&quot;, line 728, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File &quot;logging_utilities.py&quot;, line 300, in debug_log_wrapped\n    r = f(self, *args, **kwargs)\n  File &quot;lag_lead_operator.py&quot;, line 904, in fit_transform\n    rv = super(LagLeadOperator, self).fit_transform(X, y, **fit_params)  # type: TimeSeriesDataFrame\n  File &quot;base.py&quot;, line 574, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File &quot;logging_utilities.py&quot;, line 300, in debug_log_wrapped\n    r = f(self, *args, **kwargs)\n  File &quot;lag_lead_operator.py&quot;, line 882, in transform\n    left_index=True, right_index=True)\n  File &quot;time_series_data_frame.py&quot;, line 2389, in merge\n    copy=copy, indicator=indicator)\n  File &quot;frame.py&quot;, line 7349, in merge\n    validate=validate,\n  File &quot;merge.py&quot;, line 83, in merge\n    return op.get_result()\n  File &quot;merge.py&quot;, line 642, in get_result\n    join_index, left_indexer, right_indexer = self._get_join_info()\n  File &quot;merge.py&quot;, line 859, in _get_join_info\n    (left_indexer, right_indexer) = self._get_join_indexers()\n  File &quot;merge.py&quot;, line 838, in _get_join_indexers\n    self.left_join_keys, self.right_join_keys, sort=self.sort, how=self.how\n  File &quot;merge.py&quot;, line 1312, in _get_join_indexers\n    lkey, rkey, count = fkeys(lkey, rkey)\n  File &quot;merge.py&quot;, line 1902, in _factorize_keys\n    llab = rizer.factorize(lk)\n  File &quot;hashtable.pyx&quot;, line 122, in pandas._libs.hashtable.Int64Factorizer.factorize\n  File &quot;hashtable_class_helper.pxi&quot;, line 1222, in pandas._libs.hashtable.Int64HashTable.get_labels\n  File &quot;hashtable_class_helper.pxi&quot;, line 1148, in pandas._libs.hashtable.Int64HashTable._unique\n  File &quot;hashtable_class_helper.pxi&quot;, line 201, in pandas._libs.hashtable.Int64Vector.resize\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_created_time":1607045489347,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":null,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/learn.microsoft.com\/en-us\/answers\/questions\/185758\/why-does-azure-automl-targetlag-featurization-use",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"azur automl targetlag featur us memori bug try train time seri forecast model azur automl data set seri year hourli data csv file train data comput cluster machin ram configur train target lag train run fail wai like bug look stack trace confirm error log workerpool error userscriptexcept messag memori machin request oper try run experi higher memori innerexcept resourceexcept messag memori machin request oper try run experi higher memori innerexcept memoryerror alloc memori arrai errorrespons error code usererror messag memori machin request oper try run experi higher memori detail uri http aka azurevms target skip setup featur run split begin featur logic inner error code resourceexhaust inner error code memori refer code abd faabfa error stack trace log raceback file telemetri activ logger line log activ yield file util line transform valid input data featur sweep config featur sweep config file contextlib line exit self gen throw type valu traceback file telemetri activ logger line log activ rais traceback traceback file telemetri activ logger line log activ yield file util line transform valid input data featur sweep config featur sweep config file data transform line complet featur fault verifi verifi file data transform line transform transform tst fit transform file log util line debug log wrap self arg kwarg file timeseri transform line fit transform transform self transform file log util line debug log wrap self arg kwarg file timeseri transform line transform transform data self fit column order file timeseri transform line fit column order self known train self known train valu valu file forecast pipelin line fit transform fit param file forecast pipelin line execut pipelin fit param self execut pipelin preprocess fit fit param file forecast pipelin line execut pipelin preprocess fit return self pipelin fit fit param file pipelin line fit fit param step file memori line return self func arg kwarg file pipelin line fit transform re transform fit transform fit param file log util line debug log wrap self arg kwarg file lag lead oper line fit transform super lagleadoper self fit transform fit param type timeseriesdatafram file base line fit transform return self fit fit param transform file log util line debug log wrap self arg kwarg file lag lead oper line transform left index true right index true file time seri data frame line merg copi copi indic indic file frame line merg valid valid file merg line merg return result file merg line result join index left index right index self join info file merg line join info left index right index self join index file merg line join index self left join kei self right join kei sort self sort self file merg line join index lkei rkei count fkei lkei rkei file merg line factor kei llab rizer factor file hashtabl pyx line panda lib hashtabl intfactor factor file hashtabl class helper pxi line panda lib hashtabl inthasht label file hashtabl class helper pxi line panda lib hashtabl inthasht uniqu file hashtabl class helper pxi line panda lib hashtabl intvector resiz",
        "Question_preprocessed_content":"azur automl targetlag featur us memori bug try train time seri forecast model azur automl data set seri year hourli data csv file train data comput cluster machin ram configur train target lag train run fail wai like bug stack trace confirm log stack trace log",
        "Question_gpt_summary_original":"The user is encountering an issue while training a time series forecast model using Azure AutoML. The training run fails to start when configured with three target lags, and the error message suggests that there is not enough memory on the machine to perform the operation. The error stack trace indicates that the issue may be related to the TargetLag featurization, and the user suspects that it may be a bug.",
        "Question_gpt_summary":"user encount issu train time seri forecast model azur automl train run fail start configur target lag error messag suggest memori machin perform oper error stack trace indic issu relat targetlag featur user suspect bug"
    },
    {
        "Question_title":"MisconfigurationException: You requested GPUs: [0] But your machine only has: []",
        "Question_body":"<p>I'm running <code>JupyterLab<\/code> via. <code>AWS SageMaker<\/code>.<\/p>\n<p>I've been taking AWS certifications, but this is my first time actively using <code>AWS<\/code>.<\/p>\n<hr \/>\n<p>Update:<\/p>\n<p>I have changed the Notebook instance type to <code>ml.g4dn.xlarge<\/code>, a GPU.<\/p>\n<p>Will run and see what happens.<\/p>\n<p><strong>How do I change the instance types of <code>EC2<\/code> to <code>GPU<\/code>?<\/strong><\/p>\n<hr \/>\n<p>In <code>Google Colab<\/code>, e.g., you can select which hardware accelerator to use, one of which being <code>GPU<\/code>.<\/p>\n<p>Error:<\/p>\n<pre><code>MisconfigurationException: You requested GPUs: [0]\n But your machine only has: []\n<\/code><\/pre>\n<hr \/>\n<p>SageMaker environment:\n<a href=\"https:\/\/i.stack.imgur.com\/PkusK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PkusK.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>List of Kernels:\nI'm on <code>conda_python3<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/slrgq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/slrgq.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1629731290740,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1630600574523,
        "Question_score_count":1.0,
        "Question_view_count":1901.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68894940",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"misconfigurationexcept request gpu machin run jupyterlab take aw certif time activ aw updat chang notebook instanc type gdn xlarg gpu run happen chang instanc type gpu googl colab select hardwar acceler us gpu error misconfigurationexcept request gpu machin environ list kernel conda python",
        "Question_preprocessed_content":"misconfigurationexcept request gpu machin take aw certif time activ updat chang instanc type gpu run chang instanc type select hardwar us environ list kernel",
        "Question_gpt_summary_original":"The user is encountering a MisconfigurationException error while running JupyterLab via AWS SageMaker. They have changed the Notebook instance type to ml.g4dn.xlarge, a GPU, but are unsure how to change the instance types of EC2 to GPU. The error message indicates that the user requested GPUs but their machine does not have any. The user has provided screenshots of their SageMaker environment and list of kernels.",
        "Question_gpt_summary":"user encount misconfigurationexcept error run jupyterlab chang notebook instanc type gdn xlarg gpu unsur chang instanc type gpu error messag indic user request gpu machin user provid screenshot environ list kernel"
    },
    {
        "Question_title":"Pytorch models deployment to sagemaker",
        "Question_body":"<p>I have trained my yolov5 model, and have weights.pt, now I need to deploy it using sagemaker, for that I need to create an endpoint.\nI'm following this tutoriel <a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/frameworks\/pytorch\/get_started_mnist_deploy.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/frameworks\/pytorch\/get_started_mnist_deploy.html<\/a>\nSince I'm working using images I'm trying to customise input_fn,output_fn functions but unfortunately when I run inference I always get errors, my question is what logic should I follow in order to customise these functions ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1649847001237,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":240.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71856228",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"pytorch model deploy train yolov model weight need deploi need creat endpoint follow tutoriel http exampl readthedoc latest framework pytorch start mnist deploi html work imag try customis input output function unfortun run infer error question logic follow order customis function",
        "Question_preprocessed_content":"pytorch model deploy train yolov model deploi creat endpoint tutoriel work imag try customis function unfortun run infer question logic order customis function",
        "Question_gpt_summary_original":"The user has trained a Pytorch model and has weights.pt, and now wants to deploy it using sagemaker by creating an endpoint. They are following a tutorial but are facing errors while customizing input_fn and output_fn functions for image-based work. The user is seeking guidance on the logic to follow for customizing these functions.",
        "Question_gpt_summary":"user train pytorch model weight want deploi creat endpoint follow tutori face error custom input output function imag base work user seek guidanc logic follow custom function"
    },
    {
        "Question_title":"How to share models in a multitenant enviroment with Mlflow?",
        "Question_body":"<p>The company I work for are using Databricks with Azure as a storage service. My group is trying to create a centralized model registry that allows us to push and pull models into different instances of Databricks. We are aware that we can share models within the same subscription (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/machine-learning\/manage-model-lifecycle\/multiple-workspaces\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/machine-learning\/manage-model-lifecycle\/multiple-workspaces<\/a>) however we have multiple subscriptions so this wont work for us. From what I've read there are two solutions for this. Use Azure blob storage or an SQL solution. Unfortunately I cant find much info online. Anyone have any idea how I can implement this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1658248426997,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":49.0,
        "Poster_created_time":1632199544943,
        "Poster_location":null,
        "Poster_reputation_count":1.0,
        "Poster_view_count":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73040570",
        "Tool":"MLflow",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"share model multiten enviro compani work databrick azur storag servic group try creat central model registri allow push pull model differ instanc databrick awar share model subscript http doc microsoft com azur databrick applic machin learn manag model lifecycl multipl workspac multipl subscript wont work read solut us azur blob storag sql solut unfortun info onlin idea implement",
        "Question_preprocessed_content":"share model multiten enviro compani work databrick azur storag servic group try creat central model registri push model instanc databrick awar share model subscript multipl subscript wont work read solut us azur blob storag sql solut unfortun info onlin idea implement",
        "Question_gpt_summary_original":"The user is trying to create a centralized model registry in a multitenant environment using Databricks with Azure as a storage service. They are unable to share models across multiple subscriptions and are seeking solutions such as using Azure blob storage or an SQL solution. The user is looking for guidance on how to implement this.",
        "Question_gpt_summary":"user try creat central model registri multiten environ databrick azur storag servic unabl share model multipl subscript seek solut azur blob storag sql solut user look guidanc implement"
    },
    {
        "Question_title":"how to re-package sagemaker model with lambda?",
        "Question_body":"I have a sagemaker training process which dumps a model.tar file in a s3 bucket. I don't have access to this training code . I want to download the model.tar file from that s3 location, and repackage it with my own inference code , saved in some s3 bucket. i am aware that there is a way to package this in sagemaker via pipelines but are there any examples to do it in lambda?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1681182700915,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1681529527003,
        "Question_score_count":0.0,
        "Question_view_count":46.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDtd-nb75TZGb0QNcOUI-8w\/how-to-re-package-sagemaker-model-with-lambda",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"packag model lambda train process dump model tar file bucket access train code want download model tar file locat repackag infer code save bucket awar wai packag pipelin exampl lambda",
        "Question_preprocessed_content":"model lambda train dump file bucket train code want download file locat repackag infer code save bucket awar wai packag pipelin exampl lambda",
        "Question_gpt_summary_original":"The user wants to re-package a sagemaker model with their own inference code, but they don't have access to the training code. They have the model.tar file in an S3 bucket and want to download it and repackage it with their own code, saved in another S3 bucket. They are looking for examples of how to do this in Lambda.",
        "Question_gpt_summary":"user want packag model infer code access train code model tar file bucket want download repackag code save bucket look exampl lambda"
    },
    {
        "Question_title":"Cant install imbalanced-learn on an Azure ML Environment",
        "Question_body":"<p>I have an Azure ML Workspace which comes by default with some pre-installed packages.<\/p>\n<p>I tried to install<\/p>\n<pre><code>!pip install -U imbalanced-learn\n<\/code><\/pre>\n<p>But I got this error<\/p>\n<pre><code>Requirement already up-to-date: scikit-learn in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (0.24.2)\nRequirement already satisfied, skipping upgrade: scipy&gt;=0.19.1 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.18.5)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (2.1.0)\nCollecting imbalanced-learn\n  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: scipy&gt;=1.1.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (1.4.1)\nERROR: Could not find a version that satisfies the requirement scikit-learn&gt;=1.0.1 (from imbalanced-learn) (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\nERROR: No matching distribution found for scikit-learn&gt;=1.0.1 (from imbalanced-\n<\/code><\/pre>\n<p>learn)<\/p>\n<p>Not sure how to solve this, I have read in other posts to use conda, but that didnt work either.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1644933997077,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1644960360047,
        "Question_score_count":1.0,
        "Question_view_count":219.0,
        "Poster_created_time":1302030303092,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation_count":30340.0,
        "Poster_view_count":2937.0,
        "Answer_body":"<p><a href=\"https:\/\/pypi.org\/project\/scikit-learn\/1.0.1\/\" rel=\"nofollow noreferrer\"><code>scikit-learn<\/code> 1.0.1<\/a> and up require Python &gt;= 3.7; you use Python 3.6. You need to upgrade Python or downgrade <code>imbalanced-learn<\/code>. <a href=\"https:\/\/pypi.org\/project\/imbalanced-learn\/0.8.1\/\" rel=\"nofollow noreferrer\"><code>imbalanced-learn<\/code> 0.8.1<\/a> allows Python 3.6 so<\/p>\n<pre><code>!pip install -U &quot;imbalanced-learn &lt; 0.9&quot;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Question_closed_time":1644935036627,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1494171603136,
        "Answerer_location":null,
        "Answerer_reputation_count":73187.0,
        "Answerer_view_count":8473.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71127858",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"instal imbalanc learn environ workspac come default pre instal packag tri instal pip instal imbalanc learn got error requir date scikit learn anaconda env lib python site packag requir satisfi skip upgrad scipi anaconda env lib python site packag scikit learn requir satisfi skip upgrad joblib anaconda env lib python site packag scikit learn requir satisfi skip upgrad numpi anaconda env lib python site packag scikit learn requir satisfi skip upgrad threadpoolctl anaconda env lib python site packag scikit learn collect imbalanc learn cach imbalanc learn whl requir satisfi skip upgrad threadpoolctl anaconda env lib python site packag imbalanc learn requir satisfi skip upgrad joblib anaconda env lib python site packag imbalanc learn requir satisfi skip upgrad scipi anaconda env lib python site packag imbalanc learn error version satisfi requir scikit learn imbalanc learn version post post dev error match distribut scikit learn imbalanc learn sure solv read post us conda didnt work",
        "Question_preprocessed_content":"environ workspac come default packag tri got learn sure solv read post us conda didnt work",
        "Question_gpt_summary_original":"The user is trying to install imbalanced-learn on an Azure ML Workspace, but is encountering an error stating that it cannot find a matching distribution for scikit-learn>=1.0.1. The user has tried using conda but it did not work.",
        "Question_gpt_summary":"user try instal imbalanc learn workspac encount error state match distribut scikit learn user tri conda work"
    },
    {
        "Question_title":"How to return a Dataframe in Azure ML from a python script",
        "Question_body":"<p>I have the below Python script. And I'm trying to return the new Dataframe created. Unfortunately, I'm getting \"NotImplementedError:\"<\/p>\n\n<p>Please let me know, how to return a Dataframe in Azure ML from a python script<\/p>\n\n<p>Code:<\/p>\n\n<pre><code>def azureml_main(df) :\n\n    therm_patterns,therm_missed_patterns = thermostat_phase(df)\n    th_pat = json.loads(therm_patterns)\n    missed_th_pat = json.loads(therm_missed_patterns)\n\n    light_patterns, light_missed_patterns = light_phase(df)\n    lt_pat = json.loads(light_patterns)\n    missed_lt_pat = json.loads(light_missed_patterns)\n\n    for j in range (0,len(lt_pat)):\n        for i in range (0,len(lt_pat[0]['John'])):\n            th_pat[0]['John'].append(lt_pat[0]['John'][i])\n    for j in range (0,len(lt_pat)):\n        for i in range (0,len(lt_pat[0]['Jane'])):\n            th_pat[0]['Jane'].append(lt_pat[0]['Jane'][i])\n    for j in range (0,len(lt_pat)):\n        for i in range (0,len(lt_pat[0]['Jen'])):\n            th_pat[0]['Jen'].append(lt_pat[0]['Jen'][i])\n\n    for j in range (0,len(missed_lt_pat)):\n        for i in range (0,len(missed_lt_pat[0]['John'])):\n            missed_th_pat[0]['John'].append(missed_lt_pat[0]['John'][i])\n    for j in range (0,len(missed_lt_pat)):\n        for i in range (0,len(missed_lt_pat[0]['Jane'])):\n            missed_th_pat[0]['Jane'].append(missed_lt_pat[0]['Jane'][i])\n    for j in range (0,len(missed_lt_pat)):\n        for i in range (0,len(missed_lt_pat[0]['Jen'])):\n            missed_th_pat[0]['Jen'].append(missed_lt_pat[0]['Jen'][i])\n\n    output = json.dumps(th_pat)\n\n    df = pd.DataFrame.from_dict({k: v[0] for k, v in json.loads(output) \n    [0].items()}, 'index' ).rename_axis('User').reset_index()\n\n\n    return df         \n<\/code><\/pre>\n\n<p>Error:<\/p>\n\n<pre><code>  Error 0085: The following error occurred during script evaluation, please  \n  view the output log for more information:\n\n   ---------- Start of error message from Python interpreter ----------\n   Caught exception while executing function: Traceback (most recent call  \n   last):\n\n       File \"C:\\server\\invokepy.py\", line 211, in batch\n       xdrutils.XDRUtils.DataFrameToRFile(outlist[i], outfiles[i], True)\n\n       File \"C:\\server\\XDRReader\\xdrutils.py\", line 54, in DataFrameToRFile\n       xdrwriter.write_attribute_list(attributes)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 155, in   \n       write_attribute_list\n       self.write_object(value)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 215, in write_object\n       write_func(flags, converted, missingIndices)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 185, in write_objects\n       self.write_object(value)\n\n       File \"C:\\server\\XDRReader\\xdrwriter2.py\", line 200, in write_object\n       raise NotImplementedError('Python Bridge conversion table not    \n       implemented for type [{0}]'.format(value.getType()))\n\n       NotImplementedError: Python Bridge conversion table not implemented  \n       for type [&lt;type 'list'&gt;]\n\n       Process returned with non-zero exit code 1\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_created_time":1484124401607,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1484584933627,
        "Question_score_count":3.0,
        "Question_view_count":1101.0,
        "Poster_created_time":1479194627132,
        "Poster_location":null,
        "Poster_reputation_count":2713.0,
        "Poster_view_count":358.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41586275",
        "Tool":"Azure Machine Learning",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"return datafram python script python script try return new datafram creat unfortun get notimplementederror let know return datafram python script code def main therm pattern therm miss pattern thermostat phase pat json load therm pattern miss pat json load therm miss pattern light pattern light miss pattern light phase pat json load light pattern miss pat json load light miss pattern rang len pat rang len pat john pat john append pat john rang len pat rang len pat jane pat jane append pat jane rang len pat rang len pat jen pat jen append pat jen rang len miss pat rang len miss pat john miss pat john append miss pat john rang len miss pat rang len miss pat jane miss pat jane append miss pat jane rang len miss pat rang len miss pat jen miss pat jen append miss pat jen output json dump pat datafram dict json load output item index renam axi user reset index return error error follow error occur script evalu view output log inform start error messag python interpret caught except execut function traceback recent file server invokepi line batch xdrutil xdrutil dataframetorfil outlist outfil true file server xdrreader xdrutil line dataframetorfil xdrwriter write attribut list attribut file server xdrreader xdrwriter line write attribut list self write object valu file server xdrreader xdrwriter line write object write func flag convert missingindic file server xdrreader xdrwriter line write object self write object valu file server xdrreader xdrwriter line write object rais notimplementederror python bridg convers tabl implement type format valu gettyp notimplementederror python bridg convers tabl implement type process return non zero exit code",
        "Question_preprocessed_content":"return datafram python script python script try return new datafram creat unfortun let know return datafram python script code",
        "Question_gpt_summary_original":"The user is trying to return a new Dataframe created in a Python script in Azure ML, but is encountering a \"NotImplementedError\" and is unable to do so. The error message suggests that the Python Bridge conversion table is not implemented for the type \"list\".",
        "Question_gpt_summary":"user try return new datafram creat python script encount notimplementederror unabl error messag suggest python bridg convers tabl implement type list"
    },
    {
        "Question_title":"Converting PDF and PPTX files drawn from S3 into a JPG format",
        "Question_body":"<p>I am using a Jupyter Notebook within AWS SageMaker for a project that I am working on, (Jupyter Notebook is running conda-python3) and I need to convert several files that I have in the formats: PPTX, PPT and PDF, into an Image format (JPG, JPEG, PNG). I believe the major issue I am having is that I cannot download libraries into a Jupyter instance as it doesn't have a conventional download path I can draw from. <\/p>\n\n<p>I have tried using Wand and Imagemagick, but I always run into problems. I have also used some 3rd party packages like pdf2jpg but none of them work (I believe it is because I do not know how to download the Poppler library in Jupyter). <\/p>\n\n<p>I believe that the below issue is do to with a fact that I have a 32 bit Python running when Imagemagick is running as 64 bit, but I do not know how to check what bit size I am running on Python or how to download 32 bit Imagemagick on Jupyter <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ImportError: MagickWand shared library not found.\nYou probably had not installed ImageMagick library.\n<\/code><\/pre>\n\n<p>this happens when i call <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from wand.image import Image\n<\/code><\/pre>\n\n<p>Could you please advise me?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_created_time":1561580945217,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1561585708543,
        "Question_score_count":0.0,
        "Question_view_count":393.0,
        "Poster_created_time":1561577493756,
        "Poster_location":"New Jersey, USA",
        "Poster_reputation_count":21.0,
        "Poster_view_count":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56780426",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"convert pdf pptx file drawn jpg format jupyt notebook project work jupyt notebook run conda python need convert file format pptx ppt pdf imag format jpg jpeg png believ major issu have download librari jupyt instanc convent download path draw tri wand imagemagick run problem parti packag like pdfjpg work believ know download poppler librari jupyt believ issu fact bit python run imagemagick run bit know check bit size run python download bit imagemagick jupyt importerror magickwand share librari probabl instal imagemagick librari happen wand imag import imag advis",
        "Question_preprocessed_content":"convert pdf file drawn jpg format jupyt project work convert file format pdf imag format believ major have download librari jupyt instanc convent download path draw tri wand imagemagick run problem parti packag like pdf jpg work believ fact bit python imagemagick bit know check bit size python download bit imagemagick jupyt advis",
        "Question_gpt_summary_original":"The user is working on a project in a Jupyter Notebook within AWS SageMaker and needs to convert PPTX, PPT, and PDF files into JPG, JPEG, or PNG formats. They are unable to download libraries into the Jupyter instance and have tried using Wand and Imagemagick, but encountered issues. The user is unsure of the bit size of their Python and how to download 32 bit Imagemagick on Jupyter. They are receiving an ImportError when calling \"from wand.image import Image\".",
        "Question_gpt_summary":"user work project jupyt notebook need convert pptx ppt pdf file jpg jpeg png format unabl download librari jupyt instanc tri wand imagemagick encount issu user unsur bit size python download bit imagemagick jupyt receiv importerror call wand imag import imag"
    },
    {
        "Question_title":"Wandb training kills kernel in jupyter lab",
        "Question_body":"<p>In my jupyter I can train my model on batch_size=8, but when I use wandb always after 9 iterations the process is killed and kernel restarts. What's more weird is that the same code worked on colab, but with my GPU (RTX 3080) I can never finish the process.<\/p>\n<p>Does anyone have any idea how to overcome this issue?<\/p>\n<p>Edit: <strong>I noticed that the kernel dies every time it tries to log the gradients to wandb. Can this be solved?<\/strong><\/p>\n<p>Code with wandb:<\/p>\n<pre><code>def train_batch(images, labels, model, optimizer, criterion):\n    images, labels = images.to(device), labels.to(device)\n    \n    # Forward pass \u27a1\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n    \n    # Backward pass \u2b05\n    optimizer.zero_grad()\n    loss.backward()\n\n    # Step with optimizer\n    optimizer.step()\n    \n    size = images.size(0)\n    del images, labels\n    return loss, size\n\nfrom loss import YoloLoss\n\n# train the model\ndef train(model, train_dl, criterion, optimizer, config, is_one_batch):\n    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n    wandb.watch(model, criterion, log=&quot;all&quot;, log_freq=10)\n\n    example_ct = 0  # number of examples seen\n    batch_ct = 0\n    \n    # enumerate epochs\n    for epoch in range(config.epochs):\n        running_loss = 0.0\n        \n        if not is_one_batch:\n            for i, (inputs, _, targets) in enumerate(train_dl):\n                loss, batch_size = train_batch(inputs, targets, model, optimizer, criterion)\n                running_loss += loss.item() * batch_size\n        else:\n            # for one batch only\n            loss, batch_size = train_batch(train_dl[0], train_dl[2], model, optimizer, criterion)\n            running_loss += loss.item() * batch_size\n            \n        epoch_loss = running_loss \/ len(train_dl)\n#         loss_values.append(epoch_loss)\n        wandb.log({&quot;epoch&quot;: epoch, &quot;avg_batch_loss&quot;: epoch_loss})\n#         wandb.log({&quot;epoch&quot;: epoch, &quot;loss&quot;: loss}, step=example_ct)\n        print(&quot;Average epoch loss {}&quot;.format(epoch_loss))\ndef make(config, is_one_batch, data_predefined=True):\n    optimizers = {\n        &quot;Adam&quot;:torch.optim.Adam,\n        &quot;SGD&quot;:torch.optim.SGD\n    }\n    \n    if data_predefined:\n        train_dl, test_dl = train_dl_predef, test_dl_predef\n    else:\n        train_dl, test_dl = dataset.prepare_data()\n        \n    if is_one_batch:\n        train_dl = next(iter(train_dl))\n        test_dl = train_dl\n    \n    # Make the model\n    model = architecture.darknet(config.batch_norm)\n    model.to(device)\n\n    # Make the loss and optimizer\n    criterion = YoloLoss()\n    optimizer = optimizers[config.optimizer](\n        model.parameters(), \n        lr=config.learning_rate,\n        momentum=config.momentum\n    )\n    \n    return model, train_dl, test_dl, criterion, optimizer\n        \ndef model_pipeline(hyp, is_one_batch=False, device=device):\n    with wandb.init(project=&quot;YOLO-recreated&quot;, entity=&quot;bindas1&quot;, config=hyp):\n        config = wandb.config\n        \n        # make the model, data, and optimization problem\n        model, train_dl, test_dl, criterion, optimizer = make(config, is_one_batch)\n        \n        # and use them to train the model\n        train(model, train_dl, criterion, optimizer, config, is_one_batch)\n        \n    return model\n\n<\/code><\/pre>\n<p>Code without wandb:<\/p>\n<pre><code>def train_model(train_dl, model, is_one_batch=False):\n    # define the optimization\n    criterion = YoloLoss()\n    optimizer = SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n    \n    # for loss plotting\n    loss_values = []\n    \n    # enumerate epochs\n    for epoch in tqdm(range(EPOCHS)):\n        if epoch % 10 == 0:\n            print(epoch)\n        running_loss = 0.0\n        \n        if not is_one_batch:\n        # enumerate mini batches\n            for i, (inputs, _, targets) in enumerate(train_dl):\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n                # clear the gradients\n                optimizer.zero_grad()\n                # compute the model output\n                yhat = model(inputs)\n                # calculate loss\n                loss = criterion(yhat, targets)\n                # credit assignment\n                loss.backward()\n#                 print(loss)\n                running_loss =+ loss.item() * inputs.size(0)\n                # update model weights\n                optimizer.step()\n        else:\n            # for one batch only\n            with torch.autograd.detect_anomaly():\n                inputs, targets = train_dl[0].to(device), train_dl[2].to(device)\n                optimizer.zero_grad()\n                # compute the model output\n                yhat = model(inputs)\n                # calculate loss\n                loss = criterion(yhat, targets)\n                # credit assignment\n                loss.backward()\n                print(loss)\n                running_loss =+ loss.item() * inputs.size(0)\n                # update model weights\n                optimizer.step()\n        loss_values.append(running_loss \/ len(train_dl))\n    \n    plot_loss(loss_values)\n\nmodel = architecture.darknet()\nmodel.to(device)\noptimizer = SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\ntrain_dl_main, test_dl_main = train_dl_predef, test_dl_predef\none_batch = next(iter(train_dl_main))\ntrain_model_wandb(one_batch, model, is_one_batch=True)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1639087126467,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1639090837216,
        "Question_score_count":0.0,
        "Question_view_count":206.0,
        "Poster_created_time":1536338822488,
        "Poster_location":null,
        "Poster_reputation_count":23.0,
        "Poster_view_count":8.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70297236",
        "Tool":"Weights & Biases",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"train kill kernel jupyt lab jupyt train model batch size us iter process kill kernel restart weird code work colab gpu rtx finish process idea overcom issu edit notic kernel di time tri log gradient solv code def train batch imag label model optim criterion imag label imag devic label devic forward pass output model imag loss criterion output label backward pass optim zero grad loss backward step optim optim step size imag size del imag label return loss size loss import yololoss train model def train model train criterion optim config batch tell watch model get gradient weight watch model criterion log log freq exampl number exampl seen batch enumer epoch epoch rang config epoch run loss batch input target enumer train loss batch size train batch input target model optim criterion run loss loss item batch size batch loss batch size train batch train train model optim criterion run loss loss item batch size epoch loss run loss len train loss valu append epoch loss log epoch epoch avg batch loss epoch loss log epoch epoch loss loss step exampl print averag epoch loss format epoch loss def config batch data predefin true optim adam torch optim adam sgd torch optim sgd data predefin train test train predef test predef train test dataset prepar data batch train iter train test train model model architectur darknet config batch norm model devic loss optim criterion yololoss optim optim config optim model paramet config learn rate momentum config momentum return model train test criterion optim def model pipelin hyp batch fals devic devic init project yolo recreat entiti binda config hyp config config model data optim problem model train test criterion optim config batch us train model train model train criterion optim config batch return model code def train model train model batch fals defin optim criterion yololoss optim sgd model paramet learn rate momentum momentum loss plot loss valu enumer epoch epoch tqdm rang epoch epoch print epoch run loss batch enumer mini batch input target enumer train input input devic target target devic clear gradient optim zero grad comput model output yhat model input calcul loss loss criterion yhat target credit assign loss backward print loss run loss loss item input size updat model weight optim step batch torch autograd detect anomali input target train devic train devic optim zero grad comput model output yhat model input calcul loss loss criterion yhat target credit assign loss backward print loss run loss loss item input size updat model weight optim step loss valu append run loss len train plot loss loss valu model architectur darknet model devic optim sgd model paramet learn rate momentum momentum train main test main train predef test predef batch iter train main train model batch model batch true",
        "Question_preprocessed_content":"train kernel jupyt lab jupyt train model us iter kernel restart weird code work colab gpu finish idea overcom edit notic kernel di time tri log gradient solv code code",
        "Question_gpt_summary_original":"The user is facing an issue where the kernel in Jupyter Lab is killed after 9 iterations when using wandb to train a model with batch_size=8. The same code worked on Colab, but with the user's GPU (RTX 3080), the process never finishes. The kernel dies every time it tries to log the gradients to wandb. The user is seeking a solution to overcome this issue.",
        "Question_gpt_summary":"user face issu kernel jupyt lab kill iter train model batch size code work colab user gpu rtx process finish kernel di time tri log gradient user seek solut overcom issu"
    },
    {
        "Question_title":"Sweep in DDP mode",
        "Question_body":"<p>I wonder how Sweep works in multi-GPU mode? I want to initialize the parameters that need to be optimized in just one process, and then use Sweep for hyperparametric optimization. However, if I only initialize parameters in one process, other processes will report an error because they did not query parameters when loading the model. I didn\u2019t find the answer to using Sweep in multi-GPU mode, thanks for answering!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_created_time":1641148429166,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":576.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-in-ddp-mode\/1664",
        "Tool":"Weights & Biases",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"sweep ddp mode wonder sweep work multi gpu mode want initi paramet need optim process us sweep hyperparametr optim initi paramet process process report error queri paramet load model didnt answer sweep multi gpu mode thank answer",
        "Question_preprocessed_content":"mode wonder work mode want initi paramet optim us hyperparametr optim initi paramet report queri paramet load model didnt answer mode thank answer",
        "Question_gpt_summary_original":"The user is seeking information on how to use Sweep in multi-GPU mode for hyperparameter optimization. They are facing an issue where initializing parameters in one process causes errors in other processes as they did not query the parameters when loading the model. The user is looking for a solution to this problem.",
        "Question_gpt_summary":"user seek inform us sweep multi gpu mode hyperparamet optim face issu initi paramet process caus error process queri paramet load model user look solut problem"
    },
    {
        "Question_title":"Dialogflow cx caller abandoned call event",
        "Question_body":"Is there a way to trigger a webhook, as soon as the caller abandons the call?",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_created_time":1666761180000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":128.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-cx-caller-abandoned-call-event\/td-p\/482361\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"dialogflow caller abandon event wai trigger webhook soon caller abandon",
        "Question_preprocessed_content":"dialogflow abandon event wai abandon",
        "Question_gpt_summary_original":"The user is looking for a way to trigger a webhook when a caller abandons a call in Dialogflow CX.",
        "Question_gpt_summary":"user look wai trigger webhook caller abandon dialogflow"
    },
    {
        "Question_title":"Why does AWS SageMaker create an S3 Bucket",
        "Question_body":"<p>Upon deploying a custom pytorch model with the boto3 client in python. I noticed that a new S3 bucket had been created with no visible objects. Is there a reason for this?<\/p>\n<p>The bucket that contained my model was named with the keyword &quot;sagemaker&quot; included, so I don't any issue there.<\/p>\n<p>Here is the code that I used for deployment:<\/p>\n<pre><code>remote_model = PyTorchModel(\n                     name = model_name, \n                     model_data=model_url,\n                     role=role,\n                     sagemaker_session = sess,\n                     entry_point=&quot;inference.py&quot;,\n                     # image=image, \n                     framework_version=&quot;1.5.0&quot;,\n                     py_version='py3'\n                    )\n\nremote_predictor = remote_model.deploy(\n                         instance_type='ml.g4dn.xlarge', \n                         initial_instance_count=1,\n                         #update_endpoint = True, # comment or False if endpoint doesns't exist\n                         endpoint_name=endpoint_name, # define a unique endpoint name; if ommited, Sagemaker will generate it based on used container\n                         wait=True\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1607931328823,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score_count":2.0,
        "Question_view_count":1047.0,
        "Poster_created_time":1577873077020,
        "Poster_location":"Perth WA, Australia",
        "Poster_reputation_count":438.0,
        "Poster_view_count":67.0,
        "Answer_body":"<p>It was likely created as a default bucket by the SageMaker Python SDK. Note that the code you wrote about is not <code>boto3<\/code> (AWS python SDK), but <code>sagemaker<\/code> (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">link<\/a>), the SageMaker-specific Python SDK, that is higher-level than boto3.<\/p>\n<p>The SageMaker Python SDK uses S3 at multiple places, for example to stage training code when using a Framework Estimator, and to stage inference code when deployment with a Framework Model (your case). It gives you control of the S3 location to use, but if you don't specify it, it may use an automatically generated bucket, if it has the permissions to do so.<\/p>\n<p>To control code staging S3 location, you can use the parameter <code>code_location<\/code> in either your <code>PyTorchEstimator<\/code> (training) or your <code>PyTorchModel<\/code> (serving)<\/p>",
        "Answer_comment_count":2.0,
        "Question_closed_time":1608132873676,
        "Answer_last_edit_time":null,
        "Answer_score_count":2.0,
        "Answerer_created_time":1442180190107,
        "Answerer_location":null,
        "Answerer_reputation_count":3203.0,
        "Answerer_view_count":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65285203",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"creat bucket deploi custom pytorch model boto client python notic new bucket creat visibl object reason bucket contain model name keyword includ issu code deploy remot model pytorchmodel model model data model url role role session sess entri point infer imag imag framework version version remot predictor remot model deploi instanc type gdn xlarg initi instanc count updat endpoint true comment fals endpoint doesn exist endpoint endpoint defin uniqu endpoint ommit gener base contain wait true",
        "Question_preprocessed_content":"creat bucket deploi custom pytorch model boto client python notic new bucket creat visibl object reason bucket contain model name keyword includ code deploy",
        "Question_gpt_summary_original":"The user has noticed that a new S3 bucket is created with no visible objects upon deploying a custom PyTorch model with the boto3 client in Python using AWS SageMaker. The bucket name contained the keyword \"sagemaker,\" and the user is unsure if this is expected behavior. The user has provided the code used for deployment.",
        "Question_gpt_summary":"user notic new bucket creat visibl object deploi custom pytorch model boto client python bucket contain keyword user unsur expect behavior user provid code deploy"
    },
    {
        "Question_title":"Custom Model for Batch Prediction on Vertex.ai",
        "Question_body":"<p>I want to run batch predictions inside Google Cloud's vertex.ai using a custom trained model.  I was able to find documentation to get online prediction working with a custom built docker image by setting up an endpoint, but I can't seem to find any documentation on what the Dockerfile should be for batch prediction.  Specifically how does my custom code get fed the input and where does it put the output?<\/p>\n<p>The documentation I've found is <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/batch-predictions?_ga=2.262616524.-1738078585.1553812508&amp;_gac=1.83826788.1630620723.Cj0KCQjw7MGJBhD-ARIsAMZ0eesi3e2_fghoLZcWRKqw_ZbncT3LeZvgYPu929bJELdeiX3RSNHPApcaAo8dEALw_wcB#custom-trained_3\" rel=\"nofollow noreferrer\">here<\/a>, it certainly looks possible to use a custom model and when I tried it didn't complain, but eventually it did throw an error.  According to the documentation no endpoint is required for running batch jobs.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":10.0,
        "Question_created_time":1632153316607,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":0.0,
        "Question_view_count":480.0,
        "Poster_created_time":1540925643620,
        "Poster_location":"Utah, USA",
        "Poster_reputation_count":1212.0,
        "Poster_view_count":110.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69257475",
        "Tool":"Vertex AI",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"custom model batch predict vertex want run batch predict insid googl cloud vertex custom train model abl document onlin predict work custom built docker imag set endpoint document dockerfil batch predict specif custom code fed input output document certainli look possibl us custom model tri complain eventu throw error accord document endpoint requir run batch job",
        "Question_preprocessed_content":"custom model batch predict want run batch predict insid cloud custom train model abl document onlin predict work custom built docker imag endpoint document dockerfil batch predict custom code fed input output document certainli us custom model tri complain throw document endpoint requir batch job",
        "Question_gpt_summary_original":"The user wants to run batch predictions using a custom trained model on Google Cloud's vertex.ai, but is unable to find documentation on the Dockerfile for batch prediction. The user is specifically looking for information on how the custom code gets fed input and where the output is placed. The documentation found suggests that it is possible to use a custom model for batch prediction without an endpoint, but the user encountered an error.",
        "Question_gpt_summary":"user want run batch predict custom train model googl cloud vertex unabl document dockerfil batch predict user specif look inform custom code get fed input output place document suggest possibl us custom model batch predict endpoint user encount error"
    },
    {
        "Question_title":"How to import\/load pretrained DeepAR model into an estimator for continuous training?",
        "Question_body":"Hi, I have a trained DeepAR model saved as model.tar.gz in some S3 location. I hope to further optimize it by training for some extra epochs with extra data having same features, like transfer learning. I have read some DeepAR documentation but cannot find a way to import the trained model. \n\nI tried using Sagemaker's Estimator's [model_uri](https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html#sagemaker.estimator.EstimatorBase.fit) config, but it reports error for deepar, saying no 'model' channel is acceptable. \n\nI am wondering does DeepAR have such functionality or it is still not developed?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1673930575516,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1674277144280,
        "Question_score_count":0.0,
        "Question_view_count":69.0,
        "Poster_created_time":null,
        "Poster_location":null,
        "Poster_reputation_count":null,
        "Poster_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU48KNg_QvQmO9v0y4ST_Abg\/how-to-import-load-pretrained-deepar-model-into-an-estimator-for-continuous-training",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":null,
        "Platform":"Tool-specific",
        "Poster_isAwsEmployee":0.0,
        "Poster_isModerator":0.0,
        "Poster_isExpert":0.0,
        "Poster_isCse":0.0,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"import load pretrain deepar model estim continu train train deepar model save model tar locat hope optim train extra epoch extra data have featur like transfer learn read deepar document wai import train model tri estim model uri http readthedoc stabl api train estim html estim estimatorbas fit config report error deepar sai model channel accept wonder deepar function develop",
        "Question_preprocessed_content":"pretrain model estim continu train train model save locat hope optim train extra epoch extra data have featur like transfer learn read document wai import train model tri estim config report sai model wonder function develop",
        "Question_gpt_summary_original":"The user is trying to import a pretrained DeepAR model saved in an S3 location for further optimization by training it with extra data having the same features. However, the user is unable to find a way to import the trained model and is encountering an error while using Sagemaker's Estimator's model_uri config. The user is wondering if DeepAR has the functionality to import a trained model or if it is still under development.",
        "Question_gpt_summary":"user try import pretrain deepar model save locat optim train extra data have featur user unabl wai import train model encount error estim model uri config user wonder deepar function import train model develop"
    },
    {
        "Question_title":"Errno 28 Low on disk space",
        "Question_body":"<p>I am trying to install a python package in aws notebook, instance g4dn.xlarge. The package itself is 3GB. The package doesnt install and I am getting this error errno 28 low on disk space. I even increased the volume size up till 50GB. But the error still persists.\nCan someone help me with this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_created_time":1650542886660,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score_count":1.0,
        "Question_view_count":33.0,
        "Poster_created_time":1610784149952,
        "Poster_location":null,
        "Poster_reputation_count":11.0,
        "Poster_view_count":9.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Question_closed_time":null,
        "Answer_last_edit_time":null,
        "Answer_score_count":null,
        "Answerer_created_time":null,
        "Answerer_location":null,
        "Answerer_reputation_count":null,
        "Answerer_view_count":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71954220",
        "Tool":"Amazon SageMaker",
        "Question_self_resolution":0.0,
        "Platform":"Stack Overflow",
        "Poster_isAwsEmployee":null,
        "Poster_isModerator":null,
        "Poster_isExpert":null,
        "Poster_isCse":null,
        "Answerer_isAwsEmployee":null,
        "Answerer_isModerator":null,
        "Answerer_isExpert":null,
        "Answerer_isCse":null,
        "Question_original_content":"errno low disk space try instal python packag aw notebook instanc gdn xlarg packag packag doesnt instal get error errno low disk space increas volum size till error persist help",
        "Question_preprocessed_content":"low disk space try python packag aw instanc packag packag doesnt low disk space increas volum size persist help",
        "Question_gpt_summary_original":"The user is trying to install a 3GB python package on an AWS notebook instance but is encountering an \"errno 28 low on disk space\" error, even after increasing the volume size to 50GB. They are seeking assistance to resolve the issue.",
        "Question_gpt_summary":"user try instal python packag aw notebook instanc encount errno low disk space error increas volum size seek assist resolv issu"
    }
]