[
    {
        "Question_id":73321505.0,
        "Question_title":"Accessing pickle file through API",
        "Question_body":"<p>I have a pickle file parameters.pkl containing some parameters and their values of a model. Is there a process to store it in Microsoft Azure Machine Learning Studio as endpoint. So that we can access the parameters and their values through API at some later stage.<\/p>\n<pre><code>The pickle file has been created through the following process:\n\ndict={'scaler': scaler,\n'features': z_tags,\n'Z_reconstruction_loss': Z_reconstruction_loss}\n\npickle.dump(dict, open('parameters.pkl', 'wb'))\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1660224417597,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":33.0,
        "Owner_creation_time":1431685926620,
        "Owner_last_access_time":1663756216812,
        "Owner_reputation":543.0,
        "Owner_up_votes":17.0,
        "Owner_down_votes":2.0,
        "Owner_views":125.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73321505",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: accessing pickle file through api; Content: i have a pickle file parameters.pkl containing some parameters and their values of a model. is there a process to store it in studio as endpoint. so that we can access the parameters and their values through api at some later stage. the pickle file has been created through the following process: dict={'scaler': scaler, 'features': z_tags, 'z_reconstruction_loss': z_reconstruction_loss} pickle.dump(dict, open('parameters.pkl', 'wb'))",
        "Question_original_content_gpt_summary":"The user is trying to access a pickle file containing parameters and their values of a model through an API.",
        "Question_preprocessed_content":"Title: accessing pickle file through api; Content: i have a pickle file containing some parameters and their values of a model. is there a process to store it in studio as endpoint. so that we can access the parameters and their values through api at some later stage.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"MLflow Release 0.3.0",
        "Question_body":"Hi mlflow-users,\n\n\nMLflow Release 0.3.0 is ready",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1532359549000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":20.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/oD6P1_dPotI",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2018-07-23T15:27:37",
                "Answer_body":"Eep! Sorry for the prematue send.\n\n\nMLflow Release 0.3.0 is ready, released 2018-07-18. The release is available on PyPI and docs are updated. Here are the release notes:\n\n\n\nBreaking changes:\n\n[MLflow Server] Renamed\u00a0--artifact-root\u00a0parameter to\u00a0--default-artifact-root\u00a0in\u00a0mlflow server\u00a0to better reflect its purpose (#165, @aarondav)\n\nFeatures:\n\nSpark MLlib integration: we now support logging SparkML Models directly in the log_model API, model format, and serving APIs (#72, @tomasatdatabricks)\nGoogle Cloud Storage is now supported as an artifact storage root (#152, @bnekolny)\nSupport asychronous\/parallel execution of MLflow runs (#82, @smurching)\n[SageMaker] Support for deleting, updating applications deployed via SageMaker (#145, @dbczumar)\n[SageMaker] Pushing the MLflow SageMaker container now includes the MLflow version that it was published with (#124, @sueann)\n[SageMaker] Simplify parameters to SageMaker deploy by providing sane defaults (#126, @sueann)\n[UI] One-element metrics are now displayed as a bar char (#118, @cryptexis)\n\nBug fixes:\n\nRequire gitpython>=2.1.0 (#98, @aarondav)\nFixed TensorFlow model loading so that columns match the output names of the exported model (#94, @smurching)\nFix SparkUDF when number of columns >= 10 (#97, @aarondav)\nMiscellaneous bug and documentation fixes from @emres, @dmatrix, @stbof, @gsganden, @dennyglee, @anabranch, @mikehuston, @andrewmchen, @juntai-zheng\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: release 0.3.0; Content: hi -users, release 0.3.0 is ready",
        "Question_original_content_gpt_summary":"The user is announcing the release of version 0.3.0 of their product.",
        "Question_preprocessed_content":"Title: release; Content: hi users, release is ready",
        "Answer_original_content":"eep! sorry for the prematue send. release 0.3.0 is ready, released 2018-07-18. the release is available on pypi and docs are updated. here are the release notes: breaking changes: [ server] renamed--artifact-rootparameter to--default-artifact-rootin serverto better reflect its purpose (#165, @aarondav) features: spark mllib integration: we now support logging sparkml models directly in the log_model api, model format, and serving apis (#72, @tomasatdatabricks) google cloud storage is now supported as an artifact storage root (#152, @bnekolny) support asychronous\/parallel execution of runs (#82, @smurching) [sagemaker] support for deleting, updating applications deployed via sagemaker (#145, @dbczumar) [sagemaker] pushing the sagemaker container now includes the version that it was published with (#124, @sueann) [sagemaker] simplify parameters to sagemaker deploy by providing sane defaults (#126, @sueann) [ui] one-element metrics are now displayed as a bar char (#118, @cryptexis) bug fixes: require gitpython>=2.1.0 (#98, @aarondav) fixed tensorflow model loading so that columns match the output names of the exported model (#94, @smurching) fix sparkudf when number of columns >= 10 (#97, @aarondav) miscellaneous bug and documentation fixes from @emres, @dmatrix, @stbof, @gsganden, @dennyglee, @anabranch, @mikehuston, @andrewmchen, @juntai-zheng",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"eep! sorry for the prematue send. release is ready, released . the release is available on pypi and docs are updated. here are the release notes breaking changes server renamed artifact rootparameter to default artifact rootin serverto better reflect its purpose features spark mllib integration we now support logging sparkml models directly in the api, model format, and serving apis google cloud storage is now supported as an artifact storage root support execution of runs sagemaker support for deleting, updating applications deployed via sagemaker sagemaker pushing the sagemaker container now includes the version that it was published with sagemaker simplify parameters to sagemaker deploy by providing sane defaults ui one element metrics are now displayed as a bar char bug fixes require fixed tensorflow model loading so that columns match the output names of the exported model fix sparkudf when number of columns > miscellaneous bug and documentation fixes from"
    },
    {
        "Question_id":57547110.0,
        "Question_title":"How to load file in sagemaker custom deploy endpoint script",
        "Question_body":"<p>I am trying to deploy a sentiment analysis model on sagemaker to an endpoint to predict sentiment in real time of an input text. This model will take a single text String as input and return the sentiment.<\/p>\n\n<p>To train the xgboost model, I followed this <a href=\"https:\/\/github.com\/NadimKawwa\/sagemaker_ml\/blob\/master\/SageMaker_IMDB_highlevel.ipynb\" rel=\"nofollow noreferrer\"> notebook<\/a> upto step 23. \nThis uploaded model.tar.gz to s3 bucket. I additionally uploaded vocabulary_dict generated by sklearn's CountVectorizer(to create bag of words)to s3 bucket as well. <\/p>\n\n<p>To deploy this pre-trained model, I can use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-endpoints-from-model-data\" rel=\"nofollow noreferrer\">this method<\/a> and supply an entry point python file predict.py.<\/p>\n\n<pre><code>sklearn_model = SKLearnModel(model_data=\"s3:\/\/bucket\/model.tar.gz\", role=\"SageMakerRole\", entry_point=\"predict.py\")\n<\/code><\/pre>\n\n<p>Documentation says that I have to provide model.tar.gz only as argument and it will be loaded in model_fn. But if I am writing my own model_fn, how do I load the model then? If I put additional files in the same directory as of model.tar.gz in S3, can I load them as well?<\/p>\n\n<p>Now to do the classification, I will have to vectorize the input text before calling model.predict(bow_vector) in the method predict_fn. In order to do that, I need word_dict which I prepared during pre-processing training data and wrote to s3. <\/p>\n\n<p>My question is how do I get the word_dict inside the model_fn? Can I load it from s3? \nBelow is code for predict.py.<\/p>\n\n<pre><code>import os\nimport re\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nfrom bs4 import BeautifulSoup\nimport sagemaker_containers\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n\ndef model_fn(model_dir):\n\n    #TODO How to load the word_dict.\n    #TODO How to load the model.\n    return model, word_dict\n\ndef predict_fn(input_data, model):\n    print('Inferring sentiment of input data.')\n    trained_model, word_dict = model\n    if word_dict is None:\n        raise Exception('Model has not been loaded properly, no word_dict.')\n\n    #Process input_data so that it is ready to be sent to our model.\n\n    input_bow_csv = process_input_text(word_dict, input_data)\n    prediction = trained_model.predict(input_bow_csv)\n    return prediction\n\n\ndef process_input_text(word_dict, input_data):\n\n    words = text_to_words(input_data);\n    vectorizer = CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, word_dict)\n    bow_array = vectorizer.transform([words]).toarray()[0]\n    bow_csv = \",\".join(str(bit) for bit in bow_array)\n    return bow_csv\n\ndef text_to_words(text):\n    \"\"\"\n    Uses the Porter Stemmer to stem words in a review\n    \"\"\"\n    #instantiate stemmer\n    stemmer = PorterStemmer()\n    text_nohtml = BeautifulSoup(text, \"html.parser\").get_text() # Remove HTML tags\n    text_lower = re.sub(r\"[^a-zA-Z0-9]\", \" \", text_nohtml.lower()) # Convert to lower case\n    words = text_lower.split() # Split string into words\n    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n    words = [PorterStemmer().stem(w) for w in words] # stem\n    return words\n\ndef input_fn(input_data, content_type):\n    return input_data;\n\ndef output_fn(prediction_output, accept):\n    return prediction_output;\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1566151471257,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":676.0,
        "Owner_creation_time":1566093196300,
        "Owner_last_access_time":1569443278816,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57547110",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to load file in custom deploy endpoint script; Content: i am trying to deploy a sentiment analysis model on to an endpoint to predict sentiment in real time of an input text. this model will take a single text string as input and return the sentiment. to train the xgboost model, i followed this notebook upto step 23. this uploaded model.tar.gz to s3 bucket. i additionally uploaded vocabulary_dict generated by sklearn's countvectorizer(to create bag of words)to s3 bucket as well. to deploy this pre-trained model, i can use this method and supply an entry point python file predict.py. sklearn_model = sklearnmodel(model_data=\"s3:\/\/bucket\/model.tar.gz\", role=\"role\", entry_point=\"predict.py\") documentation says that i have to provide model.tar.gz only as argument and it will be loaded in model_fn. but if i am writing my own model_fn, how do i load the model then? if i put additional files in the same directory as of model.tar.gz in s3, can i load them as well? now to do the classification, i will have to vectorize the input text before calling model.predict(bow_vector) in the method predict_fn. in order to do that, i need word_dict which i prepared during pre-processing training data and wrote to s3. my question is how do i get the word_dict inside the model_fn? can i load it from s3? below is code for predict.py. import os import re import pickle import numpy as np import pandas as pd import nltk nltk.download(\"stopwords\") from nltk.corpus import stopwords from nltk.stem.porter import * from bs4 import beautifulsoup import _containers from sklearn.feature_extraction.text import countvectorizer def model_fn(model_dir): #todo how to load the word_dict. #todo how to load the model. return model, word_dict def predict_fn(input_data, model): print('inferring sentiment of input data.') trained_model, word_dict = model if word_dict is none: raise exception('model has not been loaded properly, no word_dict.') #process input_data so that it is ready to be sent to our model. input_bow_csv = process_input_text(word_dict, input_data) prediction = trained_model.predict(input_bow_csv) return prediction def process_input_text(word_dict, input_data): words = text_to_words(input_data); vectorizer = countvectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x, word_dict) bow_array = vectorizer.transform([words]).toarray()[0] bow_csv = \",\".join(str(bit) for bit in bow_array) return bow_csv def text_to_words(text): \"\"\" uses the porter stemmer to stem words in a review \"\"\" #instantiate stemmer stemmer = porterstemmer() text_nohtml = beautifulsoup(text, \"html.parser\").get_text() # remove html tags text_lower = re.sub(r\"[^a-za-z0-9]\", \" \", text_nohtml.lower()) # convert to lower case words = text_lower.split() # split string into words words = [w for w in words if w not in stopwords.words(\"english\")] # remove stopwords words = [porterstemmer().stem(w) for w in words] # stem return words def input_fn(input_data, content_type): return input_data; def output_fn(prediction_output, accept): return prediction_output;",
        "Question_original_content_gpt_summary":"The user is encountering challenges in loading a pre-trained model and a vocabulary dictionary from an S3 bucket into a custom deploy endpoint script in order to perform sentiment analysis on an input text.",
        "Question_preprocessed_content":"Title: how to load file in custom deploy endpoint script; Content: i am trying to deploy a sentiment analysis model on to an endpoint to predict sentiment in real time of an input text. this model will take a single text string as input and return the sentiment. to train the xgboost model, i followed this notebook upto step . this uploaded to s bucket. i additionally uploaded generated by sklearn's countvectorizer to s bucket as well. to deploy this pre trained model, i can use this method and supply an entry point python file documentation says that i have to provide only as argument and it will be loaded in but if i am writing my own how do i load the model then? if i put additional files in the same directory as of in s , can i load them as well? now to do the classification, i will have to vectorize the input text before calling in the method in order to do that, i need which i prepared during pre processing training data and wrote to s . my question is how do i get the inside the can i load it from s ? below is code for",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":57032344.0,
        "Question_title":"Unable to get AWS SageMaker to read RecordIO files",
        "Question_body":"<p>I'm trying to convert an object detection <code>lst<\/code> file to a <code>rec<\/code> file and train with it in SageMaker.  My list looks something like this:<\/p>\n\n<pre><code>10  2   5   9.0000  1008.0000   1774.0000   1324.0000   1953.0000   3.0000  2697.0000   3340.0000   948.0000    1559.0000   0.0000  0.0000  0.0000  0.0000  0.0000  IMG_1091.JPG\n58  2   5   11.0000 1735.0000   2065.0000   1047.0000   1300.0000   6.0000  2444.0000   2806.0000   1194.0000   1482.0000   1.0000  2975.0000   3417.0000   1739.0000   2139.0000   IMG_7000.JPG\n60  2   5   12.0000 1243.0000   1861.0000   1222.0000   1710.0000   6.0000  2423.0000   2971.0000   1205.0000   1693.0000   0.0000  0.0000  0.0000  0.0000  0.0000  IMG_7061.JPG\n80  2   5   1.0000  1865.0000   2146.0000   818.0000    969.0000    14.0000 1559.0000   1918.0000   1658.0000   1914.0000   6.0000  2638.0000   3042.0000   2125.0000   2490.0000   IMG_9479.JPG\n79  2   5   13.0000 1556.0000   1812.0000   1440.0000   1637.0000   7.0000  2216.0000   2452.0000   1595.0000   1816.0000   0.0000  0.0000  0.0000  0.0000  0.0000  IMG_9443.JPG\n<\/code><\/pre>\n\n<p>Where the columns are <\/p>\n\n<p><code>index, header length, object length, class id, xmin, ymin, xmax, ymax, (repeat any other ids...), image path<\/code><\/p>\n\n<p>I then run the list through <code>im2rec<\/code> with <\/p>\n\n<p><code>$ \/incubator-mxnet\/tools\/im2rec.py my_lst.lst my_image_folder<\/code><\/p>\n\n<p>I then upload the resultant <code>.rec<\/code> file to s3.<\/p>\n\n<p>I then pull the necessary parts from <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/object_detection_birds\/object_detection_birds.ipynb\" rel=\"nofollow noreferrer\">this AWS sample notebook.<\/a><\/p>\n\n<p>I think the only key piece is probably this:<\/p>\n\n<pre><code>def set_hyperparameters(num_epochs, lr_steps):\n    num_classes = 16\n    num_training_samples = 227\n    print('num classes: {}, num training images: {}'.format(num_classes, num_training_samples))\n\n    od_model.set_hyperparameters(base_network='resnet-50',\n                                 use_pretrained_model=1,\n                                 num_classes=num_classes,\n                                 mini_batch_size=16,\n                                 epochs=num_epochs,               \n                                 learning_rate=0.001, \n                                 lr_scheduler_step=lr_steps,      \n                                 lr_scheduler_factor=0.1,\n                                 optimizer='sgd',\n                                 momentum=0.9,\n                                 weight_decay=0.0005,\n                                 overlap_threshold=0.5,\n                                 nms_threshold=0.45,\n                                 image_shape=512,\n                                 label_width=350,\n                                 num_training_samples=num_training_samples)\n\nset_hyperparameters(100, '33,67')\n<\/code><\/pre>\n\n<p>Ultimately I get the error: <code>Not enough label packed in img_list or rec file.<\/code><\/p>\n\n<p>Can someone help me identify what parts I'm missing in order to properly train with SageMaker and RecordIO files?<\/p>\n\n<p>Thanks for your help!<\/p>\n\n<p>Also, if I instead use<\/p>\n\n<p><code>$ \/incubator-mxnet\/tools\/im2rec.py my_lst.lst my_image_folder --pass-through --pack-label<\/code><\/p>\n\n<p>I get the error:<\/p>\n\n<p><code>Expected number of batches: 14, did not match the number of batches processed: 5. This may happen when some images or annotations are invalid and cannot be parsed. Please check the dataset and ensure it follows the format in the documentation.<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1563153744727,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":492.0,
        "Owner_creation_time":1360164540016,
        "Owner_last_access_time":1663899252396,
        "Owner_reputation":11190.0,
        "Owner_up_votes":136.0,
        "Owner_down_votes":11.0,
        "Owner_views":365.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Columbus, OH",
        "Question_last_edit_time":1563155807283,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57032344",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unable to get to read recordio files; Content: i'm trying to convert an object detection lst file to a rec file and train with it in . my list looks something like this: 10 2 5 9.0000 1008.0000 1774.0000 1324.0000 1953.0000 3.0000 2697.0000 3340.0000 948.0000 1559.0000 0.0000 0.0000 0.0000 0.0000 0.0000 img_1091.jpg 58 2 5 11.0000 1735.0000 2065.0000 1047.0000 1300.0000 6.0000 2444.0000 2806.0000 1194.0000 1482.0000 1.0000 2975.0000 3417.0000 1739.0000 2139.0000 img_7000.jpg 60 2 5 12.0000 1243.0000 1861.0000 1222.0000 1710.0000 6.0000 2423.0000 2971.0000 1205.0000 1693.0000 0.0000 0.0000 0.0000 0.0000 0.0000 img_7061.jpg 80 2 5 1.0000 1865.0000 2146.0000 818.0000 969.0000 14.0000 1559.0000 1918.0000 1658.0000 1914.0000 6.0000 2638.0000 3042.0000 2125.0000 2490.0000 img_9479.jpg 79 2 5 13.0000 1556.0000 1812.0000 1440.0000 1637.0000 7.0000 2216.0000 2452.0000 1595.0000 1816.0000 0.0000 0.0000 0.0000 0.0000 0.0000 img_9443.jpg where the columns are index, header length, object length, class id, xmin, ymin, xmax, ymax, (repeat any other ids...), image path i then run the list through im2rec with $ \/incubator-mxnet\/tools\/im2rec.py my_lst.lst my_image_folder i then upload the resultant .rec file to s3. i then pull the necessary parts from this aws sample notebook. i think the only key piece is probably this: def set_hyperparameters(num_epochs, lr_steps): num_classes = 16 num_training_samples = 227 print('num classes: {}, num training images: {}'.format(num_classes, num_training_samples)) od_model.set_hyperparameters(base_network='resnet-50', use_pretrained_model=1, num_classes=num_classes, mini_batch_size=16, epochs=num_epochs, learning_rate=0.001, lr_scheduler_step=lr_steps, lr_scheduler_factor=0.1, optimizer='sgd', momentum=0.9, weight_decay=0.0005, overlap_threshold=0.5, nms_threshold=0.45, image_shape=512, label_width=350, num_training_samples=num_training_samples) set_hyperparameters(100, '33,67') ultimately i get the error: not enough label packed in img_list or rec file. can someone help me identify what parts i'm missing in order to properly train with and recordio files? thanks for your help! also, if i instead use $ \/incubator-mxnet\/tools\/im2rec.py my_lst.lst my_image_folder --pass-through --pack-label i get the error: expected number of batches: 14, did not match the number of batches processed: 5. this may happen when some images or annotations are invalid and cannot be parsed. please check the dataset and ensure it follows the format in the documentation.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in converting an object detection lst file to a rec file and training with it in MXNet, resulting in errors such as \"not enough label packed in img_list or rec file\" and \"expected number of batches: 14, did not match the number of batches processed: 5\".",
        "Question_preprocessed_content":"Title: unable to get to read recordio files; Content: i'm trying to convert an object detection file to a file and train with it in . my list looks something like this where the columns are i then run the list through with i then upload the resultant file to s . i then pull the necessary parts from this aws sample notebook. i think the only key piece is probably this ultimately i get the error can someone help me identify what parts i'm missing in order to properly train with and recordio files? thanks for your help! also, if i instead use i get the error",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":47615177.0,
        "Question_title":"How can I use machine learning modules from Azure ML Studio in Azure ML Workbench",
        "Question_body":"<p>So I've been using Azure ML Studio for a while and now Microsoft have come up with the new tool called Azure ML Workbench.  The workbench seems to be pretty low level and it seems that the majority of functions need to be hand coded in Python.<\/p>\n\n<p>So if I have an experiment in Azure ML Studio using some of the drag and drop Training methods such as Matchbox and Boosted Decision Trees.  How can I convert these to run in Azure ML Workbench ?    <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1512274371540,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":192.0,
        "Owner_creation_time":1311047481463,
        "Owner_last_access_time":1663997641230,
        "Owner_reputation":2229.0,
        "Owner_up_votes":560.0,
        "Owner_down_votes":20.0,
        "Owner_views":240.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"French Polynesia",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47615177",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i use machine learning modules from studio in workbench; Content: so i've been using studio for a while and now microsoft have come up with the new tool called workbench. the workbench seems to be pretty low level and it seems that the majority of functions need to be hand coded in python. so if i have an experiment in studio using some of the drag and drop training methods such as matchbox and boosted decision trees. how can i convert these to run in workbench ?",
        "Question_original_content_gpt_summary":"The user is looking for a way to use machine learning modules from Studio in Workbench.",
        "Question_preprocessed_content":"Title: how can i use machine learning modules from studio in workbench; Content: so i've been using studio for a while and now microsoft have come up with the new tool called workbench. the workbench seems to be pretty low level and it seems that the majority of functions need to be hand coded in python. so if i have an experiment in studio using some of the drag and drop training methods such as matchbox and boosted decision trees. how can i convert these to run in workbench ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":61714177.0,
        "Question_title":"Viewing Ray Dashboard on AWS SageMaker (currently it's blank)",
        "Question_body":"<p>I am trying to use ray on AWS SageMaker in a Jupyter Notebook.<\/p>\n<p>The dashboard GUI not working for me, as it displays a blank page (rendered in browser)<\/p>\n<p>The Ray framework seems to run perhaps needs some debugging \/ evaluation.<\/p>\n<p>My goal is to access the dashboard feature. Note that I installed Ray with<\/p>\n<blockquote>\n<p>pip install ray[dashboard]<\/p>\n<\/blockquote>\n<p>When <code>ray.init()<\/code> is called, I got the message about accessing the dashboard at <code>localhost:8265<\/code>, which is equivalent to <code>https:\/\/&lt;aws-notebook-instance&gt;\/proxy\/8265\/<\/code> on AWS:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>ray.init(\n    num_cpus = num_cpus, \n    num_gpus = num_gpus,\n    ignore_reinit_error = True, \n    include_webui = None, \n    webui_host = 'localhost'\n)\n<\/code><\/pre>\n<p>After some testing, it seemed the dashboard API is there and responsive, but the browser displays only a blank page with embedded Javascript &amp; CSS objects (if viewed in dev mode).<\/p>\n<p>Looking in a GitHub repo, I tested actual API calls under the <code>\/api\/<\/code> path - and some of those return data in a raw format, through the browser didn't do what I intended.<\/p>\n<p>All I could think of is maybe some of the app framework or dependencies weren't installed on the host?<\/p>\n<p>My environment info can be found here: <a href=\"https:\/\/pastebin.com\/tsR0NB5t\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/tsR0NB5t<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1589123372793,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":697.0,
        "Owner_creation_time":1509720261312,
        "Owner_last_access_time":1637521352607,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"New Jersey, USA",
        "Question_last_edit_time":1596013416032,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61714177",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: viewing ray dashboard on (currently it's blank); Content: i am trying to use ray on in a jupyter notebook. the dashboard gui not working for me, as it displays a blank page (rendered in browser) the ray framework seems to run perhaps needs some debugging \/ evaluation. my goal is to access the dashboard feature. note that i installed ray with pip install ray[dashboard] when ray.init() is called, i got the message about accessing the dashboard at localhost:8265, which is equivalent to https:\/\/<aws-notebook-instance>\/proxy\/8265\/ on aws: ray.init( num_cpus = num_cpus, num_gpus = num_gpus, ignore_reinit_error = true, include_webui = none, webui_host = 'localhost' ) after some testing, it seemed the dashboard api is there and responsive, but the browser displays only a blank page with embedded javascript & css objects (if viewed in dev mode). looking in a github repo, i tested actual api calls under the \/api\/ path - and some of those return data in a raw format, through the browser didn't do what i intended. all i could think of is maybe some of the app framework or dependencies weren't installed on the host? my environment info can be found here: https:\/\/pastebin.com\/tsr0nb5t",
        "Question_original_content_gpt_summary":"The user is encountering challenges with viewing the Ray dashboard on a Jupyter notebook, as it displays a blank page despite the Ray framework running, and API calls returning data in a raw format.",
        "Question_preprocessed_content":"Title: viewing ray dashboard on; Content: i am trying to use ray on in a jupyter notebook. the dashboard gui not working for me, as it displays a blank page the ray framework seems to run perhaps needs some debugging \/ evaluation. my goal is to access the dashboard feature. note that i installed ray with pip install ray when is called, i got the message about accessing the dashboard at , which is equivalent to on aws after some testing, it seemed the dashboard api is there and responsive, but the browser displays only a blank page with embedded javascript & css objects . looking in a github repo, i tested actual api calls under the path and some of those return data in a raw format, through the browser didn't do what i intended. all i could think of is maybe some of the app framework or dependencies weren't installed on the host? my environment can be found here",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":54643081.0,
        "Question_title":"Amazon SageMaker Unsupported content-type application\/x-image",
        "Question_body":"<p>I have a tensorflow\/keras based CNN model deployed in Sagemaker.<\/p>\n\n<p>Now to invoke the inference, I followed this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/classify-your-own-images-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">tutorial<\/a><\/p>\n\n<p>Below code snippet<\/p>\n\n<pre><code>def inferImage(endpoint_name):\n    # Load the image bytes\n    img = open('.\/shoe.jpg', 'rb').read()\n    runtime = boto3.Session().client(service_name='sagemaker-runtime')\n\n    # Call your model for predicting which object appears in this image.\n    response = runtime.invoke_endpoint(\n        EndpointName=endpoint_name,\n        ContentType='application\/x-image',\n        Body=bytearray(img))\n    response_body = response['Body']\n    print(response_body.read()) \n<\/code><\/pre>\n\n<p>When I run this code, I get error<\/p>\n\n<p><code>Unsupported content-type application\/x-image<\/code><\/p>\n\n<p>What am I missing? Any suggestion on how to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6.0,
        "Question_creation_time":1549947506550,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":3204.0,
        "Owner_creation_time":1339474453300,
        "Owner_last_access_time":1660360250523,
        "Owner_reputation":2448.0,
        "Owner_up_votes":445.0,
        "Owner_down_votes":5.0,
        "Owner_views":337.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54643081",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unsupported content-type application\/x-image; Content: i have a tensorflow\/keras based cnn model deployed in . now to invoke the inference, i followed this tutorial below code snippet def inferimage(endpoint_name): # load the image bytes img = open('.\/shoe.jpg', 'rb').read() runtime = boto3.session().client(service_name='-runtime') # call your model for predicting which object appears in this image. response = runtime.invoke_endpoint( endpointname=endpoint_name, contenttype='application\/x-image', body=bytearray(img)) response_body = response['body'] print(response_body.read()) when i run this code, i get error unsupported content-type application\/x-image what am i missing? any suggestion on how to fix it?",
        "Question_original_content_gpt_summary":"The user encountered an error when attempting to invoke an inference on a tensorflow\/keras based cnn model deployed in AWS, due to an unsupported content-type application\/x-image.",
        "Question_preprocessed_content":"Title: unsupported content type; Content: i have a based cnn model deployed in . now to invoke the inference, i followed this tutorial below code snippet when i run this code, i get error what am i missing? any suggestion on how to fix it?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":71134673.0,
        "Question_title":"python ray tune unable to stop trial or experiment",
        "Question_body":"<p>I am trying to make ray tune with wandb stop the experiment under certain conditions.<\/p>\n<ul>\n<li>stop all experiment if any trial raises an Exception (so i can fix the code and resume)<\/li>\n<li>stop if my score gets -999<\/li>\n<li>stop if the variable <code>varcannotbezero<\/code> gets 0<\/li>\n<\/ul>\n<p><strong>The following things i tried all failed in achieving desired behavior:<\/strong><\/p>\n<ul>\n<li>stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0}<\/li>\n<li>max_failures=0<\/li>\n<li>defining a Stoper class did also not work<\/li>\n<\/ul>\n<pre><code>class RayStopper(Stopper):\n    def __init__(self):\n        self._start = time.time()\n        #self._deadline = 300\n    def __call__(self, trial_id, result):\n        self.score=result[&quot;score&quot;]\n        self.varcannotbezero=result[&quot;varcannotbezero&quot;]\n        return False\n    def stop_all(self):\n        if self.score==-999 or self.varcannotbezero==0:\n            return True\n        else:\n            return False\n<\/code><\/pre>\n<p>Ray tune just continues to run<\/p>\n<pre><code>    wandb_project=&quot;ABC&quot;\n    wandb_api_key=&quot;KEY&quot;\n    ray.init(configure_logging=False)\n\n    if current_best_params is None:\n        algo = HyperOptSearch()\n    else:\n        algo = HyperOptSearch(points_to_evaluate=current_best_params,n_initial_points=n_initial_points)\n    algo = ConcurrencyLimiter(algo, max_concurrent=1)\n\n    scheduler = AsyncHyperBandScheduler()\n    analysis = tune.run(\n        tune_obj,\n        name=&quot;Name&quot;,\n        resources_per_trial={&quot;cpu&quot;: 1},\n        search_alg=algo,\n        scheduler=scheduler,\n        metric=&quot;score&quot;,\n        mode=&quot;max&quot;,\n        num_samples=10,\n        stop={&quot;score&quot;:-999,&quot;varcannotbezero&quot;:0},\n        max_failures=0,\n        config=config,\n        callbacks=[WandbLoggerCallback(project=wandb_project,entity=&quot;mycompany&quot;,api_key=wandb_api_key,log_config=True)],\n        local_dir=local_dir,\n        resume=&quot;AUTO&quot;,\n        verbose=0\n    )\n\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1644967685327,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":284.0,
        "Owner_creation_time":1300741959900,
        "Owner_last_access_time":1664027459750,
        "Owner_reputation":2390.0,
        "Owner_up_votes":56.0,
        "Owner_down_votes":1.0,
        "Owner_views":308.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71134673",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: python ray tune unable to stop trial or experiment; Content: i am trying to make ray tune with stop the experiment under certain conditions. stop all experiment if any trial raises an exception (so i can fix the code and resume) stop if my score gets -999 stop if the variable varcannotbezero gets 0 the following things i tried all failed in achieving desired behavior: stop={\"score\":-999,\"varcannotbezero\":0} max_failures=0 defining a stoper class did also not work class raystopper(stopper): def __init__(self): self._start = time.time() #self._deadline = 300 def __call__(self, trial_id, result): self.score=result[\"score\"] self.varcannotbezero=result[\"varcannotbezero\"] return false def stop_all(self): if self.score==-999 or self.varcannotbezero==0: return true else: return false ray tune just continues to run _project=\"abc\" _api_key=\"key\" ray.init(configure_logging=false) if current_best_params is none: algo = hyperoptsearch() else: algo = hyperoptsearch(points_to_evaluate=current_best_params,n_initial_points=n_initial_points) algo = concurrencylimiter(algo, max_concurrent=1) scheduler = asynchyperbandscheduler() analysis = tune.run( tune_obj, name=\"name\", resources_per_trial={\"cpu\": 1}, search_alg=algo, scheduler=scheduler, metric=\"score\", mode=\"max\", num_samples=10, stop={\"score\":-999,\"varcannotbezero\":0}, max_failures=0, config=config, callbacks=[loggercallback(project=_project,entity=\"mycompany\",api_key=_api_key,log_config=true)], local_dir=local_dir, resume=\"auto\", verbose=0 )",
        "Question_original_content_gpt_summary":"The user is encountering challenges with Ray Tune, as they are unable to stop the experiment or trial under certain conditions, despite trying various methods.",
        "Question_preprocessed_content":"Title: python ray tune unable to stop trial or experiment; Content: i am trying to make ray tune with stop the experiment under certain conditions. stop all experiment if any trial raises an exception stop if my score gets stop if the variable gets the following things i tried all failed in achieving desired behavior stop score , varcannotbezero defining a stoper class did also not work ray tune just continues to run",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML private notebooks (multiple user\/project)",
        "Question_body":"Hello all,\n\nUse case:\nActually, I would like to prevent some users (guest user for example) from viewing some notebooks with sensitive data. I started by creating several Azure ML workspaces but given the limit of storage accounts (250 per subscription), I shared the same storage account for several workspaces... And I was surprised that the same notebooks were shared in all workspaces for all users.\n\nSo, I try to understand how to work on Azure Machine Learning while managing notebook access.\n1) Multiple workspace with multiple storage account = Limited in the long term as we have a lot of projects (max 250 storage acc)...\n2) Multiple workspace with same storage account = all notebooks shared...Impossible to work with guest users...\n\n\n\n\nBecause these two issues, I was wondering:\n- What is the way to limit access to some Azure Machine Learning Notebooks?\n- Is there any good practice about multiple project with AML workspaces ? (One workspace per project or one workspace per environment)\n\n\n\n\nKind regards,",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1637769772467,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/639765\/azure-ml-private-notebooks-multiple-userproject.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-24T16:30:28.203Z",
                "Answer_score":0,
                "Answer_body":"Hi, your notebooks are stored in your workspace's storage account, and can be shared with others, depending on their access level to your workspace. By default, your notebooks are in a folder with your username, and others can access them there. With Azure RBAC, you can manage access to your AML workspace. These are the default roles, however for your scenario, I recommend creating custom role.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: private notebooks (multiple user\/project); Content: hello all, use case: actually, i would like to prevent some users (guest user for example) from viewing some notebooks with sensitive data. i started by creating several workspaces but given the limit of storage accounts (250 per subscription), i shared the same storage account for several workspaces... and i was surprised that the same notebooks were shared in all workspaces for all users. so, i try to understand how to work on while managing notebook access. 1) multiple workspace with multiple storage account = limited in the long term as we have a lot of projects (max 250 storage acc)... 2) multiple workspace with same storage account = all notebooks shared...impossible to work with guest users... because these two issues, i was wondering: - what is the way to limit access to some notebooks? - is there any good practice about multiple project with aml workspaces ? (one workspace per project or one workspace per environment) kind regards,",
        "Question_original_content_gpt_summary":"The user is facing challenges in managing notebook access while working with multiple workspaces and storage accounts, as well as with guest users.",
        "Question_preprocessed_content":"Title: private notebooks; Content: hello all, use case actually, i would like to prevent some users from viewing some notebooks with sensitive data. i started by creating several workspaces but given the limit of storage accounts , i shared the same storage account for several and i was surprised that the same notebooks were shared in all workspaces for all users. so, i try to understand how to work on while managing notebook access. multiple workspace with multiple storage account limited in the long term as we have a lot of projects multiple workspace with same storage account all notebooks to work with guest because these two issues, i was wondering what is the way to limit access to some notebooks? is there any good practice about multiple project with aml workspaces ? kind regards,",
        "Answer_original_content":"hi, your notebooks are stored in your workspace's storage account, and can be shared with others, depending on their access level to your workspace. by default, your notebooks are in a folder with your username, and others can access them there. with azure rbac, you can manage access to your aml workspace. these are the default roles, however for your scenario, i recommend creating custom role. --- kindly accept answer if the information helps. thanks.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, your notebooks are stored in your workspace's storage account, and can be shared with others, depending on their access level to your workspace. by default, your notebooks are in a folder with your username, and others can access them there. with azure rbac, you can manage access to your aml workspace. these are the default roles, however for your scenario, i recommend creating custom role. kindly accept answer if the helps. thanks."
    },
    {
        "Question_id":null,
        "Question_title":"How do we create learning virtual machine AI assistants with smart home control and self-driving vehicle funtionality?",
        "Question_body":"I am developing Conscious Quantum Coding Living AI Virtual Assistants to help with everything.\n\nJodi, The AI Motor Home\nJodi will be an integrative, quantum coded, learning\/self-improving, online\/cloud, virtual machine, life conscious Living AI assistant who fully controls, and self drives, an RV\/Motor home\n\nHow would you create a Living AI assistant for a motor home?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1591266810177,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/31993\/how-do-we-create-learning-virtual-machine-ai-assis.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-04T23:06:32.373Z",
                "Answer_score":0,
                "Answer_body":"Thanks for reaching out. The Azure Bot Service may be useful for your scenario. I encourage you to check out our documentation on Virtual Assistant and Template Outline for best practices. There are also videos available to help you get started. Feel free to followup with any particular questions or concerns for the community to chime in. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":19.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do we create learning virtual machine ai assistants with smart home control and self-driving vehicle funtionality?; Content: i am developing conscious quantum coding living ai virtual assistants to help with everything. jodi, the ai motor home jodi will be an integrative, quantum coded, learning\/self-improving, online\/cloud, virtual machine, life conscious living ai assistant who fully controls, and self drives, an rv\/motor home how would you create a living ai assistant for a motor home?",
        "Question_original_content_gpt_summary":"The user is developing a conscious quantum-coded, learning and self-improving virtual machine AI assistant to help with everything, including controlling and self-driving an RV\/motor home.",
        "Question_preprocessed_content":"Title: how do we create learning virtual machine ai assistants with smart home control and self driving vehicle funtionality?; Content: i am developing conscious quantum coding living ai virtual assistants to help with everything. jodi, the ai motor home jodi will be an integrative, quantum coded, virtual machine, life conscious living ai assistant who fully controls, and self drives, an home how would you create a living ai assistant for a motor home?",
        "Answer_original_content":"thanks for reaching out. the azure bot service may be useful for your scenario. i encourage you to check out our documentation on virtual assistant and template outline for best practices. there are also videos available to help you get started. feel free to followup with any particular questions or concerns for the community to chime in. thanks.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for reaching out. the azure bot service may be useful for your scenario. i encourage you to check out our documentation on virtual assistant and template outline for best practices. there are also videos available to help you get started. feel free to followup with any particular questions or concerns for the community to chime in. thanks."
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Notebook keeps hanging\/freezing",
        "Question_body":"I have been using Sagemaker Studio Notebook and suddenly it started hanging. When this happens, the notebook freezes completely. Than I have to wait some seconds (the delay duration is not constant and is common to reach about 30 seconds) and then it just freezes again, making its usage impossible. I was using a temporary account provided by Udacity and after trying different approaches to find and solve the problem, I switched to a personal account but the problem persists. Approaches I have tried so far:\n\nShutdow and start kernel\nRestart kernel\nRestart kernel and clear outputs\nLog out and Login (from Sagemaker)\nLog out and Login (from AWS)\nChange region\nTrying a different browser (I tried Chrome and Firefox)\nTrying using other account (personal)\n\nI also checked CloudWatch logs but didn't find anything that seemed unusual.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1657750167416,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":50.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbUkR0L2-Q1CcAHtTbLYJmg\/sagemaker-notebook-keeps-hanging-freezing",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-15T03:52:28.682Z",
                "Answer_score":1,
                "Answer_body":"The most likely cause of this from my experience is a (very) large number of active git changes.\n\nGiven your \"current\" working folder (the one you're navigated to in the folder sidebar menu), the jupyterlab-git integration regularly checks if you're inside a git repository and polls for changes in that repository if so.\n\nWhen this list is very large, I've sometimes seen it cause significant slowdowns in the overall UI because of the way the underlying (open-source) extension works. This has been discussed before for example in this GitHub issue - which is now marked closed but I've still seen it happening.\n\nFor example, maybe you (like me \ud83d\ude05) forgot to gitignore a data folder or node_modules and generated thousands of untracked files there: You might see a significant slowdown whenever you're navigated to a folder within the scope of that git repo.\n\nSuggested solution would be:\n\nUse the folder sidebar to navigate anywhere other than the affected git repository (e.g. to your root folder?), and you should see the slowdown resolve pretty much immediately if this is the underlying cause\nNow the tricky task of finding and clearing up the problemmatic folder(s) without navigating to them in the folder GUI:\nYou could use a System Terminal, cd to the affected folder and run git status to see where the many changes are hiding, if you're not sure already\nAdd a .gitignore file (or modify your existing one) to make git ignore those changes. Because it starts with a dot, .gitignore is hidden by default in the JupyterLab file browser anyway. I usually use a system terminal to e.g. cp myrepo\/.gitignore gitignore.txt to create a visible copy (somewhere other than the repository folder which you're trying to avoid navigating to!) and then mv gitignore.txt myrepo\/.gitignore to overwrite with my edited version\n\nAlternatively (if e.g. it's a folder full of new files that you no longer care about like node_modules) you could just slog through the slowness to delete the problemmatic folder in the UI - but of course the problem would return if you re-created them later without .gitignore.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: notebook keeps hanging\/freezing; Content: i have been using studio notebook and suddenly it started hanging. when this happens, the notebook freezes completely. than i have to wait some seconds (the delay duration is not constant and is common to reach about 30 seconds) and then it just freezes again, making its usage impossible. i was using a temporary account provided by udacity and after trying different approaches to find and solve the problem, i switched to a personal account but the problem persists. approaches i have tried so far: shutdow and start kernel restart kernel restart kernel and clear outputs log out and login (from ) log out and login (from aws) change region trying a different browser (i tried chrome and firefox) trying using other account (personal) i also checked cloudwatch logs but didn't find anything that seemed unusual.",
        "Question_original_content_gpt_summary":"The user has encountered a challenge with their notebook freezing and has tried multiple approaches to solve the issue, including switching to a personal account, but the problem persists.",
        "Question_preprocessed_content":"Title: notebook keeps; Content: i have been using studio notebook and suddenly it started hanging. when this happens, the notebook freezes completely. than i have to wait some seconds and then it just freezes again, making its usage impossible. i was using a temporary account provided by udacity and after trying different approaches to find and solve the problem, i switched to a personal account but the problem persists. approaches i have tried so far shutdow and start kernel restart kernel restart kernel and clear outputs log out and login log out and login change region trying a different browser trying using other account i also checked cloudwatch logs but didn't find anything that seemed unusual.",
        "Answer_original_content":"the most likely cause of this from my experience is a (very) large number of active git changes. given your \"current\" working folder (the one you're navigated to in the folder sidebar menu), the jupyterlab-git integration regularly checks if you're inside a git repository and polls for changes in that repository if so. when this list is very large, i've sometimes seen it cause significant slowdowns in the overall ui because of the way the underlying (open-source) extension works. this has been discussed before for example in this github issue - which is now marked closed but i've still seen it happening. for example, maybe you (like me ) forgot to gitignore a data folder or node_modules and generated thousands of untracked files there: you might see a significant slowdown whenever you're navigated to a folder within the scope of that git repo. suggested solution would be: use the folder sidebar to navigate anywhere other than the affected git repository (e.g. to your root folder?), and you should see the slowdown resolve pretty much immediately if this is the underlying cause now the tricky task of finding and clearing up the problemmatic folder(s) without navigating to them in the folder gui: you could use a system terminal, cd to the affected folder and run git status to see where the many changes are hiding, if you're not sure already add a .gitignore file (or modify your existing one) to make git ignore those changes. because it starts with a dot, .gitignore is hidden by default in the jupyterlab file browser anyway. i usually use a system terminal to e.g. cp myrepo\/.gitignore gitignore.txt to create a visible copy (somewhere other than the repository folder which you're trying to avoid navigating to!) and then mv gitignore.txt myrepo\/.gitignore to overwrite with my edited version alternatively (if e.g. it's a folder full of new files that you no longer care about like node_modules) you could just slog through the slowness to delete the problemmatic folder in the ui - but of course the problem would return if you re-created them later without .gitignore.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"the most likely cause of this from my experience is a large number of active git changes. given your current working folder , the jupyterlab git integration regularly checks if you're inside a git repository and polls for changes in that repository if so. when this list is very large, i've sometimes seen it cause significant slowdowns in the overall ui because of the way the underlying extension works. this has been discussed before for example in this github issue which is now marked closed but i've still seen it happening. for example, maybe you forgot to gitignore a data folder or and generated thousands of untracked files there you might see a significant slowdown whenever you're navigated to a folder within the scope of that git repo. suggested solution would be use the folder sidebar to navigate anywhere other than the affected git repository , and you should see the slowdown resolve pretty much immediately if this is the underlying cause now the tricky task of finding and clearing up the problemmatic folder without navigating to them in the folder gui you could use a system terminal, cd to the affected folder and run git status to see where the many changes are hiding, if you're not sure already add a .gitignore file to make git ignore those changes. because it starts with a dot, .gitignore is hidden by default in the jupyterlab file browser anyway. i usually use a system terminal to cp to create a visible copy and then mv to overwrite with my edited version alternatively you could just slog through the slowness to delete the problemmatic folder in the ui but of course the problem would return if you re created them later without .gitignore."
    },
    {
        "Question_id":68192602.0,
        "Question_title":"Sagemaker - batch transform] Internal server error : 500",
        "Question_body":"<p>I am trying to do a batch transform on a training dataset in an S3 bucket. I have followed this link:\n<a href=\"https:\/\/github.com\/aws-samples\/quicksight-sagemaker-integration-blog\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/quicksight-sagemaker-integration-blog<\/a><\/p>\n<p>The training data on which transformation is being applied is of ~35 MB.\nI am getting these errors:<\/p>\n<ol>\n<li>Bad HTTP status received from algorithm: 500<\/li>\n<li>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.<\/li>\n<\/ol>\n<p>Process followed:<\/p>\n<pre><code>1. s3_input_train = sagemaker.TrainingInput(s3_data='s3:\/\/{}\/{}\/rawtrain\/'.format(bucket, prefix), content_type='csv')\n\n2. from sagemaker.sklearn.estimator import SKLearn\nsagemaker_session = sagemaker.Session()\nscript_path = 'preprocessing.py'\nsklearn_preprocessor = SKLearn(\nentry_point=script_path,\nrole=role,\ntrain_instance_type=&quot;ml.c4.xlarge&quot;,\nframework_version='0.20.0',\npy_version = 'py3',\nsagemaker_session=sagemaker_session)\nsklearn_preprocessor.fit({'train': s3_input_train})\n\n3. transform_train_output_path = 's3:\/\/{}\/{}\/{}\/'.format(bucket, prefix, 'transformtrain-train-output')\nscikit_learn_inferencee_model = sklearn_preprocessor.create_model(env={'TRANSFORM_MODE': 'feature-transform'})\ntransformer_train = scikit_learn_inferencee_model.transformer(\ninstance_count=1,\nassemble_with = 'Line',\noutput_path = transform_train_output_path,\naccept = 'text\/csv',\nstrategy = &quot;MultiRecord&quot;,\nmax_payload =40,\ninstance_type='ml.m4.xlarge')\n\n4. Preprocess training input\ntransformer_train.transform(s3_input_train.config['DataSource']['S3DataSource']['S3Uri'], \n                            content_type='text\/csv',\n                            split_type = &quot;Line&quot;)\nprint('Waiting for transform job: ' + transformer_train.latest_transform_job.job_name)\ntransformer_train.wait()\npreprocessed_train_path = transformer_train.output_path + transformer_train.latest_transform_job.job_name\n\npreprocessing.py\n\nfrom __future__ import print_function\n\nimport time\nimport sys\nfrom io import StringIO\nimport os\nimport shutil\n\nimport argparse\nimport csv\nimport json\nimport numpy as np\nimport pandas as pd\nimport logging\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.externals import joblib\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Binarizer, StandardScaler, OneHotEncoder\n\nfrom sagemaker_containers.beta.framework import (\n    content_types, encoders, env, modules, transformer, worker)\n\n# Specifying the column names here.\nfeature_columns_names = [\n    'A',\n    'B',\n    'C',\n    'D',\n    'E',\n    'F',\n    'G',\n    'H',\n    'I',\n    'J',\n    'K'\n] \n\nlabel_column = 'ab'\n\nfeature_columns_dtype = {\n    'A' :  str,\n    'B' :  np.float64,\n    'C' :  np.float64,\n    'D' :  str,\n    &quot;E&quot; :  np.float64,\n    'F' :  str,\n    'G' :  str,\n    'H' :  np.float64,\n    'I' :  str,\n    'J' :  str,\n    'K':  str,\n}\n\nlabel_column_dtype = {'ab': np.int32}  \n\ndef merge_two_dicts(x, y):\n    z = x.copy()   # start with x's keys and values\n    z.update(y)    # modifies z with y's keys and values &amp; returns None\n    return z\n\ndef _is_inverse_label_transform():\n    &quot;&quot;&quot;Returns True if if it's running in inverse label transform.&quot;&quot;&quot;\n    return os.getenv('TRANSFORM_MODE') == 'inverse-label-transform'\n\ndef _is_feature_transform():\n    &quot;&quot;&quot;Returns True if it's running in feature transform mode.&quot;&quot;&quot;\n    return os.getenv('TRANSFORM_MODE') == 'feature-transform'\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n\n    # Sagemaker specific arguments. Defaults are set in the environment variables.\n    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n\n\n    args = parser.parse_args()\n\n    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n    if len(input_files) == 0:\n        raise ValueError(('There are no files in {}.\\n' +\n                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n                          'does not have permission to access the data.').format(args.train, &quot;train&quot;))\n\n    raw_data = [ pd.read_csv(\n        file, \n        header=None,\n        names=feature_columns_names + [label_column],\n        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype)) for file in input_files ]\n    concat_data = pd.concat(raw_data)\n    \n    numeric_features = list([\n    'B',\n    'C',\n    'E',\n    'H'\n    ])\n\n\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())])\n\n    categorical_features = list(['A','D','F','G','I','J','K'])\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numeric_features),\n            ('cat', categorical_transformer, categorical_features)],\n        remainder=&quot;drop&quot;)\n\n    preprocessor.fit(concat_data)\n\n    joblib.dump(preprocessor, os.path.join(args.model_dir, &quot;model.joblib&quot;))\n\n    print(&quot;saved model!&quot;)\n    \n    \ndef input_fn(input_data, request_content_type):\n    &quot;&quot;&quot;Parse input data payload\n    \n    We currently only take csv input. Since we need to process both labelled\n    and unlabelled data we first determine whether the label column is present\n    by looking at how many columns were provided.\n    &quot;&quot;&quot;\n    \n    \n    content_type = request_content_type.lower(\n    ) if request_content_type else &quot;text\/csv&quot;\n    content_type = content_type.split(&quot;;&quot;)[0].strip()\n    \n    \n    if isinstance(input_data, str):\n        str_buffer = input_data\n    else:\n        str_buffer = str(input_data,'utf-8')\n    \n\n    if _is_feature_transform():\n        if content_type == 'text\/csv':\n            # Read the raw input data as CSV.\n            df = pd.read_csv(StringIO(input_data),  header=None)\n            if len(df.columns) == len(feature_columns_names) + 1:\n                # This is a labelled example, includes the  label\n                df.columns = feature_columns_names + [label_column]\n            elif len(df.columns) == len(feature_columns_names):\n                # This is an unlabelled example.\n                df.columns = feature_columns_names\n            return df\n        else:\n            raise ValueError(&quot;{} not supported by script!&quot;.format(content_type))\n    \n    \n    if _is_inverse_label_transform():\n        if (content_type == 'text\/csv' or content_type == 'text\/csv; charset=utf-8'):\n            # Read the raw input data as CSV.\n            df = pd.read_csv(StringIO(str_buffer),  header=None)\n            if len(df.columns) == len(feature_columns_names) + 1:\n            # This is a labelled example, includes the ring label\n               df.columns = feature_columns_names + [label_column]\n            elif len(df.columns) == len(feature_columns_names):\n            # This is an unlabelled example.\n               df.columns = feature_columns_names\n            return df\n        else:\n            raise ValueError(&quot;{} not supported by script!&quot;.format(content_type))\n            \n            \ndef output_fn(prediction, accept):\n    &quot;&quot;&quot;Format prediction output\n    \n    The default accept\/content-type between containers for serial inference is JSON.\n    We also want to set the ContentType or mimetype as the same value as accept so the next\n    container can read the response payload correctly.\n    &quot;&quot;&quot;\n    \n    accept = 'text\/csv'\n    if type(prediction) is not np.ndarray:\n        prediction=prediction.toarray()\n    \n   \n    if accept == &quot;application\/json&quot;:\n        instances = []\n        for row in prediction.tolist():\n            instances.append({&quot;features&quot;: row})\n\n        json_output = {&quot;instances&quot;: instances}\n\n        return worker.Response(json.dumps(json_output), mimetype=accept)\n    elif accept == 'text\/csv':\n        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n    else:\n        raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n\n\ndef predict_fn(input_data, model):\n    &quot;&quot;&quot;Preprocess input data\n    \n    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n    so we want to use .transform().\n\n    The output is returned in the following order:\n    \n        rest of features either one hot encoded or standardized\n    &quot;&quot;&quot;\n\n    \n    if _is_feature_transform():\n        features = model.transform(input_data)\n\n\n        if label_column in input_data:\n            # Return the label (as the first column) and the set of features.\n            return np.insert(features.toarray(), 0, pd.get_dummies(input_data[label_column])['True.'], axis=1)\n        else:\n            # Return only the set of features\n            return features\n    \n    if _is_inverse_label_transform():\n        features = input_data.iloc[:,0]&gt;0.5\n        features = features.values\n        return features\n    \n\ndef model_fn(model_dir):\n    &quot;&quot;&quot;Deserialize fitted model\n    &quot;&quot;&quot;\n    if _is_feature_transform():\n        preprocessor = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n        return preprocessor\n<\/code><\/pre>\n<p>Please help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1625047257630,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":596.0,
        "Owner_creation_time":1520579005870,
        "Owner_last_access_time":1663953474452,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":16.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Cebu, Philippines",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68192602",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - batch transform] internal server error : 500; Content: i am trying to do a batch transform on a training dataset in an s3 bucket. i have followed this link: https:\/\/github.com\/aws-samples\/quicksight--integration-blog the training data on which transformation is being applied is of ~35 mb. i am getting these errors: bad http status received from algorithm: 500 the server encountered an internal error and was unable to complete your request. either the server is overloaded or there is an error in the application. process followed: 1. s3_input_train = .traininginput(s3_data='s3:\/\/{}\/{}\/rawtrain\/'.format(bucket, prefix), content_type='csv') 2. from .sklearn.estimator import sklearn _session = .session() script_path = 'preprocessing.py' sklearn_preprocessor = sklearn( entry_point=script_path, role=role, train_instance_type=\"ml.c4.xlarge\", framework_version='0.20.0', py_version = 'py3', _session=_session) sklearn_preprocessor.fit({'train': s3_input_train}) 3. transform_train_output_path = 's3:\/\/{}\/{}\/{}\/'.format(bucket, prefix, 'transformtrain-train-output') scikit_learn_inferencee_model = sklearn_preprocessor.create_model(env={'transform_mode': 'feature-transform'}) transformer_train = scikit_learn_inferencee_model.transformer( instance_count=1, assemble_with = 'line', output_path = transform_train_output_path, accept = 'text\/csv', strategy = \"multirecord\", max_payload =40, instance_type='ml.m4.xlarge') 4. preprocess training input transformer_train.transform(s3_input_train.config['datasource']['s3datasource']['s3uri'], content_type='text\/csv', split_type = \"line\") print('waiting for transform job: ' + transformer_train.latest_transform_job.job_name) transformer_train.wait() preprocessed_train_path = transformer_train.output_path + transformer_train.latest_transform_job.job_name preprocessing.py from __future__ import print_function import time import sys from io import stringio import os import shutil import argparse import csv import json import numpy as np import pandas as pd import logging from sklearn.compose import columntransformer from sklearn.externals import joblib from sklearn.impute import simpleimputer from sklearn.pipeline import pipeline from sklearn.preprocessing import binarizer, standardscaler, onehotencoder from _containers.beta.framework import ( content_types, encoders, env, modules, transformer, worker) # specifying the column names here. feature_columns_names = [ 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k' ] label_column = 'ab' feature_columns_dtype = { 'a' : str, 'b' : np.float64, 'c' : np.float64, 'd' : str, \"e\" : np.float64, 'f' : str, 'g' : str, 'h' : np.float64, 'i' : str, 'j' : str, 'k': str, } label_column_dtype = {'ab': np.int32} def merge_two_dicts(x, y): z = x.copy() # start with x's keys and values z.update(y) # modifies z with y's keys and values & returns none return z def _is_inverse_label_transform(): \"\"\"returns true if if it's running in inverse label transform.\"\"\" return os.getenv('transform_mode') == 'inverse-label-transform' def _is_feature_transform(): \"\"\"returns true if it's running in feature transform mode.\"\"\" return os.getenv('transform_mode') == 'feature-transform' if __name__ == '__main__': parser = argparse.argumentparser() # specific arguments. defaults are set in the environment variables. parser.add_argument('--output-data-dir', type=str, default=os.environ['sm_output_data_dir']) parser.add_argument('--model-dir', type=str, default=os.environ['sm_model_dir']) parser.add_argument('--train', type=str, default=os.environ['sm_channel_train']) args = parser.parse_args() input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ] if len(input_files) == 0: raise valueerror(('there are no files in {}.\\n' + 'this usually indicates that the channel ({}) was incorrectly specified,\\n' + 'the data specification in s3 was incorrectly specified or the role specified\\n' + 'does not have permission to access the data.').format(args.train, \"train\")) raw_data = [ pd.read_csv( file, header=none, names=feature_columns_names + [label_column], dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype)) for file in input_files ] concat_data = pd.concat(raw_data) numeric_features = list([ 'b', 'c', 'e', 'h' ]) numeric_transformer = pipeline(steps=[ ('imputer', simpleimputer(strategy='median')), ('scaler', standardscaler())]) categorical_features = list(['a','d','f','g','i','j','k']) categorical_transformer = pipeline(steps=[ ('imputer', simpleimputer(strategy='constant', fill_value='missing')), ('onehot', onehotencoder(handle_unknown='ignore'))]) preprocessor = columntransformer( transformers=[ ('num', numeric_transformer, numeric_features), ('cat', categorical_transformer, categorical_features)], remainder=\"drop\") preprocessor.fit(concat_data) joblib.dump(preprocessor, os.path.join(args.model_dir, \"model.joblib\")) print(\"saved model!\") def input_fn(input_data, request_content_type): \"\"\"parse input data payload we currently only take csv input. since we need to process both labelled and unlabelled data we first determine whether the label column is present by looking at how many columns were provided. \"\"\" content_type = request_content_type.lower( ) if request_content_type else \"text\/csv\" content_type = content_type.split(\";\")[0].strip() if isinstance(input_data, str): str_buffer = input_data else: str_buffer = str(input_data,'utf-8') if _is_feature_transform(): if content_type == 'text\/csv': # read the raw input data as csv. df = pd.read_csv(stringio(input_data), header=none) if len(df.columns) == len(feature_columns_names) + 1: # this is a labelled example, includes the label df.columns = feature_columns_names + [label_column] elif len(df.columns) == len(feature_columns_names): # this is an unlabelled example. df.columns = feature_columns_names return df else: raise valueerror(\"{} not supported by script!\".format(content_type)) if _is_inverse_label_transform(): if (content_type == 'text\/csv' or content_type == 'text\/csv; charset=utf-8'): # read the raw input data as csv. df = pd.read_csv(stringio(str_buffer), header=none) if len(df.columns) == len(feature_columns_names) + 1: # this is a labelled example, includes the ring label df.columns = feature_columns_names + [label_column] elif len(df.columns) == len(feature_columns_names): # this is an unlabelled example. df.columns = feature_columns_names return df else: raise valueerror(\"{} not supported by script!\".format(content_type)) def output_fn(prediction, accept): \"\"\"format prediction output the default accept\/content-type between containers for serial inference is json. we also want to set the contenttype or mimetype as the same value as accept so the next container can read the response payload correctly. \"\"\" accept = 'text\/csv' if type(prediction) is not np.ndarray: prediction=prediction.toarray() if accept == \"application\/json\": instances = [] for row in prediction.tolist(): instances.append({\"features\": row}) json_output = {\"instances\": instances} return worker.response(json.dumps(json_output), mimetype=accept) elif accept == 'text\/csv': return worker.response(encoders.encode(prediction, accept), mimetype=accept) else: raise runtimeexception(\"{} accept type is not supported by this script.\".format(accept)) def predict_fn(input_data, model): \"\"\"preprocess input data we implement this because the default predict_fn uses .predict(), but our model is a preprocessor so we want to use .transform(). the output is returned in the following order: rest of features either one hot encoded or standardized \"\"\" if _is_feature_transform(): features = model.transform(input_data) if label_column in input_data: # return the label (as the first column) and the set of features. return np.insert(features.toarray(), 0, pd.get_dummies(input_data[label_column])['true.'], axis=1) else: # return only the set of features return features if _is_inverse_label_transform(): features = input_data.iloc[:,0]>0.5 features = features.values return features def model_fn(model_dir): \"\"\"deserialize fitted model \"\"\" if _is_feature_transform(): preprocessor = joblib.load(os.path.join(model_dir, \"model.joblib\")) return preprocessor please help.",
        "Question_original_content_gpt_summary":"The user is encountering internal server errors (500) while attempting to perform a batch transform on a training dataset in an S3 bucket.",
        "Question_preprocessed_content":"Title: batch transform internal server error; Content: i am trying to do a batch transform on a training dataset in an s bucket. i have followed this link the training data on which transformation is being applied is of mb. i am getting these errors bad http status received from algorithm the server encountered an internal error and was unable to complete your request. either the server is overloaded or there is an error in the application. process followed please help.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Why is my Hyperdrive step not completing even when the child jobs have completed?",
        "Question_body":"Hi,\n\nWhat conditions need to be met to complete a HyperDriveStep in an Azure ML pipeline?\n\nContext:\nI'm trying to run a hyperparameter sweep in Azure ML using a pipeline with a HyperDriveStep (HDS) followed by a PythonScriptStep (PSS) . The HDS step has a singular child job of the Sweep type that runs successfully and completes, with a \"best_child_by_primary_metric\" metric logged (implying that a best model has been identified). However, the HDS keeps on running despite there being nothing else for it to do other than complete and trigger the PSS. The template I've followed is the one here, as recommended in the Azure ML documentation for the HyperDriveStep Class. There doesn't seem to be anything in the output logs either indicating some computational process is occurring, so I can't figure out what's keeping it idling.\n\nThanks in advance",
        "Question_answer_count":0,
        "Question_comment_count":5.0,
        "Question_creation_time":1655222646607,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/889157\/why-is-my-hyperdrive-step-not-completing-even-when.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why is my hyperdrive step not completing even when the child jobs have completed?; Content: hi, what conditions need to be met to complete a hyperdrivestep in an pipeline? context: i'm trying to run a hyperparameter sweep in using a pipeline with a hyperdrivestep (hds) followed by a pythonscriptstep (pss) . the hds step has a singular child job of the sweep type that runs successfully and completes, with a \"best_child_by_primary_metric\" metric logged (implying that a best model has been identified). however, the hds keeps on running despite there being nothing else for it to do other than complete and trigger the pss. the template i've followed is the one here, as recommended in the documentation for the hyperdrivestep class. there doesn't seem to be anything in the output logs either indicating some computational process is occurring, so i can't figure out what's keeping it idling. thanks in advance",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where the HyperDriveStep in their pipeline is not completing even though the child jobs have completed successfully.",
        "Question_preprocessed_content":"Title: why is my hyperdrive step not completing even when the child jobs have completed?; Content: hi, what conditions need to be met to complete a hyperdrivestep in an pipeline? context i'm trying to run a hyperparameter sweep in using a pipeline with a hyperdrivestep followed by a pythonscriptstep . the hds step has a singular child job of the sweep type that runs successfully and completes, with a metric logged . however, the hds keeps on running despite there being nothing else for it to do other than complete and trigger the pss. the template i've followed is the one here, as recommended in the documentation for the hyperdrivestep class. there doesn't seem to be anything in the output logs either indicating some computational process is occurring, so i can't figure out what's keeping it idling. thanks in advance",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Error while deploying an Webservice using an ACI",
        "Question_body":"Hi,\n\nI am receiving the following message when I try the deploy a webservice using azure SDK.\n\n\"message\": \"Your container application crashed. This may be caused by errors in your scoring file's init() function.\n\nNOTE : here is my init():\n\n**def init():\ntry:\nglobal model_cbf\nglobal model_frequenceAchat\nglobal date_ajd\nglobal model_cf\nglobal Feature\nglobal Empty_Feature\nglobal MultiSparseInfo\n\n     model_path_cbf = Model.get_model_path(\"content-based_filtering\")    \n     model_cbf = joblib.load(model_path_cbf)\n        \n     model_path_frequence_achat = Model.get_model_path(\"frequence_dachat\")\n     model_frequenceAchat = joblib.load(model_path_frequence_achat)    \n     date_ajd = datetime.datetime(2014, 7, 8)\n        \n     Feature = namedtuple(\"Feature\", [\"name\", \"index\"])\n     Empty_Feature = Feature(name=[], index=[])\n    \n     MultiSparseInfo = namedtuple(\"MultiSparseInfo\",\n                              [\"field_offset\", \"field_len\", \"feat_oov\"])\n        \n     model_cf_data = Model.get_model_path(\"donnees_collaborative_filtering\") \n     data_info = DataInfo.load2(model_cf_data[1])\n     model_cf = SVD(task = model_cf_data[0]['task'], data_info = data_info)\n     model_cf.load2(model_cf_data[2])\n except:\n     Exception as e:\n     print(str(e))**\n\n\n\n\nI suspect that the presence of a class definition in the entry script might be the problem. What do you guys think?\n\nThank you,\n\nVincent",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1623853256203,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/438730\/error-while-deploying-an-webservice-using-an-aci.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-17T13:59:29.327Z",
                "Answer_score":1,
                "Answer_body":"Hi,\n\nThank you for the quick response.\n\nThe error was that I wasnt deserializing the model \"donnees_collaborative_filtering\".\n\nVincent",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: error while deploying an webservice using an aci; Content: hi, i am receiving the following message when i try the deploy a webservice using azure sdk. \"message\": \"your container application crashed. this may be caused by errors in your scoring file's init() function. note : here is my init(): **def init(): try: global model_cbf global model_frequenceachat global date_ajd global model_cf global feature global empty_feature global multisparseinfo model_path_cbf = model.get_model_path(\"content-based_filtering\") model_cbf = joblib.load(model_path_cbf) model_path_frequence_achat = model.get_model_path(\"frequence_dachat\") model_frequenceachat = joblib.load(model_path_frequence_achat) date_ajd = datetime.datetime(2014, 7, 8) feature = namedtuple(\"feature\", [\"name\", \"index\"]) empty_feature = feature(name=[], index=[]) multisparseinfo = namedtuple(\"multisparseinfo\", [\"field_offset\", \"field_len\", \"feat_oov\"]) model_cf_data = model.get_model_path(\"donnees_collaborative_filtering\") data_info = datainfo.load2(model_cf_data[1]) model_cf = svd(task = model_cf_data[0]['task'], data_info = data_info) model_cf.load2(model_cf_data[2]) except: exception as e: print(str(e))** i suspect that the presence of a class definition in the entry script might be the problem. what do you guys think? thank you, vincent",
        "Question_original_content_gpt_summary":"The user Vincent is encountering an error while deploying a webservice using Azure SDK, which may be caused by errors in the scoring file's init() function.",
        "Question_preprocessed_content":"Title: error while deploying an webservice using an aci; Content: hi, i am receiving the following message when i try the deploy a webservice using azure sdk. message your container application crashed. this may be caused by errors in your scoring file's init function. note here is my init def init try global global global global global feature global global multisparse , feature namedtuple feature multisparse namedtuple svd except exception as e print i suspect that the presence of a class definition in the entry script might be the problem. what do you guys think? thank you, vincent",
        "Answer_original_content":"hi, thank you for the quick response. the error was that i wasnt deserializing the model \"donnees_collaborative_filtering\". vincent",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, thank you for the quick response. the error was that i wasnt deserializing the model vincent"
    },
    {
        "Question_id":null,
        "Question_title":"Non-interactive login to registered dataset",
        "Question_body":"I'm trying to tune hyperparameters similar to the following guide: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\n\nMy PyTorch Dataset in train.py contains:\n\n ws = Workspace.from_config()\n ds = Dataset.get_by_name(ws, 'train')\n df = ds.to_pandas_dataframe()\n\nThis code works fine when run from the command-line, but when I submit a hyperparam tuning job to each node of a training cluster, I get the following error:\n\nWe could not find config.json in: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/adamml\/azureml\/hd_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3\/mounts\/workspaceblobstore\/azureml\/HD_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3 or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n\nIf I manually pass my subscription id, resource group, and workspace id, I don't get this error, but now every single hyperparam tuning experiment requires me to log in through the web portal. Is there a way to do a non-interactive login?",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1595346764937,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/50386\/non-interactive-login-to-registered-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-23T20:05:42.24Z",
                "Answer_score":2,
                "Answer_body":"If I read the post correctly, you were trying to get an registered dataset within a submitted run. There, Workspace.from_config() won't work since there is no config.json file as the error suggested.\n\nAnd when you created an auth object which is InteractiveLoginAuth, it is expected to perform interactive login.\n\nWithin a run the recommended way to connect to current workspace it via:\n\nfrom azureml.core import Run\nrun = Run.get_context().experiment.workspace\n\n\n\nMeanwhile, there is way to pass in an dataset object to a run without involving register and workspace signin. If that fit your scenario better, please refer to the example in this document https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#access-and-explore-input-datasets\n\nfrom azureml.core import Dataset, Run\nrun = Run.get_context()\n# get the input dataset by name\ndataset = run.input_datasets['titanic']\n# load the TabularDataset to pandas DataFrame\ndf = dataset.to_pandas_dataframe()",
                "Answer_comment_count":3,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":36.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: non-interactive login to registered dataset; Content: i'm trying to tune hyperparameters similar to the following guide: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters my pytorch dataset in train.py contains: ws = workspace.from_config() ds = dataset.get_by_name(ws, 'train') df = ds.to_pandas_dataframe() this code works fine when run from the command-line, but when i submit a hyperparam tuning job to each node of a training cluster, i get the following error: we could not find config.json in: \/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/adamml\/\/hd_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3\/mounts\/workspaceblobstore\/\/hd_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3 or in its parent directories. please provide the full path to the config file or ensure that config.json exists in the parent directories. if i manually pass my subscription id, resource group, and workspace id, i don't get this error, but now every single hyperparam tuning experiment requires me to log in through the web portal. is there a way to do a non-interactive login?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with non-interactive login to a registered dataset when submitting a hyperparam tuning job to each node of a training cluster.",
        "Question_preprocessed_content":"Title: non interactive login to registered dataset; Content: i'm trying to tune hyperparameters similar to the following guide my pytorch dataset in contains ws ds 'train' df this code works fine when run from the command line, but when i submit a hyperparam tuning job to each node of a training cluster, i get the following error we could not find in or in its parent directories. please provide the full path to the config file or ensure that exists in the parent directories. if i manually pass my subscription id, resource group, and workspace id, i don't get this error, but now every single hyperparam tuning experiment requires me to log in through the web portal. is there a way to do a non interactive login?",
        "Answer_original_content":"if i read the post correctly, you were trying to get an registered dataset within a submitted run. there, workspace.from_config() won't work since there is no config.json file as the error suggested. and when you created an auth object which is interactiveloginauth, it is expected to perform interactive login. within a run the recommended way to connect to current workspace it via: from .core import run run = run.get_context().experiment.workspace meanwhile, there is way to pass in an dataset object to a run without involving register and workspace signin. if that fit your scenario better, please refer to the example in this document https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#access-and-explore-input-datasets from .core import dataset, run run = run.get_context() # get the input dataset by name dataset = run.input_datasets['titanic'] # load the tabulardataset to pandas dataframe df = dataset.to_pandas_dataframe()",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"if i read the post correctly, you were trying to get an registered dataset within a submitted run. there, won't work since there is no file as the error suggested. and when you created an auth object which is interactiveloginauth, it is expected to perform interactive login. within a run the recommended way to connect to current workspace it via from import run run meanwhile, there is way to pass in an dataset object to a run without involving register and workspace signin. if that fit your scenario better, please refer to the example in this document from import dataset, run run get the input dataset by name dataset load the tabulardataset to pandas dataframe df"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Automated ML(interface) choosing primary metrics to handle imbalanced data",
        "Question_body":"I figured out that there are some primary metrics I can choose when I run an automated ML experiment. Yet the number of primary metrics is fewer than the run metrics in the result page. I want to deal with imbalanced data(10:1 or 20:1) and\n\nlooked up the links below:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data\nand\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\n\nIt seems F1 score is recommended to evaluate each model with imbalanced data.\n\nHere are my questions:\n\nIs there any way to set F1 score or multiple measures as a primary metric?\n\n\nIf there is no such way, should I do it manually?\n\n\nOf all the given primary metrics, which primary metric is the most appropriate(to build a Classification model with imbalanced data)?\n\n\n\n\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1593398061863,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/40792\/azure-automated-mlinterface-choosing-primary-metri.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-30T09:20:12.347Z",
                "Answer_score":0,
                "Answer_body":"For imbalanced data, it is preferred to choose AUC Weighted. Also user should then choose a metric that is appropriate to work well for imbalance. E.g. F1, micro averaged AUC, balanced accuracy for model evaluation. For primary metric (metric used for model optimization) the user should preferably choose AUC Weighted instead of accuracy.\nCurrently from the ml.azure.com the following metrics are supported. To add F1 score metric forwarded to product team to check on this.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#primary-metric",
                "Answer_comment_count":3,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure automated ml(interface) choosing primary metrics to handle imbalanced data; Content: i figured out that there are some primary metrics i can choose when i run an automated ml experiment. yet the number of primary metrics is fewer than the run metrics in the result page. i want to deal with imbalanced data(10:1 or 20:1) and looked up the links below: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data and https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train it seems f1 score is recommended to evaluate each model with imbalanced data. here are my questions: is there any way to set f1 score or multiple measures as a primary metric? if there is no such way, should i do it manually? of all the given primary metrics, which primary metric is the most appropriate(to build a classification model with imbalanced data)? thanks.",
        "Question_original_content_gpt_summary":"The user is facing challenges in choosing primary metrics to handle imbalanced data when running an automated ML experiment in Azure.",
        "Question_preprocessed_content":"Title: azure automated ml choosing primary metrics to handle imbalanced data; Content: i figured out that there are some primary metrics i can choose when i run an automated ml experiment. yet the number of primary metrics is fewer than the run metrics in the result page. i want to deal with imbalanced data and looked up the links below and it seems f score is recommended to evaluate each model with imbalanced data. here are my questions is there any way to set f score or multiple measures as a primary metric? if there is no such way, should i do it manually? of all the given primary metrics, which primary metric is the most appropriate ? thanks.",
        "Answer_original_content":"for imbalanced data, it is preferred to choose auc weighted. also user should then choose a metric that is appropriate to work well for imbalance. e.g. f1, micro averaged auc, balanced accuracy for model evaluation. for primary metric (metric used for model optimization) the user should preferably choose auc weighted instead of accuracy. currently from the ml.azure.com the following metrics are supported. to add f1 score metric forwarded to product team to check on this. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#primary-metric",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"for imbalanced data, it is preferred to choose auc weighted. also user should then choose a metric that is appropriate to work well for imbalance. f , micro averaged auc, balanced accuracy for model evaluation. for primary metric the user should preferably choose auc weighted instead of accuracy. currently from the the following metrics are supported. to add f score metric forwarded to product team to check on this."
    },
    {
        "Question_id":70429857.0,
        "Question_title":"seems like GPU is not working in AWS Sagemaker Studio Lab",
        "Question_body":"<p>I've selected compute type as GPU, and opened my project.<\/p>\n<p>but checking the local devices, it doesn't look like any GPU is deployed.<\/p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()\n<\/code><\/pre>\n<p>outputs:<\/p>\n<pre><code>[name: &quot;\/device:CPU:0&quot;\n device_type: &quot;CPU&quot;\n memory_limit: 268435456\n locality {\n }\n incarnation: 13079107644747151451]\n<\/code><\/pre>\n<p>How can I enable GPU usage in AWS Sagemaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1640051584293,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":459.0,
        "Owner_creation_time":1640051180483,
        "Owner_last_access_time":1663300671900,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70429857",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: seems like gpu is not working in studio lab; Content: i've selected compute type as gpu, and opened my project. but checking the local devices, it doesn't look like any gpu is deployed. import tensorflow as tf from tensorflow.python.client import device_lib device_lib.list_local_devices() outputs: [name: \"\/device:cpu:0\" device_type: \"cpu\" memory_limit: 268435456 locality { } incarnation: 13079107644747151451] how can i enable gpu usage in ?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with enabling GPU usage in their Studio Lab.",
        "Question_preprocessed_content":"Title: seems like gpu is not working in studio lab; Content: i've selected compute type as gpu, and opened my project. but checking the local devices, it doesn't look like any gpu is deployed. outputs how can i enable gpu usage in ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70877982.0,
        "Question_title":"How to format parameter of data type json in a aws cloudformation yaml template?",
        "Question_body":"<p>The yaml template <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-properties-sagemaker-model-containerdefinition.html\" rel=\"nofollow noreferrer\">documentation<\/a> for the AWS Cloudformation AWS::SageMaker::Model ContainerDefinition specifies that &quot;Environment&quot; is of type Json. I can't work out how to submit json in my yaml template that does not cause a &quot;CREATE_FAILED    Internal Failure&quot; after running a deploy with the below command.<\/p>\n<pre><code>aws cloudformation deploy --stack-name test1 --template-file test-template-export.yml\n<\/code><\/pre>\n<p>test-template-export.yml<\/p>\n<pre><code>Description: Example yaml\n\nResources:\n  Model:\n    Type: AWS::SageMaker::Model\n    Properties:\n      Containers:\n      - ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n      - Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n      ExecutionRoleArn: arn:aws:iam::123456789123:role\/service-role\/AmazonSageMakerServiceCatalogProductsUseRole\n<\/code><\/pre>\n<p>I have also tried the below formats as well and still no luck.<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment: '{&quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;}'\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment: | \n         {\n            &quot;SAGEMAKER_CONTAINER_LOG_LEVEL&quot;: &quot;20&quot;\n          }\n<\/code><\/pre>\n<p>--<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n- Environment:\n  - SAGEMAKER_CONTAINER_LOG_LEVEL: &quot;20&quot;\n<\/code><\/pre>\n<p>Running without Environment deploys fine.<\/p>\n<p>I have tried everything in <a href=\"https:\/\/stackoverflow.com\/questions\/39041209\/how-to-specify-json-formatted-string-in-cloudformation\">this answer.<\/a>\nHow do I format this Environment argument?<\/p>\n<p>My version of aws cli is &quot;aws-cli\/2.4.10 Python\/3.8.8&quot;<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1643283742143,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":391.0,
        "Owner_creation_time":1452981020536,
        "Owner_last_access_time":1663944762608,
        "Owner_reputation":311.0,
        "Owner_up_votes":67.0,
        "Owner_down_votes":0.0,
        "Owner_views":37.0,
        "Answer_body":"<p>Hi when you see json format think more dict.\nSo write it like this:<\/p>\n<pre><code>Containers:\n- ModelPackageName: arn:aws:sagemaker:us-east-1:123456789123:model-package\/name\/25\n  Environment:\n     SAGEMAKER_CONTAINER_LOG_LEVEL: 20\n<\/code><\/pre>\n<p>For IAM Policies the PolicyDocument is json type and this is how AWS do it in their exempel:<\/p>\n<pre><code>Type: 'AWS::IAM::Policy'\nProperties:\n  PolicyName: CFNUsers\n  PolicyDocument:\n    Version: &quot;2012-10-17&quot;\n    Statement:\n      - Effect: Allow\n        Action:\n          - 'cloudformation:Describe*'\n          - 'cloudformation:List*'\n          - 'cloudformation:Get*'\n        Resource: '*'\n  Groups:\n    - !Ref CFNUserGroup\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1643284402368,
        "Answer_score":2.0,
        "Owner_location":"UK",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1643297093190,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70877982",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to format parameter of data type json in a aws cloudformation yaml template?; Content: the yaml template documentation for the aws cloudformation aws::::model containerdefinition specifies that \"environment\" is of type json. i can't work out how to submit json in my yaml template that does not cause a \"create_failed internal failure\" after running a deploy with the below command. aws cloudformation deploy --stack-name test1 --template-file test-template-export.yml test-template-export.yml description: example yaml resources: model: type: aws::::model properties: containers: - modelpackagename: arn:aws::us-east-1:123456789123:model-package\/name\/25 - environment: '{\"_container_log_level\": \"20\"}' executionrolearn: arn:aws:iam::123456789123:role\/service-role\/amazonservicecatalogproductsuserole i have also tried the below formats as well and still no luck. containers: - modelpackagename: arn:aws::us-east-1:123456789123:model-package\/name\/25 environment: '{\"_container_log_level\": \"20\"}' -- containers: - modelpackagename: arn:aws::us-east-1:123456789123:model-package\/name\/25 - environment: | { \"_container_log_level\": \"20\" } -- containers: - modelpackagename: arn:aws::us-east-1:123456789123:model-package\/name\/25 - environment: - _container_log_level: \"20\" running without environment deploys fine. i have tried everything in this answer. how do i format this environment argument? my version of aws cli is \"aws-cli\/2.4.10 python\/3.8.8\"",
        "Question_original_content_gpt_summary":"The user is encountering challenges formatting the parameter of data type json in an AWS Cloudformation YAML template.",
        "Question_preprocessed_content":"Title: how to format parameter of data type json in a aws cloudformation yaml template?; Content: the yaml template documentation for the aws cloudformation model containerdefinition specifies that environment is of type json. i can't work out how to submit json in my yaml template that does not cause a internal failure after running a deploy with the below command. i have also tried the below formats as well and still no luck. running without environment deploys fine. i have tried everything in this answer. how do i format this environment argument? my version of aws cli is",
        "Answer_original_content":"hi when you see json format think more dict. so write it like this: containers: - modelpackagename: arn:aws::us-east-1:123456789123:model-package\/name\/25 environment: _container_log_level: 20 for iam policies the policydocument is json type and this is how aws do it in their exempel: type: 'aws::iam::policy' properties: policyname: cfnusers policydocument: version: \"2012-10-17\" statement: - effect: allow action: - 'cloudformation:describe*' - 'cloudformation:list*' - 'cloudformation:get*' resource: '*' groups: - !ref cfnusergroup",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi when you see json format think more dict. so write it like this for iam policies the policydocument is json type and this is how aws do it in their exempel"
    },
    {
        "Question_id":null,
        "Question_title":"How to use trained vision model in App Inventor",
        "Question_body":"I have trained a cloud image model with a set of images, I want to use this model to make a mobile app through MIT App Inventor 2. Please suggest how to do this.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1638677700000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":91.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-use-trained-vision-model-in-App-Inventor\/td-p\/177222\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-08T16:29:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello,\nAccording to this MIT App Inventor response\u00a0you can use an exported model that has a format compatible with Tensorflow.js.\nPlease find here how to export\u00a0your model in a TensorFlow.js format with\u00a0AutoML Vision Edge and further details about\u00a0building and deploying TensorFlow.js models with AutoML\u00a0here."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to use trained vision model in app inventor; Content: i have trained a cloud image model with a set of images, i want to use this model to make a mobile app through mit app inventor 2. please suggest how to do this.",
        "Question_original_content_gpt_summary":"The user wants to use a trained cloud image model to create a mobile app through MIT App Inventor 2.",
        "Question_preprocessed_content":"Title: how to use trained vision model in app inventor; Content: i have trained a cloud image model with a set of images, i want to use this model to make a mobile app through mit app inventor . please suggest how to do this.",
        "Answer_original_content":"hello, according to this mit app inventor responseyou can use an exported model that has a format compatible with tensorflow.js. please find here how to exportyour model in a tensorflow.js format withautoml vision edge and further details aboutbuilding and deploying tensorflow.js models with automlhere.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello, according to this mit app inventor responseyou can use an exported model that has a format compatible with please find here how to exportyour model in a format withautoml vision edge and further details aboutbuilding and deploying models with automlhere."
    },
    {
        "Question_id":null,
        "Question_title":"Can I connect a Sagemaker \"Studio\" instance to a private github repository?",
        "Question_body":"I've successfully connected a Sagemaker \"notebook\" to a private github repository, but wondering if it isn't possible for a studio instance? Failing that is there an easy way to get the remote codecommit git url for an existing \"studio\" instance so that code there can at least be pulled to my local machine?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1656504273482,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":74.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH33ZXpiAQ_aV2TesXBNOBw\/can-i-connect-a-sagemaker-studio-instance-to-a-private-github-repository",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-30T10:17:37.550Z",
                "Answer_score":1,
                "Answer_body":"Hi tansaku!\n\nFor sure, SageMaker Studio is integrated with Git, so you can connect to both your public and private repositories!\n\nWhen you try to connect to a private repository, you will be asked to enter your username and password. Best practices here are to create a personal access token instead of using your password.\n\nIf you want to cache your credentials avoiding typing them each time you interact with the Github server, you can cache them or store them in the home folder with the following command run from the System Terminal:\n\n$ git config --global git credential.helper [cache|store]\n\n\nIf you choose store to store your credentials, it will be written to the ~\/.git-credentials file located in your home folder. The cache parameter stores credentials in memory and never on disk. It also accepts the --timeout <seconds> option, which changes the amount of time the daemon is kept running (default is \"900\" or 15 minutes).\n\nOnce it executes the command, the next time it pushes it will ask for the credentials and store them, after that it shouldn't ask again.\n\nHope this helps!",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can i connect a \"studio\" instance to a private github repository?; Content: i've successfully connected a \"notebook\" to a private github repository, but wondering if it isn't possible for a studio instance? failing that is there an easy way to get the remote codecommit git url for an existing \"studio\" instance so that code there can at least be pulled to my local machine?",
        "Question_original_content_gpt_summary":"The user is trying to connect a \"studio\" instance to a private GitHub repository, and is wondering if there is an easy way to get the remote CodeCommit Git URL for an existing \"studio\" instance.",
        "Question_preprocessed_content":"Title: can i connect a studio instance to a private github repository?; Content: i've successfully connected a notebook to a private github repository, but wondering if it isn't possible for a studio instance? failing that is there an easy way to get the remote codecommit git url for an existing studio instance so that code there can at least be pulled to my local machine?",
        "Answer_original_content":"hi tansaku! for sure, studio is integrated with git, so you can connect to both your public and private repositories! when you try to connect to a private repository, you will be asked to enter your username and password. best practices here are to create a personal access token instead of using your password. if you want to cache your credentials avoiding typing them each time you interact with the github server, you can cache them or store them in the home folder with the following command run from the system terminal: $ git config --global git credential.helper [cache|store] if you choose store to store your credentials, it will be written to the ~\/.git-credentials file located in your home folder. the cache parameter stores credentials in memory and never on disk. it also accepts the --timeout option, which changes the amount of time the daemon is kept running (default is \"900\" or 15 minutes). once it executes the command, the next time it pushes it will ask for the credentials and store them, after that it shouldn't ask again. hope this helps!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi tansaku! for sure, studio is integrated with git, so you can connect to both your public and private repositories! when you try to connect to a private repository, you will be asked to enter your username and password. best practices here are to create a personal access token instead of using your password. if you want to cache your credentials avoiding typing them each time you interact with the github server, you can cache them or store them in the home folder with the following command run from the system terminal $ git config global git if you choose store to store your credentials, it will be written to the file located in your home folder. the cache parameter stores credentials in memory and never on disk. it also accepts the timeout option, which changes the amount of time the daemon is kept running . once it executes the command, the next time it pushes it will ask for the credentials and store them, after that it shouldn't ask again. hope this helps!"
    },
    {
        "Question_id":63187116.0,
        "Question_title":"Call Azure Stream Analytics UDF with multi-dimensional array of last 5 records, grouped by record",
        "Question_body":"<p>I am trying to call an AzureML UDF from Stream Analytics query and that UDF expects an array of 5 rows and 2 columns.  The input data is streamed from an IoT hub and we have two fields in the incoming messages: temperature &amp; humidity.<\/p>\n<p>This would be the 'passthrough query' :<\/p>\n<pre><code>SELECT GetMetadataPropertyValue([room-telemetry], 'IoTHub.ConnectionDeviceId') AS RoomId, \n       Temperature, Humidity\nINTO\n    [maintenance-alerts]\nFROM\n    [room-telemetry]\n<\/code><\/pre>\n<p>I have an AzureML UDF (successfully created) that should be called with the last 5 records per RoomId and that will return one value from the ML Model.  Obviously, there are multiple rooms in my stream, so I need to find a way to get some kind of windowing of 5 records Grouped per RoomId.  I don't seem to find a way to call the UDF with the right arrays selected from the input stream.  I know I can create a Javascript UDF that would return an array from the specific fields, but that would be record\/by record, where here I would need this with multiple records that are grouped by the RoomId.<\/p>\n<p>Someone has any insights?<\/p>\n<p>Best regards<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1596178511590,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":374.0,
        "Owner_creation_time":1360655430743,
        "Owner_last_access_time":1663784892907,
        "Owner_reputation":2947.0,
        "Owner_up_votes":297.0,
        "Owner_down_votes":16.0,
        "Owner_views":355.0,
        "Answer_body":"<p>After the good suggestion of @jean-s\u00e9bastien and an answer to an isolated question for the <a href=\"https:\/\/stackoverflow.com\/questions\/63357901\/how-to-convert-a-dictionary-like-structure-in-azure-stream-analytics-to-a-mult\/63373103#63373103\">array-parsing<\/a>, I finally was able to stitch everything together in a solution that builds.  (still have to get it to run at runtime, though).<\/p>\n<p>So, the solution exists in using <code>CollectTop<\/code> to aggregate the latest rows of the entity you want to group by, including the specification of a Time Window.<\/p>\n<p>And the next step was to create the javascript UDF to take that data structure and parse it into a multi-dimensional array.<\/p>\n<p>This is the query I have right now:<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>-- Taking relevant fields from the input stream\nWITH RelevantTelemetry AS\n(\n    SELECT  engineid, tmp, hum, eventtime\n    FROM    [engine-telemetry] \n    WHERE   engineid IS NOT NULL\n),\n-- Grouping by engineid in TimeWindows\nTimeWindows AS\n(\n    SELECT engineid, \n        CollectTop(2) OVER (ORDER BY eventtime DESC) as TimeWindow\n    FROM\n        [RelevantTelemetry]\n    WHERE engineid IS NOT NULL\n    GROUP BY TumblingWindow(hour, 24), engineid\n)\n--Output timewindows for verification purposes\nSELECT engineid, Udf.Predict(Udf.getTimeWindows(TimeWindow)) as Prediction\nINTO debug\nFROM TimeWindows\n<\/code><\/pre>\n<p>And this is the Javascript UDF:<\/p>\n<pre class=\"lang-js prettyprint-override\"><code>    function getTimeWindows(input){\n        var output = [];\n        for(var x in input){\n            var array = [];\n            array.push(input[x].value.tmp);\n            array.push(input[x].value.hum);\n            output.push(array);\n        }\n        return output;\n    }\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1597227776727,
        "Answer_score":1.0,
        "Owner_location":"Belgium",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63187116",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: call azure stream analytics udf with multi-dimensional array of last 5 records, grouped by record; Content: i am trying to call an udf from stream analytics query and that udf expects an array of 5 rows and 2 columns. the input data is streamed from an iot hub and we have two fields in the incoming messages: temperature & humidity. this would be the 'passthrough query' : select getmetadatapropertyvalue([room-telemetry], 'iothub.connectiondeviceid') as roomid, temperature, humidity into [maintenance-alerts] from [room-telemetry] i have an udf (successfully created) that should be called with the last 5 records per roomid and that will return one value from the ml model. obviously, there are multiple rooms in my stream, so i need to find a way to get some kind of windowing of 5 records grouped per roomid. i don't seem to find a way to call the udf with the right arrays selected from the input stream. i know i can create a javascript udf that would return an array from the specific fields, but that would be record\/by record, where here i would need this with multiple records that are grouped by the roomid. someone has any insights? best regards",
        "Question_original_content_gpt_summary":"The user is facing a challenge in calling an Azure Stream Analytics UDF with a multi-dimensional array of the last 5 records, grouped by record.",
        "Question_preprocessed_content":"Title: call azure stream analytics udf with multi dimensional array of last records, grouped by record; Content: i am trying to call an udf from stream analytics query and that udf expects an array of rows and columns. the input data is streamed from an iot hub and we have two fields in the incoming messages temperature & humidity. this would be the 'passthrough query' i have an udf that should be called with the last records per roomid and that will return one value from the ml model. obviously, there are multiple rooms in my stream, so i need to find a way to get some kind of windowing of records grouped per roomid. i don't seem to find a way to call the udf with the right arrays selected from the input stream. i know i can create a javascript udf that would return an array from the specific fields, but that would be record, where here i would need this with multiple records that are grouped by the roomid. someone has any insights? best regards",
        "Answer_original_content":"after the good suggestion of @jean-sbastien and an answer to an isolated question for the array-parsing, i finally was able to stitch everything together in a solution that builds. (still have to get it to run at runtime, though). so, the solution exists in using collecttop to aggregate the latest rows of the entity you want to group by, including the specification of a time window. and the next step was to create the javascript udf to take that data structure and parse it into a multi-dimensional array. this is the query i have right now: -- taking relevant fields from the input stream with relevanttelemetry as ( select engineid, tmp, hum, eventtime from [engine-telemetry] where engineid is not null ), -- grouping by engineid in timewindows timewindows as ( select engineid, collecttop(2) over (order by eventtime desc) as timewindow from [relevanttelemetry] where engineid is not null group by tumblingwindow(hour, 24), engineid ) --output timewindows for verification purposes select engineid, udf.predict(udf.gettimewindows(timewindow)) as prediction into debug from timewindows and this is the javascript udf: function gettimewindows(input){ var output = []; for(var x in input){ var array = []; array.push(input[x].value.tmp); array.push(input[x].value.hum); output.push(array); } return output; }",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"after the good suggestion of and an answer to an isolated question for the array parsing, i finally was able to stitch everything together in a solution that builds. . so, the solution exists in using to aggregate the latest rows of the entity you want to group by, including the specification of a time window. and the next step was to create the javascript udf to take that data structure and parse it into a multi dimensional array. this is the query i have right now and this is the javascript udf"
    },
    {
        "Question_id":53405502.0,
        "Question_title":"How can I invoke AWS SageMaker endpoint to get inferences?",
        "Question_body":"<p>I want to get real time predictions using my machine learning model with the help of SageMaker. I want to directly get inferences on my website. How can I use the deployed model for predictions?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0.0,
        "Question_creation_time":1542776226677,
        "Question_favorite_count":2.0,
        "Question_score":8.0,
        "Question_view_count":7900.0,
        "Owner_creation_time":1528095970836,
        "Owner_last_access_time":1611845381423,
        "Owner_reputation":91.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":13.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53405502",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i invoke endpoint to get inferences?; Content: i want to get real time predictions using my machine learning model with the help of . i want to directly get inferences on my website. how can i use the deployed model for predictions?",
        "Question_original_content_gpt_summary":"The user wants to use a deployed model to get real-time predictions on their website.",
        "Question_preprocessed_content":"Title: how can i invoke endpoint to get inferences?; Content: i want to get real time predictions using my machine learning model with the help of . i want to directly get inferences on my website. how can i use the deployed model for predictions?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":52939732.0,
        "Question_title":"Installing Torch on Amazon Linux",
        "Question_body":"<p>This took me an excruciatingly long time for me to figure out so I decided to save people the trouble.<\/p>\n\n<p>Trying to install torch on an ec2 instance with Amazon Linux(redhat) failed with the following command:<\/p>\n\n<pre><code>git clone https:\/\/github.com\/torch\/distro.git ~\/torch --recursive\ncd ~\/torch; bash install-deps;\n.\/install.sh\n<\/code><\/pre>\n\n<p>It gave me the error:<\/p>\n\n<pre><code>OpenBLAS Failed to compile\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1540257024407,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":478.0,
        "Owner_creation_time":1403641094843,
        "Owner_last_access_time":1653442091310,
        "Owner_reputation":754.0,
        "Owner_up_votes":127.0,
        "Owner_down_votes":3.0,
        "Owner_views":38.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Cambridge, MA, United States",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52939732",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: installing torch on amazon linux; Content: this took me an excruciatingly long time for me to figure out so i decided to save people the trouble. trying to install torch on an ec2 instance with amazon linux(redhat) failed with the following command: git clone https:\/\/github.com\/torch\/distro.git ~\/torch --recursive cd ~\/torch; bash install-deps; .\/install.sh it gave me the error: openblas failed to compile",
        "Question_original_content_gpt_summary":"The user encountered challenges while trying to install torch on an Amazon Linux (RedHat) EC2 instance.",
        "Question_preprocessed_content":"Title: installing torch on amazon linux; Content: this took me an excruciatingly long time for me to figure out so i decided to save people the trouble. trying to install torch on an ec instance with amazon linux failed with the following command it gave me the error",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Retrieve all the sweeps in a team project",
        "Question_body":"<p>I was trying to retrieve all the sweeps in a project, I did the following:<\/p>\n<pre><code class=\"lang-auto\">sweeps = wandb.Api().project(&lt;project_name&gt;, entity=&lt;team_name&gt;).sweeps()\n<\/code><\/pre>\n<p>However, I got the error that Project object has no attribute sweeps.<\/p>\n<p>Can you please help with it?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1670834961489,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":120.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/retrieve-all-the-sweeps-in-a-team-project\/3534",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-13T13:52:46.373Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohamedr002\">@mohamedr002<\/a> thank you for writing in! Can you please confirm you wandb version, and if you could try again by upgrading to our latest SDK version <code>0.13.6<\/code>? Your one line code is correct and works for me, so could you also check running the following:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\np = api.project('PROJECT', entity='ENTITY')\nprint(p.url)\n<\/code><\/pre>\n<p>Would this return a URL to your project, and can you see if there are Sweeps in it?<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-16T15:18:06.368Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohamedr002\">@mohamedr002<\/a> I wanted to follow up on this issue, does it still occur for you and could you provide a bit more details to assist you further? thanks!<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-21T17:45:23.949Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohamedr002\">@mohamedr002<\/a> since we haven\u2019t heard back from you, I will go ahead and close this ticket for now. If you\u2019re still experiencing any issue retrieving the sweeps please let us know and we will be happy to keep investigating!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-19T17:46:04.673Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"",
        "Answer_original_content":"",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":""
    },
    {
        "Question_id":null,
        "Question_title":"Removal of partial pull's",
        "Question_body":"<p>Dear DVC community,<\/p>\n<p>I am rather new to DVC. I am interested to use DVC in a use case that is maybe related to what is called \u201cData Registrty\u201d, but it is not entirely the same.<\/p>\n<p>I do have a repository that stores in a structured way folders with outputs from expensive computational runs with many files and high storage volume. I added the individual folders as one object, i.e. have one .dvc file per folder.<\/p>\n<p>Example:<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_d.dvc\n<\/code><\/pre>\n<p>A user of the repository would first clone it and, thanks to your partial pull feature, only pull those directories from the remote that are necessary for the next steps in the data analysis pipeline. Thereby the huge repository, for which a full pull would not fit on standard storage, can still be properly used.<\/p>\n<p>Example (cont.):<br>\n(result after \u201cdvc pull folder_a.dvc folder_c.dvc\u201d)<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_a\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_c\n|- folder_d.dvc\n<\/code><\/pre>\n<p>Let\u2019s assume now that the user stopped to use one of those folders (say \u201cfolder_a\u201d) after having pushed its updates. Is there an obvious way to remove that partial pull (here \u201cfolder_a\u201d) from the local working copy, without affecting the remote? (This would be done for keeping the local storage requirements on a moderate level.)<\/p>\n<p>Example (cont.):<br>\n(result after applying the searched for operation)<\/p>\n<pre><code class=\"lang-auto\">repo\n|- folder_a.dvc\n|- folder_b.dvc\n|- folder_c.dvc\n|- folder_c\n|- folder_d.dvc\n<\/code><\/pre>\n<p>I do not ask for removing data from the remote but rather a proper way to remove the folder (not it\u2019s .dvc file) and all remaining data in the local cache. I assume here that a simple removal  (\u201crm -fR folder_a\u201d) of the folder would not be enough\u2026<\/p>\n<p>Then, the simplest solution would of course be to delete the local working directory and just clone a new one, where one would start from scratch with a partial pull. However, that might become a bit unhandy over time.<\/p>\n<p>So is there any simple way to do this by a kind of \u201cunpull\u201d operation that I overlooked?<\/p>\n<p>If you need further details, please let me know.<\/p>\n<p>Thanks a lot in advance!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1659111282606,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":128.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/removal-of-partial-pulls\/1277",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-29T21:36:37.355Z",
                "Answer_body":"<p>Hello again,<\/p>\n<p>After way more reading, the above searched operation for the given example seems to be<\/p>\n<pre><code class=\"lang-auto\">rm -fR folder_a\ndvc gc -w\n<\/code><\/pre>\n<p>Would that be correct?<\/p>\n<p>Thanks!<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-30T15:19:42.852Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/zaspel\">@zaspel<\/a>. Welcome to the community.<\/p>\n<p>There\u2019s no straight way to do this. I can suggest a workaround:<\/p>\n<pre data-code-wrap=\"console\"><code class=\"lang-nohighlight\">dvc remove folder_a.dvc --outs\ndvc gc -w\ngit checkout HEAD -- folder_a.dvc\n<\/code><\/pre>\n<p>This will delete the <code>folder_a.dvc<\/code> and <code>folder_a<\/code> directory temporarily, and then run <code>gc<\/code> to delete the contents in the cache.<br>\nAfter that, we checkout the <code>folder_a.dvc<\/code> back again. <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>This is a simple usecase that we should support, feel free to create a feature request. Thanks.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-30T21:18:41.995Z",
                "Answer_body":"<p>Dear <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a>,<\/p>\n<p>thank you so much for this!! This will be of great help in our project!<\/p>\n<p>I will immediately proceed to propose this as a feature.<\/p>\n<p>Thanks again!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: removal of partial pull's; Content: dear community, i am rather new to . i am interested to use in a use case that is maybe related to what is called \u201cdata registrty\u201d, but it is not entirely the same. i do have a repository that stores in a structured way folders with outputs from expensive computational runs with many files and high storage volume. i added the individual folders as one object, i.e. have one . file per folder. example: repo |- folder_a. |- folder_b. |- folder_c. |- folder_d. a user of the repository would first clone it and, thanks to your partial pull feature, only pull those directories from the remote that are necessary for the next steps in the data analysis pipeline. thereby the huge repository, for which a full pull would not fit on standard storage, can still be properly used. example (cont.): (result after \u201c pull folder_a. folder_c.\u201d) repo |- folder_a. |- folder_a |- folder_b. |- folder_c. |- folder_c |- folder_d. let\u2019s assume now that the user stopped to use one of those folders (say \u201cfolder_a\u201d) after having pushed its updates. is there an obvious way to remove that partial pull (here \u201cfolder_a\u201d) from the local working copy, without affecting the remote? (this would be done for keeping the local storage requirements on a moderate level.) example (cont.): (result after applying the searched for operation) repo |- folder_a. |- folder_b. |- folder_c. |- folder_c |- folder_d. i do not ask for removing data from the remote but rather a proper way to remove the folder (not it\u2019s . file) and all remaining data in the local cache. i assume here that a simple removal (\u201crm -fr folder_a\u201d) of the folder would not be enough\u2026 then, the simplest solution would of course be to delete the local working directory and just clone a new one, where one would start from scratch with a partial pull. however, that might become a bit unhandy over time. so is there any simple way to do this by a kind of \u201cunpull\u201d operation that i overlooked? if you need further details, please let me know. thanks a lot in advance!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with removing partial pulls from their local working copy without affecting the remote repository.",
        "Question_preprocessed_content":"Title: removal of partial pull's; Content: dear community, i am rather new to . i am interested to use in a use case that is maybe related to what is called data registrty, but it is not entirely the same. i do have a repository that stores in a structured way folders with outputs from expensive computational runs with many files and high storage volume. i added the individual folders as one object, have one . file per folder. example a user of the repository would first clone it and, thanks to your partial pull feature, only pull those directories from the remote that are necessary for the next steps in the data analysis pipeline. thereby the huge repository, for which a full pull would not fit on standard storage, can still be properly used. example result after pull lets assume now that the user stopped to use one of those folders after having pushed its updates. is there an obvious way to remove that partial pull from the local working copy, without affecting the remote? example result after applying the searched for operation i do not ask for removing data from the remote but rather a proper way to remove the folder and all remaining data in the local cache. i assume here that a simple removal of the folder would not be enough then, the simplest solution would of course be to delete the local working directory and just clone a new one, where one would start from scratch with a partial pull. however, that might become a bit unhandy over time. so is there any simple way to do this by a kind of unpull operation that i overlooked? if you need further details, please let me know. thanks a lot in advance!",
        "Answer_original_content":"hello again, after way more reading, the above searched operation for the given example seems to be rm -fr folder_a gc -w would that be correct? thanks! hi @zaspel. welcome to the community. theres no straight way to do this. i can suggest a workaround: remove folder_a. --outs gc -w git checkout head -- folder_a. this will delete the folder_a. and folder_a directory temporarily, and then run gc to delete the contents in the cache. after that, we checkout the folder_a. back again. this is a simple usecase that we should support, feel free to create a feature request. thanks. dear @skshetry, thank you so much for this!! this will be of great help in our project! i will immediately proceed to propose this as a feature. thanks again!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello again, after way more reading, the above searched operation for the given example seems to be would that be correct? thanks! hi welcome to the community. theres no straight way to do this. i can suggest a workaround this will delete the and directory temporarily, and then run to delete the contents in the cache. after that, we checkout the back again. this is a simple usecase that we should support, feel free to create a feature request. thanks. dear thank you so much for this!! this will be of great help in our project! i will immediately proceed to propose this as a feature. thanks again!"
    },
    {
        "Question_id":null,
        "Question_title":"How to download the learning curves of grouped runs",
        "Question_body":"<p>Hi,<\/p>\n<p>Is there a way to download the learning curves of grouped runs? For grouped runs, the learning curves have shaded area. Does that represent the standard deviation or the 95% confidence interval? And can we download them (the grouped curve, not the individual ones) in a python script so that we can customize the plot with matplotlib or seaborn? Thanks.<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_creation_time":1661531147920,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":98.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-download-the-learning-curves-of-grouped-runs\/3010",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-31T20:25:32.905Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, currently it isn\u2019t possible to download these curves but you could recreate them by downloading all of the run data. The shaded area represents the variance by default but this can be changed in the graph settings. Could you let me know what you are looking to customize in Matplotlib and perhaps we could make a feature request to make this available in the UI?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-01T02:45:44.934Z",
                "Answer_body":"<p>Thanks, Nate. That makes sense. I am trying to redraw the figure in matplotlib so that the figure is better suited for paper publications. For example, customize title, font, x\/y title, etc.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-14T14:09:49.068Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, I wanted to check back and see if you were able to recreate these with the downloaded metrics or if you would like for me to submit a feature request around this?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-14T15:11:00.184Z",
                "Answer_body":"<p>Yes, I figured out a way for this.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-19T21:08:53.111Z",
                "Answer_body":"<p>Great, glad to hear it!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-18T21:09:37.447Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to download the learning curves of grouped runs; Content: hi, is there a way to download the learning curves of grouped runs? for grouped runs, the learning curves have shaded area. does that represent the standard deviation or the 95% confidence interval? and can we download them (the grouped curve, not the individual ones) in a python script so that we can customize the plot with matplotlib or seaborn? thanks.",
        "Question_original_content_gpt_summary":"The user is looking for a way to download the learning curves of grouped runs, including the shaded area which represents the standard deviation or the 95% confidence interval, and to customize the plot with matplotlib or seaborn.",
        "Question_preprocessed_content":"Title: how to download the learning curves of grouped runs; Content: hi, is there a way to download the learning curves of grouped runs? for grouped runs, the learning curves have shaded area. does that represent the standard deviation or the % confidence interval? and can we download them in a python script so that we can customize the plot with matplotlib or seaborn? thanks.",
        "Answer_original_content":"hi @taochen, currently it isnt possible to download these curves but you could recreate them by downloading all of the run data. the shaded area represents the variance by default but this can be changed in the graph settings. could you let me know what you are looking to customize in matplotlib and perhaps we could make a feature request to make this available in the ui? thank you, nate thanks, nate. that makes sense. i am trying to redraw the figure in matplotlib so that the figure is better suited for paper publications. for example, customize title, font, x\/y title, etc. hi @taochen, i wanted to check back and see if you were able to recreate these with the downloaded metrics or if you would like for me to submit a feature request around this? thank you, nate yes, i figured out a way for this. great, glad to hear it! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi currently it isnt possible to download these curves but you could recreate them by downloading all of the run data. the shaded area represents the variance by default but this can be changed in the graph settings. could you let me know what you are looking to customize in matplotlib and perhaps we could make a feature request to make this available in the ui? thank you, nate thanks, nate. that makes sense. i am trying to redraw the figure in matplotlib so that the figure is better suited for paper publications. for example, customize title, font, title, etc. hi i wanted to check back and see if you were able to recreate these with the downloaded metrics or if you would like for me to submit a feature request around this? thank you, nate yes, i figured out a way for this. great, glad to hear it! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":49365900.0,
        "Question_title":"What's a better way to load a file using boto? (getting filename too long error)",
        "Question_body":"<p>So I'm trying to use <code>tf.contrib.learn.preprocessing.VocabularyProcessor.restore()<\/code> to restore a vocabulary file from an S3 bucket. First, I tried to get the path name to the bucket to use in <code>.restore()<\/code> and I kept getting 'object doesn't exist' error. Afterwards, upon further research, I found a method people use to load text files and JSON files and applied the same method here:<\/p>\n\n<pre><code>obj = s3.Object(BUCKET_NAME, KEY).get()['Body'].read()\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(obj)\n<\/code><\/pre>\n\n<p>This worked for a while until the contents of file increased and eventually got a 'File name too long' error. Is there a better way to load and restore a file from an S3 bucket? <\/p>\n\n<p>By the way, I tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1521471311783,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":532.0,
        "Owner_creation_time":1521470035783,
        "Owner_last_access_time":1598556805912,
        "Owner_reputation":210.0,
        "Owner_up_votes":3.0,
        "Owner_down_votes":0.0,
        "Owner_views":12.0,
        "Answer_body":"<p>It looks like you\u2019re passing in the actual contents of the file as the file name?<\/p>\n\n<p>I think you\u2019ll need to download the object from S3 to a tmp file and pass the path to that file into restore.<\/p>\n\n<p>Try using the method here: <a href=\"http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file\" rel=\"nofollow noreferrer\">http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#S3.Object.download_file<\/a><\/p>\n\n<p>Update:\nI went through the code here: <a href=\"https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py<\/a> and it looks like this just saves a pickle so you can really easily just import pickle and call the following:<\/p>\n\n<pre><code>import pickle\nobj = s3.Object(BUCKET_NAME, KEY).get()['Body']\nvocab_processor = pickle.loads(obj.read())\n<\/code><\/pre>\n\n<p>Hopefully that works?<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1521485629848,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1522056862430,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49365900",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what's a better way to load a file using boto? (getting filename too long error); Content: so i'm trying to use tf.contrib.learn.preprocessing.vocabularyprocessor.restore() to restore a vocabulary file from an s3 bucket. first, i tried to get the path name to the bucket to use in .restore() and i kept getting 'object doesn't exist' error. afterwards, upon further research, i found a method people use to load text files and json files and applied the same method here: obj = s3.object(bucket_name, key).get()['body'].read() vocab_processor = tf.contrib.learn.preprocessing.vocabularyprocessor.restore(obj) this worked for a while until the contents of file increased and eventually got a 'file name too long' error. is there a better way to load and restore a file from an s3 bucket? by the way, i tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file.",
        "Question_original_content_gpt_summary":"The user is encountering an issue with loading a file from an S3 bucket using boto, resulting in a 'filename too long' error.",
        "Question_preprocessed_content":"Title: what's a better way to load a file using boto?; Content: so i'm trying to use to restore a vocabulary file from an s bucket. first, i tried to get the path name to the bucket to use in and i kept getting 'object doesn't exist' error. afterwards, upon further research, i found a method people use to load text files and json files and applied the same method here this worked for a while until the contents of file increased and eventually got a 'file name too long' error. is there a better way to load and restore a file from an s bucket? by the way, i tested this out locally on my machine and it works perfectly fine since there it just needs to take the path to the file, not the entire contents of the file.",
        "Answer_original_content":"it looks like youre passing in the actual contents of the file as the file name? i think youll need to download the object from s3 to a tmp file and pass the path to that file into restore. try using the method here: http:\/\/boto3.readthedocs.io\/en\/latest\/reference\/services\/s3.html#s3.object.download_file update: i went through the code here: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/contrib\/learn\/python\/learn\/preprocessing\/text.py and it looks like this just saves a pickle so you can really easily just import pickle and call the following: import pickle obj = s3.object(bucket_name, key).get()['body'] vocab_processor = pickle.loads(obj.read()) hopefully that works?",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"it looks like youre passing in the actual contents of the file as the file name? i think youll need to download the object from s to a tmp file and pass the path to that file into restore. try using the method here update i went through the code here and it looks like this just saves a pickle so you can really easily just import pickle and call the following hopefully that works?"
    },
    {
        "Question_id":73253909.0,
        "Question_title":"how to use kedro.versioning in latest version of kedro?",
        "Question_body":"<p>I have previously used kedro version 0.17.6 in my project. Now i have upgraded my version to 0.18.2. But in latest version of kedro there is no module of kedro.versioning. So i am getting a error that module is not found.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1659723722473,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Owner_creation_time":1659723627056,
        "Owner_last_access_time":1663924998547,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1659728907820,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73253909",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to use .versioning in latest version of ?; Content: i have previously used version 0.17.6 in my project. now i have upgraded my version to 0.18.2. but in latest version of there is no module of .versioning. so i am getting a error that module is not found.",
        "Question_original_content_gpt_summary":"The user is encountering an issue with using the .versioning module in the latest version of , as it is no longer available.",
        "Question_preprocessed_content":"Title: how to use in latest version of ?; Content: i have previously used version in my project. now i have upgraded my version to but in latest version of there is no module of so i am getting a error that module is not found.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Tracking changes in params.yml",
        "Question_body":"<p>I\u2019m trying to better understand how <code>params.yaml<\/code> works and in particular how changes in it are tracked. <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a> suggested <a href=\"https:\/\/discord.com\/channels\/485586884165107732\/563406153334128681\/710529764628103188\" rel=\"nofollow noopener\">here<\/a> to track <code>params.yaml<\/code> using <code>git<\/code>. So far so go. Now, when inspecting a <code>.dvc<\/code> generated by <code>dvc run -p ...<\/code> I see that the parameters from the <code>params.yaml<\/code> are <em>not<\/em> associated with <code>MD5<\/code>. This made me wonder: if I\u2019m changing a parameter in the YAML (and commit the change to git), how <code>dvc<\/code> will know about it? How will a <code>dvc repro ...<\/code> know that a parameter changed?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1589955027350,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":501.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-changes-in-params-yml\/394",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-20T10:14:53.512Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/drorata\">@drorata<\/a> So, if you inspect the <code>.dvc<\/code> file you can note, that chosen params are stored there too.<br>\nSo the detection is based on value stored in <code>.dvc<\/code> file versus value stored in <code>params.yaml<\/code> file.<br>\nThats how DVC will know to rerun the stage on <code>params.yaml<\/code> changed.<\/p>\n<p>Take a look at this script:<\/p>\n<pre><code class=\"lang-auto\">#!\/bin\/bash\n\nrm -rf repo\nmkdir repo\n\nset -ex\n\npushd repo\n\ngit init --quiet\ndvc init --quiet\n\necho data &gt;&gt; data\necho \"lr: 0.1\" &gt;&gt; params.yaml\n\ndvc add data -q\ndvc run -d data -o output -p lr \"cat data &gt;&gt; output\"\n\ngit add -A\ngit commit -am \"init\"\n\ncat output.dvc | grep -A1 params\n\nsed -i \"s\/0.1\/0.2\/g\" params.yaml\n\ndvc repro output.dvc\n\ncat output.dvc | grep -A1 params\n<\/code><\/pre>\n<p>If you will run it, you can note that dvc notices the change in <code>lr<\/code> and reruns the command. After run it writes new value of <code>lr<\/code> to <code>output.dvc<\/code>.<\/p>",
                "Answer_score":76.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-22T05:27:14.042Z",
                "Answer_body":"<p>Thanks for the great explanation!<\/p>",
                "Answer_score":46.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tracking changes in params.yml; Content: i\u2019m trying to better understand how params.yaml works and in particular how changes in it are tracked. @jorgeorpinel suggested here to track params.yaml using git. so far so go. now, when inspecting a . generated by run -p ... i see that the parameters from the params.yaml are not associated with md5. this made me wonder: if i\u2019m changing a parameter in the yaml (and commit the change to git), how will know about it? how will a repro ... know that a parameter changed?",
        "Question_original_content_gpt_summary":"The user is trying to understand how changes in params.yaml are tracked and how a repro will know that a parameter has changed.",
        "Question_preprocessed_content":"Title: tracking changes in; Content: im trying to better understand how works and in particular how changes in it are tracked. suggested here to track using . so far so go. now, when inspecting a generated by i see that the parameters from the are not associated with . this made me wonder if im changing a parameter in the yaml , how will know about it? how will a know that a parameter changed?",
        "Answer_original_content":"@drorata so, if you inspect the . file you can note, that chosen params are stored there too. so the detection is based on value stored in . file versus value stored in params.yaml file. thats how will know to rerun the stage on params.yaml changed. take a look at this script: #!\/bin\/bash rm -rf repo mkdir repo set -ex pushd repo git init --quiet init --quiet echo data >> data echo \"lr: 0.1\" >> params.yaml add data -q run -d data -o output -p lr \"cat data >> output\" git add -a git commit -am \"init\" cat output. | grep -a1 params sed -i \"s\/0.1\/0.2\/g\" params.yaml repro output. cat output. | grep -a1 params if you will run it, you can note that notices the change in lr and reruns the command. after run it writes new value of lr to output.. thanks for the great explanation!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"so, if you inspect the file you can note, that chosen params are stored there too. so the detection is based on value stored in file versus value stored in file. thats how will know to rerun the stage on changed. take a look at this script if you will run it, you can note that notices the change in and reruns the command. after run it writes new value of to . thanks for the great explanation!"
    },
    {
        "Question_id":null,
        "Question_title":"Spark-related tests are failing",
        "Question_body":"Hi all,\n\n\nI'm trying to set up the dev environment for contributing to MLflow. I followed the instructions\u00a0here but I have some issues running the tests.\u00a0\nI am using pytest and see that 9 tests fail. Most of them are part of the:\u00a0tests\/spark\/test_spark_model_export.py file. The error description is the following:\n\n\ntests\/spark\/test_spark_model_export.py:61:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\/anaconda2\/lib\/python2.7\/site-packages\/pyspark\/context.py:115: in __init__\n\u00a0\u00a0\u00a0 SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n\n\n\u00a0ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local-cluster[2, 1, 1024]) created by getOrCreate at \/home\/mlflow\/mlflow\/tests\/pyfunc\/test_spark.py:27\n..\/anaconda2\/lib\/python2.7\/site-packages\/pyspark\/context.py:314: ValueError\n\n\n************************************************\n\n\nIt seems that the SparkContext cannot be initialized because of an existing one. Any idea what might be the problem here?\n\n\nThanks,\nAvrilia",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1553709289000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":19.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/BgU1p2juiBo",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-03-27T21:09:54",
                "Answer_body":"That looks like a bug in the test code - in tests\/spark\/test_spark_model_export.py:61\u00a0we call the pyspark.SparkContext constructor directly so the getOrCreate is moot (we'll always try to create a SparkContext). For now you should be able to get around this by changing:\n\n\n\u00a0 sc = pyspark.SparkContext(master=\"local-cluster[2, 1, 1024]\", conf=conf).getOrCreate()\n\n\nto:\n\n\n\u00a0 \u00a0 spark = pyspark.sql.SparkSession.builder \\\n\u00a0 \u00a0 \u00a0 \u00a0 .config(key=\"spark.python.worker.reuse\", value=True) \\\n\u00a0 \u00a0 \u00a0 \u00a0 .config(key=\"spark.jars.packages\", value='ml.combust.mleap:mleap-spark-base_2.11:0.12.0,'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'ml.combust.mleap:mleap-spark_2.11:0.12.0') \\\n\u00a0 \u00a0 \u00a0 \u00a0 .master(\"local-cluster[2, 1, 1024]\") \\\n\u00a0 \u00a0 \u00a0 \u00a0 .getOrCreate()\n\u00a0 \u00a0 sc = spark.sparkContext\n\n\n\nWould also be happy to review a PR with such a change (our CI doesn't catch it because it runs Spark & Pyfunc tests separately).\n\nThanks,\nSid\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/b8b5ad46-e2fa-4d00-9f1c-149ba5cc8abe%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-03-28T16:45:39",
                "Answer_body":"Hi Sid,\n\n\nThanks for your help. I made the change but it seems it breaks sth else and I get more tests failing now. I am debugging it and will get back to you. I'd be happy to submit a PR once it is ready.\n\n\nThanks,\nAvrilia\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-03-28T20:36:15",
                "Answer_body":"Sid,\u00a0\n\n\nThanks for your help! Your suggestion is definitely addressing part of the problem but some other changes were needed as well.\n\n\nIt seems that the keras test is executed before the spark tests. The test invokes score_model_as_udf from\u00a0the\u00a0 tests\/pyfunc\/test_spark.py.\u00a0\nThe function also creates a SparkSession builder but its configuration does not include the mleap jars See below:\n\n\n\u00a0spark = pyspark.sql.SparkSession.builder \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .config(key=\"spark.python.worker.reuse\", value=True) \\\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .master(\"local-cluster[2, 1, 1024]\") \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .getOrCreate()\n\n\n\n\nI had to add that config there as well so that the subsequent spark tests (with the change that you suggested) are executed successfully. I can create a PR that contains all the changes but\u00a0\nit seems to me that the SparkConf should get automatically updated when the spark tests are executed. Is this the expected behavior or there is some bug in pyspark.sql.SparkSession that does not allow config updates?\u00a0\n\n\nAvrilia\n\n\nOn Wednesday, March 27, 2019 at 2:54:49 PM UTC-7, Avrilia Floratou wrote:\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: spark-related tests are failing; Content: hi all, i'm trying to set up the dev environment for contributing to . i followed the instructions here but i have some issues running the tests. i am using pytest and see that 9 tests fail. most of them are part of the: tests\/spark\/test_spark_model_export.py file. the error description is the following: tests\/spark\/test_spark_model_export.py:61: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ..\/anaconda2\/lib\/python2.7\/site-packages\/pyspark\/context.py:115: in __init__ sparkcontext._ensure_initialized(self, gateway=gateway, conf=conf) valueerror: cannot run multiple sparkcontexts at once; existing sparkcontext(app=pyspark-shell, master=local-cluster[2, 1, 1024]) created by getorcreate at \/home\/\/\/tests\/pyfunc\/test_spark.py:27 ..\/anaconda2\/lib\/python2.7\/site-packages\/pyspark\/context.py:314: valueerror ************************************************ it seems that the sparkcontext cannot be initialized because of an existing one. any idea what might be the problem here? thanks, avrilia",
        "Question_original_content_gpt_summary":"The user is encountering challenges with running spark-related tests, as an existing SparkContext is preventing the initialization of a new one, resulting in a ValueError.",
        "Question_preprocessed_content":"Title: spark related tests are failing; Content: hi all, i'm trying to set up the dev environment for contributing to . i followed the instructionshere but i have some issues running the tests. i am using pytest and see that tests fail. most of them are part of file. the error description is the following _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ in gateway gateway, conf conf valueerror cannot run multiple sparkcontexts at once; existing sparkcontext created by getorcreate at valueerror it seems that the sparkcontext cannot be initialized because of an existing one. any idea what might be the problem here? thanks, avrilia",
        "Answer_original_content":"that looks like a bug in the test code - in tests\/spark\/test_spark_model_export.py:61we call the pyspark.sparkcontext constructor directly so the getorcreate is moot (we'll always try to create a sparkcontext). for now you should be able to get around this by changing: sc = pyspark.sparkcontext(master=\"local-cluster[2, 1, 1024]\", conf=conf).getorcreate() to: spark = pyspark.sql.sparksession.builder \\ .config(key=\"spark.python.worker.reuse\", value=true) \\ .config(key=\"spark.jars.packages\", value='ml.combust.mleap:mleap-spark-base_2.11:0.12.0,' 'ml.combust.mleap:mleap-spark_2.11:0.12.0') \\ .master(\"local-cluster[2, 1, 1024]\") \\ .getorcreate() sc = spark.sparkcontext would also be happy to review a pr with such a change (our ci doesn't catch it because it runs spark & pyfunc tests separately). thanks, sid -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to post to this group, send email to ...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/b8b5ad46-e2fa-4d00-9f1c-149ba5cc8abe%40googlegroups.com. for more options, visit https:\/\/groups.google.com\/d\/optout. hi sid, thanks for your help. i made the change but it seems it breaks sth else and i get more tests failing now. i am debugging it and will get back to you. i'd be happy to submit a pr once it is ready. thanks, avrilia sid, thanks for your help! your suggestion is definitely addressing part of the problem but some other changes were needed as well. it seems that the keras test is executed before the spark tests. the test invokes score_model_as_udf fromthe tests\/pyfunc\/test_spark.py. the function also creates a sparksession builder but its configuration does not include the mleap jars see below: spark = pyspark.sql.sparksession.builder \\ .config(key=\"spark.python.worker.reuse\", value=true) \\ .master(\"local-cluster[2, 1, 1024]\") \\ .getorcreate() i had to add that config there as well so that the subsequent spark tests (with the change that you suggested) are executed successfully. i can create a pr that contains all the changes but it seems to me that the sparkconf should get automatically updated when the spark tests are executed. is this the expected behavior or there is some bug in pyspark.sql.sparksession that does not allow config updates? avrilia on wednesday, march 27, 2019 at 2:54:49 pm utc-7, avrilia floratou wrote:",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"that looks like a bug in the test code in call the constructor directly so the getorcreate is moot . for now you should be able to get around this by changing sc , , to spark \\ value true \\ \\ .master \\ .getorcreate sc would also be happy to review a pr with such a change . thanks, sid you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to to post to this group, send email to to view this discussion on the web visit for more options, visit hi sid, thanks for your help. i made the change but it seems it breaks sth else and i get more tests failing now. i am debugging it and will get back to you. i'd be happy to submit a pr once it is ready. thanks, avrilia sid, thanks for your help! your suggestion is definitely addressing part of the problem but some other changes were needed as well. it seems that the keras test is executed before the spark tests. the test invokes fromthe the function also creates a sparksession builder but its configuration does not include the mleap jars see below spark \\ value true \\ .master \\ .getorcreate i had to add that config there as well so that the subsequent spark tests are executed successfully. i can create a pr that contains all the changes but it seems to me that the sparkconf should get automatically updated when the spark tests are executed. is this the expected behavior or there is some bug in that does not allow config updates? avrilia on wednesday, march , at pm utc , avrilia floratou wrote"
    },
    {
        "Question_id":null,
        "Question_title":"How to continually update a model with new data",
        "Question_body":"<p>I have a reinforcement learning use case with the following initial steps:<\/p>\n<ol>\n<li>Construct initial episodes<\/li>\n<li>Add episodes to initial experience replay buffer<\/li>\n<li>Train initial model<\/li>\n<\/ol>\n<p>Then the following steps which repeat every day:<\/p>\n<ol start=\"4\">\n<li>Construct most recent time steps<\/li>\n<li>Add new time steps to existing experience replay buffer<\/li>\n<li>Fine-tune existing model<\/li>\n<li>Compare existing model and fine-tuned model and keep the one that\u2019s best<\/li>\n<\/ol>\n<p>What would be the best way to do this using DVC? Is it even possible given that circular dependencies are not allowed?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1644516069762,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":173.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-continually-update-a-model-with-new-data\/1055",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-11T01:35:31.741Z",
                "Answer_body":"<p>You can use <code>persist: true<\/code> to prevent outputs from being deleted during stage execution (see <a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#description\">https:\/\/dvc.org\/doc\/command-reference\/repro#description<\/a>), so you can feed in new data each day and have the subsequent stages read from their existing outputs before writing out newly modified versions.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-14T17:10:19.893Z",
                "Answer_body":"<p>Thanks! I think that should work<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to continually update a model with new data; Content: i have a reinforcement learning use case with the following initial steps: construct initial episodes add episodes to initial experience replay buffer train initial model then the following steps which repeat every day: construct most recent time steps add new time steps to existing experience replay buffer fine-tune existing model compare existing model and fine-tuned model and keep the one that\u2019s best what would be the best way to do this using ? is it even possible given that circular dependencies are not allowed?",
        "Question_original_content_gpt_summary":"The user is looking for the best way to continually update a reinforcement learning model with new data while avoiding circular dependencies.",
        "Question_preprocessed_content":"Title: how to continually update a model with new data; Content: i have a re rcement learning use case with the following initial steps construct initial episodes add episodes to initial experience replay buffer train initial model then the following steps which repeat every day construct most recent time steps add new time steps to existing experience replay buffer fine tune existing model compare existing model and fine tuned model and keep the one thats best what would be the best way to do this using ? is it even possible given that circular dependencies are not allowed?",
        "Answer_original_content":"you can use persist: true to prevent outputs from being deleted during stage execution (see https:\/\/.org\/doc\/command-reference\/repro#description), so you can feed in new data each day and have the subsequent stages read from their existing outputs before writing out newly modified versions. thanks! i think that should work",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"you can use to prevent outputs from being deleted during stage execution , so you can feed in new data each day and have the subsequent stages read from their existing outputs before writing out newly modified versions. thanks! i think that should work"
    },
    {
        "Question_id":null,
        "Question_title":"Problem of not seeing all the images in media,",
        "Question_body":"<p>In the workspace\/images, i am only seeing 30 images, when with oldest images getting deleted every time. Is there any limits that this images slot can show? If yes, is there any way that i can see more images that is saved on locally saved log folder??<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1666267346925,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":380.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/problem-of-not-seeing-all-the-images-in-media\/3292",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-20T13:10:32.472Z",
                "Answer_body":"<p>Hi ilhoon,<\/p>\n<p>Thanks for writing in! I was wondering if you could explain me how are you logging these images and if you could send me a link to the Workspace and so I can see the error. Thanks!<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-25T09:27:47.101Z",
                "Answer_body":"<p>Hi ilhoon,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-19T12:02:46.431Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: problem of not seeing all the images in media,; Content: in the workspace\/images, i am only seeing 30 images, when with oldest images getting deleted every time. is there any limits that this images slot can show? if yes, is there any way that i can see more images that is saved on locally saved log folder??",
        "Question_original_content_gpt_summary":"The user is encountering a challenge of not being able to view all the images in their workspace\/images, as older images are being deleted and there appears to be a limit to the number of images that can be viewed.",
        "Question_preprocessed_content":"Title: problem of not seeing all the images in media,; Content: in the i am only seeing images, when with oldest images getting deleted every time. is there any limits that this images slot can show? if yes, is there any way that i can see more images that is saved on locally saved log folder??",
        "Answer_original_content":"hi ilhoon, thanks for writing in! i was wondering if you could explain me how are you logging these images and if you could send me a link to the workspace and so i can see the error. thanks! best, luis hi ilhoon, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi ilhoon, thanks for writing in! i was wondering if you could explain me how are you logging these images and if you could send me a link to the workspace and so i can see the error. thanks! best, luis hi ilhoon, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis"
    },
    {
        "Question_id":69479488.0,
        "Question_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Question_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1633602279323,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":418.0,
        "Owner_creation_time":1583491811220,
        "Owner_last_access_time":1663774319043,
        "Owner_reputation":23.0,
        "Owner_up_votes":3.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1633946464143,
        "Answer_score":0.0,
        "Owner_location":"Baku, Azerbaijan",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69479488",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: hi. i am very new to , and want to implement project on my own ml model. however i am getting \"\"could not find main among entry points\"\"; Content: the full error message is as below: error .cli: === could not find main among entry points [] or interpret main as a runnable script. supported script file extensions: ['.py', '.sh'] === i also try the solutions suggested here https:\/\/github.com\/\/\/issues\/1094, but the result is the same. below i provide all the required files to run project. the conda.yaml file name: lightgbm-example channels: - conda-forge dependencies: - python=3.6 - pip - pip: - >=1.6.0 - lightgbm - pandas - numpy the mlproject file name: lightgbm-example conda_env: ~\/desktop\/\/conda.yaml entry-points: main: parameters: learning_rate: {type: float, default: 0.1} colsample_bytree: {type: float, default: 1.0} subsample: {type: float, default: 1.0} command: | python3 ~\/desktop\/\/test.py \\ --learning-rate={learning_rate} \\ --colsample-bytree={colsample_bytree} \\ --subsample={subsample} my test.py file import pandas as pd import lightgbm as lgb import numpy as np import import .lightgbm import argparse from sklearn.metrics import accuracy_score, confusion_matrix def parse_args(): parser = argparse.argumentparser(description=\"lightgbm example\") parser.add_argument( \"--learning-rate\", type=float, default=0.1, help=\"learning rate to update step size at each boosting step (default: 0.3)\", ) parser.add_argument( \"--colsample-bytree\", type=float, default=1.0, help=\"subsample ratio of columns when constructing each tree (default: 1.0)\", ) parser.add_argument( \"--subsample\", type=float, default=1.0, help=\"subsample ratio of the training instances (default: 1.0)\", ) return parser.parse_args() def find_specificity(c_matrix): specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1]) return specificity def main(): args = parse_args() df = pd.read_csv('~\/desktop\/\/churn_demo.csv') train_df = df.sample(frac=0.8, random_state=25) test_df = df.drop(train_df.index) train_df.drop(['subscriberid'], axis = 1, inplace = true) test_df.drop(['subscriberid'], axis = 1, inplace = true) trainx = train_df.iloc[:,:-1] trainy = train_df.iloc[:,-1] testx = test_df.iloc[:,:-1] testy = test_df.iloc[:,-1] .lightgbm.autolog() dtrain = lgb.dataset(trainx, label=trainy) dtest = lgb.dataset(testx, label=testy) with .start_run(): parameters = { 'objective': 'binary', 'device':'cpu', 'num_threads': 6, 'num_leaves': 127, 'metric' : 'binary', 'lambda_l2':5, 'max_bin': 63, 'bin_construct_sample_cnt' :2*1000*1000, 'learning_rate': args.learning_rate, 'colsample_bytree': args.colsample_bytree, 'subsample': args.subsample, 'verbose': 1 } model = lgb.train(parameters, dtrain, valid_sets=dtest, num_boost_round=10000, early_stopping_rounds=10) y_proba=model.predict(testx) pred=np.where(y_proba>0.25,1,0) conf_matrix = confusion_matrix(testy,pred) specificity = find_specificity(conf_matrix) acc = accuracy_score(testy,pred) .log_metric({\"specificity\" : specificity, \"accuracy\" : acc}) if __name__ == \"__main__\": main()",
        "Question_original_content_gpt_summary":"The user is encountering an error message \"could not find main among entry points\" while attempting to implement a project on their own ML model.",
        "Question_preprocessed_content":"Title: hi. i am very new to , and want to implement project on my own ml model. however i am getting could not find main among entry points; Content: the full error message is as below i also try the solutions suggested here , but the result is the same. below i provide all the required files to run project. the file the mlproject file my file",
        "Answer_original_content":"fortunately, i have been resolved my problem. i list some solutions for the same error which can help you in the future if you face the same problem. file names. the file names should be the same suggested in docs https:\/\/.org\/ . for example not conda.yamp, but conda.yaml, as there was such problem in https:\/\/github.com\/\/\/issues\/3856 the conda.yaml file does not support tab, please consider using spaces instead in the mlproject file name 'p' should be the upper case before 1.4. but the later versions it does not matter as explained there https:\/\/github.com\/\/\/issues\/1094 (in my case) mlproject file is space sensitive. let the https:\/\/github.com\/\/\/tree\/master\/examples github examples guide you.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"fortunately, i have been resolved my problem. i list some solutions for the same error which can help you in the future if you face the same problem. file names. the file names should be the same suggested in docs . for example not , but , as there was such problem in the file does not support tab, please consider using spaces instead in the mlproject file name 'p' should be the upper case before but the later versions it does not matter as explained there in my case mlproject file is space sensitive. let the github examples guide you."
    },
    {
        "Question_id":64849557.0,
        "Question_title":"SageMaker Batch Transform fails with ID Column",
        "Question_body":"<p>I am using SageMaker pipeline to do inference on test data. The Pipeline uses a SKLearn perprocessor and a XGBoost model. The pipeline works fine on data without an ID column. However, when I try to include an ID column to track the predictions, it fails. I have given the code snippets below.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.predictor import json_serializer, csv_serializer, json_deserializer\n\ninput_data_path = 's3:\/\/batch-transform\/input-data\/validation_data.csv'\noutput_data_path = 's3:\/\/batch-transform\/predictions\/'\n\ntransform_job = sagemaker.transformer.Transformer(\n    model_name = model_name,\n    instance_count = 1,\n    instance_type = 'ml.m4.xlarge',\n    strategy = 'MultiRecord',\n    assemble_with = 'Line',\n    output_path = output_data_path,\n    base_transform_job_name='pipeline_with_id',\n    sagemaker_session=sagemaker.Session(),\n    accept = 'text\/csv')\n\ntransform_job.transform(data = input_data_path,\n                        content_type = 'text\/csv', \n                        split_type = 'Line',\n                        input_filter='$[1:]', \n                        join_source='Input')\n                        output_filter='$[0,-1]')\n<\/code><\/pre>\n<p>This results in the following error:<\/p>\n<pre><code>Fail to join data: mismatched line count between the input and the output\n<\/code><\/pre>\n<p>I am following the example given in this page:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a><\/p>\n<p>Can someone provide pointers to what is causing the error? Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1605472910920,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1080.0,
        "Owner_creation_time":1319019150600,
        "Owner_last_access_time":1662714723143,
        "Owner_reputation":3073.0,
        "Owner_up_votes":238.0,
        "Owner_down_votes":5.0,
        "Owner_views":341.0,
        "Answer_body":"<p>Came across the same issue.<\/p>\n<p>Check the number of rows returned after prediction in your serving code. In my case, my prediction output didn't have a column header.<\/p>\n<p>e.g. As a text\/csv response, using batch transform with join will post join the input &amp; output.<\/p>\n<p>A single input record would be [[&quot;feature_1&quot;, &quot;feature_2&quot;],[0, 1]], while my model predicted output returned [1].<\/p>\n<p>add column name to predicted output like this [&quot;result&quot;, 1] then returning the csv result will yield [[&quot;result&quot;],[1]] matching input.<\/p>\n<p>P.S. you may need to find a scalable way of doing this for multi-row  batch. Not sure.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1610703448710,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64849557",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: batch transform fails with id column; Content: i am using pipeline to do inference on test data. the pipeline uses a sklearn perprocessor and a xgboost model. the pipeline works fine on data without an id column. however, when i try to include an id column to track the predictions, it fails. i have given the code snippets below. import from .predictor import json_serializer, csv_serializer, json_deserializer input_data_path = 's3:\/\/batch-transform\/input-data\/validation_data.csv' output_data_path = 's3:\/\/batch-transform\/predictions\/' transform_job = .transformer.transformer( model_name = model_name, instance_count = 1, instance_type = 'ml.m4.xlarge', strategy = 'multirecord', assemble_with = 'line', output_path = output_data_path, base_transform_job_name='pipeline_with_id', _session=.session(), accept = 'text\/csv') transform_job.transform(data = input_data_path, content_type = 'text\/csv', split_type = 'line', input_filter='$[1:]', join_source='input') output_filter='$[0,-1]') this results in the following error: fail to join data: mismatched line count between the input and the output i am following the example given in this page: https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon--batch-transform\/ can someone provide pointers to what is causing the error? thank you",
        "Question_original_content_gpt_summary":"The user is encountering challenges when attempting to use a pipeline with an ID column to do inference on test data, resulting in an error due to mismatched line count between the input and the output.",
        "Question_preprocessed_content":"Title: batch transform fails with id column; Content: i am using pipeline to do inference on test data. the pipeline uses a sklearn perprocessor and a xgboost model. the pipeline works fine on data without an id column. however, when i try to include an id column to track the predictions, it fails. i have given the code snippets below. this results in the following error i am following the example given in this page can someone provide pointers to what is causing the error? thank you",
        "Answer_original_content":"came across the same issue. check the number of rows returned after prediction in your serving code. in my case, my prediction output didn't have a column header. e.g. as a text\/csv response, using batch transform with join will post join the input & output. a single input record would be [[\"feature_1\", \"feature_2\"],[0, 1]], while my model predicted output returned [1]. add column name to predicted output like this [\"result\", 1] then returning the csv result will yield [[\"result\"],[1]] matching input. p.s. you may need to find a scalable way of doing this for multi-row batch. not sure.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"came across the same issue. check the number of rows returned after prediction in your serving code. in my case, my prediction output didn't have a column header. as a response, using batch transform with join will post join the input & output. a single input record would be , , while my model predicted output returned . add column name to predicted output like this then returning the csv result will yield , matching input. you may need to find a scalable way of doing this for multi row batch. not sure."
    },
    {
        "Question_id":60293401.0,
        "Question_title":"Hosting sklearn - SVM Classifier on AWS Sagemaker",
        "Question_body":"<p>I have a model running on my jupyter notebook instance with very basic SVM classifier <\/p>\n\n<pre><code># Text lassifier - Algorithm - SVM\n# fit the training dataset on the classifier\nSVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto',probability=True)\nSVM.fit(Train_X_Tfidf,Train_Y)\n# predict the labels on validation dataset\npredictions_SVM = SVM.predict(Test_X_Tfidf)\n# Use accuracy_score function to get the accuracy\nprint(\"SVM Accuracy Score -&gt; \",accuracy_score(predictions_SVM, Test_Y)*100)\n<\/code><\/pre>\n\n<p>Use Case : Host the model on Sagemaker and create an endpoint.Use the end point via Lambda for text classification<\/p>\n\n<p>I saw AWS has few posts on creating an endpoint E.g. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html<\/a> but majority of the content is not applicable to scikit-learn : SVM <\/p>\n\n<p>Is there an another approach I should be looking at ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1582088810897,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":388.0,
        "Owner_creation_time":1470328461176,
        "Owner_last_access_time":1625777078910,
        "Owner_reputation":686.0,
        "Owner_up_votes":36.0,
        "Owner_down_votes":1.0,
        "Owner_views":113.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60293401",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: hosting sklearn - svm classifier on ; Content: i have a model running on my jupyter notebook instance with very basic svm classifier # text lassifier - algorithm - svm # fit the training dataset on the classifier svm = svm.svc(c=1.0, kernel='linear', degree=3, gamma='auto',probability=true) svm.fit(train_x_tfidf,train_y) # predict the labels on validation dataset predictions_svm = svm.predict(test_x_tfidf) # use accuracy_score function to get the accuracy print(\"svm accuracy score -> \",accuracy_score(predictions_svm, test_y)*100) use case : host the model on and create an endpoint.use the end point via lambda for text classification i saw aws has few posts on creating an endpoint e.g. https:\/\/docs.aws.amazon.com\/\/latest\/dg\/ex1-train-model.html but majority of the content is not applicable to scikit-learn : svm is there an another approach i should be looking at ?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in hosting a Scikit-Learn SVM classifier on AWS and creating an endpoint to use via Lambda for text classification.",
        "Question_preprocessed_content":"Title: hosting sklearn svm classifier on; Content: i have a model running on my jupyter notebook instance with very basic svm classifier use case host the model on and create an the end point via lambda for text classification i saw aws has few posts on creating an endpoint but majority of the content is not applicable to scikit learn svm is there an another approach i should be looking at ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"How to debug my init git container",
        "Question_body":"From slack\n\nMy job is stack with a warning status, I configured a private bitbucket connection and the cloning fails.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1649328334000,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1472",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-07T10:51:48Z",
                "Answer_score":1,
                "Answer_body":"Before updating the connections or changing anything about your current deployment, please perform the following debugging steps:\n\nEnable logs from all containers:\n\n\n\nYou can also inspect the operations from the statuses page to get more information (for distributed runs you can select the correct pod)\n\n\nYou can suspend the init container using :\n\n  - connection: my-connection\n    git: {...}\n    container:\n      command: [\"\/bin\/bash\", \"-c\"]\n      args: [\"sleep 3600\"]\nUse shell to get inside the container (for distributed runs you can select the correct pod and container):"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0,
        "Question_original_content":"Title: how to debug my init git container; Content: from slack my job is stack with a warning status, i configured a private bitbucket connection and the cloning fails.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with debugging their init git container, as their job is stuck with a warning status, and their private Bitbucket connection and cloning fails.",
        "Question_preprocessed_content":"Title: how to debug my init git container; Content: from slack my job is stack with a warning status, i configured a private bitbucket connection and the cloning fails.",
        "Answer_original_content":"before updating the connections or changing anything about your current deployment, please perform the following debugging steps: enable logs from all containers: you can also inspect the operations from the statuses page to get more information (for distributed runs you can select the correct pod) you can suspend the init container using : - connection: my-connection git: {...} container: command: [\"\/bin\/bash\", \"-c\"] args: [\"sleep 3600\"] use shell to get inside the container (for distributed runs you can select the correct pod and container):",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"before updating the connections or changing anything about your current deployment, please perform the following debugging steps enable logs from all containers you can also inspect the operations from the statuses page to get more you can suspend the init container using connection my connection git container command args use shell to get inside the container"
    },
    {
        "Question_id":null,
        "Question_title":"Advanced legend doesn't display properly or displays [Object Object]",
        "Question_body":"<p>I am trying to customize the legend on plots using the Advanced Legend syntax. But have a couple of problems there. Here is the workspace\/plot: <a href=\"https:\/\/wandb.ai\/gat\/wandb-debug5?workspace=user-wjgat\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>\n<p>I am using the following string in the advanced legend: <code>A=(${config:spec.url}) B=(${config:url}) C=${config:spec_name}<\/code><\/p>\n<p>This should render as `A=(http:\\google.com\/) B=(http:\\google.com\/) C=(google) but:<\/p>\n<ol>\n<li>It renders as <code>A=( ) B=(http:\\\\google.com) C=google<\/code>\n<\/li>\n<li>The \u201con hover\u201d legend renders as <code>A=( ) B=([Object Object]) C=google<\/code>\n<\/li>\n<\/ol>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/3\/339707641144d812c66bf80fbc18a6b6f79de511.png\" data-download-href=\"\/uploads\/short-url\/7mnVAwnMPAidW9iqjOHkGnGgI6J.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/339707641144d812c66bf80fbc18a6b6f79de511_2_690x420.png\" alt=\"image\" data-base62-sha1=\"7mnVAwnMPAidW9iqjOHkGnGgI6J\" width=\"690\" height=\"420\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/339707641144d812c66bf80fbc18a6b6f79de511_2_690x420.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/3\/339707641144d812c66bf80fbc18a6b6f79de511_2_1035x630.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/3\/339707641144d812c66bf80fbc18a6b6f79de511.png 2x\" data-dominant-color=\"F4F6FB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1190\u00d7726 100 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Are those bugs in wandb or I don\u2019t get sth?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_creation_time":1672849220018,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":95.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/advanced-legend-doesnt-display-properly-or-displays-object-object\/3638",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-09T21:47:50.594Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/wjaskowski\">@wjaskowski<\/a> thanks for writing in and happy to help. I reviewed your workspace and your legend expression is correct. The legend label  does render correctly but as you mentioned the tooltip does not. This appears to be a bug on our end with how the tooltip handles URL visualization. I reported it to our eng team and will update you once I hear back on a fix. Thanks for reporting!<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-09T22:16:16.109Z",
                "Answer_body":"<p>Thx for looking into <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a>! But why do you say that the legend renders correctly when it renders <code>A=( )<\/code> instead of <code>A=(https:\\\\google.com)<\/code>?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-09T22:48:29.272Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/wjaskowski\">@wjaskowski<\/a> , apologies. I made some test edits o a copy of your workspace and used <code>A=(${config:spec_url})<\/code> which renders correctly. Your exact example isn\u2019t rendering.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-10T08:20:42.741Z",
                "Answer_body":"<p>Which means that this is another bug, isn\u2019t it?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-10T20:52:32.776Z",
                "Answer_body":"<p>Correct, it is a bug.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-10T22:28:12.936Z",
                "Answer_body":"<p>Thank you <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> for confirming that. This drove me crazy for a while :\/. Is there a place where I can track the status of this bug?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"",
        "Answer_original_content":"",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":""
    },
    {
        "Question_id":59881727.0,
        "Question_title":"Debugging R Scripts in azure-ml: Where can stdout and stderr logs be found? (or why are they empty?)",
        "Question_body":"<p>I'm using \"studio (preview)\" from Microsoft Azure Machine Learning to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to our data warehouse.<\/p>\n\n<p>In the \"Designer\", an \"Exectue R Script\" action can be added to the pipeline. I'm using this functionality to execute some of my own machine learning algorithms.<\/p>\n\n<p>I've got a 'hello world' version of this script working (including using the \"script bundle\" to load the functions in my own R files). It applies a very simple manipulation (compute the days difference with the date in the date column and 'today'), and stores the output as a new file. Given that the exported file has the correct information, I know that the R script works well.<\/p>\n\n<p>The script looks like this:<\/p>\n\n<pre><code># R version: 3.5.1\n# The script MUST contain a function named azureml_main\n# which is the entry point for this module.\n\n# The entry point function can contain up to two input arguments:\n#   Param&lt;medals&gt;: a R DataFrame\n#   Param&lt;matches&gt;: a R DataFrame\n\nazureml_main &lt;- function(dataframe1, dataframe2){\n\n  message(\"STARTING R script run.\")\n\n  # If a zip file is connected to the third input port, it is\n  # unzipped under \".\/Script Bundle\". This directory is added\n  # to sys.path.\n\n  message('Adding functions as source...')\n\n  if (FALSE) {\n    # This works...\n      source(\".\/Script Bundle\/first_function_for_script_bundle.R\")\n  } else {\n    # And this works as well!\n    message('Sourcing all available functions...')\n    functions_folder = '.\/Script Bundle'\n\n    list.files(path = functions_folder)\n    list_of_R_functions &lt;- list.files(path = functions_folder, pattern = \"^.*[Rr]$\", include.dirs = FALSE, full.names = TRUE)\n    for (fun in list_of_R_functions) {\n\n      message(sprintf('Sourcing &lt;%s&gt;...', fun))\n\n      source(fun)\n\n    }\n  }\n\n  message('Executing R pipeline...')\n  dataframe1 = calculate_days_difference(dataframe = dataframe1)\n\n  # Return datasets as a Named List\n  return(list(dataset1=dataframe1, dataset2=dataframe2))\n}\n<\/code><\/pre>\n\n<p>And although I do print some messages in the R Script, I haven't been able to find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages.<\/p>\n\n<p>I need the printed messages for 1) information on how the analysis went and -most importantly- 2) debugging in case the code failed.<\/p>\n\n<p>Now, I have found (on multiple locations) the files \"stdoutlogs.txt\" and \"stderrlogs.txt\". These can be found under \"Logs\" when I click on \"Exectue R Script\" in the \"Designer\".\nI can also find \"stdoutlogs.txt\" and \"stderrlogs.txt\" files under \"Experiments\" when I click on a finished \"Run\" and then both under the tab \"Outputs\" and under the tab \"Logs\".\nHowever... all of these files are empty.<\/p>\n\n<p>Can anyone tell me how I can print messages from my R Script and help me locate where I can find the printed information?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1579792546180,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":287.0,
        "Owner_creation_time":1534511592567,
        "Owner_last_access_time":1663852418420,
        "Owner_reputation":423.0,
        "Owner_up_votes":15.0,
        "Owner_down_votes":0.0,
        "Owner_views":17.0,
        "Answer_body":"<p>Can you please click on the \"Execute R module\" and download the 70_driver.log? I tried message(\"STARTING R script run.\") in an R sample and can found the output there.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z7s7h.png\" alt=\"view logs for a execute R script module\"><\/a><\/p>",
        "Answer_comment_count":5.0,
        "Answer_creation_time":1579829908723,
        "Answer_score":2.0,
        "Owner_location":"Netherlands",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1579846977960,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59881727",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: debugging r scripts in azure-ml: where can stdout and stderr logs be found? (or why are they empty?); Content: i'm using \"studio (preview)\" from to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to our data warehouse. in the \"designer\", an \"exectue r script\" action can be added to the pipeline. i'm using this functionality to execute some of my own machine learning algorithms. i've got a 'hello world' version of this script working (including using the \"script bundle\" to load the functions in my own r files). it applies a very simple manipulation (compute the days difference with the date in the date column and 'today'), and stores the output as a new file. given that the exported file has the correct information, i know that the r script works well. the script looks like this: # r version: 3.5.1 # the script must contain a function named _main # which is the entry point for this module. # the entry point function can contain up to two input arguments: # param<medals>: a r dataframe # param<matches>: a r dataframe _main <- function(dataframe1, dataframe2){ message(\"starting r script run.\") # if a zip file is connected to the third input port, it is # unzipped under \".\/script bundle\". this directory is added # to sys.path. message('adding functions as source...') if (false) { # this works... source(\".\/script bundle\/first_function_for_script_bundle.r\") } else { # and this works as well! message('sourcing all available functions...') functions_folder = '.\/script bundle' list.files(path = functions_folder) list_of_r_functions <- list.files(path = functions_folder, pattern = \"^.*[rr]$\", include.dirs = false, full.names = true) for (fun in list_of_r_functions) { message(sprintf('sourcing <%s>...', fun)) source(fun) } } message('executing r pipeline...') dataframe1 = calculate_days_difference(dataframe = dataframe1) # return datasets as a named list return(list(dataset1=dataframe1, dataset2=dataframe2)) } and although i do print some messages in the r script, i haven't been able to find the \"stdoutlogs\" nor the \"stderrlogs\" that should contain these printed messages. i need the printed messages for 1) information on how the analysis went and -most importantly- 2) debugging in case the code failed. now, i have found (on multiple locations) the files \"stdoutlogs.txt\" and \"stderrlogs.txt\". these can be found under \"logs\" when i click on \"exectue r script\" in the \"designer\". i can also find \"stdoutlogs.txt\" and \"stderrlogs.txt\" files under \"experiments\" when i click on a finished \"run\" and then both under the tab \"outputs\" and under the tab \"logs\". however... all of these files are empty. can anyone tell me how i can print messages from my r script and help me locate where i can find the printed information?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with debugging their R scripts in Azure-ML, as they are unable to locate the stdout and stderr logs which should contain the printed messages from their script.",
        "Question_preprocessed_content":"Title: debugging r scripts in where can stdout and stderr logs be found?; Content: i'm using studio from to create a pipeline that applies machine learning to a dataset in a blob storage that is connected to our data warehouse. in the designer , an exectue r script action can be added to the pipeline. i'm using this functionality to execute some of my own machine learning algorithms. i've got a 'hello world' version of this script working . it applies a very simple manipulation , and stores the output as a new file. given that the exported file has the correct , i know that the r script works well. the script looks like this and although i do print some messages in the r script, i haven't been able to find the stdoutlogs nor the stderrlogs that should contain these printed messages. i need the printed messages for on how the analysis went and most importantly debugging in case the code failed. now, i have found the files and these can be found under logs when i click on exectue r script in the designer . i can also find and files under experiments when i click on a finished run and then both under the tab outputs and under the tab logs . all of these files are empty. can anyone tell me how i can print messages from my r script and help me locate where i can find the printed ?",
        "Answer_original_content":"can you please click on the \"execute r module\" and download the 70_driver.log? i tried message(\"starting r script run.\") in an r sample and can found the output there.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"can you please click on the execute r module and download the i tried message in an r sample and can found the output there."
    },
    {
        "Question_id":53695161.0,
        "Question_title":"How do I resolve a SQL ParseError in PySpark?",
        "Question_body":"<p>I'm a new-bee to PySpark and AWS Sagemaker using Jupyter Notebook. I do know how to write SQL statements to answer my questions. This code piece is supposed to:\n1. Extract available methods of death dispositions in my dataset (CDC Death Data -in CSV) by year\n2. Count the frequency for each disposition by year<\/p>\n\n<p>I was able to run the SQL statement on the same dataset in a MySQL database. But once I added the query to my PySpark code, I got a <code>ParseError<\/code> Please, see the error below. <\/p>\n\n<p>How would I go about resolving this error? and if I want to create a graph\/plot with the output, how would I go about it?\nTHANKS<\/p>\n\n<pre><code>df.registerTempTable(\"data\")\nmethods = sqlContext.sql(\"\\\n    SELECT current_data_year AS Year, \\\n        CASE method_of_disposition \\\n            WHEN 'C' THEN 'Cremation' \\\n            WHEN 'B' THEN 'Burial' \\\n            WHEN 'D' THEN 'D' \\\n            WHEN 'E' THEN 'E' \\\n            WHEN 'O' THEN 'O' \\\n            WHEN 'R' THEN 'R' \\\n            WHEN 'U' THEN 'Unknown' \\\n            END AS 'Method of Disposition', \\\n        COUNT(method_of_disposition) AS Count \\\n        FROM data \\\n        GROUP BY current_data_year, method_of_disposition\\\n    \").show()\n<\/code><\/pre>\n\n<p>MY NEW OUTPUT<\/p>\n\n<pre><code>+----+-------------------+-------+\n|Year|MethodofDisposition|  Count|\n+----+-------------------+-------+\n|   0|               null|     10|\n|2005|              Other|   2199|\n|2005|           Donation|   4795|\n|2005|                  E|  21247|\n|2005|     RemovedFromUSA|  31954|\n|2005|          Cremation| 350018|\n|2005|             Burial| 553202|\n|2005|            Unknown|1489091|\n|2006|              Other|   2252|\n|2006|           Donation|   6883|\n|2006|                  E|  23412|\n|2006|     RemovedFromUSA|  40870|\n|2006|          Cremation| 423282|\n|2006|             Burial| 667169|\n|2006|            Unknown|1266857|\n|2007|              Other|   3119|\n|2007|           Donation|   8719|\n|2007|                  E|  26139|\n|2007|     RemovedFromUSA|  41411|\n|2007|          Cremation| 472220|\n|2007|             Burial| 725666|\n|2007|            Unknown|1151069|\n|2008|              Other|   5511|\n|2008|           Donation|  10981|\n|2008|                  E|  31913|\n|2008|     RemovedFromUSA|  44713|\n|2008|          Cremation| 579827|\n|2008|             Burial| 866384|\n|2008|            Unknown| 937482|\n|2009|              Other|   3688|\n|2009|           Donation|  12011|\n|2009|                  E|  30344|\n|2009|     RemovedFromUSA|  45451|\n|2009|          Cremation| 599202|\n|2009|             Burial| 802305|\n|2009|            Unknown| 948218|\n|2010|              Other|   3782|\n|2010|           Donation|  15208|\n|2010|                  E|  32807|\n|2010|     RemovedFromUSA|  47899|\n|2010|          Cremation| 706224|\n|2010|            Unknown| 760192|\n|2010|             Burial| 906430|\n|2011|              Other|   5169|\n|2011|           Donation|  17450|\n|2011|                  E|  33847|\n|2011|     RemovedFromUSA|  47199|\n|2011|            Unknown| 685325|\n|2011|          Cremation| 780480|\n|2011|             Burial| 950372|\n|2012|              Other|   6649|\n|2012|           Donation|  20790|\n|2012|                  E|  35110|\n|2012|     RemovedFromUSA|  52896|\n|2012|            Unknown| 440569|\n|2012|          Cremation| 898222|\n|2012|             Burial|1093628|\n|2013|              Other|   6962|\n|2013|           Donation|  21653|\n|2013|                  E|  36949|\n|2013|     RemovedFromUSA|  53678|\n|2013|            Unknown| 395080|\n|2013|          Cremation| 973768|\n|2013|             Burial|1113362|\n|2014|              Other|   7871|\n|2014|           Donation|  24004|\n|2014|                  E|  39321|\n|2014|     RemovedFromUSA|  59884|\n|2014|            Unknown| 242963|\n|2014|          Cremation|1094292|\n|2014|             Burial|1162836|\n|2015|              Other|  11729|\n|2015|           Donation|  27870|\n|2015|                  E|  40880|\n|2015|     RemovedFromUSA|  71744|\n|2015|            Unknown|  74050|\n|2015|          Cremation|1244297|\n|2015|             Burial|1247628|\n+----+-------------------+-------+\n<\/code><\/pre>\n\n<p>ERROR MESSAGE<\/p>\n\n<pre><code>---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/pyspark\/sql\/utils.py in deco(*a, **kw)\n     62         try:\n---&gt; 63             return f(*a, **kw)\n     64         except py4j.protocol.Py4JJavaError as e:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/py4j\/protocol.py in get_return_value(answer, gateway_client, target_id, name)\n    318                     \"An error occurred while calling {0}{1}{2}.\\n\".\n--&gt; 319                     format(target_id, \".\", name), value)\n    320             else:\n\nPy4JJavaError: An error occurred while calling o19.sql.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nextraneous input ''Method of Disposition'' expecting {&lt;EOF&gt;, ',', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 213)\n\n== SQL ==\nSELECT current_data_year AS Year, CASE method_of_disposition WHEN 'C' THEN 'Cremation' WHEN 'B' THEN 'Burial' WHEN 'D' THEN 'D' WHEN 'E' THEN 'E' WHEN 'O' THEN 'O' WHEN 'R' THEN 'R' WHEN 'U' THEN 'Unknown' END AS 'Method of Disposition', COUNT(method_of_disposition) AS Count FROM data GROUP BY current_data_year, method_of_disposition\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\n\n    at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:217)\n    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:114)\n    at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:68)\n    at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:632)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:280)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.GatewayConnection.run(GatewayConnection.java:214)\n    at java.lang.Thread.run(Thread.java:745)\n\n\nDuring handling of the above exception, another exception occurred:\n\nParseException                            Traceback (most recent call last)\n&lt;ipython-input-7-f99c8a5b941c&gt; in &lt;module&gt;()\n      1 #Grouping and counting Cremation vs Burial by Year\n      2 df.registerTempTable(\"data\")\n----&gt; 3 sqlContext.sql(\"SELECT current_data_year AS Year, CASE method_of_disposition WHEN 'C' THEN 'Cremation' WHEN 'B' THEN 'Burial' WHEN 'D' THEN 'D' WHEN 'E' THEN 'E' WHEN 'O' THEN 'O' WHEN 'R' THEN 'R' WHEN 'U' THEN 'Unknown' END AS 'Method of Disposition', COUNT(method_of_disposition) AS Count FROM data GROUP BY current_data_year, method_of_disposition\").show()\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/pyspark\/sql\/context.py in sql(self, sqlQuery)\n    382         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\n    383         \"\"\"\n--&gt; 384         return self.sparkSession.sql(sqlQuery)\n    385 \n    386     @since(1.0)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/pyspark\/sql\/session.py in sql(self, sqlQuery)\n    601         [Row(f1=1, f2=u'row1'), Row(f1=2, f2=u'row2'), Row(f1=3, f2=u'row3')]\n    602         \"\"\"\n--&gt; 603         return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n    604 \n    605     @since(2.0)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/py4j\/java_gateway.py in __call__(self, *args)\n   1131         answer = self.gateway_client.send_command(command)\n   1132         return_value = get_return_value(\n-&gt; 1133             answer, self.gateway_client, self.target_id, self.name)\n   1134 \n   1135         for temp_arg in temp_args:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/pyspark\/sql\/utils.py in deco(*a, **kw)\n     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n     72             if s.startswith('org.apache.spark.sql.catalyst.parser.ParseException: '):\n---&gt; 73                 raise ParseException(s.split(': ', 1)[1], stackTrace)\n     74             if s.startswith('org.apache.spark.sql.streaming.StreamingQueryException: '):\n     75                 raise StreamingQueryException(s.split(': ', 1)[1], stackTrace)\n\nParseException: \"\\nextraneous input ''Method of Disposition'' expecting {&lt;EOF&gt;, ',', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 213)\\n\\n== SQL ==\\nSELECT current_data_year AS Year, CASE method_of_disposition WHEN 'C' THEN 'Cremation' WHEN 'B' THEN 'Burial' WHEN 'D' THEN 'D' WHEN 'E' THEN 'E' WHEN 'O' THEN 'O' WHEN 'R' THEN 'R' WHEN 'U' THEN 'Unknown' END AS 'Method of Disposition', COUNT(method_of_disposition) AS Count FROM data GROUP BY current_data_year, method_of_disposition\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\\n\"\n<\/code><\/pre>\n\n<p>Sample MYSQL OUTPUT\n<a href=\"https:\/\/i.stack.imgur.com\/Jqq1I.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Jqq1I.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5.0,
        "Question_creation_time":1544378611580,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1911.0,
        "Owner_creation_time":1497281061712,
        "Owner_last_access_time":1563952634216,
        "Owner_reputation":67.0,
        "Owner_up_votes":7.0,
        "Owner_down_votes":0.0,
        "Owner_views":9.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Washing DC",
        "Question_last_edit_time":1544398273427,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53695161",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i resolve a sql parseerror in pyspark?; Content: i'm a new-bee to pyspark and using jupyter notebook. i do know how to write sql statements to answer my questions. this code piece is supposed to: 1. extract available methods of death dispositions in my dataset (cdc death data -in csv) by year 2. count the frequency for each disposition by year i was able to run the sql statement on the same dataset in a mysql database. but once i added the query to my pyspark code, i got a parseerror please, see the error below. how would i go about resolving this error? and if i want to create a graph\/plot with the output, how would i go about it? thanks df.registertemptable(\"data\") methods = sqlcontext.sql(\"\\ select current_data_year as year, \\ case method_of_disposition \\ when 'c' then 'cremation' \\ when 'b' then 'burial' \\ when 'd' then 'd' \\ when 'e' then 'e' \\ when 'o' then 'o' \\ when 'r' then 'r' \\ when 'u' then 'unknown' \\ end as 'method of disposition', \\ count(method_of_disposition) as count \\ from data \\ group by current_data_year, method_of_disposition\\ \").show() my new output +----+-------------------+-------+ |year|methodofdisposition| count| +----+-------------------+-------+ | 0| null| 10| |2005| other| 2199| |2005| donation| 4795| |2005| e| 21247| |2005| removedfromusa| 31954| |2005| cremation| 350018| |2005| burial| 553202| |2005| unknown|1489091| |2006| other| 2252| |2006| donation| 6883| |2006| e| 23412| |2006| removedfromusa| 40870| |2006| cremation| 423282| |2006| burial| 667169| |2006| unknown|1266857| |2007| other| 3119| |2007| donation| 8719| |2007| e| 26139| |2007| removedfromusa| 41411| |2007| cremation| 472220| |2007| burial| 725666| |2007| unknown|1151069| |2008| other| 5511| |2008| donation| 10981| |2008| e| 31913| |2008| removedfromusa| 44713| |2008| cremation| 579827| |2008| burial| 866384| |2008| unknown| 937482| |2009| other| 3688| |2009| donation| 12011| |2009| e| 30344| |2009| removedfromusa| 45451| |2009| cremation| 599202| |2009| burial| 802305| |2009| unknown| 948218| |2010| other| 3782| |2010| donation| 15208| |2010| e| 32807| |2010| removedfromusa| 47899| |2010| cremation| 706224| |2010| unknown| 760192| |2010| burial| 906430| |2011| other| 5169| |2011| donation| 17450| |2011| e| 33847| |2011| removedfromusa| 47199| |2011| unknown| 685325| |2011| cremation| 780480| |2011| burial| 950372| |2012| other| 6649| |2012| donation| 20790| |2012| e| 35110| |2012| removedfromusa| 52896| |2012| unknown| 440569| |2012| cremation| 898222| |2012| burial|1093628| |2013| other| 6962| |2013| donation| 21653| |2013| e| 36949| |2013| removedfromusa| 53678| |2013| unknown| 395080| |2013| cremation| 973768| |2013| burial|1113362| |2014| other| 7871| |2014| donation| 24004| |2014| e| 39321| |2014| removedfromusa| 59884| |2014| unknown| 242963| |2014| cremation|1094292| |2014| burial|1162836| |2015| other| 11729| |2015| donation| 27870| |2015| e| 40880| |2015| removedfromusa| 71744| |2015| unknown| 74050| |2015| cremation|1244297| |2015| burial|1247628| +----+-------------------+-------+ error message --------------------------------------------------------------------------- py4jjavaerror traceback (most recent call last) ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/pyspark\/sql\/utils.py in deco(*a, **kw) 62 try: ---> 63 return f(*a, **kw) 64 except py4j.protocol.py4jjavaerror as e: ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/py4j\/protocol.py in get_return_value(answer, gateway_client, target_id, name) 318 \"an error occurred while calling {0}{1}{2}.\\n\". --> 319 format(target_id, \".\", name), value) 320 else: py4jjavaerror: an error occurred while calling o19.sql. : org.apache.spark.sql.catalyst.parser.parseexception: extraneous input ''method of disposition'' expecting {<eof>, ',', 'from', 'where', 'group', 'order', 'having', 'limit', 'lateral', 'window', 'union', 'except', 'minus', 'intersect', 'sort', 'cluster', 'distribute'}(line 1, pos 213) == sql == select current_data_year as year, case method_of_disposition when 'c' then 'cremation' when 'b' then 'burial' when 'd' then 'd' when 'e' then 'e' when 'o' then 'o' when 'r' then 'r' when 'u' then 'unknown' end as 'method of disposition', count(method_of_disposition) as count from data group by current_data_year, method_of_disposition ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^ at org.apache.spark.sql.catalyst.parser.parseexception.withcommand(parsedriver.scala:217) at org.apache.spark.sql.catalyst.parser.abstractsqlparser.parse(parsedriver.scala:114) at org.apache.spark.sql.execution.sparksqlparser.parse(sparksqlparser.scala:48) at org.apache.spark.sql.catalyst.parser.abstractsqlparser.parseplan(parsedriver.scala:68) at org.apache.spark.sql.sparksession.sql(sparksession.scala:632) at sun.reflect.nativemethodaccessorimpl.invoke0(native method) at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:498) at py4j.reflection.methodinvoker.invoke(methodinvoker.java:244) at py4j.reflection.reflectionengine.invoke(reflectionengine.java:357) at py4j.gateway.invoke(gateway.java:280) at py4j.commands.abstractcommand.invokemethod(abstractcommand.java:132) at py4j.commands.callcommand.execute(callcommand.java:79) at py4j.gatewayconnection.run(gatewayconnection.java:214) at java.lang.thread.run(thread.java:745) during handling of the above exception, another exception occurred: parseexception traceback (most recent call last) <ipython-input-7-f99c8a5b941c> in <module>() 1 #grouping and counting cremation vs burial by year 2 df.registertemptable(\"data\") ----> 3 sqlcontext.sql(\"select current_data_year as year, case method_of_disposition when 'c' then 'cremation' when 'b' then 'burial' when 'd' then 'd' when 'e' then 'e' when 'o' then 'o' when 'r' then 'r' when 'u' then 'unknown' end as 'method of disposition', count(method_of_disposition) as count from data group by current_data_year, method_of_disposition\").show() ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/pyspark\/sql\/context.py in sql(self, sqlquery) 382 [row(f1=1, f2=u'row1'), row(f1=2, f2=u'row2'), row(f1=3, f2=u'row3')] 383 \"\"\" --> 384 return self.sparksession.sql(sqlquery) 385 386 @since(1.0) ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/pyspark\/sql\/session.py in sql(self, sqlquery) 601 [row(f1=1, f2=u'row1'), row(f1=2, f2=u'row2'), row(f1=3, f2=u'row3')] 602 \"\"\" --> 603 return dataframe(self._jsparksession.sql(sqlquery), self._wrapped) 604 605 @since(2.0) ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/py4j\/java_gateway.py in __call__(self, *args) 1131 answer = self.gateway_client.send_command(command) 1132 return_value = get_return_value( -> 1133 answer, self.gateway_client, self.target_id, self.name) 1134 1135 for temp_arg in temp_args: ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/pyspark\/sql\/utils.py in deco(*a, **kw) 71 raise analysisexception(s.split(': ', 1)[1], stacktrace) 72 if s.startswith('org.apache.spark.sql.catalyst.parser.parseexception: '): ---> 73 raise parseexception(s.split(': ', 1)[1], stacktrace) 74 if s.startswith('org.apache.spark.sql.streaming.streamingqueryexception: '): 75 raise streamingqueryexception(s.split(': ', 1)[1], stacktrace) parseexception: \"\\nextraneous input ''method of disposition'' expecting {<eof>, ',', 'from', 'where', 'group', 'order', 'having', 'limit', 'lateral', 'window', 'union', 'except', 'minus', 'intersect', 'sort', 'cluster', 'distribute'}(line 1, pos 213)\\n\\n== sql ==\\nselect current_data_year as year, case method_of_disposition when 'c' then 'cremation' when 'b' then 'burial' when 'd' then 'd' when 'e' then 'e' when 'o' then 'o' when 'r' then 'r' when 'u' then 'unknown' end as 'method of disposition', count(method_of_disposition) as count from data group by current_data_year, method_of_disposition\\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^\\n\" sample mysql output",
        "Question_original_content_gpt_summary":"The user is encountering a SQL parseerror in PySpark while attempting to extract available methods of death dispositions in a dataset and count the frequency for each disposition by year.",
        "Question_preprocessed_content":"Title: how do i resolve a sql parseerror in pyspark?; Content: i'm a new bee to pyspark and using jupyter notebook. i do know how to write sql statements to answer my questions. this code piece is supposed to . extract available methods of death dispositions in my dataset by year . count the frequency for each disposition by year i was able to run the sql statement on the same dataset in a mysql database. but once i added the query to my pyspark code, i got a please, see the error below. how would i go about resolving this error? and if i want to create a with the output, how would i go about it? thanks my new output error message sample mysql output",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":52437543.0,
        "Question_title":"Does sagemaker use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?",
        "Question_body":"<p>As stated in the question, \"Does sagemaker use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?\"<\/p>\n\n<p>Some common error message showed as \"CannotStartContainerError. Please ensure the model container for variant variant-name-1 starts correctly when invoked with 'docker run  serve\u2019.\" and it didn't show as running with nividia driver.<\/p>\n\n<p>So, do we need manually set up?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1537509940017,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":940.0,
        "Owner_creation_time":1537311568807,
        "Owner_last_access_time":1612898933127,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52437543",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: does use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?; Content: as stated in the question, \"does use nvidia-docker or docker runtime==nvidia by default or user need to manually set up?\" some common error message showed as \"cannotstartcontainererror. please ensure the model container for variant variant-name-1 starts correctly when invoked with 'docker run serve\u2019.\" and it didn't show as running with nividia driver. so, do we need manually set up?",
        "Question_original_content_gpt_summary":"The user is questioning whether they need to manually set up nvidia-docker or docker runtime==nvidia in order to avoid common error messages.",
        "Question_preprocessed_content":"Title: does use nvidia docker or docker by default or user need to manually set up?; Content: as stated in the question, does use nvidia docker or docker by default or user need to manually set up? some common error message showed as cannotstartcontainererror. please ensure the model container for variant variant name starts correctly when invoked with 'docker run and it didn't show as running with nividia driver. so, do we need manually set up?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Format error: extra keys not allowed @ data[u'outs'][0][u'size']",
        "Question_body":"<p>I\u2019m trying to integrate dvc with my project for easy pull\/push of AI\/ML models, while the dvc works fine on my Windows, When I placed the code on an ubuntu machine and after setting up the cloud credentials and adding the remote storage. I tried <code>dvc status<\/code> command it started giving the following error. I\u2019m not able to interact with dvc at all.<\/p>\n<p><strong>format error: extra keys not allowed @ data[u\u2019outs\u2019][0][u\u2019size\u2019]<\/strong><\/p>\n<p>Does anybody knows the solution for the same?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1636706233863,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1333.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/format-error-extra-keys-not-allowed-data-uouts-0-usize\/966",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-12T14:46:19.774Z",
                "Answer_body":"<p>Looks like the issue is using <code>Python 2.7<\/code> which we don\u2019t support. We had a similar discussion in Discord (perhaps from the OP themself):<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/discord.com\/channels\/485586884165107732\/563406153334128681\/908633337961271326\">\n  <header class=\"source\">\n      <img src=\"https:\/\/discord.com\/assets\/ec2c34cadd4b5f4594415127380a85e6.ico\" class=\"site-icon\" width=\"256\" height=\"256\">\n\n      <a href=\"https:\/\/discord.com\/channels\/485586884165107732\/563406153334128681\/908633337961271326\" target=\"_blank\" rel=\"noopener\">Discord<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/discord.com\/assets\/652f40427e1f5186ad54836074898279.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/discord.com\/channels\/485586884165107732\/563406153334128681\/908633337961271326\" target=\"_blank\" rel=\"noopener\">Discord - A New Way to Chat with Friends &amp; Communities<\/a><\/h3>\n\n  <p>Discord is the easiest way to communicate over voice, video, and text.  Chat, hang out, and stay close with your friends and communities.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":62.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-05T10:57:10.003Z",
                "Answer_body":"<p>I got that with<\/p>\n<p>dvc remote modify storage --local .dvc\/tmp\/lrg-omics-f94be8f80556.json<br>\nERROR: configuration error - config file error: extra keys not allowed @ data[\u2018remote\u2019][\u2018storage\u2019][\u2018.dvc\/tmp\/lrg-omics-f94be8f80556.json\u2019]<br>\n(pqc) ubuntu@dev:~\/workspace\/ProteomicsQC\/lib\/lrg-omics&gt;python --version<br>\nPython 3.10.2<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-05T12:39:06.902Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/rmbzmb\">@rmbzmb<\/a> what exactly are you trying to achieve with this command? What is your <code>dvc version<\/code>?<\/p>",
                "Answer_score":25.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: format error: extra keys not allowed @ data[u'outs'][0][u'size']; Content: i\u2019m trying to integrate with my project for easy pull\/push of ai\/ml models, while the works fine on my windows, when i placed the code on an ubuntu machine and after setting up the cloud credentials and adding the remote storage. i tried status command it started giving the following error. i\u2019m not able to interact with at all. format error: extra keys not allowed @ data[u\u2019outs\u2019][0][u\u2019size\u2019] does anybody knows the solution for the same?",
        "Question_original_content_gpt_summary":"The user is encountering a format error when attempting to interact with a cloud storage system on an Ubuntu machine after setting up the cloud credentials.",
        "Question_preprocessed_content":"Title: format error extra keys not allowed @ data; Content: im trying to integrate with my project for easy of models, while the works fine on my windows, when i placed the code on an ubuntu machine and after setting up the cloud credentials and adding the remote storage. i tried command it started giving the following error. im not able to interact with at all. format error extra keys not allowed @ data does anybody knows the solution for the same?",
        "Answer_original_content":"looks like the issue is using python 2.7 which we dont support. we had a similar discussion in discord (perhaps from the op themself): discord discord - a new way to chat with friends & communities discord is the easiest way to communicate over voice, video, and text. chat, hang out, and stay close with your friends and communities. i got that with remote modify storage --local .\/tmp\/lrg-omics-f94be8f80556.json error: configuration error - config file error: extra keys not allowed @ data[remote][storage][.\/tmp\/lrg-omics-f94be8f80556.json] (pqc) ubuntu@dev:~\/workspace\/proteomicsqc\/lib\/lrg-omics>python --version python 3.10.2 @rmbzmb what exactly are you trying to achieve with this command? what is your version?",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"looks like the issue is using which we dont support. we had a similar discussion in discord discord discord a new way to chat with friends & communities discord is the easiest way to communicate over voice, video, and text. chat, hang out, and stay close with your friends and communities. i got that with remote modify storage local error configuration error config file error extra keys not allowed @ pqc version python what exactly are you trying to achieve with this command? what is your ?"
    },
    {
        "Question_id":null,
        "Question_title":"Organizations using MLflow - Emerton Data",
        "Question_body":"Hi !\n\n\nAt Emerton Data, we are big fans of MLflow and are using it in our project to industrialize AI models and data projects.\n\n\nHappy to be one of the mlflow supporter and would be glad to appear on your website as an organization using MLFlow.\u00a0\n\n\nCheers,\n\n\n\nYannick LEO\nDirector Data Science\n\n16 avenue Hoche\u00a0\u00a0\n75008 Paris\u00a0\nM + 33 6 38 21 33 99\nT + 33 1 53 75 38 75\nyanni...@emerton-data.com\u00a0|\u00a0http:\/\/www.emerton-data.com",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1593271817000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":30.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/9WHVX1YBK_A",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-27T17:33:39",
                "Answer_body":"Hello Yannick,\n\n\nWe will be happy to include you as supporters and users of mlflow.\u00a0\n\n\nCan you send me your preferred logo? We will include in the list.\n\n\nThanks for being a fan and supporter of MLflow.\n\n\nCheers\nJules\u00a0\n\n\nSent from my iPhone\nPardon the dumb thumb typos :)\n\n\nOn Jun 27, 2020, at 12:32 PM, Yannick Leo <yanni...@emerton-data.com> wrote:\n\n\n\ufeffHi !\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/a52afb80-ce39-4985-8e13-ef601a2d4edfn%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: organizations using - emerton data; Content: hi ! at emerton data, we are big fans of and are using it in our project to industrialize ai models and data projects. happy to be one of the supporter and would be glad to appear on your website as an organization using . cheers, yannick leo director data science 16 avenue hoche 75008 paris m + 33 6 38 21 33 99 t + 33 1 53 75 38 75 yanni...@emerton-data.com | http:\/\/www.emerton-data.com",
        "Question_original_content_gpt_summary":"The user Yannick Leo is the Director of Data Science at Emerton Data, an organization that is using to industrialize AI models and data projects.",
        "Question_preprocessed_content":"Title: organizations using emerton data; Content: hi ! at emerton data, we are big fans of and are using it in our project to industrialize ai models and data projects. happy to be one of the supporter and would be glad to appear on your website as an organization using . cheers, yannick leo director data science avenue hoche paris m + t +",
        "Answer_original_content":"hello yannick, we will be happy to include you as supporters and users of . can you send me your preferred logo? we will include in the list. thanks for being a fan and supporter of . cheers jules sent from my iphone pardon the dumb thumb typos :) on jun 27, 2020, at 12:32 pm, yannick leo wrote: hi ! -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/a52afb80-ce39-4985-8e13-ef601a2d4edfn%40googlegroups.com.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello yannick, we will be happy to include you as supporters and users of . can you send me your preferred logo? we will include in the list. thanks for being a fan and supporter of . cheers jules sent from my iphone pardon the dumb thumb typos on jun , , at pm, yannick leo wrote hi ! you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to to view this discussion on the web visit"
    },
    {
        "Question_id":54664433.0,
        "Question_title":"Converting Cifar10 dataset to RecordIO format for Amazon SageMaker",
        "Question_body":"<p>I have downloaded cifar-10 dataset and need to convert it to <code>RecordIO<\/code> format. If you unzip the downloaded <code>cifar-10-python.tar.gz<\/code> dataset, you will see below <\/p>\n\n<pre><code>cifar-10-batches-py\/\ncifar-10-batches-py\/data_batch_4\ncifar-10-batches-py\/readme.html\ncifar-10-batches-py\/test_batch\ncifar-10-batches-py\/data_batch_3\ncifar-10-batches-py\/batches.meta\ncifar-10-batches-py\/data_batch_2\ncifar-10-batches-py\/data_batch_5\ncifar-10-batches-py\/data_batch_1\n<\/code><\/pre>\n\n<p>Now, I want to separate out the <code>data_batch_*.bin<\/code> to train set and <code>test_batch.bin<\/code> to test set and convert to <code>recordIO<\/code> format.<\/p>\n\n<p>Following this tutorial <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/tree\/master\/example\/image-classification\" rel=\"nofollow noreferrer\">link<\/a> but not sure how <code>im2rec<\/code>  actually works to make it happen.<\/p>\n\n<p>Please suggest.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1550041703390,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":191.0,
        "Owner_creation_time":1339474453300,
        "Owner_last_access_time":1660360250523,
        "Owner_reputation":2448.0,
        "Owner_up_votes":445.0,
        "Owner_down_votes":5.0,
        "Owner_views":337.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54664433",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: converting cifar10 dataset to recordio format for ; Content: i have downloaded cifar-10 dataset and need to convert it to recordio format. if you unzip the downloaded cifar-10-python.tar.gz dataset, you will see below cifar-10-batches-py\/ cifar-10-batches-py\/data_batch_4 cifar-10-batches-py\/readme.html cifar-10-batches-py\/test_batch cifar-10-batches-py\/data_batch_3 cifar-10-batches-py\/batches.meta cifar-10-batches-py\/data_batch_2 cifar-10-batches-py\/data_batch_5 cifar-10-batches-py\/data_batch_1 now, i want to separate out the data_batch_*.bin to train set and test_batch.bin to test set and convert to recordio format. following this tutorial link but not sure how im2rec actually works to make it happen. please suggest.",
        "Question_original_content_gpt_summary":"The user is facing a challenge of converting the CIFAR-10 dataset to the Recordio format in order to separate out the data_batch_*.bin to the train set and test_batch.bin to the test set.",
        "Question_preprocessed_content":"Title: converting cifar dataset to recordio format for; Content: i have downloaded cifar dataset and need to convert it to format. if you unzip the downloaded dataset, you will see below now, i want to separate out the to train set and to test set and convert to format. following this tutorial link but not sure how actually works to make it happen. please suggest.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":64157391.0,
        "Question_title":"Load Snowflake data into Pandas dataframe using AWS Sagemaker",
        "Question_body":"<p>I'm attempting to read data from Snowflake into a Pandas dataframe using a Jupyter Lab on AWS Sagemaker.  I can successfully load 100 rows, but not 2.2 Million Rows<\/p>\n<ul>\n<li>I gave up loading complete data after ~20 minutes<\/li>\n<li>On my local computer it takes 4 minutes,  If I change to fetch in batches on local computer it takes long (~10 minutes) \u2013 but could try this on Sagemaker?<\/li>\n<li>Command is sent successfully to snowflake (looking at history log)<\/li>\n<li>Placing underlying view into a table on Snowflake showed 222.5MB size<\/li>\n<\/ul>\n<p>Steps:<\/p>\n<ul>\n<li><p>Run: <code>!pip install snowflake-connector-python[pandas]<\/code><\/p>\n<\/li>\n<li><p>Restart kernel<\/p>\n<\/li>\n<li><p>Execute:<\/p>\n<pre><code> ctx = snowflake.connector.connect(\n      user='&lt;username&gt;',\n      account='&lt;account&gt;',\n      password = '&lt;password&gt;',\n      warehouse='&lt;wh&gt;',\n      database='&lt;db&gt;',\n      role='&lt;role&gt;',\n      schema='&lt;schema&gt;'\n  )\n  cur = ctx.cursor()\n  sql = &quot;&quot;&quot;\n          SELECT * &lt;db&gt;.&lt;schema&gt;.&lt;view&gt;\n  &quot;&quot;&quot;\n  cur.execute(sql)\n  # Fetch the result set from the cursor and deliver it as the Pandas DataFrame.\n  df = cur.fetch_pandas_all()\n\n  cur.close()\n  ctx.close()\n<\/code><\/pre>\n<\/li>\n<\/ul>\n<p>Are there any specific configurations I need to ensure that I am using?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1601562219787,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":836.0,
        "Owner_creation_time":1338137210000,
        "Owner_last_access_time":1661852953916,
        "Owner_reputation":1937.0,
        "Owner_up_votes":249.0,
        "Owner_down_votes":0.0,
        "Owner_views":221.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1601563751790,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64157391",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: load snowflake data into pandas dataframe using ; Content: i'm attempting to read data from snowflake into a pandas dataframe using a jupyter lab on . i can successfully load 100 rows, but not 2.2 million rows i gave up loading complete data after ~20 minutes on my local computer it takes 4 minutes, if i change to fetch in batches on local computer it takes long (~10 minutes) \u2013 but could try this on ? command is sent successfully to snowflake (looking at history log) placing underlying view into a table on snowflake showed 222.5mb size steps: run: !pip install snowflake-connector-python[pandas] restart kernel execute: ctx = snowflake.connector.connect( user='<username>', account='<account>', password = '<password>', warehouse='<wh>', database='<db>', role='<role>', schema='<schema>' ) cur = ctx.cursor() sql = \"\"\" select * <db>.<schema>.<view> \"\"\" cur.execute(sql) # fetch the result set from the cursor and deliver it as the pandas dataframe. df = cur.fetch_pandas_all() cur.close() ctx.close() are there any specific configurations i need to ensure that i am using?",
        "Question_original_content_gpt_summary":"The user is encountering challenges when attempting to read data from Snowflake into a pandas dataframe using a Jupyter Lab, as they can only successfully load 100 rows, not the 2.2 million rows they need.",
        "Question_preprocessed_content":"Title: load snowflake data into pandas dataframe using; Content: i'm attempting to read data from snowflake into a pandas dataframe using a jupyter lab on . i can successfully load rows, but not million rows i gave up loading complete data after minutes on my local computer it takes minutes, if i change to fetch in batches on local computer it takes long but could try this on ? command is sent successfully to snowflake placing underlying view into a table on snowflake showed size steps run restart kernel execute are there any specific configurations i need to ensure that i am using?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":60607041.0,
        "Question_title":"AWS Sagemaker Spark S3 access issue",
        "Question_body":"<p>I am new in AWS sagemaker. I created a notebook in a VPC with private subnet, kms default encrypted key, root access, no direct internet access. I have attached policy which have full access to Sagemaker and S3 in IAM as per documentations.  Now while one of data scientist trying to run his code in jupyter, getting below error. I can see jar files (\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/), I have even given access key and secret key in code, is there anything we are doing wrong here<\/p>\n\n<pre><code>import os\nimport boto3\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\n\nimport sagemaker\nfrom sagemaker import get_execution_role\nimport sagemaker_pyspark\nimport pyspark\n\nrole = get_execution_role()\nspark = SparkSession.builder \\\n            .appName(\"app_name2\") \\\n            .getOrCreate()\n\nsc=pyspark.SparkContext.getOrCreate()\nsc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n\nhadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", 'access_key')\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", 'secret_key')\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\nspark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\");\nspark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\ndf= spark.read.csv(\"s3a:\/\/mybucket\/ConsolidatedData\/my.csv\",header=\"true\")\n\n\nPy4JJavaError: An error occurred while calling o579.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n    at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2195)\n    at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n    at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:709)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1583781550823,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":933.0,
        "Owner_creation_time":1513883236660,
        "Owner_last_access_time":1663906475808,
        "Owner_reputation":465.0,
        "Owner_up_votes":11.0,
        "Owner_down_votes":0.0,
        "Owner_views":46.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Noida, Uttar Pradesh, India",
        "Question_last_edit_time":1583817907990,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60607041",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: spark s3 access issue; Content: i am new in . i created a notebook in a vpc with private subnet, kms default encrypted key, root access, no direct internet access. i have attached policy which have full access to and s3 in iam as per documentations. now while one of data scientist trying to run his code in jupyter, getting below error. i can see jar files (\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/_pyspark\/jars\/), i have even given access key and secret key in code, is there anything we are doing wrong here import os import boto3 from pyspark import sparkcontext, sparkconf from pyspark.sql import sparksession import from import get_execution_role import _pyspark import pyspark role = get_execution_role() spark = sparksession.builder \\ .appname(\"app_name2\") \\ .getorcreate() sc=pyspark.sparkcontext.getorcreate() sc.setsystemproperty(\"com.amazonaws.services.s3.enablev4\", \"true\") hadoop_conf = spark.sparkcontext._jsc.hadoopconfiguration() spark._jsc.hadoopconfiguration().set(\"fs.s3a.access.key\", 'access_key') spark._jsc.hadoopconfiguration().set(\"fs.s3a.secret.key\", 'secret_key') spark._jsc.hadoopconfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\") spark._jsc.hadoopconfiguration().set(\"com.amazonaws.services.s3a.enablev4\", \"true\") spark._jsc.hadoopconfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\"); spark._jsc.hadoopconfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.s3afilesystem\") df= spark.read.csv(\"s3a:\/\/mybucket\/consolidateddata\/my.csv\",header=\"true\") py4jjavaerror: an error occurred while calling o579.csv. : java.lang.runtimeexception: java.lang.classnotfoundexception: class org.apache.hadoop.fs.s3a.s3afilesystem not found at org.apache.hadoop.conf.configuration.getclass(configuration.java:2195) at org.apache.hadoop.fs.filesystem.getfilesystemclass(filesystem.java:2654) at org.apache.hadoop.fs.filesystem.createfilesystem(filesystem.java:2667) at org.apache.hadoop.fs.filesystem.access$200(filesystem.java:94) at org.apache.hadoop.fs.filesystem$cache.getinternal(filesystem.java:2703) at org.apache.hadoop.fs.filesystem$cache.get(filesystem.java:2685) at org.apache.hadoop.fs.filesystem.get(filesystem.java:373) at org.apache.hadoop.fs.path.getfilesystem(path.java:295) at org.apache.spark.sql.execution.datasources.datasource$.org$apache$spark$sql$execution$datasources$datasource$$checkandglobpathifnecessary(datasource.scala:709)",
        "Question_original_content_gpt_summary":"The user is encountering challenges with accessing S3 from a notebook in a VPC with private subnet, KMS default encrypted key, root access, and no direct internet access.",
        "Question_preprocessed_content":"Title: spark s access issue; Content: i am new in . i created a notebook in a vpc with private subnet, kms default encrypted key, root access, no direct internet access. i have attached policy which have full access to and s in iam as per documentations. now while one of data scientist trying to run his code in jupyter, getting below error. i can see jar files , i have even given access key and secret key in code, is there anything we are doing wrong here",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70596614.0,
        "Question_title":"Vertex AI Custom Container Training Job python SDK - InvalidArgument 400 error",
        "Question_body":"<p>I'm attempting to run a Vertex AI custom training job using the python SDK, following the general instructions laid out in <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\" rel=\"nofollow noreferrer\">this readme<\/a>. My code is as follows (sensitive data removed):<\/p>\n<pre><code>job = aiplatform.CustomContainerTrainingJob(\n    display_name='python_api_test',\n    container_uri='{URI FOR CUSTOM CONTAINER IN GOOGLE ARTIFACT REGISTRY}',\n    staging_bucket='{GCS BUCKET PATH IN 'gs:\/\/' FORMAT}',\n    model_serving_container_image_uri='us-docker.pkg.dev\/vertex-ai\/prediction\/tf2-cpu.2-4:latest',\n)\n\njob.run(\n    model_display_name='python_api_model',\n    args='{ARG PASSED TO CONTAINER ENTRYPOINT}',\n    replica_count=1,\n    machine_type='n1-standard-4',\n    accelerator_type='NVIDIA_TESLA_T4',\n    accelerator_count=2,\n    environment_variables={\n        {A COUPLE OF SECRETS PASSED TO CONTAINER IN DICTIONARY FORMAT}\n    }\n)\n<\/code><\/pre>\n<p>When I execute <code>job.run()<\/code>, I get the following error:<\/p>\n<pre><code>InvalidArgument: 400 Unable to parse `training_pipeline.training_task_inputs` into custom task `inputs` defined in the file: gs:\/\/google-cloud-aiplatform\/schema\/trainingjob\/definition\/custom_task_1.0.0.yaml\n<\/code><\/pre>\n<p>The full traceback does not show where it is unhappy with any specific inputs. I've successfully run jobs in the same container using the Vertex CLI.I'm confident that there is nothing wrong with my <code>aiplatform.init()<\/code> (I'm running the job from a Vertex workbench machine in the same project).<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_creation_time":1641401635217,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":79.0,
        "Owner_creation_time":1464106929608,
        "Owner_last_access_time":1663087027036,
        "Owner_reputation":457.0,
        "Owner_up_votes":12.0,
        "Owner_down_votes":0.0,
        "Owner_views":37.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70596614",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: custom container training job python sdk - invalidargument 400 error; Content: i'm attempting to run a custom training job using the python sdk, following the general instructions laid out in this readme. my code is as follows (sensitive data removed): job = aiplatform.customcontainertrainingjob( display_name='python_api_test', container_uri='{uri for custom container in google artifact registry}', staging_bucket='{gcs bucket path in 'gs:\/\/' format}', model_serving_container_image_uri='us-docker.pkg.dev\/vertex-ai\/prediction\/tf2-cpu.2-4:latest', ) job.run( model_display_name='python_api_model', args='{arg passed to container entrypoint}', replica_count=1, machine_type='n1-standard-4', accelerator_type='nvidia_tesla_t4', accelerator_count=2, environment_variables={ {a couple of secrets passed to container in dictionary format} } ) when i execute job.run(), i get the following error: invalidargument: 400 unable to parse `training_pipeline.training_task_inputs` into custom task `inputs` defined in the file: gs:\/\/google-cloud-aiplatform\/schema\/trainingjob\/definition\/custom_task_1.0.0.yaml the full traceback does not show where it is unhappy with any specific inputs. i've successfully run jobs in the same container using the vertex cli.i'm confident that there is nothing wrong with my aiplatform.init() (i'm running the job from a vertex workbench machine in the same project).",
        "Question_original_content_gpt_summary":"The user is encountering an \"invalidargument 400\" error when attempting to run a custom container training job using the Python SDK, despite successfully running jobs in the same container using the Vertex CLI.",
        "Question_preprocessed_content":"Title: custom container training job python sdk invalidargument error; Content: i'm attempting to run a custom training job using the python sdk, following the general instructions laid out in this readme. my code is as follows when i execute , i get the following error the full traceback does not show where it is unhappy with any specific inputs. i've successfully run jobs in the same container using the vertex confident that there is nothing wrong with my .",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Can I use compressed data on TabularDataset?",
        "Question_body":"I have a question about the source of TabularDataset on Azure Machine Learnigng.\n\nCan I use compressed data saved Azure Data Lake Storage Gen2 like below on TablarDataset without expansion?\n\ncsv with bzip2(.bz2)\n\n\nparquet with gzip(gz)\n\n\nparquet with snappy",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1638234150553,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/645118\/can-i-use-compressed-data-on-tabulardataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-30T02:15:20.327Z",
                "Answer_score":0,
                "Answer_body":"Hi, tabular dataset does not support compressed files. You'll need to extract the data as shown here for example before creating a tabular dataset. However, file dataset supports any format.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can i use compressed data on tabulardataset?; Content: i have a question about the source of tabulardataset on azure machine learnigng. can i use compressed data saved azure data lake storage gen2 like below on tablardataset without expansion? csv with bzip2(.bz2) parquet with gzip(gz) parquet with snappy",
        "Question_original_content_gpt_summary":"The user is inquiring about whether they can use compressed data on TabularDataset in Azure Machine Learning.",
        "Question_preprocessed_content":"Title: can i use compressed data on tabulardataset?; Content: i have a question about the source of tabulardataset on azure machine learnigng. can i use compressed data saved azure data lake storage gen like below on tablardataset without expansion? csv with parquet with gzip parquet with snappy",
        "Answer_original_content":"hi, tabular dataset does not support compressed files. you'll need to extract the data as shown here for example before creating a tabular dataset. however, file dataset supports any format. --- kindly accept answer if the information helps. thanks.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, tabular dataset does not support compressed files. you'll need to extract the data as shown here for example before creating a tabular dataset. however, file dataset supports any format. kindly accept answer if the helps. thanks."
    },
    {
        "Question_id":56393818.0,
        "Question_title":"How to retrieve auth keys for an ACI deployment in Azure Portal (or Cloud Shell)?",
        "Question_body":"<p>I have created a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">deployment on ACI with Azure ML service<\/a>, and its status is healthy.<br>\nWhen deploying, I set <code>auth_enabled=True<\/code>, so that the service requires authorization keys to respond.<\/p>\n\n<p>I can get the service auth keys for that deployment in my Azure ML service workspace <code>ws<\/code> in a Python console via<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nservices = Webservice.list(ws)\nservices[0].get_keys()\n<\/code><\/pre>\n\n<p>However, it would be convenient to access to this information through Azure Portal or the Cloud Shell. <\/p>\n\n<p>In Azure Portal (differently to what happens for AKS) there's no auth fields shown, also when accessing Advanced Settings by trying to edit the deployment:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/3Qudh.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3Qudh.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Can you suggest ways to access those credentials?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1559300302887,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":169.0,
        "Owner_creation_time":1415722650716,
        "Owner_last_access_time":1664051478172,
        "Owner_reputation":4811.0,
        "Owner_up_votes":376.0,
        "Owner_down_votes":73.0,
        "Owner_views":713.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Verona, VR, Italy",
        "Question_last_edit_time":1559323924667,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56393818",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to retrieve auth keys for an aci deployment in azure portal (or cloud shell)?; Content: i have created a deployment on aci with service, and its status is healthy. when deploying, i set auth_enabled=true, so that the service requires authorization keys to respond. i can get the service auth keys for that deployment in my service workspace ws in a python console via from .core.webservice import webservice services = webservice.list(ws) services[0].get_keys() however, it would be convenient to access to this information through azure portal or the cloud shell. in azure portal (differently to what happens for aks) there's no auth fields shown, also when accessing advanced settings by trying to edit the deployment: can you suggest ways to access those credentials?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in retrieving authorization keys for an ACI deployment in Azure Portal or Cloud Shell.",
        "Question_preprocessed_content":"Title: how to retrieve auth keys for an aci deployment in azure portal ?; Content: i have created a deployment on aci with service, and its status is healthy. when deploying, i set , so that the service requires authorization keys to respond. i can get the service auth keys for that deployment in my service workspace in a python console via however, it would be convenient to access to this through azure portal or the cloud shell. in azure portal there's no auth fields shown, also when accessing advanced settings by trying to edit the deployment can you suggest ways to access those credentials?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Azure ml notebook does not show widgets from ipywidgets",
        "Question_body":"ipywidgets seems to work fine for the simplest usages, i.e. just using a slider. However, when trying to use more complex functionality the notebook does not show \/ display the widgets anymore.\n\nSee in the picture:\n\n\nThe simple usage\n\n widgets.IntSlider()\n\n\n\nworks fine.\nHowever, using ipywidget's interact does not show any widget:\n\n def f(x):\n     return x\n    \n interact(f, x=10)\n\n\n\nWhen I change the editor using the dropdown\n\n\nand use Jupyter or JupyterLab, everything works as expected without flaws.",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1621085408217,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/396317\/azure-ml-notebook-does-not-show-widgets-from-ipywi.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-26T11:37:49.587Z",
                "Answer_score":0,
                "Answer_body":"@imbachb-2223 Thanks for the feedback. This specific type of ipywidget is currently not supported, we have roadmap to support in the future.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: notebook does not show widgets from ipywidgets; Content: ipywidgets seems to work fine for the simplest usages, i.e. just using a slider. however, when trying to use more complex functionality the notebook does not show \/ display the widgets anymore. see in the picture: the simple usage widgets.intslider() works fine. however, using ipywidget's interact does not show any widget: def f(x): return x interact(f, x=10) when i change the editor using the dropdown and use jupyter or jupyterlab, everything works as expected without flaws.",
        "Question_original_content_gpt_summary":"The user encountered challenges with ipywidgets where the notebook does not show\/display the widgets when using more complex functionality.",
        "Question_preprocessed_content":"Title: notebook does not show widgets from ipywidgets; Content: ipywidgets seems to work fine for the simplest usages, just using a slider. however, when trying to use more complex functionality the notebook does not show \/ display the widgets anymore. see in the picture the simple usage works fine. however, using ipywidget's interact does not show any widget def f return x interact when i change the editor using the dropdown and use jupyter or jupyterlab, everything works as expected without flaws.",
        "Answer_original_content":"@imbachb-2223 thanks for the feedback. this specific type of ipywidget is currently not supported, we have roadmap to support in the future.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the feedback. this specific type of ipywidget is currently not supported, we have roadmap to support in the future."
    },
    {
        "Question_id":null,
        "Question_title":"Retrieving sweep id when starting sweep from CL",
        "Question_body":"<p>I start a sweep from the command line (<code>wandb sweep config.yaml<\/code>). The sweep ID is now displayed (<code>wandb: Created sweep with ID: 6bb3459a<\/code>), but I would like to get it programatically, such that I can later start agents automatically without copy-pasting the ID.<\/p>\n<p>Is there a way to achieve this?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1660399139209,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":161.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/retrieving-sweep-id-when-starting-sweep-from-cl\/2921",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-17T20:52:11.114Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/bask0\">@bask0<\/a> ,  we currently do not have the equivalent of <code>api.sweeps()<\/code> built into our API. This is being integrated for future releases. At this time, specific sweeps functionality via the API can be found <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#get-runs-from-a-specific-sweep\">here<\/a>. I attached your inquiry to our feature request system. Once it\u2019s been implemented we will let you know.<\/p>",
                "Answer_score":20.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-18T09:00:14.054Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> ! For those looking for a temporary solution: for now, I direct the output to a file and grab the ID from there.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-17T09:00:32.721Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: retrieving sweep id when starting sweep from cl; Content: i start a sweep from the command line ( sweep config.yaml). the sweep id is now displayed (: created sweep with id: 6bb3459a), but i would like to get it programatically, such that i can later start agents automatically without copy-pasting the id. is there a way to achieve this? thanks",
        "Question_original_content_gpt_summary":"The user is looking for a way to programmatically retrieve the sweep ID when starting a sweep from the command line.",
        "Question_preprocessed_content":"Title: retrieving sweep id when starting sweep from cl; Content: i start a sweep from the command line . the sweep id is now displayed , but i would like to get it programatically, such that i can later start agents automatically without copy pasting the id. is there a way to achieve this? thanks",
        "Answer_original_content":"hi @bask0 , we currently do not have the equivalent of api.sweeps() built into our api. this is being integrated for future releases. at this time, specific sweeps functionality via the api can be found here. i attached your inquiry to our feature request system. once its been implemented we will let you know. thanks @mohammadbakir ! for those looking for a temporary solution: for now, i direct the output to a file and grab the id from there. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi , we currently do not have the equivalent of built into our api. this is being integrated for future releases. at this time, specific sweeps functionality via the api can be found here. i attached your inquiry to our feature request system. once its been implemented we will let you know. thanks ! for those looking for a temporary solution for now, i direct the output to a file and grab the id from there. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"MLOps using Azure Databricks & Azure ML - question on data prep for model inference and retraining.",
        "Question_body":"I am using this blog (https:\/\/databricks.com\/blog\/2020\/10\/13\/using-mlops-with-mlflow-and-azure.html) to set-up MLOps using Azure Databricks & Azure ML. As mentioned in the blog, we deploy MLflow model into an Azure ML environment using the built in MLflow deployment capabilities, which is used for inference. A couple of questions -\n1. How and where does the data prep come into picture before inference and how I can integrate that piece.\n2. How to create a re-training workflow for the model?\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1607958581913,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/197618\/mlops-using-azure-databricks-amp-azure-ml-question.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-15T08:22:17.753Z",
                "Answer_score":0,
                "Answer_body":"@KiranPurushotham-8848 Thanks, using Databricks to build models and track using MLFlow. Then wants to deploy the model using MLFlow->AML service integration and wants to monitor the model. To work around the limitation of MLflow deployment, you can switch to AML deployment but use the model created and registered by MLFlow at AML.\nFirst, add mflow to conda dependencies to be able to use it in your scoring script, then in init method, load the model using mlflow API, for example:\nmodel = mlflow.pytorch.load_model(model_dir)\nYou need to check artifact structure of the mode registered in AML to construct model_dir correctly because it was created using MLFlow API.\n\nYou may implement ML Ops with a hybrid setup:\nCloud Part:\n\u2022 Azure DevOps can orchestrate Azure ML Service for MLOps practices.\n\u2022 Azure ML Service can be used to training and orchestrating model development, an MLOps manual in link.\n\nOn Prems:\n\u2022 We can train models using data & CPU power on local, on prems.\n\u2022 We can run Azure DevOps pipelines on prems with the Azure DevOps Server running on an On Prems Hardware.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: mlops using azure databricks & - question on data prep for model inference and retraining.; Content: i am using this blog (https:\/\/databricks.com\/blog\/2020\/10\/13\/using-mlops-with-mlflow-and-azure.html) to set-up mlops using azure databricks & . as mentioned in the blog, we deploy mlflow model into an environment using the built in mlflow deployment capabilities, which is used for inference. a couple of questions - 1. how and where does the data prep come into picture before inference and how i can integrate that piece. 2. how to create a re-training workflow for the model? thanks.",
        "Question_original_content_gpt_summary":"The user is looking for guidance on how to integrate data preparation into their MLOps workflow using Azure Databricks and MLFlow, as well as how to create a retraining workflow for their model.",
        "Question_preprocessed_content":"Title: mlops using azure databricks & question on data prep for model inference and retraining.; Content: i am using this blog to set up mlops using azure databricks & . as mentioned in the blog, we deploy mlflow model into an environment using the built in mlflow deployment capabilities, which is used for inference. a couple of questions . how and where does the data prep come into picture before inference and how i can integrate that piece. . how to create a re training workflow for the model? thanks.",
        "Answer_original_content":"@kiranpurushotham-8848 thanks, using databricks to build models and track using mlflow. then wants to deploy the model using mlflow->aml service integration and wants to monitor the model. to work around the limitation of mlflow deployment, you can switch to aml deployment but use the model created and registered by mlflow at aml. first, add mflow to conda dependencies to be able to use it in your scoring script, then in init method, load the model using mlflow api, for example: model = mlflow.pytorch.load_model(model_dir) you need to check artifact structure of the mode registered in aml to construct model_dir correctly because it was created using mlflow api. you may implement ml ops with a hybrid setup: cloud part: azure devops can orchestrate service for mlops practices. service can be used to training and orchestrating model development, an mlops manual in link. on prems: we can train models using data & cpu power on local, on prems. we can run azure devops pipelines on prems with the azure devops server running on an on prems hardware.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks, using databricks to build models and track using mlflow. then wants to deploy the model using mlflow >aml service integration and wants to monitor the model. to work around the limitation of mlflow deployment, you can switch to aml deployment but use the model created and registered by mlflow at aml. first, add mflow to conda dependencies to be able to use it in your scoring script, then in init method, load the model using mlflow api, for example model you need to check artifact structure of the mode registered in aml to construct correctly because it was created using mlflow api. you may implement ml ops with a hybrid setup cloud part azure devops can orchestrate service for mlops practices. service can be used to training and orchestrating model development, an mlops manual in link. on prems we can train models using data & cpu power on local, on prems. we can run azure devops pipelines on prems with the azure devops server running on an on prems hardware."
    },
    {
        "Question_id":67843602.0,
        "Question_title":"Could not find model PipelineModel",
        "Question_body":"<p>When I try to build models to create a pipeline as follows,<\/p>\n<pre><code>    &lt;code for the preprocessor&gt;\n    preprocessor = sklearn_preprocessor.create_model() #successful\n\n    &lt;code for the estimator&gt;\n    xgb_model_step = xgb_model.create_model() #successful    \n    \n    sm_model = PipelineModel(name='model', role=role, models=[preprocessor, xgb_model_step]) #successful\n    sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name=end) &lt;--- failure!\n<\/code><\/pre>\n<p>The models are created successfully. In the deploy line I get an error as,<\/p>\n<p><code>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Could not find model data at s3:\/\/sagemaker-us-east-1-1356784978535\/sagemaker-scikit-learn-2021-06-04-20-07-55-519\/output\/model.tar.gz.<\/code><\/p>\n<p>I am not sure how I can specify the path and make the deploy successful. Can somebody please help me with this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1622838635930,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":77.0,
        "Owner_creation_time":1473401410976,
        "Owner_last_access_time":1646051838716,
        "Owner_reputation":129.0,
        "Owner_up_votes":8.0,
        "Owner_down_votes":0.0,
        "Owner_views":41.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67843602",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: could not find model pipelinemodel; Content: when i try to build models to create a pipeline as follows, <code for the preprocessor> preprocessor = sklearn_preprocessor.create_model() #successful <code for the estimator> xgb_model_step = xgb_model.create_model() #successful sm_model = pipelinemodel(name='model', role=role, models=[preprocessor, xgb_model_step]) #successful sm_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge', endpoint_name=end) <--- failure! the models are created successfully. in the deploy line i get an error as, clienterror: an error occurred (validationexception) when calling the createmodel operation: could not find model data at s3:\/\/-us-east-1-1356784978535\/-scikit-learn-2021-06-04-20-07-55-519\/output\/model.tar.gz. i am not sure how i can specify the path and make the deploy successful. can somebody please help me with this?",
        "Question_original_content_gpt_summary":"The user encountered a challenge when attempting to deploy a model pipeline, receiving an error that the model could not be found at a specified path.",
        "Question_preprocessed_content":"Title: could not find model pipelinemodel; Content: when i try to build models to create a pipeline as follows, the models are created successfully. in the deploy line i get an error as, i am not sure how i can specify the path and make the deploy successful. can somebody please help me with this?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":63698011.0,
        "Question_title":"AWS Notebook Instance is working but Lambda is not accepting the input",
        "Question_body":"<p>I developed an ANN tool by using pycharm\/tensorflow on my own computer. I uploaded the h5 and json files to Amazon Sagemaker by creating a Notebook Instance. I was finally able to successfully create an endpoint and make it work. The following code in Notebook Instance -Jupyter works:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\nimport sagemaker\nfrom sagemaker.tensorflow.model import TensorFlowModel\nclient = boto3.client('runtime.sagemaker')\ndata = np.random.randn(1,6).tolist()\nendpoint_name = 'sagemaker-tensorflow-**********'\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>However, the problem occurs when I created a lambda function and call the endpoint from there. The input should be a row of 6 features -that is a 1-by-6 vector. I enter the following input into lambda {&quot;data&quot;:&quot;1,1,1,1,1,1&quot;} and it gives me the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 20, in lambda_handler\n    Body=payload)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\n<\/code><\/pre>\n<p>I think the problem is that the input needs to be 1-by-6 instead of 6-by-1 and I don't know how to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1599015165303,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":51.0,
        "Owner_creation_time":1369539919747,
        "Owner_last_access_time":1663810191950,
        "Owner_reputation":311.0,
        "Owner_up_votes":6.0,
        "Owner_down_votes":0.0,
        "Owner_views":35.0,
        "Answer_body":"<p>I assume the content type you specified is <code>text\/csv<\/code>, so try out:<\/p>\n<pre><code>{&quot;data&quot;: [&quot;1,1,1,1,1,1&quot;]}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1599633575463,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63698011",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: aws notebook instance is working but lambda is not accepting the input; Content: i developed an ann tool by using pycharm\/tensorflow on my own computer. i uploaded the h5 and json files to by creating a notebook instance. i was finally able to successfully create an endpoint and make it work. the following code in notebook instance -jupyter works: import json import boto3 import numpy as np import io import from .tensorflow.model import tensorflowmodel client = boto3.client('runtime.') data = np.random.randn(1,6).tolist() endpoint_name = '-tensorflow-**********' response = client.invoke_endpoint(endpointname=endpoint_name, body=json.dumps(data)) response_body = response['body'] print(response_body.read()) however, the problem occurs when i created a lambda function and call the endpoint from there. the input should be a row of 6 features -that is a 1-by-6 vector. i enter the following input into lambda {\"data\":\"1,1,1,1,1,1\"} and it gives me the following error: traceback (most recent call last): file \"\/var\/task\/lambda_function.py\", line 20, in lambda_handler body=payload) file \"\/var\/runtime\/botocore\/client.py\", line 316, in _api_call return self._make_api_call(operation_name, kwargs) file \"\/var\/runtime\/botocore\/client.py\", line 635, in _make_api_call raise error_class(parsed_response, operation_name) i think the problem is that the input needs to be 1-by-6 instead of 6-by-1 and i don't know how to do that.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with their AWS notebook instance, where the Lambda function is not accepting the input of a 1-by-6 vector.",
        "Question_preprocessed_content":"Title: aws notebook instance is working but lambda is not accepting the input; Content: i developed an ann tool by using on my own computer. i uploaded the h and json files to by creating a notebook instance. i was finally able to successfully create an endpoint and make it work. the following code in notebook instance jupyter works however, the problem occurs when i created a lambda function and call the endpoint from there. the input should be a row of features that is a by vector. i enter the following input into lambda and it gives me the following error i think the problem is that the input needs to be by instead of by and i don't know how to do that.",
        "Answer_original_content":"i assume the content type you specified is text\/csv, so try out: {\"data\": [\"1,1,1,1,1,1\"]}",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i assume the content type you specified is , so try out"
    },
    {
        "Question_id":66692579.0,
        "Question_title":"AWS SageMaker: PermissionError: Access Denied - Reading data from S3 bucket",
        "Question_body":"<p>I am using AWS SageMaker. I already used it before and I had no problems reading data from an S3 bucket.\nSo, I set up a new notebook instance and id this:<\/p>\n<pre><code>from sagemaker import get_execution_role\nrole = get_execution_role()\n\nbucket='my-bucket'\n\ndata_key = 'myfile.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\n\ndf = pd.read_csv(data_location)\n<\/code><\/pre>\n<p>What I got is this:<\/p>\n<pre><code>PermissionError: Access Denied\n<\/code><\/pre>\n<p>Note: I checked the IAM Roles and also the policies and it seems to me that I have all the necessary rights to access the S3 bucket (AmazonS3FullAccess etc. are granted). What is different from the situation before is that my data is encrypted. Is there something I have to set up besides the roles?<\/p>\n<p>Edit:<\/p>\n<p>The role I use consist of three policies. These are<\/p>\n<ul>\n<li>AmazonS3FullAccess<\/li>\n<li>AmazonSageMakerFullAccess<\/li>\n<\/ul>\n<p>and an Execution Role where I added kms:encrypt and kms:decrypt. It looks like this one:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;xyz&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:PutObject&quot;,\n                &quot;s3:GetObject&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:DeleteObject&quot;,\n                &quot;kms:Encrypt&quot;,\n                &quot;kms:Decrypt&quot;\n            ],\n            &quot;Resource&quot;: &quot;arn:aws:s3:::*&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>Is there something missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7.0,
        "Question_creation_time":1616075760093,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":1327.0,
        "Owner_creation_time":1559131080072,
        "Owner_last_access_time":1663924217556,
        "Owner_reputation":1166.0,
        "Owner_up_votes":555.0,
        "Owner_down_votes":1.0,
        "Owner_views":248.0,
        "Answer_body":"<p>You need to add (or modify) an IAM policy to grant access to the key the bucket uses for its encryption:<\/p>\n<pre><code>{\n  &quot;Sid&quot;: &quot;KMSAccess&quot;,\n  &quot;Action&quot;: [\n    &quot;kms:Decrypt&quot;\n  ],\n  &quot;Effect&quot;: &quot;Allow&quot;,\n  &quot;Resource&quot;: &quot;arn:aws:kms:example-region-1:123456789098:key\/111aa2bb-333c-4d44-5555-a111bb2c33dd&quot;\n}\n<\/code><\/pre>\n<p>Alternatively you can change the key policy of the KMS key directly to grant the sagemaker role access directly. <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1616081046907,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1616077230863,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66692579",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: : permissionerror: access denied - reading data from s3 bucket; Content: i am using . i already used it before and i had no problems reading data from an s3 bucket. so, i set up a new notebook instance and id this: from import get_execution_role role = get_execution_role() bucket='my-bucket' data_key = 'myfile.csv' data_location = 's3:\/\/{}\/{}'.format(bucket, data_key) df = pd.read_csv(data_location) what i got is this: permissionerror: access denied note: i checked the iam roles and also the policies and it seems to me that i have all the necessary rights to access the s3 bucket (amazons3fullaccess etc. are granted). what is different from the situation before is that my data is encrypted. is there something i have to set up besides the roles? edit: the role i use consist of three policies. these are amazons3fullaccess amazonfullaccess and an execution role where i added kms:encrypt and kms:decrypt. it looks like this one: { \"version\": \"2012-10-17\", \"statement\": [ { \"sid\": \"xyz\", \"effect\": \"allow\", \"action\": [ \"s3:putobject\", \"s3:getobject\", \"s3:listbucket\", \"s3:deleteobject\", \"kms:encrypt\", \"kms:decrypt\" ], \"resource\": \"arn:aws:s3:::*\" } ] } is there something missing?",
        "Question_original_content_gpt_summary":"The user is encountering a PermissionError when attempting to read data from an S3 bucket, despite having the necessary IAM roles and policies in place.",
        "Question_preprocessed_content":"Title: permissionerror access denied reading data from s bucket; Content: i am using . i already used it before and i had no problems reading data from an s bucket. so, i set up a new notebook instance and id this what i got is this note i checked the iam roles and also the policies and it seems to me that i have all the necessary rights to access the s bucket . what is different from the situation before is that my data is encrypted. is there something i have to set up besides the roles? edit the role i use consist of three policies. these are amazons fullaccess amazonfullaccess and an execution role where i added kms encrypt and kms decrypt. it looks like this one is there something missing?",
        "Answer_original_content":"you need to add (or modify) an iam policy to grant access to the key the bucket uses for its encryption: { \"sid\": \"kmsaccess\", \"action\": [ \"kms:decrypt\" ], \"effect\": \"allow\", \"resource\": \"arn:aws:kms:example-region-1:123456789098:key\/111aa2bb-333c-4d44-5555-a111bb2c33dd\" } alternatively you can change the key policy of the kms key directly to grant the role access directly. https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/s3-bucket-access-default-encryption\/",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"you need to add an iam policy to grant access to the key the bucket uses for its encryption alternatively you can change the key policy of the kms key directly to grant the role access directly."
    },
    {
        "Question_id":null,
        "Question_title":"Microsoft Azure ML : How can I add multiple score labels",
        "Question_body":"Hello all,\n\nI created an experiment in Azure ML.:\n10 columns used as features;\n03 columns required to be predicted via Azure ML experiment =&amp;amp;gt; 3 components prices to predict!\n\nWhat I did : My experiment has been divided into 3 modules that deliver each a &amp;amp;#34;score label&amp;amp;#34;\n\nAt the end of the process, how can I merge\/add these 3 score labels into one only column:\ntotal price = price component 1 + price component 2 + price component 3.\nAnd multiply this total value by another column (from the dataset) so that I get only 1 score label.\n\n\n\n\nRegards,\n\nMohamed.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1590575222127,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/29846\/microsoft-azure-ml-how-can-i-add-multiple-score-la.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-29T04:54:33.72Z",
                "Answer_score":0,
                "Answer_body":"Hi, for this data transformation task, an approach could be to use 'Select Columns in Dataset' and 'Join Data' modules to combine the data and then use 'Execute Python\/R Script' to perform other data transformation tasks. You may also directly feed the data from 'Score Model module' to 'Execute Python\/R Script' module and perform data transformations.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: : how can i add multiple score labels; Content: hello all, i created an experiment in .: 10 columns used as features; 03 columns required to be predicted via experiment =&amp;gt; 3 components prices to predict! what i did : my experiment has been divided into 3 modules that deliver each a &amp;#34;score label&amp;#34; at the end of the process, how can i merge\/add these 3 score labels into one only column: total price = price component 1 + price component 2 + price component 3. and multiply this total value by another column (from the dataset) so that i get only 1 score label. regards, mohamed.",
        "Question_original_content_gpt_summary":"The user is attempting to combine three score labels into one column and multiply the total value by another column to get a single score label.",
        "Question_preprocessed_content":"Title: how can i add multiple score labels; Content: hello all, i created an experiment in azure columns used as features; columns required to be predicted via experiment &amp;gt; components prices to predict! what i did my experiment has been divided into modules that deliver each a &amp; ;score label&amp; ; at the end of the process, how can i these score labels into one only column total price price component + price component + price component . and multiply this total value by another column so that i get only score label. regards, mohamed.",
        "Answer_original_content":"hi, for this data transformation task, an approach could be to use 'select columns in dataset' and 'join data' modules to combine the data and then use 'execute python\/r script' to perform other data transformation tasks. you may also directly feed the data from 'score model module' to 'execute python\/r script' module and perform data transformations.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, for this data transformation task, an approach could be to use 'select columns in dataset' and 'join data' modules to combine the data and then use 'execute script' to perform other data transformation tasks. you may also directly feed the data from 'score model module' to 'execute script' module and perform data transformations."
    },
    {
        "Question_id":62789549.0,
        "Question_title":"Endpoint in Azure Machine Learning not consumable",
        "Question_body":"<p>I have created a pipeline in Azure Machine Learning Designer. Below is the training pipeline. As you can see I have created a custom python model as I have a requirement to use an external package known as K-Modes (<code>pip install modes<\/code>). I have created a real-time inference pipeline off the back of the training pipeline and it ran successfully. When I deploy the inference pipeline, it is in a healthy state however I can not consume or test it.<\/p>\n<p>In python, I get this error when trying to consume it.<\/p>\n<pre><code>{'error': {'code': 500, 'message': 'Internal Server Error. Run: Server internal error is from Module Score Model', 'details': ''}}\n<\/code><\/pre>\n<p>The <code>Module Score Model<\/code> has completed successfully and is green in the inference pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bnbVQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bnbVQ.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1594192948917,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":194.0,
        "Owner_creation_time":1369068264256,
        "Owner_last_access_time":1663772456043,
        "Owner_reputation":2907.0,
        "Owner_up_votes":68.0,
        "Owner_down_votes":8.0,
        "Owner_views":402.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"South Africa",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62789549",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: endpoint in not consumable; Content: i have created a pipeline in designer. below is the training pipeline. as you can see i have created a custom python model as i have a requirement to use an external package known as k-modes (pip install modes). i have created a real-time inference pipeline off the back of the training pipeline and it ran successfully. when i deploy the inference pipeline, it is in a healthy state however i can not consume or test it. in python, i get this error when trying to consume it. {'error': {'code': 500, 'message': 'internal server error. run: server internal error is from module score model', 'details': ''}} the module score model has completed successfully and is green in the inference pipeline.",
        "Question_original_content_gpt_summary":"The user is encountering an issue where they are unable to consume or test their deployed inference pipeline due to an internal server error from the 'module score model'.",
        "Question_preprocessed_content":"Title: endpoint in not consumable; Content: i have created a pipeline in designer. below is the training pipeline. as you can see i have created a custom python model as i have a requirement to use an external package known as k modes . i have created a real time inference pipeline off the back of the training pipeline and it ran successfully. when i deploy the inference pipeline, it is in a healthy state however i can not consume or test it. in python, i get this error when trying to consume it. the has completed successfully and is green in the inference pipeline.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Built-in Algorithms",
        "Question_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1652686627960,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-16T08:11:20.111Z",
                "Answer_score":0,
                "Answer_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\nBlazingText: BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs, Gupta et Khare\nDeepAR DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks, Salinas et al.\nFactorization Machines\nIP Insights\nKMeans\nKNN\nLDA\nLinear Learner\nNTM\nObject2Vec\nObject Detection (it's an SSD model)\nPCA\nRandom Cut Forest: Robust Random Cut Forest Based Anomaly Detection On Streams, Guha et al\nSemantic Segmentation\nSeq2seq\nXGBoost",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: built-in algorithms; Content: i am exploring the built-in algorithms, and i am curious to learn more about the details of the algorithms. however, i am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. if such information exists somewhere, i would highly appreciate a pointer. thanks a lot in advance!",
        "Question_original_content_gpt_summary":"The user is struggling to find research background and implementation details for built-in algorithms.",
        "Question_preprocessed_content":"Title: built in algorithms; Content: i am exploring the built in algorithms, and i am curious to learn more about the details of the algorithms. however, i am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. if such exists somewhere, i would highly appreciate a pointer. thanks a lot in advance!",
        "Answer_original_content":"thanks for your interest in the built-in algorithms! you can find research papers in the documentation of many of them. and documentation page has a section \"how it works\" explaining the science of every algorithm. for example: blazingtext: blazingtext: scaling and accelerating word2vec using multiple gpus, gupta et khare deepar deepar: probabilistic forecasting with autoregressive recurrent networks, salinas et al. factorization machines ip insights kmeans knn lda linear learner ntm object2vec object detection (it's an ssd model) pca random cut forest: robust random cut forest based anomaly detection on streams, guha et al semantic segmentation seq2seq xgboost",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for your interest in the built in algorithms! you can find research papers in the documentation of many of them. and documentation page has a section how it works explaining the science of every algorithm. for example blazingtext blazingtext scaling and accelerating word vec using multiple gpus, gupta et khare deepar deepar probabilistic forecasting with autoregressive recurrent networks, salinas et al. factorization machines ip insights kmeans knn lda linear learner ntm object vec object detection pca random cut forest robust random cut forest based anomaly detection on streams, guha et al semantic segmentation seq seq xgboost"
    },
    {
        "Question_id":54433767.0,
        "Question_title":"how to add new R packages in azure machine learning for time series anomaly detection",
        "Question_body":"<p>I am trying to find out time series anomaly detection in which i need to install new R packages. In this i m following <a href=\"https:\/\/github.com\/business-science\/anomalize\" rel=\"nofollow noreferrer\">https:\/\/github.com\/business-science\/anomalize<\/a> site. In this i needed to install 2 packages: <code>tidyverse<\/code> and <code>anomalize<\/code>.<\/p>\n\n<ol>\n<li><p>can anyone help me on installing package mentioned above as I am getting <\/p>\n\n<blockquote>\n  <p>error \"package or namespace load failed for tidyverse\"<\/p>\n<\/blockquote><\/li>\n<li><p>Also while adding zip of <code>tidyverse<\/code> and <code>anomalize<\/code> do I need to add any other packages and dependencies in that as I am adding only those 2 packages thinking there r no other dependencies I needed for those 2?<\/p><\/li>\n<\/ol>\n\n<p>you can see in code that I created <code>R_Package.zip<\/code> and put <code>tidyverse.zip<\/code> and <code>anomalize.zip<\/code> in that that <\/p>\n\n<pre><code>dataset1 &lt;- maml.mapInputPort(1)\ndata.set &lt;- data.frame(installed.packages())\n#install.packages(\u201csrc\/R_Package\/tidyverse_1.2.1.zip\u201d, lib = \u201c.\u201d, \n                  repos = NULL, verbose = TRUE);\n#library(tidyverse, lib.loc=\u201d.\u201d, verbose=TRUE);\n\ninstall.packages(\"src\/tidyverse.zip\",lib=\".\",repos=NULL,verbose=TRUE)\nlibrary(R_package, lib.loc = \".\", verbose=TRUE);\n\ninstall.packages(\"src\/anomalize.zip\",lib=\".\",repos=NULL,verbose=TRUE)\nlibrary(R_package, lib.loc = \".\", verbose=TRUE);\n\n#success &lt;- library(\"tidyverse\", lib.loc = \".\", \n                    logical.return = TRUE, verbose = TRUE)\n#library(tidyverse)\n\n\nmaml.mapOutputPort(\"dataset1\");\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1548825762143,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":168.0,
        "Owner_creation_time":1492081349832,
        "Owner_last_access_time":1556108056507,
        "Owner_reputation":19.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":42.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1548834576212,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54433767",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to add new r packages in for time series anomaly detection; Content: i am trying to find out time series anomaly detection in which i need to install new r packages. in this i m following https:\/\/github.com\/business-science\/anomalize site. in this i needed to install 2 packages: tidyverse and anomalize. can anyone help me on installing package mentioned above as i am getting error \"package or namespace load failed for tidyverse\" also while adding zip of tidyverse and anomalize do i need to add any other packages and dependencies in that as i am adding only those 2 packages thinking there r no other dependencies i needed for those 2? you can see in code that i created r_package.zip and put tidyverse.zip and anomalize.zip in that that dataset1 <- maml.mapinputport(1) data.set <- data.frame(installed.packages()) #install.packages(\u201csrc\/r_package\/tidyverse_1.2.1.zip\u201d, lib = \u201c.\u201d, repos = null, verbose = true); #library(tidyverse, lib.loc=\u201d.\u201d, verbose=true); install.packages(\"src\/tidyverse.zip\",lib=\".\",repos=null,verbose=true) library(r_package, lib.loc = \".\", verbose=true); install.packages(\"src\/anomalize.zip\",lib=\".\",repos=null,verbose=true) library(r_package, lib.loc = \".\", verbose=true); #success <- library(\"tidyverse\", lib.loc = \".\", logical.return = true, verbose = true) #library(tidyverse) maml.mapoutputport(\"dataset1\");",
        "Question_original_content_gpt_summary":"The user is encountering challenges with installing the necessary R packages for time series anomaly detection, including receiving an error message and determining if additional packages and dependencies are needed.",
        "Question_preprocessed_content":"Title: how to add new r packages in for time series anomaly detection; Content: i am trying to find out time series anomaly detection in which i need to install new r packages. in this i m following site. in this i needed to install packages and . can anyone help me on installing package mentioned above as i am getting error package or namespace load failed for tidyverse also while adding zip of and do i need to add any other packages and dependencies in that as i am adding only those packages thinking there r no other dependencies i needed for those ? you can see in code that i created and put and in that that",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":65253655.0,
        "Question_title":"Sagemaker, get spark dataframe from data image url on S3",
        "Question_body":"<p>I am trying to obtain a sparkdataframe which contains the paths and image for all images in my data. The data is store as follow :\nfolder\/image_category\/image_n.jpg<\/p>\n<p>I worked on a local jupyter notebook and got no problem with using following code:<\/p>\n<p><code>dataframe = spark.read.format(&quot;image&quot;).load(path)<\/code><\/p>\n<p>I need to do the same exercise using AWS sagemaker and S3. I created a bucket following the same pattern :\ns3:\/\/my_bucket\/folder\/image_category\/image_n.jpg<\/p>\n<p>I've tried a lot of possible solutions i found online, based on boto3, s3fs and other stuff, but unfortunately i am still unable to make it work (and i am starting to lose faith ...).<\/p>\n<p>Would anyone have something reliable i could base my work on ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1607698709410,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":152.0,
        "Owner_creation_time":1607697647720,
        "Owner_last_access_time":1626108591612,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Yutz, France",
        "Question_last_edit_time":1607714459630,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65253655",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: , get spark dataframe from data image url on s3; Content: i am trying to obtain a sparkdataframe which contains the paths and image for all images in my data. the data is store as follow : folder\/image_category\/image_n.jpg i worked on a local jupyter notebook and got no problem with using following code: dataframe = spark.read.format(\"image\").load(path) i need to do the same exercise using and s3. i created a bucket following the same pattern : s3:\/\/my_bucket\/folder\/image_category\/image_n.jpg i've tried a lot of possible solutions i found online, based on boto3, s3fs and other stuff, but unfortunately i am still unable to make it work (and i am starting to lose faith ...). would anyone have something reliable i could base my work on ?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in obtaining a Spark DataFrame which contains the paths and images for all images stored in an S3 bucket.",
        "Question_preprocessed_content":"Title: , get spark dataframe from data image url on s; Content: i am trying to obtain a sparkdataframe which contains the paths and image for all images in my data. the data is store as follow i worked on a local jupyter notebook and got no problem with using following code i need to do the same exercise using and s . i created a bucket following the same pattern i've tried a lot of possible solutions i found online, based on boto , s fs and other stuff, but unfortunately i am still unable to make it work . would anyone have something reliable i could base my work on ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Trying Sagemaker example but getting error: AttributeError: module 'sagemaker' has no attribute 'create_transform_job'",
        "Question_body":"Hi, I keep getting this error: AttributeError: module 'sagemaker' has no attribute 'create_transform_job', when using a batch transform example that AWS graciously had in the notebook instances. Code: ***Also, I updated Sagemaker to the newest package and its still not working.\n\n%%time\nimport time\nfrom time import gmtime, strftime\n\nbatch_job_name = \"Batch-Transform-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\ninput_location = \"s3:\/\/{}\/{}\/batch\/{}\".format(\n    bucket, prefix, batch_file\n)  # use input data without ID column\noutput_location = \"s3:\/\/{}\/{}\/output\/{}\".format(bucket, prefix, batch_job_name)\n\nrequest = {\n    \"TransformJobName\": batch_job_name,\n    \"ModelName\": 'xgboost-parquet-example-training-2022-03-28-16-02-31-model',\n    \"TransformOutput\": {\n        \"S3OutputPath\": output_location,\n        \"Accept\": \"text\/csv\",\n        \"AssembleWith\": \"Line\",\n    },\n    \"TransformInput\": {\n        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": input_location}},\n        \"ContentType\": \"text\/csv\",\n        \"SplitType\": \"Line\",\n        \"CompressionType\": \"None\",\n    },\n    \"TransformResources\": {\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n}\n\nsagemaker.create_transform_job(**request)\nprint(\"Created Transform job with name: \", batch_job_name)\n\n# Wait until the job finishes\ntry:\n    sagemaker.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name)\nfinally:\n    response = sagemaker.describe_transform_job(TransformJobName=batch_job_name)\n    status = response[\"TransformJobStatus\"]\n    print(\"Transform job ended with status: \" + status)\n    if status == \"Failed\":\n        message = response[\"FailureReason\"]\n        print(\"Transform failed with the following error: {}\".format(message))\n        raise Exception(\"Transform job failed\")\n\n\nEverything else is working well. I've had no luck with this on anyother forum.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1648494191878,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":260.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUj8LepwTyQkq0ABgtX-nfew\/trying-sagemaker-example-but-getting-error-attribute-error-module-sagemaker-has-no-attribute-create-transform-job",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-28T21:45:21.581Z",
                "Answer_score":1,
                "Answer_body":"Please double check what type is sagemaker object. Check out this example\n\nsagemaker = boto3.client(service_name=\"sagemaker\") sagemaker.create_transform_job(...)",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: trying example but getting error: attributeerror: module '' has no attribute 'create_transform_job'; Content: hi, i keep getting this error: attributeerror: module '' has no attribute 'create_transform_job', when using a batch transform example that aws graciously had in the notebook instances. code: ***also, i updated to the newest package and its still not working. %%time import time from time import gmtime, strftime batch_job_name = \"batch-transform-\" + strftime(\"%y-%m-%d-%h-%m-%s\", gmtime()) input_location = \"s3:\/\/{}\/{}\/batch\/{}\".format( bucket, prefix, batch_file ) # use input data without id column output_location = \"s3:\/\/{}\/{}\/output\/{}\".format(bucket, prefix, batch_job_name) request = { \"transformjobname\": batch_job_name, \"modelname\": 'xgboost-parquet-example-training-2022-03-28-16-02-31-model', \"transformoutput\": { \"s3outputpath\": output_location, \"accept\": \"text\/csv\", \"assemblewith\": \"line\", }, \"transforminput\": { \"datasource\": {\"s3datasource\": {\"s3datatype\": \"s3prefix\", \"s3uri\": input_location}}, \"contenttype\": \"text\/csv\", \"splittype\": \"line\", \"compressiontype\": \"none\", }, \"transformresources\": {\"instancetype\": \"ml.m4.xlarge\", \"instancecount\": 1}, } .create_transform_job(**request) print(\"created transform job with name: \", batch_job_name) # wait until the job finishes try: .get_waiter(\"transform_job_completed_or_stopped\").wait(transformjobname=batch_job_name) finally: response = .describe_transform_job(transformjobname=batch_job_name) status = response[\"transformjobstatus\"] print(\"transform job ended with status: \" + status) if status == \"failed\": message = response[\"failurereason\"] print(\"transform failed with the following error: {}\".format(message)) raise exception(\"transform job failed\") everything else is working well. i've had no luck with this on anyother forum.",
        "Question_original_content_gpt_summary":"The user is encountering an error when trying to use a batch transform example provided by AWS, and has been unable to find a solution on other forums.",
        "Question_preprocessed_content":"Title: trying example but getting error attributeerror module '' has no attribute; Content: hi, i keep getting this error attributeerror module '' has no attribute when using a batch transform example that aws graciously had in the notebook instances. code also, i updated to the newest package and its still not working. %%time import time from time import gmtime, strftime batch transform + strftime bucket, prefix, use input data without id column prefix, request , transforminput , contenttype splittype line , compressiontype none , , transformresources , print wait until the job finishes try finally response status response print if status failed message response print raise exception everything else is working well. i've had no luck with this on anyother forum.",
        "Answer_original_content":"please double check what type is object. check out this example = boto3.client(service_name=\"\") .create_transform_job(...)",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"please double check what type is object. check out this example"
    },
    {
        "Question_id":73049314.0,
        "Question_title":"Using CLI to start using SageMaker in browser",
        "Question_body":"<p>In the case of using AWS only via aws cli, how can I open a sagemaker app in the browser if I already create sagemaker app and have apparn?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1658310173033,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":64.0,
        "Owner_creation_time":1554458829380,
        "Owner_last_access_time":1663752870560,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73049314",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: using cli to start using in browser; Content: in the case of using aws only via aws cli, how can i open a app in the browser if i already create app and have apparn?",
        "Question_original_content_gpt_summary":"The user is struggling to open an AWS app in the browser using the AWS CLI.",
        "Question_preprocessed_content":"Title: using cli to start using in browser; Content: in the case of using aws only via aws cli, how can i open a app in the browser if i already create app and have apparn?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":53549566.0,
        "Question_title":"Install packages in conda terminal jupyter amazon sagemaker",
        "Question_body":"<p>I have a project in Amazon-Sage-Maker. For this, I have to uninstall specific packages and install others in the terminal. But every time I close or stop the instance I have to go to the terminal and make all the installations again. Why is this happening?<\/p>\n\n<p>The package with which I am experimenting with this trouble is psycopg2:<\/p>\n\n<pre><code>import psycopg2\n<\/code><\/pre>\n\n<p>Gives me a warning that suggests that I should uninstall it and install psycopg2-binary.\nSo I open the terminal and code:<\/p>\n\n<pre><code>pip uninstall psycopg2\n<\/code><\/pre>\n\n<p>Then in the notebook, I code:<\/p>\n\n<pre><code>import psycopg2\n<\/code><\/pre>\n\n<p>And have no problem, but if I close and open the instance back, I get the same error and have to go through all the process again. <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2.0,
        "Question_creation_time":1543537622477,
        "Question_favorite_count":2.0,
        "Question_score":2.0,
        "Question_view_count":6766.0,
        "Owner_creation_time":1458416707412,
        "Owner_last_access_time":1661464986272,
        "Owner_reputation":617.0,
        "Owner_up_votes":74.0,
        "Owner_down_votes":0.0,
        "Owner_views":117.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Ecuador, Quito",
        "Question_last_edit_time":1543593181703,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53549566",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: install packages in conda terminal jupyter ; Content: i have a project in amazon-sage-maker. for this, i have to uninstall specific packages and install others in the terminal. but every time i close or stop the instance i have to go to the terminal and make all the installations again. why is this happening? the package with which i am experimenting with this trouble is psycopg2: import psycopg2 gives me a warning that suggests that i should uninstall it and install psycopg2-binary. so i open the terminal and code: pip uninstall psycopg2 then in the notebook, i code: import psycopg2 and have no problem, but if i close and open the instance back, i get the same error and have to go through all the process again.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with installing packages in a conda terminal jupyter, as they have to repeatedly uninstall and reinstall packages when they close or stop the instance.",
        "Question_preprocessed_content":"Title: install packages in conda terminal jupyter; Content: i have a project in amazon sage maker. for this, i have to uninstall specific packages and install others in the terminal. but every time i close or stop the instance i have to go to the terminal and make all the installations again. why is this happening? the package with which i am experimenting with this trouble is psycopg gives me a warning that suggests that i should uninstall it and install psycopg binary. so i open the terminal and code then in the notebook, i code and have no problem, but if i close and open the instance back, i get the same error and have to go through all the process again.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":60879944.0,
        "Question_title":"SageMaker - What IAM Permission to specify for ECR?",
        "Question_body":"<h1>Question<\/h1>\n\n<p>Is this the ECR IAM permission required for SageMaker to use the XGBoost of the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Amazon SageMaker built-in algorithms<\/a> in the <strong>us-west-1<\/strong> region?<\/p>\n\n<pre><code>\"Effect\": \"Allow\",\n\"Action\": [\n  \"ecr:GetAuthorizationToken\",\n  \"ecr:BatchCheckLayerAvailability\",\n  \"ecr:GetDownloadUrlForLayer\",\n  \"ecr:BatchGetImage\"\n],\n\"Resource\": [\n  \"arn:aws:ecr:us-west-1:632365934929:repository\/632365934929.dkr.ecr.us-west-1.amazonaws.com\/xgboost:1\"\n]\n<\/code><\/pre>\n\n<h1>Background<\/h1>\n\n<p>The AWS document <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html#sagemaker-roles-createnotebookinstance-perms\" rel=\"nofollow noreferrer\">Amazon SageMaker Roles<\/a> tells to specify <strong>TrainingImage<\/strong> value of the <strong>CreateTrainingJob<\/strong> API.<\/p>\n\n<pre><code>Scope ecr permissions as follows:\n- Scope to the AlgorithmSpecification.TrainingImage value that you specify in a CreateTrainingJob request.\n- Scope to the PrimaryContainer.Image value that you specify in a CreateModel request:\n\n\n\"Effect\": \"Allow\",\n\"Action\": [\n  \"ecr:BatchCheckLayerAvailability\",\n  \"ecr:GetDownloadUrlForLayer\",\n  \"ecr:BatchGetImage\"\n],\n\"Resource\": [\n  \"arn:aws:ecr:::repository\/my-repo1\",\n  \"arn:aws:ecr:::repository\/my-repo2\",\n  \"arn:aws:ecr:::repository\/my-repo3\"\n]\n<\/code><\/pre>\n\n<p>The AWS SageMaker API document <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_AlgorithmSpecification.html#sagemaker-Type-AlgorithmSpecification-TrainingImage\" rel=\"nofollow noreferrer\">TrainingImage<\/a> tells to specify the algorithm <strong>docker image registry path<\/strong> as the value.<\/p>\n\n<blockquote>\n  <p><strong>TrainingImage<\/strong><br\/><br\/>\n  The <strong>registry path of the Docker image<\/strong> that contains the training\n  algorithm. For information about docker registry paths for built-in\n  algorithms, see Algorithms Provided by Amazon SageMaker: Common\n  Parameters. Amazon SageMaker supports both <strong>registry\/repository[:tag]<\/strong>\n  and registry\/repository[@digest] image path formats.<\/p>\n<\/blockquote>\n\n<p>The AWS document <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-algo-docker-registry-paths.html\" rel=\"nofollow noreferrer\">Common parameters for built-in algorithms<\/a> indicates the XGBoost registry path is <code>632365934929.dkr.ecr.us-west-1.amazonaws.com\/xgboost:1<\/code>.<\/p>\n\n<blockquote>\n  <p><br\/>\n  |Algorithm name|Training image and inference image registry path|<br\/>\n  |XGBoost       | <strong>ecr_path<\/strong>\/xgboost:<strong>tag<\/strong>|<br\/>\n  <br\/>\n  <strong>ecr_path<\/strong> (Algorithms: BlazingText, ..., Seq2Seq, and XGBoost (0.72)<br\/>\n  | us-west-1 | 632365934929.dkr.ecr.us-west-1.amazonaws.com |  <\/p>\n  \n  <p>For the Training Image and Inference Image Registry Path column, <strong>use the :1 version tag<\/strong> to ensure that you are using a stable version of the algorithm. You can reliably host a model trained using an image with the :1 tag on an inference image that has the :1 tag. <\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1585282753820,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1170.0,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583236,
        "Owner_reputation":14749.0,
        "Owner_up_votes":641.0,
        "Owner_down_votes":62.0,
        "Owner_views":968.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1585283190403,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60879944",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - what iam permission to specify for ecr?; Content: question is this the ecr iam permission required for to use the xgboost of the built-in algorithms in the us-west-1 region? \"effect\": \"allow\", \"action\": [ \"ecr:getauthorizationtoken\", \"ecr:batchchecklayeravailability\", \"ecr:getdownloadurlforlayer\", \"ecr:batchgetimage\" ], \"resource\": [ \"arn:aws:ecr:us-west-1:632365934929:repository\/632365934929.dkr.ecr.us-west-1.amazonaws.com\/xgboost:1\" ] background the aws document roles tells to specify trainingimage value of the createtrainingjob api. scope ecr permissions as follows: - scope to the algorithmspecification.trainingimage value that you specify in a createtrainingjob request. - scope to the primarycontainer.image value that you specify in a createmodel request: \"effect\": \"allow\", \"action\": [ \"ecr:batchchecklayeravailability\", \"ecr:getdownloadurlforlayer\", \"ecr:batchgetimage\" ], \"resource\": [ \"arn:aws:ecr:::repository\/my-repo1\", \"arn:aws:ecr:::repository\/my-repo2\", \"arn:aws:ecr:::repository\/my-repo3\" ] the api document trainingimage tells to specify the algorithm docker image registry path as the value. trainingimage the registry path of the docker image that contains the training algorithm. for information about docker registry paths for built-in algorithms, see algorithms provided by : common parameters. supports both registry\/repository[:tag] and registry\/repository[@digest] image path formats. the aws document common parameters for built-in algorithms indicates the xgboost registry path is 632365934929.dkr.ecr.us-west-1.amazonaws.com\/xgboost:1. |algorithm name|training image and inference image registry path| |xgboost | ecr_path\/xgboost:tag| ecr_path (algorithms: blazingtext, ..., seq2seq, and xgboost (0.72) | us-west-1 | 632365934929.dkr.ecr.us-west-1.amazonaws.com | for the training image and inference image registry path column, use the :1 version tag to ensure that you are using a stable version of the algorithm. you can reliably host a model trained using an image with the :1 tag on an inference image that has the :1 tag.",
        "Question_original_content_gpt_summary":"The user is trying to determine the IAM permissions required to use the XGBoost algorithm in the US-West-1 region, and is navigating the AWS documentation to understand the registry path and version tag needed for the training and inference images.",
        "Question_preprocessed_content":"Title: what iam permission to specify for ecr?; Content: question is this the ecr iam permission required for to use the xgboost of the built in algorithms in the us west region? background the aws document roles tells to specify trainingimage value of the createtrainingjob api. the api document trainingimage tells to specify the algorithm docker image registry path as the value. trainingimage the registry path of the docker image that contains the training algorithm. for about docker registry paths for built in algorithms, see algorithms provided by common parameters. supports both and image path formats. the aws document common parameters for built in algorithms indicates the xgboost registry path is . algorithm name training image and inference image registry path xgboost us west for the training image and inference image registry path column, use the version tag to ensure that you are using a stable version of the algorithm. you can reliably host a model trained using an image with the tag on an inference image that has the tag.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":56927813.0,
        "Question_title":"Using of Estamator.evaluate() on trained sagemaker tensorflow model",
        "Question_body":"<p>After I've trained and deployed the model with AWS SageMaker, I want to evaluate it on several csv files:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>- category-1-eval.csv (~700000 records)\n- category-2-eval.csv (~500000 records)\n- category-3-eval.csv (~800000 records)\n...\n<\/code><\/pre>\n\n<p>The right way to do this is with using <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/estimator\/Estimator#evaluate\" rel=\"nofollow noreferrer\">Estimator.evaluate()<\/a> method, as it is fast.<\/p>\n\n<p>The problem is - I cannot find the way to restore SageMaker model into Tensorflow Estimator, is it possible?<\/p>\n\n<p>I've tried to restore a model like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>tf.estimator.DNNClassifier(\n    feature_columns=...,\n    hidden_units=[...],\n    model_dir=\"s3:\/\/&lt;bucket_name&gt;\/checkpoints\",\n)\n<\/code><\/pre>\n\n<p>In AWS SageMaker documentation a different approach is described - <a href=\"https:\/\/docs.aws.amazon.com\/en_us\/sagemaker\/latest\/dg\/how-it-works-model-validation.html\" rel=\"nofollow noreferrer\">to test the actual endpoint from the Notebook<\/a> - but it takes to much time and requires a lot of API calls to the endpoint.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1562553970617,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":155.0,
        "Owner_creation_time":1530642335903,
        "Owner_last_access_time":1622266440183,
        "Owner_reputation":53.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":24.0,
        "Answer_body":"<p>if you used the built-in Tensorflow container, your model has been saved in Tensorflow Serving format, e.g.:<\/p>\n\n<pre><code>$ tar tfz model.tar.gz\nmodel\/\nmodel\/1\/\nmodel\/1\/saved_model.pb\nmodel\/1\/variables\/\nmodel\/1\/variables\/variables.index\nmodel\/1\/variables\/variables.data-00000-of-00001\n<\/code><\/pre>\n\n<p>You can easily load it with Tensorflow Serving on your local machine, and send it samples to predict. More info at <a href=\"https:\/\/www.tensorflow.org\/tfx\/guide\/serving\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/tfx\/guide\/serving<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1562591149540,
        "Answer_score":1.0,
        "Owner_location":"Vancouver, BC, Canada",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1562591826680,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56927813",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: using of estamator.evaluate() on trained tensorflow model; Content: after i've trained and deployed the model with , i want to evaluate it on several csv files: - category-1-eval.csv (~700000 records) - category-2-eval.csv (~500000 records) - category-3-eval.csv (~800000 records) ... the right way to do this is with using estimator.evaluate() method, as it is fast. the problem is - i cannot find the way to restore model into tensorflow estimator, is it possible? i've tried to restore a model like this: tf.estimator.dnnclassifier( feature_columns=..., hidden_units=[...], model_dir=\"s3:\/\/<bucket_name>\/checkpoints\", ) in documentation a different approach is described - to test the actual endpoint from the notebook - but it takes to much time and requires a lot of api calls to the endpoint.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with using the estimator.evaluate() method to evaluate a trained TensorFlow model on several CSV files, and is looking for a way to restore the model into a TensorFlow estimator.",
        "Question_preprocessed_content":"Title: using of on trained tensorflow model; Content: after i've trained and deployed the model with , i want to evaluate it on several csv files the right way to do this is with using method, as it is fast. the problem is i cannot find the way to restore model into tensorflow estimator, is it possible? i've tried to restore a model like this in documentation a different approach is described to test the actual endpoint from the notebook but it takes to much time and requires a lot of api calls to the endpoint.",
        "Answer_original_content":"if you used the built-in tensorflow container, your model has been saved in tensorflow serving format, e.g.: $ tar tfz model.tar.gz model\/ model\/1\/ model\/1\/saved_model.pb model\/1\/variables\/ model\/1\/variables\/variables.index model\/1\/variables\/variables.data-00000-of-00001 you can easily load it with tensorflow serving on your local machine, and send it samples to predict. more info at https:\/\/www.tensorflow.org\/tfx\/guide\/serving",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"if you used the built in tensorflow container, your model has been saved in tensorflow serving format, you can easily load it with tensorflow serving on your local machine, and send it samples to predict. more at"
    },
    {
        "Question_id":70940773.0,
        "Question_title":"Misstated quota exceed errors on managed notebooks in GCP",
        "Question_body":"<p>I am migrating some of my notebooks from the soon to be deprecated AI-Platform to the new Vertex platform in GCP. In Vertex I am using the &quot;Managed Notebooks&quot;, and all seemed to be working fine, but then suddenly I got this strange quota exceed error, when I am still far below my limits (I am currently at 4-5% of limit for the given APIs). See the error below:<\/p>\n<p>*Restarting notebook prototyping-notebook: Quota of &quot;::internal: operation &quot;projects\/1096432937575\/locations\/us-central1\/operations\/start-92b393a0-6c2e-4e11-be32-0172418d33c11643717891501967646&quot; completed with error: %!w(<em>status.Status=&amp;{{{} [] [] } 13 INTERNAL: operation name: operation-1643717891545-5d6f3e5095a3c-9aa08cdd-4bd8b9cd error code: QUOTA_EXCEEDED error message: Quota &quot; exceeded limit: 40 in region us-central1.<\/em><\/p>\n<p>I know it is still in preview, but anyone faced the same error or have any experience in how to fix this? Is there some combination of configurations that seems to be more stable than others?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6.0,
        "Question_creation_time":1643719337910,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":129.0,
        "Owner_creation_time":1445972391512,
        "Owner_last_access_time":1663852274612,
        "Owner_reputation":181.0,
        "Owner_up_votes":16.0,
        "Owner_down_votes":0.0,
        "Owner_views":80.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70940773",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: misstated quota exceed errors on managed notebooks in gcp; Content: i am migrating some of my notebooks from the soon to be deprecated ai-platform to the new vertex platform in gcp. in vertex i am using the \"managed notebooks\", and all seemed to be working fine, but then suddenly i got this strange quota exceed error, when i am still far below my limits (i am currently at 4-5% of limit for the given apis). see the error below: *restarting notebook prototyping-notebook: quota of \"::internal: operation \"projects\/1096432937575\/locations\/us-central1\/operations\/start-92b393a0-6c2e-4e11-be32-0172418d33c11643717891501967646\" completed with error: %!w(status.status=&{{{} [] [] } 13 internal: operation name: operation-1643717891545-5d6f3e5095a3c-9aa08cdd-4bd8b9cd error code: quota_exceeded error message: quota \" exceeded limit: 40 in region us-central1. i know it is still in preview, but anyone faced the same error or have any experience in how to fix this? is there some combination of configurations that seems to be more stable than others?",
        "Question_original_content_gpt_summary":"The user is encountering a strange quota exceed error when using managed notebooks in GCP, despite being far below their limits.",
        "Question_preprocessed_content":"Title: misstated quota exceed errors on managed notebooks in gcp; Content: i am migrating some of my notebooks from the soon to be deprecated ai platform to the new vertex platform in gcp. in vertex i am using the managed notebooks , and all seemed to be working fine, but then suddenly i got this strange quota exceed error, when i am still far below my limits . see the error below restarting notebook prototyping notebook quota of internal operation completed with error internal operation name operation d f e a c aa cdd bd b cd error code error message quota exceeded limit in region us central . i know it is still in preview, but anyone faced the same error or have any experience in how to fix this? is there some combination of configurations that seems to be more stable than others?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":52876202.0,
        "Question_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Question_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1539872573950,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":112.0,
        "Owner_creation_time":1489873508190,
        "Owner_last_access_time":1648846099743,
        "Owner_reputation":509.0,
        "Owner_up_votes":5.0,
        "Owner_down_votes":0.0,
        "Owner_views":84.0,
        "Answer_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1539965456440,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to bulk test the object detection model with a .mat dataset or s3 folder of images?; Content: i have trained the following model: https:\/\/github.com\/awslabs\/amazon--examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco i've tried both the json and recordio version. in both, the algorithm is tested on one sample image. however, i have a dataset of 2000 pictures, which i would like to test. i have saved the 2000 jpg pictures in a folder within an s3 bucket and i also have two .mat files (pics + ground truth). how can i apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time? i am using the code below to load a single picture from my s3 bucket: object = bucket.object('pictures\/pic1.jpg') object.download_file('pic1.jpg') img=mpimg.imread('pic1.jpg') img_name = 'pic1.jpg' imgplot = plt.imshow(img) plt.show(imgplot) with open(img_name, 'rb') as image: f = image.read() b = bytearray(f) ne = open('n.txt','wb') ne.write(b) import json object_detector.content_type = 'image\/jpeg' results = object_detector.predict(b) detections = json.loads(results) print (detections['prediction'])",
        "Question_original_content_gpt_summary":"The user is facing a challenge of how to bulk test an object detection model with a .mat dataset or s3 folder of images.",
        "Question_preprocessed_content":"Title: how to bulk test the object detection model with a .mat dataset or s folder of images?; Content: i have trained the following model i've tried both the json and recordio version. in both, the algorithm is tested on one sample image. however, i have a dataset of pictures, which i would like to test. i have saved the jpg pictures in a folder within an s bucket and i also have two .mat files . how can i apply this model to all pictures at once and then save the results, rather than doing it one picture at a time? i am using the code below to load a single picture from my s bucket",
        "Answer_original_content":"i'm not sure if i understood your question correctly. however, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model. the code would look something like this. import numpy as np ... # predict_images_list is a python list of byte arrays predict_images = np.stack(predict_images_list) with graph.as_default(): # results is an list of typical results you'd get. results = object_detector.predict(predict_images) but, i'm not sure if it's a good idea to feed 2000 images at once. better to batch them in 20-30 images at a time and predict.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i'm not sure if i understood your question correctly. however, if you want to feed multiple images to the model at once, you can create a multi dimensional array of images to feed the model. the code would look something like this. but, i'm not sure if it's a good idea to feed images at once. better to batch them in images at a time and predict."
    },
    {
        "Question_id":null,
        "Question_title":"Is there any way to display the entire source path of the file",
        "Question_body":"Hi,\n\nIn the MLFLOW UI, the source only displays the name of the file whereas when I set the source tag I set the entire path in that.\nis there any way to display the entire path, specially if the file is saved in a GIT repository.\n\n\nThanks",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1564461471000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":8.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/CKfSORLcfRs",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-07-31T16:37:34",
                "Answer_body":"Hi Shevy,\n\n\nWhich file specifically are you referring to? Is it an artifact or is it the entry point for a project?\n\n\nMatei\u00a0\n\n\n\n\ue5d3\n\ue5d3\nConfidentiality Notice and Disclaimer: This email (including any attachments) contains information that may be confidential, privileged and\/or copyrighted. If you are not the intended recipient, please notify the sender immediately and destroy this email. Any unauthorized use of the contents of this email in any manner whatsoever, is strictly prohibited. If improper activity is suspected, all available information may be used by the sender for possible disciplinary action, prosecution, civil claim or any remedy or lawful purpose. Email transmission cannot be guaranteed to be secure or error-free, as information could be intercepted, lost, arrive late, or contain viruses. The sender is not liable whatsoever for damage resulting from the opening of this message and\/or the use of the information contained in this message and\/or attachments. Expressions in this email cannot be treated as opined by the sender company management \u2013 they are solely expressed by the sender unless authorized.\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/5c3e66c2-e149-4c44-abd9-4f98c15566bc%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2019-08-01T02:02:56",
                "Answer_body":"Yeah, the UI shortens it to the filename, but the full source is logged and is available through the API. If you want, you could submit a patch to mlflow\/server\/js\/src\/utils\/Utils.js that renders the source in a tooltip. I don\u2019t think we should show the full path by default because it can be very long.\n\nMatei\n\n> On Jul 31, 2019, at 10:35 PM, Shevy Mittal <shevy....@gslab.com> wrote:\n>\n> I mean the entry point of the project. In the MLflow UI screen. What is the \"SOURCE\" tag for?\n> Currently I was hardcoding it as a path on my local so I know which python file it belongs to. However all that is visible is the name of the file and not the whole path.\n\n\ue5d3\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/2A5D407E-16B1-47DE-9C79-85FEAF8C4DDB%40databricks.com.\n\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is there any way to display the entire source path of the file; Content: hi, in the ui, the source only displays the name of the file whereas when i set the source tag i set the entire path in that. is there any way to display the entire path, specially if the file is saved in a git repository. thanks",
        "Question_original_content_gpt_summary":"The user is looking for a way to display the entire source path of a file in the UI, rather than just the file name.",
        "Question_preprocessed_content":"Title: is there any way to display the entire source path of the file; Content: hi, in the ui, the source only displays the name of the file whereas when i set the source tag i set the entire path in that. is there any way to display the entire path, specially if the file is saved in a git repository. thanks",
        "Answer_original_content":"hi shevy, which file specifically are you referring to? is it an artifact or is it the entry point for a project? matei confidentiality notice and disclaimer: this email (including any attachments) contains information that may be confidential, privileged and\/or copyrighted. if you are not the intended recipient, please notify the sender immediately and destroy this email. any unauthorized use of the contents of this email in any manner whatsoever, is strictly prohibited. if improper activity is suspected, all available information may be used by the sender for possible disciplinary action, prosecution, civil claim or any remedy or lawful purpose. email transmission cannot be guaranteed to be secure or error-free, as information could be intercepted, lost, arrive late, or contain viruses. the sender is not liable whatsoever for damage resulting from the opening of this message and\/or the use of the information contained in this message and\/or attachments. expressions in this email cannot be treated as opined by the sender company management they are solely expressed by the sender unless authorized. -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/5c3e66c2-e149-4c44-abd9-4f98c15566bc%40googlegroups.com. yeah, the ui shortens it to the filename, but the full source is logged and is available through the api. if you want, you could submit a patch to \/server\/js\/src\/utils\/utils.js that renders the source in a tooltip. i dont think we should show the full path by default because it can be very long. matei > on jul 31, 2019, at 10:35 pm, shevy mittal wrote: > > i mean the entry point of the project. in the ui screen. what is the \"source\" tag for? > currently i was hardcoding it as a path on my local so i know which python file it belongs to. however all that is visible is the name of the file and not the whole path. > to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/2a5d407e-16b1-47de-9c79-85feaf8c4ddb%40databricks.com.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi shevy, which file specifically are you referring to? is it an artifact or is it the entry point for a project? matei confidentiality notice and disclaimer this email contains that may be confidential, privileged copyrighted. if you are not the intended recipient, please notify the sender immediately and destroy this email. any unauthorized use of the contents of this email in any manner whatsoever, is strictly prohibited. if improper activity is suspected, all available may be used by the sender for possible disciplinary action, prosecution, civil claim or any remedy or lawful purpose. email transmission cannot be guaranteed to be secure or error free, as could be intercepted, lost, arrive late, or contain viruses. the sender is not liable whatsoever for damage resulting from the opening of this message the use of the contained in this message attachments. expressions in this email cannot be treated as opined by the sender company management they are solely expressed by the sender unless authorized. you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to to view this discussion on the web visit yeah, the ui shortens it to the filename, but the full source is logged and is available through the api. if you want, you could submit a patch to that renders the source in a tooltip. i dont think we should show the full path by default because it can be very long. matei > on jul , , at pm, shevy mittal wrote > > i mean the entry point of the project. in the ui screen. what is the source tag for? > currently i was hardcoding it as a path on my local so i know which python file it belongs to. however all that is visible is the name of the file and not the whole path. > to view this discussion on the web visit"
    },
    {
        "Question_id":71222258.0,
        "Question_title":"Error in AWS SageMaker Ground Truth labeled job creation",
        "Question_body":"<p>I'm using AWS SageMaker Ground Truth for labeling images. I have uploaded the data into s3 bucket, create the IAM role to access 'S3,SageMaker,Groundtruth, and IAM'. When I am trying to create labeling job, it give me this error:<\/p>\n<blockquote>\n<p>NetworkingError: Network Failure - The S3 bucket 'sm-gt-s3-enron' you entered in Input dataset location cannot be reached. Either the bucket does not exist, or you do not have permission to access it. If the bucket does not exist, update Input dataset location with a new S3 URI. If the bucket exists, give the IAM entity you are using to create this labeling job permission to read and write to this S3 bucket, and try your request again.<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1645536752887,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":225.0,
        "Owner_creation_time":1436595102336,
        "Owner_last_access_time":1650532390720,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1645599167900,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71222258",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: error in ground truth labeled job creation; Content: i'm using ground truth for labeling images. i have uploaded the data into s3 bucket, create the iam role to access 's3,,groundtruth, and iam'. when i am trying to create labeling job, it give me this error: networkingerror: network failure - the s3 bucket 'sm-gt-s3-enron' you entered in input dataset location cannot be reached. either the bucket does not exist, or you do not have permission to access it. if the bucket does not exist, update input dataset location with a new s3 uri. if the bucket exists, give the iam entity you are using to create this labeling job permission to read and write to this s3 bucket, and try your request again.",
        "Question_original_content_gpt_summary":"The user is encountering a network error when attempting to create a labeling job using Ground Truth, due to a lack of permission to access the S3 bucket.",
        "Question_preprocessed_content":"Title: error in ground truth labeled job creation; Content: i'm using ground truth for labeling images. i have uploaded the data into s bucket, create the iam role to access 's ,,groundtruth, and iam'. when i am trying to create labeling job, it give me this error networkingerror network failure the s bucket 'sm gt s enron' you entered in input dataset location cannot be reached. either the bucket does not exist, or you do not have permission to access it. if the bucket does not exist, update input dataset location with a new s uri. if the bucket exists, give the iam entity you are using to create this labeling job permission to read and write to this s bucket, and try your request again.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Designer: Export Code",
        "Question_body":"Is there an option to export the Azure ML Designer to code so we can copy between workspaces?",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1646150939773,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/755142\/azure-ml-designer-export-code.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-02T06:20:05.287Z",
                "Answer_score":1,
                "Answer_body":"Hi, this feature is currently not supported as mentioned on this thread. However, it's on the roadmap.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-11-10T18:23:47.713Z",
                "Answer_score":0,
                "Answer_body":"Is the roadmap public? When is this feature planning on being released?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: designer: export code; Content: is there an option to export the designer to code so we can copy between workspaces?",
        "Question_original_content_gpt_summary":"The user is looking for an option to export the Designer to code so they can copy between workspaces.",
        "Question_preprocessed_content":"Title: designer export code; Content: is there an option to export the designer to code so we can copy between workspaces?",
        "Answer_original_content":"hi, this feature is currently not supported as mentioned on this thread. however, it's on the roadmap.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, this feature is currently not supported as mentioned on this thread. however, it's on the roadmap."
    },
    {
        "Question_id":62579563.0,
        "Question_title":"NameError: name 'dbutils' is not defined",
        "Question_body":"<p>I've .py file with following code line and it lives in git.<\/p>\n<pre><code>dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234') \n<\/code><\/pre>\n<p>I am using mlflow to run it in remote databricks job cluster. I've conda.yml and MLProject file to pick it up from git and run it in databricks job cluster but I am getting following error.<\/p>\n<pre><code>  File &quot;tea\/src\/cltv_xgb_tea.py&quot;, line 40, in &lt;module&gt;\n    dbutils.widgets.text(name='CORPORATION_ID', defaultValue='1234')\nNameError: name 'dbutils' is not defined\n<\/code><\/pre>\n<p>Any help\/solution is much appreciated.<\/p>\n<hr \/>\n<p>My current files in git<\/p>\n<p>Conda.yml has<\/p>\n<pre><code>name: cicd-environment\nchannels:\n  - defaults\ndependencies:\n  - python=3.7\n  - pip=19.0.3\n  - pip:\n    - mlflow==1.7.2\n    - DBUtils==1.3\n    - ipython==7.14.0\n    - databricks-connect==6.5.1\n    - invoke==1.4.1\n    - awscli==1.18.87\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_creation_time":1593101026770,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":810.0,
        "Owner_creation_time":1555347036127,
        "Owner_last_access_time":1663694487236,
        "Owner_reputation":352.0,
        "Owner_up_votes":27.0,
        "Owner_down_votes":1.0,
        "Owner_views":88.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Minnesota, USA",
        "Question_last_edit_time":1593403896032,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62579563",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: nameerror: name 'dbutils' is not defined; Content: i've .py file with following code line and it lives in git. dbutils.widgets.text(name='corporation_id', defaultvalue='1234') i am using to run it in remote databricks job cluster. i've conda.yml and mlproject file to pick it up from git and run it in databricks job cluster but i am getting following error. file \"tea\/src\/cltv_xgb_tea.py\", line 40, in <module> dbutils.widgets.text(name='corporation_id', defaultvalue='1234') nameerror: name 'dbutils' is not defined any help\/solution is much appreciated. my current files in git conda.yml has name: cicd-environment channels: - defaults dependencies: - python=3.7 - pip=19.0.3 - pip: - ==1.7.2 - dbutils==1.3 - ipython==7.14.0 - databricks-connect==6.5.1 - invoke==1.4.1 - awscli==1.18.87",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to run a .py file in a remote Databricks job cluster, due to the 'dbutils' module not being defined.",
        "Question_preprocessed_content":"Title: nameerror name 'dbutils' is not defined; Content: i've .py file with following code line and it lives in git. i am using to run it in remote databricks job cluster. i've and mlproject file to pick it up from git and run it in databricks job cluster but i am getting following error. any is much appreciated. my current files in git has",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Constant Liar algorithm for sweeps?",
        "Question_body":"<p>Hi,<\/p>\n<p>Just wondering if you have constant liar algorithm implemented internally for hyper-parameter suggestions in parallel. If I understand wandb API correctly, it is geared more for sequential suggestions, and considering our company can run pods in parallel, would be amazing if you guys can implement this on your end, rather than us hacking it on our end.<\/p>\n<p>The basic idea is that for the first pod (in a parllel set) it will suggest the hyper-parameters as usual, but for the 2nd and other pods starting now in parallel, it will send back the worst loss it has currently seen. The logic being that the next suggested hyper-parameters will be far away from ones suggested to first one. You could probably be smarter here since wandb has access to loss metrics as it trains, but that would be a side project.<\/p>\n<p>Here is a link with more depth: <a href=\"https:\/\/github.com\/microsoft\/nni\/blob\/98f66f76d310b0e0679823d966fdaa6adafb66c2\/docs\/en_US\/CommunitySharings\/ParallelizingTpeSearch.md\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">nni\/ParallelizingTpeSearch.md at 98f66f76d310b0e0679823d966fdaa6adafb66c2 \u00b7 microsoft\/nni \u00b7 GitHub<\/a><\/p>\n<p>Edit 1: follow up question, do you use anything more advanced than sklearn GPs for bayes search (basing my question on <a href=\"https:\/\/github.com\/wandb\/client\/blob\/master\/wandb\/sweeps\/bayes_search.py#L85\" rel=\"noopener nofollow ugc\">this<\/a>).<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1645510975954,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":190.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/constant-liar-algorithm-for-sweeps\/1961",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-23T14:23:44.845Z",
                "Answer_body":"<p>Hi Sachin, we currently use the Baye\u2019s implementation (<a href=\"https:\/\/github.com\/wandb\/sweeps\/blob\/master\/src\/sweeps\/bayes_search.py\" class=\"inline-onebox-loading\" rel=\"noopener nofollow ugc\">https:\/\/github.com\/wandb\/sweeps\/blob\/master\/src\/sweeps\/bayes_search.py<\/a>) for running parallel sweeps. I can put a ticket for this request, but can you tell me why your company prefers the Constant Liar algorithm?<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-27T23:46:58.599Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lesliewandb\">@lesliewandb<\/a>, Thanks for getting back to me. I feel like the CL algorithm is independent of whether it is Bayes Search or TPE.<\/p>\n<p>Consider this example. Suppose we have conducted 1000 sweeps already, so the sampler is more or less confident about the parameter space. The problem with the current implementation is that if I were to spin up the next 5 sweeps in parallel, wandb ignores the fact they are happening in parallel and would independently suggest 5 hyper-parameters. There is a good chance that all these 5 parameters are extremely similar. However, if each sample \u201clies\u201d and says that it was a bad location, it would force the sampler to look at a different location and makes sure we can \u201cexplore\u201d the hyper-parameter space better.<\/p>\n<p>So basically what I\u2019m asking for is for the constant liar algorithm on top of bayes search. I do however, think that TPE algorithm is better than sklearn\u2019s GP\u2019s but that can be another discussion.<\/p>",
                "Answer_score":15.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-01T15:08:23.685Z",
                "Answer_body":"<p>I see, thank you for the clarification! I\u2019ll create a ticket for this and I\u2019ll let you know if there\u2019s any updates<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-28T23:47:35.395Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: constant liar algorithm for sweeps?; Content: hi, just wondering if you have constant liar algorithm implemented internally for hyper-parameter suggestions in parallel. if i understand api correctly, it is geared more for sequential suggestions, and considering our company can run pods in parallel, would be amazing if you guys can implement this on your end, rather than us hacking it on our end. the basic idea is that for the first pod (in a parllel set) it will suggest the hyper-parameters as usual, but for the 2nd and other pods starting now in parallel, it will send back the worst loss it has currently seen. the logic being that the next suggested hyper-parameters will be far away from ones suggested to first one. you could probably be smarter here since has access to loss metrics as it trains, but that would be a side project. here is a link with more depth: nni\/parallelizingtpesearch.md at 98f66f76d310b0e0679823d966fdaa6adafb66c2 \u00b7 microsoft\/nni \u00b7 github edit 1: follow up question, do you use anything more advanced than sklearn gps for bayes search (basing my question on this).",
        "Question_original_content_gpt_summary":"The user is asking if the company has implemented a constant liar algorithm for hyper-parameter suggestions in parallel, which would suggest the worst loss seen so far for subsequent pods in a parallel set.",
        "Question_preprocessed_content":"Title: constant liar algorithm for sweeps?; Content: hi, just wondering if you have constant liar algorithm implemented internally for hyper parameter suggestions in parallel. if i understand api correctly, it is geared more for sequential suggestions, and considering our company can run pods in parallel, would be amazing if you guys can implement this on your end, rather than us hacking it on our end. the basic idea is that for the first pod it will suggest the hyper parameters as usual, but for the nd and other pods starting now in parallel, it will send back the worst loss it has currently seen. the logic being that the next suggested hyper parameters will be far away from ones suggested to first one. you could probably be smarter here since has access to loss metrics as it trains, but that would be a side project. here is a link with more depth at f f d b e d fdaa adafb c github edit follow up question, do you use anything more advanced than sklearn gps for bayes search .",
        "Answer_original_content":"hi sachin, we currently use the bayes implementation (https:\/\/github.com\/\/sweeps\/blob\/master\/src\/sweeps\/bayes_search.py) for running parallel sweeps. i can put a ticket for this request, but can you tell me why your company prefers the constant liar algorithm? hi @leslie, thanks for getting back to me. i feel like the cl algorithm is independent of whether it is bayes search or tpe. consider this example. suppose we have conducted 1000 sweeps already, so the sampler is more or less confident about the parameter space. the problem with the current implementation is that if i were to spin up the next 5 sweeps in parallel, ignores the fact they are happening in parallel and would independently suggest 5 hyper-parameters. there is a good chance that all these 5 parameters are extremely similar. however, if each sample lies and says that it was a bad location, it would force the sampler to look at a different location and makes sure we can explore the hyper-parameter space better. so basically what im asking for is for the constant liar algorithm on top of bayes search. i do however, think that tpe algorithm is better than sklearns gps but that can be another discussion. i see, thank you for the clarification! ill create a ticket for this and ill let you know if theres any updates this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi sachin, we currently use the bayes implementation for running parallel sweeps. i can put a ticket for this request, but can you tell me why your company prefers the constant liar algorithm? hi thanks for getting back to me. i feel like the cl algorithm is independent of whether it is bayes search or tpe. consider this example. suppose we have conducted sweeps already, so the sampler is more or less confident about the parameter space. the problem with the current implementation is that if i were to spin up the next sweeps in parallel, ignores the fact they are happening in parallel and would independently suggest hyper parameters. there is a good chance that all these parameters are extremely similar. however, if each sample lies and says that it was a bad location, it would force the sampler to look at a different location and makes sure we can explore the hyper parameter space better. so basically what im asking for is for the constant liar algorithm on top of bayes search. i do however, think that tpe algorithm is better than sklearns gps but that can be another discussion. i see, thank you for the clarification! ill create a ticket for this and ill let you know if theres any updates this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":65753455.0,
        "Question_title":"How to get the version of gremlin python client on AWS SageMaker",
        "Question_body":"<p>What is the command to check the version of Gremlin Python client running on a AWS Sagemaker jupyter notebook? I would like to run the command on the jupyter notebook cell.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1610823303577,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":198.0,
        "Owner_creation_time":1475704783950,
        "Owner_last_access_time":1639340625583,
        "Owner_reputation":473.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":34.0,
        "Answer_body":"<p>From a notebook cell you should be able to just ask Pip which version is being used<\/p>\n<pre><code>!pip list | grep gremlinpython\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1610839775480,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65753455",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to get the version of gremlin python client on ; Content: what is the command to check the version of gremlin python client running on a jupyter notebook? i would like to run the command on the jupyter notebook cell.",
        "Question_original_content_gpt_summary":"The user would like to know the command to check the version of the Gremlin Python client running on a Jupyter notebook.",
        "Question_preprocessed_content":"Title: how to get the version of gremlin python client on; Content: what is the command to check the version of gremlin python client running on a jupyter notebook? i would like to run the command on the jupyter notebook cell.",
        "Answer_original_content":"from a notebook cell you should be able to just ask pip which version is being used !pip list | grep gremlinpython",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"from a notebook cell you should be able to just ask pip which version is being used"
    },
    {
        "Question_id":null,
        "Question_title":"Providing Practitioner license to user [Reposted]",
        "Question_body":"A user posted the following question on our legacy community:\u00a0\n\nHello,\n\nI would like to know if it is possible to give a practitioner license to the user using Domino REST API in Domino V4.2 or higher.\n\nAnswering here since we are retiring the legacy community this evening.\u00a0\u00a0\n\nAnswer: Granting license types is not officially supported via REST API.\u00a0 Our engineering team recommends configuring role synchronization via SAML attributes passed to Keycloak: https:\/\/admin.dominodatalab.com\/en\/latest\/keycloak.html?highlight=role%20synchronization#role-synchronization If this is not a workable solution for your team, we recommend submitting a support ticket or asking your CSM\/account team for a solution that fits your needs.\u00a0 Please let us know if there's anything else you need, thanks!",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1624934845000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4402871497108-Providing-Practitioner-license-to-user-Reposted-",
        "Tool":"Domino",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[

        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: providing practitioner license to user [reposted]; Content: a user posted the following question on our legacy community: hello, i would like to know if it is possible to give a practitioner license to the user using rest api in v4.2 or higher. answering here since we are retiring the legacy community this evening. answer: granting license types is not officially supported via rest api. our engineering team recommends configuring role synchronization via saml attributes passed to keycloak: https:\/\/admin.datalab.com\/en\/latest\/keycloak.html?highlight=role%20synchronization#role-synchronization if this is not a workable solution for your team, we recommend submitting a support ticket or asking your csm\/account team for a solution that fits your needs. please let us know if there's anything else you need, thanks!",
        "Question_original_content_gpt_summary":"The user is seeking a way to grant a practitioner license to a user using REST API in v4.2 or higher.",
        "Question_preprocessed_content":"Title: providing practitioner license to user; Content: a user posted the following question on our legacy community hello, i would like to know if it is possible to give a practitioner license to the user using rest api in or higher. answering here since we are retiring the legacy community this evening. answer granting license types is not officially supported via rest api. our engineering team recommends configuring role synchronization via saml attributes passed to keycloak if this is not a workable solution for your team, we recommend submitting a support ticket or asking your team for a solution that fits your needs. please let us know if there's anything else you need, thanks!",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":49426248.0,
        "Question_title":"How to host a composite models on AWS SageMaker",
        "Question_body":"<p>I created separate predictive models (using SageMaker's in-built algorithms) on different segments of the data. In production these models needs to be called based on the segment of input data.<\/p>\n\n<p>Is it possible to host a composite model in SageMaker? How to define the config for deploying a composite model?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1521714475410,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":313.0,
        "Owner_creation_time":1498153762836,
        "Owner_last_access_time":1574344261787,
        "Owner_reputation":31.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49426248",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to host a composite models on ; Content: i created separate predictive models (using 's in-built algorithms) on different segments of the data. in production these models needs to be called based on the segment of input data. is it possible to host a composite model in ? how to define the config for deploying a composite model?",
        "Question_original_content_gpt_summary":"The user is looking for a way to host a composite model on and define the configuration for deploying it.",
        "Question_preprocessed_content":"Title: how to host a composite models on; Content: i created separate predictive models on different segments of the data. in production these models needs to be called based on the segment of input data. is it possible to host a composite model in ? how to define the config for deploying a composite model?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":37196368.0,
        "Question_title":"How do I improve Azure ML learning performance?",
        "Question_body":"<p>Training a simple convolutional network to recognize MNIST digits on Microsoft Azure (in Machine Learning Studio) takes many many times longer than it does for (already very slow) learning of exactly the same model locally, on a CPU (MacBook Pro, with limited memory) with TensorFlow.<\/p>\n\n<p>Is there a way \u2014 perhaps purchasing resources or connecting virtual GPUs \u2014 to improve performance of Azure Machine Learning?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1463083181150,
        "Question_favorite_count":1.0,
        "Question_score":1.0,
        "Question_view_count":1048.0,
        "Owner_creation_time":1299959670312,
        "Owner_last_access_time":1663787198420,
        "Owner_reputation":41475.0,
        "Owner_up_votes":1198.0,
        "Owner_down_votes":107.0,
        "Owner_views":1912.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"United States",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37196368",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i improve learning performance?; Content: training a simple convolutional network to recognize mnist digits on microsoft azure (in machine learning studio) takes many many times longer than it does for (already very slow) learning of exactly the same model locally, on a cpu (macbook pro, with limited memory) with tensorflow. is there a way \u2014 perhaps purchasing resources or connecting virtual gpus \u2014 to improve performance of ?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with training a convolutional network to recognize MNIST digits on Microsoft Azure, and is looking for ways to improve performance.",
        "Question_preprocessed_content":"Title: how do i improve learning performance?; Content: training a simple convolutional network to recognize mnist digits on microsoft azure takes many many times longer than it does for learning of exactly the same model locally, on a cpu with tensorflow. is there a way perhaps purchasing resources or connecting virtual gpus to improve performance of ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":54193723.0,
        "Question_title":"Size of image for prediction with SageMaker object detection?",
        "Question_body":"<p>I'm using the AWS SageMaker \"built in\" object detection algorithm (SSD) and we've trained it on a series of annotated 512x512 images (image_shape=512).  We've deployed an endpoint and when using it for prediction we're getting mixed results.  <\/p>\n\n<p>If the image we use for prediciton is around that 512x512 size we're getting great accuracy and good results.  If the image is significantly larger (e.g. 8000x10000) we get either wildly inaccurate, or no results.  If I manually resize those large images to 512x512pixels the features we're looking for are no longer discernable to the eye.  Which suggests that if my endpoint is resizing images, then that would explain why the model is struggling.<\/p>\n\n<p><strong>Note:<\/strong> Although the size in pexels is large, my images are basically line drawings on a white background. They have very little color and large patches of solid white, so they compress very well.  I'm mot running into the 6Mb request size limit.<\/p>\n\n<p>So, my questions are:<\/p>\n\n<ol>\n<li>Does training the model at image_shape=512 mean my prediction images should also be that same size?<\/li>\n<li>Is there a generally accepted method for doing object detection on very large images?  I can envisage how I might chop the image into smaller tiles then feed each tile to my model, but if there's something \"out of the box\" that will do it for me, then that'd save some effort.<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1547533709270,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":771.0,
        "Owner_creation_time":1226984969400,
        "Owner_last_access_time":1663733393408,
        "Owner_reputation":5789.0,
        "Owner_up_votes":532.0,
        "Owner_down_votes":9.0,
        "Owner_views":464.0,
        "Answer_body":"<p>Your understanding is correct. The endpoint resizes images based on the parameter <code>image_shape<\/code>. To answer your questions:<\/p>\n\n<ol>\n<li>As long as the scale of objects (i.e., expansion of pixels) in the resized images are similar between training and prediction data, the trained model should work.<\/li>\n<li>Cropping is one option. Another method is to train separate models for large and small images as David suggested.<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1547584358316,
        "Answer_score":1.0,
        "Owner_location":"Adelaide, Australia",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54193723",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: size of image for prediction with object detection?; Content: i'm using the \"built in\" object detection algorithm (ssd) and we've trained it on a series of annotated 512x512 images (image_shape=512). we've deployed an endpoint and when using it for prediction we're getting mixed results. if the image we use for prediciton is around that 512x512 size we're getting great accuracy and good results. if the image is significantly larger (e.g. 8000x10000) we get either wildly inaccurate, or no results. if i manually resize those large images to 512x512pixels the features we're looking for are no longer discernable to the eye. which suggests that if my endpoint is resizing images, then that would explain why the model is struggling. note: although the size in pexels is large, my images are basically line drawings on a white background. they have very little color and large patches of solid white, so they compress very well. i'm mot running into the 6mb request size limit. so, my questions are: does training the model at image_shape=512 mean my prediction images should also be that same size? is there a generally accepted method for doing object detection on very large images? i can envisage how i might chop the image into smaller tiles then feed each tile to my model, but if there's something \"out of the box\" that will do it for me, then that'd save some effort.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with object detection accuracy when using images that are significantly larger than the 512x512 size used for training, and is looking for a generally accepted method for doing object detection on very large images.",
        "Question_preprocessed_content":"Title: size of image for prediction with object detection?; Content: i'm using the built in object detection algorithm and we've trained it on a series of annotated x images . we've deployed an endpoint and when using it for prediction we're getting mixed results. if the image we use for prediciton is around that x size we're getting great accuracy and good results. if the image is significantly larger we get either wildly inaccurate, or no results. if i manually resize those large images to x pixels the features we're looking for are no longer discernable to the eye. which suggests that if my endpoint is resizing images, then that would explain why the model is struggling. note although the size in pexels is large, my images are basically line drawings on a white background. they have very little color and large patches of solid white, so they compress very well. i'm mot running into the mb request size limit. so, my questions are does training the model at mean my prediction images should also be that same size? is there a generally accepted method for doing object detection on very large images? i can envisage how i might chop the image into smaller tiles then feed each tile to my model, but if there's something out of the box that will do it for me, then that'd save some effort.",
        "Answer_original_content":"your understanding is correct. the endpoint resizes images based on the parameter image_shape. to answer your questions: as long as the scale of objects (i.e., expansion of pixels) in the resized images are similar between training and prediction data, the trained model should work. cropping is one option. another method is to train separate models for large and small images as david suggested.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"your understanding is correct. the endpoint resizes images based on the parameter . to answer your questions as long as the scale of objects in the resized images are similar between training and prediction data, the trained model should work. cropping is one option. another method is to train separate models for large and small images as david suggested."
    },
    {
        "Question_id":62657204.0,
        "Question_title":"How we can execute a Jupyter notebook python script automatically in Sagemaker?",
        "Question_body":"<p>I used terraform to create Sagemaker notebook instance and deploy Jupyter notebook python script to create and deploy a regression model.<\/p>\n<p>I was able to run the scribe and create the model successfully via AWS console manually. However, I could not find a way to get it executed automatically. I even tried executing the script via shell commands through notebook instance\u2019s lifecycle configuration. However, it did not work as expected.  Any other idea please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1593519191873,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":1732.0,
        "Owner_creation_time":1558199275883,
        "Owner_last_access_time":1663537572392,
        "Owner_reputation":409.0,
        "Owner_up_votes":7.0,
        "Owner_down_votes":0.0,
        "Owner_views":30.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Colombo, Sri Lanka",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62657204",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how we can execute a jupyter notebook python script automatically in ?; Content: i used terraform to create notebook instance and deploy jupyter notebook python script to create and deploy a regression model. i was able to run the scribe and create the model successfully via aws console manually. however, i could not find a way to get it executed automatically. i even tried executing the script via shell commands through notebook instance\u2019s lifecycle configuration. however, it did not work as expected. any other idea please?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in executing a Jupyter Notebook Python script automatically.",
        "Question_preprocessed_content":"Title: how we can execute a jupyter notebook python script automatically in ?; Content: i used terraform to create notebook instance and deploy jupyter notebook python script to create and deploy a regression model. i was able to run the scribe and create the model successfully via aws console manually. however, i could not find a way to get it executed automatically. i even tried executing the script via shell commands through notebook instances lifecycle configuration. however, it did not work as expected. any other idea please?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"AWS SageMaker - Extending Pre-built Container, Deploy Endpoint Failed. No such file or directory: 'serve'\"",
        "Question_body":"I am trying to deploy the SageMaker Inference Endpoint by extending the Pre-built image. However, it failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\"\n\nMy Dockerfile\n\nARG REGION=us-west-2\n\n# SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n\nRUN apt-get update\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, use the \/code subdirectory to store your user code.\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\n# Defines inference.py as script entrypoint \nENV SAGEMAKER_PROGRAM inference.py\n\n\nCloudWatch Log From \/aws\/sagemaker\/Endpoints\/mytestEndpoint\n\n2022-09-30T04:47:09.178-07:00\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module>\n    subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nTraceback (most recent call last): File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module> subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with Popen(*popenargs, **kwargs) as p: File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename)\n\n2022-09-30T04:47:13.409-07:00\nFileNotFoundError: [Errno 2] No such file or directory: 'serve'",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1664542013756,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":97.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUR-uTDaDsQBGjMoAUcsi2sQ\/aws-sage-maker-extending-pre-built-container-deploy-endpoint-failed-no-such-file-or-directory-serve",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-01T09:28:00.773Z",
                "Answer_score":1,
                "Answer_body":"Hi, @holopekochan!\n\nThe serve script is installed by SageMaker PyTorch Inference Toolkit when you pip-install it in the Dockerfile.\n\nHowever, it's hard to say why it's not found in your container. Are you sure that you use the inference container, not training container, for your endpoint? If you go to the AWS Console > Amazon SageMaker > Models > your model, what ECR image it shows in Container 1 - Image?\n\nIt will be useful if you can share the code that you used to setup the SageMaker PyTorch estimator (if any) how you define your PyTorchModel and how you deploy() it.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-30T15:14:58.846Z",
                "Answer_score":0,
                "Answer_body":"Should use the Sagemaker image\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-sagemaker\n\n\ninstead of ec2\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - extending pre-built container, deploy endpoint failed. no such file or directory: 'serve'\"; Content: i am trying to deploy the inference endpoint by extending the pre-built image. however, it failed with \"filenotfounderror: [errno 2] no such file or directory: 'serve'\" my dockerfile arg region=us-west-2 # pytorch image from 763104351884.dkr.ecr.$region.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2 run apt-get update env path=\"\/opt\/ml\/code:${path}\" # this environment variable is used by the pytorch container to determine our user code directory. env _submit_directory \/opt\/ml\/code # \/opt\/ml and all subdirectories are utilized by , use the \/code subdirectory to store your user code. copy inference.py \/opt\/ml\/code\/inference.py # defines inference.py as script entrypoint env _program inference.py cloudwatch log from \/aws\/\/endpoints\/mytestendpoint 2022-09-30t04:47:09.178-07:00 traceback (most recent call last): file \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) file \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) file \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with popen(*popenargs, **kwargs) as p: file \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, file \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename) traceback (most recent call last): file \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) file \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) file \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with popen(*popenargs, **kwargs) as p: file \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, file \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename) 2022-09-30t04:47:13.409-07:00 filenotfounderror: [errno 2] no such file or directory: 'serve'",
        "Question_original_content_gpt_summary":"The user encountered a challenge while attempting to deploy an inference endpoint by extending a pre-built image, resulting in a \"FileNotFoundError: [errno 2] no such file or directory: 'serve'\" error.",
        "Question_preprocessed_content":"Title: extending pre built container, deploy endpoint failed. no such file or directory 'serve'; Content: i am trying to deploy the inference endpoint by extending the pre built image. however, it failed with filenotfounderror no such file or directory 'serve' my dockerfile arg region us west pytorch image from run apt get update env this environment variable is used by the pytorch container to determine our user code directory. env and all subdirectories are utilized by , use the subdirectory to store your user code. copy defines as script entrypoint env cloudwatch log from traceback file line , in file line , in retcode call file line , in call with popen as p file line , in executable, file line , in raise traceback file line , in file line , in retcode call file line , in call with popen as p file line , in executable, file line , in raise filenotfounderror no such file or directory 'serve'",
        "Answer_original_content":"should use the image 763104351884.dkr.ecr.$region.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04- instead of ec2 763104351884.dkr.ecr.$region.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"should use the image instead of ec"
    },
    {
        "Question_id":56266967.0,
        "Question_title":"How to solve programming error when storing pandas data frame to snowflake",
        "Question_body":"<p>I'm trying to use SQLAlchemy to store a data frame I created in sagemaker to snowflake. The code only works with certain columns. When I add other columns it gives me an error even though they have the same data type. In the following example, if I only upload TA_ID it works, yet if I upload Cluster_ID, the code throws me an error. <\/p>\n\n<p>I checked SQLAlchemy website but didn't find much information on programming error. <\/p>\n\n<h2>SQL codes used to create table<\/h2>\n\n<pre><code>CREATE OR REPLACE TABLE test.m (\n    TA_ID string,\n     Cluster_ID string\n)\n<\/code><\/pre>\n\n<h2>Python code<\/h2>\n\n<pre><code>master2.to_sql(name='m', con=engine2, if_exists='append',  schema='test',index=False, index_label=None, chunksize=2000 )\n<\/code><\/pre>\n\n<p>ProgrammingError: <\/p>\n\n<pre><code>(snowflake.connector.errors.ProgrammingError) 000904 (42000): SQL compilation error: error line 1 at position 29\ninvalid identifier '\"Cluster_ID\"' [SQL: 'INSERT INTO test.m (\"TA_ID\", \"Cluster_ID\") VALUES (%(TA_ID)s, %(Cluster_ID)s)'] [parameters: ({'TA_ID': 'TA007', 'Cluster_ID': '0'}, {'TA_ID': 'TA007', 'Cluster_ID': '16'}, {'TA_ID': 'TA007', 'Cluster_ID': '40'}, {'TA_ID': 'TA007', 'Cluster_ID': '15'}, {'TA_ID': 'TA007', 'Cluster_ID': '29'}, {'TA_ID': 'TA007', 'Cluster_ID': '23'}, {'TA_ID': 'TA007', 'Cluster_ID': '9'}, {'TA_ID': 'TA007', 'Cluster_ID': '25'}, {'TA_ID': 'TA007', 'Cluster_ID': '42'}, {'TA_ID': 'TA007', 'Cluster_ID': '28'})] (Background on this error at: http:\/\/sqlalche.me\/e\/f405)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1558575022990,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":934.0,
        "Owner_creation_time":1422045322283,
        "Owner_last_access_time":1571758090928,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1558587243900,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56266967",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to solve programming error when storing pandas data frame to snowflake; Content: i'm trying to use sqlalchemy to store a data frame i created in to snowflake. the code only works with certain columns. when i add other columns it gives me an error even though they have the same data type. in the following example, if i only upload ta_id it works, yet if i upload cluster_id, the code throws me an error. i checked sqlalchemy website but didn't find much information on programming error. sql codes used to create table create or replace table test.m ( ta_id string, cluster_id string ) python code master2.to_sql(name='m', con=engine2, if_exists='append', schema='test',index=false, index_label=none, chunksize=2000 ) programmingerror: (snowflake.connector.errors.programmingerror) 000904 (42000): sql compilation error: error line 1 at position 29 invalid identifier '\"cluster_id\"' [sql: 'insert into test.m (\"ta_id\", \"cluster_id\") values (%(ta_id)s, %(cluster_id)s)'] [parameters: ({'ta_id': 'ta007', 'cluster_id': '0'}, {'ta_id': 'ta007', 'cluster_id': '16'}, {'ta_id': 'ta007', 'cluster_id': '40'}, {'ta_id': 'ta007', 'cluster_id': '15'}, {'ta_id': 'ta007', 'cluster_id': '29'}, {'ta_id': 'ta007', 'cluster_id': '23'}, {'ta_id': 'ta007', 'cluster_id': '9'}, {'ta_id': 'ta007', 'cluster_id': '25'}, {'ta_id': 'ta007', 'cluster_id': '42'}, {'ta_id': 'ta007', 'cluster_id': '28'})] (background on this error at: http:\/\/sqlalche.me\/e\/f405)",
        "Question_original_content_gpt_summary":"The user is encountering a programming error when attempting to store a pandas data frame to Snowflake using SQLAlchemy.",
        "Question_preprocessed_content":"Title: how to solve programming error when storing pandas data frame to snowflake; Content: i'm trying to use sqlalchemy to store a data frame i created in to snowflake. the code only works with certain columns. when i add other columns it gives me an error even though they have the same data type. in the following example, if i only upload it works, yet if i upload the code throws me an error. i checked sqlalchemy website but didn't find much on programming error. sql codes used to create table python code programmingerror",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Lock error with parallelized dvc repro",
        "Question_body":"<p>I have a pipeline with many parallel stages. I use a combination of a (generated) <code>params.yaml<\/code> file and <code>foreach<\/code> approach. See below for simplified examples of <code>dvc.yaml<\/code>, <code>params.yaml<\/code>, and <code>dvc.lock<\/code>.<\/p>\n<p>I wrote a simple scheduler in Python to run the stages with <code>dvc repro<\/code> in parallel using multiprocessing. Essentially, it\u2019s running running <code>dvc repro setup@sim1<\/code>, <code>dvc repro setup@sim2<\/code>, etc. in parallel.<\/p>\n<p>From what I <a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#parallel-stage-execution\" rel=\"noopener nofollow ugc\">understand<\/a> this should work. However, I\u2019m getting lock errors: \u201cERROR: Unable to acquire lock. Most likely another DVC process is running or was terminated abruptly.\u201d<\/p>\n<p>dvc.yaml:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  setup:\n    foreach: ${sims}\n    do:\n      cmd: python my_script.py --simlabel ${item.label}\n      deps:\n        - my_script.py\n      outs:\n        - ${item.work_dir}\/outputs.txt\n<\/code><\/pre>\n<p>params.yaml<\/p>\n<pre><code class=\"lang-auto\">sims:\n\tsim1:\n\t\tlabel: label1\n\t\tparameters:\n\t\t  param1: 1\n\t\t  param2: 2\n\t\twork_dir: label1_workdir\n\tsim2:\n\t\tlabel: label2\n\t\tparameters:\n\t\t  param1: 1\n\t\t  param2: 2\n\t\twork_dir: label2_workdir\n<\/code><\/pre>\n<p>dvc.lock:<\/p>\n<pre><code class=\"lang-auto\">schema: '2.0'\nstages:\n  setup@sim1\n    cmd: python python my_script.py --simlabel label1\n    deps:\n    - path: my_script.py\n      md5: 614d9f23b42a36de65981aa76026600e\n      size: 17401\n    outs:\n    - path: label1_workdir\/outputs.txt\n      md5: eaf7cdf7540bb96a6307394ad421ad7b\n      size: 4412\n  setup@sim2\n    cmd: python python my_script.py --simlabel label2\n    deps:\n    - path: my_script.py\n      md5: 614d9f23b42a36de65981aa76026600e\n      size: 17401\n    outs:\n    - path: label2_workdir\/outputs.txt\n      md5: eaf7cdf7540bb96a6307394ad421ad7b\n      size: 4412\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1634713211772,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":255.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/lock-error-with-parallelized-dvc-repro\/929",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-26T18:57:42.709Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/maartenb\">@maartenb<\/a> sorry for late response,<br>\nSo the problem here is that both stages (while different due to parameters use) use the same dependency:<\/p>\n<ul>\n<li>my_script.py<\/li>\n<\/ul>\n<p>dvc is locking it and hence you get an error. So this is a little bit different use case than the one linked by you in your post, where the pipelines are separate completely.<br>\nI am afraid there is no straightforward workaround for that, besides creating several copies of the script and naming them differently, though that would probably not be very elegant.<\/p>\n<p>Having said so, you might be interested in <code>exp<\/code> command, which should allow you to achieve your goal:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#exp-run\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon.ico\" class=\"site-icon\" width=\"64\" height=\"64\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#exp-run\" target=\"_blank\" rel=\"noopener\">dvc.org<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#exp-run\" target=\"_blank\" rel=\"noopener\">exp run<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>You could get parallel execution with the following commands:<\/p>\n<p><code>dvc exp run setup@sim1 --queue<\/code><br>\n<code>dvc exp run setup@sim2 --queue<\/code><br>\n<code>dvc exp run -j {2}<\/code><\/p>\n<p>Please read on <code>exp<\/code> and share your thoughts on whether this is something you might find useful.<\/p>",
                "Answer_score":160.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: lock error with parallelized repro; Content: i have a pipeline with many parallel stages. i use a combination of a (generated) params.yaml file and foreach approach. see below for simplified examples of .yaml, params.yaml, and .lock. i wrote a simple scheduler in python to run the stages with repro in parallel using multiprocessing. essentially, it\u2019s running running repro setup@sim1, repro setup@sim2, etc. in parallel. from what i understand this should work. however, i\u2019m getting lock errors: \u201cerror: unable to acquire lock. most likely another process is running or was terminated abruptly.\u201d .yaml: stages: setup: foreach: ${sims} do: cmd: python my_script.py --simlabel ${item.label} deps: - my_script.py outs: - ${item.work_dir}\/outputs.txt params.yaml sims: sim1: label: label1 parameters: param1: 1 param2: 2 work_dir: label1_workdir sim2: label: label2 parameters: param1: 1 param2: 2 work_dir: label2_workdir .lock: schema: '2.0' stages: setup@sim1 cmd: python python my_script.py --simlabel label1 deps: - path: my_script.py md5: 614d9f23b42a36de65981aa76026600e size: 17401 outs: - path: label1_workdir\/outputs.txt md5: eaf7cdf7540bb96a6307394ad421ad7b size: 4412 setup@sim2 cmd: python python my_script.py --simlabel label2 deps: - path: my_script.py md5: 614d9f23b42a36de65981aa76026600e size: 17401 outs: - path: label2_workdir\/outputs.txt md5: eaf7cdf7540bb96a6307394ad421ad7b size: 4412",
        "Question_original_content_gpt_summary":"The user is encountering lock errors when running a pipeline with many parallel stages using a combination of a generated params.yaml file and foreach approach.",
        "Question_preprocessed_content":"Title: lock error with parallelized repro; Content: i have a pipeline with many parallel stages. i use a combination of a file and approach. see below for simplified examples of , , and . i wrote a simple scheduler in python to run the stages with in parallel using multiprocessing. essentially, its running running , , etc. in parallel. from what i understand this should work. however, im getting lock errors error unable to acquire lock. most likely another process is running or was terminated abruptly.",
        "Answer_original_content":"hi @maartenb sorry for late response, so the problem here is that both stages (while different due to parameters use) use the same dependency: my_script.py is locking it and hence you get an error. so this is a little bit different use case than the one linked by you in your post, where the pipelines are separate completely. i am afraid there is no straightforward workaround for that, besides creating several copies of the script and naming them differently, though that would probably not be very elegant. having said so, you might be interested in exp command, which should allow you to achieve your goal: .org exp run open-source version control system for data science and machine learning projects. git-like experience to organize your data, models, and experiments. you could get parallel execution with the following commands: exp run setup@sim1 --queue exp run setup@sim2 --queue exp run -j {2} please read on exp and share your thoughts on whether this is something you might find useful.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi sorry for late response, so the problem here is that both stages use the same dependency is locking it and hence you get an error. so this is a little bit different use case than the one linked by you in your post, where the pipelines are separate completely. i am afraid there is no straightforward workaround for that, besides creating several copies of the script and naming them differently, though that would probably not be very elegant. having said so, you might be interested in command, which should allow you to achieve your goal exp run open source version control system for data science and machine learning projects. git like experience to organize your data, models, and experiments. you could get parallel execution with the following commands please read on and share your thoughts on whether this is something you might find useful."
    },
    {
        "Question_id":null,
        "Question_title":"Azure AutoML maximum columns supported",
        "Question_body":"Hi,\n\nIs there any documentation on the maximum columns supported by the AML both on the Portal and SDK?\n\nI tried to input a training sets with more than 60k columns and the process failed on the portal but still running on my notebooks.\n\nWonder is there any limits on the number of columns that we can put it?\n\nThanks!",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1638323249770,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/646730\/azure-automl-maximum-columns-supported.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure automl maximum columns supported; Content: hi, is there any documentation on the maximum columns supported by the aml both on the portal and sdk? i tried to input a training sets with more than 60k columns and the process failed on the portal but still running on my notebooks. wonder is there any limits on the number of columns that we can put it? thanks!",
        "Question_original_content_gpt_summary":"The user is trying to understand the maximum number of columns supported by Azure AutoML on the portal and SDK.",
        "Question_preprocessed_content":"Title: azure automl maximum columns supported; Content: hi, is there any documentation on the maximum columns supported by the aml both on the portal and sdk? i tried to input a training sets with more than k columns and the process failed on the portal but still running on my notebooks. wonder is there any limits on the number of columns that we can put it? thanks!",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":73406979.0,
        "Question_title":"How do I concurrently train TensorFlow models on SageMaker with multiprocessing?",
        "Question_body":"<p>I've tried a lot of approaches to training models concurrently with the multiprocessing module in a SageMaker instance.<\/p>\n<p>Either multiple processes spawn, and they crash when they hit the train() call, or the processes run sequentially (sometimes crashing after 1 process runs its course).<\/p>\n<p>I've run a few examples of multiprocessing that generate random arrays to make sure multiprocessing is working in general. This leads me to believe there's an issue either in the way I'm handling data or tensorflow in conjunction with the async functionality.<\/p>\n<p>I stripped everything down to mimic the code <a href=\"https:\/\/stackoverflow.com\/questions\/16631334\/train-two-models-concurrently\">here<\/a>, but my print statements following the async map\/before the wait do not print:<\/p>\n<pre><code>def trainNetworks(numProcesses=1):\n    if __name__ == '__main__':\n        batches = []\n        netInCt = len(master_in.network_inputs)\n        batchSize = math.ceil(netInCt \/ numProcesses)\n        for i in range(numProcesses):\n            batchStartIdx = (i * batchSize) # inclusive idx\n            batchEndIdx = ((i + 1) * batchSize) if (((i + 1) * batchSize) &lt; netInCt) else netInCt # exclusive idx\n            netBatch = copy.deepcopy(master_in.network_inputs[batchStartIdx:batchEndIdx])\n            batches.append(netBatch)\n\n        po = multiprocessing.Pool(numProcesses) \n        batchOut = po.map_async(trainNetBatch, batches)#.get()\n        # get will start the processes and execute them\n        print('abc')\n        po.wait()\n        print('zyx')\n        po.terminate()\n\n        # pickle master output\n        print('All done, pickling')\n\n    return 0\n\ndef trainNetBatch(netBatch):\n    for netIn in netBatch:\n        print('!!!starting ', batchStartIdx)\n        x = np.array(netIn.table)\n        y = np.array(netIn.labels)\n\n        from sklearn.model_selection import train_test_split\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n        ann = getNetModel()\n        print('!!!fitting ', batchStartIdx)\n        # train network\n        ann.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 0)\n        print('!donefit ', batchStartIdx)\n<\/code><\/pre>\n<p>The instance CPU usage and memory never exceed ~30%, but the execution hangs.<\/p>\n<p>In a few cases, I've had training execute for a few models, then a syntax error from BEFORE the fit() call halts execution. How would the code get past the error to call fit in the first place?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1660842481980,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":35.0,
        "Owner_creation_time":1450254972380,
        "Owner_last_access_time":1663949903423,
        "Owner_reputation":171.0,
        "Owner_up_votes":15.0,
        "Owner_down_votes":0.0,
        "Owner_views":11.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73406979",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i concurrently train tensorflow models on with multiprocessing?; Content: i've tried a lot of approaches to training models concurrently with the multiprocessing module in a instance. either multiple processes spawn, and they crash when they hit the train() call, or the processes run sequentially (sometimes crashing after 1 process runs its course). i've run a few examples of multiprocessing that generate random arrays to make sure multiprocessing is working in general. this leads me to believe there's an issue either in the way i'm handling data or tensorflow in conjunction with the async functionality. i stripped everything down to mimic the code here, but my print statements following the async map\/before the wait do not print: def trainnetworks(numprocesses=1): if __name__ == '__main__': batches = [] netinct = len(master_in.network_inputs) batchsize = math.ceil(netinct \/ numprocesses) for i in range(numprocesses): batchstartidx = (i * batchsize) # inclusive idx batchendidx = ((i + 1) * batchsize) if (((i + 1) * batchsize) < netinct) else netinct # exclusive idx netbatch = copy.deepcopy(master_in.network_inputs[batchstartidx:batchendidx]) batches.append(netbatch) po = multiprocessing.pool(numprocesses) batchout = po.map_async(trainnetbatch, batches)#.get() # get will start the processes and execute them print('abc') po.wait() print('zyx') po.terminate() # pickle master output print('all done, pickling') return 0 def trainnetbatch(netbatch): for netin in netbatch: print('!!!starting ', batchstartidx) x = np.array(netin.table) y = np.array(netin.labels) from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0) ann = getnetmodel() print('!!!fitting ', batchstartidx) # train network ann.fit(x_train, y_train, batch_size = 32, epochs = 100, verbose = 0) print('!donefit ', batchstartidx) the instance cpu usage and memory never exceed ~30%, but the execution hangs. in a few cases, i've had training execute for a few models, then a syntax error from before the fit() call halts execution. how would the code get past the error to call fit in the first place?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with concurrently training tensorflow models using the multiprocessing module, resulting in either multiple processes spawning and crashing when they hit the train() call, or the processes running sequentially and sometimes crashing after one process runs its course.",
        "Question_preprocessed_content":"Title: how do i concurrently train tensorflow models on with multiprocessing?; Content: i've tried a lot of approaches to training models concurrently with the multiprocessing module in a instance. either multiple processes spawn, and they crash when they hit the train call, or the processes run sequentially . i've run a few examples of multiprocessing that generate random arrays to make sure multiprocessing is working in general. this leads me to believe there's an issue either in the way i'm handling data or tensorflow in conjunction with the async functionality. i stripped everything down to mimic the code here, but my print statements following the async the wait do not print the instance cpu usage and memory never exceed %, but the execution hangs. in a few cases, i've had training execute for a few models, then a syntax error from before the fit call halts execution. how would the code get past the error to call fit in the first place?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":58965946.0,
        "Question_title":"Native logging module not printing to stdout in AzureML",
        "Question_body":"<p>I am trying to use the standard <code>logging<\/code> module in my AzureML run but nothing gets printed out to stdout.<\/p>\n\n<p>I tried following instructions from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-enable-logging#python-native-logging-settings\" rel=\"nofollow noreferrer\">here<\/a> but I have not been successful.<\/p>\n\n<p>This is what I am running:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import logging\n\nlogging.basicConfig(level=logging.DEBUG)\n\nlogger = logging.getLogger(__name__)\n\nlogger.info('WHY IS THIS NOT WORKING?')\n<\/code><\/pre>\n\n<p>Am I doing something wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1574300061780,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":526.0,
        "Owner_creation_time":1410706091816,
        "Owner_last_access_time":1646159427267,
        "Owner_reputation":300.0,
        "Owner_up_votes":38.0,
        "Owner_down_votes":1.0,
        "Owner_views":30.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Brazil",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58965946",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: native logging module not printing to stdout in ; Content: i am trying to use the standard logging module in my run but nothing gets printed out to stdout. i tried following instructions from here but i have not been successful. this is what i am running: import logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) logger.info('why is this not working?') am i doing something wrong?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with the native logging module not printing to stdout.",
        "Question_preprocessed_content":"Title: native logging module not printing to stdout in; Content: i am trying to use the standard module in my run but nothing gets printed out to stdout. i tried following instructions from here but i have not been successful. this is what i am running am i doing something wrong?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":72541583.0,
        "Question_title":"aws auto-stop-idle does not detect papermill",
        "Question_body":"<p>I am using papermill to parametrize jupyter notebook deployed on AWS Sagemaker. I also used this <a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/tree\/master\/scripts\/auto-stop-idle\" rel=\"nofollow noreferrer\">lifestyle configuration<\/a> that will auto shutdown if there are no running\/idle notebooks. Unfortunately, it does not detect the Papermill process and continues to shutdown after reaching the specified idle time. What do I need to do to keep Sagemaker alive until the completion of Papermill<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1654672910050,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":29.0,
        "Owner_creation_time":1418975692670,
        "Owner_last_access_time":1663836167216,
        "Owner_reputation":594.0,
        "Owner_up_votes":464.0,
        "Owner_down_votes":8.0,
        "Owner_views":181.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Philippines",
        "Question_last_edit_time":1654673776512,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72541583",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: aws auto-stop-idle does not detect papermill; Content: i am using papermill to parametrize jupyter notebook deployed on . i also used this lifestyle configuration that will auto shutdown if there are no running\/idle notebooks. unfortunately, it does not detect the papermill process and continues to shutdown after reaching the specified idle time. what do i need to do to keep alive until the completion of papermill",
        "Question_original_content_gpt_summary":"The user is encountering an issue with AWS auto-stop-idle not detecting their Papermill process, causing their Jupyter Notebook to shut down prematurely.",
        "Question_preprocessed_content":"Title: aws auto stop idle does not detect papermill; Content: i am using papermill to parametrize jupyter notebook deployed on . i also used this lifestyle configuration that will auto shutdown if there are no notebooks. unfortunately, it does not detect the papermill process and continues to shutdown after reaching the specified idle time. what do i need to do to keep alive until the completion of papermill",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML MSI deployment over ARM Templates enables purge protection on Key Vault",
        "Question_body":"I have discovered lately that when you deploy an Azure ML instance from the ARM Template, the MSI will override the purge protection settings of the Key Vault. It will enable purge protection on the Key Vault. This is not the behavior that I am looking for, because when trying to deploy it again, the template will fail saying that the Key Vault with the name already exists and you can't deleted before the retention period. This was my conclusion after doing several tests. Is my assumption correct?\n\nIf you deploy the Azure ML instance manually and select the Key Vault, it will keep the disable purge settings. Any ideas how can we keep purge disabled hier?\n\nThe Azure ML properties that we used are mentioned bellow:\n\n   {\n     \"type\": \"Microsoft.MachineLearningServices\/workspaces\",\n     \"apiVersion\": \"2020-09-01-preview\",\n     \"name\": \"[variables('machineLearningWorkspaceName')]\",\n     \"location\": \"[parameters('location')]\",\n     \"identity\": {\n       \"type\": \"[parameters('amlManagedIdentityOption')]\"\n     },\n     \"dependsOn\": [\n       \"[resourceId('Microsoft.Storage\/storageAccounts', variables('storageAccountName'))]\",\n       \"[resourceId('Microsoft.Insights\/components', variables('applicationInsightsName'))]\",\n       \"[resourceId('Microsoft.ContainerRegistry\/registries', variables('containerRegistryName'))]\"\n     ],\n     \"tags\": \"[parameters('resourceTags')]\",\n     \"properties\": {\n       \"friendlyName\": \"[variables('machineLearningWorkspaceName')]\",\n       \"storageAccount\": \"[variables('storageAccount')]\",\n       **\"keyVault\": \"[variables('keyVault')]\",**\n       \"applicationInsights\": \"[variables('applicationInsights')]\",\n       \"containerRegistry\": \"[ variables('containerRegistry')]\",\n       \"adbWorkspace\": \"[variables('adbWorkSpace')]\",\n       \"hbiWorkspace\": \"[parameters('confidential_data')]\",\n       \"allowPublicAccessWhenBehindVnet\": \"[parameters('allowPublicAccessWhenBehindVnet')]\"\n     }\n   }\n\n\n\n\n\nOn the Key Vault created also via ARM we have:\n\n         \"properties\": {\n                 \"enabledForDeployment\": \"[parameters('enabledForDeployment')]\",\n                 \"enabledForTemplateDeployment\": \"[parameters('enabledForTemplateDeployment')]\",\n                 \"enabledForVolumeEncryption\": \"[parameters('enableVaultForVolumeEncryption')]\",\n                 \"softDeleteRetentionInDays\": 7,\n                 \"tenantId\": \"[subscription().tenantId]\",\n                 \"copy\": [\n                     {\n                         \"name\": \"accessPolicies\",\n                         \"count\": \"[length(parameters('userObjectId'))]\",\n                         \"input\": {\n                             \"tenantId\": \"[subscription().tenantId]\",\n                             \"objectId\": \"[parameters('userObjectId')[copyIndex('accessPolicies')].Id]\",\n                             \"permissions\": \"[parameters('userObjectId')[copyIndex('accessPolicies')].Permissions]\"\n                         }\n }",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1627468001263,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/492841\/azure-ml-msi-deployment-over-arm-templates-enables.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-29T19:22:12.813Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for your feedback. Soft delete is enabled by default on new key vaults that are created with a new workspace (without bringing existing key vault).",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: msi deployment over arm templates enables purge protection on key vault; Content: i have discovered lately that when you deploy an instance from the arm template, the msi will override the purge protection settings of the key vault. it will enable purge protection on the key vault. this is not the behavior that i am looking for, because when trying to deploy it again, the template will fail saying that the key vault with the name already exists and you can't deleted before the retention period. if you deploy the instance manually and select the key vault, it will keep the disable purge settings. any ideas how can we keep purge disabled hier? the properties that we used are mentioned bellow: { \"type\": \"microsoft.machinelearningservices\/workspaces\", \"apiversion\": \"2020-09-01-preview\", \"name\": \"[variables('machinelearningworkspacename')]\", \"location\": \"[parameters('location')]\", \"identity\": { \"type\": \"[parameters('amlmanagedidentityoption')]\" }, \"dependson\": [ \"[resourceid('microsoft.storage\/storageaccounts', variables('storageaccountname'))]\", \"[resourceid('microsoft.insights\/components', variables('applicationinsightsname'))]\", \"[resourceid('microsoft.containerregistry\/registries', variables('containerregistryname'))]\" ], \"tags\": \"[parameters('resourcetags')]\", \"properties\": { \"friendlyname\": \"[variables('machinelearningworkspacename')]\", \"storageaccount\": \"[variables('storageaccount')]\", \"keyvault\": \"[variables('keyvault')]\", \"applicationinsights\": \"[variables('applicationinsights')]\", \"containerregistry\": \"[ variables('containerregistry')]\", \"adbworkspace\": \"[variables('adbworkspace')]\", \"hbiworkspace\": \"[parameters('confidential_data')]\", \"allowpublicaccesswhenbehindvnet\": \"[parameters('allowpublicaccesswhenbehindvnet')]\" } } on the key vault arm we have the following properties: \"properties\": { \"enabledfordeployment\": \"[parameters('enabledfordeployment')]\", \"enabledfortemplatedeployment\": \"[parameters('enabledfortemplatedeployment')]\", \"enabledforvolumeencryption\": \"[parameters('enablevaultforvolumeencryption')]\", \"softdeleteretentionindays\": 7, \"tenantid\": \"[subscription().tenantid]\", \"copy\": [ { \"name\": \"accesspolicies\", \"count\": \"[length(parameters('userobjectid'))]\", \"input\": { \"tenantid\": \"[subscription().tenantid]\", \"objectid\": \"[parameters('userobjectid')[copyindex('accesspolicies')].id]\", \"permissions\": \"[parameters('userobjectid')[copyindex('accesspolicies')].permissions]\" } }",
        "Question_original_content_gpt_summary":"The user encountered a challenge where deploying an instance from an ARM template would override the purge protection settings of the key vault, enabling purge protection on the key vault and causing the template to fail when trying to deploy again.",
        "Question_preprocessed_content":"Title: msi deployment over arm templates enables purge protection on key vault; Content: i have discovered lately that when you deploy an instance from the , the will override the purge protection settings of the . it will enable purge protection on the . this is not the behavior that i am looking for, because when trying to deploy it again, the template will fail saying that the with the name already exists and you can't deleted before the retention period. if you deploy the instance manually and select the key vault, it will keep the disable purge settings. any ideas how can we keep purge disabled hier? the properties that we used are mentioned bellow on the key vault arm we have the following properties",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":64006314.0,
        "Question_title":"PartitionedDataSet not found when Kedro pipeline is run in Docker",
        "Question_body":"<p>I have multiple text files in an S3 bucket which I read and process. So, I defined PartitionedDataSet in Kedro datacatalog which looks like this:<\/p>\n<pre><code>raw_data:\n  type: PartitionedDataSet\n  path: s3:\/\/reads\/raw\n  dataset: pandas.CSVDataSet\n  load_args:\n    sep: &quot;\\t&quot;\n    comment: &quot;#&quot;\n<\/code><\/pre>\n<p>In addition, I implemented this <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\/403\" rel=\"nofollow noreferrer\">solution<\/a> to get all secrets from credentials file via environment variables including AWS secret keys.<\/p>\n<p>When I run things locally using <code>kedro run<\/code> everything works just fine, but when I build Docker image (using <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro-docker\" rel=\"nofollow noreferrer\">kedro-docker<\/a>) and run pipeline in Docker environement  with <code>kedro docker run<\/code> and by providing all enviornement variables using <code>--docker-args<\/code> option I get the following error:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/local\/bin\/kedro&quot;, line 8, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/framework\/cli\/cli.py&quot;, line 724, in main\n    cli_collection()\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 829, in __call__\n    return self.main(*args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 782, in main\n    rv = self.invoke(ctx)\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py&quot;, line 610, in invoke\n    return callback(*args, **kwargs)\n  File &quot;\/home\/kedro\/kedro_cli.py&quot;, line 230, in run\n    pipeline_name=pipeline,\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py&quot;, line 767, in run\n    raise exc\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/framework\/context\/context.py&quot;, line 759, in run\n    run_result = runner.run(filtered_pipeline, catalog, run_id)\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py&quot;, line 101, in run\n    self._run(pipeline, catalog, run_id)\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/runner\/sequential_runner.py&quot;, line 90, in _run\n    run_node(node, catalog, self._is_async, run_id)\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py&quot;, line 213, in run_node\n    node = _run_node_sequential(node, catalog, run_id)\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py&quot;, line 221, in _run_node_sequential\n    inputs = {name: catalog.load(name) for name in node.inputs}\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/runner\/runner.py&quot;, line 221, in &lt;dictcomp&gt;\n    inputs = {name: catalog.load(name) for name in node.inputs}\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/io\/data_catalog.py&quot;, line 392, in load\n    result = func()\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/io\/core.py&quot;, line 213, in load\n    return self._load()\n  File &quot;\/usr\/local\/lib\/python3.7\/site-packages\/kedro\/io\/partitioned_data_set.py&quot;, line 240, in _load\n    raise DataSetError(&quot;No partitions found in `{}`&quot;.format(self._path))\nkedro.io.core.DataSetError: No partitions found in `s3:\/\/reads\/raw`\n<\/code><\/pre>\n<p>Note: Pipeline works just fine in Docker environment, if I move files to some local directory, define PartitionedDataSet and build Docker image and provide environment variables through <code>--docker-args<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7.0,
        "Question_creation_time":1600764983890,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":513.0,
        "Owner_creation_time":1360504316372,
        "Owner_last_access_time":1657283263380,
        "Owner_reputation":86.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1600782027867,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64006314",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: partitioneddataset not found when pipeline is run in docker; Content: i have multiple text files in an s3 bucket which i read and process. so, i defined partitioneddataset in datacatalog which looks like this: raw_data: type: partitioneddataset path: s3:\/\/reads\/raw dataset: pandas.csvdataset load_args: sep: \"\\t\" comment: \"#\" in addition, i implemented this solution to get all secrets from credentials file via environment variables including aws secret keys. when i run things locally using run everything works just fine, but when i build docker image (using -docker) and run pipeline in docker environement with docker run and by providing all enviornement variables using --docker-args option i get the following error: traceback (most recent call last): file \"\/usr\/local\/bin\/\", line 8, in <module> sys.exit(main()) file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/framework\/cli\/cli.py\", line 724, in main cli_collection() file \"\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py\", line 829, in __call__ return self.main(*args, **kwargs) file \"\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py\", line 782, in main rv = self.invoke(ctx) file \"\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py\", line 1259, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) file \"\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py\", line 1066, in invoke return ctx.invoke(self.callback, **ctx.params) file \"\/usr\/local\/lib\/python3.7\/site-packages\/click\/core.py\", line 610, in invoke return callback(*args, **kwargs) file \"\/home\/\/_cli.py\", line 230, in run pipeline_name=pipeline, file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/framework\/context\/context.py\", line 767, in run raise exc file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/framework\/context\/context.py\", line 759, in run run_result = runner.run(filtered_pipeline, catalog, run_id) file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/runner\/runner.py\", line 101, in run self._run(pipeline, catalog, run_id) file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/runner\/sequential_runner.py\", line 90, in _run run_node(node, catalog, self._is_async, run_id) file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/runner\/runner.py\", line 213, in run_node node = _run_node_sequential(node, catalog, run_id) file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/runner\/runner.py\", line 221, in _run_node_sequential inputs = {name: catalog.load(name) for name in node.inputs} file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/runner\/runner.py\", line 221, in <dictcomp> inputs = {name: catalog.load(name) for name in node.inputs} file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/io\/data_catalog.py\", line 392, in load result = func() file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/io\/core.py\", line 213, in load return self._load() file \"\/usr\/local\/lib\/python3.7\/site-packages\/\/io\/partitioned_data_set.py\", line 240, in _load raise dataseterror(\"no partitions found in `{}`\".format(self._path)) .io.core.dataseterror: no partitions found in `s3:\/\/reads\/raw` note: pipeline works just fine in docker environment, if i move files to some local directory, define partitioneddataset and build docker image and provide environment variables through --docker-args",
        "Question_original_content_gpt_summary":"The user is encountering an issue where the partitioneddataset is not found when the pipeline is run in a docker environment, despite the pipeline working fine when run locally.",
        "Question_preprocessed_content":"Title: partitioneddataset not found when pipeline is run in docker; Content: i have multiple text files in an s bucket which i read and process. so, i defined partitioneddataset in datacatalog which looks like this in addition, i implemented this solution to get all secrets from credentials file via environment variables including aws secret keys. when i run things locally using everything works just fine, but when i build docker image and run pipeline in docker environement with and by providing all enviornement variables using option i get the following error note pipeline works just fine in docker environment, if i move files to some local directory, define partitioneddataset and build docker image and provide environment variables through",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":60423734.0,
        "Question_title":"SageMaker XGBoost hyperparameter tuning versus XGBoost python package",
        "Question_body":"<p>I am trying to do hyperparameter tuning of xgboost model. I started with AWS Sagemaker Hyperparameter Tuning, with the following parameter range:<\/p>\n\n<pre><code>xgb.set_hyperparameters(eval_metric='auc',\n                        objective='binary:logistic',\n                        early_stopping_rounds=500,\n                        rate_drop=0.1,\n                        colsample_bytree=0.8,\n                        subsample=0.75,\n                        min_child_weight=0)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0.01, 0.3),\n                         'lambda': ContinuousParameter(0.1, 2),\n                         'alpha': ContinuousParameter(0.5, 2),\n                         'max_depth': IntegerParameter(5, 10),\n                         'num_round': IntegerParameter(500, 2000)}\n\nobjective_metric_name = 'validation:auc'\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            max_jobs=10,  \n                            max_parallel_jobs=3,\n                            tags=[{'Key': 'Application', 'Value': 'cxxx'}])\n<\/code><\/pre>\n\n<p>And get a best model with the following set of hyperparameters:<\/p>\n\n<pre><code>{\n  \"alpha\": \"1.4009334471163981\",\n  \"eta\": \"0.05726016655019904\",\n  \"lambda\": \"1.2070623852474922\",\n  \"max_depth\": \"7\",\n  \"num_round\": \"1052\"\n}\n<\/code><\/pre>\n\n<p>Out of curiosity, I hooked up these hyperparameters into xgboost python package, as such:<\/p>\n\n<pre><code>xgb_model = xgb.XGBClassifier(max_depth = 7,\n                          silent = False,\n                          random_state = 42,\n                          n_estimators = 1052,\n                          learning_rate = 0.05726016655019904,\n                          objective = 'binary:logistic',\n                          verbosity = 1,\n                          reg_alpha = 1.4009334471163981,\n                          reg_lambda = 1.2070623852474922,\n                          rate_drop=0.1,\n                          colsample_bytree=0.8,\n                          subsample=0.75,\n                          min_child_weight=0\n                        )\n\n<\/code><\/pre>\n\n<p>I retrained the model and realized the results I got from the latter is better than that from SageMaker.\nxgboost (auc of validation set): 0.766\nSageMaker best model (auc of validation set):0.751<\/p>\n\n<p>I wonder why SageMaker perform so poorly? If SageMaker usually perform worse than xgboost python package, how do people usually do xgboost hyperparameter tuning? Thanks for any hints!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1582757058690,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1300.0,
        "Owner_creation_time":1401031858507,
        "Owner_last_access_time":1658704433412,
        "Owner_reputation":1021.0,
        "Owner_up_votes":92.0,
        "Owner_down_votes":0.0,
        "Owner_views":120.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Seattle, WA, United States",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60423734",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: xgboost hyperparameter tuning versus xgboost python package; Content: i am trying to do hyperparameter tuning of xgboost model. i started with hyperparameter tuning, with the following parameter range: xgb.set_hyperparameters(eval_metric='auc', objective='binary:logistic', early_stopping_rounds=500, rate_drop=0.1, colsample_bytree=0.8, subsample=0.75, min_child_weight=0) hyperparameter_ranges = {'eta': continuousparameter(0.01, 0.3), 'lambda': continuousparameter(0.1, 2), 'alpha': continuousparameter(0.5, 2), 'max_depth': integerparameter(5, 10), 'num_round': integerparameter(500, 2000)} objective_metric_name = 'validation:auc' tuner = hyperparametertuner(xgb, objective_metric_name, hyperparameter_ranges, max_jobs=10, max_parallel_jobs=3, tags=[{'key': 'application', 'value': 'cxxx'}]) and get a best model with the following set of hyperparameters: { \"alpha\": \"1.4009334471163981\", \"eta\": \"0.05726016655019904\", \"lambda\": \"1.2070623852474922\", \"max_depth\": \"7\", \"num_round\": \"1052\" } out of curiosity, i hooked up these hyperparameters into xgboost python package, as such: xgb_model = xgb.xgbclassifier(max_depth = 7, silent = false, random_state = 42, n_estimators = 1052, learning_rate = 0.05726016655019904, objective = 'binary:logistic', verbosity = 1, reg_alpha = 1.4009334471163981, reg_lambda = 1.2070623852474922, rate_drop=0.1, colsample_bytree=0.8, subsample=0.75, min_child_weight=0 ) i retrained the model and realized the results i got from the latter is better than that from . xgboost (auc of validation set): 0.766 best model (auc of validation set):0.751 i wonder why perform so poorly? if usually perform worse than xgboost python package, how do people usually do xgboost hyperparameter tuning? thanks for any hints!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with xgboost hyperparameter tuning, as the results from the xgboost python package are better than those from the hyperparameter tuning process.",
        "Question_preprocessed_content":"Title: xgboost hyperparameter tuning versus xgboost python package; Content: i am trying to do hyperparameter tuning of xgboost model. i started with hyperparameter tuning, with the following parameter range and get a best model with the following set of hyperparameters out of curiosity, i hooked up these hyperparameters into xgboost python package, as such i retrained the model and realized the results i got from the latter is better than that from . xgboost best model i wonder why perform so poorly? if usually perform worse than xgboost python package, how do people usually do xgboost hyperparameter tuning? thanks for any hints!",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":65995020.0,
        "Question_title":"Sagemaker script processor",
        "Question_body":"<blockquote>\n<pre><code>script_processor = ScriptProcessor(\n    base_job_name=job_name,\n    image_uri=processing_repository_uri,\n    role=role,\n    command=[&quot;python3&quot;],\n    instance_count=instance_count,\n    instance_type=instance_type,\n    max_runtime_in_seconds=MAX_RUN_TIM)\n<\/code><\/pre>\n<\/blockquote>\n<pre><code>script_processor.run(\n    code=&quot;src\/extract_data.py&quot;,\n    outputs=[\n        ProcessingOutput(source=path, destination=destination),\n    ])\n<\/code><\/pre>\n<p>Is there a possibility to specify <code>source_dir<\/code> when using ScriptProcessor. I tried to figure it out but I could not find anything. Is it designed to run independent scripts? if it is the case what are the alternative for doing it with sagemaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1612191837503,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":320.0,
        "Owner_creation_time":1437484646180,
        "Owner_last_access_time":1663852338152,
        "Owner_reputation":71.0,
        "Owner_up_votes":17.0,
        "Owner_down_votes":0.0,
        "Owner_views":11.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1612198361483,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65995020",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: script processor; Content: script_processor = scriptprocessor( base_job_name=job_name, image_uri=processing_repository_uri, role=role, command=[\"python3\"], instance_count=instance_count, instance_type=instance_type, max_runtime_in_seconds=max_run_tim) script_processor.run( code=\"src\/extract_data.py\", outputs=[ processingoutput(source=path, destination=destination), ]) is there a possibility to specify source_dir when using scriptprocessor. i tried to figure it out but i could not find anything. is it designed to run independent scripts? if it is the case what are the alternative for doing it with .",
        "Question_original_content_gpt_summary":"The user is encountering challenges with using the scriptprocessor to specify a source_dir and is looking for alternatives to running independent scripts.",
        "Question_preprocessed_content":"Title: script processor; Content: is there a possibility to specify when using scriptprocessor. i tried to figure it out but i could not find anything. is it designed to run independent scripts? if it is the case what are the alternative for doing it with .",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":57152695.0,
        "Question_title":"Unable to run mlflow ui in Jupyter",
        "Question_body":"<p>I am new to MLflow. I was trying to use it in Jupyter. As part of the quickstart, I ran the following code:<\/p>\n\n<pre><code>import os\nfrom mlflow import log_metric, log_param, log_artifact\n\nif __name__ == \"__main__\":\n    # Log a parameter (key-value pair)\n    log_param(\"param1\", 5)\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(\"foo\", 1)\n    log_metric(\"foo\", 2)\n    log_metric(\"foo\", 3)\n\n    # Log an artifact (output file)\n    with open(\"output.txt\", \"w\") as f:\n        f.write(\"Hello world!\")\n    log_artifact(\"output.txt\")\n<\/code><\/pre>\n\n<p>which ran without any problems. However when I then typed in mlflow ui, I got the error: invalid syntax. What could I be doing wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1563824253953,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":5356.0,
        "Owner_creation_time":1484581680952,
        "Owner_last_access_time":1661401894580,
        "Owner_reputation":148.0,
        "Owner_up_votes":9.0,
        "Owner_down_votes":0.0,
        "Owner_views":39.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1563824393660,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57152695",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unable to run ui in jupyter; Content: i am new to . i was trying to use it in jupyter. as part of the quickstart, i ran the following code: import os from import log_metric, log_param, log_artifact if __name__ == \"__main__\": # log a parameter (key-value pair) log_param(\"param1\", 5) # log a metric; metrics can be updated throughout the run log_metric(\"foo\", 1) log_metric(\"foo\", 2) log_metric(\"foo\", 3) # log an artifact (output file) with open(\"output.txt\", \"w\") as f: f.write(\"hello world!\") log_artifact(\"output.txt\") which ran without any problems. however when i then typed in ui, i got the error: invalid syntax. what could i be doing wrong?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge running the  UI in Jupyter, resulting in an \"invalid syntax\" error.",
        "Question_preprocessed_content":"Title: unable to run ui in jupyter; Content: i am new to . i was trying to use it in jupyter. as part of the quickstart, i ran the following code which ran without any problems. however when i then typed in ui, i got the error invalid syntax. what could i be doing wrong?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Span filtering prodigy datasets using W&B Tables",
        "Question_body":"<p>Hi! I am currently testing the new prodigy integration to visualize NER datasets and it works great, thank you! But I have some problems.<br>\nI would like to filter  <code>row[\"spans\"][\"label\"] <\/code> by the special entities and I have problems here. My use case:<\/p>\n<ul>\n<li>Which texts contain only people (PERSON)? Which organisations (ORG) or locations (LOCATION)? How many people do I find in total?<\/li>\n<li>I would like to create new rows\/columns to visualize the results.<\/li>\n<\/ul>\n<p>My problem: I can\u2019t manage to filter according to the entities. What am I doing wrong?<br>\nYour example here is also ok for the test:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/kshen\/prodigy\/reports\/Visualizing-Prodigy-Datasets-Using-W-B-Tables--Vmlldzo5NDE2MTc\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7a7a7077833cb4ec4be6e63ad7c2db322d3e15a6.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/kshen\/prodigy\/reports\/Visualizing-Prodigy-Datasets-Using-W-B-Tables--Vmlldzo5NDE2MTc\" target=\"_blank\" rel=\"noopener\" title=\"02:05PM - 17 August 2021\">W&amp;B \u2013 17 Aug 21<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6f5cc42f5be5838eaa024b6166848b71312afb4e.png\" class=\"thumbnail onebox-avatar\" width=\"300\" height=\"300\">\n\n<h3><a href=\"https:\/\/wandb.ai\/kshen\/prodigy\/reports\/Visualizing-Prodigy-Datasets-Using-W-B-Tables--Vmlldzo5NDE2MTc\" target=\"_blank\" rel=\"noopener\">Visualizing Prodigy Datasets Using W&amp;B Tables<\/a><\/h3>\n\n  <p>Use the W&amp;B\/Prodigy integration to upload your Prodigy annotated datasets to W&amp;B for easier visualization. Made by Kevin Shen using Weights &amp; Biases<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0.png\" data-download-href=\"\/uploads\/short-url\/quowLbwH5KL7ZM97JjbPxcjnfNe.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_517x186.png\" alt=\"image\" data-base62-sha1=\"quowLbwH5KL7ZM97JjbPxcjnfNe\" width=\"517\" height=\"186\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_517x186.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_775x279.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_1034x372.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1762\u00d7637 64 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Thank you<br>\nAlfred<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1631795821780,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":290.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/span-filtering-prodigy-datasets-using-w-b-tables\/646",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-18T20:59:51.751Z",
                "Answer_body":"<p>Glad you\u2019re enjoying the prodigy integration!<\/p>\n<p>I\u2019ll try get an answer for this for you ASAP.<\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T10:20:07.557Z",
                "Answer_body":"<p>I don\u2019t know what I did wrong last time, but it works perfectly. Thanks!<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/65110a2cb18407338e0a0ea30428977c2e81e590.png\" alt=\"image\" data-base62-sha1=\"eq4JnNYDJ8fJExgVYAeDukhCpjO\" width=\"447\" height=\"320\"><\/p>",
                "Answer_score":66.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T11:30:29.718Z",
                "Answer_body":"<p>Nice! Glad you got it working <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: span filtering prodigy datasets using w&b tables; Content: hi! i am currently testing the new prodigy integration to visualize ner datasets and it works great, thank you! but i have some problems. i would like to filter row[\"spans\"][\"label\"] by the special entities and i have problems here. my use case: which texts contain only people (person)? which organisations (org) or locations (location)? how many people do i find in total? i would like to create new rows\/columns to visualize the results. my problem: i can\u2019t manage to filter according to the entities. what am i doing wrong? your example here is also ok for the test: w&b \u2013 17 aug 21 visualizing prodigy datasets using w&b tables use the w&b\/prodigy integration to upload your prodigy annotated datasets to w&b for easier visualization. made by kevin shen using weights & biases image1762\u00d7637 64 kb thank you alfred",
        "Question_original_content_gpt_summary":"The user is encountering challenges with filtering Prodigy datasets using W&B tables to visualize NER datasets and create new rows\/columns to filter according to entities.",
        "Question_preprocessed_content":"Title: span filtering prodigy datasets using w&b tables; Content: hi! i am currently testing the new prodigy integration to visualize ner datasets and it works great, thank you! but i have some problems. i would like to filter by the special entities and i have problems here. my use case which texts contain only people ? which organisations or locations ? how many people do i find in total? i would like to create new to visualize the results. my problem i cant manage to filter according to the entities. what am i doing wrong? your example here is also ok for the test w&b aug visualizing prodigy datasets using w&b tables use the integration to upload your prodigy annotated datasets to w&b for easier visualization. made by kevin shen using image kb thank you alfred",
        "Answer_original_content":"glad youre enjoying the prodigy integration! ill try get an answer for this for you asap. i dont know what i did wrong last time, but it works perfectly. thanks! nice! glad you got it working",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"glad youre enjoying the prodigy integration! ill try get an answer for this for you asap. i dont know what i did wrong last time, but it works perfectly. thanks! nice! glad you got it working"
    },
    {
        "Question_id":null,
        "Question_title":"Shared development details",
        "Question_body":"<p>Hi,<\/p>\n<p>we are trying to use DVC very close to the described <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" rel=\"noopener nofollow ugc\">shared-development-server<\/a> use-case. This is great for starting, but the devil is in the details.<br>\nThe setup is that we use a shared storage in an HPC environment, where we try to reduce data duplication.<\/p>\n<p>We use a shared cache and because we later might want to connect this to a remote storage, we create a local \u201cremote\u201d storage for the moment.<\/p>\n<p>In the setup we have 2 different types of data. Smaller data files, that are ok if they get downloaded to the individual development folders and a folder containing millions of large files that we don\u2019t want to duplicate.<\/p>\n<p>We have 2 issues now.<\/p>\n<p>Firstly, the local \u201cremote\u201d causes error messages because of writing conflicts. We used the group setting \u201cdvc config cache.shared group\u201d, which seems to work in the cache, but if a folder on the remote 00 to FF seems to be reused, we get write conflicts during \u201cdvc push\u201d.<\/p>\n<p>Secondly, we are not sure how to fare with the large folder with the millions of large files. We probably should create a link to the folder from each individual directory. The data in this folder is generated elsewhere. We want to make sure if we ever have to re-run the data generation, we are informed if this lead to different files. Hence, we want to hash the folder or it\u2019s content, but we are not sure if it\u2019s viable to do this for this amount of files individually. What would be best solution here?<\/p>\n<p>Thanks a lot and best regards<br>\nMarius<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1607527227363,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":520.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-development-details\/580",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-09T16:24:47.846Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/garmhausenmarius\">@garmhausenmarius<\/a>!<\/p>\n<aside class=\"quote no-group\" data-username=\"garmhausenmarius\" data-post=\"1\" data-topic=\"580\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/g\/a698b9\/40.png\" class=\"avatar\"> garmhausenmarius:<\/div>\n<blockquote>\n<p>Firstly, the local \u201cremote\u201d causes error messages because of writing conflicts. We used the group setting \u201cdvc config cache.shared group\u201d, which seems to work in the cache, but if a folder on the remote 00 to FF seems to be reused, we get write conflicts during \u201cdvc push\u201d.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Could you provide more info on how such conflict looks like? Do you, by any chance have some log showing those errors?<\/p>\n<p>As to your second question: I presume each dev\/scientist have hers\/his own folder on this HPC environment, right? I believe it would be possible to even work with the \u201cbig\u201d dataset, if you set cache type to <code>reflink<\/code>, <code>symlink<\/code>, or <code>hardlink<\/code>. Take a look here: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a>.<\/p>\n<p>Is your data versioning handled by <code>git\/dvc<\/code> repository? If so, you can always utilize <code>dvc import<\/code> and <code>dvc get<\/code> to obtain the smaller dataset, if <code>*link<\/code> optimizations are not what you are looking for.<\/p>\n<p>Best regards,<br>\nPawe\u0142<\/p>",
                "Answer_score":16.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-09T17:13:56.646Z",
                "Answer_body":"<p>Thank you for your swift reply Pawel!<\/p>\n<p>Here are the commands that reproduce the error:<\/p>\n<p>507 dvc pull<br>\n508 dvc push<\/p>\n<p>510 git checkout -b dev\/obsolete<br>\n512 echo \u201ctest\u201d &gt; data\/test.txt<br>\n513 dvc add data\/test.txt<br>\n514 git add data\/.gitignore data\/test.txt.dvc<br>\n515 git commit -m \u201cX\u201d.<br>\n516 dvc push<br>\nERROR: failed to upload \u2018\u2026\/\u2026\/\u2026\/\u2026\/data\/mm\/shared\/dvc\/d8\/e8fca2dc0f896fd7cb4cb0031ba249\u2019 to \u2018\u2026\/\u2026\/\u2026\/\u2026\/data\/mm\/shared\/dvc-storage\/d8\/e8fca2dc0f896fd7cb4cb0031ba249\u2019 - [Errno 13] Permission denied: \u2018\/data\/mm\/shared\/dvc-storage\/d8\/e8fca2dc0f896fd7cb4cb0031ba249.hzdYodwh7UgSBGuVAD4iSE.tmp\u2019<br>\nERROR: failed to push data to the cloud - 1 files failed to upload<\/p>\n<p>The differences in the dvc (cache) and dvc-strorage (local remote) is that the files and folder get created with permissions on group level but not in the uplink case. So there is a new file, we can add it to the cache, but if the folder exists already in the \u201clocal remote\u201d that folder has user specific permission set, which prevents in upload. The target folder seems to iterate, in this case d8. Maybe we could force all of these folder to have the right permissions if not further folder are added after this point?<\/p>\n<p>I will have a look at the link that you provided, to see if this is already what we are looking for.<\/p>\n<p>Best regards<br>\nMarius<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: shared development details; Content: hi, we are trying to use very close to the described shared-development-server use-case. this is great for starting, but the devil is in the details. the setup is that we use a shared storage in an hpc environment, where we try to reduce data duplication. we use a shared cache and because we later might want to connect this to a remote storage, we create a local \u201cremote\u201d storage for the moment. in the setup we have 2 different types of data. smaller data files, that are ok if they get downloaded to the individual development folders and a folder containing millions of large files that we don\u2019t want to duplicate. we have 2 issues now. firstly, the local \u201cremote\u201d causes error messages because of writing conflicts. we used the group setting \u201c config cache.shared group\u201d, which seems to work in the cache, but if a folder on the remote 00 to ff seems to be reused, we get write conflicts during \u201c push\u201d. secondly, we are not sure how to fare with the large folder with the millions of large files. we probably should create a link to the folder from each individual directory. the data in this folder is generated elsewhere. we want to make sure if we ever have to re-run the data generation, we are informed if this lead to different files. hence, we want to hash the folder or it\u2019s content, but we are not sure if it\u2019s viable to do this for this amount of files individually. what would be best solution here? thanks a lot and best regards marius",
        "Question_original_content_gpt_summary":"The user is facing two challenges in their shared development setup: write conflicts when using a local \"remote\" storage, and hashing a folder containing millions of large files to ensure data consistency.",
        "Question_preprocessed_content":"Title: shared development details; Content: hi, we are trying to use very close to the described shared development server use case. this is great for starting, but the devil is in the details. the setup is that we use a shared storage in an hpc environment, where we try to reduce data duplication. we use a shared cache and because we later might want to connect this to a remote storage, we create a local remote storage for the moment. in the setup we have different types of data. smaller data files, that are ok if they get downloaded to the individual development folders and a folder containing millions of large files that we dont want to duplicate. we have issues now. firstly, the local remote causes error messages because of writing conflicts. we used the group setting config group, which seems to work in the cache, but if a folder on the remote to ff seems to be reused, we get write conflicts during push. secondly, we are not sure how to fare with the large folder with the millions of large files. we probably should create a link to the folder from each individual directory. the data in this folder is generated elsewhere. we want to make sure if we ever have to re run the data generation, we are rmed if this lead to different files. hence, we want to hash the folder or its content, but we are not sure if its viable to do this for this amount of files individually. what would be best solution here? thanks a lot and best regards marius",
        "Answer_original_content":"hi @garmhausenmarius! garmhausenmarius: firstly, the local remote causes error messages because of writing conflicts. we used the group setting config cache.shared group, which seems to work in the cache, but if a folder on the remote 00 to ff seems to be reused, we get write conflicts during push. could you provide more info on how such conflict looks like? do you, by any chance have some log showing those errors? as to your second question: i presume each dev\/scientist have hers\/his own folder on this hpc environment, right? i believe it would be possible to even work with the big dataset, if you set cache type to reflink, symlink, or hardlink. take a look here: https:\/\/.org\/doc\/user-guide\/large-dataset-optimization. is your data versioning handled by git\/ repository? if so, you can always utilize import and get to obtain the smaller dataset, if *link optimizations are not what you are looking for. best regards, pawe thank you for your swift reply pawel! here are the commands that reproduce the error: 507 pull 508 push 510 git checkout -b dev\/obsolete 512 echo test > data\/test.txt 513 add data\/test.txt 514 git add data\/.gitignore data\/test.txt. 515 git commit -m x. 516 push error: failed to upload \/\/\/\/data\/mm\/shared\/\/d8\/e8fca2dc0f896fd7cb4cb0031ba249 to \/\/\/\/data\/mm\/shared\/-storage\/d8\/e8fca2dc0f896fd7cb4cb0031ba249 - [errno 13] permission denied: \/data\/mm\/shared\/-storage\/d8\/e8fca2dc0f896fd7cb4cb0031ba249.hzdyodwh7ugsbguvad4ise.tmp error: failed to push data to the cloud - 1 files failed to upload the differences in the (cache) and -strorage (local remote) is that the files and folder get created with permissions on group level but not in the uplink case. so there is a new file, we can add it to the cache, but if the folder exists already in the local remote that folder has user specific permission set, which prevents in upload. the target folder seems to iterate, in this case d8. maybe we could force all of these folder to have the right permissions if not further folder are added after this point? i will have a look at the link that you provided, to see if this is already what we are looking for. best regards marius",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi garmhausenmarius firstly, the local remote causes error messages because of writing conflicts. we used the group setting config group, which seems to work in the cache, but if a folder on the remote to ff seems to be reused, we get write conflicts during push. could you provide more on how such conflict looks like? do you, by any chance have some log showing those errors? as to your second question i presume each have own folder on this hpc environment, right? i believe it would be possible to even work with the big dataset, if you set cache type to , , or . take a look here is your data versioning handled by repository? if so, you can always utilize and to obtain the smaller dataset, if optimizations are not what you are looking for. best regards, pawe thank you for your swift reply pawel! here are the commands that reproduce the error pull push git checkout b echo test > add git add git commit m x. push error failed to upload to permission denied error failed to push data to the cloud files failed to upload the differences in the and strorage is that the files and folder get created with permissions on group level but not in the uplink case. so there is a new file, we can add it to the cache, but if the folder exists already in the local remote that folder has user specific permission set, which prevents in upload. the target folder seems to iterate, in this case d . maybe we could force all of these folder to have the right permissions if not further folder are added after this point? i will have a look at the link that you provided, to see if this is already what we are looking for. best regards marius"
    },
    {
        "Question_id":34047782.0,
        "Question_title":"Jupyter notebook kernel dies when creating dummy variables with pandas",
        "Question_body":"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the \"FinelineNumber\" column. For context, <code>df.shape<\/code> returns <code>(647054, 7)<\/code>. I am trying to make a dummy column for <code>df['FinelineNumber']<\/code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)<\/code>, which I then plan to <code>concat<\/code> to the original dataframe. <\/p>\n\n<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')<\/code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.<\/code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.<\/p>\n\n<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1449073471520,
        "Question_favorite_count":1.0,
        "Question_score":5.0,
        "Question_view_count":5063.0,
        "Owner_creation_time":1373475615300,
        "Owner_last_access_time":1663780318292,
        "Owner_reputation":2037.0,
        "Owner_up_votes":61.0,
        "Owner_down_votes":1.0,
        "Owner_views":193.0,
        "Answer_body":"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.<\/p>\n\n<p>You might try doing <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/sparse.html\" rel=\"noreferrer\">.to_sparse()<\/a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1449768300887,
        "Answer_score":8.0,
        "Owner_location":"New York, United States",
        "Question_last_edit_time":1454300528203,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34047782",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: jupyter notebook kernel dies when creating dummy variables with pandas; Content: i am working on the walmart kaggle competition and i'm trying to create a dummy column of of the \"finelinenumber\" column. for context, df.shape returns (647054, 7). i am trying to make a dummy column for df['finelinenumber'], which has 5,196 unique values. the results should be a dataframe of shape (647054, 5196), which i then plan to concat to the original dataframe. nearly every time i run fineline_dummies = pd.get_dummies(df['finelinenumber'], prefix='fl'), i get the following error message the kernel appears to have died. it will restart automatically. i am running python 2.7 in jupyter notebook on a macbookpro with 16gb ram. can someone explain why this is happening (and why it happens most of the time but not every time)? is it a jupyter notebook or pandas bug? also, i thought it might have to do with not enough ram but i get the same error on a notebook with >100 gb of ram. on , the kernel dies every time - almost immediately.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where their Jupyter Notebook kernel dies when creating dummy variables with Pandas for a Walmart Kaggle competition.",
        "Question_preprocessed_content":"Title: jupyter notebook kernel dies when creating dummy variables with pandas; Content: i am working on the walmart kaggle competition and i'm trying to create a dummy column of of the finelinenumber column. for context, returns . i am trying to make a dummy column for , which has , unique values. the results should be a dataframe of shape , which i then plan to to the original dataframe. nearly every time i run , i get the following error message i am running python in jupyter notebook on a macbookpro with gb ram. can someone explain why this is happening ? is it a jupyter notebook or pandas bug? also, i thought it might have to do with not enough ram but i get the same error on a notebook with > gb of ram. on , the kernel dies every time almost immediately.",
        "Answer_original_content":"it very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24gb just for the pointers to the objects on a 64-bit system. on while the vm has a large amount of memory you're actually limited in how much memory you have available (currently 2gb, soon to be 4gb) - and when you hit the limit the kernel typically dies. so it seems very likely it is a memory usage issue. you might try doing .to_sparse() on the data frame first before doing any additional manipulations. that should allow pandas to keep most of the data frame out of memory.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"it very much could be memory usage a , data frame has , , , elements, which would be gb just for the pointers to the objects on a bit system. on while the vm has a large amount of memory you're actually limited in how much memory you have available and when you hit the limit the kernel typically dies. so it seems very likely it is a memory usage issue. you might try doing on the data frame first before doing any additional manipulations. that should allow pandas to keep most of the data frame out of memory."
    },
    {
        "Question_id":null,
        "Question_title":"RFC: Migrating Python RunData interface to expose metrics, params, tags as dicts",
        "Question_body":"Hi all, in the next MLflow release we're thinking of updating the Python RunData interface to make it easier to look up metric\/param\/tag values by key.\n\n\nWe're considering a few possible ways to make this happen (TL;DR we want to make metrics\/params\/tags dicts) & would love to get your feedback on the APIs, especially around use cases we should strive to make easy: https:\/\/gist.github.com\/smurching\/f60f7c418ec03f2bcee0f3374bfa1d81\n\nThanks!\nSid",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1554319503000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":10.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/TkPB76lf_fY",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[

        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: rfc: migrating python rundata interface to expose metrics, params, tags as dicts; Content: hi all, in the next release we're thinking of updating the python rundata interface to make it easier to look up metric\/param\/tag values by key. we're considering a few possible ways to make this happen (tl;dr we want to make metrics\/params\/tags dicts) & would love to get your feedback on the apis, especially around use cases we should strive to make easy: https:\/\/gist.github.com\/smurching\/f60f7c418ec03f2bcee0f3374bfa1d81 thanks! sid",
        "Question_original_content_gpt_summary":"The user is considering updating the Python RunData interface to make it easier to look up metric\/param\/tag values by key.",
        "Question_preprocessed_content":"Title: rfc migrating python rundata interface to expose metrics, params, tags as dicts; Content: hi all, in the next release we're thinking of updating the python rundata interface to make it easier to look up values by key. we're considering a few possible ways to make this happen & would love to get your feedback on the apis, especially around use cases we should strive to make easy thanks! sid",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":57725994.0,
        "Question_title":"Python AWS- Unicode error Algorithm Error Caused by: 'ascii' codec can't encode characters in position 1-2: ordinal not in range(128)",
        "Question_body":"<p>I am learning to train and transform using machine learning models on AWS Sagemaker which uses Python 3.6.<\/p>\n\n<p>I am able to train the Linear Learner model successfully but during transformation, I get an error:<\/p>\n\n<blockquote>\n  <p>UnexpectedStatusException: Error for Transform job linear-learner-2019-08-30-11-22-02-821: Failed. Reason: ClientError: See job logs for more information<\/p>\n<\/blockquote>\n\n<p>Which maps to this error in the CloudWatch logs<\/p>\n\n<blockquote>\n  <p>Algorithm Error: (caused by UnicodeEncodeError)<br>\n   Caused by: 'ascii' codec can't encode characters in position 1-2: ordinal not in range(128)<\/p>\n<\/blockquote>\n\n<p>The code which I am using is this<br>\n<em>To Create and train model<\/em><\/p>\n\n<pre><code>import boto3\nimport sagemaker\n\nsess = sagemaker.Session()\n\nlinear = sagemaker.estimator.Estimator(container,\n                                       role, \n                                       train_instance_count=1, \n                                       train_instance_type='ml.c4.xlarge',\n                                       output_path=output_location,\n                                       sagemaker_session=sess)\nlinear.set_hyperparameters(feature_dim=18,\n                           predictor_type='regressor')\n\nlinear.fit({'train': s3_train_data})\n<\/code><\/pre>\n\n<p>To create input and output S3 locations for test data<\/p>\n\n<pre><code>batch_input ='s3:\/\/{}\/{}\/test\/examples'.format(bucket, prefix) # The location of the test dataset   \nbatch_output = 's3:\/\/{}\/{}\/batch-inference'.format(bucket, prefix) # The location to store the results of the batch transform job\n\nprint(batch_input)\nprint(batch_output)\n<\/code><\/pre>\n\n<p>Transform test data<\/p>\n\n<pre><code>housing_test=strat_test_set\nhousing_test_inputs =full_pipeline.transform (housing_test)\nhousing_test_inputs=np.float32(housing_test_inputs)\nhousing_test_labels=strat_test_set['median_house_value'].values\nhousing_test_labels=np.float32(housing_test_labels)\n<\/code><\/pre>\n\n<p>To check consistency of number of features<\/p>\n\n<pre><code>print(housing_test_labels.shape)\nprint(housing_test_inputs.shape)\nprint(housing_labels.shape)\nprint(housing_inputs.shape)\n<\/code><\/pre>\n\n<p>Values returned from above are<\/p>\n\n<pre><code>(4128,)\n(4128, 18)\n(16512,)\n(16512, 18)\n<\/code><\/pre>\n\n<p>Test data upload to S3<\/p>\n\n<pre><code>buf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, housing_test_inputs, housing_test_labels)\nbuf.seek(0)\nkey = 'examples'\nboto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test', key)).upload_fileobj(buf)\ns3_test_data = 's3:\/\/{}\/{}\/test\/{}'.format(bucket, prefix, key)\nprint('uploaded test data location: {}'.format(s3_test_data))\n<\/code><\/pre>\n\n<p>Predicting values<\/p>\n\n<pre><code>transformer = linear.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)\n\ntransformer.transform(data=batch_input, data_type='S3Prefix', content_type='text\/csv', split_type='Line')\n\ntransformer.wait()\n<\/code><\/pre>\n\n<p><strong>The error I receive at the console is this-<\/strong><\/p>\n\n<blockquote>\n  <p>UnexpectedStatusException                 Traceback (most recent call last)\n   in ()\n        3 transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text\/csv', split_type='Line')\n        4 \n  ----> 5 transformer.wait()<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n      227         \"\"\"Placeholder docstring\"\"\"\n      228         self._ensure_last_transform_job()\n  --> 229         self.latest_transform_job.wait()\n      230 \n      231     def _ensure_last_transform_job(self):<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/transformer.py in wait(self)\n      344 \n      345     def wait(self):\n  --> 346         self.sagemaker_session.wait_for_transform_job(self.job_name)\n      347 \n      348     @staticmethod<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in wait_for_transform_job(self, job, poll)\n     1050         \"\"\"\n     1051         desc = _wait_until(lambda: _transform_job_status(self.sagemaker_client, job), poll)\n  -> 1052         self._check_job_status(job, desc, \"TransformJobStatus\")\n     1053         return desc\n     1054 <\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n     1077                 ),\n     1078                 allowed_statuses=[\"Completed\", \"Stopped\"],\n  -> 1079                 actual_status=status,\n     1080             )\n     1081 <\/p>\n  \n  <p>UnexpectedStatusException: Error for Transform job linear-learner-2019-08-30-11-22-02-821: Failed. Reason: ClientError: See job logs for more information<\/p>\n<\/blockquote>\n\n<p>Can somebody please suggest what is the root cause of this error and how to fix this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_creation_time":1567164905797,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":515.0,
        "Owner_creation_time":1515227856092,
        "Owner_last_access_time":1604700217056,
        "Owner_reputation":246.0,
        "Owner_up_votes":22.0,
        "Owner_down_votes":1.0,
        "Owner_views":60.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1567264423112,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57725994",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: python aws- unicode error algorithm error caused by: 'ascii' codec can't encode characters in position 1-2: ordinal not in range(128); Content: i am learning to train and transform using machine learning models on which uses python 3.6. i am able to train the linear learner model successfully but during transformation, i get an error: unexpectedstatusexception: error for transform job linear-learner-2019-08-30-11-22-02-821: failed. reason: clienterror: see job logs for more information which maps to this error in the cloudwatch logs algorithm error: (caused by unicodeencodeerror) caused by: 'ascii' codec can't encode characters in position 1-2: ordinal not in range(128) the code which i am using is this to create and train model import boto3 import sess = .session() linear = .estimator.estimator(container, role, train_instance_count=1, train_instance_type='ml.c4.xlarge', output_path=output_location, _session=sess) linear.set_hyperparameters(feature_dim=18, predictor_type='regressor') linear.fit({'train': s3_train_data}) to create input and output s3 locations for test data batch_input ='s3:\/\/{}\/{}\/test\/examples'.format(bucket, prefix) # the location of the test dataset batch_output = 's3:\/\/{}\/{}\/batch-inference'.format(bucket, prefix) # the location to store the results of the batch transform job print(batch_input) print(batch_output) transform test data housing_test=strat_test_set housing_test_inputs =full_pipeline.transform (housing_test) housing_test_inputs=np.float32(housing_test_inputs) housing_test_labels=strat_test_set['median_house_value'].values housing_test_labels=np.float32(housing_test_labels) to check consistency of number of features print(housing_test_labels.shape) print(housing_test_inputs.shape) print(housing_labels.shape) print(housing_inputs.shape) values returned from above are (4128,) (4128, 18) (16512,) (16512, 18) test data upload to s3 buf = io.bytesio() smac.write_numpy_to_dense_tensor(buf, housing_test_inputs, housing_test_labels) buf.seek(0) key = 'examples' boto3.resource('s3').bucket(bucket).object(os.path.join(prefix, 'test', key)).upload_fileobj(buf) s3_test_data = 's3:\/\/{}\/{}\/test\/{}'.format(bucket, prefix, key) print('uploaded test data location: {}'.format(s3_test_data)) predicting values transformer = linear.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output) transformer.transform(data=batch_input, data_type='s3prefix', content_type='text\/csv', split_type='line') transformer.wait() the error i receive at the console is this- unexpectedstatusexception traceback (most recent call last) in () 3 transformer.transform(data=batch_input, data_type='s3prefix', content_type='text\/csv', split_type='line') 4 ----> 5 transformer.wait() ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/transformer.py in wait(self) 227 \"\"\"placeholder docstring\"\"\" 228 self._ensure_last_transform_job() --> 229 self.latest_transform_job.wait() 230 231 def _ensure_last_transform_job(self): ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/transformer.py in wait(self) 344 345 def wait(self): --> 346 self._session.wait_for_transform_job(self.job_name) 347 348 @staticmethod ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/session.py in wait_for_transform_job(self, job, poll) 1050 \"\"\" 1051 desc = _wait_until(lambda: _transform_job_status(self._client, job), poll) -> 1052 self._check_job_status(job, desc, \"transformjobstatus\") 1053 return desc 1054 ~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/session.py in _check_job_status(self, job, desc, status_key_name) 1077 ), 1078 allowed_statuses=[\"completed\", \"stopped\"], -> 1079 actual_status=status, 1080 ) 1081 unexpectedstatusexception: error for transform job linear-learner-2019-08-30-11-22-02-821: failed. reason: clienterror: see job logs for more information can somebody please suggest what is the root cause of this error and how to fix this?",
        "Question_original_content_gpt_summary":"The user encountered an unexpected status exception error caused by a 'ascii' codec unable to encode characters in position 1-2, while attempting to train and transform using machine learning models on Python 3.6.",
        "Question_preprocessed_content":"Title: python aws unicode error algorithm error caused by 'ascii' codec can't encode characters in position ordinal not in range; Content: i am learning to train and transform using machine learning models on which uses python i am able to train the linear learner model successfully but during transformation, i get an error unexpectedstatusexception error for transform job linear learner failed. reason clienterror see job logs for more which maps to this error in the cloudwatch logs algorithm error caused by 'ascii' codec can't encode characters in position ordinal not in range the code which i am using is this to create and train model to create input and output s locations for test data transform test data to check consistency of number of features values returned from above are test data upload to s predicting values the error i receive at the console is this unexpectedstatusexception traceback in > in wait placeholder docstring > def in wait def wait > in job, poll desc job , poll > desc, transformjobstatus return desc in job, desc, , stopped , > unexpectedstatusexception error for transform job linear learner failed. reason clienterror see job logs for more can somebody please suggest what is the root cause of this error and how to fix this?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Right way to provide optional parameters to script in experiments",
        "Question_body":"<p>Hi! What is a supposed way to deal with optional params for scripts in dvc.yaml?<br>\nLet\u2019s suppose we have a script which could be run like <code>python train.py<\/code> or <code>python train.py --resume path-to-model-weights<\/code>.<\/p>\n<p>I can come up to something like this:<\/p>\n<pre><code class=\"lang-auto\"># dvc.yaml\nstages:\n  train:\n    deps:\n      - train.py\n    cmd: python train.py ${resume}\n<\/code><\/pre>\n<pre><code class=\"lang-auto\"># params.yaml\nresume: \"\"\n<\/code><\/pre>\n<p>and in case I want to run an experiment and resume training, use <code>dvc exp run -S resume=\"--resume path-to-model-weights\"<\/code><\/p>\n<p>But maybe I\u2019ve missed more elegant solution? Something that will allow <code>dvc exp run -S resume=path-to-model-weights<\/code>.<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":null,
        "Question_creation_time":1622567503315,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":695.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/right-way-to-provide-optional-parameters-to-script-in-experiments\/773",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-02T10:46:23.728Z",
                "Answer_body":"<p><span class=\"mention\">@agushin<\/span><br>\nI think that we did not have this use case before, and your approach seems to be valid. <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a> might have more information then me in that matter.<\/p>",
                "Answer_score":17.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-02T13:53:37.473Z",
                "Answer_body":"<p>Thank you for the answer! I hope <a class=\"mention\" href=\"\/u\/skshetry\">@skshetry<\/a> could provide more information about this case.<\/p>\n<p>Also I\u2019m not sure what is the right way to handle situations when a script should be called like this (note the arbitrary amount of values supplied to <code>--numbers<\/code><\/p>\n<pre><code class=\"lang-auto\">python calculator.py --numbers 1 2 3 4 --operation sum\n# or\npython calculator.py --numbers 1 2 --operation multiply\n# or\npython calculator.py --operation sum --numbers 1 2 3 4 5 6\n<\/code><\/pre>\n<p>If I\u2019d have constant number of arguments, then I would do something like this, which is already not very beatiful:<\/p>\n<pre><code class=\"lang-auto\"># params.yaml\nnumbers: [1, 2, 3, 4]\noperation: sum\n\n# dvc.yaml\nstages:\n  calculate:\n    cmd: python calculator.py --numbers ${numbers[0]} ${numbers[1]} ${numbers[2]} ${numbers[3]} --operation ${operation}\n<\/code><\/pre>\n<p>And if for some reason either<\/p>\n<ol>\n<li>I want to have arbitrary number of values in this list<\/li>\n<li>I want to have an option to skip this argument<\/li>\n<\/ol>\n<p>then I don\u2019t have a good idea how to handle this without modifying my python script, except may be by treating this parameter as a string again:<\/p>\n<pre><code class=\"lang-auto\"># params.yaml\nnumbers: \"--numbers 1 2 3 4\"\noperation: sum\n\n# dvc.yaml\nstages:\n  calculate:\n    cmd: python calculator.py ${numbers} --operation ${operation}\n<\/code><\/pre>\n<p>Would be great to know more beatiful solution, if it exists!<\/p>",
                "Answer_score":37.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-03T01:37:06.084Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"aguschin\" data-post=\"3\" data-topic=\"773\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/a\/a4c791\/40.png\" class=\"avatar\"> aguschin:<\/div>\n<blockquote>\n<p>Also I\u2019m not sure what is the right way to handle situations when a script should be called like this (note the arbitrary amount of values supplied to <code>--numbers<\/code><\/p>\n<\/blockquote>\n<\/aside>\n<p>This seems to me like a case where it would be better to just support reading values directly from <code>params.yaml<\/code> in your python script, instead of always passing them via the command line<\/p>",
                "Answer_score":22.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-16T15:33:57.772Z",
                "Answer_body":"<p>I\u2019d also be interested in how to handle command-line program arguments in DVC. The example provided by <a class=\"mention\" href=\"\/u\/aguschin\">@aguschin<\/a> is a good one. For some use cases, it just doesn\u2019t make a lot of sense to have a parameter defined in YAML, but instead pass it as a program argument - particularly for mandatory \u201cruntime\u201d parameters, that are not a configuration option in that sense.<\/p>\n<p>Another example would be a pipeline, that processes and analyzes satellite image data for certain points in time. One of the early pipeline steps would be to download that data from some online repository, so I\u2019d like to have a param like <code>--date<\/code>, that I can just pass without first having to edit <code>params.yaml<\/code>, perhaps even because I\u2019m running the pipeline headlessly on some server.<\/p>\n<p>Does DVC support such use cases?<\/p>\n<p><strong>Edit:<\/strong> Moreover, from my understanding, the params file is meant to be checked in to version control, which is another hint that this is the wrong place for \u201cruntime\u201d arguments like the <code>date<\/code> one from above, which have no default value or so.<\/p>",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-17T09:34:55.232Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ferdi\">@ferdi<\/a> , even though there might be some limitations depending on your use case, having parameters tracked by DVC should not be incompatible with the use cases you described.<\/p>\n<p>Let\u2019s say you have:<\/p>\n<pre><code class=\"lang-auto\"># params.yaml\ndata: 16-02-2022\n<\/code><\/pre>\n<p>You can use <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#templating\" rel=\"noopener nofollow ugc\">Templating<\/a> and DVC will interpolate those params into program arguments:<\/p>\n<pre><code class=\"lang-auto\"># dvc.yaml\nstages: \n  process_satellite:\n    cmd: python process_satellite.py --date ${date}\n<\/code><\/pre>\n<p>In addition, you can use <code>--set-param<\/code> option of <code>dvc exp run<\/code> to <a href=\"https:\/\/dvc.org\/doc\/command-reference\/exp\/run#example-modify-parameters-on-the-fly\" rel=\"noopener nofollow ugc\">modify parameters on runtime<\/a>:<\/p>\n<pre><code class=\"lang-auto\">dvc exp run -S date='17-02-2022'\n<\/code><\/pre>\n<p>The above snippet is from CLI, so you could have other logic setting an env var (i.e. <code>$DATE<\/code>) instead of hardcoding values:<\/p>\n<pre><code class=\"lang-auto\">dvc exp run -S date=$DATE\n<\/code><\/pre>",
                "Answer_score":51.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-17T13:33:11.241Z",
                "Answer_body":"<p>Very helpful, thanks! Especially for pointing me to the on-the-fly parameter changes.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-21T11:32:11.663Z",
                "Answer_body":"<p>I think I found a problem with this approach. Take your <code>dvc.yml<\/code>, but add another stage (say <code>postprocess_satellite<\/code>), that depends on some output from the <code>process_satellite<\/code> stage.<\/p>\n<ol>\n<li>Run <code>dvc exp run -S date='17-02-2022' &amp;&amp; dvc repro<\/code> <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/arrow_right.png?v=12\" title=\":arrow_right:\" class=\"emoji\" alt=\":arrow_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> Both stages will run initially<\/li>\n<li>Run <code>dvc exp run -S date='18-02-2022' &amp;&amp; dvc repro<\/code> <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/arrow_right.png?v=12\" title=\":arrow_right:\" class=\"emoji\" alt=\":arrow_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> Both stages will run for the new date<\/li>\n<li>Run <code>dvc exp run -S date='17-02-2022' &amp;&amp; dvc repro process_satellite<\/code> <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/arrow_right.png?v=12\" title=\":arrow_right:\" class=\"emoji\" alt=\":arrow_right:\" loading=\"lazy\" width=\"20\" height=\"20\"> First stage will run again, but shouldn\u2019t.<\/li>\n<\/ol>\n<p>In step 3, first stage should be skipped, because it had already run before with same parameter input. However, it runs for reason <code>'cmd' of stage: 'preprocess' has changed<\/code>.<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-22T02:09:04.370Z",
                "Answer_body":"<p><code>dvc exp run<\/code> is meant to be a complete replacement for <code>dvc repro<\/code>, and it is intended for generating experiments which are completely independent of each other (see: <a href=\"https:\/\/dvc.org\/doc\/start\/experiments\">https:\/\/dvc.org\/doc\/start\/experiments<\/a>)<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: right way to provide optional parameters to script in experiments; Content: hi! what is a supposed way to deal with optional params for scripts in .yaml? let\u2019s suppose we have a script which could be run like python train.py or python train.py --resume path-to-model-weights. i can come up to something like this: # .yaml stages: train: deps: - train.py cmd: python train.py ${resume} # params.yaml resume: \"\" and in case i want to run an experiment and resume training, use exp run -s resume=\"--resume path-to-model-weights\" but maybe i\u2019ve missed more elegant solution? something that will allow exp run -s resume=path-to-model-weights.",
        "Question_original_content_gpt_summary":"The user is seeking an elegant solution to provide optional parameters to a script in a .yaml file, such as the ability to run the script with or without a \"--resume\" flag.",
        "Question_preprocessed_content":"Title: right way to provide optional parameters to script in experiments; Content: hi! what is a supposed way to deal with optional params for scripts in lets suppose we have a script which could be run like or . i can come up to something like this and in case i want to run an experiment and resume training, use but maybe ive missed more elegant solution? something that will allow .",
        "Answer_original_content":"@agushin i think that we did not have this use case before, and your approach seems to be valid. @skshetry might have more information then me in that matter. thank you for the answer! i hope @skshetry could provide more information about this case. also im not sure what is the right way to handle situations when a script should be called like this (note the arbitrary amount of values supplied to --numbers python calculator.py --numbers 1 2 3 4 --operation sum # or python calculator.py --numbers 1 2 --operation multiply # or python calculator.py --operation sum --numbers 1 2 3 4 5 6 if id have constant number of arguments, then i would do something like this, which is already not very beatiful: # params.yaml numbers: [1, 2, 3, 4] operation: sum # .yaml stages: calculate: cmd: python calculator.py --numbers ${numbers[0]} ${numbers[1]} ${numbers[2]} ${numbers[3]} --operation ${operation} and if for some reason either i want to have arbitrary number of values in this list i want to have an option to skip this argument then i dont have a good idea how to handle this without modifying my python script, except may be by treating this parameter as a string again: # params.yaml numbers: \"--numbers 1 2 3 4\" operation: sum # .yaml stages: calculate: cmd: python calculator.py ${numbers} --operation ${operation} would be great to know more beatiful solution, if it exists! aguschin: also im not sure what is the right way to handle situations when a script should be called like this (note the arbitrary amount of values supplied to --numbers this seems to me like a case where it would be better to just support reading values directly from params.yaml in your python script, instead of always passing them via the command line id also be interested in how to handle command-line program arguments in . the example provided by @aguschin is a good one. for some use cases, it just doesnt make a lot of sense to have a parameter defined in yaml, but instead pass it as a program argument - particularly for mandatory runtime parameters, that are not a configuration option in that sense. another example would be a pipeline, that processes and analyzes satellite image data for certain points in time. one of the early pipeline steps would be to download that data from some online repository, so id like to have a param like --date, that i can just pass without first having to edit params.yaml, perhaps even because im running the pipeline headlessly on some server. does support such use cases? edit: moreover, from my understanding, the params file is meant to be checked in to version control, which is another hint that this is the wrong place for runtime arguments like the date one from above, which have no default value or so. hi @ferdi , even though there might be some limitations depending on your use case, having parameters tracked by should not be incompatible with the use cases you described. lets say you have: # params.yaml data: 16-02-2022 you can use templating and will interpolate those params into program arguments: # .yaml stages: process_satellite: cmd: python process_satellite.py --date ${date} in addition, you can use --set-param option of exp run to modify parameters on runtime: exp run -s date='17-02-2022' the above snippet is from cli, so you could have other logic setting an env var (i.e. $date) instead of hardcoding values: exp run -s date=$date very helpful, thanks! especially for pointing me to the on-the-fly parameter changes. i think i found a problem with this approach. take your .yml, but add another stage (say postprocess_satellite), that depends on some output from the process_satellite stage. run exp run -s date='17-02-2022' && repro both stages will run initially run exp run -s date='18-02-2022' && repro both stages will run for the new date run exp run -s date='17-02-2022' && repro process_satellite first stage will run again, but shouldnt. in step 3, first stage should be skipped, because it had already run before with same parameter input. however, it runs for reason 'cmd' of stage: 'preprocess' has changed. exp run is meant to be a complete replacement for repro, and it is intended for generating experiments which are completely independent of each other (see: https:\/\/.org\/doc\/start\/experiments)",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i think that we did not have this use case before, and your approach seems to be valid. might have more then me in that matter. thank you for the answer! i hope could provide more about this case. also im not sure what is the right way to handle situations when a script should be called like this instead of hardcoding values very helpful, thanks! especially for pointing me to the on the fly parameter changes. i think i found a problem with this approach. take your , but add another stage , that depends on some output from the stage. run both stages will run initially run both stages will run for the new date run first stage will run again, but shouldnt. in step , first stage should be skipped, because it had already run before with same parameter input. however, it runs for reason . is meant to be a complete replacement for , and it is intended for generating experiments which are completely independent of each other"
    },
    {
        "Question_id":72106030.0,
        "Question_title":"I am not able to create a feature store in vertexAI using labels",
        "Question_body":"<p>I am passing the values of lables as below to create a featurestore with labels. But after creation of the featurestore, I do not see the featurestore created with labels. Is it still not supported in VertexAI<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>    fs = aiplatform.Featurestore.create(\n        featurestore_id=featurestore_id,\n        labels=dict(project='retail', env='prod'),\n        online_store_fixed_node_count=online_store_fixed_node_count,\n        sync=sync\n    )\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/viOSu.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/viOSu.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1651616413553,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":83.0,
        "Owner_creation_time":1530457174832,
        "Owner_last_access_time":1664033927316,
        "Owner_reputation":1043.0,
        "Owner_up_votes":17.0,
        "Owner_down_votes":6.0,
        "Owner_views":212.0,
        "Answer_body":"<p>As mentioned in this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-featurestores\" rel=\"nofollow noreferrer\">featurestore documentation<\/a>:<\/p>\n<blockquote>\n<p>A <strong>featurestore<\/strong> is a top-level container for entity types, features,\nand feature values.<\/p>\n<\/blockquote>\n<p>With this, the GCP console UI &quot;labels&quot; are the &quot;labels&quot; at the <strong>Feature<\/strong> level.<\/p>\n<p>Once a <strong>featurestore<\/strong> is created, you will need to create an <strong>entity<\/strong> and then create a <strong>Feature<\/strong> that has the <em>labels<\/em> parameter as shown on the below sample python code.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from google.cloud import aiplatform\n\ntest_label = {'key1' : 'value1'}\n\ndef create_feature_sample(\n    project: str,\n    location: str,\n    feature_id: str,\n    value_type: str,\n    entity_type_id: str,\n    featurestore_id: str,\n):\n\n    aiplatform.init(project=project, location=location)\n\n    my_feature = aiplatform.Feature.create(\n        feature_id=feature_id,\n        value_type=value_type,\n        entity_type_name=entity_type_id,\n        featurestore_id=featurestore_id,\n        labels=test_label,\n    )\n\n    my_feature.wait()\n\n    return my_feature\n\ncreate_feature_sample('your-project','us-central1','test_feature3','STRING','test_entity3','test_fs3')\n<\/code><\/pre>\n<p>Below is the screenshot of the GCP console which shows that <em>labels<\/em> for <strong>test_feature3<\/strong> feature has the values defined in the above sample python code.\n<a href=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7S2oa.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/featurestore\/managing-features#create-feature\" rel=\"nofollow noreferrer\">creation of feature documentation<\/a> using python for more details.<\/p>\n<p>On the other hand, you may still view the <em>labels<\/em> you defined for your featurestore using the REST API as shown on the below sample.<\/p>\n<pre><code>curl -X GET \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\n&quot;https:\/\/&lt;your-location&gt;-aiplatform.googleapis.com\/v1\/projects\/&lt;your-project&gt;\/locations\/&lt;your-location&gt;\/featurestores&quot;\n<\/code><\/pre>\n<p>Below is the result of the REST API which also shows the value of the <em>labels<\/em> I defined for my &quot;test_fs3&quot; featurestore.\n<a href=\"https:\/\/i.stack.imgur.com\/gW45X.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gW45X.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1651709813300,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1651623411107,
        "Answer_last_edit_time":1651710305260,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72106030",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: i am not able to create a feature store in vertexai using labels; Content: i am passing the values of lables as below to create a featurestore with labels. but after creation of the featurestore, i do not see the featurestore created with labels. is it still not supported in vertexai fs = aiplatform.featurestore.create( featurestore_id=featurestore_id, labels=dict(project='retail', env='prod'), online_store_fixed_node_count=online_store_fixed_node_count, sync=sync )",
        "Question_original_content_gpt_summary":"The user is encountering challenges creating a feature store in VertexAI using labels.",
        "Question_preprocessed_content":"Title: i am not able to create a feature store in vertexai using labels; Content: i am passing the values of lables as below to create a featurestore with labels. but after creation of the featurestore, i do not see the featurestore created with labels. is it still not supported in vertexai",
        "Answer_original_content":"as mentioned in this featurestore documentation: a featurestore is a top-level container for entity types, features, and feature values. with this, the gcp console ui \"labels\" are the \"labels\" at the feature level. once a featurestore is created, you will need to create an entity and then create a feature that has the labels parameter as shown on the below sample python code. from google.cloud import aiplatform test_label = {'key1' : 'value1'} def create_feature_sample( project: str, location: str, feature_id: str, value_type: str, entity_type_id: str, featurestore_id: str, ): aiplatform.init(project=project, location=location) my_feature = aiplatform.feature.create( feature_id=feature_id, value_type=value_type, entity_type_name=entity_type_id, featurestore_id=featurestore_id, labels=test_label, ) my_feature.wait() return my_feature create_feature_sample('your-project','us-central1','test_feature3','string','test_entity3','test_fs3') below is the screenshot of the gcp console which shows that labels for test_feature3 feature has the values defined in the above sample python code. you may refer to this creation of feature documentation using python for more details. on the other hand, you may still view the labels you defined for your featurestore using the rest api as shown on the below sample. curl -x get \\ -h \"authorization: bearer \"$(gcloud auth application-default print-access-token) \\ \"https:\/\/<your-location>-aiplatform.googleapis.com\/v1\/projects\/<your-project>\/locations\/<your-location>\/featurestores\" below is the result of the rest api which also shows the value of the labels i defined for my \"test_fs3\" featurestore.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"as mentioned in this featurestore documentation a featurestore is a top level container for entity types, features, and feature values. with this, the gcp console ui labels are the labels at the feature level. once a featurestore is created, you will need to create an entity and then create a feature that has the labels parameter as shown on the below sample python code. below is the screenshot of the gcp console which shows that labels for feature has the values defined in the above sample python code. you may refer to this creation of feature documentation using python for more details. on the other hand, you may still view the labels you defined for your featurestore using the rest api as shown on the below sample. below is the result of the rest api which also shows the value of the labels i defined for my featurestore."
    },
    {
        "Question_id":48774210.0,
        "Question_title":"AWS SageMaker models popping in and out of the vacuum",
        "Question_body":"<p>I have proven quantum mechanics while trying to deploy and use an AWS SageMaker Models. Notice the errors seem to indicate that the model\/endpoint both do and don't exist.<\/p>\n\n<h3>Training my model<\/h3>\n\n<pre><code>m = MXNet(\"lstm_trainer.py\", \n          role=role, \n          train_instance_count=10, \n          train_instance_type=\"ml.c4.xlarge\",\n          hyperparameters={'batch_size': 100, \n                         'epochs': 400, \n                         'learning_rate': 0.1, \n                         'momentum': 0.9, \n                         'log_interval': 100})\nm.fit(inputs)\n<\/code><\/pre>\n\n<p>runs fine.<\/p>\n\n<h3>Deploying the endpoint<\/h3>\n\n<pre><code>predictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>gets the error,<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateModel operation: Cannot create already existing model \"arn:aws:sagemaker:us-west-2:01234567890:model\/sagemaker-mxnet-py2-cpu-2018-02-13-17-18-59-047\".\n<\/code><\/pre>\n\n<h3>Running the endpoint,<\/h3>\n\n<pre><code>predictor.predict(np.arange(100))\n<\/code><\/pre>\n\n<p>gives,<\/p>\n\n<pre><code>ValidationError: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint sagemaker-mxnet-py2-cpu-2018-02-13-17-18-59-047 of account 01234567890 not found.\n<\/code><\/pre>\n\n<h3>Deleting the endpoint like,<\/h3>\n\n<pre><code>sagemaker.Session().delete_endpoint(predictor.endpoint)\n<\/code><\/pre>\n\n<p>gives,<\/p>\n\n<pre><code>ClientError: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-west-2:01234567890:endpoint\/sagemaker-mxnet-py2-cpu-2018-02-13-17-18-59-047\".\n<\/code><\/pre>\n\n<p>How do I fix this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1518549873797,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":561.0,
        "Owner_creation_time":1384712661608,
        "Owner_last_access_time":1649138369127,
        "Owner_reputation":5151.0,
        "Owner_up_votes":213.0,
        "Owner_down_votes":12.0,
        "Owner_views":1038.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"San Diego, CA, United States",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48774210",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: models popping in and out of the vacuum; Content: i have proven quantum mechanics while trying to deploy and use an models. notice the errors seem to indicate that the model\/endpoint both do and don't exist. training my model m = mxnet(\"lstm_trainer.py\", role=role, train_instance_count=10, train_instance_type=\"ml.c4.xlarge\", hyperparameters={'batch_size': 100, 'epochs': 400, 'learning_rate': 0.1, 'momentum': 0.9, 'log_interval': 100}) m.fit(inputs) runs fine. deploying the endpoint predictor = m.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge') gets the error, clienterror: an error occurred (validationexception) when calling the createmodel operation: cannot create already existing model \"arn:aws::us-west-2:01234567890:model\/-mxnet-py2-cpu-2018-02-13-17-18-59-047\". running the endpoint, predictor.predict(np.arange(100)) gives, validationerror: an error occurred (validationerror) when calling the invokeendpoint operation: endpoint -mxnet-py2-cpu-2018-02-13-17-18-59-047 of account 01234567890 not found. deleting the endpoint like, .session().delete_endpoint(predictor.endpoint) gives, clienterror: an error occurred (validationexception) when calling the deleteendpoint operation: could not find endpoint \"arn:aws::us-west-2:01234567890:endpoint\/-mxnet-py2-cpu-2018-02-13-17-18-59-047\". how do i fix this?",
        "Question_original_content_gpt_summary":"The user encountered challenges while trying to deploy and use an models, with errors indicating that the model\/endpoint both do and don't exist.",
        "Question_preprocessed_content":"Title: models popping in and out of the vacuum; Content: i have proven quantum mechanics while trying to deploy and use an models. notice the errors seem to indicate that the both do and don't exist. training my model runs fine. deploying the endpoint gets the error, running the endpoint, gives, deleting the endpoint like, gives, how do i fix this?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"How can I add a table to a run after it has completed via the API?",
        "Question_body":"<p>I would like to log a table to a wandb run, like shown here: <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/tables-quickstart\">https:\/\/docs.wandb.ai\/guides\/data-vis\/tables-quickstart<\/a><\/p>\n<p>The table will contain information about the performance of an RL agent in environments which differ from its training environment. I want to add the table to the wandb created during the training of the RL agent. Is this possible?<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1651088864988,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":119.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-can-i-add-a-table-to-a-run-after-it-has-completed-via-the-api\/2334",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-29T19:42:20.636Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jcoholich\">@jcoholich<\/a>,<\/p>\n<p>You can <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming?q=resume\">resume<\/a> a run and log your table through that way.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-04T04:36:06.496Z",
                "Answer_body":"<p>Hi <strong><a class=\"mention\" href=\"\/u\/jcoholich\">@jcoholich<\/a><\/strong>,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-10T19:13:04.965Z",
                "Answer_body":"<p>Hi Jeremiah, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-28T19:42:42.352Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i add a table to a run after it has completed via the api?; Content: i would like to log a table to a run, like shown here: https:\/\/docs..ai\/guides\/data-vis\/tables-quickstart the table will contain information about the performance of an rl agent in environments which differ from its training environment. i want to add the table to the created during the training of the rl agent. is this possible? thanks.",
        "Question_original_content_gpt_summary":"The user is looking to add a table to a run after it has completed via the API, containing information about the performance of an RL agent in environments which differ from its training environment.",
        "Question_preprocessed_content":"Title: how can i add a table to a run after it has completed via the api?; Content: i would like to log a table to a run, like shown here the table will contain about the performance of an rl agent in environments which differ from its training environment. i want to add the table to the created during the training of the rl agent. is this possible? thanks.",
        "Answer_original_content":"hi @jcoholich, you can resume a run and log your table through that way. thanks, ramit hi @jcoholich, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi jeremiah, since we have not heard back from you we are going to close this request. if you would like to re-open the conversation, please let us know! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi you can resume a run and log your table through that way. thanks, ramit hi we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, hi jeremiah, since we have not heard back from you we are going to close this request. if you would like to re open the conversation, please let us know! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Shared cache without commiting",
        "Question_body":"<p>Hi<\/p>\n<p>As far as i understand, when I setup a shared cache directory, I have to commit it and others also have to use it.<br>\nIs there a way that I can define a shared cache dir for me only?<\/p>\n<p>Use Cases:<\/p>\n<ul>\n<li>I have a project, where I want to run jobs from two branches simultaneously. So I need to clone the repo twice, but only want the cache to exist once.<\/li>\n<li>I have projects with similar data dependencies and I want to share the cache between both projects.<\/li>\n<\/ul>\n<p>Is it possible or do you have any other recommendations for my use cases?<\/p>\n<p>Thanks and Regards<br>\nMatthias<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1537882485174,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":557.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/shared-cache-without-commiting\/100",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-25T13:54:29.020Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ynop\">@ynop<\/a> !<\/p>\n<p>Sure, it is possible and it is no different from what you would use when working with colleagues like described in this <a href=\"https:\/\/dvc.org\/doc\/use-cases\/multiple-data-scientists-on-a-single-machine\">article<\/a> . You can also specify the same shared directory for multiple different projects just as you\u2019ve described in your use case.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-25T14:02:55.513Z",
                "Answer_body":"<p>Thanks for the prompt reply!<\/p>\n<p>But is it also possible to define the shared cache folder without exposing it to my colleagues via git?<\/p>\n<p>Since I want to share my cache for multiple projects, but my colleagues may using an other directory on another machine.<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-25T14:06:59.111Z",
                "Answer_body":"<p>Sure. There is a local config that is designed specifically for such cases of private configs. It is not tracked by git and has priority over regular config. To use your local config, just add <code>--local<\/code> flag to your <code>dvc config<\/code> commands and it will write to <code>.dvc\/config.local<\/code> instead of <code>.dvc\/config<\/code>.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: shared cache without commiting; Content: hi as far as i understand, when i setup a shared cache directory, i have to commit it and others also have to use it. is there a way that i can define a shared cache dir for me only? use cases: i have a project, where i want to run jobs from two branches simultaneously. so i need to clone the repo twice, but only want the cache to exist once. i have projects with similar data dependencies and i want to share the cache between both projects. is it possible or do you have any other recommendations for my use cases? thanks and regards matthias",
        "Question_original_content_gpt_summary":"The user is looking for a way to set up a shared cache directory without committing it, in order to run jobs from two branches simultaneously and share the cache between two projects.",
        "Question_preprocessed_content":"Title: shared cache without commiting; Content: hi as far as i understand, when i setup a shared cache directory, i have to commit it and others also have to use it. is there a way that i can define a shared cache dir for me only? use cases i have a project, where i want to run jobs from two branches simultaneously. so i need to clone the repo twice, but only want the cache to exist once. i have projects with similar data dependencies and i want to share the cache between both projects. is it possible or do you have any other recommendations for my use cases? thanks and regards matthias",
        "Answer_original_content":"hi @ynop ! sure, it is possible and it is no different from what you would use when working with colleagues like described in this article . you can also specify the same shared directory for multiple different projects just as youve described in your use case. thanks, ruslan thanks for the prompt reply! but is it also possible to define the shared cache folder without exposing it to my colleagues via git? since i want to share my cache for multiple projects, but my colleagues may using an other directory on another machine. sure. there is a local config that is designed specifically for such cases of private configs. it is not tracked by git and has priority over regular config. to use your local config, just add --local flag to your config commands and it will write to .\/config.local instead of .\/config.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi ! sure, it is possible and it is no different from what you would use when working with colleagues like described in this article . you can also specify the same shared directory for multiple different projects just as youve described in your use case. thanks, ruslan thanks for the prompt reply! but is it also possible to define the shared cache folder without exposing it to my colleagues via git? since i want to share my cache for multiple projects, but my colleagues may using an other directory on another machine. sure. there is a local config that is designed specifically for such cases of private configs. it is not tracked by git and has priority over regular config. to use your local config, just add flag to your commands and it will write to instead of ."
    },
    {
        "Question_id":40018320.0,
        "Question_title":"Is it secure to pass the DB query to AzureML as a global parameter?",
        "Question_body":"<p>When using AzureMLBatchExecution activity in Azure Data Factory, is it secure to pass the DB query as a global parameter to the AzureML web service? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1476354083450,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":68.0,
        "Owner_creation_time":1452608563363,
        "Owner_last_access_time":1562103924248,
        "Owner_reputation":105.0,
        "Owner_up_votes":10.0,
        "Owner_down_votes":0.0,
        "Owner_views":12.0,
        "Answer_body":"<p>When you talk about \"secure\", are you worried about secure transmission between AML and ADF, or secure storage of your DB query information? For the former, all communication between these two services will be done with HTTPS. For the latter, our production storage has its strict access control. Besides, we only log the count of the global parameters and never the values. I believe it's secure to pass your DB query as a global parameter to the AzureML web service.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1476431199480,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40018320",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is it secure to pass the db query to as a global parameter?; Content: when using batchexecution activity in azure data factory, is it secure to pass the db query as a global parameter to the web service?",
        "Question_original_content_gpt_summary":"The user is questioning the security of passing a database query as a global parameter to a web service when using a batchexecution activity in Azure Data Factory.",
        "Question_preprocessed_content":"Title: is it secure to pass the db query to as a global parameter?; Content: when using batchexecution activity in azure data factory, is it secure to pass the db query as a global parameter to the web service?",
        "Answer_original_content":"when you talk about \"secure\", are you worried about secure transmission between aml and adf, or secure storage of your db query information? for the former, all communication between these two services will be done with https. for the latter, our production storage has its strict access control. besides, we only log the count of the global parameters and never the values. i believe it's secure to pass your db query as a global parameter to the web service.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"when you talk about secure , are you worried about secure transmission between aml and adf, or secure storage of your db query ? for the former, all communication between these two services will be done with https. for the latter, our production storage has its strict access control. besides, we only log the count of the global parameters and never the values. i believe it's secure to pass your db query as a global parameter to the web service."
    },
    {
        "Question_id":52208668.0,
        "Question_title":"tensorflow serving Error: Invalid argument: JSON object: does not have named input",
        "Question_body":"<p>I am trying to train a model with Amazon Sagemaker and I want serve it using with Tensorflow serving. To achieve that, I am downloading the model to a Tensorflow serving docker and I am trying to serve it from there.<\/p>\n<p>The Sagemaker's training and evaluating stages are completed without errors, but when I load my model to the Tensorflow serving server and try to invoke it I get Tensorflow serving errors that suggest that my model has no defined inputs. It can be seen that the Tensorflow serving server that the model is being served.<\/p>\n<p>For debugging purposes, I tried to serve it with Sagemaker but all I got was a vague error message saying I have an error invoking the endpoint.<\/p>\n<p>I think that the problem is that I am not defining well either the serving_input_fn or invoking it wrong or both. Can anyone help?<\/p>\n<h3>Tensorflow serving server invocation curl:<\/h3>\n<pre><code>curl -d '{&quot;instances&quot;: [{&quot;col3&quot;: 1.0}]}' -X POST http:\/\/localhost:8501\/v1\/models\/test_model:predict\n<\/code><\/pre>\n<h3>The error I receive from Tensorflow serving:<\/h3>\n<pre><code>{ &quot;error&quot;: &quot;Failed to process element: 0 key: col3 of \\'instances\\' list. Error: Invalid argument: JSON object: does not have named input: col3&quot; }%    \n<\/code><\/pre>\n<h3>Sagemaker's training python file:<\/h3>\n<pre><code>import os\nimport tensorflow as tf\nfrom tensorflow.python.ops import nn\n\n\nTRAIN_FILENAME = 'test.csv'\nTEST_FILENAME = 'train.csv'\n\nNODES_IN_LAYER = 6\nLAYERS_NUM = 10\nNUM_LINES_TO_SKIP = 1\n\nCSV_COLUMNS = ['col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8', 'label']\nRECORDS_DEFAULTS = [[0], [0], [0.0], [0.0], [0], [0.0], [0.0], [0], [0.0]]\n\nBATCH_SIZE = 32\n\nFEATURE_SPEC = {\n    'col3': tf.FixedLenFeature(dtype=tf.float32, shape=[]),\n}\n\n\ndef estimator_fn(run_config, params):\n    feature_columns = [\n        tf.feature_column.numeric_column('col3')]\n    return tf.estimator.DNNRegressor(feature_columns=feature_columns,\n                                     hidden_units=[NODES_IN_LAYER] * LAYERS_NUM,\n                                     activation_fn=nn.tanh,\n                                     config=run_config)\n\n\ndef serving_input_fn(params):\n    return tf.estimator.export.build_raw_serving_input_receiver_fn(FEATURE_SPEC)\n\n\ndef train_input_fn(training_dir, params):\n    &quot;&quot;&quot;Returns input function that would feed the model during training&quot;&quot;&quot;\n    return _generate_input_fn(training_dir, TRAIN_FILENAME)\n\n\ndef eval_input_fn(training_dir, params):\n    &quot;&quot;&quot;Returns input function that would feed the model during evaluation&quot;&quot;&quot;\n    return _generate_input_fn(training_dir, TEST_FILENAME)\n\n\ndef parse_csv(line):\n    columns = tf.decode_csv(line, record_defaults=RECORDS_DEFAULTS)\n    line_features = dict(zip(CSV_COLUMNS, columns))\n    line_label = line_features.pop('label')\n    return {'col3': line_features.pop('col3')}, line_label\n\n\ndef _generate_input_fn(training_dir, training_filename):\n    filename = os.path.join(training_dir, training_filename)\n    dataset = tf.data.TextLineDataset(filename)\n    dataset = dataset.skip(NUM_LINES_TO_SKIP).map(parse_csv).batch(BATCH_SIZE)\n    return dataset\n<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":0.0,
        "Question_creation_time":1536251773583,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":5954.0,
        "Owner_creation_time":1536227267183,
        "Owner_last_access_time":1561465737643,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1592644375060,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52208668",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tensorflow serving error: invalid argument: json object: does not have named input; Content: i am trying to train a model with and i want serve it using with tensorflow serving. to achieve that, i am downloading the model to a tensorflow serving docker and i am trying to serve it from there. the 's training and evaluating stages are completed without errors, but when i load my model to the tensorflow serving server and try to invoke it i get tensorflow serving errors that suggest that my model has no defined inputs. it can be seen that the tensorflow serving server that the model is being served. for debugging purposes, i tried to serve it with but all i got was a vague error message saying i have an error invoking the endpoint. i think that the problem is that i am not defining well either the serving_input_fn or invoking it wrong or both. can anyone help? tensorflow serving server invocation curl: curl -d '{\"instances\": [{\"col3\": 1.0}]}' -x post http:\/\/localhost:8501\/v1\/models\/test_model:predict the error i receive from tensorflow serving: { \"error\": \"failed to process element: 0 key: col3 of \\'instances\\' list. error: invalid argument: json object: does not have named input: col3\" }% 's training python file: import os import tensorflow as tf from tensorflow.python.ops import nn train_filename = 'test.csv' test_filename = 'train.csv' nodes_in_layer = 6 layers_num = 10 num_lines_to_skip = 1 csv_columns = ['col1', 'col2', 'col3', 'col4', 'col5', 'col6', 'col7', 'col8', 'label'] records_defaults = [[0], [0], [0.0], [0.0], [0], [0.0], [0.0], [0], [0.0]] batch_size = 32 feature_spec = { 'col3': tf.fixedlenfeature(dtype=tf.float32, shape=[]), } def estimator_fn(run_config, params): feature_columns = [ tf.feature_column.numeric_column('col3')] return tf.estimator.dnnregressor(feature_columns=feature_columns, hidden_units=[nodes_in_layer] * layers_num, activation_fn=nn.tanh, config=run_config) def serving_input_fn(params): return tf.estimator.export.build_raw_serving_input_receiver_fn(feature_spec) def train_input_fn(training_dir, params): \"\"\"returns input function that would feed the model during training\"\"\" return _generate_input_fn(training_dir, train_filename) def eval_input_fn(training_dir, params): \"\"\"returns input function that would feed the model during evaluation\"\"\" return _generate_input_fn(training_dir, test_filename) def parse_csv(line): columns = tf.decode_csv(line, record_defaults=records_defaults) line_features = dict(zip(csv_columns, columns)) line_label = line_features.pop('label') return {'col3': line_features.pop('col3')}, line_label def _generate_input_fn(training_dir, training_filename): filename = os.path.join(training_dir, training_filename) dataset = tf.data.textlinedataset(filename) dataset = dataset.skip(num_lines_to_skip).map(parse_csv).batch(batch_size) return dataset",
        "Question_original_content_gpt_summary":"The user is encountering an error when trying to serve a model with Tensorflow Serving, suggesting that the model has no defined inputs.",
        "Question_preprocessed_content":"Title: tensorflow serving error invalid argument json object does not have named input; Content: i am trying to train a model with and i want serve it using with tensorflow serving. to achieve that, i am downloading the model to a tensorflow serving docker and i am trying to serve it from there. the 's training and evaluating stages are completed without errors, but when i load my model to the tensorflow serving server and try to invoke it i get tensorflow serving errors that suggest that my model has no defined inputs. it can be seen that the tensorflow serving server that the model is being served. for debugging purposes, i tried to serve it with but all i got was a vague error message saying i have an error invoking the endpoint. i think that the problem is that i am not defining well either the or invoking it wrong or both. can anyone help? tensorflow serving server invocation curl the error i receive from tensorflow serving 's training python file",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":60748509.0,
        "Question_title":"AWS Sagemaker Notebook Stuck on Pending",
        "Question_body":"<p>I have an AWS Sagemaker notebook that is I attempted to launch again. The status of the notebook has been <code>Pending<\/code> for over 3 hours now. I've had a look at the Cloudwatch logs and the last few entry in there are: <\/p>\n\n<pre><code>[I 19:14:57.107 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[W 19:14:57.138 NotebookApp] No web browser found: could not locate runnable browser.\n[I 19:14:57.140 NotebookApp] Starting initial scan of virtual environments...\n[I 19:15:28.507 NotebookApp] Found new kernels in environments: conda_pytorch_p36, conda_amazonei_mxnet_p27, conda_chainer_p27, conda_mxnet_p27, conda_tensorflow_p27, conda_amazonei_tensorflow_p27, conda_amazonei_tensorflow_p36, conda_mxnet_p36, conda_python3, conda_tensorflow_p36, conda_python2, conda_pytorch_p27, conda_chainer_p36, conda_amazonei_mxnet_p36\n<\/code><\/pre>\n\n<p>There isn't anything in the logs the would indicate why it failed. Looking at that the last time I launched everything looks identical to that point. Is there anything I can do start the notebook or stop and relaunch the notebook?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1584571420993,
        "Question_favorite_count":1.0,
        "Question_score":1.0,
        "Question_view_count":3553.0,
        "Owner_creation_time":1374455933408,
        "Owner_last_access_time":1664077365950,
        "Owner_reputation":2306.0,
        "Owner_up_votes":1283.0,
        "Owner_down_votes":3.0,
        "Owner_views":182.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Montreal, QC, Canada",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60748509",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: notebook stuck on pending; Content: i have an notebook that is i attempted to launch again. the status of the notebook has been pending for over 3 hours now. i've had a look at the cloudwatch logs and the last few entry in there are: [i 19:14:57.107 notebookapp] use control-c to stop this server and shut down all kernels (twice to skip confirmation). [w 19:14:57.138 notebookapp] no web browser found: could not locate runnable browser. [i 19:14:57.140 notebookapp] starting initial scan of virtual environments... [i 19:15:28.507 notebookapp] found new kernels in environments: conda_pytorch_p36, conda_amazonei_mxnet_p27, conda_chainer_p27, conda_mxnet_p27, conda_tensorflow_p27, conda_amazonei_tensorflow_p27, conda_amazonei_tensorflow_p36, conda_mxnet_p36, conda_python3, conda_tensorflow_p36, conda_python2, conda_pytorch_p27, conda_chainer_p36, conda_amazonei_mxnet_p36 there isn't anything in the logs the would indicate why it failed. looking at that the last time i launched everything looks identical to that point. is there anything i can do start the notebook or stop and relaunch the notebook?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with their notebook, which has been stuck in a pending state for over 3 hours, and the cloudwatch logs do not indicate why it has failed.",
        "Question_preprocessed_content":"Title: notebook stuck on pending; Content: i have an notebook that is i attempted to launch again. the status of the notebook has been for over hours now. i've had a look at the cloudwatch logs and the last few entry in there are there isn't anything in the logs the would indicate why it failed. looking at that the last time i launched everything looks identical to that point. is there anything i can do start the notebook or stop and relaunch the notebook?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":54109316.0,
        "Question_title":"How to use secret keys in Azure Machine Learning Service pipelines",
        "Question_body":"<p>I am using Azure Machine Learning services and the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/concept-ml-pipelines\" rel=\"nofollow noreferrer\">pipeline functionality<\/a> for data preparation, training and testing of my Machine Learning models. However, during my data preparation step, I need to connect to a database and I want to find a way to pass my secret passwords or keys without writing them in plain text in my script files.<\/p>\n\n<p>Locally, I make use of environment variables for using secret passwords and keys, but to my best knowledge, this is impossible in the pipeline infrastructure, since Conda doesn't support passing environment variables. If anyone can confirm or deny this, it would be helpful.<\/p>\n\n<p>In the Azure Machine Learning services in the Azure Portal, I have found a 'key vault' resource, that is created automatically when I create a 'Machine Learning service workspace' resource. This seems to be exactly what I need. Is it? And if so, how do I use it?<\/p>\n\n<p>If neither of the above solves my issue, is there any other way to safely use secret passwords and keys in my scripts, without writing them in plain text in the scripts?<\/p>\n\n<p>EDIT: I realize my question have a strong focus on database connections. However, the question is really about any kinds of secrets or passwords, not just database credentials. As have been pointed out in an answer, that could be worth mentioning here, is that Azure SQL database connections can (and should) be solved using the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-steps\/azureml.pipeline.steps.data_transfer_step.datatransferstep?view=azure-ml-py\" rel=\"nofollow noreferrer\">DataTransferStep<\/a>.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1547034090033,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":2044.0,
        "Owner_creation_time":1463756509236,
        "Owner_last_access_time":1662389533070,
        "Owner_reputation":400.0,
        "Owner_up_votes":146.0,
        "Owner_down_votes":0.0,
        "Owner_views":43.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Uppsala, Sverige",
        "Question_last_edit_time":1562822197676,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54109316",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to use secret keys in service pipelines; Content: i am using services and the pipeline functionality for data preparation, training and testing of my machine learning models. however, during my data preparation step, i need to connect to a database and i want to find a way to pass my secret passwords or keys without writing them in plain text in my script files. locally, i make use of environment variables for using secret passwords and keys, but to my best knowledge, this is impossible in the pipeline infrastructure, since conda doesn't support passing environment variables. if anyone can confirm or deny this, it would be helpful. in the services in the azure portal, i have found a 'key vault' resource, that is created automatically when i create a 'machine learning service workspace' resource. this seems to be exactly what i need. is it? and if so, how do i use it? if neither of the above solves my issue, is there any other way to safely use secret passwords and keys in my scripts, without writing them in plain text in the scripts? edit: i realize my question have a strong focus on database connections. however, the question is really about any kinds of secrets or passwords, not just database credentials. as have been pointed out in an answer, that could be worth mentioning here, is that azure sql database connections can (and should) be solved using the datatransferstep.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with securely passing secret passwords or keys to their service pipelines without writing them in plain text in their script files.",
        "Question_preprocessed_content":"Title: how to use secret keys in service pipelines; Content: i am using services and the pipeline functionality for data preparation, training and testing of my machine learning models. however, during my data preparation step, i need to connect to a database and i want to find a way to pass my secret passwords or keys without writing them in plain text in my script files. locally, i make use of environment variables for using secret passwords and keys, but to my best knowledge, this is impossible in the pipeline infrastructure, since conda doesn't support passing environment variables. if anyone can confirm or deny this, it would be helpful. in the services in the azure portal, i have found a 'key vault' resource, that is created automatically when i create a 'machine learning service workspace' resource. this seems to be exactly what i need. is it? and if so, how do i use it? if neither of the above solves my issue, is there any other way to safely use secret passwords and keys in my scripts, without writing them in plain text in the scripts? edit i realize my question have a strong focus on database connections. however, the question is really about any kinds of secrets or passwords, not just database credentials. as have been pointed out in an answer, that could be worth mentioning here, is that azure sql database connections can be solved using the datatransferstep.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70516388.0,
        "Question_title":"Defining routes in MLflow serving",
        "Question_body":"<p>When we serve mlflow model we define different ports for each serving and to access these models we use IP:port\/invocations<\/p>\n<p>ex:<\/p>\n<p>app 1 : IP:2020\/invocations<\/p>\n<p>app 2 : IP:2021\/invocations<\/p>\n<p>But I want to serve 2 mlflow models at same port with different routes.<\/p>\n<p>ex:<\/p>\n<p>app 1 : IP:2020\/app1<\/p>\n<p>app 2 : IP:2020\/app2<\/p>\n<p>How can I achieve it using MLflow.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1640763158347,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Owner_creation_time":1504498312456,
        "Owner_last_access_time":1663747537620,
        "Owner_reputation":334.0,
        "Owner_up_votes":105.0,
        "Owner_down_votes":8.0,
        "Owner_views":24.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1640842613347,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70516388",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: defining routes in serving; Content: when we serve model we define different ports for each serving and to access these models we use ip:port\/invocations ex: app 1 : ip:2020\/invocations app 2 : ip:2021\/invocations but i want to serve 2 models at same port with different routes. ex: app 1 : ip:2020\/app1 app 2 : ip:2020\/app2 how can i achieve it using .",
        "Question_original_content_gpt_summary":"The user is encountering challenges in defining routes in serving, where they want to serve two models at the same port with different routes.",
        "Question_preprocessed_content":"Title: defining routes in serving; Content: when we serve model we define different ports for each serving and to access these models we use ex app app but i want to serve models at same port with different routes. ex app app how can i achieve it using .",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":41485715.0,
        "Question_title":"Training Model for Each Individual in AzureML",
        "Question_body":"<p>I want to train an ANN model for each individual, in azure ml. For example, there is an application which wants to learn the behavior of each individual separately. How is this possible in azure-ml? Any suggestion?<\/p>\n\n<p>As I know, I can create a model and train it with some data, but I don't know how can I train it specifically for each user. I should mention that I am seeking for a scalable idea which is applicable for a real situation (might be for 100 thousands users).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1483620989100,
        "Question_favorite_count":1.0,
        "Question_score":2.0,
        "Question_view_count":205.0,
        "Owner_creation_time":1403553737940,
        "Owner_last_access_time":1664024595667,
        "Owner_reputation":17835.0,
        "Owner_up_votes":636.0,
        "Owner_down_votes":1612.0,
        "Owner_views":2203.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Belgium",
        "Question_last_edit_time":1485439340663,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41485715",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: training model for each individual in ; Content: i want to train an ann model for each individual, in . for example, there is an application which wants to learn the behavior of each individual separately. how is this possible in azure-ml? any suggestion? as i know, i can create a model and train it with some data, but i don't know how can i train it specifically for each user. i should mention that i am seeking for a scalable idea which is applicable for a real situation (might be for 100 thousands users).",
        "Question_original_content_gpt_summary":"The user is seeking a scalable solution to train an ANN model for each individual in a real-world situation with potentially 100,000 users.",
        "Question_preprocessed_content":"Title: training model for each individual in; Content: i want to train an ann model for each individual, in . for example, there is an application which wants to learn the behavior of each individual separately. how is this possible in ? any suggestion? as i know, i can create a model and train it with some data, but i don't know how can i train it specifically for each user. i should mention that i am seeking for a scalable idea which is applicable for a real situation .",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":57946451.0,
        "Question_title":"Incremental learning with a built-in sagemaker algorithm",
        "Question_body":"<p>I am training the DeepAR AWS SageMaker built-in algorithm. With the sagemaker SDK, I can train the model with particular specified hyper-parameters:<\/p>\n\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    sagemaker_session=sagemaker_session,\n    image_name=image_name,\n    role=role,\n    train_instance_count=1,\n    train_instance_type='ml.c4.2xlarge',\n    base_job_name='wfp-deepar',\n    output_path=join(s3_path, 'output')\n)\n\nestimator.set_hyperparameters(**{\n    'time_freq': 'M',\n    'epochs': '50',\n    'mini_batch_size': '96',\n    'learning_rate': '1E-3',\n    'context_length': '12',\n    'dropout_rate': 0,\n    'prediction_length': '12'\n})\n\nestimator.fit(inputs=data_channels, wait=True, job_name='wfp-deepar-job-level-5')\n<\/code><\/pre>\n\n<p>I would like to train the resulting model again with a <strong>smaller learning rate<\/strong>. I followed the incremental training method described here: <a href=\"https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/en_pv\/sagemaker\/latest\/dg\/incremental-training.html<\/a>, but it does not work, apparently (according to the link), only two built-in models support incremental learning. <\/p>\n\n<p>Has anyone found a workaround for this so that they can train a built-in algorithm with a scheduled learning rate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1568567773507,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":225.0,
        "Owner_creation_time":1445974115332,
        "Owner_last_access_time":1664071374768,
        "Owner_reputation":5939.0,
        "Owner_up_votes":1063.0,
        "Owner_down_votes":11.0,
        "Owner_views":324.0,
        "Answer_body":"<p>Unfortunately, the SageMaker built-in DeepAR model doesn't support learning rate scheduling nor incremental learning.  If you want to implement learning rate plateau schedule on a DeepAR architecture I recommend to consider:<\/p>\n\n<ul>\n<li>using the open-source DeepAR implementation (<a href=\"https:\/\/gluon-ts.mxnet.io\/api\/gluonts\/gluonts.model.deepar.html\" rel=\"nofollow noreferrer\">code<\/a>, <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/creating-neural-time-series-models-with-gluon-time-series\/\" rel=\"nofollow noreferrer\">demo<\/a>)<\/li>\n<li>or using the <a href=\"https:\/\/docs.aws.amazon.com\/forecast\/latest\/dg\/aws-forecast-recipe-deeparplus.html\" rel=\"nofollow noreferrer\">DeepAR+ algo of the Amazon Forecast service<\/a>, that features learning rate scheduling ability.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1568630636312,
        "Answer_score":2.0,
        "Owner_location":"Washington, United States",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57946451",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: incremental learning with a built-in algorithm; Content: i am training the deepar built-in algorithm. with the sdk, i can train the model with particular specified hyper-parameters: estimator = .estimator.estimator( _session=_session, image_name=image_name, role=role, train_instance_count=1, train_instance_type='ml.c4.2xlarge', base_job_name='wfp-deepar', output_path=join(s3_path, 'output') ) estimator.set_hyperparameters(**{ 'time_freq': 'm', 'epochs': '50', 'mini_batch_size': '96', 'learning_rate': '1e-3', 'context_length': '12', 'dropout_rate': 0, 'prediction_length': '12' }) estimator.fit(inputs=data_channels, wait=true, job_name='wfp-deepar-job-level-5') i would like to train the resulting model again with a smaller learning rate. i followed the incremental training method described here: https:\/\/docs.aws.amazon.com\/en_pv\/\/latest\/dg\/incremental-training.html, but it does not work, apparently (according to the link), only two built-in models support incremental learning. has anyone found a workaround for this so that they can train a built-in algorithm with a scheduled learning rate?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with incremental learning using the DeepAR built-in algorithm, as the method described in the AWS documentation does not work for this algorithm.",
        "Question_preprocessed_content":"Title: incremental learning with a built in algorithm; Content: i am training the deepar built in algorithm. with the sdk, i can train the model with particular specified hyper parameters i would like to train the resulting model again with a smaller learning rate. i followed the incremental training method described here but it does not work, apparently , only two built in models support incremental learning. has anyone found a workaround for this so that they can train a built in algorithm with a scheduled learning rate?",
        "Answer_original_content":"unfortunately, the built-in deepar model doesn't support learning rate scheduling nor incremental learning. if you want to implement learning rate plateau schedule on a deepar architecture i recommend to consider: using the open-source deepar implementation (code, demo) or using the deepar+ algo of the amazon forecast service, that features learning rate scheduling ability.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"unfortunately, the built in deepar model doesn't support learning rate scheduling nor incremental learning. if you want to implement learning rate plateau schedule on a deepar architecture i recommend to consider using the open source deepar implementation or using the deepar+ algo of the amazon forecast service, that features learning rate scheduling ability."
    },
    {
        "Question_id":67161293.0,
        "Question_title":"Issues accessing a FileDataset created from HTTP URIs in a PythonScriptStep",
        "Question_body":"<p>I\u2019m having some issues trying to access a FileDataset created from two http URIs in an Azure ML Pipeline PythonScriptStep.<\/p>\n<p>In the step, I\u2019m only getting a single file named <code>['https%3A\u2019]<\/code> when doing an <code>os.listdir()<\/code> on my mount point. I would have expected two files, with their actual names instead. This happens both when sending the dataset <code>as_upload<\/code> and <code>as_mount<\/code>. Even happens when I send the dataset reference to the pipeline step and mount it directly from the step.<\/p>\n<p>The dataset is registered in a notebook, the same notebook that creates and invokes the pipeline, as seen below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>tempFileData = Dataset.File.from_files(\n        ['https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg',\n        'https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg'])\ntempFileData.register(ws, name='FileData', create_new_version=True)\n\n#...\n\nread_datasets_step = PythonScriptStep(\n    name='The Dataset Reader',\n    script_name='read-datasets.py',\n    inputs=[fileData.as_named_input('Files'), fileData.as_named_input('Files_mount').as_mount(), fileData.as_named_input('Files_download').as_download()],\n    compute_target=compute_target,\n    source_directory='.\/dataset-reader',\n    allow_reuse=False,\n)\n\n<\/code><\/pre>\n<p>The <code>FileDataset<\/code> seems to be registered properly, if I examine it within the notebook I get the following result:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [\n    &quot;https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot;,\n    &quot;https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;\n  ],\n  &quot;definition&quot;: [\n    &quot;GetFiles&quot;\n  ],\n  &quot;registration&quot;: {\n    &quot;id&quot;: &quot;...&quot;,\n    &quot;name&quot;: &quot;FileData&quot;,\n    &quot;version&quot;: 4,\n    &quot;workspace&quot;: &quot;Workspace.create(...)&quot;\n  }\n}\n<\/code><\/pre>\n<p>For reference, the machine running the notebook is using AML SDK v1.24, whereas the node running the pipeline steps is running v1.25.<\/p>\n<p>Has anybody encountered anything like this? Is there a way to make it work?<\/p>\n<p>Note that I'm specifically looking at file datasets created from web uris, and not necessarily interested in getting a <code>FileDataset<\/code> to work with blob storage or similar.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1618832324140,
        "Question_favorite_count":1.0,
        "Question_score":1.0,
        "Question_view_count":91.0,
        "Owner_creation_time":1250158552416,
        "Owner_last_access_time":1663847198323,
        "Owner_reputation":7916.0,
        "Owner_up_votes":1735.0,
        "Owner_down_votes":33.0,
        "Owner_views":801.0,
        "Answer_body":"<p>The files should've been mounted at path &quot;https%3A\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot; and &quot;https%3A\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;.<\/p>\n<p>We retain the directory structure following the url structure to avoid potential conflicts.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618855169223,
        "Answer_score":2.0,
        "Owner_location":"Romania",
        "Question_last_edit_time":1618849094430,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67161293",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: issues accessing a filedataset created from http uris in a pythonscriptstep; Content: i\u2019m having some issues trying to access a filedataset created from two http uris in an pipeline pythonscriptstep. in the step, i\u2019m only getting a single file named ['https%3a\u2019] when doing an os.listdir() on my mount point. i would have expected two files, with their actual names instead. this happens both when sending the dataset as_upload and as_mount. even happens when i send the dataset reference to the pipeline step and mount it directly from the step. the dataset is registered in a notebook, the same notebook that creates and invokes the pipeline, as seen below: tempfiledata = dataset.file.from_files( ['https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg', 'https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg']) tempfiledata.register(ws, name='filedata', create_new_version=true) #... read_datasets_step = pythonscriptstep( name='the dataset reader', script_name='read-datasets.py', inputs=[filedata.as_named_input('files'), filedata.as_named_input('files_mount').as_mount(), filedata.as_named_input('files_download').as_download()], compute_target=compute_target, source_directory='.\/dataset-reader', allow_reuse=false, ) the filedataset seems to be registered properly, if i examine it within the notebook i get the following result: { \"source\": [ \"https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg\", \"https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg\" ], \"definition\": [ \"getfiles\" ], \"registration\": { \"id\": \"...\", \"name\": \"filedata\", \"version\": 4, \"workspace\": \"workspace.create(...)\" } } for reference, the machine running the notebook is using aml sdk v1.24, whereas the node running the pipeline steps is running v1.25. has anybody encountered anything like this? is there a way to make it work? note that i'm specifically looking at file datasets created from web uris, and not necessarily interested in getting a filedataset to work with blob storage or similar.",
        "Question_original_content_gpt_summary":"The user is encountering issues accessing a filedataset created from two http uris in a pipeline pythonscriptstep.",
        "Question_preprocessed_content":"Title: issues accessing a filedataset created from http uris in a pythonscriptstep; Content: im having some issues trying to access a filedataset created from two http uris in an pipeline pythonscriptstep. in the step, im only getting a single file named when doing an on my mount point. i would have expected two files, with their actual names instead. this happens both when sending the dataset and . even happens when i send the dataset reference to the pipeline step and mount it directly from the step. the dataset is registered in a notebook, the same notebook that creates and invokes the pipeline, as seen below the seems to be registered properly, if i examine it within the notebook i get the following result for reference, the machine running the notebook is using aml sdk whereas the node running the pipeline steps is running has anybody encountered anything like this? is there a way to make it work? note that i'm specifically looking at file datasets created from web uris, and not necessarily interested in getting a to work with blob storage or similar.",
        "Answer_original_content":"the files should've been mounted at path \"https%3a\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg\" and \"https%3a\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg\". we retain the directory structure following the url structure to avoid potential conflicts.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"the files should've been mounted at path and we retain the directory structure following the url structure to avoid potential conflicts."
    },
    {
        "Question_id":49775557.0,
        "Question_title":"How can I invoke a SageMaker model, trained with TensorFlow, using a csv file in the body of the call?",
        "Question_body":"<p>I have deployed a TensorFlow model on AWS SageMaker, and I want to be able to invoke it using a csv file as the body of the call. The documentation says about creating a <code>serving_input_function<\/code> like the one below: <\/p>\n\n<pre><code>def serving_input_fn(hyperparameters):\n  # Logic to the following:\n  # 1. Defines placeholders that TensorFlow serving will feed with inference requests\n  # 2. Preprocess input data\n  # 3. Returns a tf.estimator.export.ServingInputReceiver or tf.estimator.export.TensorServingInputReceiver,\n  # which packages the placeholders and the resulting feature Tensors together.\n<\/code><\/pre>\n\n<p>In step 2, where it says preprocess input data, how do I get a handle on input data to process them?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1523450778890,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1249.0,
        "Owner_creation_time":1423231864696,
        "Owner_last_access_time":1555605940390,
        "Owner_reputation":1109.0,
        "Owner_up_votes":29.0,
        "Owner_down_votes":0.0,
        "Owner_views":157.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Athens, Greece",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49775557",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i invoke a model, trained with tensorflow, using a csv file in the body of the call?; Content: i have deployed a tensorflow model on , and i want to be able to invoke it using a csv file as the body of the call. the documentation says about creating a serving_input_function like the one below: def serving_input_fn(hyperparameters): # logic to the following: # 1. defines placeholders that tensorflow serving will feed with inference requests # 2. preprocess input data # 3. returns a tf.estimator.export.servinginputreceiver or tf.estimator.export.tensorservinginputreceiver, # which packages the placeholders and the resulting feature tensors together. in step 2, where it says preprocess input data, how do i get a handle on input data to process them?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in invoking a TensorFlow model using a CSV file as the body of the call, and is seeking guidance on how to preprocess the input data.",
        "Question_preprocessed_content":"Title: how can i invoke a model, trained with tensorflow, using a csv file in the body of the call?; Content: i have deployed a tensorflow model on , and i want to be able to invoke it using a csv file as the body of the call. the documentation says about creating a like the one below in step , where it says preprocess input data, how do i get a handle on input data to process them?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Gdrive-user-credentials.json is missing",
        "Question_body":"<p>Hello<br>\nI\u2019m learning CI with DVC and GitHub actions and I have a problem: the file drive-user-credentials.json cant be found in my .dvc directory even after dvc push\/pull as on tutorials. I read all material on this particular matter available on DVC page but could not find an answer. Can you help me? Without this file, GitHub actions do not have the authorization to run my pipeline.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1672953158705,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":78.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/gdrive-user-credentials-json-is-missing\/1453",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-06T12:36:14.310Z",
                "Answer_body":"<p>Hi, Tiago.<\/p>\n<p>The credential file of gdrive will be generated after the first time you try to connect to the server and login into your account from the browser.<\/p>",
                "Answer_score":21.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-06T13:12:47.132Z",
                "Answer_body":"<p>Hello, Yanxiang. Thanks for replying.<\/p>\n<p>Yes. After login, the file is supposed to be at my_project_path\/.dvc\/tmp\/, right? But I cant find it there. This is my tmp directory tree:<\/p>\n<p>tmp<br>\n\u251c\u2500\u2500 hashes<br>\n\u2502   \u2514\u2500\u2500 local<br>\n\u2502       \u2514\u2500\u2500 cache.db<br>\n\u251c\u2500\u2500 index<br>\n\u2502   \u2514\u2500\u2500 a67465ebfb6db11959d405312e9e0388803482ab0192da9db4bcc92d42bba0db<br>\n\u2502       \u2514\u2500\u2500 cache.db<br>\n\u251c\u2500\u2500 links<br>\n\u2502   \u2514\u2500\u2500 cache.db<br>\n\u251c\u2500\u2500 lock<br>\n\u251c\u2500\u2500 rwlock<br>\n\u2514\u2500\u2500 rwlock.lock<\/p>\n<p>Have I missed something or did something during installation? Btw, I don\u2019t know whether this is relevant but I\u2019m using anaconda.<\/p>\n<p>Thanks for your help.<\/p>",
                "Answer_score":16.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-06T23:04:49.650Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/tiagoborelli\">@tiagoborelli<\/a> .<\/p>\n<p>It should be $CACHE_HOME\/pydrive2fs\/{gdrive_client_id}\/default.json by default. What tutorials are you using? Looks like it is outdated and we need to fix it up. The docs in <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\/modify#google-drive\" class=\"inline-onebox\">remote modify<\/a> and in <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote\" class=\"inline-onebox\">How to Setup a Google Drive DVC Remote<\/a> are pointing to the correct location, so I\u2019m a bit confused about which tutorial you are talking about.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":21.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-06T23:12:31.896Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"tiagoborelli\" data-post=\"1\" data-topic=\"1453\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/t\/ecae2f\/40.png\" class=\"avatar\"> tiagoborelli:<\/div>\n<blockquote>\n<p>file drive-user-credentials.json cant be found in my .dvc directory<\/p>\n<\/blockquote>\n<\/aside>\n<p><a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> He\u2019s referring to the Getting Started Tutorial!  I found a comment from there too in YouTube.  This video is old.  It may be due for updating!  Sorry for your frustration <a class=\"mention\" href=\"\/u\/tiagoborelli\">@tiagoborelli<\/a>! We will work on what needs to be corrected in the video. You can also checkout our free online course which was created more recently at <a href=\"https:\/\/learn.iterative.ai\">https:\/\/learn.iterative.ai<\/a><\/p>",
                "Answer_score":71.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"",
        "Answer_original_content":"",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":""
    },
    {
        "Question_id":null,
        "Question_title":"Mlflow integration with Google's Vertex AI",
        "Question_body":"Hi,\n\n\n\u00a0 Is there any way through which we can integrate mlflow with vertex AI ?\u00a0\nAny articles or resources I can go through ?\n\nThanks\nAvinash",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1645165866000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":82.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/q9759ED2C2A",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[

        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: integration with google's vertex ai; Content: hi, is there any way through which we can integrate with vertex ai ? any articles or resources i can go through ? thanks avinash",
        "Question_original_content_gpt_summary":"The user is looking for resources to help them integrate with Google's Vertex AI.",
        "Question_preprocessed_content":"Title: integration with google's vertex ai; Content: hi, is there any way through which we can integrate with vertex ai ? any articles or resources i can go through ? thanks avinash",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Dask scheduler not using multiple gpus on remote",
        "Question_body":"<p>Hi,<br>\nIt\u2019s a fresh feature and most likely I\u2019m doing something wrong, but I cannot get the Dask scheduler to work properly on a remote with 4 gpus.<\/p>\n<p>This is what I do, and apart from the remote part it is basically a copy of the steps in the How To guide:<\/p>\n<ol>\n<li>\n<p>Connect to remote. I have successfully staged runs on remote, run them directly, and also used multiple gpus by assigning runs to gpus manually using --gpus flag. So the remote works corectly.<\/p>\n<\/li>\n<li>\n<p>Start the Dask scheduler on the remote.<\/p>\n<\/li>\n<li>\n<p>Then I stage trials on remote, let\u2019s say 4, with different parameters, using a single command.<\/p>\n<\/li>\n<\/ol>\n<pre><code>guild run TABL:train window=[100,200,300,400] --remote cerberus --stage-trials\n<\/code><\/pre>\n<ol start=\"4\">\n<li>The trials are sent to remote, and the scheduler starts them. If workers is set to 4, it starts correctly 4 processes of loading data etc.<\/li>\n<\/ol>\n<p>And this is where something goes wrong. After doing the pre-procesing stage and creating 4 models concurrently, the scheduler places all 4 models and training processses on all 4 gpus (which can be seen on the screenshot - all gpus have allocated memory). And then only begins the training on 1 gpu.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png\" data-download-href=\"\/uploads\/short-url\/2PTbvO3vWfJgJUE57EqoCMII9Jc.png?dl=1\" title=\"Screenshot from 2021-03-29 18-26-25\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_490x500.png\" alt=\"Screenshot from 2021-03-29 18-26-25\" data-base62-sha1=\"2PTbvO3vWfJgJUE57EqoCMII9Jc\" width=\"490\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_490x500.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/13e1f55d40cd637c5f942a9330bfa853e5a3ccf6_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot from 2021-03-29 18-26-25<\/span><span class=\"informations\">731\u00d7745 103 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p><strong>Expected<\/strong><br>\nI would expect the scheduler to assign the trials to available gpus and train them concurrently. Furthermore, when the number of trials is bigger than the number of gpus, I would expect the scheduler to automatically run the pending operation when a gpus becomes free.<\/p>\n<p>Did I understand what the scheduler is capable of correctly? Is there maybe some manual step somewhere that I missed?<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1617036688225,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":457.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/dask-scheduler-not-using-multiple-gpus-on-remote\/583",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-31T22:19:46.513Z",
                "Answer_body":"<p>By default the scheduler will simply run the staged runs concurrently, up to the number of workers specified. It doesn\u2019t assign runs to GPUs. This is a good feature idea, but the initial pass doesn\u2019t do this.<\/p>\n<p>Instead, you need to <a href=\"https:\/\/my.guild.ai\/t\/parallel-processing-with-dask-scheduler\/550#scheduling-runs-on-specific-gpus\">assign runs to GPUs explicitly<\/a>. This uses Dask resources to limit runs, as well as configuring each run for the target GPU.<\/p>\n<p>The real target is to support this with just the <code>run<\/code> command\u2014so you don\u2019t need to mess with schedulers. But this first pass introduced concurrency with Dask distributed and so exposes the resource based scheduling that Dask offers.<\/p>\n<p>If you have any questions about the steps in <a href=\"https:\/\/my.guild.ai\/t\/parallel-processing-with-dask-scheduler\/550#scheduling-runs-on-specific-gpus\">Scheduling Runs on Specific GPUs<\/a> please feel free to followup here. We\u2019ll look for ways to clean this up, either via better docs or tweaks to the code.<\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-05T07:03:28.712Z",
                "Answer_body":"<p>Ok, thanks for the clarification, I\u2019ll continue assigning the runs manually <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: dask scheduler not using multiple gpus on remote; Content: hi, it\u2019s a fresh feature and most likely i\u2019m doing something wrong, but i cannot get the dask scheduler to work properly on a remote with 4 gpus. this is what i do, and apart from the remote part it is basically a copy of the steps in the how to guide: connect to remote. i have successfully staged runs on remote, run them directly, and also used multiple gpus by assigning runs to gpus manually using --gpus flag. so the remote works corectly. start the dask scheduler on the remote. then i stage trials on remote, let\u2019s say 4, with different parameters, using a single command. guild run tabl:train window=[100,200,300,400] --remote cerberus --stage-trials the trials are sent to remote, and the scheduler starts them. if workers is set to 4, it starts correctly 4 processes of loading data etc. and this is where something goes wrong. after doing the pre-procesing stage and creating 4 models concurrently, the scheduler places all 4 models and training processses on all 4 gpus (which can be seen on the screenshot - all gpus have allocated memory). and then only begins the training on 1 gpu. screenshot from 2021-03-29 18-26-25731\u00d7745 103 kb expected i would expect the scheduler to assign the trials to available gpus and train them concurrently. furthermore, when the number of trials is bigger than the number of gpus, i would expect the scheduler to automatically run the pending operation when a gpus becomes free. did i understand what the scheduler is capable of correctly? is there maybe some manual step somewhere that i missed? thanks in advance.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with the Dask scheduler not using multiple GPUs on a remote, resulting in all models and training processes being placed on all GPUs but only one GPU being used for training.",
        "Question_preprocessed_content":"Title: dask scheduler not using multiple gpus on remote; Content: hi, its a fresh feature and most likely im doing something wrong, but i cannot get the dask scheduler to work properly on a remote with gpus. this is what i do, and apart from the remote part it is basically a copy of the steps in the how to guide connect to remote. i have successfully staged runs on remote, run them directly, and also used multiple gpus by assigning runs to gpus manually using gpus flag. so the remote works corectly. start the dask scheduler on the remote. then i stage trials on remote, lets say , with different parameters, using a single command. the trials are sent to remote, and the scheduler starts them. if workers is set to , it starts correctly processes of loading data etc. and this is where something goes wrong. after doing the pre procesing stage and creating models concurrently, the scheduler places all models and training processses on all gpus . and then only begins the training on gpu. screenshot from kb expected i would expect the scheduler to assign the trials to available gpus and train them concurrently. furthermore, when the number of trials is bigger than the number of gpus, i would expect the scheduler to automatically run the pending operation when a gpus becomes free. did i understand what the scheduler is capable of correctly? is there maybe some manual step somewhere that i missed? thanks in advance.",
        "Answer_original_content":"by default the scheduler will simply run the staged runs concurrently, up to the number of workers specified. it doesnt assign runs to gpus. this is a good feature idea, but the initial pass doesnt do this. instead, you need to assign runs to gpus explicitly. this uses dask resources to limit runs, as well as configuring each run for the target gpu. the real target is to support this with just the run commandso you dont need to mess with schedulers. but this first pass introduced concurrency with dask distributed and so exposes the resource based scheduling that dask offers. if you have any questions about the steps in scheduling runs on specific gpus please feel free to followup here. well look for ways to clean this up, either via better docs or tweaks to the code. ok, thanks for the clarification, ill continue assigning the runs manually",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"by default the scheduler will simply run the staged runs concurrently, up to the number of workers specified. it doesnt assign runs to gpus. this is a good feature idea, but the initial pass doesnt do this. instead, you need to assign runs to gpus explicitly. this uses dask resources to limit runs, as well as configuring each run for the target gpu. the real target is to support this with just the commandso you dont need to mess with schedulers. but this first pass introduced concurrency with dask distributed and so exposes the resource based scheduling that dask offers. if you have any questions about the steps in scheduling runs on specific gpus please feel free to followup here. well look for ways to clean this up, either via better docs or tweaks to the code. ok, thanks for the clarification, ill continue assigning the runs manually"
    },
    {
        "Question_id":57552455.0,
        "Question_title":"How to fix 'incorrect artifact\/model path on HDFS showing on MLflow server'",
        "Question_body":"<p>I run a mlflow server with the following command using mlflow, version 1.2.0<\/p>\n\n<pre><code>mlflow server --host myhost -p myport --backend-store-uri mysql:\/\/user@localhost\/mlflow --default-artifact-root hdfs:\/\/myhost\/user\/myid\/mlflow_test\n<\/code><\/pre>\n\n<p>I run the experiment from MLflow tutorial quickstart <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">https:\/\/www.mlflow.org\/docs\/latest\/quickstart.html<\/a><\/p>\n\n<p>the command:<\/p>\n\n<pre><code>mlflow run sklearn_elasticnet_wine -P alpha=0.5 --no-conda\n<\/code><\/pre>\n\n<p>the code to log the model is <\/p>\n\n<pre><code>mlflow.sklearn.log_model(lr, \"model\")\n<\/code><\/pre>\n\n<p>in <\/p>\n\n<pre><code>https:\/\/github.com\/mlflow\/mlflow\/blob\/master\/examples\/sklearn_elasticnet_wine\/train.py\n<\/code><\/pre>\n\n<p>I visit the server by webbrowser myhost: myport and check the run I ran.<\/p>\n\n<p>I successfully get the ran info by myhost: <code>myport\/#\/experiments\/0\/runs\/run_id<\/code><\/p>\n\n<p>in this page, i found that the first layer (model directory) path is correct. that is, <code>run_id\/artifacts\/model<\/code>\n<a href=\"https:\/\/i.stack.imgur.com\/bJZWv.png\" rel=\"nofollow noreferrer\">correct path<\/a><\/p>\n\n<p>but once I click the MLmodel file under model folder, the path get wrong:\nI expect to see        <code>run_id\/artifacts\/model\/MLmodel<\/code>\nbut actually it was    <code>run_id\/artifacts\/MLmodel<\/code>\n<a href=\"https:\/\/i.stack.imgur.com\/WIhVR.png\" rel=\"nofollow noreferrer\">wrong path<\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1566200834727,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":812.0,
        "Owner_creation_time":1566200326392,
        "Owner_last_access_time":1577419329052,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1566208349487,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57552455",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to fix 'incorrect artifact\/model path on hdfs showing on server'; Content: i run a server with the following command using , version 1.2.0 server --host myhost -p myport --backend-store-uri mysql:\/\/user@localhost\/ --default-artifact-root hdfs:\/\/myhost\/user\/myid\/_test i run the experiment from tutorial quickstart https:\/\/www..org\/docs\/latest\/quickstart.html the command: run sklearn_elasticnet_wine -p alpha=0.5 --no-conda the code to log the model is .sklearn.log_model(lr, \"model\") in https:\/\/github.com\/\/\/blob\/master\/examples\/sklearn_elasticnet_wine\/train.py i visit the server by webbrowser myhost: myport and check the run i ran. i successfully get the ran info by myhost: myport\/#\/experiments\/0\/runs\/run_id in this page, i found that the first layer (model directory) path is correct. that is, run_id\/artifacts\/model correct path but once i click the mlmodel file under model folder, the path get wrong: i expect to see run_id\/artifacts\/model\/mlmodel but actually it was run_id\/artifacts\/mlmodel wrong path",
        "Question_original_content_gpt_summary":"The user encountered a challenge where the incorrect artifact\/model path on HDFS was showing on the server when attempting to log a model using the sklearn_elasticnet_wine command.",
        "Question_preprocessed_content":"Title: how to fix 'incorrect path on hdfs showing on server'; Content: i run a server with the following command using , version i run the experiment from tutorial quickstart the command the code to log the model is in i visit the server by webbrowser myhost myport and check the run i ran. i successfully get the ran by myhost in this page, i found that the first layer path is correct. that is, correct path but once i click the mlmodel file under model folder, the path get wrong i expect to see but actually it was wrong path",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":54431484.0,
        "Question_title":"Running scripts from a datastore on Azure Machine Learning Service",
        "Question_body":"<p>I am migrating from Batch AI to the new Azure Machine Learning Service. Previously I had my python scripts on an Azure Files share and those scripts ran directly from there.<\/p>\n\n<p>In the new service when you create an Estimator you have to provide a source directory and an entry script. The documentation states the source directory is a local directory that is copied to the remote computer.<\/p>\n\n<p>However, the Estimator constructor also allows you to specify a datastore name that is supposed to specify the datastore for the project share.<\/p>\n\n<p>To me, this sounds like you can specify a datastore and then the source directory is relative to that however this does not work, it still wants to find the source directory on the local machine.<\/p>\n\n<pre><code>tf_est = TensorFlow(source_directory='.\/script',\n                source_directory_data_store=ds,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script='helloworld.py',\n                use_gpu=False)\n<\/code><\/pre>\n\n<p>Does anybody know if its possible to run a training job using a datastore for execution?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1548806606603,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":141.0,
        "Owner_creation_time":1336005137536,
        "Owner_last_access_time":1663876727532,
        "Owner_reputation":752.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":4.0,
        "Owner_views":117.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1554813331196,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54431484",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: running scripts from a datastore on service; Content: i am migrating from batch ai to the new service. previously i had my python scripts on an azure files share and those scripts ran directly from there. in the new service when you create an estimator you have to provide a source directory and an entry script. the documentation states the source directory is a local directory that is copied to the remote computer. however, the estimator constructor also allows you to specify a datastore name that is supposed to specify the datastore for the project share. to me, this sounds like you can specify a datastore and then the source directory is relative to that however this does not work, it still wants to find the source directory on the local machine. tf_est = tensorflow(source_directory='.\/script', source_directory_data_store=ds, script_params=script_params, compute_target=compute_target, entry_script='helloworld.py', use_gpu=false) does anybody know if its possible to run a training job using a datastore for execution?",
        "Question_original_content_gpt_summary":"The user is encountering challenges running scripts from a datastore on the new service, as the estimator constructor allows them to specify a datastore but still wants to find the source directory on the local machine.",
        "Question_preprocessed_content":"Title: running scripts from a datastore on service; Content: i am migrating from batch ai to the new service. previously i had my python scripts on an azure files share and those scripts ran directly from there. in the new service when you create an estimator you have to provide a source directory and an entry script. the documentation states the source directory is a local directory that is copied to the remote computer. however, the estimator constructor also allows you to specify a datastore name that is supposed to specify the datastore for the project share. to me, this sounds like you can specify a datastore and then the source directory is relative to that however this does not work, it still wants to find the source directory on the local machine. does anybody know if its possible to run a training job using a datastore for execution?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":71375452.0,
        "Question_title":"How to train multi item timeseries forecasting in Sagemaker?",
        "Question_body":"<p>I have data like below.<\/p>\n<pre><code>   date      sku   unitprice    trand_item    target\n2018-01-01    A      10            Black        3\n2018-01-02    A      10            Black        7\n2018-01-03    A      10            Black        0\n2018-01-04    A      10            Black        13\n.\n.\n.\n2017-08-01    B      20            White        4\n2017-08-02    B      20            White        0\n2017-08-03    B      20            White        17\n2017-08-04    B      20            White        9\n.\n.\n.\n<\/code><\/pre>\n<p>Every timestamp is filled in 'D' without blank and 'sku' is item number.\nI have 25 items and i want to forecast 'target'.\nAlso want to use 'unit price', 'trand_item' for meta data.<\/p>\n<p>How can i train timeseries forecasting model in sagemaker?\n1 model for 25 items. (For example, i want to forecast 30days for each item's 'target'.)<\/p>\n<p>Please help me...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1646614962640,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Owner_creation_time":1645529233420,
        "Owner_last_access_time":1663652813223,
        "Owner_reputation":23.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1646619582716,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71375452",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to train multi item timeseries forecasting in ?; Content: i have data like below. date sku unitprice trand_item target 2018-01-01 a 10 black 3 2018-01-02 a 10 black 7 2018-01-03 a 10 black 0 2018-01-04 a 10 black 13 . . . 2017-08-01 b 20 white 4 2017-08-02 b 20 white 0 2017-08-03 b 20 white 17 2017-08-04 b 20 white 9 . . . every timestamp is filled in 'd' without blank and 'sku' is item number. i have 25 items and i want to forecast 'target'. also want to use 'unit price', 'trand_item' for meta data. how can i train timeseries forecasting model in ? 1 model for 25 items. (for example, i want to forecast 30days for each item's 'target'.) please help me...",
        "Question_original_content_gpt_summary":"The user is facing a challenge of training a multi-item timeseries forecasting model in order to predict the 'target' of 25 items over a 30-day period, while also taking into account 'unit price' and 'trand_item' as meta data.",
        "Question_preprocessed_content":"Title: how to train multi item timeseries forecasting in ?; Content: i have data like below. every timestamp is filled in 'd' without blank and 'sku' is item number. i have items and i want to forecast 'target'. also want to use 'unit price', for meta data. how can i train timeseries forecasting model in ? model for items. please help",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":60056978.0,
        "Question_title":"AzureML: Unable to unpickle LightGBM model",
        "Question_body":"<p>I am trying to run an Azure ML pipeline. This pipeline trains a model, saves it a pickle file and then tries to unpickle it in the next step. When trying to unpickle it, I am facing the below issue in any random run:<\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n    File \"batch_scoring.py\", line 199, in \n      clf = joblib.load(open(model_path, 'rb'))\n    File \"\/azureml-envs\/azureml_347514cea2002d6bd71b42aceb1e4eeb\/lib\/python3.6\/site-packages\/joblib\/numpy_pickle.py\", line 595, in load\n      obj = _unpickle(fobj)\n    File \"\/azureml-envs\/azureml_347514cea2002d6bd71b42aceb1e4eeb\/lib\/python3.6\/site-packages\/joblib\/numpy_pickle.py\", line 529, in _unpickle\n      obj = unpickler.load()\n    File \"\/azureml-envs\/azureml_347514cea2002d6bd71b42aceb1e4eeb\/lib\/python3.6\/pickle.py\", line 1048, in load\n      raise EOFError\n  EOFError<\/p>\n<\/blockquote>\n\n<p>Has anyone faced this issue before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5.0,
        "Question_creation_time":1580817662377,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":319.0,
        "Owner_creation_time":1577025845723,
        "Owner_last_access_time":1582794024303,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":10.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60056978",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: : unable to unpickle lightgbm model; Content: i am trying to run an pipeline. this pipeline trains a model, saves it a pickle file and then tries to unpickle it in the next step. when trying to unpickle it, i am facing the below issue in any random run: traceback (most recent call last): file \"batch_scoring.py\", line 199, in clf = joblib.load(open(model_path, 'rb')) file \"\/-envs\/_347514cea2002d6bd71b42aceb1e4eeb\/lib\/python3.6\/site-packages\/joblib\/numpy_pickle.py\", line 595, in load obj = _unpickle(fobj) file \"\/-envs\/_347514cea2002d6bd71b42aceb1e4eeb\/lib\/python3.6\/site-packages\/joblib\/numpy_pickle.py\", line 529, in _unpickle obj = unpickler.load() file \"\/-envs\/_347514cea2002d6bd71b42aceb1e4eeb\/lib\/python3.6\/pickle.py\", line 1048, in load raise eoferror eoferror has anyone faced this issue before?",
        "Question_original_content_gpt_summary":"The user is encountering an issue with unpickling a lightgbm model while running a pipeline.",
        "Question_preprocessed_content":"Title: unable to unpickle lightgbm model; Content: i am trying to run an pipeline. this pipeline trains a model, saves it a pickle file and then tries to unpickle it in the next step. when trying to unpickle it, i am facing the below issue in any random run traceback file line , in clf 'rb' file line , in load obj file line , in obj file line , in load raise eoferror eoferror has anyone faced this issue before?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"AzureML pipeline not working",
        "Question_body":"I have designed a pipeline that makes predictions and saves the results to a blob container.\nThe pipeline works fine after submitting the experiment. However, after I publish it and call it via its REST endpoint, it does not work (I don't get my results). The portal shows that the job has been completed, without any error.\nCan someone enlighten me on how to use a publish pipeline?",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1654438612767,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/877179\/azureml-pipeline-not-working.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-07T12:23:00.923Z",
                "Answer_score":0,
                "Answer_body":"Thanks for the details. All published pipelines have a REST endpoint. With the pipeline endpoint, you can trigger a run of the pipeline from external systems, such as non-Python clients. For information about how to authenticate when calling REST endpoints, see https:\/\/aka.ms\/pl-restep-auth.\n\nUsing the endpoint enables \"managed repeatability\" in batch scoring and retraining scenarios, for example. For more information, see https:\/\/aka.ms\/pl-first-pipeline.\n\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.graph.publishedpipeline?view=azure-ml-py\n\nHere is the sample and document for publish.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pipeline not working; Content: i have designed a pipeline that makes predictions and saves the results to a blob container. the pipeline works fine after submitting the experiment. however, after i publish it and call it via its rest endpoint, it does not work (i don't get my results). the portal shows that the job has been completed, without any error. can someone enlighten me on how to use a publish pipeline?",
        "Question_original_content_gpt_summary":"The user is encountering an issue with their pipeline not working after publishing it and calling it via its REST endpoint, despite the portal showing that the job has been completed without any errors.",
        "Question_preprocessed_content":"Title: pipeline not working; Content: i have designed a pipeline that makes predictions and saves the results to a blob container. the pipeline works fine after submitting the experiment. however, after i publish it and call it via its rest endpoint, it does not work . the portal shows that the job has been completed, without any error. can someone enlighten me on how to use a publish pipeline?",
        "Answer_original_content":"thanks for the details. all published pipelines have a rest endpoint. with the pipeline endpoint, you can trigger a run of the pipeline from external systems, such as non-python clients. for information about how to authenticate when calling rest endpoints, see https:\/\/aka.ms\/pl-restep-auth. using the endpoint enables \"managed repeatability\" in batch scoring and retraining scenarios, for example. for more information, see https:\/\/aka.ms\/pl-first-pipeline. https:\/\/docs.microsoft.com\/en-us\/python\/api\/-pipeline-core\/.pipeline.core.graph.publishedpipeline?view=azure-ml-py here is the sample and document for publish.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the details. all published pipelines have a rest endpoint. with the pipeline endpoint, you can trigger a run of the pipeline from external systems, such as non python clients. for about how to authenticate when calling rest endpoints, see using the endpoint enables managed repeatability in batch scoring and retraining scenarios, for example. for more , see here is the sample and document for publish."
    },
    {
        "Question_id":null,
        "Question_title":"How to debug invocation timeout in sagemaker?",
        "Question_body":"I am testing inference in sagemaker , by using one of the container listed here -> https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md. the model is zipped up as below and with in inference.py file , i am overwriting functions like model_fn method and predict_fn. I tested this with batch transform and it worked but for few small input files but for other larger files, i keep getting \"Model server did not respond to \/invocations request within 3600 seconds\" . I'm trying to find out what is the cause of it? 3600 is the max we can set for \"invocation timeout in seconds\" parameter and the default input size for batch is 6mb , the input files i'm using are way smaller than that but i still get that error.\n\nDirectory structure\n\nmodel.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  \n\n\n\nfile : inference.py\n\nimport torch\nimport os\n\ndef model_fn(model_dir):\n    model = Your_Model()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f))\n    return model\n\ndef predict_fn():\n    \/\/\n\n\nbased on docs here, https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-batch-code.html#your-algorithms-batch-code-how-containers-should-respond-to-inferences, do we need to install flask and have an \/invocations endpoint , that responds 200 ok , when we are using custom container?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1649727502799,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":432.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE4UPZjwNQveIG8zuZeXIgA\/how-to-debug-invocation-timeout-in-sagemaker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-15T20:48:32.532Z",
                "Answer_score":0,
                "Answer_body":"One of the best ways to debug a custom inference script would be to start off with using the SageMaker \"local mode\". Once you are sure that your script is working fine, move over to hosting on the SageMaker endpoint. Here are some of the examples to get started.\n\nExample for a TF serving model that I have a custom Inference script, I would use local mode as shown below for my testing-\n\nfrom sagemaker.tensorflow.model import TensorFlowModel\nfrom sagemaker.local import LocalSession\n\ntensorflow_serving_model = TensorFlowModel(\n    model_data=model_data,\n    role=sagemaker_role,\n    framework_version=\"2.6\",\n  # sagemaker_session=sagemaker_session,\n  sagemaker_session=LocalSession()\n)",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to debug invocation timeout in ?; Content: i am testing inference in , by using one of the container listed here -> https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md. the model is zipped up as below and with in inference.py file , i am overwriting functions like model_fn method and predict_fn. i tested this with batch transform and it worked but for few small input files but for other larger files, i keep getting \"model server did not respond to \/invocations request within 3600 seconds\" . i'm trying to find out what is the cause of it? 3600 is the max we can set for \"invocation timeout in seconds\" parameter and the default input size for batch is 6mb , the input files i'm using are way smaller than that but i still get that error. directory structure model.tar.gz\/ |- model.pth |- code\/ |- inference.py |- requirements.txt file : inference.py import torch import os def model_fn(model_dir): model = your_model() with open(os.path.join(model_dir, 'model.pth'), 'rb') as f: model.load_state_dict(torch.load(f)) return model def predict_fn(): \/\/ based on docs here, https:\/\/docs.aws.amazon.com\/\/latest\/dg\/your-algorithms-batch-code.html#your-algorithms-batch-code-how-containers-should-respond-to-inferences, do we need to install flask and have an \/invocations endpoint , that responds 200 ok , when we are using custom container?",
        "Question_original_content_gpt_summary":"The user is encountering an invocation timeout error when attempting to use a custom container for inference in AWS, despite the input files being smaller than the default input size for batch.",
        "Question_preprocessed_content":"Title: how to debug invocation timeout in ?; Content: i am testing inference in , by using one of the container listed here > the model is zipped up as below and with in file , i am overwriting functions like method and i tested this with batch transform and it worked but for few small input files but for other larger files, i keep getting model server did not respond to request within seconds . i'm trying to find out what is the cause of it? is the max we can set for invocation timeout in seconds parameter and the default input size for batch is mb , the input files i'm using are way smaller than that but i still get that error. directory structure code\/ file import torch import os def model with 'rb' as f return model def based on docs here, do we need to install flask and have an endpoint , that responds ok , when we are using custom container?",
        "Answer_original_content":"one of the best ways to debug a custom inference script would be to start off with using the \"local mode\". once you are sure that your script is working fine, move over to hosting on the endpoint. here are some of the examples to get started. example for a tf serving model that i have a custom inference script, i would use local mode as shown below for my testing- from .tensorflow.model import tensorflowmodel from .local import localsession tensorflow_serving_model = tensorflowmodel( model_data=model_data, role=_role, framework_version=\"2.6\", # _session=_session, _session=localsession() )",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"one of the best ways to debug a custom inference script would be to start off with using the local mode . once you are sure that your script is working fine, move over to hosting on the endpoint. here are some of the examples to get started. example for a tf serving model that i have a custom inference script, i would use local mode as shown below for my testing from import tensorflowmodel from import localsession tensorflowmodel"
    },
    {
        "Question_id":null,
        "Question_title":"Model predictions",
        "Question_body":"Hi,\nI build a Bagged GLM model on azure studio with tons of data split 50\/50 as training and testing input. Mow I am trying to deploy it as web service, it wont let me to input from the start point.\n\n1) Where should I input a simple data to the model? Before the loop for bagging when I got a single request? Would the result same for fast requests\/input?\nbeggin<-function(trainx,testx,length_divisor=4,iterations=100)\nprediction<-foreach(m=1:iterations, .combine=rbind) %dopar% {\npredict(glm,newdata=testx, na.action=na.omit)\n}\ndata.set<-rowMeans(prediction)\n2) Another model built using use r-script modules. Each time it predicts almost the same value using our algorithm not from the studio, the variation of the result was trivial to the .0xx\n\nThanks\nN.A.W.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1634657889403,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/596074\/model-predictions.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-20T10:38:50.637Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nAfter you creating your web service, you need to call it as below.\n\nI am giving Python as an example: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=python#call-the-service-python\n\n import requests\n import json\n    \n # URL for the web service\n scoring_uri = '<your web service URI>'\n # If the service is authenticated, set the key or token\n key = '<your key or token>'\n    \n # Two sets of data to score, so we get two results back\n data = {\"data\":\n         [\n             [\n                 0.0199132141783263,\n                 0.0506801187398187,\n                 0.104808689473925,\n                 0.0700725447072635,\n                 -0.0359677812752396,\n                 -0.0266789028311707,\n                 -0.0249926566315915,\n                 -0.00259226199818282,\n                 0.00371173823343597,\n                 0.0403433716478807\n             ],\n             [\n                 -0.0127796318808497,\n                 -0.044641636506989,\n                 0.0606183944448076,\n                 0.0528581912385822,\n                 0.0479653430750293,\n                 0.0293746718291555,\n                 -0.0176293810234174,\n                 0.0343088588777263,\n                 0.0702112981933102,\n                 0.00720651632920303]\n         ]\n         }\n # Convert to JSON string\n input_data = json.dumps(data)\n    \n # Set the content type\n headers = {'Content-Type': 'application\/json'}\n # If authentication is enabled, set the authorization header\n headers['Authorization'] = f'Bearer {key}'\n    \n # Make the request and display the response\n resp = requests.post(scoring_uri, input_data, headers=headers)\n print(resp.text)\n\n\n\nMore information please refer to the documentation. Please let me know if you need more details.\n\nRegards,\nYutong",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: model predictions; Content: hi, i build a bagged glm model on azure studio with tons of data split 50\/50 as training and testing input. mow i am trying to deploy it as web service, it wont let me to input from the start point. 1) where should i input a simple data to the model? before the loop for bagging when i got a single request? would the result same for fast requests\/input? beggin<-function(trainx,testx,length_divisor=4,iterations=100) prediction<-foreach(m=1:iterations, .combine=rbind) %dopar% { predict(glm,newdata=testx, na.action=na.omit) } data.set<-rowmeans(prediction) 2) another model built using use r-script modules. each time it predicts almost the same value using our algorithm not from the studio, the variation of the result was trivial to the .0xx thanks n.a.w.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with deploying a bagged GLM model as a web service, as well as with predicting values using an algorithm not from Azure Studio with minimal variation.",
        "Question_preprocessed_content":"Title: model predictions; Content: hi, i build a bagged glm model on azure studio with tons of data split as training and testing input. mow i am trying to deploy it as web service, it wont let me to input from the start point. where should i input a simple data to the model? before the loop for bagging when i got a single request? would the result same for fast prediction< foreach %dopar% another model built using use r script modules. each time it predicts almost the same value using our algorithm not from the studio, the variation of the result was trivial to the . xx thanks",
        "Answer_original_content":"hello, after you creating your web service, you need to call it as below. i am giving python as an example: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-consume-web-service?tabs=python#call-the-service-python import requests import json # url for the web service scoring_uri = '' # if the service is authenticated, set the key or token key = '' # two sets of data to score, so we get two results back data = {\"data\": [ [ 0.0199132141783263, 0.0506801187398187, 0.104808689473925, 0.0700725447072635, -0.0359677812752396, -0.0266789028311707, -0.0249926566315915, -0.00259226199818282, 0.00371173823343597, 0.0403433716478807 ], [ -0.0127796318808497, -0.044641636506989, 0.0606183944448076, 0.0528581912385822, 0.0479653430750293, 0.0293746718291555, -0.0176293810234174, 0.0343088588777263, 0.0702112981933102, 0.00720651632920303] ] } # convert to json string input_data = json.dumps(data) # set the content type headers = {'content-type': 'application\/json'} # if authentication is enabled, set the authorization header headers['authorization'] = f'bearer {key}' # make the request and display the response resp = requests.post(scoring_uri, input_data, headers=headers) print(resp.text) more information please refer to the documentation. please let me know if you need more details. regards, yutong",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello, after you creating your web service, you need to call it as below. i am giving python as an example import requests import json url for the web service '' if the service is authenticated, set the key or token key '' two sets of data to score, so we get two results back data convert to json string set the content type headers if authentication is enabled, set the authorization header headers f'bearer ' make the request and display the response resp headers headers more please refer to the documentation. please let me know if you need more details. regards, yutong"
    },
    {
        "Question_id":71314659.0,
        "Question_title":"How send to image payload to the Deep learning model using lambda and send get the Image respone",
        "Question_body":"<p>I am working on Deploying background removal using AWS lambda, I didn't understand how to create a proper image payload to model and get the response back as an image.\nplease help me with this, I am struggling with this for one week.\nhere I tried:<\/p>\n<pre><code> model_dir='model\/u2net.pth'\n    model=load_models(model_dir)\n    \n    def lambda_handler(event, context):\n        \n        url = event['url']\n        img = io.imread(url)\n        # classify image\n        img = u2net_api_call(img, model)\n        output_image = img.numpy()\n        output_image = np.uint8(output_image.transpose(1, 2, 0) * 255)\n        output_image = Image.fromarray(output_image)\n    \n      # convert the PIL image to base64\n        result = {\n          &quot;output&quot;: img_to_base64_str(output_image)\n      }\n    \n      # send the result back to the client inside the body field\n        return {\n          &quot;statusCode&quot;: 200,\n          &quot;body&quot;: json.dumps(result),\n          &quot;headers&quot;: {\n              'Content-Type': 'application\/json',\n              'Access-Control-Allow-Origin': '*'\n          }\n      }\n<\/code><\/pre>\n<p>I have deployed these with lambda and docker images, I have created the docker image and push ECR and I am adding with lambda.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1646168848977,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Owner_creation_time":1583415050007,
        "Owner_last_access_time":1646745705963,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":15.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Bangalore, Karnataka, India",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71314659",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how send to image payload to the deep learning model using lambda and send get the image respone; Content: i am working on deploying background removal using aws lambda, i didn't understand how to create a proper image payload to model and get the response back as an image. please help me with this, i am struggling with this for one week. here i tried: model_dir='model\/u2net.pth' model=load_models(model_dir) def lambda_handler(event, context): url = event['url'] img = io.imread(url) # classify image img = u2net_api_call(img, model) output_image = img.numpy() output_image = np.uint8(output_image.transpose(1, 2, 0) * 255) output_image = image.fromarray(output_image) # convert the pil image to base64 result = { \"output\": img_to_base64_str(output_image) } # send the result back to the client inside the body field return { \"statuscode\": 200, \"body\": json.dumps(result), \"headers\": { 'content-type': 'application\/json', 'access-control-allow-origin': '*' } } i have deployed these with lambda and docker images, i have created the docker image and push ecr and i am adding with lambda.",
        "Question_original_content_gpt_summary":"The user is struggling to create a proper image payload to a deep learning model and get the response back as an image using AWS Lambda.",
        "Question_preprocessed_content":"Title: how send to image payload to the deep learning model using lambda and send get the image respone; Content: i am working on deploying background removal using aws lambda, i didn't understand how to create a proper image payload to model and get the response back as an image. please help me with this, i am struggling with this for one week. here i tried i have deployed these with lambda and docker images, i have created the docker image and push ecr and i am adding with lambda.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Damage detection on mobile",
        "Question_body":"Hi, I am new to ML\/AI, Can someone help me create model for detection damage on mobile i.e. screen damage, back panel damaged etc, How sagemaker can help",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1667293813732,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":18.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUjpP7mZ07RSmhLNC0lXA3YQ\/damage-detection-on-mobile",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-01T21:02:17.749Z",
                "Answer_score":0,
                "Answer_body":"Hope this helps to get started on - https:\/\/awslabs.github.io\/sagemaker-defect-detection\/",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: damage detection on mobile; Content: hi, i am new to ml\/ai, can someone help me create model for detection damage on mobile i.e. screen damage, back panel damaged etc, how can help",
        "Question_original_content_gpt_summary":"The user is looking for help creating a model for detecting damage on mobile devices, such as screen damage and back panel damage.",
        "Question_preprocessed_content":"Title: damage detection on mobile; Content: hi, i am new to can someone help me create model for detection damage on mobile screen damage, back panel damaged etc, how can help",
        "Answer_original_content":"hope this helps to get started on - https:\/\/awslabs.github.io\/-defect-detection\/",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hope this helps to get started on"
    },
    {
        "Question_id":71858668.0,
        "Question_title":"How to use tensorflow hub in Azure ML",
        "Question_body":"<p>I am trying to use  TensorFlow Hub in Azure ML Studio<\/p>\n<p>I am using the kernel Python 3.8 PT and TF<\/p>\n<p>And I installed  a few modules:<\/p>\n<pre><code>!pip install bert-for-tf2\n!pip install sentencepiece\n!pip install &quot;tensorflow&gt;=2.0.0&quot;\n!pip install --upgrade tensorflow-hub\n<\/code><\/pre>\n<p>With pip list, I can see they are installed:<\/p>\n<pre><code>tensorflow                              2.8.0\ntensorflow-estimator                    2.3.0\ntensorflow-gpu                          2.3.0\ntensorflow-hub                          0.12.0\ntensorflow-io-gcs-filesystem            0.24.0\n<\/code><\/pre>\n<p>However when I try to use it as per the documentation (<a href=\"https:\/\/www.tensorflow.org\/hub\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/hub<\/a>)<\/p>\n<p>Then I get the classic:<\/p>\n<pre><code>ModuleNotFoundError: No module named 'tensorflow_hub'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1649857871967,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":112.0,
        "Owner_creation_time":1302030303092,
        "Owner_last_access_time":1663332147472,
        "Owner_reputation":30340.0,
        "Owner_up_votes":1667.0,
        "Owner_down_votes":79.0,
        "Owner_views":2937.0,
        "Answer_body":"<p>To resolve this <code>ModuleNotFoundError: No module named 'tensorflow_hub'<\/code>  error, try following ways:<\/p>\n<ul>\n<li>Try installing\/upgrading the latest version of <code>tensorflow<\/code> and <code>tensorflow-hub<\/code> and then import:<\/li>\n<\/ul>\n<pre><code>!pip install --upgrade tensorflow\n\n!pip install --upgrade tensorflow_hub\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n<\/code><\/pre>\n<ul>\n<li>Install the current environment as a new kernel:<\/li>\n<\/ul>\n<pre><code>python3 -m ipykernel install --user --name=testenvironment\n<\/code><\/pre>\n<p>You can refer to <a href=\"https:\/\/stackoverflow.com\/questions\/63884339\/modulenotfounderror-no-module-named-tensorflow-hub\">ModuleNotFoundError: No module named 'tensorflow_hub', No module named 'tensorflow_hub'<\/a> and <a href=\"https:\/\/github.com\/tensorflow\/hub\/issues\/767\" rel=\"nofollow noreferrer\">How to use Tensorflow Hub Model?<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1650950165912,
        "Answer_score":1.0,
        "Owner_location":"Brussels, B\u00e9lgica",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71858668",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to use tensorflow hub in ; Content: i am trying to use tensorflow hub in studio i am using the kernel python 3.8 pt and tf and i installed a few modules: !pip install bert-for-tf2 !pip install sentencepiece !pip install \"tensorflow>=2.0.0\" !pip install --upgrade tensorflow-hub with pip list, i can see they are installed: tensorflow 2.8.0 tensorflow-estimator 2.3.0 tensorflow-gpu 2.3.0 tensorflow-hub 0.12.0 tensorflow-io-gcs-filesystem 0.24.0 however when i try to use it as per the documentation (https:\/\/www.tensorflow.org\/hub) then i get the classic: modulenotfounderror: no module named 'tensorflow_hub'",
        "Question_original_content_gpt_summary":"The user is encountering challenges with using TensorFlow Hub in Studio, as they are unable to import the module despite having installed it.",
        "Question_preprocessed_content":"Title: how to use tensorflow hub in; Content: i am trying to use tensorflow hub in studio i am using the kernel python pt and tf and i installed a few modules with pip list, i can see they are installed however when i try to use it as per the documentation then i get the classic",
        "Answer_original_content":"to resolve this modulenotfounderror: no module named 'tensorflow_hub' error, try following ways: try installing\/upgrading the latest version of tensorflow and tensorflow-hub and then import: !pip install --upgrade tensorflow !pip install --upgrade tensorflow_hub import tensorflow as tf import tensorflow_hub as hub install the current environment as a new kernel: python3 -m ipykernel install --user --name=testenvironment you can refer to modulenotfounderror: no module named 'tensorflow_hub', no module named 'tensorflow_hub' and how to use tensorflow hub model?",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"to resolve this error, try following ways try the latest version of and and then import install the current environment as a new kernel you can refer to modulenotfounderror no module named no module named and how to use tensorflow hub model?"
    },
    {
        "Question_id":49658834.0,
        "Question_title":"Error while deserializing the Apache MXNet object",
        "Question_body":"<p>I have trained and saved a model using Amazon SageMaker which saves the model in the format of <code>model.tar.gz<\/code> which when untarred, has a file <code>model_algo-1<\/code> which is a serialized Apache MXNet object. To load the model in memory I need to deserialize the model. I tried doing so as follows:<\/p>\n\n<p><code>import mxnet as mx\nprint(mx.ndarray.load('model_algo-1'))<\/code><\/p>\n\n<p>Reference taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html<\/a><\/p>\n\n<p>However, doing this yields me the following error:<\/p>\n\n<pre><code>Traceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nFile \"\/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/ndarray\/utils.py\", line \n175, in load\nctypes.byref(names)))\nFile \"\/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/base.py\", line 146, in \ncheck_call\nraise MXNetError(py_str(_LIB.MXGetLastError()))\nmxnet.base.MXNetError: [19:06:25] src\/ndarray\/ndarray.cc:1112: Check failed: \nheader == kMXAPINDArrayListMagic Invalid NDArray file format\n\nStack trace returned 10 entries:\n[bt] (0) \/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/libmxnet.so(+0x192112) \n[0x7fe432bfa112]\n[bt] (1) \/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/libmxnet.so(+0x192738) \n[0x7fe432bfa738]\n[bt] (2) \/usr\/local\/lib\/python3.4\/site-\npackages\/mxnet\/libmxnet.so(+0x24a5c44) [0x7fe434f0dc44]\n[bt] (3) \/usr\/local\/lib\/python3.4\/site-\npackages\/mxnet\/libmxnet.so(MXNDArrayLoad+0x248) [0x7fe434d19ad8]\n[bt] (4) \/usr\/lib64\/libffi.so.6(ffi_call_unix64+0x4c) [0x7fe48c5bbcec]\n[bt] (5) \/usr\/lib64\/libffi.so.6(ffi_call+0x1f5) [0x7fe48c5bb615]\n[bt] (6) \/usr\/lib64\/python3.4\/lib-dynload\/_ctypes.cpython-\n34m.so(_ctypes_callproc+0x2fb) [0x7fe48c7ce18b]\n[bt] (7) \/usr\/lib64\/python3.4\/lib-dynload\/_ctypes.cpython-34m.so(+0xa4cf) \n[0x7fe48c7c84cf]\n[bt] (8) \/usr\/lib64\/libpython3.4m.so.1.0(PyObject_Call+0x8c) \n[0x7fe4942fcb5c]\n[bt] (9) \/usr\/lib64\/libpython3.4m.so.1.0(PyEval_EvalFrameEx+0x36c5) \n[0x7fe4943ac915]\n<\/code><\/pre>\n\n<p>Could someone suggest how this can be resolved?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3.0,
        "Question_creation_time":1522869667753,
        "Question_favorite_count":1.0,
        "Question_score":1.0,
        "Question_view_count":971.0,
        "Owner_creation_time":1474557627236,
        "Owner_last_access_time":1584038662640,
        "Owner_reputation":1039.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":89.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49658834",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: error while deserializing the apache mxnet object; Content: i have trained and saved a model using which saves the model in the format of model.tar.gz which when untarred, has a file model_algo-1 which is a serialized apache mxnet object. to load the model in memory i need to deserialize the model. i tried doing so as follows: import mxnet as mx print(mx.ndarray.load('model_algo-1')) reference taken from https:\/\/docs.aws.amazon.com\/\/latest\/dg\/cdf-training.html however, doing this yields me the following error: traceback (most recent call last): file \"<stdin>\", line 1, in <module> file \"\/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/ndarray\/utils.py\", line 175, in load ctypes.byref(names))) file \"\/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/base.py\", line 146, in check_call raise mxneterror(py_str(_lib.mxgetlasterror())) mxnet.base.mxneterror: [19:06:25] src\/ndarray\/ndarray.cc:1112: check failed: header == kmxapindarraylistmagic invalid ndarray file format stack trace returned 10 entries: [bt] (0) \/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/libmxnet.so(+0x192112) [0x7fe432bfa112] [bt] (1) \/usr\/local\/lib\/python3.4\/site-packages\/mxnet\/libmxnet.so(+0x192738) [0x7fe432bfa738] [bt] (2) \/usr\/local\/lib\/python3.4\/site- packages\/mxnet\/libmxnet.so(+0x24a5c44) [0x7fe434f0dc44] [bt] (3) \/usr\/local\/lib\/python3.4\/site- packages\/mxnet\/libmxnet.so(mxndarrayload+0x248) [0x7fe434d19ad8] [bt] (4) \/usr\/lib64\/libffi.so.6(ffi_call_unix64+0x4c) [0x7fe48c5bbcec] [bt] (5) \/usr\/lib64\/libffi.so.6(ffi_call+0x1f5) [0x7fe48c5bb615] [bt] (6) \/usr\/lib64\/python3.4\/lib-dynload\/_ctypes.cpython- 34m.so(_ctypes_callproc+0x2fb) [0x7fe48c7ce18b] [bt] (7) \/usr\/lib64\/python3.4\/lib-dynload\/_ctypes.cpython-34m.so(+0xa4cf) [0x7fe48c7c84cf] [bt] (8) \/usr\/lib64\/libpython3.4m.so.1.0(pyobject_call+0x8c) [0x7fe4942fcb5c] [bt] (9) \/usr\/lib64\/libpython3.4m.so.1.0(pyeval_evalframeex+0x36c5) [0x7fe4943ac915] could someone suggest how this can be resolved?",
        "Question_original_content_gpt_summary":"The user is encountering an error while attempting to deserialize an Apache MXNet object in order to load a model into memory.",
        "Question_preprocessed_content":"Title: error while deserializing the apache mxnet object; Content: i have trained and saved a model using which saves the model in the format of which when untarred, has a file which is a serialized apache mxnet object. to load the model in memory i need to deserialize the model. i tried doing so as follows reference taken from however, doing this yields me the following error could someone suggest how this can be resolved?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":73838064.0,
        "Question_title":"Mlflow unable to log sklearn model",
        "Question_body":"<p>Mlflow was activated as per following:<\/p>\n<pre><code>import mlflow \nmlflow.set_tracking_uri(&quot;sqlite:\/\/\/mlflow.db&quot;) \nmlflow.set_experiment(&quot;project-2&quot;) \nmlflow.autolog()\n<\/code><\/pre>\n<p>Output message after activation was fine as per following:<\/p>\n<pre><code>2022\/09\/24 18:00:58 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n2022\/09\/24 18:00:58 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2022\/09\/24 18:00:58 INFO mlflow.tracking.fluent: Autologging successfully enabled for lightgbm.\n<\/code><\/pre>\n<p>However, when i tried to run the following regression model.<\/p>\n<pre><code>lr_baseline = LinearRegression()\nlr_baseline.fit(X_train, y_train)\ny_pred = lr_baseline.predict(X_test)\n\nwith mlflow.start_run():\n    baseline_train_r2 = lr_baseline.score(X_train, y_train)\n    baseline_test_r2 = lr_baseline.score(X_test, y_test)\n    baseline_cv_r2 = cross_val_score(lr_baseline, X_train, y_train).mean()\nprint(f&quot;&quot;&quot;Baseline R2 score for train data: {baseline_train_r2}\nBaseline R2 score for test data: {baseline_test_r2}, \nBaseline cross validated R2 score for train data: {baseline_cv_r2}&quot;&quot;&quot;)\n<\/code><\/pre>\n<p>MLflow shows the following warning.<\/p>\n<pre><code>2022\/09\/24 18:24:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'a843adb4410d46ff855b725479b0bcf0', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n2022\/09\/24 18:24:09 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under .\/mlruns\/1\/a843adb4410d46ff855b725479b0bcf0\/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.\n2022\/09\/24 18:24:14 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.\n2022\/09\/24 18:24:18 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.\n2022\/09\/24 18:24:22 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.\n2022\/09\/24 18:24:26 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.\n2022\/09\/24 18:24:30 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.\n<\/code><\/pre>\n<p>Although i am still able to download from mlflow, i am unable to run the downloaded model.<\/p>\n<pre><code>from mlflow.artifacts import download_artifacts\n\nimport mlflow.pyfunc\n\nfull_path = '.\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts\/model'\ndownload_artifacts(full_path, dst_path='.')\n\nmodel = mlflow.pyfunc.load_model(model_uri=&quot;.\/model&quot;) # saved trained model is within the best_estimator folder\nmodel.predict(df_submit_cleaned)\n<\/code><\/pre>\n<p>Output error message when i tried to run the downloaded model is as follows:<\/p>\n<pre><code>---------------------------------------------------------------------------\nMlflowException                           Traceback (most recent call last)\nCell In [58], line 1\n----&gt; 1 model.predict(df_submit_cleaned)\n\nFile ~\\anaconda3\\envs\\dsi-sg\\lib\\site-packages\\mlflow\\pyfunc\\__init__.py:372, in PyFuncModel.predict(self, data)\n    370 input_schema = self.metadata.get_input_schema()\n    371 if input_schema is not None:\n--&gt; 372     data = _enforce_schema(data, input_schema)\n    373 return self._predict_fn(data)\n\nFile ~\\anaconda3\\envs\\dsi-sg\\lib\\site-packages\\mlflow\\models\\utils.py:545, in _enforce_schema(pfInput, input_schema)\n    543         if extra_cols:\n    544             message += &quot; Note that there were extra inputs: {0}&quot;.format(extra_cols)\n--&gt; 545         raise MlflowException(message)\n    546 elif not input_schema.is_tensor_spec():\n    547     # The model signature does not specify column names =&gt; we can only verify column count.\n    548     num_actual_columns = len(pfInput.columns)\n\nMlflowException: Model is missing inputs ['MS SubClass_120', 'MS SubClass_150', 'MS SubClass_160', 'MS SubClass_180', 'MS SubClass_190', 'MS SubClass_20', 'MS SubClass_30', 'MS SubClass_40', 'MS SubClass_45', 'MS SubClass_50', 'MS SubClass_60', 'MS SubClass_70', 'MS SubClass_75', 'MS SubClass_80', 'MS SubClass_85', 'MS SubClass_90', 'MS Zoning_A (agr)', 'MS Zoning_C (all)', 'MS Zoning_FV', 'MS Zoning_I (all)', 'MS Zoning_RH', 'MS Zoning_RL', 'MS Zoning_RM', 'Street_Grvl', 'Street_Pave', 'Alley_0', 'Alley_Grvl', 'Alley_Pave', 'Lot Shape_IR1', 'Lot Shape_IR2', 'Lot Shape_IR3', 'Lot Shape_Reg', 'Land Contour_Bnk', 'Land Contour_HLS', 'Land Contour_Low', 'Land Contour_Lvl', 'Utilities_AllPub', 'Utilities_NoSeWa', 'Utilities_NoSewr', 'Lot Config_Corner', 'Lot Config_CulDSac', 'Lot Config_FR2', 'Lot Config_FR3', 'Lot Config_Inside', 'Land Slope_Gtl', 'Land Slope_Mod', 'Land Slope_Sev', 'Neighborhood_Blmngtn', 'Neighborhood_Blueste', 'Neighborhood_BrDale', 'Neighborhood_BrkSide', 'Neighborhood_ClearCr', 'Neighborhood_CollgCr', 'Neighborhood_Crawfor', 'Neighborhood_Edwards', 'Neighborhood_Gilbert', 'Neighborhood_Greens', 'Neighborhood_GrnHill', 'Neighborhood_IDOTRR', 'Neighborhood_Landmrk', 'Neighborhood_MeadowV', 'Neighborhood_Mitchel', 'Neighborhood_NAmes', 'Neighborhood_NPkVill', 'Neighborhood_NWAmes', 'Neighborhood_NoRidge', 'Neighborhood_NridgHt', 'Neighborhood_OldTown', 'Neighborhood_SWISU', 'Neighborhood_Sawyer', 'Neighborhood_SawyerW', 'Neighborhood_Somerst', 'Neighborhood_StoneBr', 'Neighborhood_Timber', 'Neighborhood_Veenker', 'Condition 1_Artery', 'Condition 1_Feedr', 'Condition 1_Norm', 'Condition 1_PosA', 'Condition 1_PosN', 'Condition 1_RRAe', 'Condition 1_RRAn', 'Condition 1_RRNe', 'Condition 1_RRNn', 'Condition 2_Artery', 'Condition 2_Feedr', 'Condition 2_Norm', 'Condition 2_PosA', 'Condition 2_PosN', 'Condition 2_RRAe', 'Condition 2_RRAn', 'Condition 2_RRNn', 'Bldg Type_1Fam', 'Bldg Type_2fmCon', 'Bldg Type_Duplex', 'Bldg Type_Twnhs', 'Bldg Type_TwnhsE', 'House Style_1.5Fin', 'House Style_1.5Unf', 'House Style_1Story', 'House Style_2.5Fin', 'House Style_2.5Unf', 'House Style_2Story', 'House Style_SFoyer', 'House Style_SLvl', 'Roof Style_Flat', 'Roof Style_Gable', 'Roof Style_Gambrel', 'Roof Style_Hip', 'Roof Style_Mansard', 'Roof Style_Shed', 'Roof Matl_ClyTile', 'Roof Matl_CompShg', 'Roof Matl_Membran', 'Roof Matl_Tar&amp;Grv', 'Roof Matl_WdShake', 'Roof Matl_WdShngl', 'Exterior 1st_AsbShng', 'Exterior 1st_AsphShn', 'Exterior 1st_BrkComm', 'Exterior 1st_BrkFace', 'Exterior 1st_CBlock', 'Exterior 1st_CemntBd', 'Exterior 1st_HdBoard', 'Exterior 1st_ImStucc', 'Exterior 1st_MetalSd', 'Exterior 1st_Plywood', 'Exterior 1st_Stone', 'Exterior 1st_Stucco', 'Exterior 1st_VinylSd', 'Exterior 1st_Wd Sdng', 'Exterior 1st_WdShing', 'Mas Vnr Type_0', 'Mas Vnr Type_BrkCmn', 'Mas Vnr Type_BrkFace', 'Mas Vnr Type_None', 'Mas Vnr Type_Stone', 'Exter Qual_Ex', 'Exter Qual_Fa', 'Exter Qual_Gd', 'Exter Qual_TA', 'Exter Cond_Ex', 'Exter Cond_Fa', 'Exter Cond_Gd', 'Exter Cond_Po', 'Exter Cond_TA', 'Foundation_BrkTil', 'Foundation_CBlock', 'Foundation_PConc', 'Foundation_Slab', 'Foundation_Stone', 'Foundation_Wood', 'Bsmt Qual_0', 'Bsmt Qual_Ex', 'Bsmt Qual_Fa', 'Bsmt Qual_Gd', 'Bsmt Qual_Po', 'Bsmt Qual_TA', 'Bsmt Cond_0', 'Bsmt Cond_Ex', 'Bsmt Cond_Fa', 'Bsmt Cond_Gd', 'Bsmt Cond_Po', 'Bsmt Cond_TA', 'Bsmt Exposure_0', 'Bsmt Exposure_Av', 'Bsmt Exposure_Gd', 'Bsmt Exposure_Mn', 'Bsmt Exposure_No', 'BsmtFin Type 1_0', 'BsmtFin Type 1_ALQ', 'BsmtFin Type 1_BLQ', 'BsmtFin Type 1_GLQ', 'BsmtFin Type 1_LwQ', 'BsmtFin Type 1_Rec', 'BsmtFin Type 1_Unf', 'BsmtFin Type 2_0', 'BsmtFin Type 2_ALQ', 'BsmtFin Type 2_BLQ', 'BsmtFin Type 2_GLQ', 'BsmtFin Type 2_LwQ', 'BsmtFin Type 2_Rec', 'BsmtFin Type 2_Unf', 'Heating_GasA', 'Heating_GasW', 'Heating_Grav', 'Heating_OthW', 'Heating_Wall', 'Heating QC_Ex', 'Heating QC_Fa', 'Heating QC_Gd', 'Heating QC_Po', 'Heating QC_TA', 'Central Air_N', 'Central Air_Y', 'Electrical_FuseA', 'Electrical_FuseF', 'Electrical_FuseP', 'Electrical_Mix', 'Electrical_SBrkr', 'Kitchen Qual_Ex', 'Kitchen Qual_Fa', 'Kitchen Qual_Gd', 'Kitchen Qual_TA', 'Functional_Maj1', 'Functional_Maj2', 'Functional_Min1', 'Functional_Min2', 'Functional_Mod', 'Functional_Sal', 'Functional_Sev', 'Functional_Typ', 'Garage Type_0', 'Garage Type_2Types', 'Garage Type_Attchd', 'Garage Type_Basment', 'Garage Type_BuiltIn', 'Garage Type_CarPort', 'Garage Type_Detchd', 'Garage Finish_0', 'Garage Finish_Fin', 'Garage Finish_RFn', 'Garage Finish_Unf', 'Garage Cond_0', 'Garage Cond_Ex', 'Garage Cond_Fa', 'Garage Cond_Gd', 'Garage Cond_Po', 'Garage Cond_TA', 'Paved Drive_N', 'Paved Drive_P', 'Paved Drive_Y', 'Fence_0', 'Fence_GdPrv', 'Fence_GdWo', 'Fence_MnPrv', 'Fence_MnWw', 'Misc Feature_0', 'Misc Feature_Elev', 'Misc Feature_Gar2', 'Misc Feature_Othr', 'Misc Feature_Shed', 'Misc Feature_TenC', 'Sale Type_COD', 'Sale Type_CWD', 'Sale Type_Con', 'Sale Type_ConLD', 'Sale Type_ConLI', 'Sale Type_ConLw', 'Sale Type_New', 'Sale Type_Oth', 'Sale Type_WD ']. Note that there were extra inputs: ['Exter Qual', 'Lot Shape', 'Bsmt Qual', 'Foundation', 'Functional', 'Neighborhood', 'Fence', 'Sale Type', 'Condition 2', 'BsmtFin Type 2', 'Lot Config', 'Garage Finish', 'BsmtFin Type 1', 'Garage Cond', 'MS SubClass', 'Exterior 1st', 'Roof Style', 'Street', 'Heating', 'Central Air', 'Garage Type', 'Paved Drive', 'Land Contour', 'Roof Matl', 'Alley', 'Bsmt Cond', 'Mas Vnr Type', 'Kitchen Qual', 'House Style', 'Bldg Type', 'MS Zoning', 'Misc Feature', 'Condition 1', 'Utilities', 'Electrical', 'Exter Cond', 'Land Slope', 'Heating QC', 'Bsmt Exposure']\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1664030399280,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":20.0,
        "Owner_creation_time":1400824650460,
        "Owner_last_access_time":1664084864692,
        "Owner_reputation":105.0,
        "Owner_up_votes":21.0,
        "Owner_down_votes":0.0,
        "Owner_views":13.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1664051310407,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73838064",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unable to log sklearn model; Content: was activated as per following: import .set_tracking_uri(\"sqlite:\/\/\/.db\") .set_experiment(\"project-2\") .autolog() output message after activation was fine as per following: 2022\/09\/24 18:00:58 info .tracking.fluent: autologging successfully enabled for statsmodels. 2022\/09\/24 18:00:58 info .tracking.fluent: autologging successfully enabled for sklearn. 2022\/09\/24 18:00:58 info .tracking.fluent: autologging successfully enabled for lightgbm. however, when i tried to run the following regression model. lr_baseline = linearregression() lr_baseline.fit(x_train, y_train) y_pred = lr_baseline.predict(x_test) with .start_run(): baseline_train_r2 = lr_baseline.score(x_train, y_train) baseline_test_r2 = lr_baseline.score(x_test, y_test) baseline_cv_r2 = cross_val_score(lr_baseline, x_train, y_train).mean() print(f\"\"\"baseline r2 score for train data: {baseline_train_r2} baseline r2 score for test data: {baseline_test_r2}, baseline cross validated r2 score for train data: {baseline_cv_r2}\"\"\") shows the following warning. 2022\/09\/24 18:24:04 info .utils.autologging_utils: created autologging run with id 'a843adb4410d46ff855b725479b0bcf0', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow 2022\/09\/24 18:24:09 warning .models.model: logging model metadata to the tracking server has failed, possibly due older server version. the model artifacts have been logged successfully under .\/mlruns\/1\/a843adb4410d46ff855b725479b0bcf0\/artifacts. in addition to exporting model artifacts, clients 1.7.0 and above attempt to record model metadata to the tracking store. if logging to a server via rest, consider upgrading the server version to 1.7.0 or above. 2022\/09\/24 18:24:14 warning .models.model: logging model metadata to the tracking server has failed, possibly due older server version. the model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. in addition to exporting model artifacts, clients 1.7.0 and above attempt to record model metadata to the tracking store. if logging to a server via rest, consider upgrading the server version to 1.7.0 or above. 2022\/09\/24 18:24:18 warning .models.model: logging model metadata to the tracking server has failed, possibly due older server version. the model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. in addition to exporting model artifacts, clients 1.7.0 and above attempt to record model metadata to the tracking store. if logging to a server via rest, consider upgrading the server version to 1.7.0 or above. 2022\/09\/24 18:24:22 warning .models.model: logging model metadata to the tracking server has failed, possibly due older server version. the model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. in addition to exporting model artifacts, clients 1.7.0 and above attempt to record model metadata to the tracking store. if logging to a server via rest, consider upgrading the server version to 1.7.0 or above. 2022\/09\/24 18:24:26 warning .models.model: logging model metadata to the tracking server has failed, possibly due older server version. the model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. in addition to exporting model artifacts, clients 1.7.0 and above attempt to record model metadata to the tracking store. if logging to a server via rest, consider upgrading the server version to 1.7.0 or above. 2022\/09\/24 18:24:30 warning .models.model: logging model metadata to the tracking server has failed, possibly due older server version. the model artifacts have been logged successfully under .\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts. in addition to exporting model artifacts, clients 1.7.0 and above attempt to record model metadata to the tracking store. if logging to a server via rest, consider upgrading the server version to 1.7.0 or above. although i am still able to download from , i am unable to run the downloaded model. from .artifacts import download_artifacts import .pyfunc full_path = '.\/mlruns\/1\/ed3439cbaed04b87bd4ab8764eaad894\/artifacts\/model' download_artifacts(full_path, dst_path='.') model = .pyfunc.load_model(model_uri=\".\/model\") # saved trained model is within the best_estimator folder model.predict(df_submit_cleaned) output error message when i tried to run the downloaded model is as follows: --------------------------------------------------------------------------- exception traceback (most recent call last) cell in [58], line 1 ----> 1 model.predict(df_submit_cleaned) file ~\\anaconda3\\envs\\dsi-sg\\lib\\site-packages\\\\pyfunc\\__init__.py:372, in pyfuncmodel.predict(self, data) 370 input_schema = self.metadata.get_input_schema() 371 if input_schema is not none: --> 372 data = _enforce_schema(data, input_schema) 373 return self._predict_fn(data) file ~\\anaconda3\\envs\\dsi-sg\\lib\\site-packages\\\\models\\utils.py:545, in _enforce_schema(pfinput, input_schema) 543 if extra_cols: 544 message += \" note that there were extra inputs: {0}\".format(extra_cols) --> 545 raise exception(message) 546 elif not input_schema.is_tensor_spec(): 547 # the model signature does not specify column names => we can only verify column count. 548 num_actual_columns = len(pfinput.columns) exception: model is missing inputs ['ms subclass_120', 'ms subclass_150', 'ms subclass_160', 'ms subclass_180', 'ms subclass_190', 'ms subclass_20', 'ms subclass_30', 'ms subclass_40', 'ms subclass_45', 'ms subclass_50', 'ms subclass_60', 'ms subclass_70', 'ms subclass_75', 'ms subclass_80', 'ms subclass_85', 'ms subclass_90', 'ms zoning_a (agr)', 'ms zoning_c (all)', 'ms zoning_fv', 'ms zoning_i (all)', 'ms zoning_rh', 'ms zoning_rl', 'ms zoning_rm', 'street_grvl', 'street_pave', 'alley_0', 'alley_grvl', 'alley_pave', 'lot shape_ir1', 'lot shape_ir2', 'lot shape_ir3', 'lot shape_reg', 'land contour_bnk', 'land contour_hls', 'land contour_low', 'land contour_lvl', 'utilities_allpub', 'utilities_nosewa', 'utilities_nosewr', 'lot config_corner', 'lot config_culdsac', 'lot config_fr2', 'lot config_fr3', 'lot config_inside', 'land slope_gtl', 'land slope_mod', 'land slope_sev', 'neighborhood_blmngtn', 'neighborhood_blueste', 'neighborhood_brdale', 'neighborhood_brkside', 'neighborhood_clearcr', 'neighborhood_collgcr', 'neighborhood_crawfor', 'neighborhood_edwards', 'neighborhood_gilbert', 'neighborhood_greens', 'neighborhood_grnhill', 'neighborhood_idotrr', 'neighborhood_landmrk', 'neighborhood_meadowv', 'neighborhood_mitchel', 'neighborhood_names', 'neighborhood_npkvill', 'neighborhood_nwames', 'neighborhood_noridge', 'neighborhood_nridght', 'neighborhood_oldtown', 'neighborhood_swisu', 'neighborhood_sawyer', 'neighborhood_sawyerw', 'neighborhood_somerst', 'neighborhood_stonebr', 'neighborhood_timber', 'neighborhood_veenker', 'condition 1_artery', 'condition 1_feedr', 'condition 1_norm', 'condition 1_posa', 'condition 1_posn', 'condition 1_rrae', 'condition 1_rran', 'condition 1_rrne', 'condition 1_rrnn', 'condition 2_artery', 'condition 2_feedr', 'condition 2_norm', 'condition 2_posa', 'condition 2_posn', 'condition 2_rrae', 'condition 2_rran', 'condition 2_rrnn', 'bldg type_1fam', 'bldg type_2fmcon', 'bldg type_duplex', 'bldg type_twnhs', 'bldg type_twnhse', 'house style_1.5fin', 'house style_1.5unf', 'house style_1story', 'house style_2.5fin', 'house style_2.5unf', 'house style_2story', 'house style_sfoyer', 'house style_slvl', 'roof style_flat', 'roof style_gable', 'roof style_gambrel', 'roof style_hip', 'roof style_mansard', 'roof style_shed', 'roof matl_clytile', 'roof matl_compshg', 'roof matl_membran', 'roof matl_tar&grv', 'roof matl_wdshake', 'roof matl_wdshngl', 'exterior 1st_asbshng', 'exterior 1st_asphshn', 'exterior 1st_brkcomm', 'exterior 1st_brkface', 'exterior 1st_cblock', 'exterior 1st_cemntbd', 'exterior 1st_hdboard', 'exterior 1st_imstucc', 'exterior 1st_metalsd', 'exterior 1st_plywood', 'exterior 1st_stone', 'exterior 1st_stucco', 'exterior 1st_vinylsd', 'exterior 1st_wd sdng', 'exterior 1st_wdshing', 'mas vnr type_0', 'mas vnr type_brkcmn', 'mas vnr type_brkface', 'mas vnr type_none', 'mas vnr type_stone', 'exter qual_ex', 'exter qual_fa', 'exter qual_gd', 'exter qual_ta', 'exter cond_ex', 'exter cond_fa', 'exter cond_gd', 'exter cond_po', 'exter cond_ta', 'foundation_brktil', 'foundation_cblock', 'foundation_pconc', 'foundation_slab', 'foundation_stone', 'foundation_wood', 'bsmt qual_0', 'bsmt qual_ex', 'bsmt qual_fa', 'bsmt qual_gd', 'bsmt qual_po', 'bsmt qual_ta', 'bsmt cond_0', 'bsmt cond_ex', 'bsmt cond_fa', 'bsmt cond_gd', 'bsmt cond_po', 'bsmt cond_ta', 'bsmt exposure_0', 'bsmt exposure_av', 'bsmt exposure_gd', 'bsmt exposure_mn', 'bsmt exposure_no', 'bsmtfin type 1_0', 'bsmtfin type 1_alq', 'bsmtfin type 1_blq', 'bsmtfin type 1_glq', 'bsmtfin type 1_lwq', 'bsmtfin type 1_rec', 'bsmtfin type 1_unf', 'bsmtfin type 2_0', 'bsmtfin type 2_alq', 'bsmtfin type 2_blq', 'bsmtfin type 2_glq', 'bsmtfin type 2_lwq', 'bsmtfin type 2_rec', 'bsmtfin type 2_unf', 'heating_gasa', 'heating_gasw', 'heating_grav', 'heating_othw', 'heating_wall', 'heating qc_ex', 'heating qc_fa', 'heating qc_gd', 'heating qc_po', 'heating qc_ta', 'central air_n', 'central air_y', 'electrical_fusea', 'electrical_fusef', 'electrical_fusep', 'electrical_mix', 'electrical_sbrkr', 'kitchen qual_ex', 'kitchen qual_fa', 'kitchen qual_gd', 'kitchen qual_ta', 'functional_maj1', 'functional_maj2', 'functional_min1', 'functional_min2', 'functional_mod', 'functional_sal', 'functional_sev', 'functional_typ', 'garage type_0', 'garage type_2types', 'garage type_attchd', 'garage type_basment', 'garage type_builtin', 'garage type_carport', 'garage type_detchd', 'garage finish_0', 'garage finish_fin', 'garage finish_rfn', 'garage finish_unf', 'garage cond_0', 'garage cond_ex', 'garage cond_fa', 'garage cond_gd', 'garage cond_po', 'garage cond_ta', 'paved drive_n', 'paved drive_p', 'paved drive_y', 'fence_0', 'fence_gdprv', 'fence_gdwo', 'fence_mnprv', 'fence_mnww', 'misc feature_0', 'misc feature_elev', 'misc feature_gar2', 'misc feature_othr', 'misc feature_shed', 'misc feature_tenc', 'sale type_cod', 'sale type_cwd', 'sale type_con', 'sale type_conld', 'sale type_conli', 'sale type_conlw', 'sale type_new', 'sale type_oth', 'sale type_wd ']. note that there were extra inputs: ['exter qual', 'lot shape', 'bsmt qual', 'foundation', 'functional', 'neighborhood', 'fence', 'sale type', 'condition 2', 'bsmtfin type 2', 'lot config', 'garage finish', 'bsmtfin type 1', 'garage cond', 'ms subclass', 'exterior 1st', 'roof style', 'street', 'heating', 'central air', 'garage type', 'paved drive', 'land contour', 'roof matl', 'alley', 'bsmt cond', 'mas vnr type', 'kitchen qual', 'house style', 'bldg type', 'ms zoning', 'misc feature', 'condition 1', 'utilities', 'electrical', 'exter cond', 'land slope', 'heating qc', 'bsmt exposure']",
        "Question_original_content_gpt_summary":"The user encountered challenges with logging a sklearn model, resulting in warnings about an older server version and missing inputs.",
        "Question_preprocessed_content":"Title: unable to log sklearn model; Content: was activated as per following output message after activation was fine as per following however, when i tried to run the following regression model. shows the following warning. although i am still able to download from , i am unable to run the downloaded model. output error message when i tried to run the downloaded model is as follows",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":69940900.0,
        "Question_title":"Upload tensorboard logs from cloud storage to vertex ai - tensorboard",
        "Question_body":"<p>I created a pipeline with vertex ai and added the code for creating and storing my tensorboard logs in cloud storage. The next step in the instructions here <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/experiments\/tensorboard-overview#getting_started\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/vertex-ai\/docs\/experiments\/tensorboard-overview#getting_started<\/a> is to use the tb-gcp-uploader command to upload the logs to the tensboard experiment page. But I'm getting this message &quot;'tb-gcp-uploader' is not recognized as an internal or external command&quot;. Any thoughts?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1636710548300,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":263.0,
        "Owner_creation_time":1636708800180,
        "Owner_last_access_time":1663592218140,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69940900",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: upload tensorboard logs from cloud storage to - tensorboard; Content: i created a pipeline with and added the code for creating and storing my tensorboard logs in cloud storage. the next step in the instructions here https:\/\/cloud.google.com\/vertex-ai\/docs\/experiments\/tensorboard-overview#getting_started is to use the tb-gcp-uploader command to upload the logs to the tensboard experiment page. but i'm getting this message \"'tb-gcp-uploader' is not recognized as an internal or external command\". any thoughts?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with uploading their tensorboard logs from cloud storage to the tensorboard experiment page, receiving an error message that 'tb-gcp-uploader' is not recognized as an internal or external command.",
        "Question_preprocessed_content":"Title: upload tensorboard logs from cloud storage to tensorboard; Content: i created a pipeline with and added the code for creating and storing my tensorboard logs in cloud storage. the next step in the instructions here is to use the tb gcp uploader command to upload the logs to the tensboard experiment page. but i'm getting this message 'tb gcp uploader' is not recognized as an internal or external command . any thoughts?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70973526.0,
        "Question_title":"sagemaker-sklearn-container 1.0 requires pandas==0.25.*, but you have pandas 1.3.5 which is incompatible",
        "Question_body":"<p><strong>ERROR:<\/strong> pip's dependency resolver does not currently take into account all the packages that are installed. This behavior is the source of the following dependency conflicts.<\/p>\n<p><code>sagemaker-sklearn-container 1.0 requires pandas==0.25.*,<\/code> but you have <code>pandas 1.3.5<\/code> which is incompatible.<\/p>\n<p>I am running my notebook locally under python virtual machine , and I have <code>pandas 0.25.3<\/code> version but when I am training the model on sagemaker , it shows an error that amazon scikitlearn container <code>1.0 using 0.25.*<\/code> but I have <code>1.3.5<\/code>,<\/p>\n<p>I don't understand how can I solve it , though locally I have <code>0.25.3<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1643900263570,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":191.0,
        "Owner_creation_time":1643899849347,
        "Owner_last_access_time":1663682953803,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1644905942447,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70973526",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: -sklearn-container 1.0 requires pandas==0.25.*, but you have pandas 1.3.5 which is incompatible; Content: error: pip's dependency resolver does not currently take into account all the packages that are installed. this behavior is the source of the following dependency conflicts. -sklearn-container 1.0 requires pandas==0.25.*, but you have pandas 1.3.5 which is incompatible. i am running my notebook locally under python virtual machine , and i have pandas 0.25.3 version but when i am training the model on , it shows an error that amazon scikitlearn container 1.0 using 0.25.* but i have 1.3.5, i don't understand how can i solve it , though locally i have 0.25.3",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with their pip dependency resolver not taking into account all the packages installed, resulting in a conflict between the sklearn-container 1.0 requiring pandas==0.25.* and the user having pandas 1.3.5.",
        "Question_preprocessed_content":"Title: sklearn container requires but you have pandas which is incompatible; Content: error pip's dependency resolver does not currently take into account all the packages that are installed. this behavior is the source of the following dependency conflicts. but you have which is incompatible. i am running my notebook locally under python virtual machine , and i have version but when i am training the model on , it shows an error that amazon scikitlearn container but i have , i don't understand how can i solve it , though locally i have",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":60088889.0,
        "Question_title":"How Do You \"Permanently\" Delete An Experiment In Mlflow?",
        "Question_body":"<p>Permanent deletion of an experiment isn't documented anywhere. I'm using Mlflow w\/ backend postgres db<\/p>\n\n<p>Here's what I've run: <\/p>\n\n<pre><code>client = MlflowClient(tracking_uri=server)\nclient.delete_experiment(1)\n<\/code><\/pre>\n\n<p>This deletes the the experiment, but when I run a new experiment with the same name as the one I just deleted, it will return this error:<\/p>\n\n<pre><code>mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\n<\/code><\/pre>\n\n<p>I cannot find anywhere in the documentation that shows how to permanently delete everything.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0.0,
        "Question_creation_time":1580970401043,
        "Question_favorite_count":3.0,
        "Question_score":20.0,
        "Question_view_count":13984.0,
        "Owner_creation_time":1443225809767,
        "Owner_last_access_time":1663898334270,
        "Owner_reputation":2332.0,
        "Owner_up_votes":133.0,
        "Owner_down_votes":3.0,
        "Owner_views":560.0,
        "Answer_body":"<p>Unfortunately it seems there is no way to do this via the UI or CLI at the moment :-\/<\/p>\n\n<p>The way to do it depends on the type of backend file store that you are using.<\/p>\n\n<p><strong>Filestore<\/strong>:<\/p>\n\n<p>If you are using the filesystem as a storage mechanism (the default) then it is easy. The 'deleted' experiments are moved to a <code>.trash<\/code> folder. You just need to clear that out:<\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>rm -rf mlruns\/.trash\/*\n<\/code><\/pre>\n\n<p>As of the current version of the <a href=\"https:\/\/www.mlflow.org\/docs\/latest\/cli.html#mlflow-experiments-delete\" rel=\"noreferrer\">documentation<\/a> (1.7.2), they remark:<\/p>\n\n<blockquote>\n  <p>It is recommended to use a cron job or an alternate workflow mechanism to clear <code>.trash<\/code> folder.<\/p>\n<\/blockquote>\n\n<p><strong>SQL Database:<\/strong><\/p>\n\n<p>This is more tricky, as there are dependencies that need to be deleted. I am using MySQL, and these commands work for me:<\/p>\n\n<pre class=\"lang-sql prettyprint-override\"><code>USE mlflow_db;  # the name of your database\nDELETE FROM experiment_tags WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM latest_metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM metrics WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM tags WHERE run_uuid=ANY(\n    SELECT run_uuid FROM runs WHERE experiment_id=ANY(\n        SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n    )\n);\nDELETE FROM runs WHERE experiment_id=ANY(\n    SELECT experiment_id FROM experiments where lifecycle_stage=\"deleted\"\n);\nDELETE FROM experiments where lifecycle_stage=\"deleted\";\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1585231513452,
        "Answer_score":22.0,
        "Owner_location":"Vancouver, BC, Canada",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60088889",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do you \"permanently\" delete an experiment in ?; Content: permanent deletion of an experiment isn't documented anywhere. i'm using w\/ backend postgres db here's what i've run: client = client(tracking_uri=server) client.delete_experiment(1) this deletes the the experiment, but when i run a new experiment with the same name as the one i just deleted, it will return this error: .exceptions.exception: cannot set a deleted experiment 'cross-sell' as the active experiment. you can restore the experiment, or permanently delete the experiment to create a new one. i cannot find anywhere in the documentation that shows how to permanently delete everything.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with permanently deleting an experiment in , as the documentation does not provide any guidance on how to do so.",
        "Question_preprocessed_content":"Title: how do you permanently delete an experiment in ?; Content: permanent deletion of an experiment isn't documented anywhere. i'm using w\/ backend postgres db here's what i've run this deletes the the experiment, but when i run a new experiment with the same name as the one i just deleted, it will return this error i cannot find anywhere in the documentation that shows how to permanently delete everything.",
        "Answer_original_content":"unfortunately it seems there is no way to do this via the ui or cli at the moment :-\/ the way to do it depends on the type of backend file store that you are using. filestore: if you are using the filesystem as a storage mechanism (the default) then it is easy. the 'deleted' experiments are moved to a .trash folder. you just need to clear that out: rm -rf mlruns\/.trash\/* as of the current version of the documentation (1.7.2), they remark: it is recommended to use a cron job or an alternate workflow mechanism to clear .trash folder. sql database: this is more tricky, as there are dependencies that need to be deleted. i am using mysql, and these commands work for me: use _db; # the name of your database delete from experiment_tags where experiment_id=any( select experiment_id from experiments where lifecycle_stage=\"deleted\" ); delete from latest_metrics where run_uuid=any( select run_uuid from runs where experiment_id=any( select experiment_id from experiments where lifecycle_stage=\"deleted\" ) ); delete from metrics where run_uuid=any( select run_uuid from runs where experiment_id=any( select experiment_id from experiments where lifecycle_stage=\"deleted\" ) ); delete from tags where run_uuid=any( select run_uuid from runs where experiment_id=any( select experiment_id from experiments where lifecycle_stage=\"deleted\" ) ); delete from runs where experiment_id=any( select experiment_id from experiments where lifecycle_stage=\"deleted\" ); delete from experiments where lifecycle_stage=\"deleted\";",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"unfortunately it seems there is no way to do this via the ui or cli at the moment \/ the way to do it depends on the type of backend file store that you are using. filestore if you are using the filesystem as a storage mechanism then it is easy. the 'deleted' experiments are moved to a folder. you just need to clear that out as of the current version of the documentation , they remark it is recommended to use a cron job or an alternate workflow mechanism to clear folder. sql database this is more tricky, as there are dependencies that need to be deleted. i am using mysql, and these commands work for me"
    },
    {
        "Question_id":null,
        "Question_title":"Does the pre and post-processing need to be incorporate in SageMaker?",
        "Question_body":"Hi All,\n\nGreetings!!\n\nPlease address my below queries.\n\nDoes the pre and post-processing need to be incorporate in SM?\nIsn't SM supposed to be used for inference only?\n\nWhy I'm asking these questions because I am using PyTorch model server for pre-processing, predictions and post-processing for NER use cases.\n\nWe use utokenize for creating tokens which supports >= Python 3.8 but PyTorch model server supports <= 3.6, what would be the solution? How to install Python 3.8 in PyTorch model server?\n\nDon't we have basic word piece tokenization as pre-processing in SM?\n\nThanks, Vinayak",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1644776794437,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":85.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUMhy15ZuZTNeZFajUWrFXbw\/does-the-pre-and-post-processing-need-to-be-incorporate-in-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-02T17:25:22.518Z",
                "Answer_score":0,
                "Answer_body":"Hi Vinayak\n\nI just want to make sure I understand the context: Are you referring to pre and postprocessing for an inference request or for training a model? I'm asking because SageMaker offers both:\n\nProcessing jobs which are used to process data before training a model: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-job.html\nProcessing data before sending it to an endpoint for inference: https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#write-an-inference-script\n\nCould you help clarify the question?\n\nThanks Heiko",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-17T05:57:42.469Z",
                "Answer_score":0,
                "Answer_body":"Hello @Heiko,\n\nI am talking about inference pipeline only.\n\nDon't we have pre and post-processing in-built feature for NER in SM PyTorch or HuggingFace model servers?\n\nWe thought that pre and post-processing taken care by SM and we just need to bring our fine-tuned model on our dataset.\n\nThanks, Vinayak",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-15T16:29:03.191Z",
                "Answer_score":0,
                "Answer_body":"Hi Vinayak - apologies for the late response, I was OOTO.\n\nYes, you are correct, pre- and post-processing for Huggingface inference is \"built in\" via the SageMaker Hugging Face Inference Toolkit. The documentation on the Github repo shows how this is being done.\n\nHope that helps!\n\nThanks Heiko",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: does the pre and post-processing need to be incorporate in ?; Content: hi all, greetings!! please address my below queries. does the pre and post-processing need to be incorporate in sm? isn't sm supposed to be used for inference only? why i'm asking these questions because i am using pytorch model server for pre-processing, predictions and post-processing for ner use cases. we use utokenize for creating tokens which supports >= python 3.8 but pytorch model server supports <= 3.6, what would be the solution? how to install python 3.8 in pytorch model server? don't we have basic word piece tokenization as pre-processing in sm? thanks, vinayak",
        "Question_original_content_gpt_summary":"The user is encountering challenges with incorporating pre and post-processing into SM, as well as the incompatibility of Python versions between UTokenize and PyTorch Model Server.",
        "Question_preprocessed_content":"Title: does the pre and post processing need to be incorporate in ?; Content: hi all, greetings!! please address my below queries. does the pre and post processing need to be incorporate in sm? isn't sm supposed to be used for inference only? why i'm asking these questions because i am using pytorch model server for pre processing, predictions and post processing for ner use cases. we use utokenize for creating tokens which supports > python but pytorch model server supports < what would be the solution? how to install python in pytorch model server? don't we have basic word piece tokenization as pre processing in sm? thanks, vinayak",
        "Answer_original_content":"hi vinayak i just want to make sure i understand the context: are you referring to pre and postprocessing for an inference request or for training a model? i'm asking because offers both: processing jobs which are used to process data before training a model: https:\/\/docs.aws.amazon.com\/\/latest\/dg\/processing-job.html processing data before sending it to an endpoint for inference: https:\/\/.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#write-an-inference-script could you help clarify the question? thanks heiko hello @heiko, i am talking about inference pipeline only. don't we have pre and post-processing in-built feature for ner in sm pytorch or huggingface model servers? we thought that pre and post-processing taken care by sm and we just need to bring our fine-tuned model on our dataset. thanks, vinayak hi vinayak - apologies for the late response, i was ooto. yes, you are correct, pre- and post-processing for huggingface inference is \"built in\" via the hugging face inference toolkit. the documentation on the github repo shows how this is being done. hope that helps! thanks heiko",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi vinayak i just want to make sure i understand the context are you referring to pre and postprocessing for an inference request or for training a model? i'm asking because offers both processing jobs which are used to process data before training a model processing data before sending it to an endpoint for inference could you help clarify the question? thanks heiko hello i am talking about inference pipeline only. don't we have pre and post processing in built feature for ner in sm pytorch or huggingface model servers? we thought that pre and post processing taken care by sm and we just need to bring our fine tuned model on our dataset. thanks, vinayak hi vinayak apologies for the late response, i was ooto. yes, you are correct, pre and post processing for huggingface inference is built in via the hugging face inference toolkit. the documentation on the github repo shows how this is being done. hope that helps! thanks heiko"
    },
    {
        "Question_id":null,
        "Question_title":"How to force dvc get",
        "Question_body":"<p>From my CI pipeline, I would like to download some files tracked using DVC.<\/p>\n<pre><code class=\"lang-auto\">data\n\u2514\u2500\u2500 test_data\n    \u251c\u2500\u2500 tests_subfolder_1\n    \u2502   \u251c\u2500\u2500 file1.test\n    \u2502   \u2514\u2500\u2500 file2.test\n    \u251c\u2500\u2500 tests_subfolder_1.dvc\n    \u251c\u2500\u2500 tests_subfolder_2\n<\/code><\/pre>\n<p>\u2026 etc.<\/p>\n<p>When I run <code>dvc get . data\/test_data --out data\/<\/code>, I get the error:<\/p>\n<pre><code class=\"lang-bash\">ERROR: failed to get 'data\/test_data\/' from '.' - unable to remove 'data\/test_data\/test_subfolder.dvc' without a confirmation. Use `-f` to force.\n<\/code><\/pre>\n<p>This is surprising to me, because I would expect <code>test_subfolder.dvc<\/code> and <code>test_subfolder\/<\/code> to be able to coexist.<\/p>\n<p>However, I don\u2019t know where to put the <code>-f<\/code> argument? I tried:<\/p>\n<ul>\n<li><code>dvc -f get . data\/test_data --out data\/<\/code><\/li>\n<li><code>dvc get . data\/tests_data --out data\/ --force<\/code><\/li>\n<\/ul>\n<p>I also tried to work around the problem with:<br>\n<code>dvc get . data\/test_data\/* --out data\/<\/code><\/p>\n<p>However, my final solution was:<br>\n<code>ls data\/test_data\/*.dvc | sed -e 's\/\\.dvc$\/\/' | xargs -I {} dvc get . {} --out data\/test_data\/<\/code><\/p>\n<p>This work-around feels over-complicated. What was I supposed to do?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1593011149596,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":887.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-force-dvc-get\/417",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-24T16:57:36.496Z",
                "Answer_body":"<p>Hi<\/p>\n<p><code>-f<\/code> or <code>--force<\/code> can go anywhere after <code>dvc get<\/code>.<\/p>\n<p>However, I\u2019m trying to understand your scenario here and I don\u2019t quite get it. You are trying to \u201cdownload\u201d data already tracked in your own project, and put it in the same location it already exists? That\u2019s what <code>dvc get . data\/test_data --out data\/<\/code> looks like.<\/p>\n<blockquote>\n<p>TBH I\u2019m not even sure why we support <code>.<\/code> as the URL provided to <code>get<\/code> (and <code>import<\/code>). See the ref. here: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\">https:\/\/dvc.org\/doc\/command-reference\/get<\/a><\/p>\n<\/blockquote>\n<p>Also, is there a <code>data\/test_data.dvc<\/code> file in your project? Otherwise the command should fail, I believe. Maybe the error message is wrong and this is the problem. If so would  you mind opening a bug report for that in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\">https:\/\/github.com\/iterative\/dvc\/issues<\/a>? <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<p>p.s. please note we\u2019ve officially <a href=\"https:\/\/dvc.org\/blog\/dvc-1-0-release\">released DVC 1.0<\/a>! Your project looks like it\u2019s still using 0.x \u2014 we highly recommend migrating.<\/p>",
                "Answer_score":17.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-23T21:53:16.472Z",
                "Answer_body":"<p>6 months later, I\u2019m more comfortable with <code>dvc<\/code> and <code>bash<\/code>, I can better explain my problem. At the highest level, I wanted a one-liner that downloads all <code>.dvc<\/code> files in a given sub-folder and I didn\u2019t want to write a for-loop. I don\u2019t want to do <code>dvc pull<\/code>, because I\u2019m not going to be using the version-control features in a CI pipeline and don\u2019t need a cache.<\/p>\n<p>The original directory structure I posted wasn\u2019t clear. Here is an improved illustration where each DVC file points to a full folder.<\/p>\n<pre><code class=\"lang-auto\">data\/\n\u251c\u2500\u2500 other_data.dvc\n\u2514\u2500\u2500 test_assets\n    \u251c\u2500\u2500 rough_tests.dvc\n    \u2514\u2500\u2500 smooth_tests.dvc\n<\/code><\/pre>\n<p>DVC has gone through a lot of improvements over the last six months. Is there now a better alternative to my one-liner?<\/p>",
                "Answer_score":31.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-23T23:05:13.313Z",
                "Answer_body":"<aside class=\"quote no-group quote-modified\" data-username=\"Seanny123\" data-post=\"3\" data-topic=\"417\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/seanny123\/40\/60_2.png\" class=\"avatar\"> Seanny123:<\/div>\n<blockquote>\n<p>I wanted a one-liner that downloads all <code>.dvc<\/code> files in a given sub-folder\u2026<\/p>\n<\/blockquote>\n<\/aside>\n<p>You would use Git to sync .dvc files though. I\u2019ll assume you mean to get the actual data tracked by DVC.<\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"Seanny123\" data-post=\"1\" data-topic=\"417\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/seanny123\/40\/60_2.png\" class=\"avatar\"> Seanny123:<\/div>\n<blockquote>\n<p>my final solution was:<br>\n<code>ls data\/test_data\/*.dvc | sed -e 's\/\\.dvc$\/\/' | xargs -I {} dvc get . {} --out data\/test_data\/<\/code>\u2026<br>\nI don\u2019t want to do dvc pull, because (a) I\u2019m not going to be using the version-control features in a CI pipeline and (b) don\u2019t need a cache<\/p>\n<\/blockquote>\n<\/aside>\n<p>I\u2019m still confused about the <code>.<\/code> URL given to <code>get<\/code>. So the environment in CI is a copy of the DVC repo, but you\u2019re just trying to avoid <code>dvc pull<\/code>? If so a) you\u2019re already using Git it so avoiding SCM doesn\u2019t seem to be a concern (or I don\u2019t get what the problem is); and b) similarly, \u201cnot needing a cache\u201d shouldn\u2019t be a concern? Think about that as some internal mechanism DVC uses during the download process which is not a problem: the final result is that you get the data files you want placed where you want them.<\/p>\n<p>BTW note that <code>pull<\/code> accepts targets so you can tell it to only download the data in that folder (not everything in the project).<\/p>\n<p>The only other path I can think of now would be to change the structure of your DVC repo. Instead of <code>dvc add<\/code>ing each file in <code>data\/test_assets<\/code> (resulting in multiple .dvc files) you can <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#adding-entire-directories\">add the whole directory<\/a>, so you can do this from anywhere in the CI (don\u2019t even need to clone the repo):<\/p>\n<pre><code class=\"lang-auto\">$ dvc get https:\/\/&lt;Git URL to DVC repo) data\/test_assets --out data\/\n<\/code><\/pre>\n<aside class=\"quote no-group\" data-username=\"Seanny123\" data-post=\"3\" data-topic=\"417\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/seanny123\/40\/60_2.png\" class=\"avatar\"> Seanny123:<\/div>\n<blockquote>\n<p>Is there now a better alternative to my one-liner?<\/p>\n<\/blockquote>\n<\/aside>\n<p>As for changes in DVC 1.x or 2.x (coming up very soon!), some commands are getting a <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/4816\">new <code>--glob<\/code> option<\/a> to accept wildcards but <code>get<\/code> doesn\u2019t have it. Not sure if it\u2019s planned but feel free to request that feature in our repo!<\/p>",
                "Answer_score":11.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-07T20:22:06.682Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"jorgeorpinel\" data-post=\"4\" data-topic=\"417\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/jorgeorpinel\/40\/46_2.png\" class=\"avatar\"> jorgeorpinel:<\/div>\n<blockquote>\n<p>I\u2019m still confused about the <code>.<\/code> URL given to <code>get<\/code> . So the environment in CI is a copy of the DVC repo, but you\u2019re just trying to avoid <code>dvc pull<\/code> ? If so a) you\u2019re already using Git it so avoiding SCM doesn\u2019t seem to be a concern (or I don\u2019t get what the problem is); and b) similarly, \u201cnot needing a cache\u201d shouldn\u2019t be a concern? Think about that as some internal mechanism DVC uses during the download process which is not a problem: the final result is that you get the data files you want placed where you want them.<\/p>\n<\/blockquote>\n<\/aside>\n<p>In hindsight, I don\u2019t know why I was fixated on \u201cnot needing a cache\u201d. I guess I just wanted my CI containers to consume as few resources (including disk space) as possible. However, I never checked my assumption that the cache even took up that much space in this case, so I\u2019m just going to use <code>dvc pull<\/code> now.<\/p>",
                "Answer_score":36.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-07T20:30:47.950Z",
                "Answer_body":"<p>Sounds good <a class=\"mention\" href=\"\/u\/seanny123\">@Seanny123<\/a> ! Here\u2019s some info on how the cache is optimized in DVC (avoiding file duplication in the system): <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a><\/p>\n<p>Best<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to force get; Content: from my ci pipeline, i would like to download some files tracked using . data \u2514\u2500\u2500 test_data \u251c\u2500\u2500 tests_subfolder_1 \u2502 \u251c\u2500\u2500 file1.test \u2502 \u2514\u2500\u2500 file2.test \u251c\u2500\u2500 tests_subfolder_1. \u251c\u2500\u2500 tests_subfolder_2 \u2026 etc. when i run get . data\/test_data --out data\/, i get the error: error: failed to get 'data\/test_data\/' from '.' - unable to remove 'data\/test_data\/test_subfolder.' without a confirmation. use `-f` to force. this is surprising to me, because i would expect test_subfolder. and test_subfolder\/ to be able to coexist. however, i don\u2019t know where to put the -f argument? i tried: -f get . data\/test_data --out data\/ get . data\/tests_data --out data\/ --force i also tried to work around the problem with: get . data\/test_data\/* --out data\/ however, my final solution was: ls data\/test_data\/*. | sed -e 's\/\\.$\/\/' | xargs -i {} get . {} --out data\/test_data\/ this work-around feels over-complicated. what was i supposed to do?",
        "Question_original_content_gpt_summary":"The user encountered challenges when attempting to download files tracked using a CI pipeline, and was unable to remove a subfolder without a confirmation, requiring the use of the '-f' argument to force the action.",
        "Question_preprocessed_content":"Title: how to force get; Content: from my ci pipeline, i would like to download some files tracked using . etc. when i run , i get the error this is surprising to me, because i would expect and to be able to coexist. however, i dont know where to put the argument? i tried i also tried to work around the problem with however, my final solution was this work around feels over complicated. what was i supposed to do?",
        "Answer_original_content":"hi -f or --force can go anywhere after get. however, im trying to understand your scenario here and i dont quite get it. you are trying to download data already tracked in your own project, and put it in the same location it already exists? thats what get . data\/test_data --out data\/ looks like. tbh im not even sure why we support . as the url provided to get (and import). see the ref. here: https:\/\/.org\/doc\/command-reference\/get also, is there a data\/test_data. file in your project? otherwise the command should fail, i believe. maybe the error message is wrong and this is the problem. if so would you mind opening a bug report for that in https:\/\/github.com\/iterative\/\/issues? p.s. please note weve officially released 1.0! your project looks like its still using 0.x we highly recommend migrating. 6 months later, im more comfortable with and bash, i can better explain my problem. at the highest level, i wanted a one-liner that downloads all . files in a given sub-folder and i didnt want to write a for-loop. i dont want to do pull, because im not going to be using the version-control features in a ci pipeline and dont need a cache. the original directory structure i posted wasnt clear. here is an improved illustration where each file points to a full folder. data\/ other_data. test_assets rough_tests. smooth_tests. has gone through a lot of improvements over the last six months. is there now a better alternative to my one-liner? seanny123: i wanted a one-liner that downloads all . files in a given sub-folder you would use git to sync . files though. ill assume you mean to get the actual data tracked by . seanny123: my final solution was: ls data\/test_data\/*. | sed -e 's\/\\.$\/\/' | xargs -i {} get . {} --out data\/test_data\/ i dont want to do pull, because (a) im not going to be using the version-control features in a ci pipeline and (b) dont need a cache im still confused about the . url given to get. so the environment in ci is a copy of the repo, but youre just trying to avoid pull? if so a) youre already using git it so avoiding scm doesnt seem to be a concern (or i dont get what the problem is); and b) similarly, not needing a cache shouldnt be a concern? think about that as some internal mechanism uses during the download process which is not a problem: the final result is that you get the data files you want placed where you want them. btw note that pull accepts targets so you can tell it to only download the data in that folder (not everything in the project). the only other path i can think of now would be to change the structure of your repo. instead of adding each file in data\/test_assets (resulting in multiple . files) you can add the whole directory, so you can do this from anywhere in the ci (dont even need to clone the repo): $ get https:\/\/<git url to repo) data\/test_assets --out data\/ seanny123: is there now a better alternative to my one-liner? as for changes in 1.x or 2.x (coming up very soon!), some commands are getting a new --glob option to accept wildcards but get doesnt have it. not sure if its planned but feel free to request that feature in our repo! jorgeorpinel: im still confused about the . url given to get . so the environment in ci is a copy of the repo, but youre just trying to avoid pull ? if so a) youre already using git it so avoiding scm doesnt seem to be a concern (or i dont get what the problem is); and b) similarly, not needing a cache shouldnt be a concern? think about that as some internal mechanism uses during the download process which is not a problem: the final result is that you get the data files you want placed where you want them. in hindsight, i dont know why i was fixated on not needing a cache. i guess i just wanted my ci containers to consume as few resources (including disk space) as possible. however, i never checked my assumption that the cache even took up that much space in this case, so im just going to use pull now. sounds good @seanny123 ! heres some info on how the cache is optimized in (avoiding file duplication in the system): https:\/\/.org\/doc\/user-guide\/large-dataset-optimization best",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi or can go anywhere after . however, im trying to understand your scenario here and i dont quite get it. you are trying to download data already tracked in your own project, and put it in the same location it already exists? thats what looks like. tbh im not even sure why we support as the url provided to . see the ref. here also, is there a file in your project? otherwise the command should fail, i believe. maybe the error message is wrong and this is the problem. if so would you mind opening a bug report for that in please note weve officially released your project looks like its still using we highly recommend migrating. months later, im more comfortable with and , i can better explain my problem. at the highest level, i wanted a one liner that downloads all files in a given sub folder and i didnt want to write a for loop. i dont want to do , because im not going to be using the version control features in a ci pipeline and dont need a cache. the original directory structure i posted wasnt clear. here is an improved illustration where each file points to a full folder. has gone through a lot of improvements over the last six months. is there now a better alternative to my one liner? seanny i wanted a one liner that downloads all files in a given sub folder you would use git to sync . files though. ill assume you mean to get the actual data tracked by . seanny my final solution was i dont want to do pull, because im not going to be using the version control features in a ci pipeline and dont need a cache im still confused about the url given to . so the environment in ci is a copy of the repo, but youre just trying to avoid ? if so a youre already using git it so avoiding scm doesnt seem to be a concern ; and b similarly, not needing a cache shouldnt be a concern? think about that as some internal mechanism uses during the download process which is not a problem the final result is that you get the data files you want placed where you want them. btw note that accepts targets so you can tell it to only download the data in that folder . the only other path i can think of now would be to change the structure of your repo. instead of ing each file in you can add the whole directory, so you can do this from anywhere in the ci seanny is there now a better alternative to my one liner? as for changes in or , some commands are getting a new option to accept wildcards but doesnt have it. not sure if its planned but feel free to request that feature in our repo! jorgeorpinel im still confused about the url given to . so the environment in ci is a copy of the repo, but youre just trying to avoid ? if so a youre already using git it so avoiding scm doesnt seem to be a concern ; and b similarly, not needing a cache shouldnt be a concern? think about that as some internal mechanism uses during the download process which is not a problem the final result is that you get the data files you want placed where you want them. in hindsight, i dont know why i was fixated on not needing a cache. i guess i just wanted my ci containers to consume as few resources as possible. however, i never checked my assumption that the cache even took up that much space in this case, so im just going to use now. sounds good ! heres some on how the cache is optimized in best"
    },
    {
        "Question_id":null,
        "Question_title":"Issues with Guild file - output-scalars and sourcecode",
        "Question_body":"<p>This looks like it would solve the issue but I\u2019m getting new issues with my guild file.<\/p>\n<p>It\u2019s complaining about <code>output-scalars: off<\/code><\/p>\n<pre><code>  operations:\n    search_lr:\n      main: search_lr\n      flags-import: all\n      output-scalars: off\n      requires:\n        - database\n    train:\n<\/code><\/pre>\n<pre><code>ERROR: error loading guildfile from .: error in \/home\/richard\/Documents\/league\/guild.yml: invalid value for output-scalars: False\n<\/code><\/pre>\n<p>I checked here: <a href=\"https:\/\/my.guild.ai\/t\/scalars\/160\" class=\"inline-onebox\">Scalars<\/a> and it seems like I\u2019m doing it correctly?<\/p>\n<p>If I remove that then it starts complaining about my sourcecode:<\/p>\n<pre><code>  sourcecode:\n    - '*.py'\n    - '*.json'\n    - guild.yml\n    - exclude:\n        file:\n          - riot_api_key.py\n        dir:\n          - tb\n          - checkpoints\n          - matches\n          - match_histories\n<\/code><\/pre>\n<pre><code>ERROR: error loading guildfile from .: error in \/home\/richard\/Documents\/league\/guild.yml: invalid exclude value: {'file': ['riot_api_key.py'], 'dir': ['tb', 'checkpoints', 'matches', 'match_histories']}\n<\/code><\/pre>\n<p>The list probably continues after fixing these issues so I think it\u2019s better to wait with this.<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":null,
        "Question_creation_time":1593895791379,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":559.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/issues-with-guild-file-output-scalars-and-sourcecode\/218",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-04T20:59:52.339Z",
                "Answer_body":"<p>I\u2019ll look into the <code>off<\/code> issue - likely a bug.<\/p>\n<p>For your <code>exclude<\/code> spec, it\u2019s either <code>dir<\/code> or <code>file<\/code>. The intent is to denote what type of path is being excluded. There\u2019s an optimization for excluding dirs where Guild doesn\u2019t bother scanning the dir for matches. Otherwise an exclude is treated as a pattern is applied to every file in your project.<\/p>\n<p>You want this:<\/p>\n<pre><code class=\"lang-yaml\"> sourcecode:\n    - '*.py'\n    - '*.json'\n    - guild.yml\n    - exclude: riot_api_key.py\n    - exclude:\n        dir:\n          - tb\n          - checkpoints\n          - matches\n          - match_histories\n<\/code><\/pre>\n<p>I\u2019ve never really liked this spelling. There are other options that apply to <code>include<\/code> and <code>exclude<\/code> so there\u2019s some sense to it, but it\u2019s not particularly intuitive to me.<\/p>",
                "Answer_score":18.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-04T21:02:48.364Z",
                "Answer_body":"<p>Regarding the <code>output-scalars<\/code> error when you use <code>off<\/code> what version of Guild are you using? This is working on my end with 0.7.0.rc11.<\/p>",
                "Answer_score":3.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-04T21:40:47.280Z",
                "Answer_body":"<p>I used the latest master 341abcd46eb13158275cf5254888d32b4b8203cf<\/p>",
                "Answer_score":8.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-04T21:54:45.620Z",
                "Answer_body":"<p>I can\u2019t explain this error with <code>output-scalars<\/code>. Guild doesn\u2019t even generate that string format.<\/p>\n<p>It\u2019s possible that an older version of Guild is in play here?<\/p>\n<p>Does <code>guild check<\/code> shows version <code>0.7.0.rc11<\/code> and <code>guild_install_location<\/code> pointing to the repo code?<\/p>",
                "Answer_score":8.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-04T21:56:41.106Z",
                "Answer_body":"<p>The next is to see if there\u2019s a small Guild file that replicates that and send it my way. Looking over the current source I just don\u2019t see how it\u2019s possible to get that error message.<\/p>",
                "Answer_score":3.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-05T15:46:57.987Z",
                "Answer_body":"<p>You were right. I pulled latest master but it fetched only up to 0.6.4. Didn\u2019t care to find out why but cloned everything again instead. I gave up at the point of updating my PATH variable after building guild. Sorry I don\u2019t feel strongly enough about this to warrant this much time. I can wait for the next release.<\/p>",
                "Answer_score":8.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-05T15:50:28.776Z",
                "Answer_body":"<p>Understandable. For a quick check you can run the script directly (<code>&lt;repo&gt;\/guild\/scripts\/guild<\/code>) without configuring your path. Setting up an alias to that works as well if you want to run from source. No biggie though. There\u2019ll be a quick followup release to 0.7 with this fix.<\/p>",
                "Answer_score":3.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-07-05T16:04:39.235Z",
                "Answer_body":"<p>Ok. I tried this <code>&lt;repo&gt;\/guild\/scripts\/guild tensorboard 1<\/code> and it started very quickly. Seems to work.<\/p>",
                "Answer_score":47.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: issues with guild file - output-scalars and sourcecode; Content: this looks like it would solve the issue but i\u2019m getting new issues with my guild file. it\u2019s complaining about output-scalars: off operations: search_lr: main: search_lr flags-import: all output-scalars: off requires: - database train: error: error loading guildfile from .: error in \/home\/richard\/documents\/league\/guild.yml: invalid value for output-scalars: false i checked here: scalars and it seems like i\u2019m doing it correctly? if i remove that then it starts complaining about my sourcecode: sourcecode: - '*.py' - '*.json' - guild.yml - exclude: file: - riot_api_key.py dir: - tb - checkpoints - matches - match_histories error: error loading guildfile from .: error in \/home\/richard\/documents\/league\/guild.yml: invalid exclude value: {'file': ['riot_api_key.py'], 'dir': ['tb', 'checkpoints', 'matches', 'match_histories']} the list probably continues after fixing these issues so i think it\u2019s better to wait with this.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with their guild file, including issues with output-scalars and sourcecode.",
        "Question_preprocessed_content":"Title: issues with guild file output scalars and sourcecode; Content: this looks like it would solve the issue but im getting new issues with my guild file. its complaining about i checked here scalars and it seems like im doing it correctly? if i remove that then it starts complaining about my sourcecode the list probably continues after fixing these issues so i think its better to wait with this.",
        "Answer_original_content":"ill look into the off issue - likely a bug. for your exclude spec, its either dir or file. the intent is to denote what type of path is being excluded. theres an optimization for excluding dirs where guild doesnt bother scanning the dir for matches. otherwise an exclude is treated as a pattern is applied to every file in your project. you want this: sourcecode: - '*.py' - '*.json' - guild.yml - exclude: riot_api_key.py - exclude: dir: - tb - checkpoints - matches - match_histories ive never really liked this spelling. there are other options that apply to include and exclude so theres some sense to it, but its not particularly intuitive to me. regarding the output-scalars error when you use off what version of guild are you using? this is working on my end with 0.7.0.rc11. i used the latest master 341abcd46eb13158275cf5254888d32b4b8203cf i cant explain this error with output-scalars. guild doesnt even generate that string format. its possible that an older version of guild is in play here? does guild check shows version 0.7.0.rc11 and guild_install_location pointing to the repo code? the next is to see if theres a small guild file that replicates that and send it my way. looking over the current source i just dont see how its possible to get that error message. you were right. i pulled latest master but it fetched only up to 0.6.4. didnt care to find out why but cloned everything again instead. i gave up at the point of updating my path variable after building guild. sorry i dont feel strongly enough about this to warrant this much time. i can wait for the next release. understandable. for a quick check you can run the script directly (<repo>\/guild\/scripts\/guild) without configuring your path. setting up an alias to that works as well if you want to run from source. no biggie though. therell be a quick followup release to 0.7 with this fix. ok. i tried this <repo>\/guild\/scripts\/guild tensorboard 1 and it started very quickly. seems to work.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"ill look into the issue likely a bug. for your spec, its either or . the intent is to denote what type of path is being excluded. theres an optimization for excluding dirs where guild doesnt bother scanning the dir for matches. otherwise an exclude is treated as a pattern is applied to every file in your project. you want this ive never really liked this spelling. there are other options that apply to and so theres some sense to it, but its not particularly intuitive to me. regarding the error when you use what version of guild are you using? this is working on my end with i used the latest master abcd eb cf d b b cf i cant explain this error with . guild doesnt even generate that string format. its possible that an older version of guild is in play here? does shows version and pointing to the repo code? the next is to see if theres a small guild file that replicates that and send it my way. looking over the current source i just dont see how its possible to get that error message. you were right. i pulled latest master but it fetched only up to didnt care to find out why but cloned everything again instead. i gave up at the point of updating my path variable after building guild. sorry i dont feel strongly enough about this to warrant this much time. i can wait for the next release. understandable. for a quick check you can run the script directly without configuring your path. setting up an alias to that works as well if you want to run from source. no biggie though. therell be a quick followup release to with this fix. ok. i tried this and it started very quickly. seems to work."
    },
    {
        "Question_id":69577161.0,
        "Question_title":"Automatically Install OpenJDK into SageMaker Notebook",
        "Question_body":"<p>I have the following lines of code<\/p>\n<pre><code>import tabula\ntabula.environment_info()\n<\/code><\/pre>\n<p>This results in the following error:<\/p>\n<pre><code>    Python version:\n    3.7.10 (default, Jun  4 2021, 14:48:32) \n[GCC 7.5.0]\nJava version:\n    `java -version` faild. `java` command is not found from this Pythonprocess. Please ensure Java is installed and PATH is set for `java`\ntabula-py version: 2.3.0\nplatform: Linux-4.14.243-185.433.amzn2.x86_64-x86_64-with-debian-10.6\nuname:\n    uname_result(system='Linux', node='datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395', release='4.14.243-185.433.amzn2.x86_64', version='#1 SMP Mon Aug 9 05:55:52 UTC 2021', machine='x86_64', processor='')\nlinux_distribution: ('Debian GNU\/Linux', '10', 'buster')\nmac_ver: ('', ('', '', ''), '')\n<\/code><\/pre>\n<p>I am trying to figure out how to set up the appropriate cmd line statement to install OpenJDK from the SageMaker Notebook environment.<\/p>\n<p>I have opened up the terminal and inserted the following line of code and am still seeing the above error.<\/p>\n<pre><code>pip install install-jdk -t.\n<\/code><\/pre>\n<p>Overall, Before installing tabula-py, I need to ensure that I have Java runtime on my environment. How can I facilitate this in SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1634244437823,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":665.0,
        "Owner_creation_time":1615555459547,
        "Owner_last_access_time":1653444707036,
        "Owner_reputation":141.0,
        "Owner_up_votes":4.0,
        "Owner_down_votes":0.0,
        "Owner_views":8.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"United States",
        "Question_last_edit_time":1634263814856,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69577161",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: automatically install openjdk into notebook; Content: i have the following lines of code import tabula tabula.environment_info() this results in the following error: python version: 3.7.10 (default, jun 4 2021, 14:48:32) [gcc 7.5.0] java version: `java -version` faild. `java` command is not found from this pythonprocess. please ensure java is installed and path is set for `java` tabula-py version: 2.3.0 platform: linux-4.14.243-185.433.amzn2.x86_64-x86_64-with-debian-10.6 uname: uname_result(system='linux', node='datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395', release='4.14.243-185.433.amzn2.x86_64', version='#1 smp mon aug 9 05:55:52 utc 2021', machine='x86_64', processor='') linux_distribution: ('debian gnu\/linux', '10', 'buster') mac_ver: ('', ('', '', ''), '') i am trying to figure out how to set up the appropriate cmd line statement to install openjdk from the notebook environment. i have opened up the terminal and inserted the following line of code and am still seeing the above error. pip install install-jdk -t. overall, before installing tabula-py, i need to ensure that i have java runtime on my environment. how can i facilitate this in ?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with automatically installing openjdk into their notebook environment in order to install tabula-py.",
        "Question_preprocessed_content":"Title: automatically install openjdk into notebook; Content: i have the following lines of code this results in the following error i am trying to figure out how to set up the appropriate cmd line statement to install openjdk from the notebook environment. i have opened up the terminal and inserted the following line of code and am still seeing the above error. overall, before installing tabula py, i need to ensure that i have java runtime on my environment. how can i facilitate this in ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":63073171.0,
        "Question_title":"AWS notebook instance boto3 permission denied when reading from s3",
        "Question_body":"<p>I have an AWS instance running in a loop which loads files from an s3 folder as they appear using boto3, reads them, does some processing, then deletes the file. Instance is a sagemaker instance, with full access to the s3 folder.<\/p>\n<p>This process below works fine when there are any number of files in the s3 folder to work through. However if a new file is created while the below loop is running, then when it tries to load that new file at some later point (dataframe = read_csv(filepath, header=None)) then I get a permission denied error. 'is_file_available' spots the file is there, but error occurs when triying to open the file.<\/p>\n<p>Is there something I am missing, e.g.. closing connection?<\/p>\n<p>I have to close \/ restart the kernel and restart the process to fix the issue.<\/p>\n<pre><code># Check if file is available to predict and return file id (int)\ndef is_file_available():\n    my_bucket = s3.Bucket('processing-ml')\n    id = -1\n    for obj in my_bucket.objects.filter(Prefix='to-process\/acc'): #Delimiter=''):\n        filename = obj.key\n        id = mk_int(filename)\n        print('acc.csv found id = ',id)\n        \n    return id\n\n# load a single file as a numpy array\ndef load_file(filepath):\n    dataframe = read_csv(filepath, header=None)\n    return dataframe.values\n\n#load data\ndef load_dataset_group(id):\n    filepath = 's3:\/\/processing-ml\/to-process\/acc' + str(id) + '.csv'\n    print('filepath',filepath)\n    data = load_file(filepath)\n    loaded = list()\n    loaded.append(data)\n    print(data.shape)\n    return loaded\n\nwhile True:\n    #Run forever\n    file_id = is_file_available()\n    if file_id != -1:\n        data = load_dataset_group(file_id)\n\n        ... do stuff with data ...\n\n        #delete the file in s3 now finished with it\n        s3.Object('processing-ml', 'to-process\/acc' + str(file_id) + '.csv').delete()\n\n    time.sleep(1)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1595592190147,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":233.0,
        "Owner_creation_time":1340863305523,
        "Owner_last_access_time":1635364081736,
        "Owner_reputation":177.0,
        "Owner_up_votes":4.0,
        "Owner_down_votes":0.0,
        "Owner_views":14.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1595593113232,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63073171",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: aws notebook instance boto3 permission denied when reading from s3; Content: i have an aws instance running in a loop which loads files from an s3 folder as they appear using boto3, reads them, does some processing, then deletes the file. instance is a instance, with full access to the s3 folder. this process below works fine when there are any number of files in the s3 folder to work through. however if a new file is created while the below loop is running, then when it tries to load that new file at some later point (dataframe = read_csv(filepath, header=none)) then i get a permission denied error. 'is_file_available' spots the file is there, but error occurs when triying to open the file. is there something i am missing, e.g.. closing connection? i have to close \/ restart the kernel and restart the process to fix the issue. # check if file is available to predict and return file id (int) def is_file_available(): my_bucket = s3.bucket('processing-ml') id = -1 for obj in my_bucket.objects.filter(prefix='to-process\/acc'): #delimiter=''): filename = obj.key id = mk_int(filename) print('acc.csv found id = ',id) return id # load a single file as a numpy array def load_file(filepath): dataframe = read_csv(filepath, header=none) return dataframe.values #load data def load_dataset_group(id): filepath = 's3:\/\/processing-ml\/to-process\/acc' + str(id) + '.csv' print('filepath',filepath) data = load_file(filepath) loaded = list() loaded.append(data) print(data.shape) return loaded while true: #run forever file_id = is_file_available() if file_id != -1: data = load_dataset_group(file_id) ... do stuff with data ... #delete the file in s3 now finished with it s3.object('processing-ml', 'to-process\/acc' + str(file_id) + '.csv').delete() time.sleep(1)",
        "Question_original_content_gpt_summary":"The user is encountering a permission denied error when attempting to read a file from an S3 folder using boto3 while the loop is running, despite having full access to the S3 folder.",
        "Question_preprocessed_content":"Title: aws notebook instance boto permission denied when reading from s; Content: i have an aws instance running in a loop which loads files from an s folder as they appear using boto , reads them, does some processing, then deletes the file. instance is a instance, with full access to the s folder. this process below works fine when there are any number of files in the s folder to work through. however if a new file is created while the below loop is running, then when it tries to load that new file at some later point then i get a permission denied error. spots the file is there, but error occurs when triying to open the file. is there something i am missing, closing connection? i have to close \/ restart the kernel and restart the process to fix the issue.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":55386981.0,
        "Question_title":"How to create a docker image for Sagemaker that is not part of the amazon estimator to create an endpoint?",
        "Question_body":"<p>I have built a custom model in Sagemaker and serialized the model through pickle. I want to deploy my model through Sagemaker hosting services and read through this <\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-hosting.html<\/a><\/p>\n\n<p>But I am lost on how to build my own Docker container for a custom model with an algorithm that is currently not implemented as part of the Amazon Estimator.<\/p>\n\n<p>How do I build my own docker image to load into ECR to then build the container that allows me to create an endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1553723285483,
        "Question_favorite_count":1.0,
        "Question_score":3.0,
        "Question_view_count":1616.0,
        "Owner_creation_time":1409314118608,
        "Owner_last_access_time":1560971093567,
        "Owner_reputation":161.0,
        "Owner_up_votes":2.0,
        "Owner_down_votes":0.0,
        "Owner_views":15.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"New York, NY, United States",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55386981",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to create a docker image for that is not part of the amazon estimator to create an endpoint?; Content: i have built a custom model in and serialized the model through pickle. i want to deploy my model through hosting services and read through this https:\/\/docs.aws.amazon.com\/\/latest\/dg\/how-it-works-hosting.html but i am lost on how to build my own docker container for a custom model with an algorithm that is currently not implemented as part of the amazon estimator. how do i build my own docker image to load into ecr to then build the container that allows me to create an endpoint?",
        "Question_original_content_gpt_summary":"The user is facing difficulty in creating a Docker image for a custom model with an algorithm that is not part of the Amazon Estimator in order to create an endpoint.",
        "Question_preprocessed_content":"Title: how to create a docker image for that is not part of the amazon estimator to create an endpoint?; Content: i have built a custom model in and serialized the model through pickle. i want to deploy my model through hosting services and read through this but i am lost on how to build my own docker container for a custom model with an algorithm that is currently not implemented as part of the amazon estimator. how do i build my own docker image to load into ecr to then build the container that allows me to create an endpoint?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":73666883.0,
        "Question_title":"Ingest image to GCP Vertex AI Feature Store",
        "Question_body":"<p>I want to ingest images to Vertex AI Feature Store from Cloud Storage.\nIn what format can I ingest images?<\/p>\n<p>As documentation says:<\/p>\n<p>For batch ingestion, Vertex AI Feature Store can ingest data from tables in BigQuery or files in Cloud Storage. For files in Cloud Storage, they must be in the Avro or CSV format.<\/p>\n<p>For streaming ingestion, you provide the feature values to ingest as part of the API request. These source data requirements don't apply. For more information, see the writeFeatureValues API reference.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1662752993020,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":23.0,
        "Owner_creation_time":1662752793243,
        "Owner_last_access_time":1663269835247,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73666883",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: ingest image to gcp feature store; Content: i want to ingest images to feature store from cloud storage. in what format can i ingest images? as documentation says: for batch ingestion, feature store can ingest data from tables in bigquery or files in cloud storage. for files in cloud storage, they must be in the avro or csv format. for streaming ingestion, you provide the feature values to ingest as part of the api request. these source data requirements don't apply. for more information, see the writefeaturevalues api reference.",
        "Question_original_content_gpt_summary":"The user wants to ingest images to the Feature Store from Cloud Storage and is looking for information on the format requirements for batch and streaming ingestion.",
        "Question_preprocessed_content":"Title: ingest image to gcp feature store; Content: i want to ingest images to feature store from cloud storage. in what format can i ingest images? as documentation says for batch ingestion, feature store can ingest data from tables in bigquery or files in cloud storage. for files in cloud storage, they must be in the avro or csv format. for streaming ingestion, you provide the feature values to ingest as part of the api request. these source data requirements don't apply. for more , see the writefeaturevalues api reference.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":73140531.0,
        "Question_title":"Deploy Pre-Trained model to SageMaker Endpoint from CloudFormation",
        "Question_body":"<p>This seems to be a tricky thing to do, as I haven't found too much documentation for it. I'm trying to deploy a Huggingface pre-trained model for NLU to a SageMaker endpoint. Naturally, I don't want to do this manually, I'd like to automate it through CloudFormation. I found a somewhat <a href=\"https:\/\/faun.pub\/mastering-the-mystical-art-of-model-deployment-part-2-deploying-amazon-sagemaker-endpoints-with-cf9539dc2579\" rel=\"nofollow noreferrer\">useful article<\/a> on how to deploy, but the name of the training model is confusing and I don't know where I would find the right name for the model I want to deploy or where I would put that name (I want to deploy an <a href=\"https:\/\/huggingface.co\/sentence-transformers\/all-MiniLM-L6-v2\" rel=\"nofollow noreferrer\">all-MiniLM-L6-v2<\/a> model).<\/p>\n<p>Is this possible to do? Do I need to deploy a container? If so, how do I set up the container to process requests and return the text embeddings from the model? I've looked into doing this with just a lambda (which would satisfy the automated deployment process), but the packages I need to use greatly exceed the 250MB limit for lambda+layers.<\/p>\n<p>How do I deploy an endpoint from CloudFormation? Does anyone have experience doing this? If so, please share your wisdom.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1658934967237,
        "Question_favorite_count":1.0,
        "Question_score":1.0,
        "Question_view_count":103.0,
        "Owner_creation_time":1605283363407,
        "Owner_last_access_time":1663456472463,
        "Owner_reputation":31.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":5.0,
        "Answer_body":"<p>To anyone curious, this is how I ended up solving this issue:<\/p>\n<p>I ran a Jupyter notebook locally to create the model artifact. Once complete, I zipped the model artifact into a tar.gz file.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import AutoModel, AutoTokenizer\nfrom os import makedirs\n\nsaved_model_dir = 'saved_model_dir'\nmakedirs(saved_model_dir, exist_ok=True)\n\n# models were obtained from https:\/\/huggingface.co\/models\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers\/multi-qa-MiniLM-L6-cos-v1')\n\ntokenizer.save_pretrained(saved_model_dir)\nmodel.save_pretrained(saved_model_dir)\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>cd saved_model_dir &amp;&amp; tar czvf ..\/model.tar.gz *\n<\/code><\/pre>\n<p>I included a script in my pipeline to then upload that artifact to S3.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>aws s3 cp path\/to\/model.tar.gz s3:\/\/bucket-name\/prefix\n<\/code><\/pre>\n<p>I also created a CloudFormation template that would stand up my SageMaker resources. The tricky part of this was finding a container image to use, and a colleague was able to point me to <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">this repo<\/a> that contained a massive list of AWS-maintained container images for deep learning and inference. From there, I just needed to select the one that fit my needs.<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>Resources:\n  SageMakerModel:\n    Type: AWS::SageMaker::Model\n    Properties:\n      PrimaryContainer:\n        Image: 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.12.0-cpu-py38-ubuntu20.04-sagemaker # image resource found at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\n        Mode: SingleModel\n        ModelDataUrl: s3:\/\/path\/to\/model.tar.gz\n      ExecutionRole: \n      ModelName: inference-model\n\n  SageMakerEndpointConfig:\n    Type: AWS::SageMaker::EndpointConfig\n    Properties:\n      EndpointConfigName: endpoint-config-name\n      ProductionVariants:\n        - ModelName: inference-model\n          InitialInstanceCount: 1\n          InstanceType: ml.t2.medium\n          VariantName: dev\n  \n  SageMakerEndpoint:\n    Type: AWS::SageMaker::Endpoint\n    Properties:\n      EndpointName: endpoint-name\n      EndpointConfigName: !GetAtt SageMakerEndpointConfig.EndpointConfigName\n<\/code><\/pre>\n<p>Once the PyTorch model is created locally, this solution essentially automates the process of provisioning and deploying a SageMaker endpoint for inference. If I need to switch the model, I just need to run my notebook code and it will overwrite my existing model artifact. Then I can redeploy and my pipeline will re-upload the artifact to S3, modify the existing SageMaker resources, and the solution will begin operating using the new model.<\/p>\n<p>This may not be the most elegant solution out there, so any suggestions or pointers would be much appreciated!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1659976220936,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1659976426276,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73140531",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deploy pre-trained model to endpoint from cloudformation; Content: this seems to be a tricky thing to do, as i haven't found too much documentation for it. i'm trying to deploy a huggingface pre-trained model for nlu to a endpoint. naturally, i don't want to do this manually, i'd like to automate it through cloudformation. i found a somewhat useful article on how to deploy, but the name of the training model is confusing and i don't know where i would find the right name for the model i want to deploy or where i would put that name (i want to deploy an all-minilm-l6-v2 model). is this possible to do? do i need to deploy a container? if so, how do i set up the container to process requests and return the text embeddings from the model? i've looked into doing this with just a lambda (which would satisfy the automated deployment process), but the packages i need to use greatly exceed the 250mb limit for lambda+layers. how do i deploy an endpoint from cloudformation? does anyone have experience doing this? if so, please share your wisdom.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in deploying a Huggingface pre-trained model for NLU to an endpoint from Cloudformation, as they have not found much documentation for it and are unsure of where to find the right name for the model they want to deploy.",
        "Question_preprocessed_content":"Title: deploy pre trained model to endpoint from cloudformation; Content: this seems to be a tricky thing to do, as i haven't found too much documentation for it. i'm trying to deploy a huggingface pre trained model for nlu to a endpoint. naturally, i don't want to do this manually, i'd like to automate it through cloudformation. i found a somewhat useful article on how to deploy, but the name of the training model is confusing and i don't know where i would find the right name for the model i want to deploy or where i would put that name . is this possible to do? do i need to deploy a container? if so, how do i set up the container to process requests and return the text embeddings from the model? i've looked into doing this with just a lambda , but the packages i need to use greatly exceed the mb limit for lambda+layers. how do i deploy an endpoint from cloudformation? does anyone have experience doing this? if so, please share your wisdom.",
        "Answer_original_content":"to anyone curious, this is how i ended up solving this issue: i ran a jupyter notebook locally to create the model artifact. once complete, i zipped the model artifact into a tar.gz file. from transformers import automodel, autotokenizer from os import makedirs saved_model_dir = 'saved_model_dir' makedirs(saved_model_dir, exist_ok=true) # models were obtained from https:\/\/huggingface.co\/models tokenizer = autotokenizer.from_pretrained('sentence-transformers\/multi-qa-minilm-l6-cos-v1') model = automodel.from_pretrained('sentence-transformers\/multi-qa-minilm-l6-cos-v1') tokenizer.save_pretrained(saved_model_dir) model.save_pretrained(saved_model_dir) cd saved_model_dir && tar czvf ..\/model.tar.gz * i included a script in my pipeline to then upload that artifact to s3. aws s3 cp path\/to\/model.tar.gz s3:\/\/bucket-name\/prefix i also created a cloudformation template that would stand up my resources. the tricky part of this was finding a container image to use, and a colleague was able to point me to this repo that contained a massive list of aws-maintained container images for deep learning and inference. from there, i just needed to select the one that fit my needs. resources: model: type: aws::::model properties: primarycontainer: image: 763104351884.dkr.ecr.us-east-1.amazonaws.com\/pytorch-inference:1.12.0-cpu-py38-ubuntu20.04- # image resource found at https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md mode: singlemodel modeldataurl: s3:\/\/path\/to\/model.tar.gz executionrole: modelname: inference-model endpointconfig: type: aws::::endpointconfig properties: endpointconfigname: endpoint-config-name productionvariants: - modelname: inference-model initialinstancecount: 1 instancetype: ml.t2.medium variantname: dev endpoint: type: aws::::endpoint properties: endpointname: endpoint-name endpointconfigname: !getatt endpointconfig.endpointconfigname once the pytorch model is created locally, this solution essentially automates the process of provisioning and deploying a endpoint for inference. if i need to switch the model, i just need to run my notebook code and it will overwrite my existing model artifact. then i can redeploy and my pipeline will re-upload the artifact to s3, modify the existing resources, and the solution will begin operating using the new model. this may not be the most elegant solution out there, so any suggestions or pointers would be much appreciated!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"to anyone curious, this is how i ended up solving this issue i ran a jupyter notebook locally to create the model artifact. once complete, i zipped the model artifact into a file. i included a script in my pipeline to then upload that artifact to s . i also created a cloudformation template that would stand up my resources. the tricky part of this was finding a container image to use, and a colleague was able to point me to this repo that contained a massive list of aws maintained container images for deep learning and inference. from there, i just needed to select the one that fit my needs. once the pytorch model is created locally, this solution essentially automates the process of provisioning and deploying a endpoint for inference. if i need to switch the model, i just need to run my notebook code and it will overwrite my existing model artifact. then i can redeploy and my pipeline will re upload the artifact to s , modify the existing resources, and the solution will begin operating using the new model. this may not be the most elegant solution out there, so any suggestions or pointers would be much appreciated!"
    },
    {
        "Question_id":null,
        "Question_title":"Managing pipelines operating per dataset element",
        "Question_body":"<p>Hello! I would like to understand the appropriate way to manage pipeline stages that are not \u201csliced\u201d stage-wise, but rather dataset element-wise.<\/p>\n<p>For example, suppose a medical imaging dataset comprises different subjects, each of whom has a collection of files associated to them in a subject-specific directory. Such organization is convenient for manual inspection. Initially, only an image exists for each patient, which is located in S3 and obtained using <code>dvc import-url<\/code>. The to-be-written DVC pipeline will produce additional files (preprocessing, etc.) in each subject\u2019s directory.<\/p>\n<p>My current understanding from DVC\u2019s (excellent) documentation is there are potentially several ways to accomplish this:<br>\n[a] Reshape the dataset such that subjects are distributed across multiple stage directories, which are given as <code>dep<\/code> or <code>out<\/code> to the pipeline.<br>\n[b] List every file as <code>dep<\/code> or <code>out<\/code>.<br>\n[c] Decouple stages from directory structure by having each stage depend on a \u201csummary\u201d or \u201csuccess\u201d file from previous stages instead of a subject\u2019s files or a stage directory.<br>\n[d] Introduce a \u201csummary\u201d stage which creates the desired directory structure view, e.g. via symlinks.<\/p>\n<p>Option [a] discards the desired directory structure, which is extremely undesirable since manual inspection of subjects\u2019 intermediate outputs is common and it is convenient to have all outputs collocated. Option [b] allows to maintain desired directory structure but is unwieldy and perhaps <a href=\"https:\/\/discuss.dvc.org\/t\/advice-for-versioning-many-many-small-files\/609\">not performant<\/a>(?). Option [c] allows to maintain desired directory structure but requires error prone user implementation (e.g. the summary file must reflect the unique combination of files it summarizes, which somewhat reinvents what DVC already does for tracking a directory). Option [d] allows to maintain desired directory structure as a view of another structure DVC likes more, but produces outputs that should not be tracked.<\/p>\n<p>Are there perhaps other options possible? Thank you!<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_creation_time":1609963458190,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":841.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/managing-pipelines-operating-per-dataset-element\/613",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-06T23:04:43.379Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ekhahniii\">@ekhahniii<\/a><\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"ekhahniii\" data-post=\"1\" data-topic=\"613\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/e\/edb3f5\/40.png\" class=\"avatar\"> ekhahniii:<\/div>\n<blockquote>\n<p>pipeline stages that are not \u201csliced\u201d stage-wise, but rather dataset element-wise\u2026<br>\nThe to-be-written DVC pipeline will produce additional files (preprocessing, etc.)  each subject\u2019s directory.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Well, I\u2019m not sure what you mean by \u201cstages that are not sliced stage-wise\u201d. Do you mean most stages use the same dependency (base image) \u2014 and could happen in parallel? Because that is OK, pipelines don\u2019t need to be serial, any graph is supported (examples <a href=\"https:\/\/dvc.org\/doc\/command-reference\/dag#example-visualize-a-dvc-pipeline\">here<\/a> and <a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#parallel-stage-execution\">here<\/a>).<\/p>\n<p><em>But please lmk if you had a different impression from the docs! We could def. improve that.<\/em><\/p>\n<p>In any case, yes: because DVC doesn\u2019t support overlapping output paths (<a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2984#issuecomment-567591765\">here\u2019s why<\/a>), stages can\u2019t share an output directory as a whole.<\/p>\n<p>The simplest workarounds for that are options [a] and [b] as you already identified <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/+1.png?v=9\" title=\":+1:\" class=\"emoji\" alt=\":+1:\"> But [b] becomes undesirable as the number of files grows, which I guess I how you came up with workarounds [c] and [d] (great ideas BTW). I\u2019m not sure about [d] though, since DVC deletes output files every time <code>repro<\/code> starts, and already <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">uses links<\/a> (depending on your file system and <code>cache.type<\/code> config).<\/p>\n<p>That leaves us with between<br>\n<img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/2nd_place_medal.png?v=9\" title=\":2nd_place_medal:\" class=\"emoji\" alt=\":2nd_place_medal:\"> [a] change your project\u2019s structure (but you don\u2019t want that); or<br>\n<img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/1st_place_medal.png?v=9\" title=\":1st_place_medal:\" class=\"emoji\" alt=\":1st_place_medal:\"> [c] generate an intermediate file listing what the next stage should read (which would be the formal <code>dep<\/code> of that stage)<\/p>\n<aside class=\"quote no-group\" data-username=\"ekhahniii\" data-post=\"1\" data-topic=\"613\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/e\/edb3f5\/40.png\" class=\"avatar\"> ekhahniii:<\/div>\n<blockquote>\n<p>Are there perhaps other options possible?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Targeting groups of paths with certain DVC actions (like <code>push<\/code> and <code>pull<\/code>) is now possible via \u201cglobbing\u201d (POSIX-style wildcards e.g. <code>dir\/image-*.*<\/code>) but that\u2019s not yet supported for dependencies and outputs, unfortunately. Feel free to express your support on that in <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1462\" class=\"inline-onebox\">Reconfigurable pipelines \u00b7 Issue #1462 \u00b7 iterative\/dvc \u00b7 GitHub<\/a> and\/or <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/4816\" class=\"inline-onebox\">add support for wildcard\/patterns \u00b7 Issue #4816 \u00b7 iterative\/dvc \u00b7 GitHub<\/a> (we take user opinions very seriously!)<\/p>\n<p>If I come up with, or find another good alternative that\u2019s currently possible, I\u2019ll be sure to let you know.<\/p>",
                "Answer_score":38.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-06T23:26:20.573Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a>,<\/p>\n<p>Thanks so much for the response.<\/p>\n<p>The notion of stage-wise vs element-wise is an allusion to DVC\u2019s view of data as either a collection of files or a single opaque blob (the containing directory). There doesn\u2019t seem (yet?) to be support for any intermediate view. For example, one cannot track a directory and list a file in that directory as a <code>dep<\/code> or \u2018out\u2019(?). And indeed, the difficulty of overlapping output paths is clear (as you noted in this <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2984#issuecomment-567591765\" rel=\"noopener nofollow ugc\">github issue<\/a>).<\/p>\n<p>I generally agree with your assessments of the different options.<\/p>\n<p>For option [c], I guess one gotcha is that the list must also contain hashes of the files.<\/p>\n<p>Regarding [d], I imagined that the pipeline would be constructed as <code>dep<\/code>s between directories (one of the preferred DVC use cases seen in the docs), and that the final \u201csummary\u201d stage would depend on all previous stages\u2019 directories and create a collated view of the content therein, according to the desired directory structure.<\/p>\n<p>It seems no matter what, a long list of <code>dep<\/code>s is unavoidable, since the dataset itself is constructed via many <code>import-url<\/code> commands. Depending on which of the above solutions is used, this list may need only to be in a single location.<\/p>\n<p>I definitely appreciate any further thoughts you might have!<\/p>",
                "Answer_score":18.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-07T03:23:36.269Z",
                "Answer_body":"<p>Np.<\/p>\n<aside class=\"quote no-group\" data-username=\"ekhahniii\" data-post=\"3\" data-topic=\"613\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/e\/edb3f5\/40.png\" class=\"avatar\"> ekhahniii:<\/div>\n<blockquote>\n<p>The notion of stage-wise vs element-wise is an allusion to DVC\u2019s view of data as either a collection of files or a single opaque blob (the containing directory).<\/p>\n<\/blockquote>\n<\/aside>\n<p>I see what you mean. This may not help in answering your practical question but that\u2019s not exactly the case. The only assumption that DVC makes about data is that is file-based (i.e. we don\u2019t provide database versioning or try to understand file formats). In fact you can pull\/push\/import\/etc. specific files inside directories tracked as a whole (we call this \u201ctarget granularity\u201d), so they\u2019re not opaque.<\/p>\n<p>I think that this is more about how files can be grouped. DVC currently only supports files grouped in directories. Hopefully soon we will support the use of wildcards in paths too, which should cover most use cases via file name patterns (please upvote those issues if you would like that).<\/p>\n<aside class=\"quote no-group\" data-username=\"ekhahniii\" data-post=\"3\" data-topic=\"613\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/e\/edb3f5\/40.png\" class=\"avatar\"> ekhahniii:<\/div>\n<blockquote>\n<p>For option [c], I guess one gotcha is that the list must also contain hashes of the files.<\/p>\n<\/blockquote>\n<\/aside>\n<p>So unfortunately, now that I think about this, I don\u2019t think [c] will work because in order for DVC to cache files for versioning, they do need to be specified as outputs in the stage, and again, overlapping output paths are not supported. So <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/1st_place_medal.png?v=9\" title=\":1st_place_medal:\" class=\"emoji\" alt=\":1st_place_medal:\"> after all goes to [a] restructure the dataset so each stage \u201cowns\u201d one+ directories.<\/p>\n<p>But what about introducing a 3rd directory level to keep the best of both worlds? Example:<\/p>\n<pre><code class=\"lang-auto\">medimg\n\u251c\u2500\u2500 imports # base images in a special dir\n\u2502   \u251c\u2500\u2500 patient1.jpg\n\u2502   \u251c\u2500\u2500 patient2.jpg\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 subj1 # subject dirs are shared but sliced:\n\u2502   \u251c\u2500\u2500 1-clean # owned by preprocessing stage\n\u2502   \u2502   \u251c\u2500\u2500 patient1-cln.jpg # artifacts in 3rd level\n\u2502   \u2502   \u251c\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 2-feat  # owned by featurization stage\n\u2502   \u2502   \u251c\u2500\u2500 patientn-ft.jpg # artifacts in 3rd level\n\u2502   \u2514\u2500\u2500 3-train\/\n\u251c\u2500\u2500 subj2\n    \u251c\u2500\u2500 1-clean\/\n    \u251c\u2500\u2500 2-feat\/\n    \u2514\u2500\u2500 3-train\/\n ...\n<\/code><\/pre>\n<p>The downside is that you still need to specify multiple outs and deps per stage (and figure out all these paths inside your code) but it\u2019s only one per subject, which I imagine is much more manageable. E.g.:<\/p>\n<pre><code class=\"lang-auto\"># dvc.yaml\nstages:\n  preprocess:\n    cmd: ...\n    deps:\n    - medimg\/imports\n    outs:\n    - medimg\/subj1\/1-clean\n    - medimg\/subj2\/1-clean\n    - ...\n  featurize:\n    cmd: ...\n    deps: # preprocess outs\n    - medimg\/subj1\/1-clean\n    - medimg\/subj2\/1-clean\n    - ...\n    outs:\n    - medimg\/subj1\/2-feat\n    ...\n<\/code><\/pre>\n<p>And yes, [d] is definitely worth trying too.<\/p>",
                "Answer_score":38.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-13T19:13:59.021Z",
                "Answer_body":"<p>i,<\/p>\n<p>I have recently started to use dvc on a relatively large project and I am very happy with it, except for the problem that is addressed in this thread. Indeed, as a design pattern for data management, I am also relying on the principle of a \u201cdirectory per patient\u201d. The principle is that (1) every \u201cpatient\u201d directory must contains a predefined set of mandatory files; (2) over time as the project progresses, result files from additional\/new analysis can be added to every directory; (3) simultaneously over time as the project progresses, new patients can be added, starting with the mandatory files. Scripts written to update or populate this repository all work based on the principle of looking for the missing files and generating them if required. This permits the addition of a new analysis or a new patient at any time, without having to recompute the whole dataset. This strategy scales relatively well and its parallelisation is also relatively trivial IMHO. But to date, I have not succeeded in deploying this strategy into the stage logic currently provided by dvc. Something is missing, as for example a template yaml file that will serve to generate an up-to-date dvc.yaml file in every directory. Just an idea \u2026<\/p>",
                "Answer_score":27.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-13T21:00:01.621Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"mpagni\" data-post=\"5\" data-topic=\"613\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/mpagni\/40\/87_2.png\" class=\"avatar\"> mpagni:<\/div>\n<blockquote>\n<ol start=\"3\">\n<li>simultaneously over time as the project progresses, new patients can be added<\/li>\n<\/ol>\n<\/blockquote>\n<\/aside>\n<p>Hmmm. I just realized maybe subjects = patients in <a class=\"mention\" href=\"\/u\/ekhahniii\">@ekhahniii<\/a>\u2019s comments (before I thought they referred to areas of study or something like that \u2014 a small number of them would exist).<\/p>\n<p>Anyway, so<\/p>\n<pre><code class=\"lang-auto\">medimg\n\u251c\u2500\u2500 subj1\n\u2502   \u251c\u2500\u2500 base files ...\n\u2502   \u251c\u2500\u2500 1-clean # dir\/file owned by preprocessing stage\n\u2502   \u251c\u2500\u2500 2-feat # owned by featurization stage\n\u2502   ... # etc.\n\u251c\u2500\u2500 subj2 ... # repeat\n...\n\u251c\u2500\u2500 subj_n\n<\/code><\/pre>\n<p>Right? Where <em>n<\/em> may be pretty large. Hmmm\u2026 So again I must conclude the only current workaround is to [a] restructure the structure by introducing intermediate directories, one per stage. And optionally also [d] to add a stage that produces a (symlinked?) view of that underlying structure in the desired format (kind of hacky though).<\/p>\n<p>But there may be upcoming features to solve this (will explain in another comment)\u2026 <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/hourglass.png?v=9\" title=\":hourglass:\" class=\"emoji\" alt=\":hourglass:\"><\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-13T21:44:42.896Z",
                "Answer_body":"<aside class=\"quote no-group quote-modified\" data-username=\"mpagni\" data-post=\"5\" data-topic=\"613\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/mpagni\/40\/87_2.png\" class=\"avatar\"> mpagni:<\/div>\n<blockquote>\n<p>Scripts written to update or populate (work by traversing the dataset directory,) looking for the missing files and generating them if required\u2026<br>\nI have not succeeded in deploying this strategy into the stage logic currently provided by dvc.<\/p>\n<\/blockquote>\n<\/aside>\n<p><a class=\"mention\" href=\"\/u\/mpagni\">@mpagni<\/a> currently while possible it\u2019s not sustainable: you\u2019d need to list every single output\/dependency file per stage in <code>dvc.yaml<\/code>, and update that definition dynamically each time the scripts run.<\/p>\n<p>But there is the possibility that we will enable wildcard patterns in <code>outs\/deps<\/code> definition (see <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/4816#issuecomment-759721635\">https:\/\/github.com\/iterative\/dvc\/issues\/4816#issuecomment-759721635<\/a> and feel free to participate there). Would that solve your issue? Something like<\/p>\n<pre><code class=\"lang-auto\">stages:\n  clean:\n    cmd: .\/cleanup.py medimg\/\n    outs: medimg\/subj*\/cleaned.?\n...\n<\/code><\/pre>\n<aside class=\"quote no-group\" data-username=\"mpagni\" data-post=\"5\" data-topic=\"613\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/mpagni\/40\/87_2.png\" class=\"avatar\"> mpagni:<\/div>\n<blockquote>\n<p>Something is missing, as for example a template yaml file that will serve to generate an up-to-date dvc.yaml file in every directory<\/p>\n<\/blockquote>\n<\/aside>\n<p>We are about to introduce some templating for dvc.yaml in fact, as you can see already published here: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files\/advanced-dvc-yaml\">https:\/\/dvc.org\/doc\/user-guide\/dvc-files\/advanced-dvc-yaml<\/a> . As it is now you could use it as a workaroud for this too, but you would need to generate\/managa a list of outputs\/deps (in a YAML \u201cparameters file\u201d) and load them as global <code>vars<\/code> to iterate on in a <code>foreach<\/code> multi-stage. Probably too involved, but doable. Please take a look and feel free to recommend improvements!<\/p>",
                "Answer_score":27.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: managing pipelines operating per dataset element; Content: hello! i would like to understand the appropriate way to manage pipeline stages that are not \u201csliced\u201d stage-wise, but rather dataset element-wise. for example, suppose a medical imaging dataset comprises different subjects, each of whom has a collection of files associated to them in a subject-specific directory. such organization is convenient for manual inspection. initially, only an image exists for each patient, which is located in s3 and obtained using import-url. the to-be-written pipeline will produce additional files (preprocessing, etc.) in each subject\u2019s directory. my current understanding from \u2019s (excellent) documentation is there are potentially several ways to accomplish this: [a] reshape the dataset such that subjects are distributed across multiple stage directories, which are given as dep or out to the pipeline. [b] list every file as dep or out. [c] decouple stages from directory structure by having each stage depend on a \u201csummary\u201d or \u201csuccess\u201d file from previous stages instead of a subject\u2019s files or a stage directory. [d] introduce a \u201csummary\u201d stage which creates the desired directory structure view, e.g. via symlinks. option [a] discards the desired directory structure, which is extremely undesirable since manual inspection of subjects\u2019 intermediate outputs is common and it is convenient to have all outputs collocated. option [b] allows to maintain desired directory structure but is unwieldy and perhaps not performant(?). option [c] allows to maintain desired directory structure but requires error prone user implementation (e.g. the summary file must reflect the unique combination of files it summarizes, which somewhat reinvents what already does for tracking a directory). option [d] allows to maintain desired directory structure as a view of another structure likes more, but produces outputs that should not be tracked. are there perhaps other options possible? thank you!",
        "Question_original_content_gpt_summary":"The user is facing the challenge of managing pipelines operating per dataset element in order to maintain the desired directory structure while avoiding unwieldy and potentially non-performant solutions.",
        "Question_preprocessed_content":"Title: managing pipelines operating per dataset element; Content: hello! i would like to understand the appropriate way to manage pipeline stages that are not sliced stage wise, but rather dataset element wise. for example, suppose a medical imaging dataset comprises different subjects, each of whom has a collection of files associated to them in a subject specific directory. such organization is convenient for manual inspection. initially, only an image exists for each patient, which is located in s and obtained using . the to be written pipeline will produce additional files in each subjects directory. my current understanding from s documentation is there are potentially several ways to accomplish this a reshape the dataset such that subjects are distributed across multiple stage directories, which are given as or to the pipeline. b list every file as or . c decouple stages from directory structure by having each stage depend on a summary or success file from previous stages instead of a subjects files or a stage directory. d introduce a summary stage which creates the desired directory structure view, via symlinks. option discards the desired directory structure, which is extremely undesirable since manual inspection of subjects intermediate outputs is common and it is convenient to have all outputs collocated. option allows to maintain desired directory structure but is unwieldy and perhaps not performant . option allows to maintain desired directory structure but requires error prone user implementation . option allows to maintain desired directory structure as a view of another structure likes more, but produces outputs that should not be tracked. are there perhaps other options possible? thank you!",
        "Answer_original_content":"hi @ekhahniii ekhahniii: pipeline stages that are not sliced stage-wise, but rather dataset element-wise the to-be-written pipeline will produce additional files (preprocessing, etc.) each subjects directory. well, im not sure what you mean by stages that are not sliced stage-wise. do you mean most stages use the same dependency (base image) and could happen in parallel? because that is ok, pipelines dont need to be serial, any graph is supported (examples here and here). but please lmk if you had a different impression from the docs! we could def. improve that. in any case, yes: because doesnt support overlapping output paths (heres why), stages cant share an output directory as a whole. the simplest workarounds for that are options [a] and [b] as you already identified but [b] becomes undesirable as the number of files grows, which i guess i how you came up with workarounds [c] and [d] (great ideas btw). im not sure about [d] though, since deletes output files every time repro starts, and already uses links (depending on your file system and cache.type config). that leaves us with between [a] change your projects structure (but you dont want that); or [c] generate an intermediate file listing what the next stage should read (which would be the formal dep of that stage) ekhahniii: are there perhaps other options possible? targeting groups of paths with certain actions (like push and pull) is now possible via globbing (posix-style wildcards e.g. dir\/image-*.*) but thats not yet supported for dependencies and outputs, unfortunately. feel free to express your support on that in reconfigurable pipelines issue #1462 iterative\/ github and\/or add support for wildcard\/patterns issue #4816 iterative\/ github (we take user opinions very seriously!) if i come up with, or find another good alternative thats currently possible, ill be sure to let you know. hi @jorgeorpinel, thanks so much for the response. the notion of stage-wise vs element-wise is an allusion to s view of data as either a collection of files or a single opaque blob (the containing directory). there doesnt seem (yet?) to be support for any intermediate view. for example, one cannot track a directory and list a file in that directory as a dep or out(?). and indeed, the difficulty of overlapping output paths is clear (as you noted in this github issue). i generally agree with your assessments of the different options. for option [c], i guess one gotcha is that the list must also contain hashes of the files. regarding [d], i imagined that the pipeline would be constructed as deps between directories (one of the preferred use cases seen in the docs), and that the final summary stage would depend on all previous stages directories and create a collated view of the content therein, according to the desired directory structure. it seems no matter what, a long list of deps is unavoidable, since the dataset itself is constructed via many import-url commands. depending on which of the above solutions is used, this list may need only to be in a single location. i definitely appreciate any further thoughts you might have! np. ekhahniii: the notion of stage-wise vs element-wise is an allusion to s view of data as either a collection of files or a single opaque blob (the containing directory). i see what you mean. this may not help in answering your practical question but thats not exactly the case. the only assumption that makes about data is that is file-based (i.e. we dont provide database versioning or try to understand file formats). in fact you can pull\/push\/import\/etc. specific files inside directories tracked as a whole (we call this target granularity), so theyre not opaque. i think that this is more about how files can be grouped. currently only supports files grouped in directories. hopefully soon we will support the use of wildcards in paths too, which should cover most use cases via file name patterns (please upvote those issues if you would like that). ekhahniii: for option [c], i guess one gotcha is that the list must also contain hashes of the files. so unfortunately, now that i think about this, i dont think [c] will work because in order for to cache files for versioning, they do need to be specified as outputs in the stage, and again, overlapping output paths are not supported. so after all goes to [a] restructure the dataset so each stage owns one+ directories. but what about introducing a 3rd directory level to keep the best of both worlds? example: medimg imports # base images in a special dir patient1.jpg patient2.jpg ... subj1 # subject dirs are shared but sliced: 1-clean # owned by preprocessing stage patient1-cln.jpg # artifacts in 3rd level ... 2-feat # owned by featurization stage patientn-ft.jpg # artifacts in 3rd level 3-train\/ subj2 1-clean\/ 2-feat\/ 3-train\/ ... the downside is that you still need to specify multiple outs and deps per stage (and figure out all these paths inside your code) but its only one per subject, which i imagine is much more manageable. e.g.: # .yaml stages: preprocess: cmd: ... deps: - medimg\/imports outs: - medimg\/subj1\/1-clean - medimg\/subj2\/1-clean - ... featurize: cmd: ... deps: # preprocess outs - medimg\/subj1\/1-clean - medimg\/subj2\/1-clean - ... outs: - medimg\/subj1\/2-feat ... and yes, [d] is definitely worth trying too. i, i have recently started to use on a relatively large project and i am very happy with it, except for the problem that is addressed in this thread. indeed, as a design pattern for data management, i am also relying on the principle of a directory per patient. the principle is that (1) every patient directory must contains a predefined set of mandatory files; (2) over time as the project progresses, result files from additional\/new analysis can be added to every directory; (3) simultaneously over time as the project progresses, new patients can be added, starting with the mandatory files. scripts written to update or populate this repository all work based on the principle of looking for the missing files and generating them if required. this permits the addition of a new analysis or a new patient at any time, without having to recompute the whole dataset. this strategy scales relatively well and its parallelisation is also relatively trivial imho. but to date, i have not succeeded in deploying this strategy into the stage logic currently provided by . something is missing, as for example a template yaml file that will serve to generate an up-to-date .yaml file in every directory. just an idea mpagni: simultaneously over time as the project progresses, new patients can be added hmmm. i just realized maybe subjects = patients in @ekhahniiis comments (before i thought they referred to areas of study or something like that a small number of them would exist). anyway, so medimg subj1 base files ... 1-clean # dir\/file owned by preprocessing stage 2-feat # owned by featurization stage ... # etc. subj2 ... # repeat ... subj_n right? where n may be pretty large. hmmm so again i must conclude the only current workaround is to [a] restructure the structure by introducing intermediate directories, one per stage. and optionally also [d] to add a stage that produces a (symlinked?) view of that underlying structure in the desired format (kind of hacky though). but there may be upcoming features to solve this (will explain in another comment) mpagni: scripts written to update or populate (work by traversing the dataset directory,) looking for the missing files and generating them if required i have not succeeded in deploying this strategy into the stage logic currently provided by . @mpagni currently while possible its not sustainable: youd need to list every single output\/dependency file per stage in .yaml, and update that definition dynamically each time the scripts run. but there is the possibility that we will enable wildcard patterns in outs\/deps definition (see https:\/\/github.com\/iterative\/\/issues\/4816#issuecomment-759721635 and feel free to participate there). would that solve your issue? something like stages: clean: cmd: .\/cleanup.py medimg\/ outs: medimg\/subj*\/cleaned.? ... mpagni: something is missing, as for example a template yaml file that will serve to generate an up-to-date .yaml file in every directory we are about to introduce some templating for .yaml in fact, as you can see already published here: https:\/\/.org\/doc\/user-guide\/-files\/advanced--yaml . as it is now you could use it as a workaroud for this too, but you would need to generate\/managa a list of outputs\/deps (in a yaml parameters file) and load them as global vars to iterate on in a foreach multi-stage. probably too involved, but doable. please take a look and feel free to recommend improvements!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi ekhahniii pipeline stages that are not sliced stage wise, but rather dataset element wise the to be written pipeline will produce additional files each subjects directory. well, im not sure what you mean by stages that are not sliced stage wise. do you mean most stages use the same dependency and could happen in parallel? because that is ok, pipelines dont need to be serial, any graph is supported . but please lmk if you had a different impression from the docs! we could def. improve that. in any case, yes because doesnt support overlapping output paths , stages cant share an output directory as a whole. the simplest workarounds for that are options and as you already identified but becomes undesirable as the number of files grows, which i guess i how you came up with workarounds and . im not sure about though, since deletes output files every time starts, and already uses links . that leaves us with between change your projects structure ; or generate an intermediate file listing what the next stage should read ekhahniii are there perhaps other options possible? targeting groups of paths with certain actions is now possible via globbing but thats not yet supported for dependencies and outputs, unfortunately. feel free to express your support on that in reconfigurable pipelines issue github add support for issue github if i come up with, or find another good alternative thats currently possible, ill be sure to let you know. hi thanks so much for the response. the notion of stage wise vs element wise is an allusion to s view of data as either a collection of files or a single opaque blob . there doesnt seem to be support for any intermediate view. for example, one cannot track a directory and list a file in that directory as a or out . and indeed, the difficulty of overlapping output paths is clear . i generally agree with your assessments of the different options. for option , i guess one gotcha is that the list must also contain hashes of the files. regarding , i imagined that the pipeline would be constructed as s between directories , and that the final summary stage would depend on all previous stages directories and create a collated view of the content therein, according to the desired directory structure. it seems no matter what, a long list of s is unavoidable, since the dataset itself is constructed via many commands. depending on which of the above solutions is used, this list may need only to be in a single location. i definitely appreciate any further thoughts you might have! np. ekhahniii the notion of stage wise vs element wise is an allusion to s view of data as either a collection of files or a single opaque blob . i see what you mean. this may not help in answering your practical question but thats not exactly the case. the only assumption that makes about data is that is file based . in fact you can specific files inside directories tracked as a whole , so theyre not opaque. i think that this is more about how files can be grouped. currently only supports files grouped in directories. hopefully soon we will support the use of wildcards in paths too, which should cover most use cases via file name patterns . ekhahniii for option , i guess one gotcha is that the list must also contain hashes of the files. so unfortunately, now that i think about this, i dont think will work because in order for to cache files for versioning, they do need to be specified as outputs in the stage, and again, overlapping output paths are not supported. so after all goes to restructure the dataset so each stage owns one+ directories. but what about introducing a rd directory level to keep the best of both worlds? example the downside is that you still need to specify multiple outs and deps per stage but its only one per subject, which i imagine is much more manageable. and yes, is definitely worth trying too. i, i have recently started to use on a relatively large project and i am very happy with it, except for the problem that is addressed in this thread. indeed, as a design pattern for data management, i am also relying on the principle of a directory per patient. the principle is that every patient directory must contains a predefined set of mandatory files; over time as the project progresses, result files from analysis can be added to every directory; simultaneously over time as the project progresses, new patients can be added, starting with the mandatory files. scripts written to update or populate this repository all work based on the principle of looking for the missing files and generating them if required. this permits the addition of a new analysis or a new patient at any time, without having to recompute the whole dataset. this strategy scales relatively well and its parallelisation is also relatively trivial imho. but to date, i have not succeeded in deploying this strategy into the stage logic currently provided by . something is missing, as for example a template yaml file that will serve to generate an up to date file in every directory. just an idea mpagni simultaneously over time as the project progresses, new patients can be added hmmm. i just realized maybe subjects patients in comments . anyway, so right? where n may be pretty large. hmmm so again i must conclude the only current workaround is to restructure the structure by introducing intermediate directories, one per stage. and optionally also to add a stage that produces a view of that underlying structure in the desired format . but there may be upcoming features to solve this mpagni scripts written to update or populate looking for the missing files and generating them if required i have not succeeded in deploying this strategy into the stage logic currently provided by . currently while possible its not sustainable youd need to list every single file per stage in , and update that definition dynamically each time the scripts run. but there is the possibility that we will enable wildcard patterns in definition . would that solve your issue? something like mpagni something is missing, as for example a template yaml file that will serve to generate an up to date file in every directory we are about to introduce some templating for in fact, as you can see already published here . as it is now you could use it as a workaroud for this too, but you would need to a list of and load them as global to iterate on in a multi stage. probably too involved, but doable. please take a look and feel free to recommend improvements!"
    },
    {
        "Question_id":null,
        "Question_title":"slowness of azure ml cloud compute",
        "Question_body":"I am doing a speed comparison test between my machine and azure ml cloud compute. My PC is 8 cores with 64 GB RAM. The compute instance I created on azure is \"Standard_D14_v2 (16 cores, 112 GB RAM, 800 GB disk)\". One test I did was to run a XGBoost model and it took about 1h15m locally. On the cloud, it took 1h45m. I thought I would have a better performance with the instance created on azure. Could someone explain to me why I saw the opposite? Thank you so much!",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1618714586367,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/361307\/slowness-of-azure-ml-cloud-compute.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-19T06:01:13.52Z",
                "Answer_score":0,
                "Answer_body":"@XrLi23-3087 One factor that could effect while running your experiment on Azure ML is the setup time of the compute while running the experiment. Normally, compute is setup run time when the experiment run is initiated to avoid costs of running compute when not required. In this case the difference is still 30 minutes which is large and the setup usually takes few minutes depending on the availability of compute and region you are using.\n\nAnother factor could be the settings used in the experiment, since you are using the SDK or designer the default settings might be set for some configurations for optimization. If these settings are not changed then these might slow down the run time of the experiment, depending on your configuration these settings can be reviewed to check if there is a possibility to optimize it further.\n\nIf the above scenarios do not help we would recommend to raise a support issue to check what could be causing slowness in your workspace. Thanks.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: slowness of cloud compute; Content: i am doing a speed comparison test between my machine and cloud compute. my pc is 8 cores with 64 gb ram. the compute instance i created on azure is \"standard_d14_v2 (16 cores, 112 gb ram, 800 gb disk)\". one test i did was to run a xgboost model and it took about 1h15m locally. on the cloud, it took 1h45m. i thought i would have a better performance with the instance created on azure. could someone explain to me why i saw the opposite? thank you so much!",
        "Question_original_content_gpt_summary":"The user encountered a challenge with the slowness of cloud compute, as their xgboost model took 1h45m to run on the cloud instance they created on Azure, despite having more cores and RAM than their local machine.",
        "Question_preprocessed_content":"Title: slowness of cloud compute; Content: i am doing a speed comparison test between my machine and cloud compute. my pc is cores with gb ram. the compute instance i created on azure is . one test i did was to run a xgboost model and it took about h m locally. on the cloud, it took h m. i thought i would have a better performance with the instance created on azure. could someone explain to me why i saw the opposite? thank you so much!",
        "Answer_original_content":"@xrli23-3087 one factor that could effect while running your experiment on is the setup time of the compute while running the experiment. normally, compute is setup run time when the experiment run is initiated to avoid costs of running compute when not required. in this case the difference is still 30 minutes which is large and the setup usually takes few minutes depending on the availability of compute and region you are using. another factor could be the settings used in the experiment, since you are using the sdk or designer the default settings might be set for some configurations for optimization. if these settings are not changed then these might slow down the run time of the experiment, depending on your configuration these settings can be reviewed to check if there is a possibility to optimize it further. if the above scenarios do not help we would recommend to raise a support issue to check what could be causing slowness in your workspace. thanks.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"one factor that could effect while running your experiment on is the setup time of the compute while running the experiment. normally, compute is setup run time when the experiment run is initiated to avoid costs of running compute when not required. in this case the difference is still minutes which is large and the setup usually takes few minutes depending on the availability of compute and region you are using. another factor could be the settings used in the experiment, since you are using the sdk or designer the default settings might be set for some configurations for optimization. if these settings are not changed then these might slow down the run time of the experiment, depending on your configuration these settings can be reviewed to check if there is a possibility to optimize it further. if the above scenarios do not help we would recommend to raise a support issue to check what could be causing slowness in your workspace. thanks."
    },
    {
        "Question_id":60334889.0,
        "Question_title":"\"No Kernel!\" error Azure ML compute JupyterLab",
        "Question_body":"<p>When using the JupyterLab found within the azure ML compute instance, every now and then, I run into an issue where it will say that network connection is lost. <\/p>\n\n<p>I have confirmed that the computer is still running.\nthe notebook itself can be edited and saved, so the computer\/VM is definitely running\nOf course, the internet is fully functional<\/p>\n\n<p>On the top right corner <em>next to the now blank circle<\/em> it will say \"No Kernel!\"<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0.0,
        "Question_creation_time":1582274394260,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":1026.0,
        "Owner_creation_time":1442334437952,
        "Owner_last_access_time":1664002198907,
        "Owner_reputation":2272.0,
        "Owner_up_votes":1340.0,
        "Owner_down_votes":67.0,
        "Owner_views":516.0,
        "Answer_body":"<p>We can't repro the issue, can you help gives us more details? One possibility is that the kernel has bugs and hangs (could be due to extensions, widgets installed) or the resources on the machine are exhausted and kernel dies. What VM type are you using? If it's a small VM you may ran out of resources.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1582323157643,
        "Answer_score":1.0,
        "Owner_location":"Bangalore, Karnataka, India",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60334889",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: \"no kernel!\" error compute jupyterlab; Content: when using the jupyterlab found within the compute instance, every now and then, i run into an issue where it will say that network connection is lost. i have confirmed that the computer is still running. the notebook itself can be edited and saved, so the computer\/vm is definitely running of course, the internet is fully functional on the top right corner next to the now blank circle it will say \"no kernel!\"",
        "Question_original_content_gpt_summary":"The user encountered an issue where their JupyterLab instance within their compute instance would display a \"no kernel!\" error despite the computer\/VM still running and the internet being fully functional.",
        "Question_preprocessed_content":"Title: no kernel! error compute jupyterlab; Content: when using the jupyterlab found within the compute instance, every now and then, i run into an issue where it will say that network connection is lost. i have confirmed that the computer is still running. the notebook itself can be edited and saved, so the is definitely running of course, the internet is fully functional on the top right corner next to the now blank circle it will say no kernel!",
        "Answer_original_content":"we can't repro the issue, can you help gives us more details? one possibility is that the kernel has bugs and hangs (could be due to extensions, widgets installed) or the resources on the machine are exhausted and kernel dies. what vm type are you using? if it's a small vm you may ran out of resources.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"we can't repro the issue, can you help gives us more details? one possibility is that the kernel has bugs and hangs or the resources on the machine are exhausted and kernel dies. what vm type are you using? if it's a small vm you may ran out of resources."
    },
    {
        "Question_id":71790268.0,
        "Question_title":"Trouble reading Blob Storage File into Azure ML Notebook",
        "Question_body":"<p>I have an Excel file uploaded to my ML workspace.<\/p>\n<p>I can access the file as an azure FileDataset object. However, I don't know how to get it into a pandas DataFrame since 'FileDataset' object has no attribute 'to_dataframe'.<\/p>\n<p>Azure ML notebooks seem to make a point of avoiding pandas for some reason.<\/p>\n<p>Does anyone know how to get blob files into pandas dataframes from within Azure ML notebooks?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1649376475963,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":363.0,
        "Owner_creation_time":1532366391747,
        "Owner_last_access_time":1663965033456,
        "Owner_reputation":29.0,
        "Owner_up_votes":6.0,
        "Owner_down_votes":0.0,
        "Owner_views":22.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Huntington Beach, CA, USA",
        "Question_last_edit_time":1649415226020,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71790268",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: trouble reading blob storage file into notebook; Content: i have an excel file uploaded to my ml workspace. i can access the file as an azure filedataset object. however, i don't know how to get it into a pandas dataframe since 'filedataset' object has no attribute 'to_dataframe'. notebooks seem to make a point of avoiding pandas for some reason. does anyone know how to get blob files into pandas dataframes from within notebooks?",
        "Question_original_content_gpt_summary":"The user is encountering difficulty reading a blob storage file into a notebook, as the 'filedataset' object has no attribute 'to_dataframe' and notebooks seem to avoid using pandas.",
        "Question_preprocessed_content":"Title: trouble reading blob storage file into notebook; Content: i have an excel file uploaded to my ml workspace. i can access the file as an azure filedataset object. however, i don't know how to get it into a pandas dataframe since 'filedataset' object has no attribute notebooks seem to make a point of avoiding pandas for some reason. does anyone know how to get blob files into pandas dataframes from within notebooks?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":35057156.0,
        "Question_title":"Azure ML batch execution latency",
        "Question_body":"<p>After reading this post here: <a href=\"https:\/\/stackoverflow.com\/questions\/34990561\/azure-machine-learning-request-response-latency\/35020997#35020997?newreg=18fbd305056d4e6ba239783ebefb9629\">Azure Machine Learning Request Response latency<\/a>\nand the article mentioned in the comments I was wondering if this behavior is also true when a published webservice is called in batch mode. \nEspecially since I have read somewhere (sorry, can't find the link at the moment) that the batch calls are not influenced by the \"concurrent calls\" config...<\/p>\n\n<p>In our scenario we have a custom R module uploaded to our workspace which includes some libraries that are not available on aML by default. The module takes a dataset, trains a binary tree, creates some plots and encodes them in base64 before returning those as a dataset. Locally that does not take more than 5s. But in the aML webservice it takes approx. 90s and it seems that the runtime in batchmode does not improve when calling the service multiple times.<\/p>\n\n<p>Additionally it would be nice to know for how long the containers, mentioned in the linked post, will stay warm.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":5.0,
        "Question_creation_time":1453972229010,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":212.0,
        "Owner_creation_time":1453971071070,
        "Owner_last_access_time":1458566284110,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1495540755230,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35057156",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: batch execution latency; Content: after reading this post here: request response latency and the article mentioned in the comments i was wondering if this behavior is also true when a published webservice is called in batch mode. especially since i have read somewhere (sorry, can't find the link at the moment) that the batch calls are not influenced by the \"concurrent calls\" config... in our scenario we have a custom r module uploaded to our workspace which includes some libraries that are not available on aml by default. the module takes a dataset, trains a binary tree, creates some plots and encodes them in base64 before returning those as a dataset. locally that does not take more than 5s. but in the aml webservice it takes approx. 90s and it seems that the runtime in batchmode does not improve when calling the service multiple times. additionally it would be nice to know for how long the containers, mentioned in the linked post, will stay warm.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with batch execution latency when calling an Azure Machine Learning webservice with a custom R module, which is taking significantly longer than expected and not improving with multiple calls.",
        "Question_preprocessed_content":"Title: batch execution latency; Content: after reading this post here request response latency and the article mentioned in the comments i was wondering if this behavior is also true when a published webservice is called in batch mode. especially since i have read somewhere that the batch calls are not influenced by the concurrent calls in our scenario we have a custom r module uploaded to our workspace which includes some libraries that are not available on aml by default. the module takes a dataset, trains a binary tree, creates some plots and encodes them in base before returning those as a dataset. locally that does not take more than s. but in the aml webservice it takes approx. s and it seems that the runtime in batchmode does not improve when calling the service multiple times. additionally it would be nice to know for how long the containers, mentioned in the linked post, will stay warm.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"How can I use a working pipeline",
        "Question_body":"Hi,\n\nI created a working pipeline in azure machine learning studio but I am stuck how i can use it with a live dataset. Could anybody help to me in this issue? I dont have such option to deploy it.\n\nthank you in advance",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1640685619343,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/677175\/how-can-i-use-a-working-pipeline.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-28T21:24:18.68Z",
                "Answer_score":0,
                "Answer_body":"Hi, please review Test the real-time endpoint for more details on how to test your model. You can consume your model using a Client or PowerBI.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i use a working pipeline; Content: hi, i created a working pipeline in studio but i am stuck how i can use it with a live dataset. could anybody help to me in this issue? i dont have such option to deploy it. thank you in advance",
        "Question_original_content_gpt_summary":"The user is stuck on how to use their working pipeline with a live dataset and is looking for help on how to deploy it.",
        "Question_preprocessed_content":"Title: how can i use a working pipeline; Content: hi, i created a working pipeline in studio but i am stuck how i can use it with a live dataset. could anybody help to me in this issue? i dont have such option to deploy it. thank you in advance",
        "Answer_original_content":"hi, please review test the real-time endpoint for more details on how to test your model. you can consume your model using a client or powerbi. --- kindly accept answer if the information helps. thanks.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, please review test the real time endpoint for more details on how to test your model. you can consume your model using a client or powerbi. kindly accept answer if the helps. thanks."
    },
    {
        "Question_id":72516242.0,
        "Question_title":"Is there any limitations for runs per users in Azure ML experiments?",
        "Question_body":"<p>I and my team members are working on a machine learning project through the <strong>Azure ML<\/strong> portal.\nWe have created a specific <em>experiment<\/em> in our <em>workspace<\/em> in Azure ML and are submitting our Python script <em>runs<\/em> from our local or remote machines in this experiment.<\/p>\n<p>Although I'm collaborating with my colleagues, most of the runs in this specific experiment are submitted by me.<\/p>\n<p>Recently, I have faced a problem with experiment submissions.\nThe problem is that after some number of experiments created by me, I cannot add any other runs to this experiment, but my colleagues can!!!<\/p>\n<p>Unfortunately, the Azure ML portal does not show any clear error message for this problem. It continues submitting the run till a <em>timeout<\/em> exception occurs!<\/p>\n<p>As a temporary solution, I've just changed the name of the experiment and I could conquer this problem.<\/p>\n<p>This solution helped me to submit my run on <strong>Azure ML<\/strong> but it didn\u2019t satisfy me because we want to collect all related runs under a specific experiment. On the other hand creating multiple number of experiments for each run is overwhelming!<\/p>\n<p>What I know is that there are some service limits for the number of runs in a workspace on this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-limits-quotas-capacity\" rel=\"nofollow noreferrer\">page<\/a>. I am sure that the number of runs in our workspace has not reached to the 10 millions, because I can created new runs under new experiments dashboard. But I don\u2019t know anything about the limitations on the number of runs in a specific experiment or even any limitations for the number of runs per users in a specific experiment.\nI couldn't find any clear document explaining this fact.<\/p>\n<p>Is there anyone who can help me for this issue?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1654510800217,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":82.0,
        "Owner_creation_time":1558366212312,
        "Owner_last_access_time":1663775379147,
        "Owner_reputation":425.0,
        "Owner_up_votes":216.0,
        "Owner_down_votes":6.0,
        "Owner_views":32.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Zanjan, Zanjan Province, Iran",
        "Question_last_edit_time":1655619106363,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72516242",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is there any limitations for runs per users in experiments?; Content: i and my team members are working on a machine learning project through the portal. we have created a specific experiment in our workspace in and are submitting our python script runs from our local or remote machines in this experiment. although i'm collaborating with my colleagues, most of the runs in this specific experiment are submitted by me. recently, i have faced a problem with experiment submissions. the problem is that after some number of experiments created by me, i cannot add any other runs to this experiment, but my colleagues can!!! unfortunately, the portal does not show any clear error message for this problem. it continues submitting the run till a timeout exception occurs! as a temporary solution, i've just changed the name of the experiment and i could conquer this problem. this solution helped me to submit my run on but it didn\u2019t satisfy me because we want to collect all related runs under a specific experiment. on the other hand creating multiple number of experiments for each run is overwhelming! what i know is that there are some service limits for the number of runs in a workspace on this page. i am sure that the number of runs in our workspace has not reached to the 10 millions, because i can created new runs under new experiments dashboard. but i don\u2019t know anything about the limitations on the number of runs in a specific experiment or even any limitations for the number of runs per users in a specific experiment. i couldn't find any clear document explaining this fact. is there anyone who can help me for this issue?",
        "Question_original_content_gpt_summary":"The user is facing a challenge with submitting runs to a specific experiment on the portal, as they are unable to add any more runs after a certain number, while their colleagues can.",
        "Question_preprocessed_content":"Title: is there any limitations for runs per users in experiments?; Content: i and my team members are working on a machine learning project through the portal. we have created a specific experiment in our workspace in and are submitting our python script runs from our local or remote machines in this experiment. although i'm collaborating with my colleagues, most of the runs in this specific experiment are submitted by me. recently, i have faced a problem with experiment submissions. the problem is that after some number of experiments created by me, i cannot add any other runs to this experiment, but my colleagues can!!! unfortunately, the portal does not show any clear error message for this problem. it continues submitting the run till a timeout exception occurs! as a temporary solution, i've just changed the name of the experiment and i could conquer this problem. this solution helped me to submit my run on but it didnt satisfy me because we want to collect all related runs under a specific experiment. on the other hand creating multiple number of experiments for each run is overwhelming! what i know is that there are some service limits for the number of runs in a workspace on this page. i am sure that the number of runs in our workspace has not reached to the millions, because i can created new runs under new experiments dashboard. but i dont know anything about the limitations on the number of runs in a specific experiment or even any limitations for the number of runs per users in a specific experiment. i couldn't find any clear document explaining this fact. is there anyone who can help me for this issue?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":69916579.0,
        "Question_title":"how to provide secrets to an azure machine learning inference server",
        "Question_body":"<p>Within our azure machine learning inference server we want to be able to make calls to a password protected database.\nWhat is the best way to provide a secret to the running inference server?<\/p>\n<p>I have tried following this tutorial: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-secrets-in-runs\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-secrets-in-runs<\/a>.\nHowever the request to <code>run = Run.get_context()<\/code> always returns an offline instance.\nPresumably that is only used for passing secrets to models when training them?<\/p>\n<p>I was also thinking I could pass my secrets in as env variables but I'm not sure if that is safe?<\/p>\n<pre><code>env = Environment.from_pip_requirements(env_name, &quot;requirements-azure.txt&quot;)\nenv.environment_variables.update({&quot;mysecret&quot;: &quot;password&quot;})\n<\/code><\/pre>\n<p>Any suggestions are greatly appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1636560628423,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":159.0,
        "Owner_creation_time":1464625819448,
        "Owner_last_access_time":1664014836943,
        "Owner_reputation":1076.0,
        "Owner_up_votes":59.0,
        "Owner_down_votes":2.0,
        "Owner_views":106.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1638211541472,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69916579",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to provide secrets to an inference server; Content: within our inference server we want to be able to make calls to a password protected database. what is the best way to provide a secret to the running inference server? i have tried following this tutorial: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-secrets-in-runs. however the request to run = run.get_context() always returns an offline instance. presumably that is only used for passing secrets to models when training them? i was also thinking i could pass my secrets in as env variables but i'm not sure if that is safe? env = environment.from_pip_requirements(env_name, \"requirements-azure.txt\") env.environment_variables.update({\"mysecret\": \"password\"}) any suggestions are greatly appreciated!",
        "Question_original_content_gpt_summary":"The user is encountering challenges in providing secrets to an inference server, and is trying to find the best way to do so.",
        "Question_preprocessed_content":"Title: how to provide secrets to an inference server; Content: within our inference server we want to be able to make calls to a password protected database. what is the best way to provide a secret to the running inference server? i have tried following this tutorial however the request to always returns an offline instance. presumably that is only used for passing secrets to models when training them? i was also thinking i could pass my secrets in as env variables but i'm not sure if that is safe? any suggestions are greatly appreciated!",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70385171.0,
        "Question_title":"MLFLOW on Databricks - Cannot log a Keras model as a mlflow.pyfunc model. Get TypeError: cannot pickle 'weakref' object",
        "Question_body":"<p>Hi all: this is one of my first posts on Stackoverflow - so apologies in advance if i'm not conforming to certain standards!<\/p>\n<p>I'm having trouble saving my Keras model as a <code>mlflow.pyfunc<\/code> model as it's giving me a &quot;cannot pickle a 'weakref' object when I try to log it.<\/p>\n<p><strong>So why am i saving my Keras model as a pyfunc model object in the first place? This is because I want to override the default predict method and output something custom<\/strong>. I also want to do some pre-processing steps on the X_test or new data by encoding it with a tf.keras.StringLookup and then invert it back to get the original categorical variable class. For this reason, I was advised by Databricks that the mlflow.pyfunc flavor is the best way to go for these types of use-cases<\/p>\n<p>The Keras model works just fine and i'm able to log it using <code>mlflow.keras.log_model<\/code>. But it fails when i try to wrap it inside a cutomer &quot;KerasWrapper&quot; class.<\/p>\n<p>Here are some snippets of my code. For the purpose of debugging, the current <code>predict<\/code> method in the custom class is just the default. I simplified it to help debug, but obviously I haven't been able to resolve it.<\/p>\n<p>I would be extremely grateful for any help. Thanks in advance!<\/p>\n<p><strong>ALL CODE ON AZURE DATABRICKS<\/strong><\/p>\n<p><strong>Custom mlflow.pyfunc class<\/strong><\/p>\n<pre><code>class KerasWrapper(mlflow.pyfunc.PythonModel):\n  \n  def __init__(self, keras_model, labelEncoder, labelDecoder, n):         \n    self.keras_model = keras_model\n    self.labelEncoder = labelEncoder\n    self.labelDecoder = labelDecoder\n    self.topn = n\n    \n  def load_context(self, context): \n    self.keras_model = mlflow.keras.load_model(model_uri=context.artifacts[self.keras_model], compile=False)\n  \n  def predict(self, context, input_data):\n    scores = self.keras_model.predict(input_data)\n    return scores\n<\/code><\/pre>\n<p><strong>My Keras Deep Learning Model<\/strong> (this works fine by the way)<\/p>\n<pre><code>def build_model(vocab_size, steps, drop_embed, n_dim, encoder, modelType):\n  \n  model = None\n\n  i = Input(shape=(None,), dtype=&quot;int64&quot;)\n  \n  #embedding layer\n  e = Embedding(vocab_size, 16)(i)\n  s = SpatialDropout1D(drop_embed)(e)\n\n  x = Conv1D(256, steps, activation='relu')(s)\n  x = GlobalMaxPooling1D()(x)\n  x = Dense(128, activation='relu')(x)\n  x = Dropout(0.2)(x)\n\n  #output layer\n  x = Dense(vocab_size, activation='softmax')(x)\n\n  model = Model(i, x)\n  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n  model.summary()\n  \n  return model\n<\/code><\/pre>\n<p><strong>MLFLOW Section<\/strong><\/p>\n<pre><code>with mlflow.start_run(run_name=runName):\n\n  mlflow.tensorflow.autolog()   \n\n  #Build the model, compile and train on the training set\n  #signature: build_model(vocab_size, steps, drop_embed, n_dim, encoder, modelType):\n  keras_model = build_model((vocab_size + 1), timeSteps, drop_embed, embedding_dimensions, encoder, modelType)      \n\n\n  keras_model.fit(X_train_encoded, y_train_encoded, epochs=epochs, verbose=1, batch_size=32, use_multiprocessing = True, \n                            validation_data=(X_test_encoded, y_test_encoded))\n\n  # Log the model parameters used for this run.  \n  mlflow.log_param(&quot;numofActionsinWorkflow&quot;, numofActionsinWf)\n  mlflow.log_param(&quot;timeSteps&quot;, timeSteps)\n\n  #wrap it up in a pyfunc model\n  wrappedModel = KerasWrapper(keras_model, encoder, decoder, bestActionCount)\n\n  # Create a model signature using the tensor input to store in the MLflow model registry\n  signature = infer_signature(X_test_encoded, wrappedModel.predict(None, X_test_encoded))\n  # Let's check out how it looks\n  print(signature)\n\n  # Create an input example to store in the MLflow model registry\n  input_example = np.expand_dims(X_train[17], axis=0)\n  \n  # The necessary dependencies are added to a conda.yaml file which is logged along with the model.\n  model_env = mlflow.pyfunc.get_default_conda_env()\n  # Record specific additional dependencies required by the serving model\n  model_env['dependencies'][-1]['pip'] += [\n    f'tensorflow=={tf.__version__}',\n    f'mlflow=={mlflow.__version__}',\n    f'sklearn=={sklearn.__version__}',\n    f'cloudpickle=={cloudpickle.__version__}',\n  ]\n  \n  #log the model to experiment\n  #mlflow.keras.log_model(keras_model, artifact_path = runName, signature=signature, input_example=input_example, conda_env = model_env)\n  \n  wrapped_model_path = runName\n  \n  if (os.path.exists(wrapped_model_path)):\n    shutil.rmtree(wrapped_model_path)\n  \n  #Log model as pyfunc model\n  mlflow.pyfunc.log_model(runName, python_model=wrappedModel, signature=signature, input_example=input_example, conda_env = model_env)\n\n  #return the run ID for model registration\n  run_id = mlflow.active_run().info.run_id\n  \n  mlflow.end_run()\n<\/code><\/pre>\n<p>Here is the error that i receive\n<a href=\"https:\/\/i.stack.imgur.com\/hegaZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hegaZ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/kZ2oy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kZ2oy.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_creation_time":1639686171803,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":595.0,
        "Owner_creation_time":1427492676943,
        "Owner_last_access_time":1663998407848,
        "Owner_reputation":61.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":18.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70385171",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: on databricks - cannot log a keras model as a .pyfunc model. get typeerror: cannot pickle 'weakref' object; Content: hi all: this is one of my first posts on stackoverflow - so apologies in advance if i'm not conforming to certain standards! i'm having trouble saving my keras model as a .pyfunc model as it's giving me a \"cannot pickle a 'weakref' object when i try to log it. so why am i saving my keras model as a pyfunc model object in the first place? this is because i want to override the default predict method and output something custom. i also want to do some pre-processing steps on the x_test or new data by encoding it with a tf.keras.stringlookup and then invert it back to get the original categorical variable class. for this reason, i was advised by databricks that the .pyfunc flavor is the best way to go for these types of use-cases the keras model works just fine and i'm able to log it using .keras.log_model. but it fails when i try to wrap it inside a cutomer \"keraswrapper\" class. here are some snippets of my code. for the purpose of debugging, the current predict method in the custom class is just the default. i simplified it to help debug, but obviously i haven't been able to resolve it. i would be extremely grateful for any help. thanks in advance! all code on azure databricks custom .pyfunc class class keraswrapper(.pyfunc.pythonmodel): def __init__(self, keras_model, labelencoder, labeldecoder, n): self.keras_model = keras_model self.labelencoder = labelencoder self.labeldecoder = labeldecoder self.topn = n def load_context(self, context): self.keras_model = .keras.load_model(model_uri=context.artifacts[self.keras_model], compile=false) def predict(self, context, input_data): scores = self.keras_model.predict(input_data) return scores my keras deep learning model (this works fine by the way) def build_model(vocab_size, steps, drop_embed, n_dim, encoder, modeltype): model = none i = input(shape=(none,), dtype=\"int64\") #embedding layer e = embedding(vocab_size, 16)(i) s = spatialdropout1d(drop_embed)(e) x = conv1d(256, steps, activation='relu')(s) x = globalmaxpooling1d()(x) x = dense(128, activation='relu')(x) x = dropout(0.2)(x) #output layer x = dense(vocab_size, activation='softmax')(x) model = model(i, x) model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.summary() return model section with .start_run(run_name=runname): .tensorflow.autolog() #build the model, compile and train on the training set #signature: build_model(vocab_size, steps, drop_embed, n_dim, encoder, modeltype): keras_model = build_model((vocab_size + 1), timesteps, drop_embed, embedding_dimensions, encoder, modeltype) keras_model.fit(x_train_encoded, y_train_encoded, epochs=epochs, verbose=1, batch_size=32, use_multiprocessing = true, validation_data=(x_test_encoded, y_test_encoded)) # log the model parameters used for this run. .log_param(\"numofactionsinworkflow\", numofactionsinwf) .log_param(\"timesteps\", timesteps) #wrap it up in a pyfunc model wrappedmodel = keraswrapper(keras_model, encoder, decoder, bestactioncount) # create a model signature using the tensor input to store in the model registry signature = infer_signature(x_test_encoded, wrappedmodel.predict(none, x_test_encoded)) # let's check out how it looks print(signature) # create an input example to store in the model registry input_example = np.expand_dims(x_train[17], axis=0) # the necessary dependencies are added to a conda.yaml file which is logged along with the model. model_env = .pyfunc.get_default_conda_env() # record specific additional dependencies required by the serving model model_env['dependencies'][-1]['pip'] += [ f'tensorflow=={tf.__version__}', f'=={.__version__}', f'sklearn=={sklearn.__version__}', f'cloudpickle=={cloudpickle.__version__}', ] #log the model to experiment #.keras.log_model(keras_model, artifact_path = runname, signature=signature, input_example=input_example, conda_env = model_env) wrapped_model_path = runname if (os.path.exists(wrapped_model_path)): shutil.rmtree(wrapped_model_path) #log model as pyfunc model .pyfunc.log_model(runname, python_model=wrappedmodel, signature=signature, input_example=input_example, conda_env = model_env) #return the run id for model registration run_id = .active_run().info.run_id .end_run() here is the error that i receive",
        "Question_original_content_gpt_summary":"The user is encountering a challenge when attempting to log a Keras model as a .pyfunc model on Databricks, receiving a \"cannot pickle a 'weakref' object\" error.",
        "Question_preprocessed_content":"Title: on databricks cannot log a keras model as a model. get typeerror cannot pickle 'weakref' object; Content: hi all this is one of my first posts on stackoverflow so apologies in advance if i'm not conforming to certain standards! i'm having trouble saving my keras model as a model as it's giving me a cannot pickle a 'weakref' object when i try to log it. so why am i saving my keras model as a pyfunc model object in the first place? this is because i want to override the default predict method and output something custom. i also want to do some pre processing steps on the or new data by encoding it with a and then invert it back to get the original categorical variable class. for this reason, i was advised by databricks that the flavor is the best way to go for these types of use cases the keras model works just fine and i'm able to log it using . but it fails when i try to wrap it inside a cutomer keraswrapper class. here are some snippets of my code. for the purpose of debugging, the current method in the custom class is just the default. i simplified it to help debug, but obviously i haven't been able to resolve it. i would be extremely grateful for any help. thanks in advance! all code on azure databricks custom class my keras deep learning model section here is the error that i receive",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":52705769.0,
        "Question_title":"Azure ML Tune Model Hyper Parameters",
        "Question_body":"<p>Here's question proposed at the end of the chapter in 70-774 exam reference book. <\/p>\n\n<blockquote>\n  <p>If you connect a neural network with a Tune Model Hyperparameters module configured\n  with Random Sweep and Maximum number of runs on random sweep = 1, how\n  many neural networks are trained during the execution of the experiment? Why? If you\n  connect a validation dataset to the third input of the Tune Model Hyperparameters\n  module, how many neural networks are trained now?<\/p>\n<\/blockquote>\n\n<p>And the answer is :<\/p>\n\n<blockquote>\n  <p>Without validation dataset 11 (10 of k-fold cross validation + 1 trained with all the data\n  with the best combination of hyperparameters). With the validation set only 1 neural\n  network is trained, so the best model is not trained using the validation set if you provide\n  it.<\/p>\n<\/blockquote>\n\n<p>Where does 10 come from? As far as I understand the number should be 2 and 1 respectively. Shouldn't it create n-folds where n is equal to the number of runs?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1539013176610,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":329.0,
        "Owner_creation_time":1528790837107,
        "Owner_last_access_time":1660146049396,
        "Owner_reputation":610.0,
        "Owner_up_votes":143.0,
        "Owner_down_votes":0.0,
        "Owner_views":203.0,
        "Answer_body":"<p>When you use the Tune Model Hyperparameters module without a validation dataset, this means, when you use only the 2nd input data port, the module works in cross-validation mode. So the best-parameters model is found by doing cross-validation over the provided dataset, and to do this, the dataset is splitted in k-folds. By default, the module splits the data in 10 folds. In case you want to split the data in a different number of folds, you can connect a Partition and Sample module at the 2nd input, selecting Assign to Folds and indicating the number of folds desired. In many cases k=5 is a reasonable option.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1539202336323,
        "Answer_score":3.0,
        "Owner_location":"Paris, France",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52705769",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tune model hyper parameters; Content: here's question proposed at the end of the chapter in 70-774 exam reference book. if you connect a neural network with a tune model hyperparameters module configured with random sweep and maximum number of runs on random sweep = 1, how many neural networks are trained during the execution of the experiment? why? if you connect a validation dataset to the third input of the tune model hyperparameters module, how many neural networks are trained now? and the answer is : without validation dataset 11 (10 of k-fold cross validation + 1 trained with all the data with the best combination of hyperparameters). with the validation set only 1 neural network is trained, so the best model is not trained using the validation set if you provide it. where does 10 come from? as far as i understand the number should be 2 and 1 respectively. shouldn't it create n-folds where n is equal to the number of runs?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in understanding why the number of neural networks trained during the execution of an experiment with a tune model hyperparameters module configured with random sweep and maximum number of runs on random sweep = 1 is 11 without a validation dataset and 1 with a validation dataset.",
        "Question_preprocessed_content":"Title: tune model hyper parameters; Content: here's question proposed at the end of the chapter in exam reference book. if you connect a neural network with a tune model hyperparameters module configured with random sweep and maximum number of runs on random sweep , how many neural networks are trained during the execution of the experiment? why? if you connect a validation dataset to the third input of the tune model hyperparameters module, how many neural networks are trained now? and the answer is without validation dataset . with the validation set only neural network is trained, so the best model is not trained using the validation set if you provide it. where does come from? as far as i understand the number should be and respectively. shouldn't it create n folds where n is equal to the number of runs?",
        "Answer_original_content":"when you use the tune model hyperparameters module without a validation dataset, this means, when you use only the 2nd input data port, the module works in cross-validation mode. so the best-parameters model is found by doing cross-validation over the provided dataset, and to do this, the dataset is splitted in k-folds. by default, the module splits the data in 10 folds. in case you want to split the data in a different number of folds, you can connect a partition and sample module at the 2nd input, selecting assign to folds and indicating the number of folds desired. in many cases k=5 is a reasonable option.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"when you use the tune model hyperparameters module without a validation dataset, this means, when you use only the nd input data port, the module works in cross validation mode. so the best parameters model is found by doing cross validation over the provided dataset, and to do this, the dataset is splitted in k folds. by default, the module splits the data in folds. in case you want to split the data in a different number of folds, you can connect a partition and sample module at the nd input, selecting assign to folds and indicating the number of folds desired. in many cases k is a reasonable option."
    },
    {
        "Question_id":null,
        "Question_title":"Scan_history() is empty",
        "Question_body":"<p>Hi, when I run <code>run.history()<\/code>, I get a sampled version of the history as expected (although the number of samples fluctuates). But when I run <code>run.scan_history()<\/code>, I get an empty object (i.e. 0 rows).<\/p>\n<p>Any idea why this is happening or how it could be fixed?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1675419840337,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":80.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/scan-history-is-empty\/3811",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-03T16:11:05.149Z",
                "Answer_body":"<p>Hi Christian,<br>\nHappy Friday!<\/p>\n<p>Could you send me some code I can try reproducing this with?<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-07T11:08:30.162Z",
                "Answer_body":"<p>Hi Artsiom, thanks for your reply!<\/p>\n<p>Sure, here is an API call to my project:<\/p>\n<pre><code class=\"lang-auto\">import wandb\napi = wandb.Api()\nruns = api.runs('chs20\/scratch-public')\nfor run in runs:\n    history = run.history()\n    scan_history = run.scan_history()\n    print(f'run: {run.name}')\n    print(f'history columns: {len(history.columns)}')\n    print(f'scan_history rows: {len(scan_history.rows)}')\n<\/code><\/pre>\n<p>This outputs for me:<\/p>\n<pre><code class=\"lang-auto\">run: 2023-02-07_10:48:43\nhistory columns: 13\nscan_history rows: 0\n<\/code><\/pre>\n<p>I would have expected the number of <code>history<\/code> columns  to match the number of <code>scan_history<\/code> rows.  In particular I would not expect <code>scan_history<\/code> to have zero rows.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T09:04:57.998Z",
                "Answer_body":"<p>Any ideas what is causing this?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T21:36:11.943Z",
                "Answer_body":"<p>Sorry for the delay in response! So far I haven\u2019t been able to reproduce this on my side so trying a few things <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-20T08:56:15.814Z",
                "Answer_body":"<p>Does the code snippet yield a different output for you?<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"",
        "Answer_original_content":"",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":""
    },
    {
        "Question_id":null,
        "Question_title":"Tensorflow and Azure machine learning",
        "Question_body":"Is azure working well with Tensorflow framework? I don\u2019t see any document about it. Any help is good.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1653989511207,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/871068\/tensorflow-and-azure-machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-31T10:26:09.017Z",
                "Answer_score":1,
                "Answer_body":"Hello @Chungsun-1776\n\nWelcome to the Microsoft Q&A Platform,\n\nTensorFlow is supported on Azure Machine Learning:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-tensorflow\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-machine-learning-tensorflow?tabs=bash\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-keras\n\nI hope this helps!\n\nPlease don\u2019t forget to \"Accept the answer\" and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tensorflow and ; Content: is azure working well with tensorflow framework? i don\u2019t see any document about it. any help is good.",
        "Question_original_content_gpt_summary":"The user is seeking assistance in determining if Azure is compatible with the TensorFlow framework.",
        "Question_preprocessed_content":"Title: tensorflow and; Content: is azure working well with tensorflow framework? i dont see any document about it. any help is good.",
        "Answer_original_content":"hello @chungsun-1776 welcome to the microsoft q&a platform, tensorflow is supported on : https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-tensorflow https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-machine-learning-tensorflow?tabs=bash https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-keras i hope this helps! please dont forget to \"accept the answer\" and up-vote wherever the information provided helps you, this can be beneficial to other community members.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello welcome to the microsoft q&a platform, tensorflow is supported on i hope this helps! please dont forget to accept the answer and up vote wherever the provided helps you, this can be beneficial to other community members."
    },
    {
        "Question_id":70619317.0,
        "Question_title":"How to add retry in mlflow",
        "Question_body":"<p>In order to track machine learning modules I am using mlflow.\nI am starting a mlflow server with command line script <code>my backend store uri is postgresql<\/code><\/p>\n<pre><code>something like \nmlflow server --backend-store-uri postgresql --default-artifact-roots 3:\/\/my-mlflow-bucket\/ --host 0.0.0.0 \n<\/code><\/pre>\n<p>Many a time a postgresql gets down how do to ensure retry a connectivity to  postgresql to n limit only if its not up.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1641548537390,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Owner_creation_time":1625731242200,
        "Owner_last_access_time":1659594320980,
        "Owner_reputation":145.0,
        "Owner_up_votes":8.0,
        "Owner_down_votes":0.0,
        "Owner_views":22.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1641556269630,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70619317",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to add retry in ; Content: in order to track machine learning modules i am using . i am starting a server with command line script my backend store uri is postgresql something like server --backend-store-uri postgresql --default-artifact-roots 3:\/\/my--bucket\/ --host 0.0.0.0 many a time a postgresql gets down how do to ensure retry a connectivity to postgresql to n limit only if its not up.",
        "Question_original_content_gpt_summary":"The user is looking for a way to ensure retry of a connection to PostgreSQL up to a certain limit if it is not available.",
        "Question_preprocessed_content":"Title: how to add retry in; Content: in order to track machine learning modules i am using . i am starting a server with command line script many a time a postgresql gets down how do to ensure retry a connectivity to postgresql to n limit only if its not up.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Hp-sweep with function with arguments",
        "Question_body":"<p>Hi<\/p>\n<p>Im wondering if its possible to have hp-sweep and instantiating wandb agent (using wandb.agent api) , such that the function will have arguments, in all of the example I\u2019ve seen that the passed functions to wandb.agent has no args at all.<\/p>\n<p>My training script looks as follows<\/p>\n<pre><code class=\"lang-auto\">\ndef trainer(input1, input2, input3):\n  # construct cfg file given the inputs\n  # passing the cfg file to wandb.init() \n  # receiving the returned config from wandb.config (with the decided params by the wandb Params controler)\n  # Continue training as usual, given the latest cfg.\n<\/code><\/pre>\n<p>I will be happy to get some inspiration about other ways to implement this<br>\nHope this is helpful<\/p>\n<p>Dor<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1670934400394,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":182.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hp-sweep-with-function-with-arguments\/3539",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-14T12:46:58.632Z",
                "Answer_body":"<p>Hi Dor,<\/p>\n<p>Thanks for writing in! You can pass arguments to the function in the agent in the normal way <code>function=main(args)<\/code>, is this raising you any error?<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-19T15:21:42.859Z",
                "Answer_body":"<p>Hi Dor,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-11T12:26:56.570Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"",
        "Answer_original_content":"",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":""
    },
    {
        "Question_id":71611203.0,
        "Question_title":"Azure function in python 3.8 in Linux failing when invoking Azure ML",
        "Question_body":"<p>I am working on an Azure Function which is written in Python 3.8. Azure function in Azure in Python is deployed in Linux. The Azure function is invoking Azure ML pipeline. When invoking the pipeline I am seeing the following exception:<\/p>\n<p>Exception: NotImplementedError: Linux distribution debian 11. does not have automatic support.\n.NET Core 2.1 can still be used via dotnetcore2 if the required dependencies are installed.\nVisit <a href=\"https:\/\/aka.ms\/dotnet-install-linux\" rel=\"nofollow noreferrer\">https:\/\/aka.ms\/dotnet-install-linux<\/a> for Linux distro specific .NET Core install instructions.\nFollow your distro specific instructions to install <code>dotnet-runtime-*<\/code> and replace <code>*<\/code> with <code>2.1<\/code><\/p>\n<p>I have tried installing dotnetcore2 runtime via requirements.txt file but still getting the error. Any help is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1648171692863,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":107.0,
        "Owner_creation_time":1455754466032,
        "Owner_last_access_time":1648249335728,
        "Owner_reputation":9.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71611203",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure function in python 3.8 in linux failing when invoking ; Content: i am working on an azure function which is written in python 3.8. azure function in azure in python is deployed in linux. the azure function is invoking pipeline. when invoking the pipeline i am seeing the following exception: exception: notimplementederror: linux distribution debian 11. does not have automatic support. .net core 2.1 can still be used via dotnetcore2 if the required dependencies are installed. visit https:\/\/aka.ms\/dotnet-install-linux for linux distro specific .net core install instructions. follow your distro specific instructions to install dotnet-runtime-* and replace * with 2.1 i have tried installing dotnetcore2 runtime via requirements.txt file but still getting the error. any help is appreciated.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with an Azure Function written in Python 3.8, deployed in Linux, when attempting to invoke a pipeline, receiving an error related to .NET Core 2.1.",
        "Question_preprocessed_content":"Title: azure function in python in linux failing when invoking; Content: i am working on an azure function which is written in python azure function in azure in python is deployed in linux. the azure function is invoking pipeline. when invoking the pipeline i am seeing the following exception exception notimplementederror linux distribution debian . does not have automatic support. .net core can still be used via dotnetcore if the required dependencies are installed. visit for linux distro specific .net core install instructions. follow your distro specific instructions to install and replace with i have tried installing dotnetcore runtime via file but still getting the error. any help is appreciated.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70582506.0,
        "Question_title":"Read Excel file in Amazon Sage maker using R Notebook",
        "Question_body":"<p>I am having S3 bucket named &quot;Temp-Bucket&quot;. Inside that I am having folder named &quot;folder&quot;.\nI want to read file named file1.xlsx. This file is present inside the S3 bucket(Temp-Bucket) under the folder (folder). How to read that file ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1641315920903,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":121.0,
        "Owner_creation_time":1587724461987,
        "Owner_last_access_time":1660372257292,
        "Owner_reputation":95.0,
        "Owner_up_votes":39.0,
        "Owner_down_votes":0.0,
        "Owner_views":30.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70582506",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: read excel file in amazon sage maker using r notebook; Content: i am having s3 bucket named \"temp-bucket\". inside that i am having folder named \"folder\". i want to read file named file1.xlsx. this file is present inside the s3 bucket(temp-bucket) under the folder (folder). how to read that file ?",
        "Question_original_content_gpt_summary":"The user is trying to read an Excel file located in an S3 bucket using an R notebook in Amazon Sage Maker.",
        "Question_preprocessed_content":"Title: read excel file in amazon sage maker using r notebook; Content: i am having s bucket named temp bucket . inside that i am having folder named folder . i want to read file named this file is present inside the s bucket under the folder . how to read that file ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70754016.0,
        "Question_title":"Creating a Vertex AI Workbench with a Non Organization Account and problems with constraints\/compute.vmExternalIpAccess",
        "Question_body":"<p>I'm trying to create a Vertex AI Workbench on GCP, but every time I try I get the following error:<\/p>\n<p><em>&lt;Workbench Name&gt; Constraint constraints\/compute.vmExternalIpAccess violated for project &lt;Project ID&gt;. Add instance &lt;Workbench ID&gt; to the constraint to use external IP with it.<\/em><\/p>\n<p>I went to the Organization Policies page to edit the constraint: <em>constraints\/compute.vmExternalIpAccess<\/em> and saw that is denied for all (which is odd because in the <a href=\"https:\/\/cloud.google.com\/resource-manager\/docs\/organization-policy\/org-policy-constraints\" rel=\"nofollow noreferrer\">constraints documentation<\/a> it says that is should be enabled for all by default). Now, the problem is that when I go to edit the constraint, it says that it requires this set of permissions:<\/p>\n<ul>\n<li><em>orgpolicy.policies.create<\/em><\/li>\n<li><em>orgpolicy.policies.delete<\/em><\/li>\n<li><em>orgpolicy.policies.update<\/em><\/li>\n<li><em>orgpolicy.policy.get<\/em><\/li>\n<\/ul>\n<p>which are all part of the role: <em>roles\/orgpolicy.policyAdmin<\/em> that can only be granted at an organization level, and well, I have a Non Organization Account.<\/p>\n<p>Am I missing something?<\/p>\n<p>Thank for your time!<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4.0,
        "Question_creation_time":1642501704547,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":333.0,
        "Owner_creation_time":1353379904267,
        "Owner_last_access_time":1653505313252,
        "Owner_reputation":215.0,
        "Owner_up_votes":15.0,
        "Owner_down_votes":0.0,
        "Owner_views":30.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70754016",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: creating a workbench with a non organization account and problems with constraints\/compute.vmexternalipaccess; Content: i'm trying to create a workbench on gcp, but every time i try i get the following error: <workbench name> constraint constraints\/compute.vmexternalipaccess violated for project <project id>. add instance <workbench id> to the constraint to use external ip with it. i went to the organization policies page to edit the constraint: constraints\/compute.vmexternalipaccess and saw that is denied for all (which is odd because in the constraints documentation it says that is should be enabled for all by default). now, the problem is that when i go to edit the constraint, it says that it requires this set of permissions: orgpolicy.policies.create orgpolicy.policies.delete orgpolicy.policies.update orgpolicy.policy.get which are all part of the role: roles\/orgpolicy.policyadmin that can only be granted at an organization level, and well, i have a non organization account. am i missing something? thank for your time!",
        "Question_original_content_gpt_summary":"The user is encountering challenges creating a workbench on GCP due to a constraint violation and the inability to edit the constraint without an organization account.",
        "Question_preprocessed_content":"Title: creating a workbench with a non organization account and problems with; Content: i'm trying to create a workbench on gcp, but every time i try i get the following error constraint violated for project . add instance to the constraint to use external ip with it. i went to the organization policies page to edit the constraint and saw that is denied for all . now, the problem is that when i go to edit the constraint, it says that it requires this set of permissions which are all part of the role that can only be granted at an organization level, and well, i have a non organization account. am i missing something? thank for your time!",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Permission denied error when overwritting files to DVC tracked directory",
        "Question_body":"<p>Hi there,<br>\nI\u2019m still figuring DVC out so sorry if this is an obvious question. I feel like I\u2019m missing something.<\/p>\n<p>I have a project where some directories are tracked with DVC.<br>\nWhen someone reruns a script that overwrites existing files inside a DVC tracked directory we get \u201cPermission denied error\u201d. If we try to manually modify the same file with the vim editor we get that the file is flagged as readonly but we can force writting the file and it gets modified. But when we check file permissions on the file they all seem to be set properly. If we delete all files and rerun the script regenerating the files everything works fine again.<\/p>\n<p>Is this how DVC is supposed to work? Shouldn\u2019t we be able to overwrite files and then add the changes to the DVC tracked directory and then push them?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1676391679082,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":21.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/permission-denied-error-when-overwritting-files-to-dvc-tracked-directory\/1518",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-14T16:45:36.508Z",
                "Answer_body":"<p>What platform are you running on (windows\/linux\/mac) and what dvc command in the script is giving you the permission error? It would also help if you could run <code>dvc doctor<\/code> in the repo where you get the errors and post the output here.<\/p>\n<p>One possibility I can think of is that if you are using the <code>hardlink<\/code> or <code>symlink<\/code> cache link type setting, you need to be using <code>dvc unprotect<\/code> before you try and edit or write to the dvc-tracked files: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/unprotect\" class=\"inline-onebox\">unprotect<\/a><\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-16T15:35:02.161Z",
                "Answer_body":"<p>Many thanks <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a>. I am indeed using symlinks. I\u2019ll check that. Thanks!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"",
        "Answer_original_content":"",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":""
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Workspace Windows Server Compute Instance",
        "Question_body":"Hi,\n\nWould like to know is Windows Server Compute Instance available in Azure ML Workspace?\n\nI do understand there is DSVM for windows, but it won't have direct link to datasources and the jupyter notebook, or it is possible for me to link the compute once I have created the VM?\n\nI'm having a problem to migrate a conda environment from Windows to Linux, may I know what is the best practice for such migration?\n\nThanks!",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1626842114250,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/483713\/azure-ml-workspace-windows-server-compute-instance.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-04T23:20:17.447Z",
                "Answer_score":0,
                "Answer_body":"@SoonJooGenting-3682 Hello, I have reached out to DSVM, actually they support Azure service and JupyterNotebook.\n\nAs the document said:\n\n> It also allows you to access services on the Azure cloud platform. Azure provides several compute, storage, data analytics, and other services that you can administer and access from your DSVM.\n\n\nTo administer your Azure subscription and cloud resources, you have two options:\n\n\nUse your browser and go to the Azure portal.\n\n\nUse PowerShell scripts. Run Azure PowerShell from a shortcut on the desktop or from the Start menu. See the Microsoft Azure PowerShell documentation for full details.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/vm-do-ten-things#manage-azure-resources\n\nTo start the Jupyter Notebook, select the Jupyter Notebook icon on the Start menu or on the desktop. In the DSVM command prompt, you can also run the command jupyter notebook from the directory where you have existing notebooks or where you want to create new notebooks.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/vm-do-ten-things#use-jupyter-notebooks\n\nHope this helps.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: workspace windows server compute instance; Content: hi, would like to know is windows server compute instance available in workspace? i do understand there is dsvm for windows, but it won't have direct link to datasources and the jupyter notebook, or it is possible for me to link the compute once i have created the vm? i'm having a problem to migrate a conda environment from windows to linux, may i know what is the best practice for such migration? thanks!",
        "Question_original_content_gpt_summary":"The user is looking to understand if Windows Server Compute Instance is available in Workspace, and is having difficulty migrating a Conda environment from Windows to Linux.",
        "Question_preprocessed_content":"Title: workspace windows server compute instance; Content: hi, would like to know is windows server compute instance available in workspace? i do understand there is dsvm for windows, but it won't have direct link to datasources and the jupyter notebook, or it is possible for me to link the compute once i have created the vm? i'm having a problem to migrate a conda environment from windows to linux, may i know what is the best practice for such migration? thanks!",
        "Answer_original_content":"@soonjoogenting-3682 hello, i have reached out to dsvm, actually they support azure service and jupyternotebook. as the document said: > it also allows you to access services on the azure cloud platform. azure provides several compute, storage, data analytics, and other services that you can administer and access from your dsvm. to administer your azure subscription and cloud resources, you have two options: use your browser and go to the azure portal. use powershell scripts. run azure powershell from a shortcut on the desktop or from the start menu. see the microsoft azure powershell documentation for full details. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/vm-do-ten-things#manage-azure-resources to start the jupyter notebook, select the jupyter notebook icon on the start menu or on the desktop. in the dsvm command prompt, you can also run the command jupyter notebook from the directory where you have existing notebooks or where you want to create new notebooks. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/vm-do-ten-things#use-jupyter-notebooks hope this helps. regards, yutong",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello, i have reached out to dsvm, actually they support azure service and jupyternotebook. as the document said > it also allows you to access services on the azure cloud platform. azure provides several compute, storage, data analytics, and other services that you can administer and access from your dsvm. to administer your azure subscription and cloud resources, you have two options use your browser and go to the azure portal. use powershell scripts. run azure powershell from a shortcut on the desktop or from the start menu. see the microsoft azure powershell documentation for full details. to start the jupyter notebook, select the jupyter notebook icon on the start menu or on the desktop. in the dsvm command prompt, you can also run the command jupyter notebook from the directory where you have existing notebooks or where you want to create new notebooks. hope this helps. regards, yutong"
    },
    {
        "Question_id":52619920.0,
        "Question_title":"AWS Sagemaker: Can the existing train_image demo be modified?",
        "Question_body":"<p>I am trying a demo \"DeepAR-Electricity.ipynb\" on AWS Sagemaker.<\/p>\n\n<p>But after running \"estimator.fit\" for training, I can only get the log of avg_epoch_loss in every epoch, printed by some functions that I could not find. How can I plot the training and validation loss curves? <\/p>\n\n<p>Can the existing train_image demo be modified? If no, how to plot the training and validation loss curves? If yes, how to access the source training function?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cegii.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cegii.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1538542459573,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":156.0,
        "Owner_creation_time":1405944892916,
        "Owner_last_access_time":1648802229136,
        "Owner_reputation":45.0,
        "Owner_up_votes":10.0,
        "Owner_down_votes":0.0,
        "Owner_views":14.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52619920",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: : can the existing train_image demo be modified?; Content: i am trying a demo \"deepar-electricity.ipynb\" on . but after running \"estimator.fit\" for training, i can only get the log of avg_epoch_loss in every epoch, printed by some functions that i could not find. how can i plot the training and validation loss curves? can the existing train_image demo be modified? if no, how to plot the training and validation loss curves? if yes, how to access the source training function?",
        "Question_original_content_gpt_summary":"The user is trying to modify the existing train_image demo to plot the training and validation loss curves, but is having difficulty accessing the source training function.",
        "Question_preprocessed_content":"Title: can the existing demo be modified?; Content: i am trying a demo on . but after running for training, i can only get the log of in every epoch, printed by some functions that i could not find. how can i plot the training and validation loss curves? can the existing demo be modified? if no, how to plot the training and validation loss curves? if yes, how to access the source training function?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Dialogflow should have its own official facebook app for integration",
        "Question_body":"Current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. While most of the page's owner are not developer and just want to link some of their page to dialogflowI want to propose that dialogflow should have facebook app with `manage_pages` permission. Have button for oauth with facebook for integration. And just allow user to choose some of their pages to link with dialogflow project. Then all the process in the guideline can be automated. Dialogflow could also config the settings for Webhooks channels it needI want to comment that this was a very roadblock that I have faced when I try to start integrate facebook. The message was not get to dialogflow properly and I don't know I also need `messaging_postbacks` channel, not only `messages`. If Dialogflow app will manage these for us it will be the far much better integration experienceps. Please also add label `Dialogflow` and `Dialogflow ES` to the available label",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1665696420000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-should-have-its-own-official-facebook-app-for\/td-p\/477998\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-13T21:27:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. While most of the page's owner are not developer and just want to link some of their page to dialogflow\n\nI want to propose that dialogflow should have facebook app with `manage_pages` permission. Have button for oauth with facebook for integration. And just allow user to choose some of their pages to link with dialogflow project. Then all the process in the guideline can be automated. Dialogflow could also config the\u00a0settings for\u00a0Webhooks channels it need\n\nI want to comment that this was a very roadblock that I have faced when I try to start integrate facebook. The message was not get to dialogflow properly and I don't know I also need `messaging_postbacks` channel, not only `messages`. If Dialogflow app will manage these for us it will be the far much better integration experience\n\nps. Please also add label `Dialogflow` and `Dialogflow ES` to the available label"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: dialogflow should have its own official facebook app for integration; Content: current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. while most of the page's owner are not developer and just want to link some of their page to dialogflowi want to propose that dialogflow should have facebook app with `manage_pages` permission. have button for oauth with facebook for integration. and just allow user to choose some of their pages to link with dialogflow project. then all the process in the guideline can be automated. dialogflow could also config the settings for webhooks channels it needi want to comment that this was a very roadblock that i have faced when i try to start integrate facebook. the message was not get to dialogflow properly and i don't know i also need `messaging_postbacks` channel, not only `messages`. if dialogflow app will manage these for us it will be the far much better integration experienceps. please also add label `dialogflow` and `dialogflow es` to the available label",
        "Question_original_content_gpt_summary":"The user encountered a roadblock when attempting to integrate Facebook with Dialogflow, as they had to become a Facebook developer and create their own app in order to do so.",
        "Question_preprocessed_content":"Title: dialogflow should have its own official facebook app for integration; Content: current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. while most of the page's owner are not developer and just want to link some of their page to dialogflowi want to propose that dialogflow should have facebook app with permission. have button for oauth with facebook for integration. and just allow user to choose some of their pages to link with dialogflow project. then all the process in the guideline can be automated. dialogflow could also config the settings for webhooks channels it needi want to comment that this was a very roadblock that i have faced when i try to start integrate facebook. the message was not get to dialogflow properly and i don't know i also need channel, not only . if dialogflow app will manage these for us it will be the far much better integration experienceps. please also add label and to the available label",
        "Answer_original_content":"current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. while most of the page's owner are not developer and just want to link some of their page to dialogflow i want to propose that dialogflow should have facebook app with `manage_pages` permission. have button for oauth with facebook for integration. and just allow user to choose some of their pages to link with dialogflow project. then all the process in the guideline can be automated. dialogflow could also config thesettings forwebhooks channels it need i want to comment that this was a very roadblock that i have faced when i try to start integrate facebook. the message was not get to dialogflow properly and i don't know i also need `messaging_postbacks` channel, not only `messages`. if dialogflow app will manage these for us it will be the far much better integration experience ps. please also add label `dialogflow` and `dialogflow es` to the available label",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"current dialogflow integration is sensible, however it was very tedious for anyone to must become facebook developer and create their own facebook app. while most of the page's owner are not developer and just want to link some of their page to dialogflow i want to propose that dialogflow should have facebook app with permission. have button for oauth with facebook for integration. and just allow user to choose some of their pages to link with dialogflow project. then all the process in the guideline can be automated. dialogflow could also config thesettings forwebhooks channels it need i want to comment that this was a very roadblock that i have faced when i try to start integrate facebook. the message was not get to dialogflow properly and i don't know i also need channel, not only . if dialogflow app will manage these for us it will be the far much better integration experience ps. please also add label and to the available label"
    },
    {
        "Question_id":72741757.0,
        "Question_title":"Vertex AI Custom Container Training Job python SDK - google.api_core.exceptions.FailedPrecondition: 400 '",
        "Question_body":"<p>I have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when I create the training job on the Vertex AI website interface.<\/p>\n<p>But now I'm trying to create the training job from a python script using<\/p>\n<pre><code>class google.cloud.aiplatform.CustomContainerTrainingJob\n<\/code><\/pre>\n<p>I load a managed dataset that I have on vertex AI with<\/p>\n<pre><code>dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\n<\/code><\/pre>\n<p>But when I try to run the following code:<\/p>\n<pre><code>model = job.run(\n        dataset=dataset,\n        model_display_name=model_display_name,\n        args=args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        training_fraction_split=training_fraction_split,\n        validation_fraction_split=validation_fraction_split,\n        test_fraction_split=test_fraction_split,\n        sync=sync,\n    )\n\n    model.wait()\n\n    print(model.display_name)\n    print(model.resource_name)\n    print(model.uri)\n    return model\n<\/code><\/pre>\n<p>I got the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset.\n<\/code><\/pre>\n<p>I feel like something is wrong because when I create the job on the website I specify an export directory for the managed dataset, but I have not found where to do it here.<\/p>\n<p>Any ideas?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1656062302777,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":170.0,
        "Owner_creation_time":1656061360900,
        "Owner_last_access_time":1659951704647,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":0.0,
        "Answer_body":"<p>Well I found the answer in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform?hl=fr#class-googlecloudaiplatformcustomcontainertrainingjobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-containeruri-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-command-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerimageuri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerpredictroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerhealthroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainercommand-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerargs-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerenvironmentvariables-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerports-optionalsequenceinthttpspythonreadthedocsioenlatestlibraryfunctionshtmlint--none-modeldescription-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelinstanceschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelparametersschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelpredictionschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-trainingencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-stagingbucket-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a>, data are automatically exported to the provided bucket thus it was not the issue. The issue was in the error ( obviously ).\nTo provide a good annotation URI, it is enough to just add a parameter to run():<\/p>\n<pre><code>annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification\n<\/code><\/pre>\n<p>image.classification was what I needed here but can be replaced by text.extraction if you do text extraction for example.<\/p>\n<p>This will pass as string value the following value which is the asked gs uri:<\/p>\n<pre><code>gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1656071944492,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1656343337227,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72741757",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: custom container training job python sdk - google.api_core.exceptions.failedprecondition: 400 '; Content: i have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when i create the training job on the website interface. but now i'm trying to create the training job from a python script using class google.cloud.aiplatform.customcontainertrainingjob i load a managed dataset that i have on with dataset = aiplatform.imagedataset(dataset_id) if dataset_id else none but when i try to run the following code: model = job.run( dataset=dataset, model_display_name=model_display_name, args=args, replica_count=replica_count, machine_type=machine_type, accelerator_type=accelerator_type, accelerator_count=accelerator_count, training_fraction_split=training_fraction_split, validation_fraction_split=validation_fraction_split, test_fraction_split=test_fraction_split, sync=sync, ) model.wait() print(model.display_name) print(model.resource_name) print(model.uri) return model i got the following error: google.api_core.exceptions.failedprecondition: 400 'annotation_schema_uri' should be set in the trainingpipeline.input_data_config for custom training or hyperparameter tuning with managed dataset. i feel like something is wrong because when i create the job on the website i specify an export directory for the managed dataset, but i have not found where to do it here. any ideas? thank you",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to create a custom container training job using the Python SDK, and is unable to find where to specify an export directory for the managed dataset.",
        "Question_preprocessed_content":"Title: custom container training job python sdk '; Content: i have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when i create the training job on the website interface. but now i'm trying to create the training job from a python script using i load a managed dataset that i have on with but when i try to run the following code i got the following error i feel like something is wrong because when i create the job on the website i specify an export directory for the managed dataset, but i have not found where to do it here. any ideas? thank you",
        "Answer_original_content":"well i found the answer in the documentation, data are automatically exported to the provided bucket thus it was not the issue. the issue was in the error ( obviously ). to provide a good annotation uri, it is enough to just add a parameter to run(): annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification image.classification was what i needed here but can be replaced by text.extraction if you do text extraction for example. this will pass as string value the following value which is the asked gs uri: gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"well i found the answer in the documentation, data are automatically exported to the provided bucket thus it was not the issue. the issue was in the error . to provide a good annotation uri, it is enough to just add a parameter to run was what i needed here but can be replaced by if you do text extraction for example. this will pass as string value the following value which is the asked gs uri"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb Tables to Latex Tables, anyway?",
        "Question_body":"<p>I discovered Wandb few years back but I finally sat down to integrate everything with my experiments. I am loving this so much! Something I\u2019d like to know is if there is anyway to export tables to Latex from Wandb itself.<\/p>\n<p>I\u2019m in ML research and I have to include my results in Latex tables for papers. So it would be really cool if I could export them from Wandb, just like reports or graphs. I\u2019m open to suggestions you may have. thank you<\/p>\n<p>(willing to submit a feature request as well)<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1634821750155,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":819.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-tables-to-latex-tables-anyway\/1060",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-25T13:02:11.187Z",
                "Answer_body":"<p>Thanks for sharing this. This is an interesting idea and would definitely be a valuable function for other W&amp;B users! Just to clarify, is this to display metrics from your runs within Latex Tables or are you looking to extract information from our model evaluation tool <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/tables\">W&amp;B Tables<\/a>?<\/p>\n<p>If you\u2019re looking for metrics from your runs, you can use the Runs API to get these, which you could then format into a Latex Table dynamically. I\u2019ve seen this being done from the team at Github to display run metrics within a markdown table in Github Issue comments:<\/p><aside class=\"onebox githubblob\" data-onebox-src=\"https:\/\/github.com\/machine-learning-apps\/wandb-action\/blob\/master\/wandb_get_runs.py\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/machine-learning-apps\/wandb-action\/blob\/master\/wandb_get_runs.py\" target=\"_blank\" rel=\"noopener\">github.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https:\/\/github.com\/machine-learning-apps\/wandb-action\/blob\/master\/wandb_get_runs.py\" target=\"_blank\" rel=\"noopener\">machine-learning-apps\/wandb-action\/blob\/master\/wandb_get_runs.py<\/a><\/h4>\n\n\n    <pre><code class=\"lang-py\">\"\"\"\nRetrieves all runs from wandb that either:\n- correspond to a Git SHA\n- have specific tags.\n\nThe purpose is to compare runs from a given SHA to runs you may have tagged as baselines.\n\"\"\"\n\n\nimport os\nos.environ[\"WANDB_API_KEY\"] = os.getenv('INPUT_WANDB_API_KEY')\nimport wandb\nimport logging\nimport pandas as pd\n\nlogging.root.setLevel(logging.DEBUG)\n\napi = wandb.Api()\n\n# Read Inputs\n<\/code><\/pre>\n\n\n  This file has been truncated. <a href=\"https:\/\/github.com\/machine-learning-apps\/wandb-action\/blob\/master\/wandb_get_runs.py\" target=\"_blank\" rel=\"noopener\">show original<\/a>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\">\n  <header class=\"source\">\n      \n\n      <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\" target=\"_blank\" rel=\"noopener\">docs.wandb.ai<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_690x362.png\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\" target=\"_blank\" rel=\"noopener\">Import &amp; Export Data<\/a><\/h3>\n\n  <p>Best practices and common use cases for our public API to export data and update existing runs<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":29.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-20T17:39:09.458Z",
                "Answer_body":"<p>So thanks to a chat in Reddit, progress has been made on this. Just putting runs into a DataFrame using the runs api and then use to_latex.<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#querying-multiple-runs\">\n  <header class=\"source\">\n      \n\n      <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#querying-multiple-runs\" target=\"_blank\" rel=\"noopener\">docs.wandb.ai<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_690x362.png\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b3ea0f16b83c6c9d59a62cd31bf26f336a797687_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide#querying-multiple-runs\" target=\"_blank\" rel=\"noopener\">Import &amp; Export Data<\/a><\/h3>\n\n  <p>Best practices and common use cases for our public API to export data and update existing runs<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/qc2apm\/d_what_is_your_ml_experiment_workflow_discussion\/hlegwga\/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf&amp;context=3\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/a70ab5d077374748901898cfcd9c7a09b8a303a9.png\" class=\"site-icon\" width=\"192\" height=\"192\">\n\n      <a href=\"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/qc2apm\/d_what_is_your_ml_experiment_workflow_discussion\/hlegwga\/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf&amp;context=3\" target=\"_blank\" rel=\"noopener\">reddit<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e1e6bc8397e7c5baf7819d4743b7e8d04145c522_2_690x362.jpeg\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e1e6bc8397e7c5baf7819d4743b7e8d04145c522_2_690x362.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e1e6bc8397e7c5baf7819d4743b7e8d04145c522_2_1035x543.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e1e6bc8397e7c5baf7819d4743b7e8d04145c522.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/e1e6bc8397e7c5baf7819d4743b7e8d04145c522_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/qc2apm\/d_what_is_your_ml_experiment_workflow_discussion\/hlegwga\/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf&amp;context=3\" target=\"_blank\" rel=\"noopener\">r\/MachineLearning - [D] What is your ML experiment workflow? Discussion on...<\/a><\/h3>\n\n  <p>1 vote and 7 comments so far on Reddit<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<pre><code class=\"lang-python\">\nimport pandas as pd \nimport wandb\n\napi = wandb.Api()\nentity, project = \"&lt;entity&gt;\", \"&lt;project&gt;\"  # set to your entity and project \nruns = api.runs(entity + \"\/\" + project) \n\nsummary_list, config_list, name_list = [], [], []\nfor run in runs: \n    # .summary contains the output keys\/values for metrics like accuracy.\n    #  We call ._json_dict to omit large files \n    summary_list.append(run.summary._json_dict)\n\n    # .config contains the hyperparameters.\n    #  We remove special values that start with _.\n    config_list.append(\n        {k: v for k,v in run.config.items()\n         if not k.startswith('_')})\n\n    # .name is the human-readable name of the run.\n    name_list.append(run.name)\n\nruns_df = pd.DataFrame({\n    \"summary\": summary_list,\n    \"config\": config_list,\n    \"name\": name_list\n    })\n\nruns_df.to_latex()\n\n<\/code><\/pre>",
                "Answer_score":1272.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:06.677Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tables to latex tables, anyway?; Content: i discovered few years back but i finally sat down to integrate everything with my experiments. i am loving this so much! something i\u2019d like to know is if there is anyway to export tables to latex from itself. i\u2019m in ml research and i have to include my results in latex tables for papers. so it would be really cool if i could export them from , just like reports or graphs. i\u2019m open to suggestions you may have. thank you (willing to submit a feature request as well)",
        "Question_original_content_gpt_summary":"The user is looking for a way to export tables from to Latex tables for use in their ML research papers.",
        "Question_preprocessed_content":"Title: tables to latex tables, anyway?; Content: i discovered few years back but i finally sat down to integrate everything with my experiments. i am loving this so much! something id like to know is if there is anyway to export tables to latex from itself. im in ml research and i have to include my results in latex tables for papers. so it would be really cool if i could export them from , just like reports or graphs. im open to suggestions you may have. thank you willing to submit a feature request as well",
        "Answer_original_content":"thanks for sharing this. this is an interesting idea and would definitely be a valuable function for other w&b users! just to clarify, is this to display metrics from your runs within latex tables or are you looking to extract information from our model evaluation tool w&b tables? if youre looking for metrics from your runs, you can use the runs api to get these, which you could then format into a latex table dynamically. ive seen this being done from the team at github to display run metrics within a markdown table in github issue comments: github.com machine-learning-apps\/-action\/blob\/master\/_get_runs.py \"\"\" retrieves all runs from that either: - correspond to a git sha - have specific tags. the purpose is to compare runs from a given sha to runs you may have tagged as baselines. \"\"\" import os os.environ[\"_api_key\"] = os.getenv('input__api_key') import import logging import pandas as pd logging.root.setlevel(logging.debug) api = .api() # read inputs this file has been truncated. show original docs..ai import & export data best practices and common use cases for our public api to export data and update existing runs so thanks to a chat in reddit, progress has been made on this. just putting runs into a dataframe using the runs api and then use to_latex. docs..ai import & export data best practices and common use cases for our public api to export data and update existing runs reddit r\/machinelearning - [d] what is your ml experiment workflow? discussion on... 1 vote and 7 comments so far on reddit import pandas as pd import api = .api() entity, project = \"<entity>\", \"<project>\" # set to your entity and project runs = api.runs(entity + \"\/\" + project) summary_list, config_list, name_list = [], [], [] for run in runs: # .summary contains the output keys\/values for metrics like accuracy. # we call ._json_dict to omit large files summary_list.append(run.summary._json_dict) # .config contains the hyperparameters. # we remove special values that start with _. config_list.append( {k: v for k,v in run.config.items() if not k.startswith('_')}) # .name is the human-readable name of the run. name_list.append(run.name) runs_df = pd.dataframe({ \"summary\": summary_list, \"config\": config_list, \"name\": name_list }) runs_df.to_latex() this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for sharing this. this is an interesting idea and would definitely be a valuable function for other w&b users! just to clarify, is this to display metrics from your runs within latex tables or are you looking to extract from our model evaluation tool w&b tables? if youre looking for metrics from your runs, you can use the runs api to get these, which you could then format into a latex table dynamically. ive seen this being done from the team at github to display run metrics within a markdown table in github issue comments this file has been truncated. show original import & export data best practices and common use cases for our public api to export data and update existing runs so thanks to a chat in reddit, progress has been made on this. just putting runs into a dataframe using the runs api and then use import & export data best practices and common use cases for our public api to export data and update existing runs reddit what is your ml experiment workflow? discussion vote and comments so far on reddit this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":57693162.0,
        "Question_title":"Specify database backend store creation in specific schema",
        "Question_body":"<p>When creating an mlflow tracking server and specifying that a SQL Server database is to be used as a backend store, mlflow creates a bunch of table within the dbo schema. Does anyone know if it is possible to specify a different schema in which to create these tables?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1566997596983,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":823.0,
        "Owner_creation_time":1566996760528,
        "Owner_last_access_time":1630269065472,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57693162",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: specify database backend store creation in specific schema; Content: when creating an tracking server and specifying that a sql server database is to be used as a backend store, creates a bunch of table within the dbo schema. does anyone know if it is possible to specify a different schema in which to create these tables?",
        "Question_original_content_gpt_summary":"The user is looking for a way to specify a different schema in which to create tables when creating a SQL Server database as a backend store for a tracking server.",
        "Question_preprocessed_content":"Title: specify database backend store creation in specific schema; Content: when creating an tracking server and specifying that a sql server database is to be used as a backend store, creates a bunch of table within the dbo schema. does anyone know if it is possible to specify a different schema in which to create these tables?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":66887340.0,
        "Question_title":"How do you clear the persistent storage for a notebook instance on AWS SageMaker?",
        "Question_body":"<p>So I'm running into the following error on AWS SageMaker when trying to save:<\/p>\n<blockquote>\n<p>Unexpected error while saving file: untitled.ipynb [Errno 28] No space left on device<\/p>\n<\/blockquote>\n<p>If I remove my notebook, create a new identical one and run it, everything works fine. However, I'm suspecting the Jupyter checkpoint takes up too much space if I save the notebook while it's running and therefore I'm running out of space. Sadly, getting more storage is not an option for me, so I'm wondering if there's any command I can use to clear the storage before running my notebook?<\/p>\n<p>More specifically, clearing the persistent storage in the beginning and at the end of the training process.<\/p>\n<p>I have googled like a maniac but there is no suggestion aside from &quot;just increase the amount of storage bro&quot; and that's why I'm asking the question here.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1617191466313,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":973.0,
        "Owner_creation_time":1504372857016,
        "Owner_last_access_time":1663921906523,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":7.0,
        "Answer_body":"<p>If you don't want your data to be persistent across multiple notebook runs, just store them in <code>\/tmp<\/code> which is not persistent. You have at least 10GB. More details <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-create-ws.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_creation_time":1617198319367,
        "Answer_score":0.0,
        "Owner_location":"Sweden",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66887340",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do you clear the persistent storage for a notebook instance on ?; Content: so i'm running into the following error on when trying to save: unexpected error while saving file: untitled.ipynb [errno 28] no space left on device if i remove my notebook, create a new identical one and run it, everything works fine. however, i'm suspecting the jupyter checkpoint takes up too much space if i save the notebook while it's running and therefore i'm running out of space. sadly, getting more storage is not an option for me, so i'm wondering if there's any command i can use to clear the storage before running my notebook? more specifically, clearing the persistent storage in the beginning and at the end of the training process. i have googled like a maniac but there is no suggestion aside from \"just increase the amount of storage bro\" and that's why i'm asking the question here. thanks in advance!",
        "Question_original_content_gpt_summary":"The user is encountering an error when trying to save a notebook instance on , and is looking for a way to clear the persistent storage before running the notebook in order to avoid running out of space.",
        "Question_preprocessed_content":"Title: how do you clear the persistent storage for a notebook instance on ?; Content: so i'm running into the following error on when trying to save unexpected error while saving file no space left on device if i remove my notebook, create a new identical one and run it, everything works fine. however, i'm suspecting the jupyter checkpoint takes up too much space if i save the notebook while it's running and therefore i'm running out of space. sadly, getting more storage is not an option for me, so i'm wondering if there's any command i can use to clear the storage before running my notebook? more specifically, clearing the persistent storage in the beginning and at the end of the training process. i have googled like a maniac but there is no suggestion aside from just increase the amount of storage bro and that's why i'm asking the question here. thanks in advance!",
        "Answer_original_content":"if you don't want your data to be persistent across multiple notebook runs, just store them in \/tmp which is not persistent. you have at least 10gb. more details here.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"if you don't want your data to be persistent across multiple notebook runs, just store them in which is not persistent. you have at least gb. more details here."
    },
    {
        "Question_id":null,
        "Question_title":"Stop button",
        "Question_body":"<p>Hi<br>\nI\u2019m running training on aws batch (on docker image) and I want to be able to stop the run manually using the button on the wandb and ideally therefore stop that aws batch instance (since the command finished executing).<br>\nThe training runs are using bot key that was given to me. And when I click the stop button (on website, using my account), it says I can\u2019t view the page.<br>\nIs it a permission issue? Will it work as I described?<br>\nThanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1631713980302,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":282.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/stop-button\/614",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-16T03:35:41.791Z",
                "Answer_body":"<p>Hi there,<\/p>\n<p>Can you share a little more about how you\u2019re running these runs? Are they attributed to your personal account or to a service account?<\/p>",
                "Answer_score":49.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-16T20:55:54.823Z",
                "Answer_body":"<p>basically we have a script to start up a aws docker run given a command, in this case to start training. The docker run stops when the command finished executing and I want to stop the training and thus stop the docker  run.<br>\nIt\u2019s service account? work account.<\/p>",
                "Answer_score":8.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-04T15:34:04.339Z",
                "Answer_body":"<p>Hi! Were you the original creator of the run? I think what is likely happening is that someone else created the run and therefore you may not have permissions to stop it.<\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: stop button; Content: hi i\u2019m running training on aws batch (on docker image) and i want to be able to stop the run manually using the button on the and ideally therefore stop that aws batch instance (since the command finished executing). the training runs are using bot key that was given to me. and when i click the stop button (on website, using my account), it says i can\u2019t view the page. is it a permission issue? will it work as i described? thanks",
        "Question_original_content_gpt_summary":"The user is encountering an issue with a stop button on a website, where they are unable to view the page when they click it, and is wondering if it is a permission issue or if it will work as they have described.",
        "Question_preprocessed_content":"Title: stop button; Content: hi im running training on aws batch and i want to be able to stop the run manually using the button on the and ideally therefore stop that aws batch instance . the training runs are using bot key that was given to me. and when i click the stop button , it says i cant view the page. is it a permission issue? will it work as i described? thanks",
        "Answer_original_content":"hi there, can you share a little more about how youre running these runs? are they attributed to your personal account or to a service account? basically we have a script to start up a aws docker run given a command, in this case to start training. the docker run stops when the command finished executing and i want to stop the training and thus stop the docker run. its service account? work account. hi! were you the original creator of the run? i think what is likely happening is that someone else created the run and therefore you may not have permissions to stop it.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi there, can you share a little more about how youre running these runs? are they attributed to your personal account or to a service account? basically we have a script to start up a aws docker run given a command, in this case to start training. the docker run stops when the command finished executing and i want to stop the training and thus stop the docker run. its service account? work account. hi! were you the original creator of the run? i think what is likely happening is that someone else created the run and therefore you may not have permissions to stop it."
    },
    {
        "Question_id":72210450.0,
        "Question_title":"Is there any way to create or delete workspaces in AML studio using powershell?",
        "Question_body":"<p>I am working on a prediction model and am about to use the azure machine learning studio resources. The main operation is to create a workspace on azure ML studio through Powershell. I would like to operate my workspace through the command line. Is there any way to develop and operate the ML Studio workspace through Powershell?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1652332894457,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":104.0,
        "Owner_creation_time":1652331444420,
        "Owner_last_access_time":1652336195336,
        "Owner_reputation":3.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":8.0,
        "Answer_body":"<p>According to the requirements, there is no procedure developed to create\/delete workspaces through PowerShell in machine learning studio. For reference of creation of workspaces, you can check the below link and the point to be noted is we can create\/delete workspaces using <em><strong>Az<\/strong><\/em><\/p>\n<p>Here is the table link to check PowerShell support table<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PGfhb.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1652333522488,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72210450",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is there any way to create or delete workspaces in aml studio using powershell?; Content: i am working on a prediction model and am about to use the studio resources. the main operation is to create a workspace on studio through powershell. i would like to operate my workspace through the command line. is there any way to develop and operate the ml studio workspace through powershell?",
        "Question_original_content_gpt_summary":"The user is looking for a way to create and delete workspaces in Azure Machine Learning Studio using PowerShell.",
        "Question_preprocessed_content":"Title: is there any way to create or delete workspaces in aml studio using powershell?; Content: i am working on a prediction model and am about to use the studio resources. the main operation is to create a workspace on studio through powershell. i would like to operate my workspace through the command line. is there any way to develop and operate the ml studio workspace through powershell?",
        "Answer_original_content":"according to the requirements, there is no procedure developed to create\/delete workspaces through powershell in machine learning studio. for reference of creation of workspaces, you can check the below link and the point to be noted is we can create\/delete workspaces using az here is the table link to check powershell support table https:\/\/docs.microsoft.com\/en-us\/previous-versions\/azure\/machine-learning\/classic\/powershell-module",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"according to the requirements, there is no procedure developed to workspaces through powershell in machine learning studio. for reference of creation of workspaces, you can check the below link and the point to be noted is we can workspaces using az here is the table link to check powershell support table"
    },
    {
        "Question_id":null,
        "Question_title":"Could it trigger the train model with Restful API?",
        "Question_body":"Hi, there\n\u00a0 \u00a0 I sent the message\u00a0 to \"2.0\/preview\/mlflow\/runs\/create\" \uff0cthen I can see the detail on the UI including\u00a0the run command.\u00a0 But when the start time is up\uff0c still not run.\u00a0\n\n\n\u00a0I hope that it could trigger a JOB when call the some Restful API. How could it work ?\u00a0\n\n\n\u00a0The following is the response message:\n\n\nhttp:\/\/host:port\/api\/2.0\/preview\/mlflow\/runs\/create\n\n\n\n\n{\n\"experiment_id\": \"0\",\n\"user_id\": \"me\",\n\"run_name\": \"test_run\",\n\"source_type\": \"PROJECT\",\n\"source_name\": \"git@test\/test.git\",\n\"entry_point_name\": \"main\",\n\"start_time\":1543314730000,\n\"source_version\": \"\",\n\"tags\":[]\n}\n\n\nrequest :\n\n\n{\n\u00a0 \u00a0 \"run\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \"info\": {\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"start_time\": \"1543314730000\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"user_id\": \"me\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"run_uuid\": \"3e178d7173874811a27b83ebbb3881e7\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"entry_point_name\": \"main\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"experiment_id\": \"0\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"source_type\": \"PROJECT\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"source_name\": \"git@test\/test.git\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"lifecycle_stage\": \"active\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"status\": \"RUNNING\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"artifact_uri\": \"$WORK_PATH\/mlruns\/0\/3e178d7173874811a27b83ebbb3881e7\/artifacts\"\n\u00a0 \u00a0 \u00a0 \u00a0 }\n\u00a0 \u00a0 }\n}",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1543301201000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":24.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/D38-4wDIVsg",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2018-11-29T02:10:49",
                "Answer_body":"Hi He,\n\nThis Tracking API is only for reporting runs to the server, not for asking it to execute code. You should execute code wherever you want outside of MLflow, and then use this REST API or the MLflow Python\/Java\/R libraries to report results to the server. The server can then visualize them.\n\nMatei\n\n\ue5d3\n> --\n> You received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n> To post to this group, send email to mlflow...@googlegroups.com.\n> To view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/5961775f-2fa2-4101-bab6-a524789df1a6%40googlegroups.com.\n> For more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2018-11-29T05:09:46",
                "Answer_body":"Hi\u00a0 Matei,\n\n\nUnderstood. Thanks for your response.\n\n\u5728 2018\u5e7411\u670829\u65e5\u661f\u671f\u56db UTC+8\u4e0b\u53483:10:49\uff0cMatei Zaharia\u5199\u9053\uff1a\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: could it trigger the train model with restful api?; Content: hi, there i sent the message to \"2.0\/preview\/\/runs\/create\" \uff0cthen i can see the detail on the ui including the run command. but when the start time is up\uff0c still not run. i hope that it could trigger a job when call the some restful api. how could it work ? the following is the response message: http:\/\/host:port\/api\/2.0\/preview\/\/runs\/create { \"experiment_id\": \"0\", \"user_id\": \"me\", \"run_name\": \"test_run\", \"source_type\": \"project\", \"source_name\": \"git@test\/test.git\", \"entry_point_name\": \"main\", \"start_time\":1543314730000, \"source_version\": \"\", \"tags\":[] } request : { \"run\": { \"info\": { \"start_time\": \"1543314730000\", \"user_id\": \"me\", \"run_uuid\": \"3e178d7173874811a27b83ebbb3881e7\", \"name\": \"\", \"entry_point_name\": \"main\", \"experiment_id\": \"0\", \"source_type\": \"project\", \"source_name\": \"git@test\/test.git\", \"lifecycle_stage\": \"active\", \"status\": \"running\", \"artifact_uri\": \"$work_path\/mlruns\/0\/3e178d7173874811a27b83ebbb3881e7\/artifacts\" } } }",
        "Question_original_content_gpt_summary":"The user is encountering challenges with triggering a job when calling a RESTful API to start a training model.",
        "Question_preprocessed_content":"Title: could it trigger the train model with restful api?; Content: hi, there i sent the message to then i can see the detail on the ui includingthe run command. but when the start time is up still not run. i hope that it could trigger a job when call the some restful api. how could it work ? the following is the response message , me , project , main , , tags request run",
        "Answer_original_content":"hi he, this tracking api is only for reporting runs to the server, not for asking it to execute code. you should execute code wherever you want outside of , and then use this rest api or the python\/java\/r libraries to report results to the server. the server can then visualize them. matei > -- > you received this message because you are subscribed to the google groups \"-users\" group. > to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. > to post to this group, send email to ...@googlegroups.com. > to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/5961775f-2fa2-4101-bab6-a524789df1a6%40googlegroups.com. > for more options, visit https:\/\/groups.google.com\/d\/optout. hi matei, understood. thanks for your response. 20181129 utc+83:10:49matei zaharia",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi he, this tracking api is only for reporting runs to the server, not for asking it to execute code. you should execute code wherever you want outside of , and then use this rest api or the libraries to report results to the server. the server can then visualize them. matei > > you received this message because you are subscribed to the google groups users group. > to unsubscribe from this group and stop receiving emails from it, send an email to > to post to this group, send email to > to view this discussion on the web visit > for more options, visit hi matei, understood. thanks for your response. utc+ matei zaharia"
    },
    {
        "Question_id":null,
        "Question_title":"Prediction of Cancer",
        "Question_body":"I have made a prediction algorithm in which I have predicted whether a patient has cancer or not based on the past data. I have also run the model successfully and have received the parameters. Now my question is, which parameter should I give the most importance for this case of prediction? Is it the precision, recall, accuracy or the threshold?",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1600172964937,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/95802\/prediction-of-cancer.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-16T13:18:10.467Z",
                "Answer_score":0,
                "Answer_body":"@MUJEEBURRAHMAN-1177 Thanks, If your ML model has good performance metrics (you should decide which works best based on confusion matrix and which error can be handled and which should be reduced) then it is a great solution. Can you please share link to the model and the features that you are trying, also please share what feature engineering have you tried.\n\nCould you please add more details about how you\u2019re measuring your model error. If it\u2019s doing worse than validation against your test set, then something weird is going on. If it\u2019s doing better but not reaching 100% accuracy, that\u2019s probably fine, and maybe preferable since it suggests that you\u2019re not overfitting.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: prediction of cancer; Content: i have made a prediction algorithm in which i have predicted whether a patient has cancer or not based on the past data. i have also run the model successfully and have received the parameters. now my question is, which parameter should i give the most importance for this case of prediction? is it the precision, recall, accuracy or the threshold?",
        "Question_original_content_gpt_summary":"The user is facing a challenge of determining which parameter to prioritize in order to accurately predict whether a patient has cancer or not.",
        "Question_preprocessed_content":"Title: prediction of cancer; Content: i have made a prediction algorithm in which i have predicted whether a patient has cancer or not based on the past data. i have also run the model successfully and have received the parameters. now my question is, which parameter should i give the most importance for this case of prediction? is it the precision, recall, accuracy or the threshold?",
        "Answer_original_content":"@mujeeburrahman-1177 thanks, if your ml model has good performance metrics (you should decide which works best based on confusion matrix and which error can be handled and which should be reduced) then it is a great solution. can you please share link to the model and the features that you are trying, also please share what feature engineering have you tried. could you please add more details about how youre measuring your model error. if its doing worse than validation against your test set, then something weird is going on. if its doing better but not reaching 100% accuracy, thats probably fine, and maybe preferable since it suggests that youre not overfitting.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks, if your ml model has good performance metrics then it is a great solution. can you please share link to the model and the features that you are trying, also please share what feature engineering have you tried. could you please add more details about how youre measuring your model error. if its doing worse than validation against your test set, then something weird is going on. if its doing better but not reaching % accuracy, thats probably fine, and maybe preferable since it suggests that youre not overfitting."
    },
    {
        "Question_id":null,
        "Question_title":"Add_reference() with nested folders",
        "Question_body":"<p>We need to set a dataset folder in S3 as an artifact.  The folder has many sub-directories (only one layer though).<br>\nWhen I use the a <code>add_reference()<\/code> command it only stores the directory names of the top-level.<br>\nOf course, I could loop across it, but I\u2019m wondering if there is a command option to make the operation recursive?<\/p>\n<pre><code class=\"lang-auto\">run  = wandb.init(project=WB_PROJECT)\nart = wandb.Artifact(WB_ENTITY, type=WB_DATASET)\nart.add_reference(s3_full, max_objects=WB_MAX_OBJECTS_TO_UPLOAD)\nrun.log_artifact(art)\nwandb.finish()\n<\/code><\/pre>\n<p>EDIT 1: I conclude that the all files are not being added because the <code>Num Files<\/code> in the Artifact Overview shows only <code>5<\/code>.  If I click on the directories, it seems I can see the files, but I assume they are not actually there because of the <code>5<\/code> being reported for the number of files.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1662827790113,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":370.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/add-reference-with-nested-folders\/3092",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-14T13:35:14.013Z",
                "Answer_body":"<p>Hi Kevin,<\/p>\n<p>Thanks for your question! I think that the only thing that may work for you would be adding an S3 prefix without an explicit name (documentation here), the other way could be using a loop. Please let me know if this would be helpful<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-19T04:29:06.402Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/system\">@system<\/a> (luis) Thank you for the reply.  I think the core question is: why does \u201cNum Files\u201d (In the Artifact view) incorrectly list only the top-level files?  I may have 200K files in the artifact, but the \u201cNum Files\u201d only says \u201c5\u201d.  See example below.  Perhaps this could be fixed?  It is somewhat distressing for this parameter to be so wrong.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e.png\" data-download-href=\"\/uploads\/short-url\/pLJ7xdYJeVEm5H2IwETfNri6E2O.png?dl=1\" title=\"Artifact\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_690x150.png\" alt=\"Artifact\" data-base62-sha1=\"pLJ7xdYJeVEm5H2IwETfNri6E2O\" width=\"690\" height=\"150\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_690x150.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_1035x225.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_1380x300.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b49b825923f1f84758e445e5fe3ff1c2f4fd209e_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Artifact<\/span><span class=\"informations\">2228\u00d7486 49.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-21T11:32:02.548Z",
                "Answer_body":"<p>Hi Kevin,<\/p>\n<p>Thanks for the detailed explanation! I see your issue, I will create a request for this feature, thanks for reporting it! May I help you with any other issue?<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-11-18T04:29:28.422Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: add_reference() with nested folders; Content: we need to set a dataset folder in s3 as an artifact. the folder has many sub-directories (only one layer though). when i use the a add_reference() command it only stores the directory names of the top-level. of course, i could loop across it, but i\u2019m wondering if there is a command option to make the operation recursive? run = .init(project=wb_project) art = .artifact(wb_entity, type=wb_dataset) art.add_reference(s3_full, max_objects=wb_max_objects_to_upload) run.log_artifact(art) .finish() edit 1: i conclude that the all files are not being added because the num files in the artifact overview shows only 5. if i click on the directories, it seems i can see the files, but i assume they are not actually there because of the 5 being reported for the number of files.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with the add_reference() command when attempting to set a dataset folder in s3 as an artifact, as the command only stores the directory names of the top-level and does not make the operation recursive.",
        "Question_preprocessed_content":"Title: with nested folders; Content: we need to set a dataset folder in s as an artifact. the folder has many sub directories . when i use the a command it only stores the directory names of the top level. of course, i could loop across it, but im wondering if there is a command option to make the operation recursive? edit i conclude that the all files are not being added because the in the artifact overview shows only . if i click on the directories, it seems i can see the files, but i assume they are not actually there because of the being reported for the number of files.",
        "Answer_original_content":"hi kevin, thanks for the detailed explanation! i see your issue, i will create a request for this feature, thanks for reporting it! may i help you with any other issue? best, luis",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi kevin, thanks for the detailed explanation! i see your issue, i will create a request for this feature, thanks for reporting it! may i help you with any other issue? best, luis"
    },
    {
        "Question_id":null,
        "Question_title":"Download logged table in run locally",
        "Question_body":"<p>Hi all,<\/p>\n<p>I have been trying to download and open Wandb table locally. I have managed to get the corresponding table and its id, however, I cannot find way to download the table and open it as CSV for example.<\/p>\n<pre><code class=\"lang-auto\">runs[0].summary['avg_results'].keys()\ndict_keys(['_type', 'ncols', 'nrows', 'sha256', 'artifact_path', '_latest_artifact_path', 'path', 'size'])```\n\nAbove is a snippet of what I have managed to reach, how can I go from this point to get the table file and read it as cdv<\/code><\/pre>",
        "Question_answer_count":9,
        "Question_comment_count":null,
        "Question_creation_time":1670384556861,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":259.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/download-logged-table-in-run-locally\/3507",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-07T11:03:29.287Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohamedr002\">@mohamedr002<\/a> thank you for writing in! You could use the API to download the Table in json format which you could then easily convert to pandas dataframe. Please see below a code snippet, and feel free to ask more questions:<\/p>\n<pre><code class=\"lang-auto\">import wandb\napi = wandb.Api()\nrun = api.run(f\"ENTITY\/PROJECT\/{run_id}\")\ntable = run.logged_artifacts()[0]\ntable_dir = table.download()\ntable_name = \"my_table_name\"\ntable_path = f\"{table_dir}\/{table_name}.table.json\"\nwith open(table_path) as file:\n    json_dict = json.load(file)\ndf = pd.DataFrame(json_dict[\"data\"], columns=json_dict[\"columns\"])\n<\/code><\/pre>\n<p>Please note that logged_artifacts() is an iterator, and for simplicity I added [0] to return only the first entry as an example. Would this work for you?<\/p>",
                "Answer_score":26.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-07T11:20:59.415Z",
                "Answer_body":"<p>Hi Thanos,<\/p>\n<p>Thank you so much for your clear response. But only one issue that when I logged the table I didn\u2019t log it as artifacts I have just used the workspace<\/p>\n<blockquote>\n<p>wandb.log(wandb.table)<\/p>\n<\/blockquote>\n<p>Will your provider solution still work? Or it requires logging the table as artifact?<\/p>",
                "Answer_score":11.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-07T12:32:33.693Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"thanos-wandb\" data-post=\"2\" data-topic=\"3507\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/t\/4af34b\/40.png\" class=\"avatar\"> thanos-wandb:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">df = pd.DataFrame(json_dict[\"data\"], columns=json_dict[\"columns\"])\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>Hi Thanos,<\/p>\n<p>I have tried your script it worked but not directly. The issue as I mentioned that I don\u2019t have the artifact name, so I managed to get the table path directly.<\/p>\n<pre><code class=\"lang-auto\">avg_table_path = best_run.summary['avg_results']['path']\navg_table = json.load(open(avg_table_path))\navg_df = pd.DataFrame(avg_table['data'], columns= avg_table['columns'])\n<\/code><\/pre>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-07T12:33:21.640Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohamedr002\">@mohamedr002<\/a> that\u2019s automatically done when you\u2019re logging wandb.Table objects. You could click on the Artifacts icon (left panel) from your project\u2019s workspace. Another way to get directly <code>table<\/code> would be:<\/p>\n<p><code>table = run.use_artifact(\"run-&lt;run-id&gt;-&lt;table_name&gt;:&lt;tag&gt;\").get(\"&lt;table_name&gt;\")<\/code><\/p>\n<p>Please let me know if that works for you, or if you have any further questions.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-07T12:40:37.039Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohamedr002\">@mohamedr002<\/a> we both posted same time, is this issue now resolved for you by getting the <code>avg_table_path<\/code> first? May I also ask if these logged tables were wandb.Table objects? in that case it would also create an artifact.<\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-07T12:47:12.937Z",
                "Answer_body":"<p>Yes, you are right, I found the table as already been logged as artifiact, but rather than getting the name and directory separately, I used the \u2018path\u2019 element that exist in the table artifact dictionary.  I am really thankful for your prompt response. Really appreciated!<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-07T13:01:52.708Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohamedr002\">@mohamedr002<\/a> glad to hear that, thanks a lot for posting your workaround for future reference! I am closing this ticket for now, but please feel free to reach out to us if you have any other questions!<\/p>",
                "Answer_score":21.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-24T08:17:07.898Z",
                "Answer_body":"<p>Hey, I am trying to use, <code>table = run.use_artifact(\"run-&lt;run-id&gt;-&lt;table_name&gt;:&lt;tag&gt;\").get(\"&lt;table_name&gt;\")<\/code><br>\nCan you help how run should be initialised?<\/p>\n<p>What should I pass as arguments here?<br>\n<code>run = wandb.Api().artifact()<\/code><\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-28T20:10:47.398Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/satpalsr\">@satpalsr<\/a> there are two ways to access this artifact, either by initialising a run such as <code>run = wandb.init()<\/code> and then you could use<code> run.use_artifact<\/code> method or by using our public API. In the latter case, you could do the following:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nartifact = api.artifact('entity\/project\/artifact-name:alias')\n<\/code><\/pre>\n<p>I hope this helps!<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"",
        "Answer_original_content":"",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":""
    },
    {
        "Question_id":null,
        "Question_title":"Dialogflox cx conflict: \"intents matching\" and \"parameters form\" at the same page.",
        "Question_body":"Hello everyone,I found an unexpected behavior with the following page-level configuration.CONFLICT: After the question \"What do you want? \", if the user input is not clear there will not be any intent matching, but if the user include in the sentence the word \"Value1\" which is synonym of \"Entity1\", then the parameter \"intent_param\" (entity type \"@intent\") will be collected with value \"Entity1\". When this happens I was expecting \"sys.no-match-1\" to be activated, but this did not occurred and the page state status is \"PROCESSING_FORM\" (FormFilled: false).Does anyone knows why this happens and if is there a way to avoid this behavior? In this situation I would like to continue the workflow with the  parameter collected and no intents matched.Thank you,\nMiguel.",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1668491700000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":67.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflox-cx-conflict-quot-intents-matching-quot-and-quot\/td-p\/489582\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-15T15:25:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Can you provide an example of\u00a0Value1? It would also help if you provide the whole response for investigation purposes."
            },
            {
                "Answer_creation_time":"2022-11-16T06:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello, this would be an example for the previous explanation:\n\nEntity type:\nEntity1: Tarjetas\nSynonyms: \"Tarjetas\", \"Cart\u00f3n\", \"Pl\u00e1stico\"\n\nPage (not start page):\n- Agent Fulfillment: \"What do you want?\"\n- Required\/Not required Parameter: \"intent_param\" (I said required in the previous explanation, in both cases I can't achieve what I was expecting)\n- Some intent Routes\n- Event handler (\"sys.no-match-1\")\n\nUser input: \"Pl\u00e1stico\"\n\nThe behavior which I was expecting is that Agent tried to match any of the intents and:\n1. If match, activates the corresponding intent route\n2. If no-match, activates the\u00a0\"sys.no-match-1\" event handler\n\nAnd I also expected that simultaneously, if the input matches with the Entity\/Synonym, the parameter was also collected and mapped with the corresponding entity.\n\nSummarizing, is it possible to collect a parameter (map entity type) and try to match intents, with one unique user input at the same time?\n\nResponse for Required parameter (Parameter is collected but event handler is not activated):\n\n{\n  \"advancedSettings\": {\n    \"audioExportGcsDestination\": {},\n    \"loggingSettings\": {\n      \"enableInteractionLogging\": true\n    },\n    \"speechSettings\": {\n      \"endpointerSensitivity\": 90,\n      \"noSpeechTimeout\": \"5s\"\n    }\n  },\n  \"currentPage\": {\n    \"displayName\": \"Prueba params\",\n    \"name\": \"projects\/dev-bbva-dialogflow-poc\/locations\/us-central1\/agents\/57dc60ad-5e59-4913-9c5a-dcef53842159\/flows\/9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\/pages\/5409aed3-7c85-47a8-aa87-2a84947d8b5d\"\n  },\n  \"diagnosticInfo\": {\n    \"Alternative Matched Intents\": [\n      {\n        \"Score\": 1,\n        \"Active\": true,\n        \"Parameters\": {\n          \"intent_param\": {\n            \"resolved\": \"REDACTED\",\n            \"original\": \"REDACTED\",\n            \"type\": \"@resolve_intent\"\n          }\n        },\n        \"Type\": \"NLU_SLOT\"\n      }\n    ],\n    \"Transition Targets Chain\": [],\n    \"Session Id\": \"19db1e-467-50c-8d4-69fdb3e1a\",\n    \"Triggered Transition Names\": [],\n    \"Execution Sequence\": [\n      {\n        \"Step 1\": {\n          \"InitialState\": {\n            \"SessionParameters\": {\n              \"intent_param\": \"REDACTED\"\n            },\n            \"MatchedIntent\": {\n              \"Active\": true,\n              \"Parameters\": {\n                \"intent_param\": {\n                  \"resolved\": \"REDACTED\",\n                  \"type\": \"@resolve_intent\",\n                  \"original\": \"REDACTED\"\n                }\n              },\n              \"Type\": \"NLU_SLOT\",\n              \"Score\": 1\n            },\n            \"FlowState\": {\n              \"Name\": \"Prueba params\",\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\",\n              \"Version\": 0,\n              \"PageState\": {\n                \"Name\": \"Prueba params\",\n                \"ActiveParameter\": \"intent_param\",\n                \"PageId\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\",\n                \"FormFilled\": false,\n                \"Status\": \"PROCESSING_FORM\"\n              }\n            }\n          },\n          \"Type\": \"INITIAL_STATE\"\n        }\n      },\n      {\n        \"Step 2\": {\n          \"StateMachine\": {\n            \"FlowState\": {\n              \"Name\": \"Prueba params\",\n              \"Version\": 0,\n              \"PageState\": {\n                \"PageId\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\",\n                \"FormFilled\": true,\n                \"Name\": \"Prueba params\",\n                \"Status\": \"TRANSITION_ROUTING\"\n              },\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\"\n            }\n          },\n          \"Type\": \"STATE_MACHINE\"\n        }\n      }\n    ]\n  },\n  \"intentDetectionConfidence\": 1,\n  \"languageCode\": \"es\",\n  \"match\": {\n    \"confidence\": 1,\n    \"matchType\": \"PARAMETER_FILLING\",\n    \"parameters\": {\n      \"intent_param\": \"Tarjetas\"\n    },\n    \"parametersOriginalValues\": {\n      \"intent_param\": \"Pl\u00e1stico\"\n    },\n    \"resolvedInput\": \"Pl\u00e1stico\"\n  },\n  \"parameters\": {\n    \"intent_param\": \"Tarjetas\"\n  },\n  \"redactedParameters\": [\n    \"intent_param\"\n  ],\n  \"responseMessages\": [\n    {\n      \"interactiveVoiceResponseSettings\": {\n        \"audioExportGcsDestination\": {},\n        \"speechSettings\": {\n          \"endpointerSensitivity\": 90,\n          \"noSpeechTimeout\": \"5s\"\n        }\n      }\n    }\n  ],\n  \"text\": \"Pl\u00e1stico\"\n}\n\n\u00a0Response for no roquired parameter (Event handler is activated, but no parameter is collected):\n\n{\n  \"advancedSettings\": {\n    \"audioExportGcsDestination\": {},\n    \"loggingSettings\": {\n      \"enableInteractionLogging\": true\n    },\n    \"speechSettings\": {\n      \"endpointerSensitivity\": 90,\n      \"noSpeechTimeout\": \"5s\"\n    }\n  },\n  \"currentPage\": {\n    \"displayName\": \"End Session\",\n    \"name\": \"projects\/dev-bbva-dialogflow-poc\/locations\/us-central1\/agents\/57dc60ad-5e59-4913-9c5a-dcef53842159\/flows\/9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\/pages\/END_SESSION\"\n  },\n  \"diagnosticInfo\": {\n    \"Execution Sequence\": [\n      {\n        \"Step 1\": {\n          \"InitialState\": {\n            \"Event\": \"sys.no-match-1\",\n            \"FlowState\": {\n              \"PageState\": {\n                \"Name\": \"Prueba params\",\n                \"Status\": \"TRANSITION_ROUTING\",\n                \"PageId\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\",\n                \"FormFilled\": true\n              },\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\",\n              \"Name\": \"Prueba params\",\n              \"Version\": 0\n            }\n          },\n          \"Type\": \"INITIAL_STATE\"\n        }\n      },\n      {\n        \"Step 2\": {\n          \"Type\": \"STATE_MACHINE\",\n          \"FunctionExecution\": {\n            \"Responses\": [\n              {\n                \"responseType\": \"HANDLER_PROMPT\",\n                \"text\": {\n                  \"redactedText\": [\n                    \"No se a qu\u00e9 te refieres ex\u00e1ctamente, pero tiene que ver con $session.params.intent_param\"\n                  ],\n                  \"text\": [\n                    \"No se a qu\u00e9 te refieres ex\u00e1ctamente, pero tiene que ver con $session.params.intent_param\"\n                  ]\n                },\n                \"source\": \"VIRTUAL_AGENT\"\n              }\n            ]\n          },\n          \"StateMachine\": {\n            \"FlowState\": {\n              \"Name\": \"Prueba params\",\n              \"PageState\": {\n                \"PageId\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\",\n                \"Name\": \"Prueba params\",\n                \"FormFilled\": true,\n                \"Status\": \"TRANSITION_ROUTING\"\n              },\n              \"Version\": 0,\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\"\n            },\n            \"TriggeredEvent\": \"sys.no-match-1\",\n            \"TriggeredEventHandlerId\": \"f6760455-fb9b-49fb-acae-69142ec33188\"\n          }\n        }\n      },\n      {\n        \"Step 3\": {\n          \"StateMachine\": {\n            \"FlowState\": {\n              \"FlowId\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\",\n              \"PageState\": {\n                \"Name\": \"End Session\",\n                \"PageId\": \"END_SESSION\",\n                \"Status\": \"ENTERING_PAGE\"\n              },\n              \"Name\": \"Prueba params\",\n              \"Version\": 0\n            }\n          },\n          \"Type\": \"STATE_MACHINE\"\n        }\n      }\n    ],\n    \"Alternative Matched Intents\": [],\n    \"Session Id\": \"946ef0-346-0ee-be7-ec290fb95\",\n    \"Unfulfilled Parameters\": [\n      \"$session.params.intent_param\"\n    ],\n    \"Transition Targets Chain\": [\n      {\n        \"TargetPage\": \"END_SESSION\"\n      }\n    ],\n    \"Triggered Transition Names\": [\n      \"f6760455-fb9b-49fb-acae-69142ec33188\"\n    ]\n  },\n  \"intentDetectionConfidence\": 0.3,\n  \"languageCode\": \"es\",\n  \"match\": {\n    \"confidence\": 0.3,\n    \"event\": \"sys.no-match-1\",\n    \"matchType\": \"NO_MATCH\"\n  },\n  \"responseMessages\": [\n    {\n      \"responseType\": \"HANDLER_PROMPT\",\n      \"source\": \"VIRTUAL_AGENT\",\n      \"text\": {\n        \"redactedText\": [\n          \"No se a qu\u00e9 te refieres ex\u00e1ctamente, pero tiene que ver con $session.params.intent_param\"\n        ],\n        \"text\": [\n          \"No se a qu\u00e9 te refieres ex\u00e1ctamente, pero tiene que ver con $session.params.intent_param\"\n        ]\n      }\n    },\n    {\n      \"interactiveVoiceResponseSettings\": {\n        \"audioExportGcsDestination\": {},\n        \"speechSettings\": {\n          \"endpointerSensitivity\": 90,\n          \"noSpeechTimeout\": \"5s\"\n        }\n      }\n    },\n    {\n      \"endInteraction\": {}\n    }\n  ],\n  \"text\": \"Pl\u00e1stico\"\n}"
            },
            {
                "Answer_creation_time":"2022-11-18T08:45:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"This seems to be not reproducible on my end.\u00a0If you have premium support, you can check with\u00a0GCP Support\u00a0to further check your issue since this is specific to your project."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: dialogflox cx conflict: \"intents matching\" and \"parameters form\" at the same page.; Content: hello everyone,i found an unexpected behavior with the following page-level configuration.conflict: after the question \"what do you want? \", if the user input is not clear there will not be any intent matching, but if the user include in the sentence the word \"value1\" which is synonym of \"entity1\", then the parameter \"intent_param\" (entity type \"@intent\") will be collected with value \"entity1\". when this happens i was expecting \"sys.no-match-1\" to be activated, but this did not occurred and the page state status is \"processing_form\" (formfilled: false).does anyone knows why this happens and if is there a way to avoid this behavior? in this situation i would like to continue the workflow with the parameter collected and no intents matched.thank you, miguel.",
        "Question_original_content_gpt_summary":"The user Miguel encountered a challenge with dialogflow CX where an unexpected behavior occurred when an intent parameter was collected with a value while no intents were matched.",
        "Question_preprocessed_content":"Title: dialogflox cx conflict intents matching and parameters form at the same page.; Content: hello everyone,i found an unexpected behavior with the following page level after the question what do you want? , if the user input is not clear there will not be any intent matching, but if the user include in the sentence the word value which is synonym of entity , then the parameter will be collected with value entity . when this happens i was expecting to be activated, but this did not occurred and the page state status is .does anyone knows why this happens and if is there a way to avoid this behavior? in this situation i would like to continue the workflow with the parameter collected and no intents you, miguel.",
        "Answer_original_content":"can you provide an example ofvalue1? it would also help if you provide the whole response for investigation purposes. hello, this would be an example for the previous explanation: entity type: entity1: tarjetas synonyms: \"tarjetas\", \"cartn\", \"plstico\" page (not start page): - agent fulfillment: \"what do you want?\" - required\/not required parameter: \"intent_param\" (i said required in the previous explanation, in both cases i can't achieve what i was expecting) - some intent routes - event handler (\"sys.no-match-1\") user input: \"plstico\" the behavior which i was expecting is that agent tried to match any of the intents and: 1. if match, activates the corresponding intent route 2. if no-match, activates the\"sys.no-match-1\" event handler and i also expected that simultaneously, if the input matches with the entity\/synonym, the parameter was also collected and mapped with the corresponding entity. summarizing, is it possible to collect a parameter (map entity type) and try to match intents, with one unique user input at the same time? response for required parameter (parameter is collected but event handler is not activated): { \"advancedsettings\": { \"audioexportgcsdestination\": {}, \"loggingsettings\": { \"enableinteractionlogging\": true }, \"speechsettings\": { \"endpointersensitivity\": 90, \"nospeechtimeout\": \"5s\" } }, \"currentpage\": { \"displayname\": \"prueba params\", \"name\": \"projects\/dev-bbva-dialogflow-poc\/locations\/us-central1\/agents\/57dc60ad-5e59-4913-9c5a-dcef53842159\/flows\/9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\/pages\/5409aed3-7c85-47a8-aa87-2a84947d8b5d\" }, \"diagnosticinfo\": { \"alternative matched intents\": [ { \"score\": 1, \"active\": true, \"parameters\": { \"intent_param\": { \"resolved\": \"redacted\", \"original\": \"redacted\", \"type\": \"@resolve_intent\" } }, \"type\": \"nlu_slot\" } ], \"transition targets chain\": [], \"session id\": \"19db1e-467-50c-8d4-69fdb3e1a\", \"triggered transition names\": [], \"execution sequence\": [ { \"step 1\": { \"initialstate\": { \"sessionparameters\": { \"intent_param\": \"redacted\" }, \"matchedintent\": { \"active\": true, \"parameters\": { \"intent_param\": { \"resolved\": \"redacted\", \"type\": \"@resolve_intent\", \"original\": \"redacted\" } }, \"type\": \"nlu_slot\", \"score\": 1 }, \"flowstate\": { \"name\": \"prueba params\", \"flowid\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\", \"version\": 0, \"pagestate\": { \"name\": \"prueba params\", \"activeparameter\": \"intent_param\", \"pageid\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\", \"formfilled\": false, \"status\": \"processing_form\" } } }, \"type\": \"initial_state\" } }, { \"step 2\": { \"statemachine\": { \"flowstate\": { \"name\": \"prueba params\", \"version\": 0, \"pagestate\": { \"pageid\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\", \"formfilled\": true, \"name\": \"prueba params\", \"status\": \"transition_routing\" }, \"flowid\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\" } }, \"type\": \"state_machine\" } } ] }, \"intentdetectionconfidence\": 1, \"languagecode\": \"es\", \"match\": { \"confidence\": 1, \"matchtype\": \"parameter_filling\", \"parameters\": { \"intent_param\": \"tarjetas\" }, \"parametersoriginalvalues\": { \"intent_param\": \"plstico\" }, \"resolvedinput\": \"plstico\" }, \"parameters\": { \"intent_param\": \"tarjetas\" }, \"redactedparameters\": [ \"intent_param\" ], \"responsemessages\": [ { \"interactivevoiceresponsesettings\": { \"audioexportgcsdestination\": {}, \"speechsettings\": { \"endpointersensitivity\": 90, \"nospeechtimeout\": \"5s\" } } } ], \"text\": \"plstico\" } response for no roquired parameter (event handler is activated, but no parameter is collected): { \"advancedsettings\": { \"audioexportgcsdestination\": {}, \"loggingsettings\": { \"enableinteractionlogging\": true }, \"speechsettings\": { \"endpointersensitivity\": 90, \"nospeechtimeout\": \"5s\" } }, \"currentpage\": { \"displayname\": \"end session\", \"name\": \"projects\/dev-bbva-dialogflow-poc\/locations\/us-central1\/agents\/57dc60ad-5e59-4913-9c5a-dcef53842159\/flows\/9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\/pages\/end_session\" }, \"diagnosticinfo\": { \"execution sequence\": [ { \"step 1\": { \"initialstate\": { \"event\": \"sys.no-match-1\", \"flowstate\": { \"pagestate\": { \"name\": \"prueba params\", \"status\": \"transition_routing\", \"pageid\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\", \"formfilled\": true }, \"flowid\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\", \"name\": \"prueba params\", \"version\": 0 } }, \"type\": \"initial_state\" } }, { \"step 2\": { \"type\": \"state_machine\", \"functionexecution\": { \"responses\": [ { \"responsetype\": \"handler_prompt\", \"text\": { \"redactedtext\": [ \"no se a qu te refieres exctamente, pero tiene que ver con $session.params.intent_param\" ], \"text\": [ \"no se a qu te refieres exctamente, pero tiene que ver con $session.params.intent_param\" ] }, \"source\": \"virtual_agent\" } ] }, \"statemachine\": { \"flowstate\": { \"name\": \"prueba params\", \"pagestate\": { \"pageid\": \"5409aed3-7c85-47a8-aa87-2a84947d8b5d\", \"name\": \"prueba params\", \"formfilled\": true, \"status\": \"transition_routing\" }, \"version\": 0, \"flowid\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\" }, \"triggeredevent\": \"sys.no-match-1\", \"triggeredeventhandlerid\": \"f6760455-fb9b-49fb-acae-69142ec33188\" } } }, { \"step 3\": { \"statemachine\": { \"flowstate\": { \"flowid\": \"9aecdd7b-3a57-4342-aaf9-caf3f2c6a9f8\", \"pagestate\": { \"name\": \"end session\", \"pageid\": \"end_session\", \"status\": \"entering_page\" }, \"name\": \"prueba params\", \"version\": 0 } }, \"type\": \"state_machine\" } } ], \"alternative matched intents\": [], \"session id\": \"946ef0-346-0ee-be7-ec290fb95\", \"unfulfilled parameters\": [ \"$session.params.intent_param\" ], \"transition targets chain\": [ { \"targetpage\": \"end_session\" } ], \"triggered transition names\": [ \"f6760455-fb9b-49fb-acae-69142ec33188\" ] }, \"intentdetectionconfidence\": 0.3, \"languagecode\": \"es\", \"match\": { \"confidence\": 0.3, \"event\": \"sys.no-match-1\", \"matchtype\": \"no_match\" }, \"responsemessages\": [ { \"responsetype\": \"handler_prompt\", \"source\": \"virtual_agent\", \"text\": { \"redactedtext\": [ \"no se a qu te refieres exctamente, pero tiene que ver con $session.params.intent_param\" ], \"text\": [ \"no se a qu te refieres exctamente, pero tiene que ver con $session.params.intent_param\" ] } }, { \"interactivevoiceresponsesettings\": { \"audioexportgcsdestination\": {}, \"speechsettings\": { \"endpointersensitivity\": 90, \"nospeechtimeout\": \"5s\" } } }, { \"endinteraction\": {} } ], \"text\": \"plstico\" } this seems to be not reproducible on my end.if you have premium support, you can check withgcp supportto further check your issue since this is specific to your project.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"can you provide an example ofvalue ? it would also help if you provide the whole response for investigation purposes. hello, this would be an example for the previous explanation entity type entity tarjetas synonyms tarjetas , cartn , plstico page agent fulfillment what do you want? required parameter some intent routes event handler user input plstico the behavior which i was expecting is that agent tried to match any of the intents and . if match, activates the corresponding intent route . if no match, activates event handler and i also expected that simultaneously, if the input matches with the the parameter was also collected and mapped with the corresponding entity. summarizing, is it possible to collect a parameter and try to match intents, with one unique user input at the same time? response for required parameter advancedsettings , loggingsettings , speechsettings , currentpage , diagnostic , type , transition targets chain , session id db e c d fdb e a , triggered transition names , execution sequence , intentdetectionconfidence , languagecode es , match , parametersoriginalvalues , resolvedinput plstico , parameters , redactedparameters , responsemessages , text plstico response for no roquired parameter advancedsettings , loggingsettings , speechsettings , currentpage , diagnostic , flowid aecdd b a aaf caf f c a f , name prueba params , version , type , , source , statemachine , version , flowid aecdd b a aaf caf f c a f , triggeredevent triggeredeventhandlerid f fb b fb acae ec , , name prueba params , version , type , alternative matched intents , session id ef ee be ec fb , unfulfilled parameters , transition targets chain , triggered transition names , intentdetectionconfidence languagecode es , match , responsemessages , text , , speechsettings , , text plstico this seems to be not reproducible on my you have premium support, you can check withgcp supportto further check your issue since this is specific to your project."
    },
    {
        "Question_id":49673502.0,
        "Question_title":"Azure Machine Learning - Predicting a win\/lose\/draw API",
        "Question_body":"<p>I'm experimenting with an existing experiment on the Azure Machine learning gallery.  Its called <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Beat-the-Bookie\" rel=\"nofollow noreferrer\">beat the bookie<\/a>.  I want to take it to the next step and add a web service to it.  This particular experiment has a dataset for years of matches.  It has a small python script to calculate an ELO (like chess or most online games).  I'm struggling to create an input API with 2 inputs: homeTeam and awayTeam.  With an output of 1 value FTR (Full-Time Result) which is either W\/D\/L (Win, Draw or Lose).<\/p>\n\n<p>The issue I have is that when I create the API it has too many required inputs.  Do I have to give them averaged data or how do I reduce the inputs to 2?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_creation_time":1522934588567,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":64.0,
        "Owner_creation_time":1469533385120,
        "Owner_last_access_time":1656601407172,
        "Owner_reputation":970.0,
        "Owner_up_votes":93.0,
        "Owner_down_votes":1.0,
        "Owner_views":68.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Ireland",
        "Question_last_edit_time":1526058346292,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49673502",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - predicting a win\/lose\/draw api; Content: i'm experimenting with an existing experiment on the gallery. its called beat the bookie. i want to take it to the next step and add a web service to it. this particular experiment has a dataset for years of matches. it has a small python script to calculate an elo (like chess or most online games). i'm struggling to create an input api with 2 inputs: hometeam and awayteam. with an output of 1 value ftr (full-time result) which is either w\/d\/l (win, draw or lose). the issue i have is that when i create the api it has too many required inputs. do i have to give them averaged data or how do i reduce the inputs to 2?",
        "Question_original_content_gpt_summary":"The user is struggling to create an input API with two inputs (home team and away team) that will output a single value (win, draw, or lose) for a dataset of years of matches, and is having difficulty reducing the inputs to two.",
        "Question_preprocessed_content":"Title: predicting a api; Content: i'm experimenting with an existing experiment on the gallery. its called beat the bookie. i want to take it to the next step and add a web service to it. this particular experiment has a dataset for years of matches. it has a small python script to calculate an elo . i'm struggling to create an input api with inputs hometeam and awayteam. with an output of value ftr which is either . the issue i have is that when i create the api it has too many required inputs. do i have to give them averaged data or how do i reduce the inputs to ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Where should I begin with Machine Learning on Azure?",
        "Question_body":"Hi, I have only been programming for about 6 months and I don't have any formal training.\n\nI started on a platform called Ninjatrader8 building automated trading systems in C#5 on .NET 4.8\n\nout of curiosity I have been experimenting with ML.NET in C# using my own machine but now I have started experimenting with Azure.\n\nI have taken a peak at Python etc, and it seems easy to understand, I have also read a bit about Computer Science principles but there seems to be so much learning I can devout myself to now that I need to break it up into things that I will actually use, else I be stuck in a perpetual loop.\n\nmy goal is simply to have my strategy in the Ninjatrader platform be able to look up the most recent values output by an ML model.\n\nI was going to have my model analyse a file any time a new file was added to a data set and then serialise the results in some way, perhaps by .csv file.\n\nthe file would need to allow for multiple simultaneous read iterations but only a single write iteration.\n\nI feel like with azure I could probably have the strategy and model talking directly over the internet somehow, but it is not 100% necessary if it is to difficult\n\nI am starting on the Microsoft online courses but they are also very broad and numerous.\n\nThankyou, Any help would be appreciated.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1660784016847,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/971544\/where-should-i-begin-with-machine-learning-on-azur.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-18T01:23:57.683Z",
                "Answer_score":2,
                "Answer_body":"Hi @KevinMonaghan-1023\n\nTake a look on content of this one . I personally use udemy for learning and certification.\n\nhttps:\/\/www.udemy.com\/course\/machine-learning-using-azureml\/\n\nRegards,\n\n--please don't forget to upvote and Accept as answer if the reply is helpful--",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: where should i begin with machine learning on azure?; Content: hi, i have only been programming for about 6 months and i don't have any formal training. i started on a platform called ninjatrader8 building automated trading systems in c#5 on .net 4.8 out of curiosity i have been experimenting with ml.net in c# using my own machine but now i have started experimenting with azure. i have taken a peak at python etc, and it seems easy to understand, i have also read a bit about computer science principles but there seems to be so much learning i can devout myself to now that i need to break it up into things that i will actually use, else i be stuck in a perpetual loop. my goal is simply to have my strategy in the ninjatrader platform be able to look up the most recent values output by an ml model. i was going to have my model analyse a file any time a new file was added to a data set and then serialise the results in some way, perhaps by .csv file. the file would need to allow for multiple simultaneous read iterations but only a single write iteration. i feel like with azure i could probably have the strategy and model talking directly over the internet somehow, but it is not 100% necessary if it is to difficult i am starting on the microsoft online courses but they are also very broad and numerous. thankyou, any help would be appreciated.",
        "Question_original_content_gpt_summary":"The user is looking for guidance on how to best utilize Azure for their machine learning project, which involves creating an automated trading system in C#5 on .NET 4.8 and having the strategy look up the most recent values output by an ML model.",
        "Question_preprocessed_content":"Title: where should i begin with machine learning on azure?; Content: hi, i have only been programming for about months and i don't have any formal training. i started on a platform called ninjatrader building automated trading systems in c on .net out of curiosity i have been experimenting with in c using my own machine but now i have started experimenting with azure. i have taken a peak at python etc, and it seems easy to understand, i have also read a bit about computer science principles but there seems to be so much learning i can devout myself to now that i need to break it up into things that i will actually use, else i be stuck in a perpetual loop. my goal is simply to have my strategy in the ninjatrader platform be able to look up the most recent values output by an ml model. i was going to have my model analyse a file any time a new file was added to a data set and then serialise the results in some way, perhaps by .csv file. the file would need to allow for multiple simultaneous read iterations but only a single write iteration. i feel like with azure i could probably have the strategy and model talking directly over the internet somehow, but it is not % necessary if it is to difficult i am starting on the microsoft online courses but they are also very broad and numerous. thankyou, any help would be appreciated.",
        "Answer_original_content":"hi @kevinmonaghan-1023 take a look on content of this one . i personally use udemy for learning and certification. https:\/\/www.udemy.com\/course\/machine-learning-using-\/ regards, --please don't forget to upvote and accept as answer if the reply is helpful--",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi take a look on content of this one . i personally use udemy for learning and certification. regards, please don't forget to upvote and accept as answer if the reply is helpful"
    },
    {
        "Question_id":46320315.0,
        "Question_title":"How to create multiple endpoints through powershell for New AzureML WebServices",
        "Question_body":"<p>I have created a ML WebService on portal.azure.com. I wish to create multiple endpoints programmitically for this webservice using Powershell. \nHowever all the cmdlets available (Add-AmlWebServiceEndpoint) involve using the Old or Classic WebServices.<\/p>\n\n<p>Is there anyway to achieve this for New Azure ML WebServices<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1505905355093,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":110.0,
        "Owner_creation_time":1502280634107,
        "Owner_last_access_time":1532420237512,
        "Owner_reputation":33.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":16.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46320315",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to create multiple endpoints through powershell for new webservices; Content: i have created a ml webservice on portal.azure.com. i wish to create multiple endpoints programmitically for this webservice using powershell. however all the cmdlets available (add-amlwebserviceendpoint) involve using the old or classic webservices. is there anyway to achieve this for new webservices",
        "Question_original_content_gpt_summary":"The user is looking for a way to create multiple endpoints programmatically for a new webservice using PowerShell.",
        "Question_preprocessed_content":"Title: how to create multiple endpoints through powershell for new webservices; Content: i have created a ml webservice on i wish to create multiple endpoints programmitically for this webservice using powershell. however all the cmdlets available involve using the old or classic webservices. is there anyway to achieve this for new webservices",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":69827748.0,
        "Question_title":"get metrics out of AutoMLRun based on test_data",
        "Question_body":"<p>I\u2019m using the following script to execute an AutoML run, also passing the test dataset<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>automl_settings = {\n    &quot;n_cross_validations&quot;: 10,\n    &quot;primary_metric&quot;: 'spearman_correlation',\n    &quot;enable_early_stopping&quot;: True,\n    &quot;max_concurrent_iterations&quot;: 10, \n    &quot;max_cores_per_iteration&quot;: -1,   \n    &quot;experiment_timeout_hours&quot;: 1,\n    &quot;featurization&quot;: 'auto',\n    &quot;verbosity&quot;: logging.INFO}\nautoml_config = AutoMLConfig(task = 'regression',\n                             debug_log = 'automl_errors.log',\n                             compute_target = compute_target,\n                             training_data = training_data,\n                             test_data = test_data,\n                             label_column_name = label_column_name,\n                             model_explainability = True,\n                             **automl_settings                            )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1635954155380,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":85.0,
        "Owner_creation_time":1405457120427,
        "Owner_last_access_time":1663947733100,
        "Owner_reputation":3359.0,
        "Owner_up_votes":1187.0,
        "Owner_down_votes":14.0,
        "Owner_views":555.0,
        "Answer_body":"<p>Note that the TEST DATASET SUPPORT is a feature still in PRIVATE PREVIEW. It'll probably be released as PUBLIC PREVIEW later in NOVEMBER, but until then, you need to be enrolled in the PRIVATE PREVIEW in order to see the &quot;Test runs and metrics&quot; in the UI. You can send me an email to cesardl at microsoft dot com and send me your AZURE SUBSCRIPTION ID to be enabled so you see it in the UI.<\/p>\n<p>You can see further info on how to get started here:\n<a href=\"https:\/\/github.com\/Azure\/automl-testdataset-preview\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/automl-testdataset-preview<\/a><\/p>\n<p>About how to use it, you need to either provide the test_Data (specific Test AML Tabular Dataset that for instance you loaded from a file os split manually previously)\nor you can provide a test_size which is the % (i.e. 0.2 is 20%) to be split from the single\/original dataset.<\/p>\n<p>About the TEST metrics, since you can make multiple TEST runs against a single model, you need to go to the specific TEST run available under the link &quot;Test results&quot;<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3pPPS.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1635964127852,
        "Answer_score":3.0,
        "Owner_location":"Seattle, WA, USA",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69827748",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: get metrics out of automlrun based on test_data; Content: i\u2019m using the following script to execute an automl run, also passing the test dataset automl_settings = { \"n_cross_validations\": 10, \"primary_metric\": 'spearman_correlation', \"enable_early_stopping\": true, \"max_concurrent_iterations\": 10, \"max_cores_per_iteration\": -1, \"experiment_timeout_hours\": 1, \"featurization\": 'auto', \"verbosity\": logging.info} automl_config = automlconfig(task = 'regression', debug_log = 'automl_errors.log', compute_target = compute_target, training_data = training_data, test_data = test_data, label_column_name = label_column_name, model_explainability = true, **automl_settings )",
        "Question_original_content_gpt_summary":"The user is encountering challenges in getting metrics out of an automlrun based on the test_data provided.",
        "Question_preprocessed_content":"Title: get metrics out of automlrun based on; Content: im using the following script to execute an automl run, also passing the test dataset",
        "Answer_original_content":"note that the test dataset support is a feature still in private preview. it'll probably be released as public preview later in november, but until then, you need to be enrolled in the private preview in order to see the \"test runs and metrics\" in the ui. you can send me an email to cesardl at microsoft dot com and send me your azure subscription id to be enabled so you see it in the ui. you can see further info on how to get started here: https:\/\/github.com\/azure\/automl-testdataset-preview about how to use it, you need to either provide the test_data (specific test aml tabular dataset that for instance you loaded from a file os split manually previously) or you can provide a test_size which is the % (i.e. 0.2 is 20%) to be split from the single\/original dataset. about the test metrics, since you can make multiple test runs against a single model, you need to go to the specific test run available under the link \"test results\" enter image description here",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"note that the test dataset support is a feature still in private preview. it'll probably be released as public preview later in november, but until then, you need to be enrolled in the private preview in order to see the test runs and metrics in the ui. you can send me an email to cesardl at microsoft dot com and send me your azure subscription id to be enabled so you see it in the ui. you can see further on how to get started here about how to use it, you need to either provide the or you can provide a which is the % to be split from the dataset. about the test metrics, since you can make multiple test runs against a single model, you need to go to the specific test run available under the link test results enter image description here"
    },
    {
        "Question_id":49202038.0,
        "Question_title":"Scored Labels dont match Scored Probabilities",
        "Question_body":"<p>I have a predictive web service. The scored labels field don't match the highest value for the specific scored probabilities columns. Is it more reliable to search through the different columns and pick the highest?<\/p>\n\n<p>Like, I get a result that has a bunch of \n\"Scored Probabilities for Class\" \\\"\\\"\": 0.6,\n\"Scored Probabilities for Class\" \\\"\\\"\": 3.09<\/p>\n\n<p>But the scored labels show the one with 0.6, how is that possible?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_creation_time":1520629077973,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":122.0,
        "Owner_creation_time":1299082025616,
        "Owner_last_access_time":1663967978327,
        "Owner_reputation":1483.0,
        "Owner_up_votes":647.0,
        "Owner_down_votes":13.0,
        "Owner_views":219.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Costa Rica",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49202038",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: scored labels dont match scored probabilities; Content: i have a predictive web service. the scored labels field don't match the highest value for the specific scored probabilities columns. is it more reliable to search through the different columns and pick the highest? like, i get a result that has a bunch of \"scored probabilities for class\" \\\"\\\"\": 0.6, \"scored probabilities for class\" \\\"\\\"\": 3.09 but the scored labels show the one with 0.6, how is that possible?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where the scored labels do not match the highest value for the specific scored probabilities columns, causing confusion as to which result is more reliable.",
        "Question_preprocessed_content":"Title: scored labels dont match scored probabilities; Content: i have a predictive web service. the scored labels field don't match the highest value for the specific scored probabilities columns. is it more reliable to search through the different columns and pick the highest? like, i get a result that has a bunch of scored probabilities for class scored probabilities for class but the scored labels show the one with how is that possible?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":68319208.0,
        "Question_title":"Unable to import mlflow, getting ModuleNotFoundError: No module named 'mlflow'",
        "Question_body":"<p>Unable to import <code>mlflow<\/code> in a .py script.<\/p>\n<pre><code>ModuleNotFoundError: No module named 'mlflow'\n<\/code><\/pre>\n<p>The script runs in a <code>python:3.7-stretch Docker<\/code> container<\/p>\n<p>Use <code>requirements.txt<\/code> to pip install packages.<\/p>\n<pre><code>(...)\nsqlalchemy==1.4.1\npsycopg2==2.8.6\nmlflow==1.18.0\n<\/code><\/pre>\n<pre><code>RUN pip3 install --default-timeout=5000 --use-deprecated=legacy-resolver -r \/root\/requirements.txt\n<\/code><\/pre>\n<p>Can see that it is installed.<\/p>\n<pre><code>root@abc:~# pip uninstall mlflow\nFound existing installation: mlflow 1.18.0\nUninstalling mlflow-1.18.0:\n  Would remove:\n    \/usr\/local\/bin\/mlflow\n    \/usr\/local\/lib\/python3.7\/site-packages\/mlflow-1.18.0.dist-info\/*\n    \/usr\/local\/lib\/python3.7\/site-packages\/mlflow\/*\nProceed (y\/n)? n\n<\/code><\/pre>\n<p>Can do an import from python shell.<\/p>\n<pre><code>root@abc:~# python\nPython 3.7.10 (default, Feb 16 2021, 19:46:13)\n[GCC 6.3.0 20170516] on linux\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; import mlflow\n&gt;&gt;&gt;\n<\/code><\/pre>\n<p>But no joy when running from .py script.<\/p>\n<p>Other packages installed from <code>requirements.txt<\/code> can be imported.<\/p>\n<p>Any ideas what is wrong ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1625844282463,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":814.0,
        "Owner_creation_time":1461829551256,
        "Owner_last_access_time":1663945644783,
        "Owner_reputation":375.0,
        "Owner_up_votes":241.0,
        "Owner_down_votes":0.0,
        "Owner_views":26.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1650287875787,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68319208",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unable to import , getting modulenotfounderror: no module named ''; Content: unable to import in a .py script. modulenotfounderror: no module named '' the script runs in a python:3.7-stretch docker container use requirements.txt to pip install packages. (...) sqlalchemy==1.4.1 psycopg2==2.8.6 ==1.18.0 run pip3 install --default-timeout=5000 --use-deprecated=legacy-resolver -r \/root\/requirements.txt can see that it is installed. root@abc:~# pip uninstall found existing installation: 1.18.0 uninstalling -1.18.0: would remove: \/usr\/local\/bin\/ \/usr\/local\/lib\/python3.7\/site-packages\/-1.18.0.dist-info\/* \/usr\/local\/lib\/python3.7\/site-packages\/\/* proceed (y\/n)? n can do an import from python shell. root@abc:~# python python 3.7.10 (default, feb 16 2021, 19:46:13) [gcc 6.3.0 20170516] on linux type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> >>> >>> import >>> but no joy when running from .py script. other packages installed from requirements.txt can be imported. any ideas what is wrong ?",
        "Question_original_content_gpt_summary":"The user is encountering a ModuleNotFoundError when attempting to import a package in a .py script, despite being able to import it from the Python shell and having installed it from a requirements.txt file.",
        "Question_preprocessed_content":"Title: unable to import , getting modulenotfounderror no module named ''; Content: unable to import in a .py script. the script runs in a container use to pip install packages. can see that it is installed. can do an import from python shell. but no joy when running from .py script. other packages installed from can be imported. any ideas what is wrong ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":66216294.0,
        "Question_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Question_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1613428648627,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":276.0,
        "Owner_creation_time":1585870038407,
        "Owner_last_access_time":1626728054187,
        "Owner_reputation":45.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1613774515323,
        "Answer_score":2.0,
        "Owner_location":"Christchurch, \u041d\u043e\u0432\u0430\u044f \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u044f",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to change .conf file in aws sagemaker; Content: i am working in aws sagemaker jupyter notebook. i have installed package in aws sagemaker in jupyter. server was installed on aws ec2. i need to store artifacts and models in aws s3 bucket, so i want to specify credentials to s3 in .conf file. how can i change .conf file in aws sagemaker instance? looks like permission denied to all folders on it. or maybe somebody can suggest a better approach.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in changing the .conf file in AWS Sagemaker in order to store artifacts and models in an S3 bucket.",
        "Question_preprocessed_content":"Title: how to change file in aws sagemaker; Content: i am working in aws sagemaker jupyter notebook. i have installed package in aws sagemaker in jupyter. server was installed on aws ec . i need to store artifacts and models in aws s bucket, so i want to specify credentials to s in file. how can i change file in aws sagemaker instance? looks like permission denied to all folders on it. or maybe somebody can suggest a better approach.",
        "Answer_original_content":"disclaimer i'm part of the (formerly trains) team. to set credentials (and -server hosts) you can use task.set_credentials. to specify the s3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the files_server. for example: from import task task.set_credentials(api_host='http:\/\/-server:8008', web_host='http:\/\/-server:8080', files_host='s3:\/\/my_bucket\/folder\/', key='add__key_here', secret='add__key_secret_here') to pass your s3 credentials, just add a cell at the top of your jupyter notebook, and set the standard aws s3 environment variables: import os os.environ['aws_access_key_id'] = 's3_bucket_key_here' os.environ['aws_secret_access_key'] = 's3_bucket_secret_here' # optional os.environ['aws_default_region'] = 's3_bucket_region'",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"disclaimer i'm part of the team. to set credentials you can use . to specify the s bucket as output for all artifacts you can just set it as the . for example to pass your s credentials, just add a cell at the top of your jupyter notebook, and set the standard aws s environment variables"
    },
    {
        "Question_id":null,
        "Question_title":"[errno 28] no space left on disk",
        "Question_body":"I am trying to install a python package called rapidsai in aws notebook instance but I am getting this error errno 28 no space left on disk. I am using g4dn.xlarge instance for it. The package is almost 3 GB. While opening the notebook instance I have tried increasing volume size of notebook upto 50 GB, I am still getting this error. Let me know the solution to it. Thanks",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1650457037741,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUInf7H_9bQpCPMROevLa2fA\/errno-28-no-space-left-on-disk",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: [errno 28] no space left on disk; Content: i am trying to install a python package called rapidsai in aws notebook instance but i am getting this error errno 28 no space left on disk. i am using g4dn.xlarge instance for it. the package is almost 3 gb. while opening the notebook instance i have tried increasing volume size of notebook upto 50 gb, i am still getting this error. let me know the solution to it. thanks",
        "Question_original_content_gpt_summary":"The user is encountering an \"errno 28\" error when trying to install a 3GB Python package on an AWS notebook instance, despite increasing the volume size to 50GB.",
        "Question_preprocessed_content":"Title: no space left on disk; Content: i am trying to install a python package called rapidsai in aws notebook instance but i am getting this error errno no space left on disk. i am using instance for it. the package is almost gb. while opening the notebook instance i have tried increasing volume size of notebook upto gb, i am still getting this error. let me know the solution to it. thanks",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Azure AML - Diagnostic Logs AmlModelsEvent not collected",
        "Question_body":"Hi,\nI have configured a DiagnosticLogs setting in my AML resource, but when I check within the Log Analytcs Workspace I see all logs but no AmlModelsEvent.\nI tested by accessing my AML models and modifyng them. Why I cannot see these logs?",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1655976210980,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/900632\/azure-aml-diagnostic-logs-amlmodelsevent-not-colle.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-24T13:11:20.163Z",
                "Answer_score":0,
                "Answer_body":"Hi @ramr-msft I cannot see that menu in the portal, I see only a right-side menu\n\n\nAnyway I see the logs in my log analytcs workspace and it already collects other AML diagnostic logs (like AmlDeploymentsEvents, AmlClusterEvents, ...) but no AmlModelsEvents.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure aml - diagnostic logs amlmodelsevent not collected; Content: hi, i have configured a diagnosticlogs setting in my aml resource, but when i check within the log analytcs workspace i see all logs but no amlmodelsevent. i tested by accessing my aml models and modifyng them. why i cannot see these logs?",
        "Question_original_content_gpt_summary":"The user is encountering an issue with their Azure AML resource where diagnostic logs for amlmodelsevent are not being collected.",
        "Question_preprocessed_content":"Title: azure aml diagnostic logs amlmodelsevent not collected; Content: hi, i have configured a diagnosticlogs setting in my aml resource, but when i check within the log analytcs workspace i see all logs but no amlmodelsevent. i tested by accessing my aml models and modifyng them. why i cannot see these logs?",
        "Answer_original_content":"hi @ramr-msft i cannot see that menu in the portal, i see only a right-side menu anyway i see the logs in my log analytcs workspace and it already collects other aml diagnostic logs (like amldeploymentsevents, amlclusterevents, ...) but no amlmodelsevents.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi i cannot see that menu in the portal, i see only a right side menu anyway i see the logs in my log analytcs workspace and it already collects other aml diagnostic logs but no amlmodelsevents."
    },
    {
        "Question_id":55549880.0,
        "Question_title":"Transforming a PNG image in application\/x-recordio-protobuf for calling a sagemaker endpoint in C#",
        "Question_body":"<p>I am trying to do some inferences on a SageMaker endpoint in C# using a <code>InvokeEndpointRequest<\/code> object. My inference body is a PNG or JPEG image. However, SageMaker requires an <code>application\/x-recordio-protobuf<\/code> format. How can I convert my image file into this format to be able to use InvokeEndpoint with the above object.<\/p>\n<pre><code>InvokeEndpointRequest invokeRequest = new InvokeEndpointRequest\n{\n  EndpointName = &quot;kmeans-2019-xx-xx-xx-xx-xx-xxx&quot;,\n  Body= GetImageFromFile(),\n  ContentType= &quot;application\/x-recordio-protobuf&quot;\n};\nInvokeEndpointResponse invokeResponse = smClient.InvokeEndpoint(invokeRequest);\n<\/code><\/pre>\n<p>For the moment the <code>GetImageFromFile<\/code> method just reads an image file and transforms it in <code>MemoryStream<\/code>:<\/p>\n<pre><code>Stream stream = openFileDialog.OpenFile();\n\nbyte[] data = new byte[stream.Length];\nstream.Read(data, 0, (int)stream.Length);\nMemoryStream ms=new MemoryStream(data);\n            \nreturn ms;\n<\/code><\/pre>\n<p>I tried to serialize the <code>MemoryStream<\/code> by using <code>Protobuf-net<\/code>, but it does not work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1554559637090,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":973.0,
        "Owner_creation_time":1554558813163,
        "Owner_last_access_time":1560410909096,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1604106646103,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55549880",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: transforming a png image in application\/x-recordio-protobuf for calling a endpoint in c#; Content: i am trying to do some inferences on a endpoint in c# using a invokeendpointrequest object. my inference body is a png or jpeg image. however, requires an application\/x-recordio-protobuf format. how can i convert my image file into this format to be able to use invokeendpoint with the above object. invokeendpointrequest invokerequest = new invokeendpointrequest { endpointname = \"kmeans-2019-xx-xx-xx-xx-xx-xxx\", body= getimagefromfile(), contenttype= \"application\/x-recordio-protobuf\" }; invokeendpointresponse invokeresponse = smclient.invokeendpoint(invokerequest); for the moment the getimagefromfile method just reads an image file and transforms it in memorystream: stream stream = openfiledialog.openfile(); byte[] data = new byte[stream.length]; stream.read(data, 0, (int)stream.length); memorystream ms=new memorystream(data); return ms; i tried to serialize the memorystream by using protobuf-net, but it does not work.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in transforming a PNG or JPEG image into an application\/x-recordio-protobuf format in order to call an endpoint in C#.",
        "Question_preprocessed_content":"Title: transforming a png image in for calling a endpoint in c; Content: i am trying to do some inferences on a endpoint in c using a object. my inference body is a png or jpeg image. however, requires an format. how can i convert my image file into this format to be able to use invokeendpoint with the above object. for the moment the method just reads an image file and transforms it in i tried to serialize the by using , but it does not work.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":67093041.0,
        "Question_title":"Aws Sagemaker - ModuleNotFoundError: No module named 'cv2'",
        "Question_body":"<p>I am trying to run a object detection code in Aws. Although opencv is listed in the requirement file, i have the error &quot;no module named cv2&quot;. I am not sure how to fix this error. could someone help me please.<\/p>\n<p>My requirement.txt file has<\/p>\n<ul>\n<li>opencv-python<\/li>\n<li>numpy&gt;=1.18.2<\/li>\n<li>scipy&gt;=1.4.1<\/li>\n<li>wget&gt;=3.2<\/li>\n<li>tensorflow==2.3.1<\/li>\n<li>tensorflow-gpu==2.3.1<\/li>\n<li>tqdm==4.43.0<\/li>\n<li>pandas<\/li>\n<li>boto3<\/li>\n<li>awscli<\/li>\n<li>urllib3<\/li>\n<li>mss<\/li>\n<\/ul>\n<p>I tried installing &quot;imgaug&quot; and &quot;opencv-python headless&quot; as well.. but still not able to get rid of this error.<\/p>\n<pre><code>sh-4.2$ python train_launch.py \n[INFO-ROLE] arn:aws:iam::021945294007:role\/service-role\/AmazonSageMaker-ExecutionRole-20200225T145269\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_count has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\ntrain_instance_type has been renamed in sagemaker&gt;=2.\nSee: https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html for details.\n2021-04-14 13:29:58 Starting - Starting the training job...\n2021-04-14 13:30:03 Starting - Launching requested ML instances......\n2021-04-14 13:31:11 Starting - Preparing the instances for training......\n2021-04-14 13:32:17 Downloading - Downloading input data...\n2021-04-14 13:32:41 Training - Downloading the training image..WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n2021-04-14 13:33:03,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\n2021-04-14 13:33:05,030 sagemaker-containers INFO     Invoking user script\n\nTraining Env:\n\n{\n    &quot;additional_framework_parameters&quot;: {},\n    &quot;channel_input_dirs&quot;: {\n        &quot;training&quot;: &quot;\/opt\/ml\/input\/data\/training&quot;\n    },\n    &quot;current_host&quot;: &quot;algo-1&quot;,\n    &quot;framework_module&quot;: &quot;sagemaker_tensorflow_container.training:main&quot;,\n    &quot;hosts&quot;: [\n        &quot;algo-1&quot;\n    ],\n    &quot;hyperparameters&quot;: {\n        &quot;unfreezed_epochs&quot;: 2,\n        &quot;freezed_batch_size&quot;: 8,\n        &quot;freezed_epochs&quot;: 1,\n        &quot;unfreezed_batch_size&quot;: 8,\n        &quot;model_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;\n    },\n    &quot;input_config_dir&quot;: &quot;\/opt\/ml\/input\/config&quot;,\n    &quot;input_data_config&quot;: {\n        &quot;training&quot;: {\n            &quot;TrainingInputMode&quot;: &quot;File&quot;,\n            &quot;S3DistributionType&quot;: &quot;FullyReplicated&quot;,\n            &quot;RecordWrapperType&quot;: &quot;None&quot;\n        }\n    },\n    &quot;input_dir&quot;: &quot;\/opt\/ml\/input&quot;,\n    &quot;is_master&quot;: true,\n    &quot;job_name&quot;: &quot;yolov4-2021-04-14-15-29&quot;,\n    &quot;log_level&quot;: 20,\n    &quot;master_hostname&quot;: &quot;algo-1&quot;,\n    &quot;model_dir&quot;: &quot;\/opt\/ml\/model&quot;,\n    &quot;module_dir&quot;: &quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,\n    &quot;module_name&quot;: &quot;train_indu&quot;,\n    &quot;network_interface_name&quot;: &quot;eth0&quot;,\n    &quot;num_cpus&quot;: 8,\n    &quot;num_gpus&quot;: 1,\n    &quot;output_data_dir&quot;: &quot;\/opt\/ml\/output\/data&quot;,\n    &quot;output_dir&quot;: &quot;\/opt\/ml\/output&quot;,\n    &quot;output_intermediate_dir&quot;: &quot;\/opt\/ml\/output\/intermediate&quot;,\n    &quot;resource_config&quot;: {\n        &quot;current_host&quot;: &quot;algo-1&quot;,\n        &quot;hosts&quot;: [\n            &quot;algo-1&quot;\n        ],\n        &quot;network_interface_name&quot;: &quot;eth0&quot;\n    },\n    &quot;user_entry_point&quot;: &quot;train_indu.py&quot;\n}\n\nEnvironment variables:\n\nSM_HOSTS=[&quot;algo-1&quot;]\nSM_NETWORK_INTERFACE_NAME=eth0\nSM_HPS={&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2}\nSM_USER_ENTRY_POINT=train_indu.py\nSM_FRAMEWORK_PARAMS={}\nSM_RESOURCE_CONFIG={&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;}\nSM_INPUT_DATA_CONFIG={&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}}\nSM_OUTPUT_DATA_DIR=\/opt\/ml\/output\/data\nSM_CHANNELS=[&quot;training&quot;]\nSM_CURRENT_HOST=algo-1\nSM_MODULE_NAME=train_indu\nSM_LOG_LEVEL=20\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\nSM_INPUT_DIR=\/opt\/ml\/input\nSM_INPUT_CONFIG_DIR=\/opt\/ml\/input\/config\nSM_OUTPUT_DIR=\/opt\/ml\/output\nSM_NUM_CPUS=8\nSM_NUM_GPUS=1\nSM_MODEL_DIR=\/opt\/ml\/model\nSM_MODULE_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\nSM_TRAINING_ENV={&quot;additional_framework_parameters&quot;:{},&quot;channel_input_dirs&quot;:{&quot;training&quot;:&quot;\/opt\/ml\/input\/data\/training&quot;},&quot;current_host&quot;:&quot;algo-1&quot;,&quot;framework_module&quot;:&quot;sagemaker_tensorflow_container.training:main&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;hyperparameters&quot;:{&quot;freezed_batch_size&quot;:8,&quot;freezed_epochs&quot;:1,&quot;model_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;unfreezed_batch_size&quot;:8,&quot;unfreezed_epochs&quot;:2},&quot;input_config_dir&quot;:&quot;\/opt\/ml\/input\/config&quot;,&quot;input_data_config&quot;:{&quot;training&quot;:{&quot;RecordWrapperType&quot;:&quot;None&quot;,&quot;S3DistributionType&quot;:&quot;FullyReplicated&quot;,&quot;TrainingInputMode&quot;:&quot;File&quot;}},&quot;input_dir&quot;:&quot;\/opt\/ml\/input&quot;,&quot;is_master&quot;:true,&quot;job_name&quot;:&quot;yolov4-2021-04-14-15-29&quot;,&quot;log_level&quot;:20,&quot;master_hostname&quot;:&quot;algo-1&quot;,&quot;model_dir&quot;:&quot;\/opt\/ml\/model&quot;,&quot;module_dir&quot;:&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz&quot;,&quot;module_name&quot;:&quot;train_indu&quot;,&quot;network_interface_name&quot;:&quot;eth0&quot;,&quot;num_cpus&quot;:8,&quot;num_gpus&quot;:1,&quot;output_data_dir&quot;:&quot;\/opt\/ml\/output\/data&quot;,&quot;output_dir&quot;:&quot;\/opt\/ml\/output&quot;,&quot;output_intermediate_dir&quot;:&quot;\/opt\/ml\/output\/intermediate&quot;,&quot;resource_config&quot;:{&quot;current_host&quot;:&quot;algo-1&quot;,&quot;hosts&quot;:[&quot;algo-1&quot;],&quot;network_interface_name&quot;:&quot;eth0&quot;},&quot;user_entry_point&quot;:&quot;train_indu.py&quot;}\nSM_USER_ARGS=[&quot;--freezed_batch_size&quot;,&quot;8&quot;,&quot;--freezed_epochs&quot;,&quot;1&quot;,&quot;--model_dir&quot;,&quot;s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model&quot;,&quot;--unfreezed_batch_size&quot;,&quot;8&quot;,&quot;--unfreezed_epochs&quot;,&quot;2&quot;]\nSM_OUTPUT_INTERMEDIATE_DIR=\/opt\/ml\/output\/intermediate\nSM_CHANNEL_TRAINING=\/opt\/ml\/input\/data\/training\nSM_HP_UNFREEZED_EPOCHS=2\nSM_HP_FREEZED_BATCH_SIZE=8\nSM_HP_FREEZED_EPOCHS=1\nSM_HP_UNFREEZED_BATCH_SIZE=8\nSM_HP_MODEL_DIR=s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\nPYTHONPATH=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages\n\nInvoking script with the following command:\n\n\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\n\n\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n\n[name: &quot;\/device:CPU:0&quot;\ndevice_type: &quot;CPU&quot;\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 4667030854237447206\n, name: &quot;\/device:XLA_CPU:0&quot;\ndevice_type: &quot;XLA_CPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 3059419181456814147\nphysical_device_desc: &quot;device: XLA_CPU device&quot;\n, name: &quot;\/device:XLA_GPU:0&quot;\ndevice_type: &quot;XLA_GPU&quot;\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 6024475084695919958\nphysical_device_desc: &quot;device: XLA_GPU device&quot;\n, name: &quot;\/device:GPU:0&quot;\ndevice_type: &quot;GPU&quot;\nmemory_limit: 14949928141\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 13034103301168381073\nphysical_device_desc: &quot;device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5&quot;\n]\nTraceback (most recent call last):\n  File &quot;train_indu.py&quot;, line 12, in &lt;module&gt;\n    from yolov3.dataset import Dataset\n  File &quot;\/opt\/ml\/code\/yolov3\/dataset.py&quot;, line 3, in &lt;module&gt;\n    import cv2\nModuleNotFoundError: No module named 'cv2'\n2021-04-14 13:33:08,453 sagemaker-containers ERROR    ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n\n2021-04-14 13:33:11 Uploading - Uploading generated training model\n2021-04-14 13:33:54 Failed - Training job failed\nTraceback (most recent call last):\n  File &quot;train_launch.py&quot;, line 41, in &lt;module&gt;\n    estimator.fit(s3_data_path, logs=True, job_name=job_name) #the argument logs is crucial if you want to see what happends\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 535, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py&quot;, line 1210, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 3365, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/sagemaker\/session.py&quot;, line 2957, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job yolov4-2021-04-14-15-29: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/sagemaker-dataset-ai\/Dataset\/yolo\/Results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2&quot;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1618407986697,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":1218.0,
        "Owner_creation_time":1558009697176,
        "Owner_last_access_time":1618755603608,
        "Owner_reputation":53.0,
        "Owner_up_votes":3.0,
        "Owner_down_votes":0.0,
        "Owner_views":8.0,
        "Answer_body":"<p>Make sure your estimator has<\/p>\n<ul>\n<li>framework_version = '2.3',<\/li>\n<li>py_version = 'py37',<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1618410091900,
        "Answer_score":1.0,
        "Owner_location":"Cergy-Pontoise, Cergy, France",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67093041",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - modulenotfounderror: no module named 'cv2'; Content: i am trying to run a object detection code in aws. although opencv is listed in the requirement file, i have the error \"no module named cv2\". i am not sure how to fix this error. could someone help me please. my requirement.txt file has opencv-python numpy>=1.18.2 scipy>=1.4.1 wget>=3.2 tensorflow==2.3.1 tensorflow-gpu==2.3.1 tqdm==4.43.0 pandas boto3 awscli urllib3 mss i tried installing \"imgaug\" and \"opencv-python headless\" as well.. but still not able to get rid of this error. sh-4.2$ python train_launch.py [info-role] arn:aws:iam::021945294007:role\/service-role\/amazon-executionrole-20200225t145269 train_instance_type has been renamed in >=2. see: https:\/\/.readthedocs.io\/en\/stable\/v2.html for details. train_instance_count has been renamed in >=2. see: https:\/\/.readthedocs.io\/en\/stable\/v2.html for details. train_instance_type has been renamed in >=2. see: https:\/\/.readthedocs.io\/en\/stable\/v2.html for details. 2021-04-14 13:29:58 starting - starting the training job... 2021-04-14 13:30:03 starting - launching requested ml instances...... 2021-04-14 13:31:11 starting - preparing the instances for training...... 2021-04-14 13:32:17 downloading - downloading input data... 2021-04-14 13:32:41 training - downloading the training image..warning:tensorflow:from \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: the name tf.estimator.inputs is deprecated. please use tf.compat.v1.estimator.inputs instead. 2021-04-14 13:33:03,970 -containers info imported framework _tensorflow_container.training 2021-04-14 13:33:05,030 -containers info invoking user script training env: { \"additional_framework_parameters\": {}, \"channel_input_dirs\": { \"training\": \"\/opt\/ml\/input\/data\/training\" }, \"current_host\": \"algo-1\", \"framework_module\": \"_tensorflow_container.training:main\", \"hosts\": [ \"algo-1\" ], \"hyperparameters\": { \"unfreezed_epochs\": 2, \"freezed_batch_size\": 8, \"freezed_epochs\": 1, \"unfreezed_batch_size\": 8, \"model_dir\": \"s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\" }, \"input_config_dir\": \"\/opt\/ml\/input\/config\", \"input_data_config\": { \"training\": { \"traininginputmode\": \"file\", \"s3distributiontype\": \"fullyreplicated\", \"recordwrappertype\": \"none\" } }, \"input_dir\": \"\/opt\/ml\/input\", \"is_master\": true, \"job_name\": \"yolov4-2021-04-14-15-29\", \"log_level\": 20, \"master_hostname\": \"algo-1\", \"model_dir\": \"\/opt\/ml\/model\", \"module_dir\": \"s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\", \"module_name\": \"train_indu\", \"network_interface_name\": \"eth0\", \"num_cpus\": 8, \"num_gpus\": 1, \"output_data_dir\": \"\/opt\/ml\/output\/data\", \"output_dir\": \"\/opt\/ml\/output\", \"output_intermediate_dir\": \"\/opt\/ml\/output\/intermediate\", \"resource_config\": { \"current_host\": \"algo-1\", \"hosts\": [ \"algo-1\" ], \"network_interface_name\": \"eth0\" }, \"user_entry_point\": \"train_indu.py\" } environment variables: sm_hosts=[\"algo-1\"] sm_network_interface_name=eth0 sm_hps={\"freezed_batch_size\":8,\"freezed_epochs\":1,\"model_dir\":\"s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\",\"unfreezed_batch_size\":8,\"unfreezed_epochs\":2} sm_user_entry_point=train_indu.py sm_framework_params={} sm_resource_config={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"} sm_input_data_config={\"training\":{\"recordwrappertype\":\"none\",\"s3distributiontype\":\"fullyreplicated\",\"traininginputmode\":\"file\"}} sm_output_data_dir=\/opt\/ml\/output\/data sm_channels=[\"training\"] sm_current_host=algo-1 sm_module_name=train_indu sm_log_level=20 sm_framework_module=_tensorflow_container.training:main sm_input_dir=\/opt\/ml\/input sm_input_config_dir=\/opt\/ml\/input\/config sm_output_dir=\/opt\/ml\/output sm_num_cpus=8 sm_num_gpus=1 sm_model_dir=\/opt\/ml\/model sm_module_dir=s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz sm_training_env={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"\/opt\/ml\/input\/data\/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"freezed_batch_size\":8,\"freezed_epochs\":1,\"model_dir\":\"s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\",\"unfreezed_batch_size\":8,\"unfreezed_epochs\":2},\"input_config_dir\":\"\/opt\/ml\/input\/config\",\"input_data_config\":{\"training\":{\"recordwrappertype\":\"none\",\"s3distributiontype\":\"fullyreplicated\",\"traininginputmode\":\"file\"}},\"input_dir\":\"\/opt\/ml\/input\",\"is_master\":true,\"job_name\":\"yolov4-2021-04-14-15-29\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"\/opt\/ml\/model\",\"module_dir\":\"s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_smal\/yolov4-2021-04-14-15-29\/source\/sourcedir.tar.gz\",\"module_name\":\"train_indu\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"\/opt\/ml\/output\/data\",\"output_dir\":\"\/opt\/ml\/output\",\"output_intermediate_dir\":\"\/opt\/ml\/output\/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_indu.py\"} sm_user_args=[\"--freezed_batch_size\",\"8\",\"--freezed_epochs\",\"1\",\"--model_dir\",\"s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_small\/yolov4-2021-04-14-15-29\/model\",\"--unfreezed_batch_size\",\"8\",\"--unfreezed_epochs\",\"2\"] sm_output_intermediate_dir=\/opt\/ml\/output\/intermediate sm_channel_training=\/opt\/ml\/input\/data\/training sm_hp_unfreezed_epochs=2 sm_hp_freezed_batch_size=8 sm_hp_freezed_epochs=1 sm_hp_unfreezed_batch_size=8 sm_hp_model_dir=s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_small\/yolov4-2021-04-14-15-29\/model pythonpath=\/opt\/ml\/code:\/usr\/local\/bin:\/usr\/lib\/python36.zip:\/usr\/lib\/python3.6:\/usr\/lib\/python3.6\/lib-dynload:\/usr\/local\/lib\/python3.6\/dist-packages:\/usr\/lib\/python3\/dist-packages invoking script with the following command: \/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2 warning:tensorflow:from \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow_core\/__init__.py:1473: the name tf.estimator.inputs is deprecated. please use tf.compat.v1.estimator.inputs instead. [name: \"\/device:cpu:0\" device_type: \"cpu\" memory_limit: 268435456 locality { } incarnation: 4667030854237447206 , name: \"\/device:xla_cpu:0\" device_type: \"xla_cpu\" memory_limit: 17179869184 locality { } incarnation: 3059419181456814147 physical_device_desc: \"device: xla_cpu device\" , name: \"\/device:xla_gpu:0\" device_type: \"xla_gpu\" memory_limit: 17179869184 locality { } incarnation: 6024475084695919958 physical_device_desc: \"device: xla_gpu device\" , name: \"\/device:gpu:0\" device_type: \"gpu\" memory_limit: 14949928141 locality { bus_id: 1 links { } } incarnation: 13034103301168381073 physical_device_desc: \"device: 0, name: tesla t4, pci bus id: 0000:00:1e.0, compute capability: 7.5\" ] traceback (most recent call last): file \"train_indu.py\", line 12, in <module> from yolov3.dataset import dataset file \"\/opt\/ml\/code\/yolov3\/dataset.py\", line 3, in <module> import cv2 modulenotfounderror: no module named 'cv2' 2021-04-14 13:33:08,453 -containers error executeuserscripterror: command \"\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\" 2021-04-14 13:33:11 uploading - uploading generated training model 2021-04-14 13:33:54 failed - training job failed traceback (most recent call last): file \"train_launch.py\", line 41, in <module> estimator.fit(s3_data_path, logs=true, job_name=job_name) #the argument logs is crucial if you want to see what happends file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.6\/site-packages\/\/estimator.py\", line 535, in fit self.latest_training_job.wait(logs=logs) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.6\/site-packages\/\/estimator.py\", line 1210, in wait self._session.logs_for_job(self.job_name, wait=true, log_type=logs) file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.6\/site-packages\/\/session.py\", line 3365, in logs_for_job self._check_job_status(job_name, description, \"trainingjobstatus\") file \"\/home\/ec2-user\/anaconda3\/envs\/jupytersystemenv\/lib\/python3.6\/site-packages\/\/session.py\", line 2957, in _check_job_status actual_status=status, .exceptions.unexpectedstatusexception: error for training job yolov4-2021-04-14-15-29: failed. reason: algorithmerror: executeuserscripterror: command \"\/usr\/bin\/python3 train_indu.py --freezed_batch_size 8 --freezed_epochs 1 --model_dir s3:\/\/-dataset-ai\/dataset\/yolo\/results\/yolov4_small\/yolov4-2021-04-14-15-29\/model --unfreezed_batch_size 8 --unfreezed_epochs 2\"",
        "Question_original_content_gpt_summary":"The user is encountering a ModuleNotFoundError when attempting to run an object detection code in AWS despite OpenCV being listed in the requirements file.",
        "Question_preprocessed_content":"Title: modulenotfounderror no module named 'cv '; Content: i am trying to run a object detection code in aws. although opencv is listed in the requirement file, i have the error no module named cv . i am not sure how to fix this error. could someone help me please. my file has opencv python pandas boto awscli urllib mss i tried installing imgaug and opencv python headless as but still not able to get rid of this error.",
        "Answer_original_content":"make sure your estimator has framework_version = '2.3', py_version = 'py37',",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"make sure your estimator has 'py ',"
    },
    {
        "Question_id":60832279.0,
        "Question_title":"Influx DB as a source for MLS as a direct connection",
        "Question_body":"<p>Can we use InfluxDB as a data source for Azure ML Serv, in the form of a direct connection.  If not, what are the proposed alternatives to setup this connection?\n(Put differently, Is it possible for M LServ to connect to an InfluxDB next to some API to fetch data from. Or do we have to put all data in a SQL database?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1585057386130,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":105.0,
        "Owner_creation_time":1585057281607,
        "Owner_last_access_time":1614093458423,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60832279",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: influx db as a source for mls as a direct connection; Content: can we use influxdb as a data source for serv, in the form of a direct connection. if not, what are the proposed alternatives to setup this connection? (put differently, is it possible for m lserv to connect to an influxdb next to some api to fetch data from. or do we have to put all data in a sql database?)",
        "Question_original_content_gpt_summary":"The user is looking for a way to connect MLServ to an InfluxDB in order to fetch data, or to explore alternative methods of setting up this connection.",
        "Question_preprocessed_content":"Title: influx db as a source for mls as a direct connection; Content: can we use influxdb as a data source for serv, in the form of a direct connection. if not, what are the proposed alternatives to setup this connection? put differently, is it possible for m lserv to connect to an influxdb next to some api to fetch data from. or do we have to put all data in a sql database?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"What's the next step after creating a pipeline?",
        "Question_body":"Hi, the UI is very confused, not straightforward as Studio. Can you guide me to the next step to use the pipeline?",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1661266129010,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/978610\/what39s-the-next-step-after-creating-a-pipeline.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-23T18:07:17.917Z",
                "Answer_score":0,
                "Answer_body":"Hello @Nicholes-3441\n\nThanks for using Microsft Q&A platform. I think you are on the stage of designing your pipeline and running it.\n\nThe next step should be submit your pipeline and evaluate your model - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score#submit-the-pipeline\n\nWhen you feel good with your model, you can then deploy your pipeline as this guidance - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\n\nYou may then want to test and update your endpoint as above guidance.\n\nEach time you run a pipeline, the configuration of the pipeline and its results are stored in your workspace as a pipeline job. You can go back to any pipeline job to inspect it for troubleshooting or auditing. Clone a pipeline job to create a new pipeline draft for you to edit.\n\nPipeline jobs are grouped into experiments to organize job history. You can set the experiment for every pipeline job.\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-08-23T18:16:30.973Z",
                "Answer_score":0,
                "Answer_body":"Hi @Nicholes-3441,\n\nThere are several options of how you can run the pipeline after creating it e.g.,\n\n1) you can manually run the pipeline e.g., by Clicking on the Trigger Now option, using REST API, using PowerShell command etc.\n2) you can create a new scheduled \/ tumbling window \/ storage event \/custom event trigger\n\nPlease see the links below for details.\n\nThanks!\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/data-factory\/concepts-pipeline-execution-triggers",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what's the next step after creating a pipeline?; Content: hi, the ui is very confused, not straightforward as studio. can you guide me to the next step to use the pipeline?",
        "Question_original_content_gpt_summary":"The user is confused by the UI and is seeking guidance on the next step to use the pipeline.",
        "Question_preprocessed_content":"Title: what's the next step after creating a pipeline?; Content: hi, the ui is very confused, not straightforward as studio. can you guide me to the next step to use the pipeline?",
        "Answer_original_content":"hello @nicholes-3441 thanks for using microsft q&a platform. i think you are on the stage of designing your pipeline and running it. the next step should be submit your pipeline and evaluate your model - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-train-score#submit-the-pipeline when you feel good with your model, you can then deploy your pipeline as this guidance - https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy you may then want to test and update your endpoint as above guidance. each time you run a pipeline, the configuration of the pipeline and its results are stored in your workspace as a pipeline job. you can go back to any pipeline job to inspect it for troubleshooting or auditing. clone a pipeline job to create a new pipeline draft for you to edit. pipeline jobs are grouped into experiments to organize job history. you can set the experiment for every pipeline job. i hope this helps. regards, yutong -please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello thanks for using microsft q&a platform. i think you are on the stage of designing your pipeline and running it. the next step should be submit your pipeline and evaluate your model when you feel good with your model, you can then deploy your pipeline as this guidance you may then want to test and update your endpoint as above guidance. each time you run a pipeline, the configuration of the pipeline and its results are stored in your workspace as a pipeline job. you can go back to any pipeline job to inspect it for troubleshooting or auditing. clone a pipeline job to create a new pipeline draft for you to edit. pipeline jobs are grouped into experiments to organize job history. you can set the experiment for every pipeline job. i hope this helps. regards, yutong please kindly accept the answer if you feel helpful to support the community, thanks a lot."
    },
    {
        "Question_id":69338516.0,
        "Question_title":"how to train and deploy YOLOv5 on aws sagemaker",
        "Question_body":"<p>I want to train YOLOv5 on aws sagemaker also deploy the model on sagemaker itself,need to know about entrypoint python script as well. how can I build a pipeline for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1632686467053,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":2510.0,
        "Owner_creation_time":1556022524712,
        "Owner_last_access_time":1653309846467,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":9.0,
        "Answer_body":"<p>This official AWS <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">blog post<\/a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.<\/p>\n<p>If not, there is a 3rd party implementation of YOLOv5 <a href=\"https:\/\/github.com\/HKT-SSA\/yolov5-on-sagemaker\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1632853607047,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1632754422356,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69338516",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to train and deploy yolov5 on ; Content: i want to train yolov5 on also deploy the model on itself,need to know about entrypoint python script as well. how can i build a pipeline for this?",
        "Question_original_content_gpt_summary":"The user wants to train and deploy YOLOv5 on a platform and build a pipeline for it.",
        "Question_preprocessed_content":"Title: how to train and deploy yolov on; Content: i want to train yolov on also deploy the model on itself,need to know about entrypoint python script as well. how can i build a pipeline for this?",
        "Answer_original_content":"this official aws blog post has information on how to deploy yolov4. i wonder if you can use it as a guide and change the model to v5. if not, there is a 3rd party implementation of yolov5 here.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"this official aws blog post has on how to deploy yolov . i wonder if you can use it as a guide and change the model to v . if not, there is a rd party implementation of yolov here."
    },
    {
        "Question_id":null,
        "Question_title":"EDITOR with VS Code not working on Windows",
        "Question_body":"<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"146\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"> guildai:<\/div>\n<blockquote>\n<p>Guild uses the editor defined in <code>VISUAL<\/code> or <code>EDITOR<\/code> environment variables.<\/p>\n<\/blockquote>\n<\/aside>\n<p>I had set the EDITOR variable to code in Windows 10. This is opening up a file in VS Code. But the file is immediately deleted and guild is auto-confirming the default values, So I am not able to change the values in the text editor.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1613457478077,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":284.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/editor-with-vs-code-not-working-on-windows\/545",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-16T14:54:15.199Z",
                "Answer_body":"<p>This sounds like a bug. Would you mind <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\">opening an issue on GitHub<\/a> for this? Just paste your message in from here. Thanks a lot!<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-18T14:14:49.352Z",
                "Answer_body":"<p>(copied from GitHub issue update)<\/p>\n<p>Okay - I take it back. I would not call this a bug. What you\u2019re seeing I believe is expected behavior.<\/p>\n<p>Define <code>EDITOR<\/code> using the <code>code<\/code> command with the <code>-w<\/code> option (wait). This causes VS Code to block until you close the file. This is a requirement for the Guild interface to that file.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: editor with vs code not working on windows; Content: guildai: guild uses the editor defined in visual or editor environment variables. i had set the editor variable to code in windows 10. this is opening up a file in vs code. but the file is immediately deleted and guild is auto-confirming the default values, so i am not able to change the values in the text editor.",
        "Question_original_content_gpt_summary":"The user is encountering an issue where their Visual Studio Code editor is not working properly on Windows 10, resulting in files being deleted and Guild auto-confirming default values.",
        "Question_preprocessed_content":"Title: editor with vs code not working on windows; Content: guildai guild uses the editor defined in or environment variables. i had set the editor variable to code in windows . this is opening up a file in vs code. but the file is immediately deleted and guild is auto confirming the default values, so i am not able to change the values in the text editor.",
        "Answer_original_content":"this sounds like a bug. would you mind opening an issue on github for this? just paste your message in from here. thanks a lot! (copied from github issue update) okay - i take it back. i would not call this a bug. what youre seeing i believe is expected behavior. define editor using the code command with the -w option (wait). this causes vs code to block until you close the file. this is a requirement for the guild interface to that file.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"this sounds like a bug. would you mind opening an issue on github for this? just paste your message in from here. thanks a lot! copied from github issue update okay i take it back. i would not call this a bug. what youre seeing i believe is expected behavior. define using the command with the option . this causes vs code to block until you close the file. this is a requirement for the guild interface to that file."
    },
    {
        "Question_id":null,
        "Question_title":"AzureML Scoring Script fails with ImportError: no known parent package",
        "Question_body":"On trying to deploy a Model as a Container, endpoint gets created, however, scoring script fails with an error:\n\nImportError: attempted relative import with no known parent package\n\n\n\n\n\nThis is because i'm referencing another module (packaged in the docker image using source_directory) with a relative path from scoring file.\n\nCan you help me in resolving this error?\n\nFiles\\modules structure (a simplified version):\n\nproject\n->src\n-> scoring.py\n-> module1.py\n-> common\n-> module2.py, etc\n-> init.py\n-> init.py\n-> configs\n-> conda_env.yml\n\nIn scoring.py,\nfrom .module1.py import SomeClass\n..\n..\n\nIn module1.py,\nfrom .common.module2.py importSC2\n...\n..\n\n\n\n\n\nAnd below is how an Inference config is initialized:\ninference_config = InferenceConfig(source_directory=\".\/\",\nruntime= \"python\",\nentry_script=\"src\/scoring.py\",\nconda_file=\"configs\/conda_env.yml\"\n)\n\n\n\n\n\nI could not pass entry_script as \"src.scoring\" as this fails the Validation and relative path to scoring file is expected",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1603969996160,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/144492\/azureml-scoring-script-fails-with-importerror-no-k.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-29T16:20:54.573Z",
                "Answer_score":0,
                "Answer_body":"@SunilSinghal-3380 Thanks for the question. When deploying your inference script, beyond the entry script (score.py), inferenceConfig also let you specify source directory that include the entry script as well as all other python code (packages as a subfolder in the source directory that has its own init.py, or plain python script files modules). The score.py script can directly import from them because the whole folder including score.py and all other folders will be available at the inference running environment. There is no need to save them as a \"model\".\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\n\nFull sample available at https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-02T10:44:15.697Z",
                "Answer_score":0,
                "Answer_body":"@SunilSinghal-3380 Thanks for the details. When you specify a source directory and a path(relative to the source_directory) to entry script, and if your deployment is failing, most likely the issue is with how you entry_script references other files in the source_directory. register-model-deploy-local-advanced.ipynb has an example of how to specify source directory and perform a local deployment for faster troubleshooting.\n\nHow to deploy using environments can be found here model-register-and-deploy.ipynb . InferenceConfig class accepts source_directory and entry_script parameters, where source_directory is a path to the folder that contains all files(score.py and any other additional files) to create the image.\nThis multi-model-register-and-deploy.ipynb has code snippets on how to create InferenceConfig with source_directory and entry_script.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-12T17:44:52.173Z",
                "Answer_score":0,
                "Answer_body":"Hi, I am having the same issue. Could you please direct me to the right resource from where I can get some insight to solve my issue. I have a trained model file which also includes some supported py module files. The tree is shown below:\n\nmodel --> conf --> hmcn.json\n--> data --> (some other required json files)\n--> dataset --> init.py\n--> classification_dataset.py\n--> collator.py\n--> data_preprocessor.py\n--> dataset.py\n--> model --> (some other .py module files)\n--> config.py\n--> HMCN\n--> util.py\n\nIn the above tree HMCN is the model file and as shown there are some python module files which are imported in the score.py (not included in this tree) script for inferencing. I was using the AZURE Web UI and registered the model by selecting the \"model\" directory so that in the artifacts tab the root item is shown as \"model\" and inside the model there are all the files shown in the tree are uploaded.\n\nNow I am trying to create an endpoint and deploy the model by using a \"score.py\" script for real-time inferencing. I did it with the WebUI by going to Endpoints --> Create deployment and in the Environment I selected score.py and choose PyTorch 1.9 curated environment but the during deployment the process fails and gives error that the module not found in the \"score.py\". The module that it is referring is in the \"config.py\" as shown in the model tree. I believe I need to set the path of the model tree so that the \"score.py\" can find the module. But I do now know how.\n\nYour help would be greatly appreciated. Thanks",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: scoring script fails with importerror: no known parent package; Content: on trying to deploy a model as a container, endpoint gets created, however, scoring script fails with an error: importerror: attempted relative import with no known parent package this is because i'm referencing another module (packaged in the docker image using source_directory) with a relative path from scoring file. can you help me in resolving this error? files\\modules structure (a simplified version): project ->src -> scoring.py -> module1.py -> common -> module2.py, etc -> init.py -> init.py -> configs -> conda_env.yml in scoring.py, from .module1.py import someclass .. .. in module1.py, from .common.module2.py importsc2 ... .. and below is how an inference config is initialized: inference_config = inferenceconfig(source_directory=\".\/\", runtime= \"python\", entry_script=\"src\/scoring.py\", conda_file=\"configs\/conda_env.yml\" ) i could not pass entry_script as \"src.scoring\" as this fails the validation and relative path to scoring file is expected",
        "Question_original_content_gpt_summary":"The user is encountering an issue with their scoring script failing with an importerror when attempting to deploy a model as a container, due to referencing another module with a relative path from the scoring file.",
        "Question_preprocessed_content":"Title: scoring script fails with importerror no known parent package; Content: on trying to deploy a model as a container, endpoint gets created, however, scoring script fails with an error importerror attempted relative import with no known parent package this is because i'm referencing another module with a relative path from scoring file. can you help me in resolving this error? structure project >src > > > common > etc > > > configs > in from import someclass .. .. in from importsc .. and below is how an inference config is initialized runtime python , i could not pass as as this fails the validation and relative path to scoring file is expected",
        "Answer_original_content":"@sunilsinghal-3380 thanks for the question. when deploying your inference script, beyond the entry script (score.py), inferenceconfig also let you specify source directory that include the entry script as well as all other python code (packages as a subfolder in the source directory that has its own init.py, or plain python script files modules). the score.py script can directly import from them because the whole folder including score.py and all other folders will be available at the inference running environment. there is no need to save them as a \"model\". https:\/\/docs.microsoft.com\/en-us\/python\/api\/-core\/.core.model.inferenceconfig?view=azure-ml-py full sample available at https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/master\/how-to-use-\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb @sunilsinghal-3380 thanks for the details. when you specify a source directory and a path(relative to the source_directory) to entry script, and if your deployment is failing, most likely the issue is with how you entry_script references other files in the source_directory. register-model-deploy-local-advanced.ipynb has an example of how to specify source directory and perform a local deployment for faster troubleshooting. how to deploy using environments can be found here model-register-and-deploy.ipynb . inferenceconfig class accepts source_directory and entry_script parameters, where source_directory is a path to the folder that contains all files(score.py and any other additional files) to create the image. this multi-model-register-and-deploy.ipynb has code snippets on how to create inferenceconfig with source_directory and entry_script. hi, i am having the same issue. could you please direct me to the right resource from where i can get some insight to solve my issue. i have a trained model file which also includes some supported py module files. the tree is shown below: model --> conf --> hmcn.json --> data --> (some other required json files) --> dataset --> init.py --> classification_dataset.py --> collator.py --> data_preprocessor.py --> dataset.py --> model --> (some other .py module files) --> config.py --> hmcn --> util.py in the above tree hmcn is the model file and as shown there are some python module files which are imported in the score.py (not included in this tree) script for inferencing. i was using the azure web ui and registered the model by selecting the \"model\" directory so that in the artifacts tab the root item is shown as \"model\" and inside the model there are all the files shown in the tree are uploaded. now i am trying to create an endpoint and deploy the model by using a \"score.py\" script for real-time inferencing. i did it with the webui by going to endpoints --> create deployment and in the environment i selected score.py and choose pytorch 1.9 curated environment but the during deployment the process fails and gives error that the module not found in the \"score.py\". the module that it is referring is in the \"config.py\" as shown in the model tree. i believe i need to set the path of the model tree so that the \"score.py\" can find the module. but i do now know how. your help would be greatly appreciated. thanks",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the question. when deploying your inference script, beyond the entry script , inferenceconfig also let you specify source directory that include the entry script as well as all other python code . the script can directly import from them because the whole folder including and all other folders will be available at the inference running environment. there is no need to save them as a model . full sample available at thanks for the details. when you specify a source directory and a path to entry script, and if your deployment is failing, most likely the issue is with how you references other files in the has an example of how to specify source directory and perform a local deployment for faster troubleshooting. how to deploy using environments can be found here . inferenceconfig class accepts and parameters, where is a path to the folder that contains all and any other additional files to create the image. this has code snippets on how to create inferenceconfig with and hi, i am having the same issue. could you please direct me to the right resource from where i can get some insight to solve my issue. i have a trained model file which also includes some supported py module files. the tree is shown below model > conf > > data > > dataset > > > > > > model > > > hmcn > in the above tree hmcn is the model file and as shown there are some python module files which are imported in the script for inferencing. i was using the azure web ui and registered the model by selecting the model directory so that in the artifacts tab the root item is shown as model and inside the model there are all the files shown in the tree are uploaded. now i am trying to create an endpoint and deploy the model by using a script for real time inferencing. i did it with the webui by going to endpoints > create deployment and in the environment i selected and choose pytorch curated environment but the during deployment the process fails and gives error that the module not found in the the module that it is referring is in the as shown in the model tree. i believe i need to set the path of the model tree so that the can find the module. but i do now know how. your help would be greatly appreciated. thanks"
    },
    {
        "Question_id":null,
        "Question_title":"Does a completed training job incurr charges?",
        "Question_body":"Hello.\n\nAs you can see in the screenshot below, there are couple of completed jobs in sagemaker. (Similar in the processing jobs and training jobs menu.)\n\nIf I select one and click the actions button, the Stop menu is inactivated, as you can see.\n\nDoes this kind of completed job incurr charges?\n\nIf so, what do I have to do to Delete or Stop this job?\n\nThank you.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1659691485459,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":46.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUvqrWFrV7SEG9L-GcLmU0ag\/does-a-completed-training-job-incurr-charges",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-05T09:57:31.251Z",
                "Answer_score":0,
                "Answer_body":"Hi there. No, completed jobs don't incur charges. They only incur charges for the time they were running.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: does a completed training job incurr charges?; Content: hello. as you can see in the screenshot below, there are couple of completed jobs in . (similar in the processing jobs and training jobs menu.) if i select one and click the actions button, the stop menu is inactivated, as you can see. does this kind of completed job incurr charges? if so, what do i have to do to delete or stop this job? thank you.",
        "Question_original_content_gpt_summary":"The user is wondering if a completed training job incurs charges and what they need to do to delete or stop the job.",
        "Question_preprocessed_content":"Title: does a completed training job incurr charges?; Content: hello. as you can see in the screenshot below, there are couple of completed jobs in . if i select one and click the actions button, the stop menu is inactivated, as you can see. does this kind of completed job incurr charges? if so, what do i have to do to delete or stop this job? thank you.",
        "Answer_original_content":"hi there. no, completed jobs don't incur charges. they only incur charges for the time they were running.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi there. no, completed jobs don't incur charges. they only incur charges for the time they were running."
    },
    {
        "Question_id":null,
        "Question_title":"Batchscoring on Batch Endpoint of AutoML model giving error - UserError: {\"NonCompliant\":\"Process",
        "Question_body":"AutoML time-series model created from Sql Database data. - successful\nRegistered Model. - successful\nEndPoints creation - Deploy to Batch End Point. - successful\nCreating Batchscoring Jon in EndPoint - Fails with below error.\n\nError -\n{\"NonCompliant\":\"Process '\/azureml-envs\/azureml_c9e8754bdba5226a1ab803f256ee343b\/bin\/python' exited with code 1 and error message 'Execution failed. Process exited with status code 1. Error: Traceback (most recent call last):\\n File \\\"driver\/amlbi_main.py\\\", line 184, in <module>\\n main()\\n File \\\"driver\/amlbi_main.py\\\", line 126, in main\\n boot(driver_dir)\\n File \\\"driver\/amlbi_main.py\\\", line 58, in boot\\n booter.start()\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot.py\\\", line 383, in start\\n self.start_sys_main()\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot.py\\\", line 269, in start_sys_main\\n self.run_sys_main(cmd)\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot_node.py\\\", line 111, in run_sys_main\\n self.check_run_result(proc=proc, stdout=stdout or \\\"\\\", stderr=stderr or \\\"\\\")\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot.py\\\", line 218, in check_run_result\\n BootResult().check_result(stdout)\\n File \\\"\/mnt\/azureml\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/azureml_user\/parallel_run\/boot_result.py\\\", line 36, in check_result\\n raise Exception(message) from cause\\nException: Run failed, please check logs for details. You can check logs\/readme.txt for the layout of logs.\\n\\n'. Please check the log file 'user_logs\/std_log_0.txt' for more details.\"}\n{\n\"code\": \"ExecutionFailed\",\n\"target\": \"\",\n\"category\": \"UserError\",\n\"error_details\": [\n{\n\"key\": \"exit_codes\",\n\"value\": \"1\"\n}\n]\n}",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1669318257423,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1103764\/batchscoring-on-batch-endpoint-of-automl-model-giv.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":18.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: batchscoring on batch endpoint of automl model giving error - usererror: {\"noncompliant\":\"process; Content: automl time-series model created from sql database data. - successful registered model. - successful endpoints creation - deploy to batch end point. - successful creating batchscoring jon in endpoint - fails with below error. error - {\"noncompliant\":\"process '\/-envs\/_c9e8754bdba5226a1ab803f256ee343b\/bin\/python' exited with code 1 and error message 'execution failed. process exited with status code 1. error: traceback (most recent call last):\\n file \\\"driver\/amlbi_main.py\\\", line 184, in \\n main()\\n file \\\"driver\/amlbi_main.py\\\", line 126, in main\\n boot(driver_dir)\\n file \\\"driver\/amlbi_main.py\\\", line 58, in boot\\n booter.start()\\n file \\\"\/mnt\/\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/_user\/parallel_run\/boot.py\\\", line 383, in start\\n self.start_sys_main()\\n file \\\"\/mnt\/\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/_user\/parallel_run\/boot.py\\\", line 269, in start_sys_main\\n self.run_sys_main(cmd)\\n file \\\"\/mnt\/\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/_user\/parallel_run\/boot_node.py\\\", line 111, in run_sys_main\\n self.check_run_result(proc=proc, stdout=stdout or \\\"\\\", stderr=stderr or \\\"\\\")\\n file \\\"\/mnt\/\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/_user\/parallel_run\/boot.py\\\", line 218, in check_run_result\\n bootresult().check_result(stdout)\\n file \\\"\/mnt\/\/cr\/j\/5abd1dee5bc441ae8d26b873695fdcf8\/exe\/wd\/driver\/_user\/parallel_run\/boot_result.py\\\", line 36, in check_result\\n raise exception(message) from cause\\nexception: run failed, please check logs for details. you can check logs\/readme.txt for the layout of logs.\\n\\n'. please check the log file 'user_logs\/std_log_0.txt' for more details.\"} { \"code\": \"executionfailed\", \"target\": \"\", \"category\": \"usererror\", \"error_details\": [ { \"key\": \"exit_codes\", \"value\": \"1\" } ] }",
        "Question_original_content_gpt_summary":"The user encountered challenges with batchscoring on the batch endpoint of an Automl model created from SQL database data, resulting in an error message.",
        "Question_preprocessed_content":"Title: batchscoring on batch endpoint of automl model giving error usererror noncompliant process; Content: automl time series model created from sql database data. successful registered model. successful endpoints creation deploy to batch end point. successful creating batchscoring jon in endpoint fails with below error. error noncompliant process exited with code and error message 'execution failed. process exited with status code . error traceback file line , in file line , in file line , in file line , in file line , in file line , in stdout stdout or stderr stderr or file line , in file line , in raise exception from run failed, please check logs for details. you can check for the layout of please check the log file for more code executionfailed , target , category usererror ,",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70888883.0,
        "Question_title":"Sagemaker batch transform job failure for 'batchStrategy: MultiRecord' along with data processing",
        "Question_body":"<p>We are using SageMaker Batch Transform job and to fit as many records in a mini-batch as can fit within the <code>MaxPayloadInMB<\/code> limit, we are setting <code>BatchStrategy<\/code> to <code>MultiRecord<\/code> and <code>SplitType<\/code> to <code>Line<\/code>.<\/p>\n<p>Input to the SageMaker batch transform job is:<\/p>\n<pre><code>{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 90, &quot;Experience&quot;: 26, &quot;Income&quot;: 30, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;requestBody&quot;: {&quot;data&quot;: {&quot;Age&quot;: 55, &quot;Experience&quot;: 26, &quot;Income&quot;: 450, &quot;Family&quot;: 3, &quot;CCAvg&quot;: 1}}, &quot;mName&quot;: &quot;loanprediction&quot;, &quot;mVersion&quot;: &quot;1&quot;, &quot;testFlag&quot;: &quot;false&quot;, &quot;environment&quot;: &quot;DEV&quot;, &quot;transactionId&quot;: &quot;5-69e22778-594916685f4ceca66c08bfbc&quot;, &quot;timestamp&quot;: &quot;2022-01-15T01:46:32.386Z&quot;}\n<\/code><\/pre>\n<p>This is the SageMaker batch transform job config:<\/p>\n<pre><code>apiVersion: sagemaker.aws.amazon.com\/v1\nkind: BatchTransformJob\nmetadata:\n        generateName: '...-batchtransform'\nspec:\n        batchStrategy: MultiRecord\n        dataProcessing:\n                JoinSource: Input\n                OutputFilter: $\n                inputFilter: $.requestBody\n        modelClientConfig:\n                invocationsMaxRetries: 0\n                invocationsTimeoutInSeconds: 3\n        mName: '..'\n        region: us-west-2\n        transformInput:\n                contentType: application\/json\n                dataSource:\n                        s3DataSource:\n                                s3DataType: S3Prefix\n                                s3Uri: s3:\/\/......\/part-\n                splitType: Line\n        transformOutput:\n                accept: application\/json\n                assembleWith: Line\n                kmsKeyId: '....'\n                s3OutputPath: s3:\/\/....\/batch_output\n        transformResources:\n                instanceCount: ..\n                instanceType: '..'\n<\/code><\/pre>\n<p>The SageMaker batch transform job fails with:<\/p>\n<p>Error in batch transform data-log -<\/p>\n<blockquote>\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n\n<p>2022-01-27T00:55:39.781:[sagemaker logs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:<\/p>\n400 Bad Request 2022-01-27T00:55:39.781:[sagemaker\nlogs]:\nephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt:\n<p>Failed to decode JSON object: Extra data: line 2 column 1 (char\n163)<\/p>\n<\/blockquote>\n<p><strong>Observation:<\/strong>\nThis issue occurs when we provide <code>batchStrategy: MultiRecord<\/code> in the manifest along with these data processing configs:<\/p>\n<pre><code>dataProcessing:\n        JoinSource: Input\n        OutputFilter: $\n        inputFilter: $.requestBody\n<\/code><\/pre>\n<p><strong>NOTE:<\/strong> If we put <code>batchStrategy: SingleRecord<\/code> along with the aforementioned data processing configs, it just works fine (job succeeds)!<\/p>\n<p><strong>Question:<\/strong> How can we achieve successful run with <code>batchStrategy: MultiRecord<\/code> along with the aforementioned data processing config?<\/p>\n<p>A successful output with <code>batchStrategy: SingleRecord<\/code> looks like this:<\/p>\n<blockquote>\n<p>{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-687sdf87-0bc7e3cb3454dbf261ed1353&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:90,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:30}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:45:32.955Z&quot;}\n{&quot;SageMakerOutput&quot;:{&quot;prediction&quot;:0},&quot;environment&quot;:&quot;DEV&quot;,&quot;transactionId&quot;:&quot;5-69e22778-594916685f4ceca66c08bfbc&quot;,&quot;mName&quot;:&quot;loanprediction&quot;,&quot;mVersion&quot;:&quot;1&quot;,&quot;requestBody&quot;:{&quot;data&quot;:{&quot;Age&quot;:55,&quot;CCAvg&quot;:1,&quot;Experience&quot;:26,&quot;Family&quot;:3,&quot;Income&quot;:450}},&quot;testFlag&quot;:&quot;false&quot;,&quot;timestamp&quot;:&quot;2022-01-15T01:46:32.386Z&quot;}\nRegion name \u2013 optional: Relevant resource ARN \u2013 optional:\narn:aws:sagemaker:us-west-2:435945521637:transform-job\/my-pipeline-9v28r-bat-e548fbfb125946528957e0f123456789<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":3.0,
        "Question_creation_time":1643344870580,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":659.0,
        "Owner_creation_time":1316155655123,
        "Owner_last_access_time":1663987460072,
        "Owner_reputation":8141.0,
        "Owner_up_votes":342.0,
        "Owner_down_votes":4.0,
        "Owner_views":1066.0,
        "Answer_body":"<p>When your input data is in JSON line format and you choose a SingleRecord BatchStrategy, your container will receive a single JSON payload body like below<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>However, if you use MultiRecord, Batch transform will split your JSON line input (which might contain 100 lines for example) into multiple records (say 10 records) all sent at once to your container as shown below:<\/p>\n<pre><code>{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n{ &lt;some JSON data&gt; }\n.\n.\n.\n{ &lt;some JSON data&gt; }\n<\/code><\/pre>\n<p>Therefore your container should be able to handle such input for it to work. However, from the error message, I can see it is complaining about invalid JSON format as it reads the second row of the request.<\/p>\n<p>I also noticed that you have supplied <code>ContentType<\/code> and <code>AcceptType<\/code> as <code>application\/json<\/code> but instead should be <code>application\/jsonlines<\/code><\/p>\n<p>Could you please test your container to see if it can handle multiple JSON line records per single invocation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1643938529208,
        "Answer_score":1.0,
        "Owner_location":"Cupertino, CA, USA",
        "Question_last_edit_time":1643607224392,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70888883",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: batch transform job failure for 'batchstrategy: multirecord' along with data processing; Content: we are using batch transform job and to fit as many records in a mini-batch as can fit within the maxpayloadinmb limit, we are setting batchstrategy to multirecord and splittype to line. input to the batch transform job is: {\"requestbody\": {\"data\": {\"age\": 90, \"experience\": 26, \"income\": 30, \"family\": 3, \"ccavg\": 1}}, \"mname\": \"loanprediction\", \"mversion\": \"1\", \"testflag\": \"false\", \"environment\": \"dev\", \"transactionid\": \"5-687sdf87-0bc7e3cb3454dbf261ed1353\", \"timestamp\": \"2022-01-15t01:45:32.955z\"} {\"requestbody\": {\"data\": {\"age\": 55, \"experience\": 26, \"income\": 450, \"family\": 3, \"ccavg\": 1}}, \"mname\": \"loanprediction\", \"mversion\": \"1\", \"testflag\": \"false\", \"environment\": \"dev\", \"transactionid\": \"5-69e22778-594916685f4ceca66c08bfbc\", \"timestamp\": \"2022-01-15t01:46:32.386z\"} this is the batch transform job config: apiversion: .aws.amazon.com\/v1 kind: batchtransformjob metadata: generatename: '...-batchtransform' spec: batchstrategy: multirecord dataprocessing: joinsource: input outputfilter: $ inputfilter: $.requestbody modelclientconfig: invocationsmaxretries: 0 invocationstimeoutinseconds: 3 mname: '..' region: us-west-2 transforminput: contenttype: application\/json datasource: s3datasource: s3datatype: s3prefix s3uri: s3:\/\/......\/part- splittype: line transformoutput: accept: application\/json assemblewith: line kmskeyid: '....' s3outputpath: s3:\/\/....\/batch_output transformresources: instancecount: .. instancetype: '..' the batch transform job fails with: error in batch transform data-log - 2022-01-27t00:55:39.781:[ logs]: ephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt: 2022-01-27t00:55:39.781:[ logs]: ephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt: 400 bad request 2022-01-27t00:55:39.781:[ logs]: ephemeral-dev-435945521637\/loanprediction-usw2-dev\/my-loanprediction\/1\/my-pipeline-9v28r\/part-00001-99fb4b99-e8e7-4945-ac44-b6c5a95a2ffe-c000.txt: failed to decode json object: extra data: line 2 column 1 (char 163) observation: this issue occurs when we provide batchstrategy: multirecord in the manifest along with these data processing configs: dataprocessing: joinsource: input outputfilter: $ inputfilter: $.requestbody note: if we put batchstrategy: singlerecord along with the aforementioned data processing configs, it just works fine (job succeeds)! question: how can we achieve successful run with batchstrategy: multirecord along with the aforementioned data processing config? a successful output with batchstrategy: singlerecord looks like this: {\"output\":{\"prediction\":0},\"environment\":\"dev\",\"transactionid\":\"5-687sdf87-0bc7e3cb3454dbf261ed1353\",\"mname\":\"loanprediction\",\"mversion\":\"1\",\"requestbody\":{\"data\":{\"age\":90,\"ccavg\":1,\"experience\":26,\"family\":3,\"income\":30}},\"testflag\":\"false\",\"timestamp\":\"2022-01-15t01:45:32.955z\"} {\"output\":{\"prediction\":0},\"environment\":\"dev\",\"transactionid\":\"5-69e22778-594916685f4ceca66c08bfbc\",\"mname\":\"loanprediction\",\"mversion\":\"1\",\"requestbody\":{\"data\":{\"age\":55,\"ccavg\":1,\"experience\":26,\"family\":3,\"income\":450}},\"testflag\":\"false\",\"timestamp\":\"2022-01-15t01:46:32.386z\"} region name \u2013 optional: relevant resource arn \u2013 optional: arn:aws::us-west-2:435945521637:transform-job\/my-pipeline-9v28r-bat-e548fbfb125946528957e0f123456789",
        "Question_original_content_gpt_summary":"The user is encountering challenges with a batch transform job failing due to an error in the data-log when using 'batchstrategy: multirecord' along with data processing configs.",
        "Question_preprocessed_content":"Title: batch transform job failure for 'batchstrategy multirecord' along with data processing; Content: we are using batch transform job and to fit as many records in a mini batch as can fit within the limit, we are setting to and to . input to the batch transform job is this is the batch transform job config the batch transform job fails with error in batch transform data log logs logs bad request logs failed to decode json object extra data line column observation this issue occurs when we provide in the manifest along with these data processing configs note if we put along with the aforementioned data processing configs, it just works fine ! question how can we achieve successful run with along with the aforementioned data processing config? a successful output with looks like this region name optional relevant resource arn optional",
        "Answer_original_content":"when your input data is in json line format and you choose a singlerecord batchstrategy, your container will receive a single json payload body like below { <some json data> } however, if you use multirecord, batch transform will split your json line input (which might contain 100 lines for example) into multiple records (say 10 records) all sent at once to your container as shown below: { <some json data> } { <some json data> } { <some json data> } { <some json data> } . . . { <some json data> } therefore your container should be able to handle such input for it to work. however, from the error message, i can see it is complaining about invalid json format as it reads the second row of the request. i also noticed that you have supplied contenttype and accepttype as application\/json but instead should be application\/jsonlines could you please test your container to see if it can handle multiple json line records per single invocation.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"when your input data is in json line format and you choose a singlerecord batchstrategy, your container will receive a single json payload body like below however, if you use multirecord, batch transform will split your json line input into multiple records all sent at once to your container as shown below therefore your container should be able to handle such input for it to work. however, from the error message, i can see it is complaining about invalid json format as it reads the second row of the request. i also noticed that you have supplied and as but instead should be could you please test your container to see if it can handle multiple json line records per single invocation."
    },
    {
        "Question_id":67988138.0,
        "Question_title":"Azure ML Tutorial - Failed to load entrypoint automl",
        "Question_body":"<p>I'm doing following tutorial. I failed to run &quot;Create a control script&quot;.<\/p>\n<p>What could be wrong?<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world<\/a><\/p>\n<pre><code>azureuser@kensmlcompute:~\/cloudfiles\/code\/Users\/my.name\/get-started$ python run-hello.py \nFailure while loading azureml_run_type_providers. Failed to load entrypoint automl = \nazureml.train.automl.run:AutoMLRun._from_run_dto with exception (pyarrow 4.0.0 \n(\/anaconda\/envs\/azureml_py38\/lib\/python3.8\/site-packages), \nRequirement.parse('pyarrow&lt;4.0.0,&gt;=0.17.0'), {'azureml-dataset-runtime'}).\nhttps:\/\/ml.azure.com\/runs\/day1-experiment-hello_1623766747_073126f5? \nwsid=\/subscriptions\/1679753a-501e-4e46-9bff- \n6120ed5694cf\/resourcegroups\/kensazuremlrg\/workspaces\/kensazuremlws&amp;tid=94fe1041-ba47-4f49- \n866b- \n06c297c116cc\nazureuser@kensmlcompute:~\/cloudfiles\/code\/Users\/my.name\/get-started$\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1623766974453,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":1241.0,
        "Owner_creation_time":1478251050692,
        "Owner_last_access_time":1663835968943,
        "Owner_reputation":1519.0,
        "Owner_up_votes":116.0,
        "Owner_down_votes":0.0,
        "Owner_views":375.0,
        "Answer_body":"<p>I think the error indicates that your environment is using pyarrow package which is of version 4.0.0 whereas azureml-dataset-runtime requires the package to be &gt;=0.17.0 but &lt;4.0.0<\/p>\n<p>It would be easier for you to uninstall the package and install a specific version. The list of releases of pyarrow are available here.<\/p>\n<p>Since you are using a notebook create new cells and run these commands.<\/p>\n<pre><code> !pip uninstall pyarrow\n !pip install -y pyarrow==3.0.0\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1623861331176,
        "Answer_score":1.0,
        "Owner_location":"Finland",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67988138",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tutorial - failed to load entrypoint automl; Content: i'm doing following tutorial. i failed to run \"create a control script\". what could be wrong? https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world azureuser@kensmlcompute:~\/cloudfiles\/code\/users\/my.name\/get-started$ python run-hello.py failure while loading _run_type_providers. failed to load entrypoint automl = .train.automl.run:automlrun._from_run_dto with exception (pyarrow 4.0.0 (\/anaconda\/envs\/_py38\/lib\/python3.8\/site-packages), requirement.parse('pyarrow<4.0.0,>=0.17.0'), {'-dataset-runtime'}). https:\/\/ml.azure.com\/runs\/day1-experiment-hello_1623766747_073126f5? wsid=\/subscriptions\/1679753a-501e-4e46-9bff- 6120ed5694cf\/resourcegroups\/kensrg\/workspaces\/kensws&tid=94fe1041-ba47-4f49- 866b- 06c297c116cc azureuser@kensmlcompute:~\/cloudfiles\/code\/users\/my.name\/get-started$",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with loading the entrypoint automl while following a tutorial on Azure Machine Learning.",
        "Question_preprocessed_content":"Title: tutorial failed to load entrypoint automl; Content: i'm doing following tutorial. i failed to run create a control script . what could be wrong?",
        "Answer_original_content":"i think the error indicates that your environment is using pyarrow package which is of version 4.0.0 whereas -dataset-runtime requires the package to be >=0.17.0 but <4.0.0 it would be easier for you to uninstall the package and install a specific version. the list of releases of pyarrow are available here. since you are using a notebook create new cells and run these commands. !pip uninstall pyarrow !pip install -y pyarrow==3.0.0",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i think the error indicates that your environment is using pyarrow package which is of version whereas dataset runtime requires the package to be but it would be easier for you to uninstall the package and install a specific version. the list of releases of pyarrow are available here. since you are using a notebook create new cells and run these commands."
    },
    {
        "Question_id":63606182.0,
        "Question_title":"How should Trains be used with hyper-param optimization tools like RayTune?",
        "Question_body":"<p>What could be a reasonable setup for this? Can I call Task.init() multiple times in the same execution?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1598478428030,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":107.0,
        "Owner_creation_time":1311330349880,
        "Owner_last_access_time":1663773102327,
        "Owner_reputation":3784.0,
        "Owner_up_votes":472.0,
        "Owner_down_votes":0.0,
        "Owner_views":342.0,
        "Answer_body":"<p>Disclaimer: I'm part of the allegro.ai Trains team<\/p>\n<p>One solution is to inherit from <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/838c9cb0d2a5df5c193dfc85286abe59a80217c2\/trains\/automation\/optimization.py#L226\" rel=\"nofollow noreferrer\">trains.automation.optimization.SearchStrategy<\/a> and extend the functionality. This is similar to the <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/trains\/automation\/optuna\/optuna.py\" rel=\"nofollow noreferrer\">Optuna<\/a> integration, where Optuna is used for the Bayesian optimization and Trains does the hyper-parameter setting, launching experiments, and retrieving performance metrics.<\/p>\n<p>Another option (not scalable but probably easier to start with), is to use have the RayTuner run your code (obviously setting the environment \/ git repo \/ docker etc is on the user), and have your training code look something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># create new experimnt\ntask = Task.init('hp optimization', 'ray-tuner experiment', reuse_last_task_id=False)\n# store the hyperparams (assuming hparam is a dict) \ntask.connect(hparam) \n# training loop here\n# ...\n# shutdown experimnt\ntask.close()\n<\/code><\/pre>\n<p>This means every time the RayTuner executes the script a new experiment will be created, with new set of hyper parameters (assuming <code>haparm<\/code> is a dictionary, it will be registered on the experiment as hyper-parameters)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1598536233436,
        "Answer_score":2.0,
        "Owner_location":"Tel Aviv",
        "Question_last_edit_time":1609427499190,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63606182",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how should trains be used with hyper-param optimization tools like raytune?; Content: what could be a reasonable setup for this? can i call task.init() multiple times in the same execution?",
        "Question_original_content_gpt_summary":"The user is seeking advice on how to use hyper-param optimization tools like RayTune with trains, and is wondering if it is possible to call task.init() multiple times in the same execution.",
        "Question_preprocessed_content":"Title: how should trains be used with hyper param optimization tools like raytune?; Content: what could be a reasonable setup for this? can i call multiple times in the same execution?",
        "Answer_original_content":"disclaimer: i'm part of the allegro.ai trains team one solution is to inherit from trains.automation.optimization.searchstrategy and extend the functionality. this is similar to the optuna integration, where optuna is used for the bayesian optimization and trains does the hyper-parameter setting, launching experiments, and retrieving performance metrics. another option (not scalable but probably easier to start with), is to use have the raytuner run your code (obviously setting the environment \/ git repo \/ docker etc is on the user), and have your training code look something like: # create new experimnt task = task.init('hp optimization', 'ray-tuner experiment', reuse_last_task_id=false) # store the hyperparams (assuming hparam is a dict) task.connect(hparam) # training loop here # ... # shutdown experimnt task.close() this means every time the raytuner executes the script a new experiment will be created, with new set of hyper parameters (assuming haparm is a dictionary, it will be registered on the experiment as hyper-parameters)",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"disclaimer i'm part of the trains team one solution is to inherit from and extend the functionality. this is similar to the optuna integration, where optuna is used for the bayesian optimization and trains does the hyper parameter setting, launching experiments, and retrieving performance metrics. another option , is to use have the raytuner run your code , and have your training code look something like this means every time the raytuner executes the script a new experiment will be created, with new set of hyper parameters"
    },
    {
        "Question_id":73336777.0,
        "Question_title":"Tensorflow model saved_model.load() is getting error to predict multiple batch input",
        "Question_body":"<pre><code>tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X1_train)\n\nX1_train = tokenizer.texts_to_sequences(X1_train)\nX1_val = tokenizer.texts_to_sequences(X1_val)\nX1_test = tokenizer.texts_to_sequences(X1_test)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nmaxlen = 5000\n\nX1_train = pad_sequences(X1_train, padding='post', maxlen=maxlen)\nX1_val = pad_sequences(X1_val, padding='post', maxlen=maxlen)\nX1_test = pad_sequences(X1_test, padding='post', maxlen=maxlen)\n\n\nembeddings_dictionary = dict()\ndf_g = pd.read_csv('gs:\/\/----------\/glove.6B.100d.txt', sep=&quot; &quot;, quoting=3, header=None, index_col=0)\nembeddings_dictionary = {key: val.values for key, val in df_g.T.items()}\n\nembedding_matrix = zeros((vocab_size, 100))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\ninput_2_col_list= [x1,x2,...................., x30]\nX2_train = X_train[input_2_col_list].values\n\nX2_val = X_val[input_2_col_list].values\n\nX2_test = X_test[[input_2_col_list].values\n\n\ninput_1 = Input(shape=(maxlen,))\n\ninput_2 = Input(shape=(30,))\n\nembedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(input_1)\nBi_layer= Bidirectional(LSTM(128, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(embedding_layer) # Dimn shd be (None,200,128)\ncon_layer = Conv1D(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(Bi_layer)\navg_pool = GlobalAveragePooling1D()(con_layer)\nmax_pool = GlobalMaxPooling1D()(con_layer)\n\ndense_layer_1 = Dense(64, activation='relu')(input_2)\ndense_layer_2 = Dense(64, activation='relu')(dense_layer_1)\n\nconcat_layer = Concatenate()([avg_pool,max_pool, dense_layer_2])\ndense_layer_3 = Dense(50, activation='relu')(concat_layer)\noutput = Dense(2, activation='softmax')(dense_layer_3)\nmodel = Model(inputs=[input_1, input_2], outputs=output)\n\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n\nprint(model.summary())\n\nhistory = model.fit(x=[X1_train, X2_train], y=y_train, batch_size=30, epochs=10, verbose=1, validation_data=([X1_val,X2_val],y_val))\n\nloss, accuracy, f1_score, precision, recall = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=0)\n\n\nmodel.save('gs:\/\/----------\/Tuned_hybrid_GCP_5000_CASETYPE_8_9.tf')\n\n##################################################\n\nloaded_model=tf.keras.models.load_model( 'gs:\/\/----------\/Tuned_hybrid_GCP_5000_CASETYPE_8_9.tf', custom_objects={&quot;f1_m&quot;: f1_m , &quot;recall_m&quot;: recall_m, &quot;precision_m&quot;: precision_m } ) \nloss, accuracy, f1_score, precision, recall = loaded_model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=0) ###This is getting no error BUT the predictions are wrong\n\n\ny_pred = loaded_model.predict(x=[X1_test, X2_test], batch_size=64, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)  ###This is getting no error BUT the predictions are wrong\n##################################################################\n\n\nimport tensorflow_hub as hub\nx=[X1_test, X2_test]\nloaded_model_2 = tf.keras.Sequential([hub.KerasLayer('gs:---------------\/Tuned_hybrid_GCP_100_CASETYPE_8_11_save.tf')])\nloaded_model_2.build(x.shape) #### Getting an error\n\ny_pred_2 = loaded_model_2.predict(x=[X1_test, X2_test], batch_size=64, verbose=1)\ny_pred_bool_2 = np.argmax(y_pred_2, axis=1)\n\n\n###################################################\n\n#### Inside of the model folder the files and dirs are: assets\/, variables\/, saved_model.pb, keras_metadata.pb\n#### Using 'us-docker.pkg.dev\/vertex-ai\/training\/tf-gpu.2-8:latest' to train the model on Vertex AI\n\n<\/code><\/pre>\n<ol>\n<li>I have tried multiple saving a loading function with custom objects, but not of them are working properly<\/li>\n<li>The working loaded model is predicting, but the outputs are not accurate. I have tested the similar TEST data to predict on the loaded model with another test script. The predictions are not matching after I loaded the model.<\/li>\n<li>similar issues on StackOverflow: 'https:\/\/stackoverflow.com\/questions\/68937973\/how-can-i-fix-the-problem-of-loading-the-model-to-get-new-predictions'<\/li>\n<\/ol>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1660319481417,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Owner_creation_time":1660315189928,
        "Owner_last_access_time":1662778264710,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":0.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1660448455943,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73336777",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tensorflow model saved_model.load() is getting error to predict multiple batch input; Content: tokenizer = tokenizer(num_words=5000) tokenizer.fit_on_texts(x1_train) x1_train = tokenizer.texts_to_sequences(x1_train) x1_val = tokenizer.texts_to_sequences(x1_val) x1_test = tokenizer.texts_to_sequences(x1_test) vocab_size = len(tokenizer.word_index) + 1 maxlen = 5000 x1_train = pad_sequences(x1_train, padding='post', maxlen=maxlen) x1_val = pad_sequences(x1_val, padding='post', maxlen=maxlen) x1_test = pad_sequences(x1_test, padding='post', maxlen=maxlen) embeddings_dictionary = dict() df_g = pd.read_csv('gs:\/\/----------\/glove.6b.100d.txt', sep=\" \", quoting=3, header=none, index_col=0) embeddings_dictionary = {key: val.values for key, val in df_g.t.items()} embedding_matrix = zeros((vocab_size, 100)) for word, index in tokenizer.word_index.items(): embedding_vector = embeddings_dictionary.get(word) if embedding_vector is not none: embedding_matrix[index] = embedding_vector input_2_col_list= [x1,x2,...................., x30] x2_train = x_train[input_2_col_list].values x2_val = x_val[input_2_col_list].values x2_test = x_test[[input_2_col_list].values input_1 = input(shape=(maxlen,)) input_2 = input(shape=(30,)) embedding_layer = embedding(vocab_size, 100, weights=[embedding_matrix], trainable=false)(input_1) bi_layer= bidirectional(lstm(128, return_sequences=true, dropout=0.15, recurrent_dropout=0.15))(embedding_layer) # dimn shd be (none,200,128) con_layer = conv1d(64, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(bi_layer) avg_pool = globalaveragepooling1d()(con_layer) max_pool = globalmaxpooling1d()(con_layer) dense_layer_1 = dense(64, activation='relu')(input_2) dense_layer_2 = dense(64, activation='relu')(dense_layer_1) concat_layer = concatenate()([avg_pool,max_pool, dense_layer_2]) dense_layer_3 = dense(50, activation='relu')(concat_layer) output = dense(2, activation='softmax')(dense_layer_3) model = model(inputs=[input_1, input_2], outputs=output) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m]) print(model.summary()) history = model.fit(x=[x1_train, x2_train], y=y_train, batch_size=30, epochs=10, verbose=1, validation_data=([x1_val,x2_val],y_val)) loss, accuracy, f1_score, precision, recall = model.evaluate(x=[x1_test, x2_test], y=y_test, verbose=0) model.save('gs:\/\/----------\/tuned_hybrid_gcp_5000_casetype_8_9.tf') ################################################## loaded_model=tf.keras.models.load_model( 'gs:\/\/----------\/tuned_hybrid_gcp_5000_casetype_8_9.tf', custom_objects={\"f1_m\": f1_m , \"recall_m\": recall_m, \"precision_m\": precision_m } ) loss, accuracy, f1_score, precision, recall = loaded_model.evaluate(x=[x1_test, x2_test], y=y_test, verbose=0) ###this is getting no error but the predictions are wrong y_pred = loaded_model.predict(x=[x1_test, x2_test], batch_size=64, verbose=1) y_pred_bool = np.argmax(y_pred, axis=1) ###this is getting no error but the predictions are wrong ################################################################## import tensorflow_hub as hub x=[x1_test, x2_test] loaded_model_2 = tf.keras.sequential([hub.keraslayer('gs:---------------\/tuned_hybrid_gcp_100_casetype_8_11_save.tf')]) loaded_model_2.build(x.shape) #### getting an error y_pred_2 = loaded_model_2.predict(x=[x1_test, x2_test], batch_size=64, verbose=1) y_pred_bool_2 = np.argmax(y_pred_2, axis=1) ################################################### #### inside of the model folder the files and dirs are: assets\/, variables\/, saved_model.pb, keras_metadata.pb #### using 'us-docker.pkg.dev\/vertex-ai\/training\/tf-gpu.2-8:latest' to train the model on i have tried multiple saving a loading function with custom objects, but not of them are working properly the working loaded model is predicting, but the outputs are not accurate. i have tested the similar test data to predict on the loaded model with another test script. the predictions are not matching after i loaded the model. similar issues on stackoverflow: 'https:\/\/stackoverflow.com\/questions\/68937973\/how-can-i-fix-the-problem-of-loading-the-model-to-get-new-predictions'",
        "Question_original_content_gpt_summary":"The user is encountering challenges with loading a saved TensorFlow model and using it to predict multiple batch inputs accurately.",
        "Question_preprocessed_content":"Title: tensorflow model is getting error to predict multiple batch input; Content: i have tried multiple saving a loading function with custom objects, but not of them are working properly the working loaded model is predicting, but the outputs are not accurate. i have tested the similar test data to predict on the loaded model with another test script. the predictions are not matching after i loaded the model. similar issues on stackoverflow",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Facing Problems in mlflow deployment on windows server",
        "Question_body":"I am new to mlflow and finding its windows deployment extremely challenging. Has anyone been able to successfully deploy models with mlflow on windows machine ??",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1568860989000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":7.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/lovl7Ns6x0Y",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-09-20T06:19:56",
                "Answer_body":"Maybe you can use docker container .\n\n\nOn Thu, 19 Sep 2019, 12:13 babar ali, <bac...@gmail.com> wrote:\n\nI am new to mlflow and finding its windows deployment extremely challenging. Has anyone been able to successfully deploy models with mlflow on windows machine ??\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/5e8a0cee-7833-48d3-9fed-a61c3db8c349%40googlegroups.com."
            },
            {
                "Answer_creation_time":"2019-09-20T12:45:11",
                "Answer_body":"Hi Babar.\u00a0\n\n\nUnfortunately mlflow windows support is limited to experiment tracking for now (we would appreciate contributions).\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-09-22T19:27:11",
                "Answer_body":"Just to add to that, you may want to consider Docker on Windows as a way to run the packaged models. MLflow provides a command to package models as a Docker container already. The simple built-in model server in mlflow serve is not designed to run on Windows right now.\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/2c536152-711c-4be5-b79b-45530411ac14%40googlegroups.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: facing problems in deployment on windows server; Content: i am new to and finding its windows deployment extremely challenging. has anyone been able to successfully deploy models with on windows machine ??",
        "Question_original_content_gpt_summary":"The user is facing challenges in deploying models on a Windows server and is seeking advice from those who have successfully done so.",
        "Question_preprocessed_content":"Title: facing problems in deployment on windows server; Content: i am new to and finding its windows deployment extremely challenging. has anyone been able to successfully deploy models with on windows machine ??",
        "Answer_original_content":"maybe you can use docker container . on thu, 19 sep 2019, 12:13 babar ali, wrote: i am new to and finding its windows deployment extremely challenging. has anyone been able to successfully deploy models with on windows machine ?? -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/5e8a0cee-7833-48d3-9fed-a61c3db8c349%40googlegroups.com. hi babar. unfortunately windows support is limited to experiment tracking for now (we would appreciate contributions). just to add to that, you may want to consider docker on windows as a way to run the packaged models. provides a command to package models as a docker container already. the simple built-in model server in serve is not designed to run on windows right now. -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/2c536152-711c-4be5-b79b-45530411ac14%40googlegroups.com.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"maybe you can use docker container . on thu, sep , babar ali, wrote i am new to and finding its windows deployment extremely challenging. has anyone been able to successfully deploy models with on windows machine ?? you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to to view this discussion on the web visit hi babar. unfortunately windows support is limited to experiment tracking for now . just to add to that, you may want to consider docker on windows as a way to run the packaged models. provides a command to package models as a docker container already. the simple built in model server in serve is not designed to run on windows right now. you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to to view this discussion on the web visit"
    },
    {
        "Question_id":33708725.0,
        "Question_title":"Azure Machine learning: error with multiclass classification algo",
        "Question_body":"<p>I have <a href=\"https:\/\/yadi.sk\/i\/--Vkm7FTkTAxY\" rel=\"nofollow noreferrer\">training set<\/a> and <a href=\"https:\/\/yadi.sk\/i\/wYu0ZsmukTAw3\" rel=\"nofollow noreferrer\">test set<\/a> (csv files with header), in which I have to classify each value. There is 118.000 uniq values in X column, and only about 13000 in y1 column, so there will be 13000 categories.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Qc1i8.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Qc1i8.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>From Training set I need only <code>X<\/code> and <code>y1<\/code> column to train model. I need to classify X value to one of categories (find normal from of initial word). I tried all multi algo but failed trying to evaluate model.<\/p>\n\n<p>Visualizing Score model return this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EnDZq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EnDZq.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>What can be a problem, it just returns -2 code as error and this <a href=\"https:\/\/yadi.sk\/i\/I6WiEoCGkTCXc\" rel=\"nofollow noreferrer\">log<\/a><\/p>\n\n<p>UPD1: By Metadata Editor module under Project Column Module made column y1  as categorical,  nothing seems to be changed<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1447506491170,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":75.0,
        "Owner_creation_time":1349985908312,
        "Owner_last_access_time":1654028256076,
        "Owner_reputation":4886.0,
        "Owner_up_votes":1215.0,
        "Owner_down_votes":15.0,
        "Owner_views":1228.0,
        "Answer_body":"<p><strong>Moncef<\/strong> provided <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/azure\/en-US\/aabb9080-adf8-4fc0-b332-2a3c8ee29a28\/azure-machine-learning-error-with-multiclass-classification-algo?forum=MachineLearning\" rel=\"nofollow\">here<\/a> the solution to my problem:<\/p>\n\n<p>The point is that Azure has limitations on maximum categories 8192, this is why the number should be decreased by R or python modules or own evaluation module may be created. Or there is another way, evaluation step may be skipped, because model`ve been trained successfully. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1448271863192,
        "Answer_score":1.0,
        "Owner_location":"Moscow, Russia",
        "Question_last_edit_time":1454300637020,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/33708725",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: : error with multiclass classification algo; Content: i have training set and test set (csv files with header), in which i have to classify each value. there is 118.000 uniq values in x column, and only about 13000 in y1 column, so there will be 13000 categories. from training set i need only x and y1 column to train model. i need to classify x value to one of categories (find normal from of initial word). i tried all multi algo but failed trying to evaluate model. visualizing score model return this: what can be a problem, it just returns -2 code as error and this log upd1: by metadata editor module under project column module made column y1 as categorical, nothing seems to be changed",
        "Question_original_content_gpt_summary":"The user is encountering challenges with a multiclass classification algorithm while attempting to classify 118,000 unique values in the 'x' column into 13,000 categories from the 'y1' column.",
        "Question_preprocessed_content":"Title: error with multiclass classification algo; Content: i have training set and test set , in which i have to classify each value. there is uniq values in x column, and only about in y column, so there will be categories. from training set i need only and column to train model. i need to classify x value to one of categories . i tried all multi algo but failed trying to evaluate model. visualizing score model return this what can be a problem, it just returns code as error and this log upd by metadata editor module under project column module made column y as categorical, nothing seems to be changed",
        "Answer_original_content":"moncef provided here the solution to my problem: the point is that azure has limitations on maximum categories 8192, this is why the number should be decreased by r or python modules or own evaluation module may be created. or there is another way, evaluation step may be skipped, because model`ve been trained successfully.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"moncef provided here the solution to my problem the point is that azure has limitations on maximum categories , this is why the number should be decreased by r or python modules or own evaluation module may be created. or there is another way, evaluation step may be skipped, because model`ve been trained successfully."
    },
    {
        "Question_id":null,
        "Question_title":"Import issue with azureml-train-automl-runtime package",
        "Question_body":"I am trying to import azureml-train-automl-runtime to do explanations from azure automl pipeline following the tutorial in the link https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-machine-learning-interpretability-automl\n\nBut I am getting the below error\n\n Exception ignored in: <function _Win32Helper.del at 0x0000021ECA3AD430> Traceback (most recent call last): File \"C:\\Anaconda3\\envs\\check_win32_error\\lib\\site-packages\\azureml\\automl\\runtime\\shared\\win32_helper.py\", line 246, in del TypeError: catching classes that do not inherit from BaseException is not allowed\n\n\n\n\nError Screenshot:\n\n\n\n\n\nI asked this question in stackoverflow but did not get valid answer,\nplease refer to the stackoverflow link below:\nhttps:\/\/stackoverflow.com\/questions\/74160617\/baseexception-when-trying-to-import-azureml-train-automl-runtime-in-windows-10",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1669689516027,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1107805\/import-issue-with-azureml-train-automl-runtime-pac.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: import issue with -train-automl-runtime package; Content: i am trying to import -train-automl-runtime to do explanations from azure automl pipeline following the tutorial in the link https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-machine-learning-interpretability-automl but i am getting the below error exception ignored in: traceback (most recent call last): file \"c:\\anaconda3\\envs\\check_win32_error\\lib\\site-packages\\\\automl\\runtime\\shared\\win32_helper.py\", line 246, in del typeerror: catching classes that do not inherit from baseexception is not allowed error screenshot: i asked this question in stackoverflow but did not get valid answer, please refer to the stackoverflow link below: https:\/\/stackoverflow.com\/questions\/74160617\/baseexception-when-trying-to-import--train-automl-runtime-in-windows-10",
        "Question_original_content_gpt_summary":"The user is encountering an issue when attempting to import the -train-automl-runtime package to do explanations from an Azure Automl pipeline, resulting in a TypeError related to catching classes that do not inherit from BaseException.",
        "Question_preprocessed_content":"Title: import issue with train automl runtime package; Content: i am trying to import train automl runtime to do explanations from azure automl pipeline following the tutorial in the link but i am getting the below error exception ignored in traceback file line , in del typeerror catching classes that do not inherit from baseexception is not allowed error screenshot i asked this question in stackoverflow but did not get valid answer, please refer to the stackoverflow link below",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":62938761.0,
        "Question_title":"Sagemaker Endpoint returns error when trying to make a prediction in the Sagemaker Notebook Instances",
        "Question_body":"<p>I've deployed an inference pipeline endpoint on sagemaker, but now when I try to make a prediction call but I get an error message and to the best of my knowledge I am following the examples shown here:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/Inference%20Pipeline%20with%20Scikit-learn%20and%20Linear%20Learner.ipynb<\/a><\/p>\n<p>The code to make an endpoint call is here:<\/p>\n<pre><code>from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\nfrom sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n\npredictor = RealTimePredictor(\n    endpoint='Boston-inf-pipeline-July09-endpoint',\n    sagemaker_session=sagemaker_session,\n    serializer=csv_serializer,\n    content_type=CONTENT_TYPE_CSV,\n    accept=CONTENT_TYPE_CSV)\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Binarizer, StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n\ndff = pd.read_csv('housing.csv',delimiter=r&quot;\\s+&quot;, names=column_names)\n\ndff.drop('MEDV',axis=1,inplace=True)\n\n \n#String\nx = '0.00632,18.0,2.31,0,0.538,6.575,65.2,4.09,1,296.0,15.3,396.9,4.98'\n\n#DataFrame\ny= dff.head(1)\n\n#Array\nz = np.array([0.00632,18.0,2.31,0,0.538,6.575,65.2,4.09,1,296.0,15.3,396.9,4.98])\n\nprint(predictor.predict(x))**strong text**\n<\/code><\/pre>\n<p>and here is the error message:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6mAQD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6mAQD.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>and here is what is says on Cloud Watch<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/aLY2r.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aLY2r.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1594915218970,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":300.0,
        "Owner_creation_time":1592973297527,
        "Owner_last_access_time":1597297703647,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":7.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1594922124870,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62938761",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: endpoint returns error when trying to make a prediction in the notebook instances; Content: i've deployed an inference pipeline endpoint on , but now when i try to make a prediction call but i get an error message and to the best of my knowledge i am following the examples shown here: https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/-python-sdk\/scikit_learn_inference_pipeline\/inference%20pipeline%20with%20scikit-learn%20and%20linear%20learner.ipynb the code to make an endpoint call is here: from .predictor import json_serializer, csv_serializer, json_deserializer, realtimepredictor from .content_types import content_type_csv, content_type_json predictor = realtimepredictor( endpoint='boston-inf-pipeline-july09-endpoint', _session=_session, serializer=csv_serializer, content_type=content_type_csv, accept=content_type_csv) from sklearn.pipeline import pipeline from sklearn.preprocessing import binarizer, standardscaler, onehotencoder from sklearn.impute import simpleimputer column_names = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat', 'medv'] dff = pd.read_csv('housing.csv',delimiter=r\"\\s+\", names=column_names) dff.drop('medv',axis=1,inplace=true) #string x = '0.00632,18.0,2.31,0,0.538,6.575,65.2,4.09,1,296.0,15.3,396.9,4.98' #dataframe y= dff.head(1) #array z = np.array([0.00632,18.0,2.31,0,0.538,6.575,65.2,4.09,1,296.0,15.3,396.9,4.98]) print(predictor.predict(x))**strong text** and here is the error message: and here is what is says on cloud watch",
        "Question_original_content_gpt_summary":"The user is encountering an error when trying to make a prediction call to an inference pipeline endpoint deployed on Amazon SageMaker, despite following the examples provided.",
        "Question_preprocessed_content":"Title: endpoint returns error when trying to make a prediction in the notebook instances; Content: i've deployed an inference pipeline endpoint on , but now when i try to make a prediction call but i get an error message and to the best of my knowledge i am following the examples shown here the code to make an endpoint call is here and here is the error message and here is what is says on cloud watch",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":71191185.0,
        "Question_title":"Pip installations on Colab from local",
        "Question_body":"<p>I'd like to use wandb on Colab, and I've installed it through pip on the command line. However, the import isn't recognized on Colab, so I have to run <code>!pip install wandb<\/code> each time.<\/p>\n<p>How can I install <code>wandb<\/code> locally so that I don't have to install it on the Colab notebook each time?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":4.0,
        "Question_creation_time":1645328187297,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":516.0,
        "Owner_creation_time":1558237713156,
        "Owner_last_access_time":1663947795227,
        "Owner_reputation":421.0,
        "Owner_up_votes":20.0,
        "Owner_down_votes":5.0,
        "Owner_views":33.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71191185",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pip installations on colab from local; Content: i'd like to use on colab, and i've installed it through pip on the command line. however, the import isn't recognized on colab, so i have to run !pip install each time. how can i install locally so that i don't have to install it on the colab notebook each time?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with installing a pip package locally on Colab so that they do not have to install it each time they open the Colab notebook.",
        "Question_preprocessed_content":"Title: pip installations on colab from local; Content: i'd like to use on colab, and i've installed it through pip on the command line. however, the import isn't recognized on colab, so i have to run each time. how can i install locally so that i don't have to install it on the colab notebook each time?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":59373487.0,
        "Question_title":"How to add datapoint to an alarm in aws cloudwatch in TargetTracking Scaling using the json file?",
        "Question_body":"<p>so I am using the TargetTracking Scaling to scale up my aws sagemaker endpoint by using two commands, which are:-<\/p>\n\n<pre><code>1. aws application-autoscaling register-scalable-target --service-namespace sagemaker --resource-id endpoint\/{}\/variant\/AllTraffic --scalable-dimension sagemaker:variant:DesiredInstanceCount --min-capacity {} --max-capacity {}\".format(endpoint_name,ENDPOINT_INSTANCE_MIN_COUNT,ENDPOINT_INSTANCE_MAX_COUNT)) \n<\/code><\/pre>\n\n<p>\"This command is used to register the endpoint to do the auto-scaling\"<\/p>\n\n<pre><code>2.  aws application-autoscaling put-scaling-policy --policy-name myscalablepolicy --policy-type TargetTrackingScaling --resource-id endpoint\/{}\/variant\/AllTraffic --service-namespace sagemaker --scalable-dimension sagemaker:variant:DesiredInstanceCount --target-tracking-scaling-policy-configuration file:\/\/file.json\".format(endpoint_name))\n<\/code><\/pre>\n\n<p>\"This is used to actually add the target tracking scaling to aws sagemaker endpoint by taking the configuration from the file.json file\"<\/p>\n\n<p>Here is the content of my file.json file<\/p>\n\n<pre><code>{\n  \"TargetValue\": 50,\n  \"CustomizedMetricSpecification\": {\n    \"MetricName\": \"CPUUtilization\",\n    \"Namespace\": \"\/aws\/sagemaker\/Endpoints\",\n    \"Dimensions\": [\n      {\n        \"Name\": \"EndpointName\",\n        \"Value\": \"debitscore-2019-12-17-10-26-16-605\"\n      },\n      {\n        \"Name\": \"VariantName\",\n        \"Value\": \"AllTraffic\"\n     }\n    ],\n    \"Statistic\": \"Average\"\n  },\n  \"ScaleOutCooldown\": 60,\n  \"ScaleInCooldown\": 10,\n  \"DisableScaleIn\": false\n}\n<\/code><\/pre>\n\n<p>so it is working fine, but the issue is in AWS <\/p>\n\n<ol>\n<li>Two-alarm has been configured after 2nd command runs, one is for high and second is for low, for reference see the image attached. And also how it is deciding to set the low alarm to 45 percent, I only mention for target value in the file.json file which is 50 in this case.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/lbKOk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lbKOk.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<ol start=\"2\">\n<li>How to add the datapoint in alarm like in this it was taking 15 datapoints in 15 minutes and 3 datapoints in 3 minutes. <\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1576582781373,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":243.0,
        "Owner_creation_time":1556024336350,
        "Owner_last_access_time":1664004795587,
        "Owner_reputation":305.0,
        "Owner_up_votes":7.0,
        "Owner_down_votes":0.0,
        "Owner_views":20.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1576740937560,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59373487",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to add datapoint to an alarm in aws cloudwatch in targettracking scaling using the json file?; Content: so i am using the targettracking scaling to scale up my endpoint by using two commands, which are:- 1. aws application-autoscaling register-scalable-target --service-namespace --resource-id endpoint\/{}\/variant\/alltraffic --scalable-dimension :variant:desiredinstancecount --min-capacity {} --max-capacity {}\".format(endpoint_name,endpoint_instance_min_count,endpoint_instance_max_count)) \"this command is used to register the endpoint to do the auto-scaling\" 2. aws application-autoscaling put-scaling-policy --policy-name myscalablepolicy --policy-type targettrackingscaling --resource-id endpoint\/{}\/variant\/alltraffic --service-namespace --scalable-dimension :variant:desiredinstancecount --target-tracking-scaling-policy-configuration file:\/\/file.json\".format(endpoint_name)) \"this is used to actually add the target tracking scaling to endpoint by taking the configuration from the file.json file\" here is the content of my file.json file { \"targetvalue\": 50, \"customizedmetricspecification\": { \"metricname\": \"cpuutilization\", \"namespace\": \"\/aws\/\/endpoints\", \"dimensions\": [ { \"name\": \"endpointname\", \"value\": \"debitscore-2019-12-17-10-26-16-605\" }, { \"name\": \"variantname\", \"value\": \"alltraffic\" } ], \"statistic\": \"average\" }, \"scaleoutcooldown\": 60, \"scaleincooldown\": 10, \"disablescalein\": false } so it is working fine, but the issue is in aws two-alarm has been configured after 2nd command runs, one is for high and second is for low, for reference see the image attached. and also how it is deciding to set the low alarm to 45 percent, i only mention for target value in the file.json file which is 50 in this case. how to add the datapoint in alarm like in this it was taking 15 datapoints in 15 minutes and 3 datapoints in 3 minutes.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with adding datapoints to an alarm in AWS Cloudwatch in targettracking scaling using a JSON file.",
        "Question_preprocessed_content":"Title: how to add datapoint to an alarm in aws cloudwatch in targettracking scaling using the json file?; Content: so i am using the targettracking scaling to scale up my endpoint by using two commands, which are this command is used to register the endpoint to do the auto scaling this is used to actually add the target tracking scaling to endpoint by taking the configuration from the file here is the content of my file so it is working fine, but the issue is in aws two alarm has been configured after nd command runs, one is for high and second is for low, for reference see the image attached. and also how it is deciding to set the low alarm to percent, i only mention for target value in the file which is in this case. how to add the datapoint in alarm like in this it was taking datapoints in minutes and datapoints in minutes.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Wandb.watch with PyTorch Lightning not logging",
        "Question_body":"<p>Hi,<\/p>\n<p>I moved all of my calls to wandb  from the training loop to PyTorch Lightning (PL)'s <code>Callback<\/code> module. All of my <code>wandb.log()<\/code> calls are working properly, but the gradients and parameter tabs in my wandb dashboard are empty.  I checked two threads:<\/p>\n<ul>\n<li>Wandb.watch with pytorch not logging anything<\/li>\n<li>When is one supposed to run wandb.watch so that weights and biases tracks params and gradients?<\/li>\n<\/ul>\n<p>For the first thread, the link to the run has expired and I don\u2019t fully understand the context of the solution \"\u2026 was using <code>forward()<\/code> instead of <code>__call__()<\/code>\".<\/p>\n<p>For the second thread, <code>wandb.log<\/code> is getting called after PL\u2019s Callback hook <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/extensions\/callbacks.html#on-train-batch-end\" rel=\"noopener nofollow ugc\"><code>on_train_batch_end<\/code><\/a>, so <code>wandb.log<\/code> should be getting called after a backward pass.<\/p>\n<p>Below is a portion of the code defined in the Callback Module. At the start of training (<code>on_fit_start<\/code>) I initialize the wandb run and call <code>wandb.watch<\/code>.  And after a batch is completed, (<code>on_train_batch_end<\/code>) I log all the metrics.<\/p>\n<pre><code class=\"lang-auto\">Class PatentLoggerCallback(Callback):\n \n   # Omitted non-relevant code \n\n    def on_fit_start(self, trainer, pl_module):\n        wandb.init(project=self.project,\n                   config=pl_module.hparams,\n                   dir=self.save_dir)\n\n        wandb.watch(pl_module,\n                    criterion=torch.nn.functional.binary_cross_entropy_with_logits,\n                    log='all',\n                    log_freq=10,\n                    log_graph=True)\n\n    def on_train_batch_end(\n        self,\n        trainer: Trainer,\n        pl_module: LightningModule,\n        outputs: Sequence,\n        batch: Sequence,\n        batch_idx: int,\n        dataloader_idx: int,\n    ) -&gt; None:\n\n        metrics = outputs['metrics']\n        for metric, value in metrics.items():\n            wandb.log({f'train\/{metric}': value})\n<\/code><\/pre>\n<p>I would like to have produced a google collab for reproducibility, but there is a lot of code involved. The next best thing I can offer is this <a href=\"https:\/\/github.com\/DennisMinn\/patent-phrase-matching\/blob\/nakama\/Workspace.ipynb\" rel=\"noopener nofollow ugc\">Jupyter Notebook<\/a> that runs through my entire code.  I\u2019m not expecting you to clone to repository, but if you do make sure you\u2019re on the \u201cnakama\u201d branch.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1654815394134,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":155.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-watch-with-pytorch-lightning-not-logging\/2589",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-10T00:12:42.603Z",
                "Answer_body":"<p><strong>RESOLVED<\/strong><\/p>\n<p>In my model\u2019s forward pass I wrote <code>return torch.rand(input_ids.shape[0], requires_grad=True)<\/code> to faster debug other <code>wandb.log<\/code> calls by skipping expensive computation. B\/c none of the model\u2019s parameters were being updated, there  was no backward pass over the model being watched. The only thing that underwent the backward pass was the <code>torch.rand()<\/code>  hence no histograms.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-09T00:13:37.833Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: .watch with pytorch lightning not logging; Content: hi, i moved all of my calls to from the training loop to pytorch lightning (pl)'s callback module. all of my .log() calls are working properly, but the gradients and parameter tabs in my dashboard are empty. i checked two threads: .watch with pytorch not logging anything when is one supposed to run .watch so that tracks params and gradients? for the first thread, the link to the run has expired and i don\u2019t fully understand the context of the solution \"\u2026 was using forward() instead of __call__()\". for the second thread, .log is getting called after pl\u2019s callback hook on_train_batch_end, so .log should be getting called after a backward pass. below is a portion of the code defined in the callback module. at the start of training (on_fit_start) i initialize the run and call .watch. and after a batch is completed, (on_train_batch_end) i log all the metrics. class patentloggercallback(callback): # omitted non-relevant code def on_fit_start(self, trainer, pl_module): .init(project=self.project, config=pl_module.hparams, dir=self.save_dir) .watch(pl_module, criterion=torch.nn.functional.binary_cross_entropy_with_logits, log='all', log_freq=10, log_graph=true) def on_train_batch_end( self, trainer: trainer, pl_module: lightningmodule, outputs: sequence, batch: sequence, batch_idx: int, dataloader_idx: int, ) -> none: metrics = outputs['metrics'] for metric, value in metrics.items(): .log({f'train\/{metric}': value}) i would like to have produced a google collab for reproducibility, but there is a lot of code involved. the next best thing i can offer is this jupyter notebook that runs through my entire code. i\u2019m not expecting you to clone to repository, but if you do make sure you\u2019re on the \u201cnakama\u201d branch.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with .watch not logging gradients and parameters in their dashboard, despite their .log() calls working properly, and is seeking a solution to the issue.",
        "Question_preprocessed_content":"Title: with pytorch lightning not logging; Content: hi, i moved all of my calls to from the training loop to pytorch lightning 's module. all of my calls are working properly, but the gradients and parameter tabs in my dashboard are empty. i checked two threads with pytorch not logging anything when is one supposed to run so that tracks params and gradients? for the first thread, the link to the run has expired and i dont fully understand the context of the solution was using instead of . for the second thread, is getting called after pls callback hook , so should be getting called after a backward pass. below is a portion of the code defined in the callback module. at the start of training i initialize the run and call . and after a batch is completed, i log all the metrics. i would like to have produced a google collab for reproducibility, but there is a lot of code involved. the next best thing i can offer is this jupyter notebook that runs through my entire code. im not expecting you to clone to repository, but if you do make sure youre on the nakama branch.",
        "Answer_original_content":"resolved in my models forward pass i wrote return torch.rand(input_ids.shape[0], requires_grad=true) to faster debug other .log calls by skipping expensive computation. b\/c none of the models parameters were being updated, there was no backward pass over the model being watched. the only thing that underwent the backward pass was the torch.rand() hence no histograms. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"resolved in my models forward pass i wrote to faster debug other calls by skipping expensive computation. none of the models parameters were being updated, there was no backward pass over the model being watched. the only thing that underwent the backward pass was the hence no histograms. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker batch transform",
        "Question_body":"Hi, it seems that Sagemaker Batch Transform is limited to 100MB payloads I'd like to run preds against a 5GB csv file, what the recommended way to do so?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1532619204000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":254.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUlefH1ni4QOaulUT4870D5g\/sagemaker-batch-transform",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2018-07-26T18:02:46.000Z",
                "Answer_score":0,
                "Answer_body":"SageMaker Batch Transform will automatically split your input file into whatever payload size is specified if you use \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\". There's no need to split files yourself or to use large payload sizes unless you have very large single records.\n\nHope that helps!",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: batch transform; Content: hi, it seems that batch transform is limited to 100mb payloads i'd like to run preds against a 5gb csv file, what the recommended way to do so?",
        "Question_original_content_gpt_summary":"The user is looking for a way to run predictions against a 5GB CSV file, as batch transform is limited to 100MB payloads.",
        "Question_preprocessed_content":"Title: batch transform; Content: hi, it seems that batch transform is limited to mb payloads i'd like to run preds against a gb csv file, what the recommended way to do so?",
        "Answer_original_content":"batch transform will automatically split your input file into whatever payload size is specified if you use \"splittype\": \"line\" and \"batchstrategy\": \"multirecord\". there's no need to split files yourself or to use large payload sizes unless you have very large single records. hope that helps!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"batch transform will automatically split your input file into whatever payload size is specified if you use splittype line and batchstrategy multirecord . there's no need to split files yourself or to use large payload sizes unless you have very large single records. hope that helps!"
    },
    {
        "Question_id":61380051.0,
        "Question_title":"Sagemaker usage of EC2 instances",
        "Question_body":"<p>Is there a way to view\/monitor AWS Sagemaker's usage of EC2 instances?\nI am running a Sagemaker endpoint and tried to find its instances (ml.p3.2xlarge in this case) in the EC2 UI, but couldn't find them. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1587620263500,
        "Question_favorite_count":null,
        "Question_score":5.0,
        "Question_view_count":615.0,
        "Owner_creation_time":1458550179920,
        "Owner_last_access_time":1658058794840,
        "Owner_reputation":383.0,
        "Owner_up_votes":10.0,
        "Owner_down_votes":0.0,
        "Owner_views":19.0,
        "Answer_body":"<p>ml EC2 instances do not appear in the EC2 console. You can find their metrics in Cloudwatch though, and create dashboards to monitor what you need:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/BgUkm.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wD4w0.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1588575738648,
        "Answer_score":3.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61380051",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: usage of ec2 instances; Content: is there a way to view\/monitor 's usage of ec2 instances? i am running a endpoint and tried to find its instances (ml.p3.2xlarge in this case) in the ec2 ui, but couldn't find them.",
        "Question_original_content_gpt_summary":"The user is unable to locate their EC2 instances in the EC2 UI.",
        "Question_preprocessed_content":"Title: usage of ec instances; Content: is there a way to 's usage of ec instances? i am running a endpoint and tried to find its instances in the ec ui, but couldn't find them.",
        "Answer_original_content":"ml ec2 instances do not appear in the ec2 console. you can find their metrics in cloudwatch though, and create dashboards to monitor what you need:",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"ml ec instances do not appear in the ec console. you can find their metrics in cloudwatch though, and create dashboards to monitor what you need"
    },
    {
        "Question_id":null,
        "Question_title":"Azure-ml defaults breaking pip",
        "Question_body":"Hello,\n\nI've seen that the azureml-defaults package has been updated to version 1.41.0 updated in the past 2 days. https:\/\/pypi.org\/project\/azureml-defaults\/\n\nSince that update, I've been unable to update my web service as when the pip was installing the requirements for this dependency - it was left at the below stage in the screenshot.\n\nIt tries to run for 90 mins then times out. Was working perfectly before this update. Have tried decreasing the version as well and doesn't seem to work.\n\nThanks,\n\nCam",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1651090798580,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/829098\/azure-ml-core-breaking-pip.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-02T14:40:01.723Z",
                "Answer_score":0,
                "Answer_body":"Hi I have the same error. Is there any compatibilities issue?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure-ml defaults breaking pip; Content: hello, i've seen that the -defaults package has been updated to version 1.41.0 updated in the past 2 days. https:\/\/pypi.org\/project\/-defaults\/ since that update, i've been unable to update my web service as when the pip was installing the requirements for this dependency - it was left at the below stage in the screenshot. it tries to run for 90 mins then times out. was working perfectly before this update. have tried decreasing the version as well and doesn't seem to work. thanks, cam",
        "Question_original_content_gpt_summary":"The user encountered a challenge where the updated version of the -defaults package caused their web service to time out when attempting to install the requirements for the dependency.",
        "Question_preprocessed_content":"Title: defaults breaking pip; Content: hello, i've seen that the defaults package has been updated to version updated in the past days. since that update, i've been unable to update my web service as when the pip was installing the requirements for this dependency it was left at the below stage in the screenshot. it tries to run for mins then times out. was working perfectly before this update. have tried decreasing the version as well and doesn't seem to work. thanks, cam",
        "Answer_original_content":"hi i have the same error. is there any compatibilities issue?",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi i have the same error. is there any compatibilities issue?"
    },
    {
        "Question_id":54622191.0,
        "Question_title":"Access Denied read open data into Sagemaker",
        "Question_body":"<p>Cannot read AWS open data datasets into Sagemaker. Error is<\/p>\n\n<pre><code>download failed: s3:\/\/fast-ai-imageclas\/cifar100.tgz to ..\/..\/..\/tmp\/fastai-images\/cifar100.tgz An error occurred (AccessDenied) when calling the GetObject operation: Access Denied\n<\/code><\/pre>\n\n<p>code\n<a href=\"https:\/\/i.stack.imgur.com\/2b73H.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2b73H.png\" alt=\"sagemaker notebook s3 download access denied\"><\/a><\/p>\n\n<p>The user has the s3:getObjects * permission<\/p>\n\n<p>The user's permissions are the full s3 read policy and the full Sagemaker policies. The policies are<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:Get*\",\n                \"s3:List*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:*\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"application-autoscaling:DeleteScalingPolicy\",\n                \"application-autoscaling:DeleteScheduledAction\",\n                \"application-autoscaling:DeregisterScalableTarget\",\n                \"application-autoscaling:DescribeScalableTargets\",\n                \"application-autoscaling:DescribeScalingActivities\",\n                \"application-autoscaling:DescribeScalingPolicies\",\n                \"application-autoscaling:DescribeScheduledActions\",\n                \"application-autoscaling:PutScalingPolicy\",\n                \"application-autoscaling:PutScheduledAction\",\n                \"application-autoscaling:RegisterScalableTarget\",\n                \"aws-marketplace:ViewSubscriptions\",\n                \"cloudwatch:DeleteAlarms\",\n                \"cloudwatch:DescribeAlarms\",\n                \"cloudwatch:GetMetricData\",\n                \"cloudwatch:GetMetricStatistics\",\n                \"cloudwatch:ListMetrics\",\n                \"cloudwatch:PutMetricAlarm\",\n                \"cloudwatch:PutMetricData\",\n                \"codecommit:BatchGetRepositories\",\n                \"codecommit:CreateRepository\",\n                \"codecommit:GetRepository\",\n                \"codecommit:ListBranches\",\n                \"codecommit:ListRepositories\",\n                \"cognito-idp:AdminAddUserToGroup\",\n                \"cognito-idp:AdminCreateUser\",\n                \"cognito-idp:AdminDeleteUser\",\n                \"cognito-idp:AdminDisableUser\",\n                \"cognito-idp:AdminEnableUser\",\n                \"cognito-idp:AdminRemoveUserFromGroup\",\n                \"cognito-idp:CreateGroup\",\n                \"cognito-idp:CreateUserPool\",\n                \"cognito-idp:CreateUserPoolClient\",\n                \"cognito-idp:CreateUserPoolDomain\",\n                \"cognito-idp:DescribeUserPool\",\n                \"cognito-idp:DescribeUserPoolClient\",\n                \"cognito-idp:ListGroups\",\n                \"cognito-idp:ListIdentityProviders\",\n                \"cognito-idp:ListUserPoolClients\",\n                \"cognito-idp:ListUserPools\",\n                \"cognito-idp:ListUsers\",\n                \"cognito-idp:ListUsersInGroup\",\n                \"cognito-idp:UpdateUserPool\",\n                \"cognito-idp:UpdateUserPoolClient\",\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:CreateNetworkInterfacePermission\",\n                \"ec2:CreateVpcEndpoint\",\n                \"ec2:DeleteNetworkInterface\",\n                \"ec2:DeleteNetworkInterfacePermission\",\n                \"ec2:DescribeDhcpOptions\",\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeVpcEndpoints\",\n                \"ec2:DescribeVpcs\",\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:BatchGetImage\",\n                \"ecr:CreateRepository\",\n                \"ecr:GetAuthorizationToken\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:Describe*\",\n                \"elastic-inference:Connect\",\n                \"glue:CreateJob\",\n                \"glue:DeleteJob\",\n                \"glue:GetJob\",\n                \"glue:GetJobRun\",\n                \"glue:GetJobRuns\",\n                \"glue:GetJobs\",\n                \"glue:ResetJobBookmark\",\n                \"glue:StartJobRun\",\n                \"glue:UpdateJob\",\n                \"groundtruthlabeling:*\",\n                \"iam:ListRoles\",\n                \"kms:DescribeKey\",\n                \"kms:ListAliases\",\n                \"lambda:ListFunctions\",\n                \"logs:CreateLogGroup\",\n                \"logs:CreateLogStream\",\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\",\n                \"logs:PutLogEvents\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:SetRepositoryPolicy\",\n                \"ecr:CompleteLayerUpload\",\n                \"ecr:BatchDeleteImage\",\n                \"ecr:UploadLayerPart\",\n                \"ecr:DeleteRepositoryPolicy\",\n                \"ecr:InitiateLayerUpload\",\n                \"ecr:DeleteRepository\",\n                \"ecr:PutImage\"\n            ],\n            \"Resource\": \"arn:aws:ecr:*:*:repository\/*sagemaker*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"codecommit:GitPull\",\n                \"codecommit:GitPush\"\n            ],\n            \"Resource\": [\n                \"arn:aws:codecommit:*:*:*sagemaker*\",\n                \"arn:aws:codecommit:*:*:*SageMaker*\",\n                \"arn:aws:codecommit:*:*:*Sagemaker*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:CreateSecret\",\n                \"secretsmanager:DescribeSecret\",\n                \"secretsmanager:ListSecrets\",\n                \"secretsmanager:TagResource\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"secretsmanager:GetSecretValue\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"secretsmanager:ResourceTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationApplication\",\n                \"robomaker:DescribeSimulationApplication\",\n                \"robomaker:DeleteSimulationApplication\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"robomaker:CreateSimulationJob\",\n                \"robomaker:DescribeSimulationJob\",\n                \"robomaker:CancelSimulationJob\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::*SageMaker*\",\n                \"arn:aws:s3:::*Sagemaker*\",\n                \"arn:aws:s3:::*sagemaker*\",\n                \"arn:aws:s3:::*aws-glue*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:CreateBucket\",\n                \"s3:GetBucketLocation\",\n                \"s3:ListBucket\",\n                \"s3:ListAllMyBuckets\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEqualsIgnoreCase\": {\n                    \"s3:ExistingObjectTag\/SageMaker\": \"true\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"lambda:InvokeFunction\"\n            ],\n            \"Resource\": [\n                \"arn:aws:lambda:*:*:function:*SageMaker*\",\n                \"arn:aws:lambda:*:*:function:*sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*Sagemaker*\",\n                \"arn:aws:lambda:*:*:function:*LabelingFunction*\"\n            ]\n        },\n        {\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Effect\": \"Allow\",\n            \"Resource\": \"arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"iam:AWSServiceName\": \"sagemaker.application-autoscaling.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:CreateServiceLinkedRole\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:AWSServiceName\": \"robomaker.amazonaws.com\"\n                }\n            }\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"iam:PassedToService\": [\n                        \"sagemaker.amazonaws.com\",\n                        \"glue.amazonaws.com\",\n                        \"robomaker.amazonaws.com\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>The Sagemaker instance is in us-east-1 same as the dataset.<\/p>\n\n<p>The dataset is <a href=\"https:\/\/registry.opendata.aws\/fast-ai-imageclas\/\" rel=\"nofollow noreferrer\">https:\/\/registry.opendata.aws\/fast-ai-imageclas\/<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2.0,
        "Question_creation_time":1549841280153,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1192.0,
        "Owner_creation_time":1298484007147,
        "Owner_last_access_time":1664045053900,
        "Owner_reputation":9271.0,
        "Owner_up_votes":2074.0,
        "Owner_down_votes":44.0,
        "Owner_views":1819.0,
        "Answer_body":"<p>thanks to Matthew I looked into the permissions of the notebook itself, not just the user using Sagemaker.<\/p>\n\n<p>The policies on the notebook look like this and I can download from the aws open data datasets!<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/s0jwV.png\" alt=\"notebook settings\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/OqEHi.png\" alt=\"notebook permissions\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1549864663812,
        "Answer_score":2.0,
        "Owner_location":"New York, NY, United States",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54622191",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: access denied read open data into ; Content: cannot read aws open data datasets into . error is download failed: s3:\/\/fast-ai-imageclas\/cifar100.tgz to ..\/..\/..\/tmp\/fastai-images\/cifar100.tgz an error occurred (accessdenied) when calling the getobject operation: access denied code the user has the s3:getobjects * permission the user's permissions are the full s3 read policy and the full policies. the policies are { \"version\": \"2012-10-17\", \"statement\": [ { \"effect\": \"allow\", \"action\": [ \"s3:get*\", \"s3:list*\" ], \"resource\": \"*\" } ] } { \"version\": \"2012-10-17\", \"statement\": [ { \"effect\": \"allow\", \"action\": [ \":*\" ], \"resource\": \"*\" }, { \"effect\": \"allow\", \"action\": [ \"application-autoscaling:deletescalingpolicy\", \"application-autoscaling:deletescheduledaction\", \"application-autoscaling:deregisterscalabletarget\", \"application-autoscaling:describescalabletargets\", \"application-autoscaling:describescalingactivities\", \"application-autoscaling:describescalingpolicies\", \"application-autoscaling:describescheduledactions\", \"application-autoscaling:putscalingpolicy\", \"application-autoscaling:putscheduledaction\", \"application-autoscaling:registerscalabletarget\", \"aws-marketplace:viewsubscriptions\", \"cloudwatch:deletealarms\", \"cloudwatch:describealarms\", \"cloudwatch:getmetricdata\", \"cloudwatch:getmetricstatistics\", \"cloudwatch:listmetrics\", \"cloudwatch:putmetricalarm\", \"cloudwatch:putmetricdata\", \"codecommit:batchgetrepositories\", \"codecommit:createrepository\", \"codecommit:getrepository\", \"codecommit:listbranches\", \"codecommit:listrepositories\", \"cognito-idp:adminaddusertogroup\", \"cognito-idp:admincreateuser\", \"cognito-idp:admindeleteuser\", \"cognito-idp:admindisableuser\", \"cognito-idp:adminenableuser\", \"cognito-idp:adminremoveuserfromgroup\", \"cognito-idp:creategroup\", \"cognito-idp:createuserpool\", \"cognito-idp:createuserpoolclient\", \"cognito-idp:createuserpooldomain\", \"cognito-idp:describeuserpool\", \"cognito-idp:describeuserpoolclient\", \"cognito-idp:listgroups\", \"cognito-idp:listidentityproviders\", \"cognito-idp:listuserpoolclients\", \"cognito-idp:listuserpools\", \"cognito-idp:listusers\", \"cognito-idp:listusersingroup\", \"cognito-idp:updateuserpool\", \"cognito-idp:updateuserpoolclient\", \"ec2:createnetworkinterface\", \"ec2:createnetworkinterfacepermission\", \"ec2:createvpcendpoint\", \"ec2:deletenetworkinterface\", \"ec2:deletenetworkinterfacepermission\", \"ec2:describedhcpoptions\", \"ec2:describenetworkinterfaces\", \"ec2:describeroutetables\", \"ec2:describesecuritygroups\", \"ec2:describesubnets\", \"ec2:describevpcendpoints\", \"ec2:describevpcs\", \"ecr:batchchecklayeravailability\", \"ecr:batchgetimage\", \"ecr:createrepository\", \"ecr:getauthorizationtoken\", \"ecr:getdownloadurlforlayer\", \"ecr:describe*\", \"elastic-inference:connect\", \"glue:createjob\", \"glue:deletejob\", \"glue:getjob\", \"glue:getjobrun\", \"glue:getjobruns\", \"glue:getjobs\", \"glue:resetjobbookmark\", \"glue:startjobrun\", \"glue:updatejob\", \"groundtruthlabeling:*\", \"iam:listroles\", \"kms:describekey\", \"kms:listaliases\", \"lambda:listfunctions\", \"logs:createloggroup\", \"logs:createlogstream\", \"logs:describelogstreams\", \"logs:getlogevents\", \"logs:putlogevents\" ], \"resource\": \"*\" }, { \"effect\": \"allow\", \"action\": [ \"ecr:setrepositorypolicy\", \"ecr:completelayerupload\", \"ecr:batchdeleteimage\", \"ecr:uploadlayerpart\", \"ecr:deleterepositorypolicy\", \"ecr:initiatelayerupload\", \"ecr:deleterepository\", \"ecr:putimage\" ], \"resource\": \"arn:aws:ecr:*:*:repository\/**\" }, { \"effect\": \"allow\", \"action\": [ \"codecommit:gitpull\", \"codecommit:gitpush\" ], \"resource\": [ \"arn:aws:codecommit:*:*:**\", \"arn:aws:codecommit:*:*:**\", \"arn:aws:codecommit:*:*:**\" ] }, { \"effect\": \"allow\", \"action\": [ \"secretsmanager:createsecret\", \"secretsmanager:describesecret\", \"secretsmanager:listsecrets\", \"secretsmanager:tagresource\" ], \"resource\": \"*\" }, { \"effect\": \"allow\", \"action\": [ \"secretsmanager:getsecretvalue\" ], \"resource\": \"*\", \"condition\": { \"stringequals\": { \"secretsmanager:resourcetag\/\": \"true\" } } }, { \"effect\": \"allow\", \"action\": [ \"robomaker:createsimulationapplication\", \"robomaker:describesimulationapplication\", \"robomaker:deletesimulationapplication\" ], \"resource\": [ \"*\" ] }, { \"effect\": \"allow\", \"action\": [ \"robomaker:createsimulationjob\", \"robomaker:describesimulationjob\", \"robomaker:cancelsimulationjob\" ], \"resource\": [ \"*\" ] }, { \"effect\": \"allow\", \"action\": [ \"s3:getobject\", \"s3:putobject\", \"s3:deleteobject\" ], \"resource\": [ \"arn:aws:s3:::**\", \"arn:aws:s3:::**\", \"arn:aws:s3:::**\", \"arn:aws:s3:::*aws-glue*\" ] }, { \"effect\": \"allow\", \"action\": [ \"s3:createbucket\", \"s3:getbucketlocation\", \"s3:listbucket\", \"s3:listallmybuckets\" ], \"resource\": \"*\" }, { \"effect\": \"allow\", \"action\": [ \"s3:getobject\" ], \"resource\": \"*\", \"condition\": { \"stringequalsignorecase\": { \"s3:existingobjecttag\/\": \"true\" } } }, { \"effect\": \"allow\", \"action\": [ \"lambda:invokefunction\" ], \"resource\": [ \"arn:aws:lambda:*:*:function:**\", \"arn:aws:lambda:*:*:function:**\", \"arn:aws:lambda:*:*:function:**\", \"arn:aws:lambda:*:*:function:*labelingfunction*\" ] }, { \"action\": \"iam:createservicelinkedrole\", \"effect\": \"allow\", \"resource\": \"arn:aws:iam::*:role\/aws-service-role\/.application-autoscaling.amazonaws.com\/awsserviceroleforapplicationautoscaling_endpoint\", \"condition\": { \"stringlike\": { \"iam:awsservicename\": \".application-autoscaling.amazonaws.com\" } } }, { \"effect\": \"allow\", \"action\": \"iam:createservicelinkedrole\", \"resource\": \"*\", \"condition\": { \"stringequals\": { \"iam:awsservicename\": \"robomaker.amazonaws.com\" } } }, { \"effect\": \"allow\", \"action\": [ \"iam:passrole\" ], \"resource\": \"*\", \"condition\": { \"stringequals\": { \"iam:passedtoservice\": [ \".amazonaws.com\", \"glue.amazonaws.com\", \"robomaker.amazonaws.com\" ] } } } ] } the instance is in us-east-1 same as the dataset. the dataset is https:\/\/registry.opendata.aws\/fast-ai-imageclas\/",
        "Question_original_content_gpt_summary":"The user has encountered a challenge accessing AWS open data datasets, receiving an \"access denied\" error despite having the necessary permissions.",
        "Question_preprocessed_content":"Title: access denied read open data into; Content: cannot read aws open data datasets into . error is code the user has the s getobjects permission the user's permissions are the full s read policy and the full policies. the policies are the instance is in us east same as the dataset. the dataset is",
        "Answer_original_content":"thanks to matthew i looked into the permissions of the notebook itself, not just the user using . the policies on the notebook look like this and i can download from the aws open data datasets!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks to matthew i looked into the permissions of the notebook itself, not just the user using . the policies on the notebook look like this and i can download from the aws open data datasets!"
    },
    {
        "Question_id":null,
        "Question_title":"Custom Amazon SageMaker container registration and deployment tracking",
        "Question_body":"My customer asks that:\n\nContainer images must be registered and deployments tracked\n\nContainers must be registered within a private customer-owned registry prior to deployment\n\nOnly registered containers are to be deployed.\n\nPart of the registration process must include verification the containers have comes from a trusted source and that they have been scanned and found to be free of malware and vulnerabilities.\n\nAn inventory of all deployed containers must be maintained at all times.\n\nThe inventory must include: Software installed within the container version of all software and patch level . Where the container has been deployed . Owner of the container\n\nDo we do any of these? Please provide documentation on AWS\/SageMaker vs custom container provider's responsibilities.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1607710961000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":34.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdJqWN_WJQeuYrkZMikkibQ\/custom-amazon-sage-maker-container-registration-and-deployment-tracking",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-18T15:43:38.000Z",
                "Answer_score":0,
                "Answer_body":"Amazon Elastic Container Registry (Amazon ECR) enables customers to store images, secure their images using AWS Identity and Access Management (IAM), and scan their containers for vulnerabilities. Open Policy Agent (OPA) is an open-source project focused on codifying policy such as the approved image registries. OPA is integrated with Kubernetes via Gatekeeper, an admission controller that checks if the image is from an approved registry prior to allowing it to be deployed on the cluster. For more details see: https:\/\/aws.amazon.com\/blogs\/containers\/designing-a-secure-container-image-registry",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: custom container registration and deployment tracking; Content: my customer asks that: container images must be registered and deployments tracked containers must be registered within a private customer-owned registry prior to deployment only registered containers are to be deployed. part of the registration process must include verification the containers have comes from a trusted source and that they have been scanned and found to be free of malware and vulnerabilities. an inventory of all deployed containers must be maintained at all times. the inventory must include: software installed within the container version of all software and patch level . where the container has been deployed . owner of the container do we do any of these? please provide documentation on aws\/ vs custom container provider's responsibilities.",
        "Question_original_content_gpt_summary":"The user is facing the challenge of needing to register and track containers, verify their trustworthiness, and maintain an inventory of all deployed containers.",
        "Question_preprocessed_content":"Title: custom container registration and deployment tracking; Content: my customer asks that container images must be registered and deployments tracked containers must be registered within a private customer owned registry prior to deployment only registered containers are to be deployed. part of the registration process must include verification the containers have comes from a trusted source and that they have been scanned and found to be free of malware and vulnerabilities. an inventory of all deployed containers must be maintained at all times. the inventory must include software installed within the container version of all software and patch level . where the container has been deployed . owner of the container do we do any of these? please provide documentation on vs custom container provider's responsibilities.",
        "Answer_original_content":"amazon elastic container registry (amazon ecr) enables customers to store images, secure their images using aws identity and access management (iam), and scan their containers for vulnerabilities. open policy agent (opa) is an open-source project focused on codifying policy such as the approved image registries. opa is integrated with kubernetes via gatekeeper, an admission controller that checks if the image is from an approved registry prior to allowing it to be deployed on the cluster. for more details see: https:\/\/aws.amazon.com\/blogs\/containers\/designing-a-secure-container-image-registry",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"amazon elastic container registry enables customers to store images, secure their images using aws identity and access management , and scan their containers for vulnerabilities. open policy agent is an open source project focused on codifying policy such as the approved image registries. opa is integrated with kubernetes via gatekeeper, an admission controller that checks if the image is from an approved registry prior to allowing it to be deployed on the cluster. for more details see"
    },
    {
        "Question_id":67786052.0,
        "Question_title":"Log Pickle files as a part of Mlflow run",
        "Question_body":"<p>I am running an MLflow experiment as a part of it I would like to log a few artifacts as a python pickle.<\/p>\n<p>Ex: Trying out different categorical encoders, so wanted to log the encoder objects as a pickle file.<\/p>\n<p>Is there a way to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1622538922663,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":1843.0,
        "Owner_creation_time":1411361217027,
        "Owner_last_access_time":1662192778187,
        "Owner_reputation":569.0,
        "Owner_up_votes":41.0,
        "Owner_down_votes":3.0,
        "Owner_views":123.0,
        "Answer_body":"<p>There are two functions for there:<\/p>\n<ol>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifact\" rel=\"nofollow noreferrer\">log_artifact<\/a> - to log a local file or directory as an artifact<\/li>\n<li><a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.log_artifacts\" rel=\"nofollow noreferrer\">log_artifacts<\/a> - to log a contents of a local directory<\/li>\n<\/ol>\n<p>so it would be as simple as:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with mlflow.start_run():\n    mlflow.log_artifact(&quot;encoder.pickle&quot;)\n<\/code><\/pre>\n<p>And you will need to use the <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#model-customization\" rel=\"nofollow noreferrer\">custom MLflow model<\/a> to use that pickled file, something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow.pyfunc\n\nclass my_model(mlflow.pyfunc.PythonModel):\n    def __init__(self, encoders):\n        self.encoders = encoders\n\n    def predict(self, context, model_input):\n        _X = ...# do encoding using self.encoders.\n        return str(self.ctx.predict([_X])[0])\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1622542563552,
        "Answer_score":2.0,
        "Owner_location":"Bengaluru, Karnataka, India",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67786052",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: log pickle files as a part of run; Content: i am running an experiment as a part of it i would like to log a few artifacts as a python pickle. ex: trying out different categorical encoders, so wanted to log the encoder objects as a pickle file. is there a way to achieve this?",
        "Question_original_content_gpt_summary":"The user is looking for a way to log artifacts as a Python pickle file as part of an experiment.",
        "Question_preprocessed_content":"Title: log pickle files as a part of run; Content: i am running an experiment as a part of it i would like to log a few artifacts as a python pickle. ex trying out different categorical encoders, so wanted to log the encoder objects as a pickle file. is there a way to achieve this?",
        "Answer_original_content":"there are two functions for there: log_artifact - to log a local file or directory as an artifact log_artifacts - to log a contents of a local directory so it would be as simple as: with .start_run(): .log_artifact(\"encoder.pickle\") and you will need to use the custom model to use that pickled file, something like this: import .pyfunc class my_model(.pyfunc.pythonmodel): def __init__(self, encoders): self.encoders = encoders def predict(self, context, model_input): _x = ...# do encoding using self.encoders. return str(self.ctx.predict([_x])[0])",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"there are two functions for there to log a local file or directory as an artifact to log a contents of a local directory so it would be as simple as and you will need to use the custom model to use that pickled file, something like this"
    },
    {
        "Question_id":64210312.0,
        "Question_title":"\u2018precision_at_target_recall\u2019, \u2018recall_at_target_precision\u2019 on hyper parameters on AWS SageMaker , how does it train with that constraint?",
        "Question_body":"<p>[AWS SageMaker LinearLearner][1] ;  <strong>binary_classifier_model_selection_criteria<\/strong> is a hyper parameter useful without having cross validation and hyper param tuning. At least it seems to me so.<\/p>\n<p>If yes , can you please explain how a model can be trained with having that hyper parameter be set to <strong>\u2018precision_at_target_recall\u2019<\/strong> or <strong>\u2018recall_at_target_precision'<\/strong> ?<\/p>\n<p>I have not seen such a thing in scikit-learn. The only way it seems possible and reasonable to me is to play with threshold to keep it at the level of target_recall or target_precision, unfortunately nothing is mentioned about the threshold or cut off in documentation and Ii guess it is still at .5.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1601906959123,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":134.0,
        "Owner_creation_time":1588220429968,
        "Owner_last_access_time":1663138720448,
        "Owner_reputation":71.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":8.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64210312",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: \u2018precision_at_target_recall\u2019, \u2018recall_at_target_precision\u2019 on hyper parameters on , how does it train with that constraint?; Content: [ linearlearner][1] ; binary_classifier_model_selection_criteria is a hyper parameter useful without having cross validation and hyper param tuning. at least it seems to me so. if yes , can you please explain how a model can be trained with having that hyper parameter be set to \u2018precision_at_target_recall\u2019 or \u2018recall_at_target_precision' ? i have not seen such a thing in scikit-learn. the only way it seems possible and reasonable to me is to play with threshold to keep it at the level of target_recall or target_precision, unfortunately nothing is mentioned about the threshold or cut off in documentation and ii guess it is still at .5.",
        "Question_original_content_gpt_summary":"The user is trying to understand how to use the 'precision_at_target_recall' and 'recall_at_target_precision' hyperparameters to train a model without cross validation and hyperparam tuning.",
        "Question_preprocessed_content":"Title: on hyper parameters on , how does it train with that constraint?; Content: ; is a hyper parameter useful without having cross validation and hyper param tuning. at least it seems to me so. if yes , can you please explain how a model can be trained with having that hyper parameter be set to or ? i have not seen such a thing in scikit learn. the only way it seems possible and reasonable to me is to play with threshold to keep it at the level of or unfortunately nothing is mentioned about the threshold or cut off in documentation and ii guess it is still at . .",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":72738511.0,
        "Question_title":"Reinstall opencv will bug the entire project for opencv import",
        "Question_body":"<p>I was trying to install opencv-contrib-python but had trouble getting a hold of img_hash.\nWhen I try to reinstall opencv-python, it seems to mess up the entire cnoda enviornment. Is there any way to reset the environment from scratch?<\/p>\n<pre><code>import cv2\n!pip uninstall -y opencv-python\n!pip uninstall -y opencv-contrib-python\n!pip install opencv-python\n!pip install opencv-contrib-python\nhsh = cv2.img_hash.BlockMeanHash_create()\ncv2.imread('image.png')\nhsh.compute(a_1)\n<\/code><\/pre>\n<p>Error:<\/p>\n<pre><code>~\/.conda\/envs\/default\/lib\/python3.9\/site-packages\/cv2\/gapi\/__init__.py in &lt;module&gt;\n    288 \n    289 \n--&gt; 290 cv.gapi.wip.GStreamerPipeline = cv.gapi_wip_gst_GStreamerPipeline\n\nAttributeError: partially initialized module 'cv2' has no attribute 'gapi_wip_gst_GStreamerPipeline' (most likely due to a circular import)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1656039214693,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":55.0,
        "Owner_creation_time":1286151813432,
        "Owner_last_access_time":1656262247600,
        "Owner_reputation":1941.0,
        "Owner_up_votes":11.0,
        "Owner_down_votes":1.0,
        "Owner_views":157.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72738511",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: reinstall opencv will bug the entire project for opencv import; Content: i was trying to install opencv-contrib-python but had trouble getting a hold of img_hash. when i try to reinstall opencv-python, it seems to mess up the entire cnoda enviornment. is there any way to reset the environment from scratch? import cv2 !pip uninstall -y opencv-python !pip uninstall -y opencv-contrib-python !pip install opencv-python !pip install opencv-contrib-python hsh = cv2.img_hash.blockmeanhash_create() cv2.imread('image.png') hsh.compute(a_1) error: ~\/.conda\/envs\/default\/lib\/python3.9\/site-packages\/cv2\/gapi\/__init__.py in <module> 288 289 --> 290 cv.gapi.wip.gstreamerpipeline = cv.gapi_wip_gst_gstreamerpipeline attributeerror: partially initialized module 'cv2' has no attribute 'gapi_wip_gst_gstreamerpipeline' (most likely due to a circular import)",
        "Question_original_content_gpt_summary":"The user encountered a challenge when trying to reinstall opencv-python, which caused an attribute error due to a circular import.",
        "Question_preprocessed_content":"Title: reinstall opencv will bug the entire project for opencv import; Content: i was trying to install opencv contrib python but had trouble getting a hold of when i try to reinstall opencv python, it seems to mess up the entire cnoda enviornment. is there any way to reset the environment from scratch? error",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":62142825.0,
        "Question_title":"How to Enable SageMaker Debugger in the SageMaker AutoPilot",
        "Question_body":"<p>I'd like to (a) plot SHAP values out of the SageMaker (b) AutoML pipeline. To achieve (a), debugger shall be used according to: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon-sagemaker-debugger\/<\/a>.<\/p>\n\n<p>But how to enable the debug model in the AutoPilot without hacking into the background?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1591056499550,
        "Question_favorite_count":1.0,
        "Question_score":0.0,
        "Question_view_count":215.0,
        "Owner_creation_time":1392607100776,
        "Owner_last_access_time":1664069299612,
        "Owner_reputation":133.0,
        "Owner_up_votes":304.0,
        "Owner_down_votes":0.0,
        "Owner_views":29.0,
        "Answer_body":"<p>SageMaker Autopilot doesn't support SageMaker Debugger out of the box currently (as of Dec 2020). You can hack the Hyperparameter Tuning job to pass in a debug parameter.<\/p>\n<p>However, there is a way to use SHAP with Autopilot models. Take a look at this blog post explaining how to use SHAP with SageMaker Autopilot: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon-sagemaker-autopilot-models-with-shap\/<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1607117580710,
        "Answer_score":0.0,
        "Owner_location":"Sydney NSW, Australia",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62142825",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to enable debugger in the autopilot; Content: i'd like to (a) plot shap values out of the (b) automl pipeline. to achieve (a), debugger shall be used according to: https:\/\/aws.amazon.com\/blogs\/machine-learning\/ml-explainability-with-amazon--debugger\/. but how to enable the debug model in the autopilot without hacking into the background?",
        "Question_original_content_gpt_summary":"The user is looking for a way to enable the Debugger in the Autopilot to plot Shap values out of the AutoML pipeline.",
        "Question_preprocessed_content":"Title: how to enable debugger in the autopilot; Content: i'd like to plot shap values out of the automl pipeline. to achieve , debugger shall be used according to but how to enable the debug model in the autopilot without hacking into the background?",
        "Answer_original_content":"autopilot doesn't support debugger out of the box currently (as of dec 2020). you can hack the hyperparameter tuning job to pass in a debug parameter. however, there is a way to use shap with autopilot models. take a look at this blog post explaining how to use shap with autopilot: https:\/\/aws.amazon.com\/blogs\/machine-learning\/explaining-amazon--autopilot-models-with-shap\/.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"autopilot doesn't support debugger out of the box currently . you can hack the hyperparameter tuning job to pass in a debug parameter. however, there is a way to use shap with autopilot models. take a look at this blog post explaining how to use shap with autopilot"
    },
    {
        "Question_id":null,
        "Question_title":"Stopping ML Server Engine",
        "Question_body":"I'm running the Microsoft Machine Learning Server on my computer. Right now, there are no tasks\/nodes running, and task manager is showing multiple instances of the \"Microsoft ML Server Engine\" that are using nearly all my computer resources. I've gone into the administration utility, but can't seem to find a way of stopping this, short of brute \"End Task\" within TM.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1612666795983,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/262003\/stopping-ml-server-engine.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-07T04:05:59.56Z",
                "Answer_score":1,
                "Answer_body":"HI @ThomasGraham-8689\n\nIn Machine Learning Server 9.3 and later, you can use admin extension of the Azure Command-Line Interface (Azure CLI) to set up and manage your configuration, including stopping and starting services.\nMonitor, stop, and start web & compute nodes\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: stopping ml server engine; Content: i'm running the microsoft machine learning server on my computer. right now, there are no tasks\/nodes running, and task manager is showing multiple instances of the \"microsoft ml server engine\" that are using nearly all my computer resources. i've gone into the administration utility, but can't seem to find a way of stopping this, short of brute \"end task\" within tm.",
        "Question_original_content_gpt_summary":"The user is struggling to find a way to stop the Microsoft Machine Learning Server Engine from consuming their computer resources without resorting to brute force.",
        "Question_preprocessed_content":"Title: stopping ml server engine; Content: i'm running the microsoft machine learning server on my computer. right now, there are no running, and task manager is showing multiple instances of the microsoft ml server engine that are using nearly all my computer resources. i've gone into the administration utility, but can't seem to find a way of stopping this, short of brute end task within tm.",
        "Answer_original_content":"hi @thomasgraham-8689 in machine learning server 9.3 and later, you can use admin extension of the azure command-line interface (azure cli) to set up and manage your configuration, including stopping and starting services. monitor, stop, and start web & compute nodes if the answer is helpful, please click accept answer and up-vote, this can be beneficial to other community members.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi in machine learning server and later, you can use admin extension of the azure command line interface to set up and manage your configuration, including stopping and starting services. monitor, stop, and start web & compute nodes if the answer is helpful, please click accept answer and up vote, this can be beneficial to other community members."
    },
    {
        "Question_id":71351821.0,
        "Question_title":"Reading File from Vertex AI and Google Cloud Storage",
        "Question_body":"<p>I am trying to set up a pipeline in GCP\/Vertex AI and am having a lot of trouble. The pipeline is being written using Kubeflow Pipelines and has many different components, one thing in particular is giving me trouble however. Eventually I want to launch this from a Cloud Function with the help of the Cloud Scheduler.<\/p>\n<p>The part that is giving me issues is fairly simple and I believe I just need some form of introduction to how I should be thinking about this setup. I simply want to read and write from files (might be .csv, .txt or similar). I imagine that the analog to the filesystem on my local machine in GCP is the Cloud Storage so this is where I have been trying to read from for the time being (please correct me if I'm wrong). The component I've built is a blatant rip-off of <a href=\"https:\/\/stackoverflow.com\/questions\/48279061\/gcs-read-a-text-file-from-google-cloud-storage-directly-into-python\">this<\/a> post and looks like this.<\/p>\n<pre><code>@component(\n    packages_to_install=[&quot;google-cloud&quot;],\n    base_image=&quot;python:3.9&quot;\n)\n\n\ndef main(\n):\n    import csv\n    from io import StringIO\n\n    from google.cloud import storage\n\n    BUCKET_NAME = &quot;gs:\/\/my_bucket&quot;\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(BUCKET_NAME)\n\n    blob = bucket.blob('test\/test.txt')\n    blob = blob.download_as_string()\n    blob = blob.decode('utf-8')\n\n    blob = StringIO(blob)  #tranform bytes to string here\n\n    names = csv.reader(blob)  #then use csv library to read the content\n    for name in names:\n        print(f&quot;First Name: {name[0]}&quot;)\n<\/code><\/pre>\n<p>The error I'm getting looks like the following:<\/p>\n<pre><code>google.api_core.exceptions.NotFound: 404 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/gs:\/\/pipeline_dev?projection=noAcl&amp;prettyPrint=false: Not Found\n<\/code><\/pre>\n<p>What's going wrong in my brain? I get the feeling that it shouldn't be this difficult to read and write files. I must be missing something fundamental? Any help is highly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5.0,
        "Question_creation_time":1646398826330,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1298.0,
        "Owner_creation_time":1407245826703,
        "Owner_last_access_time":1663849819648,
        "Owner_reputation":745.0,
        "Owner_up_votes":210.0,
        "Owner_down_votes":6.0,
        "Owner_views":168.0,
        "Answer_body":"<p>Try specifying bucket name w\/o a gs:\/\/. This should fix the issue. One more stackoverflow post that says the same thing: <a href=\"https:\/\/stackoverflow.com\/questions\/53436615\/cloud-storage-python-client-fails-to-retrieve-bucket\">Cloud Storage python client fails to retrieve bucket<\/a><\/p>\n<p>any storage bucket you try to access in GCP has a unique address to access it. That address starts with a gs:\/\/ always which specifies that it is a cloud storage url. Now, GCS apis are designed such that they need the bucket name only to work with it. Hence, you just pass the bucket name. If you were accessing the bucket via browser you will need the complete address to access and hence the gs:\/\/ prefix as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1646582537360,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71351821",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: reading file from and google cloud storage; Content: i am trying to set up a pipeline in gcp\/ and am having a lot of trouble. the pipeline is being written using kubeflow pipelines and has many different components, one thing in particular is giving me trouble however. eventually i want to launch this from a cloud function with the help of the cloud scheduler. the part that is giving me issues is fairly simple and i believe i just need some form of introduction to how i should be thinking about this setup. i simply want to read and write from files (might be .csv, .txt or similar). i imagine that the analog to the filesystem on my local machine in gcp is the cloud storage so this is where i have been trying to read from for the time being (please correct me if i'm wrong). the component i've built is a blatant rip-off of this post and looks like this. @component( packages_to_install=[\"google-cloud\"], base_image=\"python:3.9\" ) def main( ): import csv from io import stringio from google.cloud import storage bucket_name = \"gs:\/\/my_bucket\" storage_client = storage.client() bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob('test\/test.txt') blob = blob.download_as_string() blob = blob.decode('utf-8') blob = stringio(blob) #tranform bytes to string here names = csv.reader(blob) #then use csv library to read the content for name in names: print(f\"first name: {name[0]}\") the error i'm getting looks like the following: google.api_core.exceptions.notfound: 404 get https:\/\/storage.googleapis.com\/storage\/v1\/b\/gs:\/\/pipeline_dev?projection=noacl&prettyprint=false: not found what's going wrong in my brain? i get the feeling that it shouldn't be this difficult to read and write files. i must be missing something fundamental? any help is highly appreciated.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with setting up a pipeline in GCP, specifically with reading and writing files from Google Cloud Storage.",
        "Question_preprocessed_content":"Title: reading file from and google cloud storage; Content: i am trying to set up a pipeline in ai and am having a lot of trouble. the pipeline is being written using kubeflow pipelines and has many different components, one thing in particular is giving me trouble however. eventually i want to launch this from a cloud function with the help of the cloud scheduler. the part that is giving me issues is fairly simple and i believe i just need some form of introduction to how i should be thinking about this setup. i simply want to read and write from files . i imagine that the analog to the filesystem on my local machine in gcp is the cloud storage so this is where i have been trying to read from for the time being . the component i've built is a blatant rip off of this post and looks like this. the error i'm getting looks like the following what's going wrong in my brain? i get the feeling that it shouldn't be this difficult to read and write files. i must be missing something fundamental? any help is highly appreciated.",
        "Answer_original_content":"try specifying bucket name w\/o a gs:\/\/. this should fix the issue. one more stackoverflow post that says the same thing: cloud storage python client fails to retrieve bucket any storage bucket you try to access in gcp has a unique address to access it. that address starts with a gs:\/\/ always which specifies that it is a cloud storage url. now, gcs apis are designed such that they need the bucket name only to work with it. hence, you just pass the bucket name. if you were accessing the bucket via browser you will need the complete address to access and hence the gs:\/\/ prefix as well.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"try specifying bucket name a this should fix the issue. one more stackoverflow post that says the same thing cloud storage python client fails to retrieve bucket any storage bucket you try to access in gcp has a unique address to access it. that address starts with a always which specifies that it is a cloud storage url. now, gcs apis are designed such that they need the bucket name only to work with it. hence, you just pass the bucket name. if you were accessing the bucket via browser you will need the complete address to access and hence the prefix as well."
    },
    {
        "Question_id":null,
        "Question_title":"Deploying Machine Learning Modules to IoT Edge running on Windows",
        "Question_body":"I'm building an IoT device that needs to run on Windows. I'm aggregating data for analytics and would like to build a Machine Learning model. I've seen that IoT Edge with Windows Containers doesn't support Azure Machine Learning and won't be able to run modules written in Python. What options do you recommend for developing machine learning modules for Windows containers? Is there a timeline for IoT Edge supporting python in Windows containers? Or supporting Azure Machine Learning? I saw a forum post back in early 2019 from a Project Manager that said both were planned features and I don't want to spend time\/money developing a C# module if a Python update is around the corner and I just have to wait for deployment",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1604258300613,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/147408\/deploying-machine-learning-modules-to-iot-edge-run.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-02T23:17:10.847Z",
                "Answer_score":0,
                "Answer_body":"Hello @bbrsa welcome to Microsoft Q&A!\n\nCan you help us with your research so far - links to the info you encountered so that we can validate they stand true?\n\n\"Windows Containers doesn't support Azure Machine Learning\"\n\n\n\"Windows devices won't be able to run modules written in Python\"\n\n\n\"...saw a forum post back in early 2019 from a Project Manager that said both were planned features...\"\n\nFinally, can you consider running Linux Containers on a Windows Device? See: https:\/\/docs.microsoft.com\/en-us\/azure\/iot-edge\/support\nThanks",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deploying machine learning modules to iot edge running on windows; Content: i'm building an iot device that needs to run on windows. i'm aggregating data for analytics and would like to build a machine learning model. i've seen that iot edge with windows containers doesn't support and won't be able to run modules written in python. what options do you recommend for developing machine learning modules for windows containers? is there a timeline for iot edge supporting python in windows containers? or supporting ? i saw a forum post back in early 2019 from a project manager that said both were planned features and i don't want to spend time\/money developing a c# module if a python update is around the corner and i just have to wait for deployment",
        "Question_original_content_gpt_summary":"The user is facing challenges in deploying machine learning modules to an IoT Edge running on Windows, as Windows containers do not currently support modules written in Python.",
        "Question_preprocessed_content":"Title: deploying machine learning modules to iot edge running on windows; Content: i'm building an iot device that needs to run on windows. i'm aggregating data for analytics and would like to build a machine learning model. i've seen that iot edge with windows containers doesn't support and won't be able to run modules written in python. what options do you recommend for developing machine learning modules for windows containers? is there a timeline for iot edge supporting python in windows containers? or supporting ? i saw a forum post back in early from a project manager that said both were planned features and i don't want to spend developing a c module if a python update is around the corner and i just have to wait for deployment",
        "Answer_original_content":"hello @bbrsa welcome to microsoft q&a! can you help us with your research so far - links to the info you encountered so that we can validate they stand true? \"windows containers doesn't support \" \"windows devices won't be able to run modules written in python\" \"...saw a forum post back in early 2019 from a project manager that said both were planned features...\" finally, can you consider running linux containers on a windows device? see: https:\/\/docs.microsoft.com\/en-us\/azure\/iot-edge\/support thanks",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello welcome to microsoft q&a! can you help us with your research so far links to the you encountered so that we can validate they stand true? windows containers doesn't support windows devices won't be able to run modules written in python a forum post back in early from a project manager that said both were planned finally, can you consider running linux containers on a windows device? see thanks"
    },
    {
        "Question_id":55277334.0,
        "Question_title":"How can I register in Azure ML Service a machine learning model trained locally?",
        "Question_body":"<p>I am trying out <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-service\/\" rel=\"nofollow noreferrer\">Azure Machine Learning Service<\/a> for ML deployment.<\/p>\n\n<p>I have already trained a model on a compute VM and saved it as pickle, and now would like to deploy it (I am using Python on Azure notebooks for the purpose as of now).<\/p>\n\n<p>From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#register-model\" rel=\"nofollow noreferrer\">guide<\/a>, it looks like I need to I need a <code>run<\/code> object to be existing in my session to execute the \"model registration\" step:<\/p>\n\n<pre><code># register model \nmodel = run.register_model(model_name='my_model', model_path='outputs\/my_model.pkl')\nprint(model.name, model.id, model.version, sep = '\\t')\n<\/code><\/pre>\n\n<p>However, I haven't created any <code>run<\/code> object as I haven't <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#submit-the-job-to-the-cluster\" rel=\"nofollow noreferrer\">executed<\/a> any experiment for training, I am just starting off with my pickled model.<\/p>\n\n<p>I also tried to register a model by uploading it via the Azure Portal (see screenshot below), but (as the model file is quite large, I assume) it fails with a <code>ajax error 413.<\/code> as in <a href=\"https:\/\/stackoverflow.com\/questions\/55064123\/unable-to-register-an-onnx-model-in-azure-machine-learning-service-workspace\">Unable to register an ONNX model in azure machine learning service workspace<\/a>.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tVKcV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tVKcV.png\" alt=\"model registering\"><\/a> <\/p>\n\n<p>Is there any way to register and then deploy a pretrained pickled mode (without the need of submitting a <code>run<\/code>, if that makes sense)?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1553160837153,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":1466.0,
        "Owner_creation_time":1415722650716,
        "Owner_last_access_time":1664051478172,
        "Owner_reputation":4811.0,
        "Owner_up_votes":376.0,
        "Owner_down_votes":73.0,
        "Owner_views":713.0,
        "Answer_body":"<p>Model registration can be done with <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">Model.register<\/a>, without the need of using a <code>run<\/code> object<\/p>\n\n<pre><code>model = Model.register(model_name='my_model', model_path='my_model.pkl', workspace = ws)\n<\/code><\/pre>\n\n<p>for the deployment one can follow steps as outlined in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-as-a-web-service\" rel=\"nofollow noreferrer\">Azure ML service doc<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1553175443707,
        "Answer_score":0.0,
        "Owner_location":"Verona, VR, Italy",
        "Question_last_edit_time":1554642485923,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55277334",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i register in service a machine learning model trained locally?; Content: i am trying out service for ml deployment. i have already trained a model on a compute vm and saved it as pickle, and now would like to deploy it (i am using python on azure notebooks for the purpose as of now). from the guide, it looks like i need to i need a run object to be existing in my session to execute the \"model registration\" step: # register model model = run.register_model(model_name='my_model', model_path='outputs\/my_model.pkl') print(model.name, model.id, model.version, sep = '\\t') however, i haven't created any run object as i haven't executed any experiment for training, i am just starting off with my pickled model. i also tried to register a model by uploading it via the azure portal (see screenshot below), but (as the model file is quite large, i assume) it fails with a ajax error 413. as in unable to register an onnx model in service workspace. is there any way to register and then deploy a pretrained pickled mode (without the need of submitting a run, if that makes sense)?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in registering and deploying a pre-trained pickled model without submitting a run in Azure Service Workspace.",
        "Question_preprocessed_content":"Title: how can i register in service a machine learning model trained locally?; Content: i am trying out service for ml deployment. i have already trained a model on a compute vm and saved it as pickle, and now would like to deploy it . from the guide, it looks like i need to i need a object to be existing in my session to execute the model registration step however, i haven't created any object as i haven't executed any experiment for training, i am just starting off with my pickled model. i also tried to register a model by uploading it via the azure portal , but it fails with a as in unable to register an onnx model in service workspace. is there any way to register and then deploy a pretrained pickled mode ?",
        "Answer_original_content":"model registration can be done with model.register, without the need of using a run object model = model.register(model_name='my_model', model_path='my_model.pkl', workspace = ws) for the deployment one can follow steps as outlined in the service doc.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"model registration can be done with without the need of using a object for the deployment one can follow steps as outlined in the service doc."
    },
    {
        "Question_id":64651724.0,
        "Question_title":"No Module named pyarrow",
        "Question_body":"<p>I installed pyarrow using this command &quot;conda install pyarrow&quot;.\nI am running a sagemaker notebook and I am getting the error no module named pyarrow.\nI have python 3.8.3 installed on mac.<\/p>\n<p>I have numpy  1.18.5 , pandas 1.0.5 and pyarrow  0.15.1<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1604343780117,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":540.0,
        "Owner_creation_time":1468599558956,
        "Owner_last_access_time":1658530343460,
        "Owner_reputation":107.0,
        "Owner_up_votes":17.0,
        "Owner_down_votes":0.0,
        "Owner_views":26.0,
        "Answer_body":"<p>I have not yet used AWS Sagemaker notebooks, but they may be similar to GCP 'AI Platform notebooks', which I have used quite extensively. Additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks (whether from command line or from GUI)?<\/p>\n<p>In GCP, I defaulted to using <code>pip install<\/code> for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments.<\/p>\n<p>Assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the <code>CONDA<\/code> tab and selecting which notebook uses which conda environment.\n-Spencer<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1604344726272,
        "Answer_score":0.0,
        "Owner_location":"San Francisco, CA, USA",
        "Question_last_edit_time":1604344440696,
        "Answer_last_edit_time":1604361341763,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64651724",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: no module named pyarrow; Content: i installed pyarrow using this command \"conda install pyarrow\". i am running a notebook and i am getting the error no module named pyarrow. i have python 3.8.3 installed on mac. i have numpy 1.18.5 , pandas 1.0.5 and pyarrow 0.15.1 thanks",
        "Question_original_content_gpt_summary":"The user is encountering an error of \"no module named pyarrow\" while running a notebook despite having installed pyarrow using the command \"conda install pyarrow\" and having the necessary versions of Python, Numpy, and Pandas installed.",
        "Question_preprocessed_content":"Title: no module named pyarrow; Content: i installed pyarrow using this command conda install pyarrow . i am running a notebook and i am getting the error no module named pyarrow. i have python installed on mac. i have numpy , pandas and pyarrow thanks",
        "Answer_original_content":"i have not yet used notebooks, but they may be similar to gcp 'ai platform notebooks', which i have used quite extensively. additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks (whether from command line or from gui)? in gcp, i defaulted to using pip install for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments. assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the conda tab and selecting which notebook uses which conda environment. -spencer",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i have not yet used notebooks, but they may be similar to gcp 'ai platform notebooks', which i have used quite extensively. additionally, if you're experiencing additional problems, could you describe how you're launching the notebooks ? in gcp, i defaulted to using for my packages, as the conda environments were a bit finicky and didn't provide much support when creating notebooks sourced from my own created conda environments. assuming you're installing conda into your base directory, when you launch jupyter notebooks, this should be the default conda environment, else if you installed to a separate conda environment, you should be able to change this within jupyter notebooks using the tab and selecting which notebook uses which conda environment. spencer"
    },
    {
        "Question_id":70738638.0,
        "Question_title":"sagemaker.image_uris.retrieve: FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packa",
        "Question_body":"<p>I have an issue while getting xgboost image URI. It is a function for generating ECR image URIs for pre-built SageMaker Docker images.<\/p>\n<p>My code:<\/p>\n<p><code>region = sagemaker.Session().boto_region_name container=sagemaker.image_uris.retrieve(&quot;xgboost&quot;, region, &quot;1.2-1&quot;) <\/code><\/p>\n<p>Output:FileNotFoundError: [Errno 2] No such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/image_uri_config\/xgboost.json'<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1642409290550,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":392.0,
        "Owner_creation_time":1568734503088,
        "Owner_last_access_time":1663922598072,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70738638",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: .image_uris.retrieve: filenotfounderror: [errno 2] no such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packa; Content: i have an issue while getting xgboost image uri. it is a function for generating ecr image uris for pre-built docker images. my code: region = .session().boto_region_name container=.image_uris.retrieve(\"xgboost\", region, \"1.2-1\") output:filenotfounderror: [errno 2] no such file or directory: '\/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/\/image_uri_config\/xgboost.json'",
        "Question_original_content_gpt_summary":"The user encountered a FileNotFoundError when attempting to use the .image_uris.retrieve function to generate an ECR image URI for a pre-built Docker image.",
        "Question_preprocessed_content":"Title: filenotfounderror no such file or directory; Content: i have an issue while getting xgboost image uri. it is a function for generating ecr image uris for pre built docker images. my code output filenotfounderror no such file or directory",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Data versioning of databases",
        "Question_body":"<p>Hi,<\/p>\n<p>Are you thinking of supporting data versioning of databases.<br>\nTracking changes of transformations from raw data to cleaned data?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1522998440046,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":543.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/data-versioning-of-databases\/20",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2018-04-06T14:06:45.113Z",
                "Answer_body":"<p>Hi!<\/p>\n<p>Could you please elaborate on the scenario you have in mind?<br>\nWhat kind of tracking are you thinking of?<\/p>\n<p>Dvc can track the way you cleanup your data by storing input, output and a command that you use. You can then replace the input with the new version, run <code>dvc repro<\/code> and have it and the new version of cleaned up data tracked by dvc.<\/p>",
                "Answer_score":8.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: data versioning of databases; Content: hi, are you thinking of supporting data versioning of databases. tracking changes of transformations from raw data to cleaned data? thanks",
        "Question_original_content_gpt_summary":"The user is inquiring about the possibility of supporting data versioning of databases to track changes of transformations from raw data to cleaned data.",
        "Question_preprocessed_content":"Title: data versioning of databases; Content: hi, are you thinking of supporting data versioning of databases. tracking changes of transformations from raw data to cleaned data? thanks",
        "Answer_original_content":"hi! could you please elaborate on the scenario you have in mind? what kind of tracking are you thinking of? can track the way you cleanup your data by storing input, output and a command that you use. you can then replace the input with the new version, run repro and have it and the new version of cleaned up data tracked by .",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi! could you please elaborate on the scenario you have in mind? what kind of tracking are you thinking of? can track the way you cleanup your data by storing input, output and a command that you use. you can then replace the input with the new version, run and have it and the new version of cleaned up data tracked by ."
    },
    {
        "Question_id":66541985.0,
        "Question_title":"'WANDB_MODE' is not recognized as an internal or external command, operable program or batch file",
        "Question_body":"<p>I am trying to run the custom <a href=\"https:\/\/github.com\/ultralytics\/yolov5\" rel=\"nofollow noreferrer\">yolo model<\/a> on my data set in my local machine. I am following some reference code from the kaggle platform. Here first time I encounter the <code>wandb<\/code> frame work. while doing so I use the following to run the <code>train.py <\/code> file in my jupyter lab.<\/p>\n<pre><code>!WANDB_MODE=&quot;dryrun&quot; python train.py --img 640 --batch 16 --epochs 30 --data D:\/Anil\/Shawn_Research\/Iamge_DataSet\/VinBigData\/New_Direct\/vinbigdata.yaml --weights yolov5x.pt --cache\n<\/code><\/pre>\n<p>This work fine on the kaggle platform but in my local machine it shows following:<\/p>\n<pre><code>'WANDB_MODE' is not recognized as an internal or external command, operable program or batch file.\n<\/code><\/pre>\n<p>While reading the similar thread I realized I might making mistake related to path variable or Environment variable.\nEven I tried to get solution from the <a href=\"https:\/\/docs.wandb.ai\/library\/environment-variables\" rel=\"nofollow noreferrer\">official document<\/a> but couldn't figure out.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1615272052933,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":263.0,
        "Owner_creation_time":1490268584216,
        "Owner_last_access_time":1663920632156,
        "Owner_reputation":37.0,
        "Owner_up_votes":101.0,
        "Owner_down_votes":0.0,
        "Owner_views":20.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1661850709696,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66541985",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: '_mode' is not recognized as an internal or external command, operable program or batch file; Content: i am trying to run the custom yolo model on my data set in my local machine. i am following some reference code from the kaggle platform. here first time i encounter the frame work. while doing so i use the following to run the train.py file in my jupyter lab. !_mode=\"dryrun\" python train.py --img 640 --batch 16 --epochs 30 --data d:\/anil\/shawn_research\/iamge_dataset\/vinbigdata\/new_direct\/vinbigdata.yaml --weights yolov5x.pt --cache this work fine on the kaggle platform but in my local machine it shows following: '_mode' is not recognized as an internal or external command, operable program or batch file. while reading the similar thread i realized i might making mistake related to path variable or environment variable. even i tried to get solution from the official document but couldn't figure out. thanks in advance.",
        "Question_original_content_gpt_summary":"The user is encountering an issue with running a custom YOLO model on their local machine, where they are receiving an error message stating that '_mode' is not recognized as an internal or external command, operable program or batch file.",
        "Question_preprocessed_content":"Title: is not recognized as an internal or external command, operable program or batch file; Content: i am trying to run the custom yolo model on my data set in my local machine. i am following some reference code from the kaggle platform. here first time i encounter the frame work. while doing so i use the following to run the file in my jupyter lab. this work fine on the kaggle platform but in my local machine it shows following while reading the similar thread i realized i might making mistake related to path variable or environment variable. even i tried to get solution from the official document but couldn't figure out. thanks in advance.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Machine Learning \/ Data Science",
        "Question_body":"Qual curso e indicado para iniciantes em Machine Learning e Data Science?",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1589856030190,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/27823\/machine-learning-data-science.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-19T07:13:52.087Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nQ&A currently supports the products listed over here https:\/\/docs.microsoft.com\/en-us\/answers\/products (more to be added later on).\n\nYou can reach the experts in the dedicated Microsoft Certification forum for courses over here:\nhttps:\/\/trainingsupport.microsoft.com\/en-us\/mcp\/forum\/mcp_courses\n\n(Please don't forget to mark helpful replies as answer)\n\nBest regards,\nLeon",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-05-19T07:51:06.393Z",
                "Answer_score":2,
                "Answer_body":"The good start is a Microsoft AI School which is available here: https:\/\/aischool.microsoft.com\/en-us",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-26T08:45:40.47Z",
                "Answer_score":0,
                "Answer_body":"Hi @ DiegoLuczk-7370,\n\nI would recommend to get prepared with the help of any of modern introductory books on Data Science like this one or this. Also, apart from the Microsoft AI School mentioned above, take a look at Udemy and Coursera beginner courses.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: machine learning \/ data science; Content: qual curso e indicado para iniciantes em machine learning e data science?",
        "Question_original_content_gpt_summary":"The user is seeking advice on which course to take to begin learning machine learning and data science.",
        "Question_preprocessed_content":"Title: machine learning \/ data science; Content: qual curso e indicado para iniciantes em machine learning e data science?",
        "Answer_original_content":"hi, q&a currently supports the products listed over here https:\/\/docs.microsoft.com\/en-us\/answers\/products (more to be added later on). you can reach the experts in the dedicated microsoft certification forum for courses over here: https:\/\/trainingsupport.microsoft.com\/en-us\/mcp\/forum\/mcp_courses (please don't forget to mark helpful replies as answer) best regards, leon the good start is a microsoft ai school which is available here: https:\/\/aischool.microsoft.com\/en-us hi @ diegoluczk-7370, i would recommend to get prepared with the help of any of modern introductory books on data science like this one or this. also, apart from the microsoft ai school mentioned above, take a look at udemy and coursera beginner courses.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, q&a currently supports the products listed over here . you can reach the experts in the dedicated microsoft certification forum for courses over here please don't forget to mark helpful replies as answer best regards, leon the good start is a microsoft ai school which is available here hi @ diegoluczk , i would recommend to get prepared with the help of any of modern introductory books on data science like this one or this. also, apart from the microsoft ai school mentioned above, take a look at udemy and coursera beginner courses."
    },
    {
        "Question_id":null,
        "Question_title":"Program on VM automatically crash after long idle",
        "Question_body":"Hi,\n\nI am training machine learning model on Azure VM with NC6 promo GPU. Everything was fine at the beginning, but after a while I went back to check and realized my training program was stopped. Also, I got this message \"client_loop: send disconnect: Broken pipe\". Is there any solution for this problem since it cost me a lot of time and money.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1615873089030,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/315909\/program-on-vm-automatically-crash-after-long-idle.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-16T11:31:53.397Z",
                "Answer_score":0,
                "Answer_body":"@LeNguyenMinhHuy-2051 Thanks for the question. We have forwarded to the product team to check on this issue. You can try the following.\n\nThe sshd\/server settings in \/etc\/ssh\/sshd_config :\nTCPKeepAlive yes\nClientAliveInterval 60\nClientAliveCountMax 40000\n\nAND\n\nthe ssh\/client setting in ~\/.ssh\/config :\nServerAliveInterval 60\n\n\n\n\nWe would recommend to raise a Azure support desk ticket from Help+Support blade from Azure portal for your service resource. This will help you to share the details securely and work with an engineer who can provide more insights about the issue that if it can be replicated.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: program on vm automatically crash after long idle; Content: hi, i am training machine learning model on azure vm with nc6 promo gpu. everything was fine at the beginning, but after a while i went back to check and realized my training program was stopped. also, i got this message \"client_loop: send disconnect: broken pipe\". is there any solution for this problem since it cost me a lot of time and money.",
        "Question_original_content_gpt_summary":"The user encountered a challenge where their machine learning model training program on an Azure VM with an NC6 promo GPU was automatically crashing after a long idle period.",
        "Question_preprocessed_content":"Title: program on vm automatically crash after long idle; Content: hi, i am training machine learning model on azure vm with nc promo gpu. everything was fine at the beginning, but after a while i went back to check and realized my training program was stopped. also, i got this message send disconnect broken pipe . is there any solution for this problem since it cost me a lot of time and money.",
        "Answer_original_content":"@lenguyenminhhuy-2051 thanks for the question. we have forwarded to the product team to check on this issue. you can try the following. the sshd\/server settings in \/etc\/ssh\/sshd_config : tcpkeepalive yes clientaliveinterval 60 clientalivecountmax 40000 and the ssh\/client setting in ~\/.ssh\/config : serveraliveinterval 60 we would recommend to raise a azure support desk ticket from help+support blade from azure portal for your service resource. this will help you to share the details securely and work with an engineer who can provide more insights about the issue that if it can be replicated.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the question. we have forwarded to the product team to check on this issue. you can try the following. the settings in tcpkeepalive yes clientaliveinterval clientalivecountmax and the setting in serveraliveinterval we would recommend to raise a azure support desk ticket from help+support blade from azure portal for your service resource. this will help you to share the details securely and work with an engineer who can provide more insights about the issue that if it can be replicated."
    },
    {
        "Question_id":59826799.0,
        "Question_title":"Unable to download artifacts from FTP server using MLFLOW",
        "Question_body":"<p>I'm not able to load my sklearn model using <code>mlflow.sklearn.load_model<\/code>. Internally, <code>mlflow<\/code> uses the function <code>_download_artifact_from_uri<\/code> from the module <code>mlflow.tracking.artifact_utils<\/code>.<\/p>\n\n<p>If I try, to download an entire artifact folder I receive the following error message: <code>PermissionError: [Errno 13] Permission denied: '\/0'<\/code>. <\/p>\n\n<p>If I try to retrieve a single file from an artifact folder I do not get the error message, and I'm able to create a folder using the <code>os<\/code> module. <\/p>\n\n<p>The following is the converted jupyter notebook I've used.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import os\n\nimport mlflow\nfrom mlflow.tracking.artifact_utils import _download_artifact_from_uri\n<\/code><\/pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.set_tracking_uri(\"file:mlruns\")\n<\/code><\/pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>artifact_uri = 'ftp:\/\/user:pass@ftp\/0\/69a874f1f8a6474cae6bca5b3b5f9ffc\/artifacts'\n<\/code><\/pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>model_uri = 'ftp:\/\/user:pass@ftp\/0\/25f46678f1d44842910f185672ca852c\/artifacts\/linear model\/model\/MLmodel'\n<\/code><\/pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>_download_artifact_from_uri(artifact_uri, \".\/mlruns\")\n<\/code><\/pre>\n\n<pre><code>---------------------------------------------------------------------------\n\nPermissionError                           Traceback (most recent call last)\n\n&lt;ipython-input-14-834201128eef&gt; in &lt;module&gt;\n----&gt; 1 _download_artifact_from_uri(artifact_uri, \".\/mlruns\")\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/tracking\/artifact_utils.py in _download_artifact_from_uri(artifact_uri, output_path)\n     73 \n     74     return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\n---&gt; 75         artifact_path=artifact_path, dst_path=output_path)\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py in download_artifacts(self, artifact_path, dst_path)\n    135         # Check if the artifacts points to a directory\n    136         if self._is_directory(artifact_path):\n--&gt; 137             return download_artifact_dir(artifact_path)\n    138         else:\n    139             return download_file(artifact_path)\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py in download_artifact_dir(dir_path)\n    116                 for file_info in dir_content:\n    117                     if file_info.is_dir:\n--&gt; 118                         download_artifact_dir(dir_path=file_info.path)\n    119                     else:\n    120                         download_file(file_info.path)\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py in download_artifact_dir(dir_path)\n    116                 for file_info in dir_content:\n    117                     if file_info.is_dir:\n--&gt; 118                         download_artifact_dir(dir_path=file_info.path)\n    119                     else:\n    120                         download_file(file_info.path)\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py in download_artifact_dir(dir_path)\n    118                         download_artifact_dir(dir_path=file_info.path)\n    119                     else:\n--&gt; 120                         download_file(file_info.path)\n    121             return local_dir\n    122         if not os.path.exists(dst_path):\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/mlflow\/store\/artifact\/artifact_repo.py in download_file(fullpath)\n    101             local_file_path = os.path.join(dst_path, fullpath)\n    102             if not os.path.exists(local_dir_path):\n--&gt; 103                 os.makedirs(local_dir_path)\n    104             self._download_file(remote_file_path=fullpath, local_path=local_file_path)\n    105             return local_file_path\n\n\n\/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok)\n    209     if head and tail and not path.exists(head):\n    210         try:\n--&gt; 211             makedirs(head, exist_ok=exist_ok)\n    212         except FileExistsError:\n    213             # Defeats race condition when another thread created the path\n\n\n\/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok)\n    209     if head and tail and not path.exists(head):\n    210         try:\n--&gt; 211             makedirs(head, exist_ok=exist_ok)\n    212         except FileExistsError:\n    213             # Defeats race condition when another thread created the path\n\n\n\/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok)\n    209     if head and tail and not path.exists(head):\n    210         try:\n--&gt; 211             makedirs(head, exist_ok=exist_ok)\n    212         except FileExistsError:\n    213             # Defeats race condition when another thread created the path\n\n\n\/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok)\n    209     if head and tail and not path.exists(head):\n    210         try:\n--&gt; 211             makedirs(head, exist_ok=exist_ok)\n    212         except FileExistsError:\n    213             # Defeats race condition when another thread created the path\n\n\n\/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok)\n    219             return\n    220     try:\n--&gt; 221         mkdir(name, mode)\n    222     except OSError:\n    223         # Cannot rely on checking for EEXIST, since the operating system\n\n\nPermissionError: [Errno 13] Permission denied: '\/0'\n<\/code><\/pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>_download_artifact_from_uri(model_uri, \".\/mlruns\")\n<\/code><\/pre>\n\n<pre><code>'\/home\/jovyan\/notebooks\/mlruns\/MLmodel'\n<\/code><\/pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>os.mkdir(\"mlruns\/0\")\n<\/code><\/pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1579535536323,
        "Question_favorite_count":1.0,
        "Question_score":4.0,
        "Question_view_count":1099.0,
        "Owner_creation_time":1579535098392,
        "Owner_last_access_time":1645077386408,
        "Owner_reputation":41.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":6.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Copenhagen, Denmark",
        "Question_last_edit_time":1593685255692,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59826799",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unable to download artifacts from ftp server using ; Content: i'm not able to load my sklearn model using .sklearn.load_model. internally, uses the function _download_artifact_from_uri from the module .tracking.artifact_utils. if i try, to download an entire artifact folder i receive the following error message: permissionerror: [errno 13] permission denied: '\/0'. if i try to retrieve a single file from an artifact folder i do not get the error message, and i'm able to create a folder using the os module. the following is the converted jupyter notebook i've used. import os import from .tracking.artifact_utils import _download_artifact_from_uri .set_tracking_uri(\"file:mlruns\") artifact_uri = 'ftp:\/\/user:pass@ftp\/0\/69a874f1f8a6474cae6bca5b3b5f9ffc\/artifacts' model_uri = 'ftp:\/\/user:pass@ftp\/0\/25f46678f1d44842910f185672ca852c\/artifacts\/linear model\/model\/mlmodel' _download_artifact_from_uri(artifact_uri, \".\/mlruns\") --------------------------------------------------------------------------- permissionerror traceback (most recent call last) <ipython-input-14-834201128eef> in <module> ----> 1 _download_artifact_from_uri(artifact_uri, \".\/mlruns\") \/opt\/conda\/lib\/python3.7\/site-packages\/\/tracking\/artifact_utils.py in _download_artifact_from_uri(artifact_uri, output_path) 73 74 return get_artifact_repository(artifact_uri=root_uri).download_artifacts( ---> 75 artifact_path=artifact_path, dst_path=output_path) \/opt\/conda\/lib\/python3.7\/site-packages\/\/store\/artifact\/artifact_repo.py in download_artifacts(self, artifact_path, dst_path) 135 # check if the artifacts points to a directory 136 if self._is_directory(artifact_path): --> 137 return download_artifact_dir(artifact_path) 138 else: 139 return download_file(artifact_path) \/opt\/conda\/lib\/python3.7\/site-packages\/\/store\/artifact\/artifact_repo.py in download_artifact_dir(dir_path) 116 for file_info in dir_content: 117 if file_info.is_dir: --> 118 download_artifact_dir(dir_path=file_info.path) 119 else: 120 download_file(file_info.path) \/opt\/conda\/lib\/python3.7\/site-packages\/\/store\/artifact\/artifact_repo.py in download_artifact_dir(dir_path) 116 for file_info in dir_content: 117 if file_info.is_dir: --> 118 download_artifact_dir(dir_path=file_info.path) 119 else: 120 download_file(file_info.path) \/opt\/conda\/lib\/python3.7\/site-packages\/\/store\/artifact\/artifact_repo.py in download_artifact_dir(dir_path) 118 download_artifact_dir(dir_path=file_info.path) 119 else: --> 120 download_file(file_info.path) 121 return local_dir 122 if not os.path.exists(dst_path): \/opt\/conda\/lib\/python3.7\/site-packages\/\/store\/artifact\/artifact_repo.py in download_file(fullpath) 101 local_file_path = os.path.join(dst_path, fullpath) 102 if not os.path.exists(local_dir_path): --> 103 os.makedirs(local_dir_path) 104 self._download_file(remote_file_path=fullpath, local_path=local_file_path) 105 return local_file_path \/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok) 209 if head and tail and not path.exists(head): 210 try: --> 211 makedirs(head, exist_ok=exist_ok) 212 except fileexistserror: 213 # defeats race condition when another thread created the path \/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok) 209 if head and tail and not path.exists(head): 210 try: --> 211 makedirs(head, exist_ok=exist_ok) 212 except fileexistserror: 213 # defeats race condition when another thread created the path \/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok) 209 if head and tail and not path.exists(head): 210 try: --> 211 makedirs(head, exist_ok=exist_ok) 212 except fileexistserror: 213 # defeats race condition when another thread created the path \/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok) 209 if head and tail and not path.exists(head): 210 try: --> 211 makedirs(head, exist_ok=exist_ok) 212 except fileexistserror: 213 # defeats race condition when another thread created the path \/opt\/conda\/lib\/python3.7\/os.py in makedirs(name, mode, exist_ok) 219 return 220 try: --> 221 mkdir(name, mode) 222 except oserror: 223 # cannot rely on checking for eexist, since the operating system permissionerror: [errno 13] permission denied: '\/0' _download_artifact_from_uri(model_uri, \".\/mlruns\") '\/home\/jovyan\/notebooks\/mlruns\/mlmodel' os.mkdir(\"mlruns\/0\")",
        "Question_original_content_gpt_summary":"The user is encountering a PermissionError when attempting to download an entire artifact folder from an FTP server using the _download_artifact_from_uri function from the module .tracking.artifact_utils, but is able to create a folder using the os module.",
        "Question_preprocessed_content":"Title: unable to download artifacts from ftp server using; Content: i'm not able to load my sklearn model using . internally, uses the function from the module . if i try, to download an entire artifact folder i receive the following error message . if i try to retrieve a single file from an artifact folder i do not get the error message, and i'm able to create a folder using the module. the following is the converted jupyter notebook i've used.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"How to input data(csv) TO R-script model?",
        "Question_body":"Hi,\nI loaded all csv files, but R-script only allow two files for input ports. So I zipped all use 7-zip and verifed all files are same as not zipped before and loaded to Azure ml studio and connect it to R-script on 3rd port.\n\nEach time I run it, it says no connection, zip file corrupt.\n\nPLZ point out what is it and how to input datasets?\n\nThanks,\nN.",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1654040013303,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/872215\/how-to-input-datacsv-to-r-script-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-01T12:53:20.8Z",
                "Answer_score":0,
                "Answer_body":"@NAW123-0733 While using the script bundle i.e 3rd port you need to place all the required files in a folder on your local machine and then zip the folder and upload it as a file dataset. If you try to select all files & create a zip without placing them in a folder the module will be unable to read.\n\nFor example, I tried with a sample csv where the file is placed in a folder and then the folder is zipped. I then, uploaded the zip file as file dataset to Azure ML studio, use this dataset in the designer canvas and connect it to the 3rd port and access the same using the corresponding path as of zipped file with a prefix of Script Bundle.\n\nThe result in this case is displaying the CSV data as output from result dataset 2.\n\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-30T21:51:41.57Z",
                "Answer_score":0,
                "Answer_body":"I tried folder thing, not working. Then I select all from within the folder, right click-> send to compression file.\n\nNow, it's connected, can open it. It worked!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to input data(csv) to r-script model?; Content: hi, i loaded all csv files, but r-script only allow two files for input ports. so i zipped all use 7-zip and verifed all files are same as not zipped before and loaded to studio and connect it to r-script on 3rd port. each time i run it, it says no connection, zip file corrupt. plz point out what is it and how to input datasets? thanks, n.",
        "Question_original_content_gpt_summary":"The user is encountering difficulty inputting multiple CSV files into an R-script model, as the model only allows two files for input ports.",
        "Question_preprocessed_content":"Title: how to input data to r script model?; Content: hi, i loaded all csv files, but r script only allow two files for input ports. so i zipped all use zip and verifed all files are same as not zipped before and loaded to studio and connect it to r script on rd port. each time i run it, it says no connection, zip file corrupt. plz point out what is it and how to input datasets? thanks, n.",
        "Answer_original_content":"@naw123-0733 while using the script bundle i.e 3rd port you need to place all the required files in a folder on your local machine and then zip the folder and upload it as a file dataset. if you try to select all files & create a zip without placing them in a folder the module will be unable to read. for example, i tried with a sample csv where the file is placed in a folder and then the folder is zipped. i then, uploaded the zip file as file dataset to studio, use this dataset in the designer canvas and connect it to the 3rd port and access the same using the corresponding path as of zipped file with a prefix of script bundle. the result in this case is displaying the csv data as output from result dataset 2. if an answer is helpful, please click on or upvote which might help other community members reading this thread. i tried folder thing, not working. then i select all from within the folder, right click-> send to compression file. now, it's connected, can open it. it worked!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"while using the script bundle rd port you need to place all the required files in a folder on your local machine and then zip the folder and upload it as a file dataset. if you try to select all files & create a zip without placing them in a folder the module will be unable to read. for example, i tried with a sample csv where the file is placed in a folder and then the folder is zipped. i then, uploaded the zip file as file dataset to studio, use this dataset in the designer canvas and connect it to the rd port and access the same using the corresponding path as of zipped file with a prefix of script bundle. the result in this case is displaying the csv data as output from result dataset . if an answer is helpful, please click on or upvote which might help other community members reading this thread. i tried folder thing, not working. then i select all from within the folder, right click > send to compression file. now, it's connected, can open it. it worked!"
    },
    {
        "Question_id":73788252.0,
        "Question_title":"Which techniques are used by SageMaker Neo for model optimizations",
        "Question_body":"<p>Does SageMaker Neo (SageMaker compilation job) use any techniques for model optimization? Are there any compression techniques used (distillation, quantization etc) to reduce the model size?<\/p>\n<p>I found some description here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo.html<\/a>) regarding quantization but it's not clear how it could be used.<\/p>\n<p>Thanks very much for any insight.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1663683615587,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":15.0,
        "Owner_creation_time":1340961229400,
        "Owner_last_access_time":1663688386772,
        "Owner_reputation":91.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":11.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73788252",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: which techniques are used by neo for model optimizations; Content: does neo ( compilation job) use any techniques for model optimization? are there any compression techniques used (distillation, quantization etc) to reduce the model size? i found some description here (https:\/\/docs.aws.amazon.com\/\/latest\/dg\/neo.html) regarding quantization but it's not clear how it could be used. thanks very much for any insight.",
        "Question_original_content_gpt_summary":"The user is inquiring about the techniques used by Neo for model optimizations, such as compression techniques like distillation and quantization, to reduce the model size.",
        "Question_preprocessed_content":"Title: which techniques are used by neo for model optimizations; Content: does neo use any techniques for model optimization? are there any compression techniques used to reduce the model size? i found some description here regarding quantization but it's not clear how it could be used. thanks very much for any insight.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":72051655.0,
        "Question_title":"Vertex AI - how to monitor training progress?",
        "Question_body":"<h1>Question<\/h1>\n<p>Is there a way to monitor the console output of model training progress during the Vertex AI training?<\/p>\n<h2>Background<\/h2>\n<p>Suppose we have a Tensorflow\/Keras model training code:<\/p>\n<pre><code>model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\n\noptimizer = tf.keras.optimizers.RMSprop(0.001)\nmodel.compile(\n    loss='mse',\n    optimizer=optimizer,\n    metrics=['mae', 'mse']\n)\n\nEPOCHS = 1000\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nearly_history = model.fit(normed_train_data, train_labels, \n                    epochs=EPOCHS, validation_split = 0.2, \n                    callbacks=[early_stop])\n<\/code><\/pre>\n<p>When run the model training from the command line, we can see the progress in the console.<\/p>\n<pre><code>Epoch 1\/1000\nOMP: Info #211: KMP_AFFINITY: decoding x2APIC ids.\nOMP: Info #209: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\nOMP: Info #156: KMP_AFFINITY: 4 available OS procs\nOMP: Info #157: KMP_AFFINITY: Uniform topology\nOMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores\/pkg x 2 threads\/core (2 total cores)\nOMP: Info #213: KMP_AFFINITY: OS proc to physical thread map:\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 0 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 1 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 28 thread 2 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 29 thread 3 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 30 thread 4 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 18 thread 5 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 31 thread 6 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 32 thread 7 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 33 thread 8 bound to OS proc set 0\n8\/8 [==============================] - 2s 31ms\/step - loss: 579.6393 - mae: 22.7661 - mse: 579.6393 - val_loss: 571.7239 - val_mae: 22.5494 - val_mse: 571.7239\nEpoch 2\/1000\n8\/8 [==============================] - 0s 7ms\/step - loss: 527.9056 - mae: 21.6268 - mse: 527.9056 - val_loss: 520.5531 - val_mae: 21.3917 - val_mse: 520.5531\n...\n<\/code><\/pre>\n<p>However, if we run the training in the Vertex AI training, there looks to be no menu\/option to see the console output. Not sure if it is logged in Log Explorer. Please help understand how to monitor the training progress realtime.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1651193148017,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":339.0,
        "Owner_creation_time":1416648155470,
        "Owner_last_access_time":1664057583236,
        "Owner_reputation":14749.0,
        "Owner_up_votes":641.0,
        "Owner_down_votes":62.0,
        "Owner_views":968.0,
        "Answer_body":"<p>You may view training logs in the <strong>GCP Logs Explorer<\/strong> by using below query.<\/p>\n<pre><code>resource.type=&quot;ml_job&quot;\nresource.labels.job_id=&quot;your-training-custom-job-ID&quot;\n<\/code><\/pre>\n<p>The <em><strong>your-training-custom-job-ID<\/strong><\/em> can be found on the ongoing Vertex AI Training in GCP console as seen on the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Below is the screenshot of the logs for the Vertex AI training in GCP logs explorer using the above query.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/waHu6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/waHu6.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may click on <strong>Jump to now<\/strong> to immediately view the latest logs. Also, you may use <strong>Stream Logs<\/strong> option to view <strong>REAL TIME<\/strong> log data which you can also adjust the buffer window in which has certain trade offs. You may refer to this <a href=\"https:\/\/cloud.google.com\/logging\/docs\/view\/streaming-live-tailing\" rel=\"nofollow noreferrer\">documentation<\/a> for more information on streaming logs in GCP logs explorer.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1651562908648,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1651563481836,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72051655",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - how to monitor training progress?; Content: question is there a way to monitor the console output of model training progress during the training? background suppose we have a tensorflow\/keras model training code: model = keras.sequential([ layers.dense(64, activation='relu', input_shape=[len(train_dataset.keys())]), layers.dense(64, activation='relu'), layers.dense(1) ]) optimizer = tf.keras.optimizers.rmsprop(0.001) model.compile( loss='mse', optimizer=optimizer, metrics=['mae', 'mse'] ) epochs = 1000 early_stop = keras.callbacks.earlystopping(monitor='val_loss', patience=10) early_history = model.fit(normed_train_data, train_labels, epochs=epochs, validation_split = 0.2, callbacks=[early_stop]) when run the model training from the command line, we can see the progress in the console. epoch 1\/1000 omp: info #211: kmp_affinity: decoding x2apic ids. omp: info #209: kmp_affinity: affinity capable, using global cpuid leaf 11 info omp: info #154: kmp_affinity: initial os proc set respected: 0-3 omp: info #156: kmp_affinity: 4 available os procs omp: info #157: kmp_affinity: uniform topology omp: info #179: kmp_affinity: 1 packages x 2 cores\/pkg x 2 threads\/core (2 total cores) omp: info #213: kmp_affinity: os proc to physical thread map: omp: info #171: kmp_affinity: os proc 0 maps to package 0 core 0 thread 0 omp: info #171: kmp_affinity: os proc 2 maps to package 0 core 0 thread 1 omp: info #171: kmp_affinity: os proc 1 maps to package 0 core 1 thread 0 omp: info #171: kmp_affinity: os proc 3 maps to package 0 core 1 thread 1 omp: info #249: kmp_affinity: pid 1 tid 17 thread 0 bound to os proc set 0 omp: info #249: kmp_affinity: pid 1 tid 17 thread 1 bound to os proc set 1 omp: info #249: kmp_affinity: pid 1 tid 28 thread 2 bound to os proc set 2 omp: info #249: kmp_affinity: pid 1 tid 29 thread 3 bound to os proc set 3 omp: info #249: kmp_affinity: pid 1 tid 30 thread 4 bound to os proc set 0 omp: info #249: kmp_affinity: pid 1 tid 18 thread 5 bound to os proc set 1 omp: info #249: kmp_affinity: pid 1 tid 31 thread 6 bound to os proc set 2 omp: info #249: kmp_affinity: pid 1 tid 32 thread 7 bound to os proc set 3 omp: info #249: kmp_affinity: pid 1 tid 33 thread 8 bound to os proc set 0 8\/8 [==============================] - 2s 31ms\/step - loss: 579.6393 - mae: 22.7661 - mse: 579.6393 - val_loss: 571.7239 - val_mae: 22.5494 - val_mse: 571.7239 epoch 2\/1000 8\/8 [==============================] - 0s 7ms\/step - loss: 527.9056 - mae: 21.6268 - mse: 527.9056 - val_loss: 520.5531 - val_mae: 21.3917 - val_mse: 520.5531 ... however, if we run the training in the training, there looks to be no menu\/option to see the console output. not sure if it is logged in log explorer. please help understand how to monitor the training progress realtime.",
        "Question_original_content_gpt_summary":"The user is looking for a way to monitor the console output of model training progress during the training when running the model training from the command line.",
        "Question_preprocessed_content":"Title: how to monitor training progress?; Content: question is there a way to monitor the console output of model training progress during the training? background suppose we have a model training code when run the model training from the command line, we can see the progress in the console. however, if we run the training in the training, there looks to be no to see the console output. not sure if it is logged in log explorer. please help understand how to monitor the training progress realtime.",
        "Answer_original_content":"you may view training logs in the gcp logs explorer by using below query. resource.type=\"ml_job\" resource.labels.job_id=\"your-training-custom-job-id\" the your-training-custom-job-id can be found on the ongoing training in gcp console as seen on the below screenshot. below is the screenshot of the logs for the training in gcp logs explorer using the above query. you may click on jump to now to immediately view the latest logs. also, you may use stream logs option to view real time log data which you can also adjust the buffer window in which has certain trade offs. you may refer to this documentation for more information on streaming logs in gcp logs explorer.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"you may view training logs in the gcp logs explorer by using below query. the your training custom job id can be found on the ongoing training in gcp console as seen on the below screenshot. below is the screenshot of the logs for the training in gcp logs explorer using the above query. you may click on jump to now to immediately view the latest logs. also, you may use stream logs option to view real time log data which you can also adjust the buffer window in which has certain trade offs. you may refer to this documentation for more on streaming logs in gcp logs explorer."
    },
    {
        "Question_id":null,
        "Question_title":"Registered for aws sagemaker studio but not able to create account",
        "Question_body":"Hi I received my aws sagemaker studio approval to create an account 1 hr ago When I go to the link to create an account it says my email has not been approved Even though I have an email to the contrary\n\nHow do I contact amazon to sort this out?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1652237658633,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":70.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSB0wO0OiQ_irUdCx9ppjbg\/registered-for-aws-sagemaker-studio-but-not-able-to-create-account",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-11T15:42:07.722Z",
                "Answer_score":0,
                "Answer_body":"Hi, If you requested a SageMaker Studio Lab free account, then please create an issue on this link.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: registered for studio but not able to create account; Content: hi i received my studio approval to create an account 1 hr ago when i go to the link to create an account it says my email has not been approved even though i have an email to the contrary how do i contact amazon to sort this out?",
        "Question_original_content_gpt_summary":"The user is unable to create an account for Amazon Studio despite having received approval to do so, and is seeking assistance to resolve the issue.",
        "Question_preprocessed_content":"Title: registered for studio but not able to create account; Content: hi i received my studio approval to create an account hr ago when i go to the link to create an account it says my email has not been approved even though i have an email to the contrary how do i contact amazon to sort this out?",
        "Answer_original_content":"hi, if you requested a studio lab free account, then please create an issue on this link.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, if you requested a studio lab free account, then please create an issue on this link."
    },
    {
        "Question_id":70494454.0,
        "Question_title":"How to run a vertex AI pipeline for Automl Time series forecasting?",
        "Question_body":"<p>I've been trying this for very long time to run a Automl time series forecasting as pipeline in GCP.<\/p>\n<p>I am able to run the time series model with GUI manually but not able to make a pipeline because of the documentation.<\/p>\n<p>In GCP documentation they provide vertex AI pipeline docs for classification regression video and image processing<\/p>\n<p><strong>but the time series alone missing<\/strong><\/p>\n<p>Any one who had done auto ml forecasting as pipeline in GCP please help me with that..<\/p>\n<pre><code>                            !! Thankyou !!\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1640601938790,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":73.0,
        "Owner_creation_time":1631541426920,
        "Owner_last_access_time":1641618229143,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Madurai, Tamil Nadu, India",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70494454",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to run a pipeline for automl time series forecasting?; Content: i've been trying this for very long time to run a automl time series forecasting as pipeline in gcp. i am able to run the time series model with gui manually but not able to make a pipeline because of the documentation. in gcp documentation they provide pipeline docs for classification regression video and image processing but the time series alone missing any one who had done auto ml forecasting as pipeline in gcp please help me with that.. !! thankyou !!",
        "Question_original_content_gpt_summary":"The user is encountering challenges in running an AutoML time series forecasting pipeline in GCP, as the documentation does not provide any guidance for this task.",
        "Question_preprocessed_content":"Title: how to run a pipeline for automl time series forecasting?; Content: i've been trying this for very long time to run a automl time series forecasting as pipeline in gcp. i am able to run the time series model with gui manually but not able to make a pipeline because of the documentation. in gcp documentation they provide pipeline docs for classification regression video and image processing but the time series alone missing any one who had done auto ml forecasting as pipeline in gcp please help me with",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"real-time inference pipeline failed to deploy for some unknown error and unable to view the logs",
        "Question_body":"After submitting a successful inference pipeline, I attempted to deploy the model to a container instance. However, it failed and to make it worse I can't see the logs due to forbidden permissions error even though I am sole owner of resource group & instance. To put the nail on the coffin, I also can't view any related container instances inside Azure Portal...only the endpoints in ML studio.\n\nHere's the permissions error:\n\n ![{\n   \"error\": {\n     \"code\": \"Forbidden\",\n     \"message\": \"Forbidden\",\n     \"details\": [\n       {\n         \"code\": \"AuthorizationFailed\",\n         \"message\": \"The client 'df9ec36b-a97d-4c60-a6fe-91048565a571' with object id 'df9ec36b-a97d-4c60-a6fe-91048565a571' does not have authorization to perform action 'Microsoft.ContainerInstance\/containerGroups\/containers\/logs\/read' over scope '\/subscriptions\/7d36b75b-8fd4-4ef9-92fe-69f951afa25d\/resourceGroups\/playground\/providers\/Microsoft.ContainerInstance\/containerGroups\/playground-pipe-ommOzSbRhUSx8qiJGQ4HiA\/containers\/playground-pipe' or the scope is invalid. If access was recently granted, please refresh your credentials.\"\n       }\n     ]\n   },\n   \"correlation\": {\n     \"RequestId\": \"745ad382-74c0-4c67-8853-053807cd6336\"\n   }\n }][1]\n\n\n\n\n[1]: \/answers\/storage\/attachments\/239641-screenshot-2022-09-09-175802.png",
        "Question_answer_count":0,
        "Question_comment_count":6.0,
        "Question_creation_time":1662748337977,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1001503\/real-time-inference-pipeline-failed-to-deploy-for.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: real-time inference pipeline failed to deploy for some unknown error and unable to view the logs; Content: after submitting a successful inference pipeline, i attempted to deploy the model to a container instance. however, it failed and to make it worse i can't see the logs due to forbidden permissions error even though i am sole owner of resource group & instance. to put the nail on the coffin, i also can't view any related container instances inside azure portal...only the endpoints in ml studio. here's the permissions error: ![{ \"error\": { \"code\": \"forbidden\", \"message\": \"forbidden\", \"details\": [ { \"code\": \"authorizationfailed\", \"message\": \"the client 'df9ec36b-a97d-4c60-a6fe-91048565a571' with object id 'df9ec36b-a97d-4c60-a6fe-91048565a571' does not have authorization to perform action 'microsoft.containerinstance\/containergroups\/containers\/logs\/read' over scope '\/subscriptions\/7d36b75b-8fd4-4ef9-92fe-69f951afa25d\/resourcegroups\/playground\/providers\/microsoft.containerinstance\/containergroups\/playground-pipe-ommozsbrhusx8qijgq4hia\/containers\/playground-pipe' or the scope is invalid. if access was recently granted, please refresh your credentials.\" } ] }, \"correlation\": { \"requestid\": \"745ad382-74c0-4c67-8853-053807cd6336\" } }][1] [1]: \/answers\/storage\/attachments\/239641-screenshot-2022-09-09-175802.png",
        "Question_original_content_gpt_summary":"The user encountered challenges deploying a real-time inference pipeline, resulting in an inability to view the logs due to a 'forbidden' permissions error.",
        "Question_preprocessed_content":"Title: real time inference pipeline failed to deploy for some unknown error and unable to view the logs; Content: after submitting a successful inference pipeline, i attempted to deploy the model to a container instance. however, it failed and to make it worse i can't see the logs due to forbidden permissions error even though i am sole owner of resource group & instance. to put the nail on the coffin, i also can't view any related container instances inside azure the endpoints in ml studio. here's the permissions error , correlation",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":71498754.0,
        "Question_title":"Secret Manager from Vertex AI Pipeline",
        "Question_body":"<p>I am working in GCP creating a Vertex AI pipeline with kubeflow and it is time for me to store my API keys more securely. I am very new to GCP and unfamiliar with the environment so I've been trying to follow a few tutorials but have hit a roadblock. I want to store my secrets in Secret Manager and then later access them from the pipeline I've written. I have no problem creating secrets and viewing them in the GUI but when it comes to compiling my pipeline i get the error: <code>google.api_core.exceptions.PermissionDenied: 403 Permission denied on resource project...<\/code><\/p>\n<p>So it seems that the account running my pipelines does not have access to the secrets I have created. My question is then, how do I check which account is running the pipeline so I can grant it access? Or is there really another underlying problem here?<\/p>\n<p>Code trying to access the secret:<\/p>\n<pre><code> client = secretmanager.SecretManagerServiceClient()\n secret_name = &quot;secret_name&quot;\n request = {'name': f&quot;path\/{secret_name}\/versions\/latest&quot;}\n response = client.access_secret_version(request)\n secret_string = response.payload.data.decode(&quot;UTF-8&quot;)\n<\/code><\/pre>\n<p>EDIT: I can add that I have been playing around a lot with account permissions but my best guess is that the account that is found under Vertex AI&gt;Workbench&gt;the notebook I am using's notebook details&gt;Service account is the one that needs permission. Is this not it?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":4.0,
        "Question_creation_time":1647440513490,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":311.0,
        "Owner_creation_time":1407245826703,
        "Owner_last_access_time":1663849819648,
        "Owner_reputation":745.0,
        "Owner_up_votes":210.0,
        "Owner_down_votes":6.0,
        "Owner_views":168.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1647514839536,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71498754",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: secret manager from pipeline; Content: i am working in gcp creating a pipeline with kubeflow and it is time for me to store my api keys more securely. i am very new to gcp and unfamiliar with the environment so i've been trying to follow a few tutorials but have hit a roadblock. i want to store my secrets in secret manager and then later access them from the pipeline i've written. i have no problem creating secrets and viewing them in the gui but when it comes to compiling my pipeline i get the error: google.api_core.exceptions.permissiondenied: 403 permission denied on resource project... so it seems that the account running my pipelines does not have access to the secrets i have created. my question is then, how do i check which account is running the pipeline so i can grant it access? or is there really another underlying problem here? code trying to access the secret: client = secretmanager.secretmanagerserviceclient() secret_name = \"secret_name\" request = {'name': f\"path\/{secret_name}\/versions\/latest\"} response = client.access_secret_version(request) secret_string = response.payload.data.decode(\"utf-8\") edit: i can add that i have been playing around a lot with account permissions but my best guess is that the account that is found under >workbench>the notebook i am using's notebook details>service account is the one that needs permission. is this not it?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with storing API keys securely in GCP Secret Manager and granting the account running the pipeline access to the secrets, while trying to compile the pipeline.",
        "Question_preprocessed_content":"Title: secret manager from pipeline; Content: i am working in gcp creating a pipeline with kubeflow and it is time for me to store my api keys more securely. i am very new to gcp and unfamiliar with the environment so i've been trying to follow a few tutorials but have hit a roadblock. i want to store my secrets in secret manager and then later access them from the pipeline i've written. i have no problem creating secrets and viewing them in the gui but when it comes to compiling my pipeline i get the error so it seems that the account running my pipelines does not have access to the secrets i have created. my question is then, how do i check which account is running the pipeline so i can grant it access? or is there really another underlying problem here? code trying to access the secret edit i can add that i have been playing around a lot with account permissions but my best guess is that the account that is found under >workbench>the notebook i am using's notebook details>service account is the one that needs permission. is this not it?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"About Speech-to-Text support area",
        "Question_body":"Does Speech-to-Text have any nodes in Hong Kong?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1632630840000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":325.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/About-Speech-to-Text-support-area\/td-p\/171226\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-01T14:26:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Speech to text is available as a global or multi regional service. You can select a region by specifying a particular endpoint.\u00a0\u00a0https:\/\/cloud.google.com\/speech-to-text\/docs\/endpoints\n\n\u00a0\n\nHong Kong is not currently available as a standalone region."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: about speech-to-text support area; Content: does speech-to-text have any nodes in hong kong?",
        "Question_original_content_gpt_summary":"The user is inquiring about whether speech-to-text has any nodes in Hong Kong.",
        "Question_preprocessed_content":"Title: about speech to text support area; Content: does speech to text have any nodes in hong kong?",
        "Answer_original_content":"speech to text is available as a global or multi regional service. you can select a region by specifying a particular endpoint.https:\/\/cloud.google.com\/speech-to-text\/docs\/endpoints hong kong is not currently available as a standalone region.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"speech to text is available as a global or multi regional service. you can select a region by specifying a particular hong kong is not currently available as a standalone region."
    },
    {
        "Question_id":72555212.0,
        "Question_title":"wandb pytorch: top1 accuracy per class",
        "Question_body":"<p>I have 5 classes in validation set and i want to draw a graph based on top1 results per class in validation loop using wandb . I have tried a single accuracy graph based on the average of 5 classes and it works fine but i want to do a separate way like top1 accuracy for each class. I am unable to achieve, are there any way to achieve it?<\/p>\n<p><strong>Validation Loader<\/strong><\/p>\n<pre><code> val_loaders = []\n    for nuisance in val_nuisances:\n        val_loaders.append((nuisance, torch.utils.data.DataLoader(\n            datasets.ImageFolder(os.path.join(valdir, nuisance), transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n                transforms.ToTensor(),\n                normalize,\n            ])),\n            batch_size=args.batch_size, shuffle=False,\n            num_workers=args.workers, pin_memory=True,\n        )))\n\n\nval_nuisances = ['shape', 'pose', 'texture', 'context', 'weather']\n<\/code><\/pre>\n<p><strong>Validation Loop<\/strong><\/p>\n<pre><code>def validate(val_loaders, model, criterion, args):\n    overall_top1 = 0\n    for nuisance, val_loader in val_loaders:\n        batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n        losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n        top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n        top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n        progress = ProgressMeter(\n            len(val_loader),\n            [batch_time, losses, top1, top5],\n            prefix=f'Test {nuisance}: ')\n\n        # switch to evaluate mode\n        model.eval()\n\n        with torch.no_grad():\n            end = time.time()\n            for i, (images, target) in enumerate(val_loader):\n                if args.gpu is not None:\n                    images = images.cuda(args.gpu, non_blocking=True)\n                if torch.cuda.is_available():\n                    target = target.cuda(args.gpu, non_blocking=True)\n\n                # compute output\n                output = model(images)\n                loss = criterion(output, target)\n\n                # measure accuracy and record loss\n                acc1, acc5 = accuracy(output, target, topk=(1, 5))\n                losses.update(loss.item(), images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % args.print_freq == 0:\n                    progress.display(i)\n\n            progress.display_summary()\n        overall_top1 += top1.avg\n    overall_top1 \/= len(val_loaders)\n    return top1.avg\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1654753323610,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":71.0,
        "Owner_creation_time":1420474938783,
        "Owner_last_access_time":1663917040323,
        "Owner_reputation":2201.0,
        "Owner_up_votes":186.0,
        "Owner_down_votes":3.0,
        "Owner_views":556.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Seoul, South Korea",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72555212",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pytorch: top1 accuracy per class; Content: i have 5 classes in validation set and i want to draw a graph based on top1 results per class in validation loop using . i have tried a single accuracy graph based on the average of 5 classes and it works fine but i want to do a separate way like top1 accuracy for each class. i am unable to achieve, are there any way to achieve it? validation loader val_loaders = [] for nuisance in val_nuisances: val_loaders.append((nuisance, torch.utils.data.dataloader( datasets.imagefolder(os.path.join(valdir, nuisance), transforms.compose([ transforms.resize(256), transforms.centercrop(224), transforms.totensor(), normalize, ])), batch_size=args.batch_size, shuffle=false, num_workers=args.workers, pin_memory=true, ))) val_nuisances = ['shape', 'pose', 'texture', 'context', 'weather'] validation loop def validate(val_loaders, model, criterion, args): overall_top1 = 0 for nuisance, val_loader in val_loaders: batch_time = averagemeter('time', ':6.3f', summary.none) losses = averagemeter('loss', ':.4e', summary.none) top1 = averagemeter('acc@1', ':6.2f', summary.average) top5 = averagemeter('acc@5', ':6.2f', summary.average) progress = progressmeter( len(val_loader), [batch_time, losses, top1, top5], prefix=f'test {nuisance}: ') # switch to evaluate mode model.eval() with torch.no_grad(): end = time.time() for i, (images, target) in enumerate(val_loader): if args.gpu is not none: images = images.cuda(args.gpu, non_blocking=true) if torch.cuda.is_available(): target = target.cuda(args.gpu, non_blocking=true) # compute output output = model(images) loss = criterion(output, target) # measure accuracy and record loss acc1, acc5 = accuracy(output, target, topk=(1, 5)) losses.update(loss.item(), images.size(0)) top1.update(acc1[0], images.size(0)) top5.update(acc5[0], images.size(0)) # measure elapsed time batch_time.update(time.time() - end) end = time.time() if i % args.print_freq == 0: progress.display(i) progress.display_summary() overall_top1 += top1.avg overall_top1 \/= len(val_loaders) return top1.avg",
        "Question_original_content_gpt_summary":"The user is struggling to draw a graph based on top1 results per class in validation loop using pytorch.",
        "Question_preprocessed_content":"Title: pytorch top accuracy per class; Content: i have classes in validation set and i want to draw a graph based on top results per class in validation loop using . i have tried a single accuracy graph based on the average of classes and it works fine but i want to do a separate way like top accuracy for each class. i am unable to achieve, are there any way to achieve it? validation loader validation loop",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"How to deploy Azure automated machine learning model to production to generate forecast",
        "Question_body":"I managed to use Azure automated machine learning to train a time series forecasting model in Azure machine learning studio.\n\nWe get new data only once a month, therefore we do not think that web service is necessary for our needs.\n\nWe want to deploy the model on the cloud or locally to start generating forecasting data. But We are not quite sure how to do so. We do not have knowledge about Python and machine learning deployment.\n\nCould you recommend some instructions on how to deploy Azure automated machine learning model on the cloud and locally? Thank you!",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1597314320727,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/66173\/how-to-deploy-azure-automated-machine-learning-mod-1.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-13T18:59:22.42Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThanks a lot for reaching out to us, please check following guidance for how to configure and train a time-series forecasting regression model using automated machine learning in the Azure Machine Learning Python SDK.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast#configure-and-run-experiment\n\nPlease let me know for more question. Thanks!\n\nRegards,\nYutong",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":2.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to deploy azure automated machine learning model to production to generate forecast; Content: i managed to use azure automated machine learning to train a time series forecasting model in studio. we get new data only once a month, therefore we do not think that web service is necessary for our needs. we want to deploy the model on the cloud or locally to start generating forecasting data. but we are not quite sure how to do so. we do not have knowledge about python and machine learning deployment. could you recommend some instructions on how to deploy azure automated machine learning model on the cloud and locally? thank you!",
        "Question_original_content_gpt_summary":"The user is looking for instructions on how to deploy an Azure Automated Machine Learning model to the cloud and locally in order to generate forecasting data, but they lack knowledge of Python and Machine Learning deployment.",
        "Question_preprocessed_content":"Title: how to deploy azure automated machine learning model to production to generate forecast; Content: i managed to use azure automated machine learning to train a time series forecasting model in studio. we get new data only once a month, therefore we do not think that web service is necessary for our needs. we want to deploy the model on the cloud or locally to start generating forecasting data. but we are not quite sure how to do so. we do not have knowledge about python and machine learning deployment. could you recommend some instructions on how to deploy azure automated machine learning model on the cloud and locally? thank you!",
        "Answer_original_content":"hi, thanks a lot for reaching out to us, please check following guidance for how to configure and train a time-series forecasting regression model using automated machine learning in the python sdk. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-auto-train-forecast#configure-and-run-experiment please let me know for more question. thanks! regards, yutong",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, thanks a lot for reaching out to us, please check following guidance for how to configure and train a time series forecasting regression model using automated machine learning in the python sdk. please let me know for more question. thanks! regards, yutong"
    },
    {
        "Question_id":null,
        "Question_title":"VM Ram vs Google Colab Ram",
        "Question_body":"Hi, @Eduardo_Ortiz  @josegutierrez sorry to bother but I`m completely lostDays a go I bought a VM that has the next configurations, when I connect to the VM with Google Colab get the next results as you can see in the next image.VM Configuration : GPUs1 x NVIDIA Tesla V100  +  n1-highmem-8 (vCPUs: 8, RAM: 52GB)Ram obtained in Google Colab from the VM: 1.31 Gb \/ 51.01 Gb Disc 43.79 \/ 186.52As you realized,  althoug I have buy a better configuration than Google Coalb Pro+ Im getting fewer RAM from the VM instance....What could be the error or situation? How can I get into colab the real VM capacity bought? Or which configuration do I need in order to have better performance than Google Colab pro+?In the next screen shot the ram and disck that I got from Google Colab:Thanks a lot for any help ",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1658926260000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":61.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/VM-Ram-vs-Google-Colab-Ram\/td-p\/447466\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-27T12:51:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi, @Eduardo_Ortiz\u00a0 @josegutierrez\u00a0sorry to bother but I`m completely lost\n\nDays a go I bought a VM that has the next configurations, when I connect to the VM with Google Colab get the next results as you can see in the next image.\n\nVM Configuration : GPUs1 x NVIDIA Tesla V100\u00a0 +\u00a0\u00a0n1-highmem-8 (vCPUs: 8, RAM: 52GB)\n\nRam obtained in Google Colab from the VM: 1.31 Gb \/ 51.01 Gb Disc 43.79 \/ 186.52\n\nAs you realized,\u00a0 althoug I have buy a better configuration than Google Coalb Pro+ Im getting fewer RAM from the VM instance....\n\nWhat could be the error or situation? How can I get into colab the real VM capacity bought? Or which configuration do I need in order to have better performance than Google Colab pro+?\n\nIn the next screen shot the ram and disck that I got from Google Colab:\n\nThanks a lot for any help"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: vm ram vs google colab ram; Content: hi, @eduardo_ortiz @josegutierrez sorry to bother but i`m completely lostdays a go i bought a vm that has the next configurations, when i connect to the vm with google colab get the next results as you can see in the next image.vm configuration : gpus1 x nvidia tesla v100 + n1-highmem-8 (vcpus: 8, ram: 52gb)ram obtained in google colab from the vm: 1.31 gb \/ 51.01 gb disc 43.79 \/ 186.52as you realized, althoug i have buy a better configuration than google coalb pro+ im getting fewer ram from the vm instance....what could be the error or situation? how can i get into colab the real vm capacity bought? or which configuration do i need in order to have better performance than google colab pro+?in the next screen shot the ram and disck that i got from google colab:thanks a lot for any help",
        "Question_original_content_gpt_summary":"The user is encountering challenges with obtaining the full RAM and disk capacity of their VM instance when connecting to Google Colab.",
        "Question_preprocessed_content":"Title: vm ram vs google colab ram; Content: hi, sorry to bother but i`m completely lostdays a go i bought a vm that has the next configurations, when i connect to the vm with google colab get the next results as you can see in the next configuration gpus x nvidia tesla v + n highmem ram obtained in google colab from the vm gb \/ gb disc \/ you realized, althoug i have buy a better configuration than google coalb pro+ im getting fewer ram from the vm could be the error or situation? how can i get into colab the real vm capacity bought? or which configuration do i need in order to have better performance than google colab pro+?in the next screen shot the ram and disck that i got from google colab thanks a lot for any help",
        "Answer_original_content":"hi, @eduardo_ortiz @josegutierrezsorry to bother but i`m completely lost days a go i bought a vm that has the next configurations, when i connect to the vm with google colab get the next results as you can see in the next image. vm configuration : gpus1 x nvidia tesla v100 +n1-highmem-8 (vcpus: 8, ram: 52gb) ram obtained in google colab from the vm: 1.31 gb \/ 51.01 gb disc 43.79 \/ 186.52 as you realized, althoug i have buy a better configuration than google coalb pro+ im getting fewer ram from the vm instance.... what could be the error or situation? how can i get into colab the real vm capacity bought? or which configuration do i need in order to have better performance than google colab pro+? in the next screen shot the ram and disck that i got from google colab: thanks a lot for any help",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, to bother but i`m completely lost days a go i bought a vm that has the next configurations, when i connect to the vm with google colab get the next results as you can see in the next image. vm configuration gpus x nvidia tesla v +n highmem ram obtained in google colab from the vm gb \/ gb disc \/ as you realized, althoug i have buy a better configuration than google coalb pro+ im getting fewer ram from the vm what could be the error or situation? how can i get into colab the real vm capacity bought? or which configuration do i need in order to have better performance than google colab pro+? in the next screen shot the ram and disck that i got from google colab thanks a lot for any help"
    },
    {
        "Question_id":null,
        "Question_title":"Data Labeling not accesible after Transfer billing ownership",
        "Question_body":"Hi,\n\nI transferred the billing ownership of my azure directory, and now I have not access to the ML data labeling project.\n\nEverything works fine in the destination account, but the data labeling project that was in progress.\n\nIf I create a new data labeling project using the same Datastore, It will work; I can label images as usual. But the original project will not work, I can even open the project but in the moment I try to \"Label Data\", it says I don't have permissions.\n\n\nThis error pops up even using the owner user.",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1609967334840,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/221503\/data-labeling-not-accesible-after-transfer-billing.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: data labeling not accesible after transfer billing ownership; Content: hi, i transferred the billing ownership of my azure directory, and now i have not access to the ml data labeling project. everything works fine in the destination account, but the data labeling project that was in progress. if i create a new data labeling project using the same datastore, it will work; i can label images as usual. but the original project will not work, i can even open the project but in the moment i try to \"label data\", it says i don't have permissions. this error pops up even using the owner user.",
        "Question_original_content_gpt_summary":"The user encountered a challenge where they were unable to access their data labeling project after transferring billing ownership of their Azure directory.",
        "Question_preprocessed_content":"Title: data labeling not accesible after transfer billing ownership; Content: hi, i transferred the billing ownership of my azure directory, and now i have not access to the ml data labeling project. everything works fine in the destination account, but the data labeling project that was in progress. if i create a new data labeling project using the same datastore, it will work; i can label images as usual. but the original project will not work, i can even open the project but in the moment i try to label data , it says i don't have permissions. this error pops up even using the owner user.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Revert wandb server upgrade?",
        "Question_body":"<p>Hi,<\/p>\n<p>When I start the wandb server locally, I got a notification to upgrade, so I upgraded the version. But now I couldn\u2019t access my runs and I run into this error whenever I run a training script:<\/p>\n<p><code>wandb: Network error (TransientError), entering retry loop.<\/code><\/p>\n<p>It also does not seem to log images anymore.<\/p>\n<p>I removed wandb from pip and reinstalled it at the version that still worked but the error still persists.<\/p>\n<p>Any ideas?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1670607492654,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":210.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/revert-wandb-server-upgrade\/3518",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-10T01:04:29.803Z",
                "Answer_body":"<p>Problem solved: had to upgrade pip wandb!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-08T01:04:52.641Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"",
        "Answer_original_content":"",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":""
    },
    {
        "Question_id":72871765.0,
        "Question_title":"Unable to attach custom image to AWS Sagemaker Studio",
        "Question_body":"<p>I am trying to attach custom image in SageMaker, it was working fine until I deleted couple of previous version, it started giving me errors as bellow and now I am unable to attach either new image or a new version for existing image.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xc1QA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xc1QA.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1657033999370,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":172.0,
        "Owner_creation_time":1548654028332,
        "Owner_last_access_time":1663256348883,
        "Owner_reputation":115.0,
        "Owner_up_votes":2.0,
        "Owner_down_votes":0.0,
        "Owner_views":22.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72871765",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: unable to attach custom image to studio; Content: i am trying to attach custom image in , it was working fine until i deleted couple of previous version, it started giving me errors as bellow and now i am unable to attach either new image or a new version for existing image.",
        "Question_original_content_gpt_summary":"The user is unable to attach a custom image to Studio after deleting previous versions, resulting in errors.",
        "Question_preprocessed_content":"Title: unable to attach custom image to studio; Content: i am trying to attach custom image in , it was working fine until i deleted couple of previous version, it started giving me errors as bellow and now i am unable to attach either new image or a new version for existing image.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"How do I logout of my account?",
        "Question_body":"<p>There\u2019s a <code>wandb login<\/code> command, but I couldn\u2019t find any way to log out.  How does one logout?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1654119866279,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-i-logout-of-my-account\/2531",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-02T07:08:00.136Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/gcx\">@gcx<\/a>, at the moment we don\u2019t have a specific command for that. But if you remove the line that contains your api key in the .netrc file in your home directory, it will effectively log you out.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-06T04:02:11.854Z",
                "Answer_body":"<p>Hey there, I wanted to follow up on this. Please let me know if you have any further questions.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-06T14:03:56.608Z",
                "Answer_body":"<p>No further questions. Thanks for the response!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-05T14:04:21.441Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i logout of my account?; Content: there\u2019s a login command, but i couldn\u2019t find any way to log out. how does one logout?",
        "Question_original_content_gpt_summary":"The user is unable to find a way to log out of their account.",
        "Question_preprocessed_content":"Title: how do i logout of my account?; Content: theres a command, but i couldnt find any way to log out. how does one logout?",
        "Answer_original_content":"hey @gcx, at the moment we dont have a specific command for that. but if you remove the line that contains your api key in the .netrc file in your home directory, it will effectively log you out. hey there, i wanted to follow up on this. please let me know if you have any further questions. no further questions. thanks for the response! this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hey at the moment we dont have a specific command for that. but if you remove the line that contains your api key in the .netrc file in your home directory, it will effectively log you out. hey there, i wanted to follow up on this. please let me know if you have any further questions. no further questions. thanks for the response! this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":60801292.0,
        "Question_title":"View Neptune Graph Schema using Jupyter notebook",
        "Question_body":"<p>Is there a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook? <\/p>\n\n<p>Like you would do a \"select * from tablename limit 10\" in an RDS using SQL, similarly is there a way to get a sense of the graph data through Jupyter Notebook?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1584891394693,
        "Question_favorite_count":2.0,
        "Question_score":2.0,
        "Question_view_count":831.0,
        "Owner_creation_time":1584891066787,
        "Owner_last_access_time":1663398099723,
        "Owner_reputation":99.0,
        "Owner_up_votes":5.0,
        "Owner_down_votes":0.0,
        "Owner_views":12.0,
        "Answer_body":"<p>It depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. From the tags you used I assume you are using Gremlin:<\/p>\n\n<pre><code>g.V().groupCount().by(label)\ng.E().groupCount().by(label)\n<\/code><\/pre>\n\n<p>If you have a very large graph try putting something like <code>limit(100000)<\/code> before the <code>groupCount<\/code> step.<\/p>\n\n<p>If you are using a programming language like Python (with gremlin python installed) then you will need to add a <code>next()<\/code> terminal step to the queries as in:<\/p>\n\n<pre><code>g.V().groupCount().by(label).next()\ng.E().groupCount().by(label).next()\n<\/code><\/pre>\n\n<p>Having found the labels and distribution of the labels you could use one of them to explore some properties. Let's imagine there is a label called \"person\".<\/p>\n\n<pre><code>g.V().hasLabel('person').limit(10).valueMap().toList()\n<\/code><\/pre>\n\n<p>Remember with Gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1584902412783,
        "Answer_score":4.0,
        "Owner_location":null,
        "Question_last_edit_time":1637708419888,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60801292",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: view neptune graph schema using jupyter notebook; Content: is there a way to view the schema of a graph in a neptune cluster using jupyter notebook? like you would do a \"select * from tablename limit 10\" in an rds using sql, similarly is there a way to get a sense of the graph data through jupyter notebook?",
        "Question_original_content_gpt_summary":"The user is looking for a way to view the schema of a graph in a Neptune cluster using Jupyter Notebook, similar to how one would view a table in an RDS using SQL.",
        "Question_preprocessed_content":"Title: view neptune graph schema using jupyter notebook; Content: is there a way to view the schema of a graph in a neptune cluster using jupyter notebook? like you would do a select from tablename limit in an rds using sql, similarly is there a way to get a sense of the graph data through jupyter notebook?",
        "Answer_original_content":"it depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. from the tags you used i assume you are using gremlin: g.v().groupcount().by(label) g.e().groupcount().by(label) if you have a very large graph try putting something like limit(100000) before the groupcount step. if you are using a programming language like python (with gremlin python installed) then you will need to add a next() terminal step to the queries as in: g.v().groupcount().by(label).next() g.e().groupcount().by(label).next() having found the labels and distribution of the labels you could use one of them to explore some properties. let's imagine there is a label called \"person\". g.v().haslabel('person').limit(10).valuemap().tolist() remember with gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"it depends on how large your graph is as to how well this will perform but you can get a sense of the type of nodes and edges you have using something like the example below. from the tags you used i assume you are using gremlin if you have a very large graph try putting something like before the step. if you are using a programming language like python then you will need to add a terminal step to the queries as in having found the labels and distribution of the labels you could use one of them to explore some properties. let's imagine there is a label called person . remember with gremlin property graphs vertices with the same label may not necessarily have all the same properties so it's good to look at more than one vertex to get a sense for that as well."
    },
    {
        "Question_id":73222264.0,
        "Question_title":"change run display name azureml",
        "Question_body":"<p>same problem as this however using script run config  <a href=\"https:\/\/stackoverflow.com\/questions\/69857591\/how-to-set-azure-experiment-name-from-the-code-after-2021-08-18-sdk-change\">How to set azure experiment name from the code after 2021-08-18 SDK change?<\/a>. Tried all the solutions however did not work<\/p>\n<pre><code>src = ScriptRunConfig(\n            source_directory=&quot;.&quot;,\n            script='train.py',\n            arguments=training_params,\n            compute_target=compute_name,\n            environment = conda_env\n            )\n        #DataReference for underlying RunConfiguration object\n        src.run_config.data_references = {Data_Refrence.data_reference_name: Data_Refrence.to_config()}\n\ntry:\n    run = exp.submit(config=src)\n    run.display_name = &quot;Test&quot;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1659531955680,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":72.0,
        "Owner_creation_time":1606756004663,
        "Owner_last_access_time":1661935729383,
        "Owner_reputation":49.0,
        "Owner_up_votes":9.0,
        "Owner_down_votes":0.0,
        "Owner_views":33.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73222264",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: change run display name ; Content: same problem as this however using script run config how to set azure experiment name from the code after 2021-08-18 sdk change?. tried all the solutions however did not work src = scriptrunconfig( source_directory=\".\", script='train.py', arguments=training_params, compute_target=compute_name, environment = conda_env ) #datareference for underlying runconfiguration object src.run_config.data_references = {data_refrence.data_reference_name: data_refrence.to_config()} try: run = exp.submit(config=src) run.display_name = \"test\"",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with setting the Azure experiment name from the code after the 2021-08-18 SDK change, and has tried all the solutions but has not been able to resolve the issue.",
        "Question_preprocessed_content":"Title: change run display name; Content: same problem as this however using script run config how to set azure experiment name from the code after sdk change?. tried all the solutions however did not work",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Specify guild home location on SSH remote",
        "Question_body":"<p>I have a remote SSH with a OS disk and a data disk. I want to put my guild runs on the data disk, so on my ssh machine I have set my <code>GUILD_HOME=\/mnt\/guild_runs<\/code>.<\/p>\n<p>When I want to run from my local machine like <code>guild runs --remote ssh-machine<\/code> it cannot find the runs on the remote. When I run with additional debug information I see that the ssh command guild issues overwrites my <code>GUILD_HOME<\/code> env variable.<\/p>\n<p>Is there a way to tell guild where my <code>GUILD_HOME<\/code> is on my remote? I know I can do <code>venv-path<\/code>, but since my venv path and <code>GUILD_HOME<\/code> directory aren\u2019t the same, I don\u2019t think this solves it.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1651083462913,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":106.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/specify-guild-home-location-on-ssh-remote\/866",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-27T18:24:33.800Z",
                "Answer_body":"<p>I see this undocumented (sorry about that!) \u2014 you can use <code>guild-home<\/code> as a config attr for the remote:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">remotes:\n  my-remote:\n    guild-home: \/mnt\/guild_runs\n<\/code><\/pre>\n<p>I\u2019ll make sure this gets added to the docs. Sorry again.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-27T18:26:25.527Z",
                "Answer_body":"<p>No worries - that worked!<\/p>",
                "Answer_score":46.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: specify guild home location on ssh remote; Content: i have a remote ssh with a os disk and a data disk. i want to put my guild runs on the data disk, so on my ssh machine i have set my guild_home=\/mnt\/guild_runs. when i want to run from my local machine like guild runs --remote ssh-machine it cannot find the runs on the remote. when i run with additional debug information i see that the ssh command guild issues overwrites my guild_home env variable. is there a way to tell guild where my guild_home is on my remote? i know i can do venv-path, but since my venv path and guild_home directory aren\u2019t the same, i don\u2019t think this solves it.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with specifying a guild home location on an SSH remote, as the SSH command issued by Guild is overwriting the guild_home environment variable.",
        "Question_preprocessed_content":"Title: specify guild home location on ssh remote; Content: i have a remote ssh with a os disk and a data disk. i want to put my guild runs on the data disk, so on my ssh machine i have set my . when i want to run from my local machine like it cannot find the runs on the remote. when i run with additional debug i see that the ssh command guild issues overwrites my env variable. is there a way to tell guild where my is on my remote? i know i can do , but since my venv path and directory arent the same, i dont think this solves it.",
        "Answer_original_content":"i see this undocumented (sorry about that!) you can use guild-home as a config attr for the remote: remotes: my-remote: guild-home: \/mnt\/guild_runs ill make sure this gets added to the docs. sorry again. no worries - that worked!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i see this undocumented you can use as a config attr for the remote ill make sure this gets added to the docs. sorry again. no worries that worked!"
    },
    {
        "Question_id":63718070.0,
        "Question_title":"How to specify the cuda version in sagemaker training instance",
        "Question_body":"<p>I am currently using tensorflow 2.3 to train a CNN model on AWS sagemaker. I want to run the training on an GPU instance but it seem that the default cuda version is cuda 10.0 but what I need is cuda 10.1. I need a way to specify the cuda version in the training instance. I use estimator.Tensorflow which consumes a script file, not a docker image.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1599114806223,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1717.0,
        "Owner_creation_time":1598513190248,
        "Owner_last_access_time":1641527189656,
        "Owner_reputation":51.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1599115800932,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63718070",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to specify the cuda version in training instance; Content: i am currently using tensorflow 2.3 to train a cnn model on . i want to run the training on an gpu instance but it seem that the default cuda version is cuda 10.0 but what i need is cuda 10.1. i need a way to specify the cuda version in the training instance. i use estimator.tensorflow which consumes a script file, not a docker image.",
        "Question_original_content_gpt_summary":"The user is looking for a way to specify the cuda version in their training instance when using TensorFlow 2.3 and an GPU instance.",
        "Question_preprocessed_content":"Title: how to specify the cuda version in training instance; Content: i am currently using tensorflow to train a cnn model on . i want to run the training on an gpu instance but it seem that the default cuda version is cuda but what i need is cuda i need a way to specify the cuda version in the training instance. i use which consumes a script file, not a docker image.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":66009324.0,
        "Question_title":"How to load a specific catalog dataset instance in kedro 0.17.0?",
        "Question_body":"<p>We were using kedro version 0.15.8 and we were loading one specific item from the catalog this way:<\/p>\n<pre><code>from kedro.context import load_context\nget_context().catalog.datasets.__dict__[key]\n<\/code><\/pre>\n<p>Now, we are changing to kedro 0.17.0 and trying to load the catalogs datasets the same way(using the framework context):<\/p>\n<pre><code>from kedro.framework.context import load_context\nget_context().catalog.datasets.__dict__[key]\n<\/code><\/pre>\n<p>And now we get the error:<\/p>\n<blockquote>\n<p>kedro.framework.context.context.KedroContextError: Expected an instance of <code>ConfigLoader<\/code>, got <code>NoneType<\/code> instead.<\/p>\n<\/blockquote>\n<p>It's because the hook register_config_loader from the project it's not being used by the hook_manager that calls the function.<\/p>\n<p>The project hooks are the defined the following way:<\/p>\n<pre><code>class ProjectHooks:\n\n    @hook_impl\n\n    def register_pipelines(self) -&gt; Dict[str, Pipeline]:\n\n        &quot;&quot;&quot;Register the project's pipeline.\n\n        Returns:\n\n            A mapping from a pipeline name to a ``Pipeline`` object.\n\n        &quot;&quot;&quot;\n\n        pm = pre_master.create_pipeline()\n\n        return {\n\n            &quot;pre_master&quot;: pm,\n\n            &quot;__default__&quot;: pm\n\n        }\n\n    @hook_impl\n\n    def register_config_loader(self, conf_paths: Iterable[str]) -&gt; ConfigLoader:\n\n        return ConfigLoader(conf_paths)\n\n    @hook_impl\n\n    def register_catalog(\n\n        self,\n\n        catalog: Optional[Dict[str, Dict[str, Any]]],\n\n        credentials: Dict[str, Dict[str, Any]],\n\n        load_versions: Dict[str, str],\n\n        save_version: str,\n\n        journal: Journal,\n\n    ) -&gt; DataCatalog:\n\n        return DataCatalog.from_config(\n\n            catalog, credentials, load_versions, save_version, journal\n\n        )\n\nproject_hooks = ProjectHooks()\n<\/code><\/pre>\n<p>And the settings are called the following way:\n&quot;&quot;&quot;Project settings.&quot;&quot;&quot;<\/p>\n<pre><code>from price_based_trading.hooks import ProjectHooks\n\n\nHOOKS = (ProjectHooks(),)\n<\/code><\/pre>\n<p>How can we configure that in a way that the hooks are used calling the method load_context(_working_dir).catalog.datasets ?<\/p>\n<p>I posted the same question in the kedro community: <a href=\"https:\/\/discourse.kedro.community\/t\/how-to-load-a-specific-catalog-item-in-kedro-0-17-0\/310\" rel=\"nofollow noreferrer\">https:\/\/discourse.kedro.community\/t\/how-to-load-a-specific-catalog-item-in-kedro-0-17-0\/310<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3.0,
        "Question_creation_time":1612267210303,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":1298.0,
        "Owner_creation_time":1462800865783,
        "Owner_last_access_time":1663944240728,
        "Owner_reputation":316.0,
        "Owner_up_votes":15.0,
        "Owner_down_votes":1.0,
        "Owner_views":18.0,
        "Answer_body":"<p>It was a silly mistake because I was not creating the Kedro session. To load an item of the catalog it can be done with the following code:<\/p>\n<pre><code>from kedro.framework.session import get_current_session\nfrom kedro.framework.session import KedroSession\n\nKedroSession.create(&quot;name_of_proyect&quot;) as session:\n    key = &quot;item_of_catalog&quot;\n    session = get_current_session()\n    context = session.load_context()\n    kedro_connector = context.catalog.datasets.__dict__[key] \n    \/\/ or kedro_connector = context.catalog._get_datasets(key)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1612452830192,
        "Answer_score":2.0,
        "Owner_location":null,
        "Question_last_edit_time":1612461225272,
        "Answer_last_edit_time":1616756019583,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66009324",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to load a specific catalog dataset instance in 0.17.0?; Content: we were using version 0.15.8 and we were loading one specific item from the catalog this way: from .context import load_context get_context().catalog.datasets.__dict__[key] now, we are changing to 0.17.0 and trying to load the catalogs datasets the same way(using the framework context): from .framework.context import load_context get_context().catalog.datasets.__dict__[key] and now we get the error: .framework.context.context.contexterror: expected an instance of configloader, got nonetype instead. it's because the hook register_config_loader from the project it's not being used by the hook_manager that calls the function. the project hooks are the defined the following way: class projecthooks: @hook_impl def register_pipelines(self) -> dict[str, pipeline]: \"\"\"register the project's pipeline. returns: a mapping from a pipeline name to a ``pipeline`` object. \"\"\" pm = pre_master.create_pipeline() return { \"pre_master\": pm, \"__default__\": pm } @hook_impl def register_config_loader(self, conf_paths: iterable[str]) -> configloader: return configloader(conf_paths) @hook_impl def register_catalog( self, catalog: optional[dict[str, dict[str, any]]], credentials: dict[str, dict[str, any]], load_versions: dict[str, str], save_version: str, journal: journal, ) -> datacatalog: return datacatalog.from_config( catalog, credentials, load_versions, save_version, journal ) project_hooks = projecthooks() and the settings are called the following way: \"\"\"project settings.\"\"\" from price_based_trading.hooks import projecthooks hooks = (projecthooks(),) how can we configure that in a way that the hooks are used calling the method load_context(_working_dir).catalog.datasets ? i posted the same question in the community: https:\/\/discourse..community\/t\/how-to-load-a-specific-catalog-item-in--0-17-0\/310",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in loading a specific catalog dataset instance in version 0.17.0 of a framework, and is looking for a way to configure the project settings to use the hooks when calling the method load_context(_working_dir).catalog.datasets.",
        "Question_preprocessed_content":"Title: how to load a specific catalog dataset instance in; Content: we were using version and we were loading one specific item from the catalog this way now, we are changing to and trying to load the catalogs datasets the same way and now we get the error expected an instance of , got instead. it's because the hook from the project it's not being used by the that calls the function. the project hooks are the defined the following way and the settings are called the following way project how can we configure that in a way that the hooks are used calling the method ? i posted the same question in the community",
        "Answer_original_content":"it was a silly mistake because i was not creating the session. to load an item of the catalog it can be done with the following code: from .framework.session import get_current_session from .framework.session import session session.create(\"name_of_proyect\") as session: key = \"item_of_catalog\" session = get_current_session() context = session.load_context() _connector = context.catalog.datasets.__dict__[key] \/\/ or _connector = context.catalog._get_datasets(key)",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"it was a silly mistake because i was not creating the session. to load an item of the catalog it can be done with the following code"
    },
    {
        "Question_id":71385524.0,
        "Question_title":"Sagemaker training job Fatal error: cannot open file 'train': No such file or directory",
        "Question_body":"<p>I am trying work on bring your own model. I have R code. when i try to run the job its failing.<\/p>\n<p><strong>Training Image:<\/strong><\/p>\n<pre><code>FROM r-base:3.6.3\n\nMAINTAINER Amazon SageMaker Examples &lt;amazon-sagemaker-examples@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n    wget \\\n    r-base \\\n    r-base-dev \\\n    apt-transport-https \\\n    ca-certificates \\\n    python3 python3-dev pip\n\nENV AWS_DEFAULT_REGION=&quot;us-east-2&quot;\nRUN R -e &quot;install.packages('reticulate', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('readr', dependencies = TRUE, warning = function(w) stop(w))&quot;\nRUN R -e &quot;install.packages('dplyr', dependencies = TRUE, warning = function(w) stop(w))&quot;\n\nRUN pip install --quiet --no-cache-dir \\\n    'boto3&gt;1.0&lt;2.0' \\\n    'sagemaker&gt;2.0&lt;3.0'    \n\nENTRYPOINT [&quot;\/usr\/bin\/Rscript&quot;]\n<\/code><\/pre>\n<p><strong>Source code:<\/strong><\/p>\n<pre><code>rcode\n    \u2514\u2500\u2500 train.R\n    \u2514\u2500\u2500 train.tar.gz\n<\/code><\/pre>\n<p>Build<\/p>\n<pre><code>- aws s3 cp $CODEBUILD_SRC_DIR\/rcode\/ s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training --recursive\n<\/code><\/pre>\n<p><strong>Serverless.com yaml<\/strong><\/p>\n<pre><code>           SagemakerRCodeTrainingStep:\n            Type: Task\n            Resource: ${self:custom.sageMakerTrainingJob}\n            Parameters:\n              TrainingJobName.$: &quot;$.sageMakerTrainingJobName&quot;\n              DebugHookConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              AlgorithmSpecification:\n                TrainingImage: ${self:custom.sagemakerRExecutionContainerURI}\n                TrainingInputMode: &quot;File&quot;\n              OutputDataConfig:\n                S3OutputPath: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/models\/rmodel&quot;\n              StoppingCondition:\n                MaxRuntimeInSeconds: ${self:custom.maxRuntime}\n              ResourceConfig:\n                InstanceCount: 1\n                InstanceType: &quot;ml.m5.xlarge&quot;\n                VolumeSizeInGB: 30\n              RoleArn: ${self:custom.stateMachineRoleARN}\n              InputDataConfig:\n                - DataSource:\n                    S3DataSource:\n                      S3DataType: &quot;S3Prefix&quot;\n                      S3Uri: &quot;s3:\/\/${self:custom.datasetsFilePath}\/data\/processed\/train&quot;\n                      S3DataDistributionType: &quot;FullyReplicated&quot;\n                  ChannelName: &quot;train&quot;\n              HyperParameters:\n                sagemaker_submit_directory: &quot;s3:\/\/${self:custom.deploymentBucket}\/${self:service}\/code\/training\/train.tar.gz&quot;\n                sagemaker_program: &quot;train.R&quot;\n                sagemaker_enable_cloudwatch_metrics: &quot;false&quot;\n                sagemaker_container_log_level: &quot;20&quot;\n                sagemaker_job_name: &quot;sagemaker-r-learn-2022-02-28-09-56-33-234&quot;\n                sagemaker_region: ${self:provider.region}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1646676784137,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":369.0,
        "Owner_creation_time":1285763771347,
        "Owner_last_access_time":1658319981990,
        "Owner_reputation":6958.0,
        "Owner_up_votes":123.0,
        "Owner_down_votes":1.0,
        "Owner_views":787.0,
        "Answer_body":"<p>I am not sure which <code>TrainingImage<\/code> you are using and all the files in your container.\nThat being said, I suspect you are using a custom container.<\/p>\n<p>SageMaker Training Jobs look for a <code>train<\/code> file and run your container as <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">follows<\/a>:<\/p>\n<pre><code>docker run image train\n<\/code><\/pre>\n<p>You can change this behavior by setting the <code>ENTRYPOINT<\/code> in your Dockerfile. Please see this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/r_examples\/r_byo_r_algo_hpo\/Dockerfile#L47\" rel=\"nofollow noreferrer\">Dockerfile<\/a> from the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/r_examples\/r_byo_r_algo_hpo\" rel=\"nofollow noreferrer\">r_byo_r_algo_hpo<\/a> example.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1646690790176,
        "Answer_score":1.0,
        "Owner_location":"Bangalore, India",
        "Question_last_edit_time":1646720798767,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71385524",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: training job fatal error: cannot open file 'train': no such file or directory; Content: i am trying work on bring your own model. i have r code. when i try to run the job its failing. training image: from r-base:3.6.3 maintainer examples <amazon--examples@amazon.com> run apt-get -y update && apt-get install -y --no-install-recommends \\ wget \\ r-base \\ r-base-dev \\ apt-transport-https \\ ca-certificates \\ python3 python3-dev pip env aws_default_region=\"us-east-2\" run r -e \"install.packages('reticulate', dependencies = true, warning = function(w) stop(w))\" run r -e \"install.packages('readr', dependencies = true, warning = function(w) stop(w))\" run r -e \"install.packages('dplyr', dependencies = true, warning = function(w) stop(w))\" run pip install --quiet --no-cache-dir \\ 'boto3>1.0<2.0' \\ '>2.0<3.0' entrypoint [\"\/usr\/bin\/rscript\"] source code: rcode \u2514\u2500\u2500 train.r \u2514\u2500\u2500 train.tar.gz build - aws s3 cp $codebuild_src_dir\/rcode\/ s3:\/\/${self:custom.deploymentbucket}\/${self:service}\/code\/training --recursive serverless.com yaml rcodetrainingstep: type: task resource: ${self:custom.trainingjob} parameters: trainingjobname.$: \"$.trainingjobname\" debughookconfig: s3outputpath: \"s3:\/\/${self:custom.deploymentbucket}\/${self:service}\/models\/rmodel\" algorithmspecification: trainingimage: ${self:custom.rexecutioncontaineruri} traininginputmode: \"file\" outputdataconfig: s3outputpath: \"s3:\/\/${self:custom.deploymentbucket}\/${self:service}\/models\/rmodel\" stoppingcondition: maxruntimeinseconds: ${self:custom.maxruntime} resourceconfig: instancecount: 1 instancetype: \"ml.m5.xlarge\" volumesizeingb: 30 rolearn: ${self:custom.statemachinerolearn} inputdataconfig: - datasource: s3datasource: s3datatype: \"s3prefix\" s3uri: \"s3:\/\/${self:custom.datasetsfilepath}\/data\/processed\/train\" s3datadistributiontype: \"fullyreplicated\" channelname: \"train\" hyperparameters: _submit_directory: \"s3:\/\/${self:custom.deploymentbucket}\/${self:service}\/code\/training\/train.tar.gz\" _program: \"train.r\" _enable_cloudwatch_metrics: \"false\" _container_log_level: \"20\" _job_name: \"-r-learn-2022-02-28-09-56-33-234\" _region: ${self:provider.region}",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with running a training job on Amazon SageMaker, resulting in a fatal error due to a missing file.",
        "Question_preprocessed_content":"Title: training job fatal error cannot open file 'train' no such file or directory; Content: i am trying work on bring your own model. i have r code. when i try to run the job its failing. training image source code build yaml",
        "Answer_original_content":"i am not sure which trainingimage you are using and all the files in your container. that being said, i suspect you are using a custom container. training jobs look for a train file and run your container as follows: docker run image train you can change this behavior by setting the entrypoint in your dockerfile. please see this example dockerfile from the r_byo_r_algo_hpo example.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i am not sure which you are using and all the files in your container. that being said, i suspect you are using a custom container. training jobs look for a file and run your container as follows you can change this behavior by setting the in your dockerfile. please see this example dockerfile from the example."
    },
    {
        "Question_id":null,
        "Question_title":"Endpoint \"predict-auto-price\" deployment failed",
        "Question_body":"I'm following the \"Exercise - Explore regression with Azure Machine Learning designer\"\n\nI've redone this exercise twice following all the instructions perfectly!\n\nOn the \"Deploy a service\" section in the exercise, I've tried it over 7 times:\n\n4. In the configuration screen, select Deploy a new real-time endpoint, using the following settings:\n\nName: predict-auto-price\nDescription: Auto price regression\nCompute type: Azure Container Instance\n\nIt always fails with \"Endpoint \"predict-auto-price\" deployment failed\"\n\nAnd nothing comes up in the Deployment Logs.\nPlease help! I'm out of idea's\n\n\"\"",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1661430065143,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/981735\/endpoint-34predict-auto-price34-deployment-failed.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-26T11:10:26.997Z",
                "Answer_score":1,
                "Answer_body":"@MervynKing-2075 Do you see any new jobs added on the jobs tab when your deployment fails? Usually, there is a drop down that loads on the Deployment logs tab to select the Deployment. But, I dont see that on your page. It looks like the page might not have loaded completely. You can also report the issue using the smiley icon on the top right corner with a screen shot of this page.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-08-31T17:46:02.323Z",
                "Answer_score":0,
                "Answer_body":"When you are deploying, you must increase CPU reserve capacity to 0.5. I think the default value of 0.1 is not enough for the predictive service deployment.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: endpoint \"predict-auto-price\" deployment failed; Content: i'm following the \"exercise - explore regression with designer\" i've redone this exercise twice following all the instructions perfectly! on the \"deploy a service\" section in the exercise, i've tried it over 7 times: 4. in the configuration screen, select deploy a new real-time endpoint, using the following settings: name: predict-auto-price description: auto price regression compute type: azure container instance it always fails with \"endpoint \"predict-auto-price\" deployment failed\" and nothing comes up in the deployment logs. please help! i'm out of idea's \"\"",
        "Question_original_content_gpt_summary":"The user is encountering challenges with deploying a new real-time endpoint named \"predict-auto-price\" using Azure Container Instance, as the deployment fails and no logs are generated.",
        "Question_preprocessed_content":"Title: endpoint predict auto price deployment failed; Content: i'm following the exercise explore regression with designer i've redone this exercise twice following all the instructions perfectly! on the deploy a service section in the exercise, i've tried it over times . in the configuration screen, select deploy a new real time endpoint, using the following settings name predict auto price description auto price regression compute type azure container instance it always fails with endpoint predict auto price deployment failed and nothing comes up in the deployment logs. please help! i'm out of idea's",
        "Answer_original_content":"@mervynking-2075 do you see any new jobs added on the jobs tab when your deployment fails? usually, there is a drop down that loads on the deployment logs tab to select the deployment. but, i dont see that on your page. it looks like the page might not have loaded completely. you can also report the issue using the smiley icon on the top right corner with a screen shot of this page. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"do you see any new jobs added on the jobs tab when your deployment fails? usually, there is a drop down that loads on the deployment logs tab to select the deployment. but, i dont see that on your page. it looks like the page might not have loaded completely. you can also report the issue using the smiley icon on the top right corner with a screen shot of this page. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":71713641.0,
        "Question_title":"How to plot on kedro mlflow ui x1=array\/list\/dict and y1=array\/list\/dict?",
        "Question_body":"<p>I am new to kedro, and I don't know if I am asking the right question here.<\/p>\n<p>Is it possible on kedro mlflow ui to plot x and y lists?<\/p>\n<p>I am running kedro pipeline with mlflow. I have catalog.yaml which I log metrics and artifacts.<\/p>\n<p>The end goal is:<\/p>\n<p>kedro run 1  # generate x1[1,2,3,4] and y1=[1,2,2,2] these numbers are just examples<\/p>\n<p>kedro run 2  # generate x2[1,2,3,4] and y2=[3,1,2,1] these numbers are just examples<\/p>\n<p>kedro run 3  # generate x2[1,2,3,4] and y2=[1,3,3,3] these numbers are just examples<\/p>\n<p>then kedro mlflow ui<\/p>\n<p>select run1, run2, and run3 then click compare.<\/p>\n<p>on scatter plot ---&gt; able to select x1, x2, and x3 and for y axis able to select y1, y2, and y3<\/p>\n<p>then I should be able to see plot with three lines.<\/p>\n<p>Something like this:\n<a href=\"https:\/\/i.stack.imgur.com\/Pfg9C.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pfg9C.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>thank you for your help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1648855588277,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":44.0,
        "Owner_creation_time":1403493187580,
        "Owner_last_access_time":1663806955772,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":29.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71713641",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to plot on mlflow ui x1=array\/list\/dict and y1=array\/list\/dict?; Content: i am new to , and i don't know if i am asking the right question here. is it possible on mlflow ui to plot x and y lists? i am running pipeline with mlflow. i have catalog.yaml which i log metrics and artifacts. the end goal is: run 1 # generate x1[1,2,3,4] and y1=[1,2,2,2] these numbers are just examples run 2 # generate x2[1,2,3,4] and y2=[3,1,2,1] these numbers are just examples run 3 # generate x2[1,2,3,4] and y2=[1,3,3,3] these numbers are just examples then mlflow ui select run1, run2, and run3 then click compare. on scatter plot ---> able to select x1, x2, and x3 and for y axis able to select y1, y2, and y3 then i should be able to see plot with three lines. something like this: thank you for your help.",
        "Question_original_content_gpt_summary":"The user is looking for a way to plot x and y lists on the MLflow UI and compare the results of multiple runs.",
        "Question_preprocessed_content":"Title: how to plot on mlflow ui and; Content: i am new to , and i don't know if i am asking the right question here. is it possible on mlflow ui to plot x and y lists? i am running pipeline with mlflow. i have which i log metrics and artifacts. the end goal is run generate x , , , and y , , , these numbers are just examples run generate x , , , and y , , , these numbers are just examples run generate x , , , and y , , , these numbers are just examples then mlflow ui select run , run , and run then click compare. on scatter plot > able to select x , x , and x and for y axis able to select y , y , and y then i should be able to see plot with three lines. something like this thank you for your help.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":44245265.0,
        "Question_title":"Azure Machine Learning + R: Reading files",
        "Question_body":"<p>I am trying to run my R-code in the Azure Machine Learning environment, and I am running into a dead end trying to import data from the blob storage.<\/p>\n\n<p>Locally I can easily import a file like this:<\/p>\n\n<pre><code>data &lt;- read.delim(\"myfile.xls\", sep = \"\\t\", skip = 9)\n<\/code><\/pre>\n\n<p>When I am using Azure machine learning, just referring to file in the blob storage location, I get the error \"cannot open the connection\".<\/p>\n\n<pre><code>data &lt;- read.delim(\"https:\/\/knnstorage.blob.core.windows.net\/knn\/myfile.xls\", sep = \"\\t\", skip = 9)\n<\/code><\/pre>\n\n<p>I have additionally attempted to import the file using the \"Import Data\" option, but it only allows one to import csv files or excel-files with at the most one line to be skipped.<\/p>\n\n<p>In the future I will also need to import unstructured text into Azure Machine Learning. Is this really not possible using R?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_creation_time":1496069269853,
        "Question_favorite_count":0.0,
        "Question_score":3.0,
        "Question_view_count":359.0,
        "Owner_creation_time":1346939900920,
        "Owner_last_access_time":1663852618247,
        "Owner_reputation":2545.0,
        "Owner_up_votes":339.0,
        "Owner_down_votes":16.0,
        "Owner_views":381.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44245265",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: + r: reading files; Content: i am trying to run my r-code in the environment, and i am running into a dead end trying to import data from the blob storage. locally i can easily import a file like this: data <- read.delim(\"myfile.xls\", sep = \"\\t\", skip = 9) when i am using , just referring to file in the blob storage location, i get the error \"cannot open the connection\". data <- read.delim(\"https:\/\/knnstorage.blob.core.windows.net\/knn\/myfile.xls\", sep = \"\\t\", skip = 9) i have additionally attempted to import the file using the \"import data\" option, but it only allows one to import csv files or excel-files with at the most one line to be skipped. in the future i will also need to import unstructured text into . is this really not possible using r?",
        "Question_original_content_gpt_summary":"The user is encountering challenges when trying to import data from a blob storage into R, as they are unable to open the connection and the \"import data\" option only allows for CSV or Excel files with one line to be skipped.",
        "Question_preprocessed_content":"Title: + r reading files; Content: i am trying to run my r code in the environment, and i am running into a dead end trying to import data from the blob storage. locally i can easily import a file like this when i am using , just referring to file in the blob storage location, i get the error cannot open the connection . i have additionally attempted to import the file using the import data option, but it only allows one to import csv files or excel files with at the most one line to be skipped. in the future i will also need to import unstructured text into . is this really not possible using r?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":67051812.0,
        "Question_title":"SageMaker : Sckit-learn RandomForest : REST API Value ERROR",
        "Question_body":"<p>My Lambda Code is Below.<\/p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\nimport csv\n\n# grab environment variables\nENDPOINT_NAME = os.environ['ENDPOINT_NAME']\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    \n    data = json.loads(json.dumps(event))\n    payload = data['data']\n    print(payload)\n    \n    wrapper = csv.reader(payload.strip().split('\\n'))\n    for record in wrapper:\n        print(record)\n   \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                       ContentType='text\/csv',\n                                       Body=wrapper)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    \n    return result\n<\/code><\/pre>\n<p>My input value is<\/p>\n<pre><code>{\n  &quot;data&quot;: &quot;231, -43&quot;\n}\n<\/code><\/pre>\n<p>Error message is<\/p>\n<pre><code>Response\n{\n  &quot;errorMessage&quot;: &quot;Parameter validation failed:\\nInvalid type for parameter Body, value: &lt;_csv.reader object at 0x7f638af1a6d8&gt;, type: &lt;class '_csv.reader'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,\n  &quot;errorType&quot;: &quot;ParamValidationError&quot;,\n  &quot;stackTrace&quot;: [\n    [\n      &quot;\/var\/task\/lambda_function.py&quot;,\n      24,\n      &quot;lambda_handler&quot;,\n      &quot;Body=wrapper)&quot;\n    ],\n    [\n      &quot;\/var\/runtime\/botocore\/client.py&quot;,\n      357,\n      &quot;_api_call&quot;,\n      &quot;return self._make_api_call(operation_name, kwargs)&quot;\n    ],\n    [\n      &quot;\/var\/runtime\/botocore\/client.py&quot;,\n      649,\n      &quot;_make_api_call&quot;,\n      &quot;api_params, operation_model, context=request_context)&quot;\n    ],\n    [\n      &quot;\/var\/runtime\/botocore\/client.py&quot;,\n      697,\n      &quot;_convert_to_request_dict&quot;,\n      &quot;api_params, operation_model)&quot;\n    ],\n    [\n      &quot;\/var\/runtime\/botocore\/validate.py&quot;,\n      293,\n      &quot;serialize_to_request&quot;,\n      &quot;raise ParamValidationError(report=report.generate_report())&quot;\n    ]\n  ]\n}\n\nFunction Logs\nSTART RequestId: 0e33f157-ae77-4524-96f2-78a2fe82bf5b Version: $LATEST\nReceived event: {\n  &quot;data&quot;: &quot;231, -43&quot;\n}\n231, -43\n['231', ' -43']\nParameter validation failed:\nInvalid type for parameter Body, value: &lt;_csv.reader object at 0x7f638af1a6d8&gt;, type: &lt;class '_csv.reader'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object: ParamValidationError\nTraceback (most recent call last):\n  File &quot;\/var\/task\/lambda_function.py&quot;, line 24, in lambda_handler\n    Body=wrapper)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 357, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 649, in _make_api_call\n    api_params, operation_model, context=request_context)\n  File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 697, in _convert_to_request_dict\n    api_params, operation_model)\n  File &quot;\/var\/runtime\/botocore\/validate.py&quot;, line 293, in serialize_to_request\n    raise ParamValidationError(report=report.generate_report())\nbotocore.exceptions.ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: &lt;_csv.reader object at 0x7f638af1a6d8&gt;, type: &lt;class '_csv.reader'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n\nEND RequestId: 0e33f157-ae77-4524-96f2-78a2fe82bf5b\nREPORT RequestId: 0e33f157-ae77-4524-96f2-78a2fe82bf5b  Duration: 19.80 ms  Billed Duration: 20 ms  Memory Size: 128 MB Max Memory Used: 68 MB  Init Duration: 261.73 ms\n\nRequest ID\n0e33f157-ae77-4524-96f2-78a2fe82bf5b\n<\/code><\/pre>\n<p>I don't know the reason about the error.<\/p>\n<p>Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1618192960303,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":21.0,
        "Owner_creation_time":1569889641900,
        "Owner_last_access_time":1620106631043,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67051812",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: : sckit-learn randomforest : rest api value error; Content: my lambda code is below. import os import io import boto3 import json import csv # grab environment variables endpoint_name = os.environ['endpoint_name'] runtime= boto3.client('runtime.') def lambda_handler(event, context): print(\"received event: \" + json.dumps(event, indent=2)) data = json.loads(json.dumps(event)) payload = data['data'] print(payload) wrapper = csv.reader(payload.strip().split('\\n')) for record in wrapper: print(record) response = runtime.invoke_endpoint(endpointname=endpoint_name, contenttype='text\/csv', body=wrapper) print(response) result = json.loads(response['body'].read().decode()) return result my input value is { \"data\": \"231, -43\" } error message is response { \"errormessage\": \"parameter validation failed:\\ninvalid type for parameter body, value: <_csv.reader object at 0x7f638af1a6d8>, type: <class '_csv.reader'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\", \"errortype\": \"paramvalidationerror\", \"stacktrace\": [ [ \"\/var\/task\/lambda_function.py\", 24, \"lambda_handler\", \"body=wrapper)\" ], [ \"\/var\/runtime\/botocore\/client.py\", 357, \"_api_call\", \"return self._make_api_call(operation_name, kwargs)\" ], [ \"\/var\/runtime\/botocore\/client.py\", 649, \"_make_api_call\", \"api_params, operation_model, context=request_context)\" ], [ \"\/var\/runtime\/botocore\/client.py\", 697, \"_convert_to_request_dict\", \"api_params, operation_model)\" ], [ \"\/var\/runtime\/botocore\/validate.py\", 293, \"serialize_to_request\", \"raise paramvalidationerror(report=report.generate_report())\" ] ] } function logs start requestid: 0e33f157-ae77-4524-96f2-78a2fe82bf5b version: $latest received event: { \"data\": \"231, -43\" } 231, -43 ['231', ' -43'] parameter validation failed: invalid type for parameter body, value: <_csv.reader object at 0x7f638af1a6d8>, type: <class '_csv.reader'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object: paramvalidationerror traceback (most recent call last): file \"\/var\/task\/lambda_function.py\", line 24, in lambda_handler body=wrapper) file \"\/var\/runtime\/botocore\/client.py\", line 357, in _api_call return self._make_api_call(operation_name, kwargs) file \"\/var\/runtime\/botocore\/client.py\", line 649, in _make_api_call api_params, operation_model, context=request_context) file \"\/var\/runtime\/botocore\/client.py\", line 697, in _convert_to_request_dict api_params, operation_model) file \"\/var\/runtime\/botocore\/validate.py\", line 293, in serialize_to_request raise paramvalidationerror(report=report.generate_report()) botocore.exceptions.paramvalidationerror: parameter validation failed: invalid type for parameter body, value: <_csv.reader object at 0x7f638af1a6d8>, type: <class '_csv.reader'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object end requestid: 0e33f157-ae77-4524-96f2-78a2fe82bf5b report requestid: 0e33f157-ae77-4524-96f2-78a2fe82bf5b duration: 19.80 ms billed duration: 20 ms memory size: 128 mb max memory used: 68 mb init duration: 261.73 ms request id 0e33f157-ae77-4524-96f2-78a2fe82bf5b i don't know the reason about the error. thank you.",
        "Question_original_content_gpt_summary":"The user encountered a ValueError when attempting to invoke an endpoint using scikit-learn's randomforest rest api.",
        "Question_preprocessed_content":"Title: sckit learn randomforest rest api value error; Content: my lambda code is below. my input value is error message is i don't know the reason about the error. thank you.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Select column by name option is faded in edit metadata object in Azure ML studio",
        "Question_body":"As shown in the image, by name option is faded. I have already ran the pipeline and it was successful. Then why not adding another step in pipeline shows the output of imported data ?",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1653833837490,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/868582\/select-column-by-name-option-is-faded-in-edit-meta.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-30T13:11:43.96Z",
                "Answer_score":1,
                "Answer_body":"@TanwarGauravSingh-9735 I think this issue is related to this thread. Recent changes to UI have rendered this functionality unusable. The issue is reported to the product group for review, meanwhile you could use the preview option from Import Data to lookup the column names that need to be used in Select Columns from dataset. I hope this helps!!",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-16T16:17:53.917Z",
                "Answer_score":1,
                "Answer_body":"Hi, I'm also facing the same error still. Could you please help to select the By name",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-16T17:08:53.983Z",
                "Answer_score":1,
                "Answer_body":"Just enter the column name, its how i solved the issue",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: select column by name option is faded in edit metadata object in studio; Content: as shown in the image, by name option is faded. i have already ran the pipeline and it was successful. then why not adding another step in pipeline shows the output of imported data ?",
        "Question_original_content_gpt_summary":"The user is encountering an issue where the \"by name\" option is faded in the Edit Metadata Object in Studio, despite having successfully run a pipeline.",
        "Question_preprocessed_content":"Title: select column by name option is faded in edit metadata object in studio; Content: as shown in the image, by name option is faded. i have already ran the pipeline and it was successful. then why not adding another step in pipeline shows the output of imported data ?",
        "Answer_original_content":"@tanwargauravsingh-9735 i think this issue is related to this thread. recent changes to ui have rendered this functionality unusable. the issue is reported to the product group for review, meanwhile you could use the preview option from import data to lookup the column names that need to be used in select columns from dataset. i hope this helps!! hi, i'm also facing the same error still. could you please help to select the by name just enter the column name, its how i solved the issue",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i think this issue is related to this thread. recent changes to ui have rendered this functionality unusable. the issue is reported to the product group for review, meanwhile you could use the preview option from import data to lookup the column names that need to be used in select columns from dataset. i hope this helps!! hi, i'm also facing the same error still. could you please help to select the by name just enter the column name, its how i solved the issue"
    },
    {
        "Question_id":57597351.0,
        "Question_title":"Running Ludwig on AML Compute: docker image failing to build on gmpy",
        "Question_body":"<p>I'm currently trying to create a TensorFlow estimator to run Ludwig's training model on Azure ML Compute with various pip and conda packages like so:<\/p>\n\n<pre><code>estimator= TensorFlow(source_directory= project_folder,\ncompute_target=compute_target, script_params=script_params, \nentry_script='.\/train.py', pip_packages=dependencies, conda_packages = \n[\"tensorflow\"], use_gpu =True)\n<\/code><\/pre>\n\n<p>One of the pip packages is gmpy, but it will not install and throws an <code>error: fatal error: gmp.h: No such file or directory compilation terminated. error: command 'gcc' failed with exit status 1<\/code>.<\/p>\n\n<p>This prevents Ludwig from installing and causes the imagine to fail to build<\/p>\n\n<p>When I run Ludwig locally in a python virtual environment on Ubuntu, I'm able to work around this issue by running \u201csudo apt-get install libgmp3-dev\u201d instead of <code>pip install gmpy<\/code>. When I try adding Gmpy2 as a library to the estimator, it throws the same error, and it seems that libgmp3-dev doesn't have a pip or conda equivalent. I tried adding the gmpy and gmpy2 .whl files directly to the environment but the wheel files were not recognized as compatible.<\/p>\n\n<p>Is there some way to add <code>RUN sudo apt-get install libgmp3-dev<\/code> to the dockerfile so that the docker container made by the estimator has this already installed without needing to create a custom dockerfile? I noticed that the TensorFlow estimator class has an \"environment_definition\" flag that can take a DockerSection but I can't find any examples of how they work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1566412065597,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":161.0,
        "Owner_creation_time":1566410815648,
        "Owner_last_access_time":1567550728812,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1566418140743,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57597351",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: running ludwig on aml compute: docker image failing to build on gmpy; Content: i'm currently trying to create a tensorflow estimator to run ludwig's training model on compute with various pip and conda packages like so: estimator= tensorflow(source_directory= project_folder, compute_target=compute_target, script_params=script_params, entry_script='.\/train.py', pip_packages=dependencies, conda_packages = [\"tensorflow\"], use_gpu =true) one of the pip packages is gmpy, but it will not install and throws an error: fatal error: gmp.h: no such file or directory compilation terminated. error: command 'gcc' failed with exit status 1. this prevents ludwig from installing and causes the imagine to fail to build when i run ludwig locally in a python virtual environment on ubuntu, i'm able to work around this issue by running \u201csudo apt-get install libgmp3-dev\u201d instead of pip install gmpy. when i try adding gmpy2 as a library to the estimator, it throws the same error, and it seems that libgmp3-dev doesn't have a pip or conda equivalent. i tried adding the gmpy and gmpy2 .whl files directly to the environment but the wheel files were not recognized as compatible. is there some way to add run sudo apt-get install libgmp3-dev to the dockerfile so that the docker container made by the estimator has this already installed without needing to create a custom dockerfile? i noticed that the tensorflow estimator class has an \"environment_definition\" flag that can take a dockersection but i can't find any examples of how they work.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with running Ludwig on Azure Machine Learning Compute, as the Docker image is failing to build due to an error with the gmpy package, and they are unable to find a way to add the necessary libgmp3-dev to the Dockerfile.",
        "Question_preprocessed_content":"Title: running ludwig on aml compute docker image failing to build on gmpy; Content: i'm currently trying to create a tensorflow estimator to run ludwig's training model on compute with various pip and conda packages like so one of the pip packages is gmpy, but it will not install and throws an . this prevents ludwig from installing and causes the imagine to fail to build when i run ludwig locally in a python virtual environment on ubuntu, i'm able to work around this issue by running sudo apt get install libgmp dev instead of . when i try adding gmpy as a library to the estimator, it throws the same error, and it seems that libgmp dev doesn't have a pip or conda equivalent. i tried adding the gmpy and gmpy .whl files directly to the environment but the wheel files were not recognized as compatible. is there some way to add to the dockerfile so that the docker container made by the estimator has this already installed without needing to create a custom dockerfile? i noticed that the tensorflow estimator class has an flag that can take a dockersection but i can't find any examples of how they work.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Studio Notebooks folder structure",
        "Question_body":"Is it possible to create non-user folders for Notebooks in Azure ML Studio? I would look to organize my notebooks into folders, but outside of my persona user folder structure (similar to how Azure Databricks allows you to create folders within the workspace) \u2013 doesn\u2019t look like that is possible, but wanted to double check.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1598909886827,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/82485\/azure-ml-studio-notebooks-folder-structure.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-08-31T22:37:02.513Z",
                "Answer_score":1,
                "Answer_body":"Hi, thanks for reaching out. Currently, there's no way to create non-user folders in AML Studio. All files and folders and stored in the user's location. Hope this helps.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: studio notebooks folder structure; Content: is it possible to create non-user folders for notebooks in studio? i would look to organize my notebooks into folders, but outside of my persona user folder structure (similar to how azure databricks allows you to create folders within the workspace) \u2013 doesn\u2019t look like that is possible, but wanted to double check.",
        "Question_original_content_gpt_summary":"The user is looking to organize their notebooks into folders outside of their personal user folder structure, similar to how Azure Databricks allows, but is unsure if this is possible.",
        "Question_preprocessed_content":"Title: studio notebooks folder structure; Content: is it possible to create non user folders for notebooks in studio? i would look to organize my notebooks into folders, but outside of my persona user folder structure doesnt look like that is possible, but wanted to double check.",
        "Answer_original_content":"hi, thanks for reaching out. currently, there's no way to create non-user folders in aml studio. all files and folders and stored in the user's location. hope this helps.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, thanks for reaching out. currently, there's no way to create non user folders in aml studio. all files and folders and stored in the user's location. hope this helps."
    },
    {
        "Question_id":70873792.0,
        "Question_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Question_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1643261540133,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":357.0,
        "Owner_creation_time":1597858315076,
        "Owner_last_access_time":1663976401100,
        "Owner_reputation":84.0,
        "Owner_up_votes":12.0,
        "Owner_down_votes":0.0,
        "Owner_views":15.0,
        "Answer_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1644424499008,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1644528409280,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to handle batch transform discarding a file with a failed model request; Content: i have a large number of json requests for a model split across multiple files in an s3 bucket. i would like to use 's batch transform feature to process all of these requests (i have done a couple of test runs using small amounts of data and the transform job succeeds). my main issue is here (https:\/\/docs.aws.amazon.com\/\/latest\/dg\/batch-transform.html#batch-transform-errors), specifically: if a batch transform job fails to process an input file because of a problem with the dataset, marks the job as failed. if an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. when your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. the processed files still generate useable results. this is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). i would ideally prefer to just write the output of the failed response to the file and keep going, rather than discarding the entire file. my question is, are there any suggestions to mitigating this issue? i was thinking about storing 1 request per file in s3, but this seems somewhat ridiculous? even if i did this, is there a good way of seeing which requests specifically failed after the transform job finishes?",
        "Question_original_content_gpt_summary":"The user is facing a challenge of how to handle batch transform discarding a file with a failed model request, and is looking for suggestions to mitigate this issue.",
        "Question_preprocessed_content":"Title: how to handle batch transform discarding a file with a failed model request; Content: i have a large number of json requests for a model split across multiple files in an s bucket. i would like to use 's batch transform feature to process all of these requests . my main issue is here , specifically if a batch transform job fails to process an input file because of a problem with the dataset, marks the job as failed. if an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. when your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. the processed files still generate useable results. this is not preferable mainly because if request fails in a file with a large number of requests, all of those requests will get discarded . i would ideally prefer to just write the output of the failed response to the file and keep going, rather than discarding the entire file. my question is, are there any suggestions to mitigating this issue? i was thinking about storing request per file in s , but this seems somewhat ridiculous? even if i did this, is there a good way of seeing which requests specifically failed after the transform job finishes?",
        "Answer_original_content":"you've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. the issue is that while you can pass a prefix with many files to createtransformjob, partitioning one datapoint per file at least requires an s3 read per datapoint, plus a model invocation per datapoint, which is probably not great. be aware also that apparently there are hidden rate limits. here are a couple options: partition into small-ish files, and plan on failures being rare. hopefully, not many of your datapoints would actually fail. if you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. note that has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on 's side. deal with failures directly in your model. the same doc you quoted in your question also says: if you are using your own algorithms, you can use placeholder text, such as error, when the algorithm finds a bad record in an input file. for example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file. note that the reason batch transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. if you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, batch transform will be happy.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"you've got the right idea the fewer datapoints are in each file, the less likely a given file is to fail. the issue is that while you can pass a prefix with many files to createtransformjob, partitioning one datapoint per file at least requires an s read per datapoint, plus a model invocation per datapoint, which is probably not great. be aware also that apparently there are hidden rate limits. here are a couple options partition into small ish files, and plan on failures being rare. hopefully, not many of your datapoints would actually fail. if you partition your dataset into files, then a single failure only requires reprocessing % of your data. note that has built in retries, too, so most of the time failures should be caused by your not randomness on 's side. deal with failures directly in your model. the same doc you quoted in your question also says if you are using your own algorithms, you can use placeholder text, such as error, when the algorithm finds a bad record in an input file. for example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file. note that the reason batch transform does this whole file failure is to maintain a mapping between rows in the input and the output. if you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, batch transform will be happy."
    },
    {
        "Question_id":null,
        "Question_title":"AzureML Error on Linux: \"Unable to retrieve .NET dependencies. Please make sure you are connected ...\"",
        "Question_body":"I am getting this error on a Linux box (Gentoo w\/ .NET via Mono properly installed)\n\n\"Unable to retrieve .NET dependencies. Please make sure you are connected to the Internet and have a stable network connection.\"\n\nThe error is triggered when creating a dataset from a directory using\n\n\"dataset = Dataset.File.from_files(path=(datastore, path_to_dataset_in_datastore))\"\n\nSome system info:\nPython: 3.8.8.\nazureml-automl-core 1.26.0\nazureml-core 1.26.0\nazureml-dataprep 2.13.2\nazureml-dataprep-native 32.0.0\nazureml-dataprep-rslex 1.11.2\nazureml-dataset-runtime 1.26.0\nazureml-pipeline 1.26.0\nazureml-pipeline-core 1.26.0\nazureml-pipeline-steps 1.26.0\nazureml-sdk 1.26.0\nazureml-telemetry 1.26.0\nazureml-train 1.26.0\nazureml-train-automl-client 1.26.0\nazureml-train-core 1.26.0\nazureml-train-restclients-hyperdrive 1.26.0\n\n.NET Info:\nMono JIT compiler version 6.6.0.161 (tarball Sat Apr 10 16:41:12 PDT 2021)\nCopyright (C) 2002-2014 Novell, Inc, Xamarin Inc and Contributors. www.mono-project.com\nTLS: __thread\nSIGSEGV: altstack\nNotifications: epoll\nArchitecture: amd64\nDisabled: none\nMisc: softdebug\nInterpreter: yes\nLLVM: supported, not enabled.\nSuspend: hybrid\nGC: sgen (concurrent by default)",
        "Question_answer_count":2,
        "Question_comment_count":2.0,
        "Question_creation_time":1618767312437,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/361522\/azureml-error-on-linux-34unable-to-retrieve-net-de.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-22T05:16:55.363Z",
                "Answer_score":0,
                "Answer_body":"@VictorFragoso-6349 Thanks for the details. Gentoo is not a 'natively' supported distribution of linux for Datasets. The Exception message doesn't link to a .NET docs page with instructions on installing the system dependencies required for .NET to work. Though it seems a different one is being thrown related to not being able to connect to out blob storage which has pre-prepared dependency sets for some linux distros (not gentoo).\n\nThis page Install .NET on Linux Distributions | Microsoft Docs does not detail support for .NET on gentoo.\nYou can get the names of the missing dependencies themselves by running:\n\n\n\n from dotnetcore2 import runtime\n runtime._enable_debug_logging()\n runtime.ensure_dependencies()\n\n\n\nThis code snippet should print the libraies missing required by .NET core 2.1.\nIf the above does not print anything, other than the Exception, then instead this should:\n\n from dotnetcore2 import runtime\n print(runtime._gather_dependencies(runtime._get_bin_folder()))",
                "Answer_comment_count":5,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-05-20T22:54:23.117Z",
                "Answer_score":0,
                "Answer_body":"Hi @ramr-msft I am facing the same issue while read my data from Datalake. Can you please help me out to resolve this issue. \n\n@VictorFragoso-6349 I try to install this package not the issue is still the same.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: error on linux: \"unable to retrieve .net dependencies. please make sure you are connected ...\"; Content: i am getting this error on a linux box (gentoo w\/ .net via mono properly installed) \"unable to retrieve .net dependencies. please make sure you are connected to the internet and have a stable network connection.\" the error is triggered when creating a dataset from a directory using \"dataset = dataset.file.from_files(path=(datastore, path_to_dataset_in_datastore))\" some system info: python: 3.8.8. -automl-core 1.26.0 -core 1.26.0 -dataprep 2.13.2 -dataprep-native 32.0.0 -dataprep-rslex 1.11.2 -dataset-runtime 1.26.0 -pipeline 1.26.0 -pipeline-core 1.26.0 -pipeline-steps 1.26.0 -sdk 1.26.0 -telemetry 1.26.0 -train 1.26.0 -train-automl-client 1.26.0 -train-core 1.26.0 -train-restclients-hyperdrive 1.26.0 .net info: mono jit compiler version 6.6.0.161 (tarball sat apr 10 16:41:12 pdt 2021) copyright (c) 2002-2014 novell, inc, xamarin inc and contributors. www.mono-project.com tls: __thread sigsegv: altstack notifications: epoll architecture: amd64 disabled: none misc: softdebug interpreter: yes llvm: supported, not enabled. suspend: hybrid gc: sgen (concurrent by default)",
        "Question_original_content_gpt_summary":"The user is encountering an error on a Linux box when creating a dataset from a directory, with the error message \"unable to retrieve .net dependencies. please make sure you are connected to the internet and have a stable network connection.\"",
        "Question_preprocessed_content":"Title: error on linux unable to retrieve .net dependencies. please make sure you are connected; Content: i am getting this error on a linux box unable to retrieve .net dependencies. please make sure you are connected to the internet and have a stable network the error is triggered when creating a dataset from a directory using dataset some system python automl core core dataprep dataprep native dataprep rslex dataset runtime pipeline pipeline core pipeline steps sdk telemetry train train automl client train core train restclients hyperdrive .net mono jit compiler version copyright novell, inc, xamarin inc and contributors. tls sigsegv altstack notifications epoll architecture amd disabled none misc softdebug interpreter yes llvm supported, not enabled. suspend hybrid gc sgen",
        "Answer_original_content":"@victorfragoso-6349 thanks for the details. gentoo is not a 'natively' supported distribution of linux for datasets. the exception message doesn't link to a .net docs page with instructions on installing the system dependencies required for .net to work. though it seems a different one is being thrown related to not being able to connect to out blob storage which has pre-prepared dependency sets for some linux distros (not gentoo). this page install .net on linux distributions | microsoft docs does not detail support for .net on gentoo. you can get the names of the missing dependencies themselves by running: from dotnetcore2 import runtime runtime._enable_debug_logging() runtime.ensure_dependencies() this code snippet should print the libraies missing required by .net core 2.1. if the above does not print anything, other than the exception, then instead this should: from dotnetcore2 import runtime print(runtime._gather_dependencies(runtime._get_bin_folder()))",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the details. gentoo is not a 'natively' supported distribution of linux for datasets. the exception message doesn't link to a .net docs page with instructions on installing the system dependencies required for .net to work. though it seems a different one is being thrown related to not being able to connect to out blob storage which has pre prepared dependency sets for some linux distros . this page install .net on linux distributions microsoft docs does not detail support for .net on gentoo. you can get the names of the missing dependencies themselves by running from dotnetcore import runtime this code snippet should print the libraies missing required by .net core if the above does not print anything, other than the exception, then instead this should from dotnetcore import runtime"
    },
    {
        "Question_id":64535892.0,
        "Question_title":"How to format body to pass input dataset as parameter in Azure ML?",
        "Question_body":"<p>I'm trying to consume my Azure ML Pipeline (Batch) from LogicApps. For that, I've deployed the Batch pipeline with the dataset as parameter :<a href=\"https:\/\/i.stack.imgur.com\/PYhU5.png\" rel=\"nofollow noreferrer\">1<\/a><\/p>\n<p>But I can't figure it out, how to format my body to invoke the pipeline and I don't find documentation on that.<\/p>\n<p>For now, this is how my logicapp looks like:\n<a href=\"https:\/\/i.stack.imgur.com\/lkKCB.png\" rel=\"nofollow noreferrer\">2<\/a>\nand I get a 415 Http Error code when trying to invoke the pipeline.<\/p>\n<p>Thanks for your help.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1603710297587,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":67.0,
        "Owner_creation_time":1431423994832,
        "Owner_last_access_time":1657729027096,
        "Owner_reputation":53.0,
        "Owner_up_votes":2.0,
        "Owner_down_votes":0.0,
        "Owner_views":19.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64535892",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to format body to pass input dataset as parameter in ?; Content: i'm trying to consume my pipeline (batch) from logicapps. for that, i've deployed the batch pipeline with the dataset as parameter :1 but i can't figure it out, how to format my body to invoke the pipeline and i don't find documentation on that. for now, this is how my logicapp looks like: 2 and i get a 415 http error code when trying to invoke the pipeline. thanks for your help.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in formatting the body to pass an input dataset as a parameter in order to consume a batch pipeline from LogicApps.",
        "Question_preprocessed_content":"Title: how to format body to pass input dataset as parameter in ?; Content: i'm trying to consume my pipeline from logicapps. for that, i've deployed the batch pipeline with the dataset as parameter but i can't figure it out, how to format my body to invoke the pipeline and i don't find documentation on that. for now, this is how my logicapp looks like and i get a http error code when trying to invoke the pipeline. thanks for your help.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"azure machine learning SubscriptionNotFound",
        "Question_body":"I have created a workspace for azure machine learning and I am trying to create processes, but I get the following error: BatchARMResponseError:\n\n{\"error\":{\"code\": \"SubscriptionNotFound\", \"message\": \"The specified subscription XXXXXXXXXXXXXXXXXXXXXXXXXX is not found\"}}}\n\nRequest error with status code 500.\nat b (https:\/\/ml.azure.com\/static\/js\/index.29c4b58c.chunk.js)",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1653319804270,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/860631\/azure-machine-learning-subscriptionnotfound.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-24T18:43:13.09Z",
                "Answer_score":0,
                "Answer_body":"Hi @YutongTie-MSFT\n\nI was able to solve it, apparently the service does not have any availability in the region I chose first, I changed it and it is already working.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: subscriptionnotfound; Content: i have created a workspace for and i am trying to create processes, but i get the following error: batcharmresponseerror: {\"error\":{\"code\": \"subscriptionnotfound\", \"message\": \"the specified subscription xxxxxxxxxxxxxxxxxxxxxxxxxx is not found\"}}} request error with status code 500. at b (https:\/\/ml.azure.com\/static\/js\/index.29c4b58c.chunk.js)",
        "Question_original_content_gpt_summary":"The user encountered an error with status code 500 when trying to create processes in a workspace due to a \"subscriptionnotfound\" error.",
        "Question_preprocessed_content":"Title: subscriptionnotfound; Content: i have created a workspace for and i am trying to create processes, but i get the following error batcharmresponseerror error code subscriptionnotfound , message the specified subscription xxxxxxxxxxxxxxxxxxxxxxxxxx is not found request error with status code . at b",
        "Answer_original_content":"hi @yutongtie-msft i was able to solve it, apparently the service does not have any availability in the region i chose first, i changed it and it is already working.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi i was able to solve it, apparently the service does not have any availability in the region i chose first, i changed it and it is already working."
    },
    {
        "Question_id":null,
        "Question_title":"Multi-run confusion matrix is missing class",
        "Question_body":"<p>I  am working on a 3 classes classification problem and my confusion matrix is getting messed up. It does not show class 1.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/d\/db00d90402775cbd38aa10db03d568c86fe1b6b1.png\" data-download-href=\"\/uploads\/short-url\/vfonHnXSPR6s8bDCiSBgLDe4P17.png?dl=1\" title=\"multi-run_confusion_matrix\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/d\/db00d90402775cbd38aa10db03d568c86fe1b6b1.png\" alt=\"multi-run_confusion_matrix\" data-base62-sha1=\"vfonHnXSPR6s8bDCiSBgLDe4P17\" width=\"690\" height=\"445\" data-dominant-color=\"F9F6F7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">multi-run_confusion_matrix<\/span><span class=\"informations\">1016\u00d7656 12.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The test accuracy for all three classes is 95% and all three classes are actually in the dataset.<\/p>\n<pre><code class=\"lang-auto\">        # log data for the confusion matrix\n        wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n                        y_true=y_test_true, preds=y_test_pred,\n                        class_names=labels)})\n<\/code><\/pre>\n<p>Any idea how to solve this issue?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1673555274420,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":51.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/multi-run-confusion-matrix-is-missing-class\/3691",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-13T16:12:22.423Z",
                "Answer_body":"<p>Hi Susanne!<\/p>\n<p>Could I see a link to your workspace?<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-19T17:12:06.657Z",
                "Answer_body":"<p>Hi Susanne,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-24T19:10:29.188Z",
                "Answer_body":"<p>Hi Susanne, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"",
        "Question_original_content_gpt_summary":"",
        "Question_preprocessed_content":"",
        "Answer_original_content":"",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":""
    },
    {
        "Question_id":null,
        "Question_title":"Logging with Tensorboard",
        "Question_body":"<p>I am trying to run the demo code from <a href=\"https:\/\/colab.research.google.com\/gist\/sayakpaul\/5b31ed03725cc6ae2af41848d4acee45\/demo_tensorboard.ipynb\" rel=\"noopener nofollow ugc\">Demo_tensorboard.ipynb<\/a> so that I can learn more about the use of Tensorboard in combination with W&amp;B. Unfortunately this code throws this warning:<\/p>\n<p>WARNING When using several event log directories, please call <code>wandb.tensorboard.patch(root_logdir=\"...\")<\/code> before <code>wandb.init<\/code><\/p>\n<p>When I implement the suggested change with:<\/p>\n<p><code>wandb.tensorboard.patch(root_logdir=\".\/logs\/debug\")<\/code><\/p>\n<p>I get the following warning:<br>\nFound log directory outside of given root_logdir, dropping given root_logdir for event file in i:\\tinyml\\tiny_cnn\\wandb\\run-20221016_205607-22b9tlzf\\files\\train<\/p>\n<p>So my questions is: What is a suitable root_logdir for Tensorboard?<\/p>\n<p>Thanks for your support.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1665946757447,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":96.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-with-tensorboard\/3265",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-17T13:16:21.990Z",
                "Answer_body":"<p>Hi Susanne,<\/p>\n<p>Thanks for writing in! The <code>root_logdir<\/code>argument is the path to the root of all tfevent files, so you can use the wandb project folder (in this case I think it is <code>I:\/tinyml\/tiny_cnn<\/code>). Could you try if setting this solves the issue?<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-10-20T08:48:45.986Z",
                "Answer_body":"<p>Hi Susanne,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-28T19:43:19.097Z",
                "Answer_body":"<p>Setting root_logdir = os.getcwd() actually solved the issue.<\/p>\n<p>Thanks for the support.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-27T19:43:39.383Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: logging with tensorboard; Content: i am trying to run the demo code from demo_tensorboard.ipynb so that i can learn more about the use of tensorboard in combination with w&b. unfortunately this code throws this warning: warning when using several event log directories, please call .tensorboard.patch(root_logdir=\"...\") before .init when i implement the suggested change with: .tensorboard.patch(root_logdir=\".\/logs\/debug\") i get the following warning: found log directory outside of given root_logdir, dropping given root_logdir for event file in i:\\tinyml\\tiny_cnn\\\\run-20221016_205607-22b9tlzf\\files\\train so my questions is: what is a suitable root_logdir for tensorboard? thanks for your support.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with logging with Tensorboard, as they are receiving warnings when trying to run the demo code and when implementing the suggested change.",
        "Question_preprocessed_content":"Title: logging with tensorboard; Content: i am trying to run the demo code from so that i can learn more about the use of tensorboard in combination with w&b. unfortunately this code throws this warning warning when using several event log directories, please call before when i implement the suggested change with i get the following warning found log directory outside of given dropping given for event file in so my questions is what is a suitable for tensorboard? thanks for your support.",
        "Answer_original_content":"hi susanne, thanks for writing in! the root_logdirargument is the path to the root of all tfevent files, so you can use the project folder (in this case i think it is i:\/tinyml\/tiny_cnn). could you try if setting this solves the issue? best, luis",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi susanne, thanks for writing in! the argument is the path to the root of all tfevent files, so you can use the project folder . could you try if setting this solves the issue? best, luis"
    },
    {
        "Question_id":68354159.0,
        "Question_title":"`pipeline_job` registered model input",
        "Question_body":"<p>I'm using the pipeline private preview from CLI (v2).<\/p>\n<p>I'd like to know how to use a registered model as an input of my pipeline.<\/p>\n<p>Similarly to how I access the blob storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>inputs:\n  input_data:\n    data:\n      datastore: azureml:dualcam\n      path: \/image-20210701*\n<\/code><\/pre>\n<p>I'm expecting to be able to use my registered model but I can find the schema to add it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1626126044697,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Owner_creation_time":1327588060552,
        "Owner_last_access_time":1664036801580,
        "Owner_reputation":802.0,
        "Owner_up_votes":288.0,
        "Owner_down_votes":0.0,
        "Owner_views":91.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68354159",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: `pipeline_job` registered model input; Content: i'm using the pipeline private preview from cli (v2). i'd like to know how to use a registered model as an input of my pipeline. similarly to how i access the blob storage: inputs: input_data: data: datastore: :dualcam path: \/image-20210701* i'm expecting to be able to use my registered model but i can find the schema to add it.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in using a registered model as an input of their pipeline in the pipeline private preview from the CLI (v2).",
        "Question_preprocessed_content":"Title: registered model input; Content: i'm using the pipeline private preview from cli . i'd like to know how to use a registered model as an input of my pipeline. similarly to how i access the blob storage i'm expecting to be able to use my registered model but i can find the schema to add it.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":65083883.0,
        "Question_title":"Example for Azure AutoML Forecasting for time series with multiple covariate features",
        "Question_body":"<p>I would like to use Azure AutoML for <code>forecasting<\/code> where I have multiple features for one timeseries. Is there any example which I can replicate?<\/p>\n<p>I have been looking into: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-beer-remote\/auto-ml-forecasting-beer-remote.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-beer-remote\/auto-ml-forecasting-beer-remote.ipynb<\/a>\nand\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb<\/a>\nbut no luck using multiple features instead of only one timeseries.<\/p>\n<p>Any help is greatly appreciated<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1606789538570,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1231.0,
        "Owner_creation_time":1581649402283,
        "Owner_last_access_time":1623273118283,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":5.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"C\u00f3rdoba, Cordoba, Argentina",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65083883",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: example for azure automl forecasting for time series with multiple covariate features; Content: i would like to use azure automl for forecasting where i have multiple features for one timeseries. is there any example which i can replicate? i have been looking into: https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/master\/how-to-use-\/automated-machine-learning\/forecasting-beer-remote\/auto-ml-forecasting-beer-remote.ipynb and https:\/\/github.com\/azure\/machinelearningnotebooks\/blob\/master\/how-to-use-\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb but no luck using multiple features instead of only one timeseries. any help is greatly appreciated",
        "Question_original_content_gpt_summary":"The user is looking for an example of using Azure Automl for forecasting with multiple covariate features.",
        "Question_preprocessed_content":"Title: example for azure automl forecasting for time series with multiple covariate features; Content: i would like to use azure automl for where i have multiple features for one timeseries. is there any example which i can replicate? i have been looking into and but no luck using multiple features instead of only one timeseries. any help is greatly appreciated",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":47550020.0,
        "Question_title":"Add model parameters and Run timestamp to scored results",
        "Question_body":"<p>I have run an Azure machine learning experiment in Studio and currently am exporting the scored results to Azure SQL.\nI want to include in the export data, the \"run timestamp\" and the model configuration as extra columns.\nThis is usefull because i can then compare the results from different runs against each other in my Azure SQL database.<\/p>\n\n<p>Anyone know how to add this data to the scored data set ?<\/p>\n\n<p>Thanks in advance,<\/p>\n\n<p>Oliver<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1511949912030,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":44.0,
        "Owner_creation_time":1511949561527,
        "Owner_last_access_time":1663853513060,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47550020",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: add model parameters and run timestamp to scored results; Content: i have run an experiment in studio and currently am exporting the scored results to azure sql. i want to include in the export data, the \"run timestamp\" and the model configuration as extra columns. this is usefull because i can then compare the results from different runs against each other in my azure sql database. anyone know how to add this data to the scored data set ? thanks in advance, oliver",
        "Question_original_content_gpt_summary":"The user Oliver is looking for a way to add the run timestamp and model configuration as extra columns to the scored results of an experiment run in Studio.",
        "Question_preprocessed_content":"Title: add model parameters and run timestamp to scored results; Content: i have run an experiment in studio and currently am exporting the scored results to azure sql. i want to include in the export data, the run timestamp and the model configuration as extra columns. this is usefull because i can then compare the results from different runs against each other in my azure sql database. anyone know how to add this data to the scored data set ? thanks in advance, oliver",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"MLflow 1.11.0 released!",
        "Question_body":"Hi all,\n\nWe are happy to announce the availability of\u00a0MLflow 1.11.0!\n\nMLflow 1.11.0 includes a number of major features and improvements, in addition to bug fixes and documentation updates:\n\nNew\u00a0mlflow.sklearn.autolog()\u00a0API for automatic logging of metrics, params, and models from scikit-learn model training (#3287,\u00a0@harupy;\u00a0#3323,\u00a0#3358\u00a0@dbczumar)\nRegistered model & model version creation APIs now support specifying an initial\u00a0description\u00a0(#3271,\u00a0@sueann)\nThe R\u00a0mlflow_log_model\u00a0and\u00a0mlflow_load_model\u00a0APIs now support XGBoost models (#3085,\u00a0@lorenzwalthert)\nNew\u00a0mlflow.list_run_infos\u00a0fluent API for listing run metadata (#3183,\u00a0@trangevi)\nAdded section for visualizing and comparing model schemas to model version and model-version-comparison UIs (#3209,\u00a0@zhidongqu-db)\nEnhanced support for using the model registry across Databricks workspaces: support for registering models to a Databricks workspace from outside the workspace (#3119,\u00a0@sueann), tracking run-lineage of these models (#3128,\u00a0#3164,\u00a0@ankitmathur-db;\u00a0#3187,\u00a0@harupy), and calling\u00a0mlflow.<flavor>.load_model\u00a0against remote Databricks model registries (#3330,\u00a0@sueann)\nUI support for setting\/deleting registered model and model version tags (#3187,\u00a0@harupy)\nUI support for archiving existing staging\/production versions of a model when transitioning a new model version to staging\/production (#3134,\u00a0@harupy)\n\nFor a comprehensive list of changes, see the\u00a0release change log, and check out the latest documentation on\u00a0mlflow.org.\n\nThanks,\nSid",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1598845265000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":31.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Cf8RLtdN5A4",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[

        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: 1.11.0 released!; Content: hi all, we are happy to announce the availability of 1.11.0! 1.11.0 includes a number of major features and improvements, in addition to bug fixes and documentation updates: new .sklearn.autolog() api for automatic logging of metrics, params, and models from scikit-learn model training (#3287, @harupy; #3323, #3358 @dbczumar) registered model & model version creation apis now support specifying an initial description (#3271, @sueann) the r _log_model and _load_model apis now support xgboost models (#3085, @lorenzwalthert) new .list_run_infos fluent api for listing run metadata (#3183, @trangevi) added section for visualizing and comparing model schemas to model version and model-version-comparison uis (#3209, @zhidongqu-db) enhanced support for using the model registry across databricks workspaces: support for registering models to a databricks workspace from outside the workspace (#3119, @sueann), tracking run-lineage of these models (#3128, #3164, @ankitmathur-db; #3187, @harupy), and calling ..load_model against remote databricks model registries (#3330, @sueann) ui support for setting\/deleting registered model and model version tags (#3187, @harupy) ui support for archiving existing staging\/production versions of a model when transitioning a new model version to staging\/production (#3134, @harupy) for a comprehensive list of changes, see the release change log, and check out the latest documentation on .org. thanks, sid",
        "Question_original_content_gpt_summary":"The user encountered a number of major features and improvements, bug fixes, and documentation updates when releasing 1.11.0.",
        "Question_preprocessed_content":"Title: released!; Content: hi all, we are happy to announce the availability of includes a number of major features and improvements, in addition to bug fixes and documentation updates for automatic logging of metrics, params, and models from scikit learn model training registered model & model version creation apis now support specifying an initialdescription the now support xgboost models api for listing run metadata added section for visualizing and comparing model schemas to model version and model version comparison uis enhanced support for using the model registry across databricks workspaces support for registering models to a databricks workspace from outside the workspace , tracking run lineage of these models , and remote databricks model registries ui support for registered model and model version tags ui support for archiving existing versions of a model when transitioning a new model version to for a comprehensive list of changes, see therelease change log, and check out the latest documentation thanks, sid",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":67543601.0,
        "Question_title":"scoring R model in Python",
        "Question_body":"<p>I am having a hard time to deploy an R model and expose it as a web service using <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">azuremlsdk<\/a> for R. The Python side of Azure machine learning appears to be more mature as Python was deemed more important by Microsoft. Anyway, I was wondering if one score an R model, persisted as .rds file, in Python. I understand R can talk to Python via reticulate. Any input from Python experts would be very much appreciated. Thanks.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_creation_time":1621057170167,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":58.0,
        "Owner_creation_time":1267440784443,
        "Owner_last_access_time":1664045779312,
        "Owner_reputation":15705.0,
        "Owner_up_votes":2171.0,
        "Owner_down_votes":91.0,
        "Owner_views":2150.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Somewhere",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67543601",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: scoring r model in python; Content: i am having a hard time to deploy an r model and expose it as a web service using sdk for r. the python side of appears to be more mature as python was deemed more important by microsoft. anyway, i was wondering if one score an r model, persisted as .rds file, in python. i understand r can talk to python via reticulate. any input from python experts would be very much appreciated. thanks.",
        "Question_original_content_gpt_summary":"The user is having difficulty deploying an R model as a web service using the SDK for R, and is looking for a way to score the R model, persisted as an .rds file, in Python.",
        "Question_preprocessed_content":"Title: scoring r model in python; Content: i am having a hard time to deploy an r model and expose it as a web service using sdk for r. the python side of appears to be more mature as python was deemed more important by microsoft. anyway, i was wondering if one score an r model, persisted as .rds file, in python. i understand r can talk to python via reticulate. any input from python experts would be very much appreciated. thanks.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"What are your requirements for a cloud server?",
        "Question_body":"<p>Amazon Web Services (AWS) presently services over 7,500 government entities and 5000 educational institutions.<\/p>\n<p>If that isn\u2019t an endorsement, we don\u2019t know what is! AWS, known as one of the world\u2019s premier IT corporations, is now one of the top four public cloud computing companies in the world.<\/p>\n<p>Source : <a href=\"https:\/\/www.sevenmentor.com\/amazon-web-services-training-institute-in-pune.php\" rel=\"noopener nofollow ugc\">AWS Course In Pune<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1659034492111,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":166.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/what-are-your-requirements-for-a-cloud-server\/2816",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-26T18:54:55.342Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what are your requirements for a cloud server?; Content: amazon web services (aws) presently services over 7,500 government entities and 5000 educational institutions. if that isn\u2019t an endorsement, we don\u2019t know what is! aws, known as one of the world\u2019s premier it corporations, is now one of the top four public cloud computing companies in the world. source : aws course in pune",
        "Question_original_content_gpt_summary":"The user is considering Amazon Web Services (AWS) as a cloud server provider due to its endorsement by numerous government entities and educational institutions.",
        "Question_preprocessed_content":"Title: what are your requirements for a cloud server?; Content: amazon web services presently services over , government entities and educational institutions. if that isnt an endorsement, we dont know what is! aws, known as one of the worlds premier it corporations, is now one of the top four public cloud computing companies in the world. source aws course in pune",
        "Answer_original_content":"this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":70187397.0,
        "Question_title":"How do I create a DatatSet with Data Type: ModelDirectory in Azure Machine Learning Studio?",
        "Question_body":"<p>I'm attempting to manually create a DataSet with Data Type: ModelDirectory in Azure Machine Learning Studio, in order to use it in an Inference Pipeline.  I have taken an existing ModelDirectory DataSet and attempted to replicate it.  Everything is identical, <em>except<\/em> that the replica has Data Type:  AnyDirectory, and can not be hooked up to the input of a ScoreModel node in the designer.  How can I (manually in the UI or, better yet, programmatically) create a DataSet with Data Type: ModelDirectory from the output files of a trained model?<\/p>\n<p>Existing DataSet:\n<a href=\"https:\/\/i.stack.imgur.com\/56g9x.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/56g9x.png\" alt=\"enter image description here\" \/><\/a>\nExisting DataSet outputs:\n<a href=\"https:\/\/i.stack.imgur.com\/0aBSc.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0aBSc.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Manually Created Replica DataSet:\n<a href=\"https:\/\/i.stack.imgur.com\/DiQdp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DiQdp.png\" alt=\"enter image description here\" \/><\/a>\nManually Created Replica DataSet outputs:\n<a href=\"https:\/\/i.stack.imgur.com\/DIYFY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DIYFY.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As you can see, the outputs of both DataSets are <em>identical<\/em>.  The only difference between the two DataSets, seems to be the 'Data Type' properties, although in the output view, you can see that both have 'type: ModelDirectory'.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1638374367860,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":80.0,
        "Owner_creation_time":1340833876128,
        "Owner_last_access_time":1663795160110,
        "Owner_reputation":751.0,
        "Owner_up_votes":68.0,
        "Owner_down_votes":5.0,
        "Owner_views":73.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70187397",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i create a datatset with data type: modeldirectory in studio?; Content: i'm attempting to manually create a dataset with data type: modeldirectory in studio, in order to use it in an inference pipeline. i have taken an existing modeldirectory dataset and attempted to replicate it. everything is identical, except that the replica has data type: anydirectory, and can not be hooked up to the input of a scoremodel node in the designer. how can i (manually in the ui or, better yet, programmatically) create a dataset with data type: modeldirectory from the output files of a trained model? existing dataset: existing dataset outputs: manually created replica dataset: manually created replica dataset outputs: as you can see, the outputs of both datasets are identical. the only difference between the two datasets, seems to be the 'data type' properties, although in the output view, you can see that both have 'type: modeldirectory'.",
        "Question_original_content_gpt_summary":"The user is attempting to manually create a dataset with data type: modeldirectory in Studio, but is unable to connect it to the input of a scoremodel node in the Designer due to the data type being set to anydirectory.",
        "Question_preprocessed_content":"Title: how do i create a datatset with data type modeldirectory in studio?; Content: i'm attempting to manually create a dataset with data type modeldirectory in studio, in order to use it in an inference pipeline. i have taken an existing modeldirectory dataset and attempted to replicate it. everything is identical, except that the replica has data type anydirectory, and can not be hooked up to the input of a scoremodel node in the designer. how can i create a dataset with data type modeldirectory from the output files of a trained model? existing dataset existing dataset outputs manually created replica dataset manually created replica dataset outputs as you can see, the outputs of both datasets are identical. the only difference between the two datasets, seems to be the 'data type' properties, although in the output view, you can see that both have 'type modeldirectory'.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Creating Artifact Manually While Using Metaflow <> WandB Integration",
        "Question_body":"<p>Hi everyone,<\/p>\n<p>I\u2019m using the <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/metaflow\">MetaFlow Integration with WandB<\/a>. I understand the decorator will help me log instance variables (e.g., pandas data frames) as artifacts to WandB automatically.<\/p>\n<p>In one of Metaflow steps, I\u2019m trying to write a set of arrow files to disk and then upload these to an artifact with WandB. To do such I must run:<\/p>\n<pre><code class=\"lang-auto\">run.log_artifact(artifact)\n<\/code><\/pre>\n<p>Is there an easy way to extract \/ get the <a href=\"https:\/\/github.com\/wandb\/client\/blob\/master\/wandb\/integration\/metaflow\/metaflow.py#L278\" rel=\"noopener nofollow ugc\">run object created by the wandb metaflow decorator<\/a> so that it can be used to log this artifact explicitly?<\/p>\n<p>Thanks!<br>\nSahil<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1634064846799,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":283.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/creating-artifact-manually-while-using-metaflow-wandb-integration\/935",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-15T13:41:14.775Z",
                "Answer_body":"<p>Hi Sahil!<\/p>\n<p>We don\u2019t expose the run object directly, but you can log to that run implicitly by calling this inside your step.<\/p>\n<pre><code class=\"lang-auto\">artifact = wandb.Artifact(name, type=\"dataset\")\nartifact.add_file(\"file.arrow\")\nwandb.log_artifact(artifact)\n<\/code><\/pre>\n<p>Are you trying to save dataframes as <code>.arrow<\/code>?  We <a href=\"https:\/\/github.com\/wandb\/client\/blob\/master\/wandb\/integration\/metaflow\/metaflow.py#L122\" rel=\"noopener nofollow ugc\">save this as <code>.parquet<\/code> today<\/a>, but can add an arrow option.<\/p>",
                "Answer_score":51.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-15T17:08:28.162Z",
                "Answer_body":"<p>That\u2019s awesome! Thanks so much <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":46.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:06.731Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: creating artifact manually while using metaflow <> integration; Content: hi everyone, i\u2019m using the metaflow integration with . i understand the decorator will help me log instance variables (e.g., pandas data frames) as artifacts to automatically. in one of metaflow steps, i\u2019m trying to write a set of arrow files to disk and then upload these to an artifact with . to do such i must run: run.log_artifact(artifact) is there an easy way to extract \/ get the run object created by the metaflow decorator so that it can be used to log this artifact explicitly? thanks! sahil",
        "Question_original_content_gpt_summary":"The user is encountering challenges with manually creating an artifact while using Metaflow integration and is looking for an easy way to extract\/get the run object created by the Metaflow decorator to log the artifact explicitly.",
        "Question_preprocessed_content":"Title: creating artifact manually while using metaflow integration; Content: hi everyone, im using the metaflow integration with . i understand the decorator will help me log instance variables as artifacts to automatically. in one of metaflow steps, im trying to write a set of arrow files to disk and then upload these to an artifact with . to do such i must run is there an easy way to extract \/ get the run object created by the metaflow decorator so that it can be used to log this artifact explicitly? thanks! sahil",
        "Answer_original_content":"hi sahil! we dont expose the run object directly, but you can log to that run implicitly by calling this inside your step. artifact = .artifact(name, type=\"dataset\") artifact.add_file(\"file.arrow\") .log_artifact(artifact) are you trying to save dataframes as .arrow? we save this as .parquet today, but can add an arrow option. thats awesome! thanks so much this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi sahil! we dont expose the run object directly, but you can log to that run implicitly by calling this inside your step. are you trying to save dataframes as ? we save this as today, but can add an arrow option. thats awesome! thanks so much this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"sanic python - instead of flask",
        "Question_body":"Hey,\n\n\nWe are not able to use the mlflow server in our production API due to the fact it's using flask.\nWe have create our own API which is taking the pickel from mlflow and uses\u00a0sanic python for the API which is much faster.\n\n\nCan you please change your flask to\u00a0sanic python?\n\n\nIt will help us a lot.",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1560415286000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":6.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/KjKwpgi8EjI",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-13T11:53:27",
                "Answer_body":"We definitely welcome contributions of this sort! If interested in contributing, it would also be great to point out the pros\/cons of sanic vs flask, and if there are backwards-compatibility issues that might arise from switching.\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/56a48ce0-6460-4f8f-91c8-a9feec6d5a1c%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sanic python - instead of flask; Content: hey, we are not able to use the server in our production api due to the fact it's using flask. we have create our own api which is taking the pickel from and uses sanic python for the api which is much faster. can you please change your flask to sanic python? it will help us a lot.",
        "Question_original_content_gpt_summary":"The user is requesting to switch from Flask to Sanic Python in order to improve the speed of their production API.",
        "Question_preprocessed_content":"Title: sanic python instead of flask; Content: hey, we are not able to use the server in our production api due to the fact it's using flask. we have create our own api which is taking the pickel from and usessanic python for the api which is much faster. can you please change your flask tosanic python? it will help us a lot.",
        "Answer_original_content":"we definitely welcome contributions of this sort! if interested in contributing, it would also be great to point out the pros\/cons of sanic vs flask, and if there are backwards-compatibility issues that might arise from switching. -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to post to this group, send email to ...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/56a48ce0-6460-4f8f-91c8-a9feec6d5a1c%40googlegroups.com. for more options, visit https:\/\/groups.google.com\/d\/optout.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"we definitely welcome contributions of this sort! if interested in contributing, it would also be great to point out the of sanic vs flask, and if there are backwards compatibility issues that might arise from switching. you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to to post to this group, send email to to view this discussion on the web visit for more options, visit"
    },
    {
        "Question_id":null,
        "Question_title":"MLFlow usage and contribution",
        "Question_body":"Hi all,\n\n\nIntertec is considering using MLFlow as the primary MLOps tool. Please let me know what are the criteria to get our logo on your site. We can also try to contribute to the product.\n\n\nKind regards\n\n\n\u00a0 \u00a0 \u00a0\nVelimir Graorkoski\nSoftware engineer\n\n\n\n\nslack\u00a0| velimir.graorkoski\n\nmail\u00a0|\u00a0velimir.g...@intertec.io\n\n\nweb\u00a0|\u00a0\u00a0https:\/\/www.intertec.io",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1633257951000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":38.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/pSnbo3Mvd0w",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[

        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: usage and contribution; Content: hi all, intertec is considering using as the primary mlops tool. please let me know what are the criteria to get our logo on your site. we can also try to contribute to the product. kind regards velimir graorkoski software engineer slack | velimir.graorkoski mail | velimir.g...@intertec.io web | https:\/\/www.intertec.io",
        "Question_original_content_gpt_summary":"The user Velimir Graorkoski is seeking criteria to get their logo on the MLOps tool site and is interested in contributing to the product.",
        "Question_preprocessed_content":"Title: usage and contribution; Content: hi all, intertec is considering using as the primary mlops tool. please let me know what are the criteria to get our logo on your site. we can also try to contribute to the product. kind regards velimir graorkoski software engineer slack",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":63691515.0,
        "Question_title":"Azure Machine Learning Studio: cannot create Datastore from Azure SQL Database",
        "Question_body":"<p>I am trying to connect to an Azure SQL Database from inside Azure Machine Learning Studio. Based on <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py<\/a>, it seems that the recommended pattern is to create a Datastore using the Datastore.register_azure_sql_database method as follows:<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Datastore\n\nws = Workspace.from_config() # asks for interactive authentication the first time\n\nsql_datastore_name  = &quot;datastore_test_01&quot; # any name should be fine\nserver_name         = os.getenv(&quot;SQL_SERVERNAME&quot;    , &quot;{SQL_SERVERNAME}&quot;) # Name of the Azure SQL server\ndatabase_name       = os.getenv(&quot;SQL_DATABASENAME&quot;  , &quot;{SQL_DATABASENAME}&quot;) # Name of the Azure SQL database\nusername            = os.getenv(&quot;SQL_USER_NAME&quot;     , &quot;{SQL_USER_NAME}&quot;) # The username of the database user.\npassword            = os.getenv(&quot;SQL_USER_PASSWORD&quot; , &quot;{SQL_USER_PASSWORD}&quot;) # The password of the database user.\n\nsql_datastore = Datastore.register_azure_sql_database(workspace      = ws,\n                                                      datastore_name = sql_datastore_name,\n                                                      server_name    = server_name,\n                                                      database_name  = database_name,\n                                                      username       = username,\n                                                      password       = password)\n<\/code><\/pre>\n<p>I am pretty sure I have set all parameters right, having copied them from the ADO.NET connection string at my SQL Database resource --&gt; Settings --&gt; Connection strings:<\/p>\n<pre><code>Server=tcp:{SQL_SERVERNAME}.database.windows.net,1433;Initial Catalog={SQL_DATABASENAME};Persist Security Info=False;User ID={SQL_USER_NAME};Password={SQL_USER_PASSWORD};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\n<\/code><\/pre>\n<p>However, I get the following error:<\/p>\n<pre><code>Registering datastore failed with a 400 error code and error message 'Azure SQL Database Error -2146232060: Please check the correctness of the datastore information.'\n<\/code><\/pre>\n<p>Am I missing something? E.g., a firewall rule? I have also tried adding the Azure ML compute resource's public IP address to the list of allowed IP addresses in my SQL Database resource, but still no success.<\/p>\n<hr \/>\n<p><strong>UPDATE<\/strong>: adding <code>skip_validation = True<\/code> to <code>Datastore.register_azure_sql_database<\/code> solves the issue. I can then query the data with<\/p>\n<pre><code>from azureml.core import Dataset\nfrom azureml.data.datapath import DataPath\n\nquery   = DataPath(sql_datastore, 'SELECT * FROM my_table')\ntabular = Dataset.Tabular.from_sql_query(query, query_timeout = 10)\ndf = tabular.to_pandas_dataframe()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1598976793243,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":793.0,
        "Owner_creation_time":1502454917960,
        "Owner_last_access_time":1611741145200,
        "Owner_reputation":132.0,
        "Owner_up_votes":10.0,
        "Owner_down_votes":0.0,
        "Owner_views":41.0,
        "Answer_body":"<p>is the datastore behind vnet? where are you running the registration code above? On a compute instance behind the same vnet?\nhere is the doc that describe what you need to do to connect to data behind vnet:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1598979186260,
        "Answer_score":2.0,
        "Owner_location":"Milano, MI, Italia",
        "Question_last_edit_time":1599032818923,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63691515",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: studio: cannot create datastore from azure sql database; Content: i am trying to connect to an azure sql database from inside studio. based on https:\/\/docs.microsoft.com\/en-us\/python\/api\/-core\/.core.datastore.datastore?view=azure-ml-py, it seems that the recommended pattern is to create a datastore using the datastore.register_azure_sql_database method as follows: import os from .core import workspace, datastore ws = workspace.from_config() # asks for interactive authentication the first time sql_datastore_name = \"datastore_test_01\" # any name should be fine server_name = os.getenv(\"sql_servername\" , \"{sql_servername}\") # name of the azure sql server database_name = os.getenv(\"sql_databasename\" , \"{sql_databasename}\") # name of the azure sql database username = os.getenv(\"sql_user_name\" , \"{sql_user_name}\") # the username of the database user. password = os.getenv(\"sql_user_password\" , \"{sql_user_password}\") # the password of the database user. sql_datastore = datastore.register_azure_sql_database(workspace = ws, datastore_name = sql_datastore_name, server_name = server_name, database_name = database_name, username = username, password = password) i am pretty sure i have set all parameters right, having copied them from the ado.net connection string at my sql database resource --> settings --> connection strings: server=tcp:{sql_servername}.database.windows.net,1433;initial catalog={sql_databasename};persist security info=false;user id={sql_user_name};password={sql_user_password};multipleactiveresultsets=false;encrypt=true;trustservercertificate=false;connection timeout=30; however, i get the following error: registering datastore failed with a 400 error code and error message 'azure sql database error -2146232060: please check the correctness of the datastore information.' am i missing something? e.g., a firewall rule? i have also tried adding the compute resource's public ip address to the list of allowed ip addresses in my sql database resource, but still no success. update: adding skip_validation = true to datastore.register_azure_sql_database solves the issue. i can then query the data with from .core import dataset from .data.datapath import datapath query = datapath(sql_datastore, 'select * from my_table') tabular = dataset.tabular.from_sql_query(query, query_timeout = 10) df = tabular.to_pandas_dataframe()",
        "Question_original_content_gpt_summary":"The user is encountering challenges in creating a datastore from an Azure SQL database in studio, despite having set all parameters correctly.",
        "Question_preprocessed_content":"Title: studio cannot create datastore from azure sql database; Content: i am trying to connect to an azure sql database from inside studio. based on it seems that the recommended pattern is to create a datastore using the method as follows i am pretty sure i have set all parameters right, having copied them from the connection string at my sql database resource > settings > connection strings however, i get the following error am i missing something? a firewall rule? i have also tried adding the compute resource's public ip address to the list of allowed ip addresses in my sql database resource, but still no success. update adding to solves the issue. i can then query the data with",
        "Answer_original_content":"is the datastore behind vnet? where are you running the registration code above? on a compute instance behind the same vnet? here is the doc that describe what you need to do to connect to data behind vnet: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"is the datastore behind vnet? where are you running the registration code above? on a compute instance behind the same vnet? here is the doc that describe what you need to do to connect to data behind vnet"
    },
    {
        "Question_id":73411852.0,
        "Question_title":"Issue with data lake mounting in custom RStudio application Azure ML",
        "Question_body":"<ol>\n<li>previously while creating a compute instance  we were able to see RStudio application by default and we were able to mount\/access the data lake from RStudio.<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/J17ne.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/J17ne.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3l8Q4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3l8Q4.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol start=\"2\">\n<li>In current situation we are not able to access RStudio application by default.<\/li>\n<\/ol>\n<p><a href=\"https:\/\/i.stack.imgur.com\/nx5GL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nx5GL.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ol start=\"3\">\n<li>with the help of below link we are able to create custom RStudio application<\/li>\n<\/ol>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=azure-studio\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=azure-studio<\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/flQyy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/flQyy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>4.In custom RStudio we are not able to mount\/access the data lake.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2dWL9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2dWL9.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is there way to mount\/access the data lake in custom RStudio app<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1660883431000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":71.0,
        "Owner_creation_time":1655285218328,
        "Owner_last_access_time":1663848796827,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":5.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Pune",
        "Question_last_edit_time":1661269087767,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73411852",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: issue with data lake mounting in custom rstudio application ; Content: previously while creating a compute instance we were able to see rstudio application by default and we were able to mount\/access the data lake from rstudio. in current situation we are not able to access rstudio application by default. with the help of below link we are able to create custom rstudio application https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=azure-studio 4.in custom rstudio we are not able to mount\/access the data lake. is there way to mount\/access the data lake in custom rstudio app",
        "Question_original_content_gpt_summary":"The user is encountering challenges with mounting\/accessing the data lake in a custom RStudio application.",
        "Question_preprocessed_content":"Title: issue with data lake mounting in custom rstudio application; Content: previously while creating a compute instance we were able to see rstudio application by default and we were able to the data lake from rstudio. in current situation we are not able to access rstudio application by default. with the help of below link we are able to create custom rstudio application .in custom rstudio we are not able to the data lake. is there way to the data lake in custom rstudio app",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Concept and data drift",
        "Question_body":"<p>Hello<br>\nI am new-ish to DVC and still getting orientated. Please could I ask - are there any specific tools or processes within DVC Studio for monitoring concept and data drift? Or any plans in that area?<br>\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1637792304507,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":239.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/concept-and-data-drift\/986",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-25T08:29:50.799Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/peterbix\">@peterbix<\/a><br>\nIf you are referring to statistically comparing old and new data distributions, Studio does not have tools for this.<\/p>\n<p>If you would like to re-evaluate model performance on new dataset, so that you can re-train if needed, you can <a href=\"https:\/\/dvc.org\/doc\/studio\/user-guide\/run-experiments\">submit new experiments<\/a> by selecting new dataset versions from the Studio UI. You can set up your CI actions to re-run model evaluation stages, and you can generate metrics and plots as needed for comparing model performance.<\/p>\n<p>Currently, Studio does not track model deployment, which means it cannot track and evaluate real-time predictions. We are working on adding model management\/deployment features to Studio.<\/p>\n<p>If you have suggestions around what would be useful for Studio to support, could you please create a ticket in <a href=\"https:\/\/github.com\/iterative\/studio-support\/issues\">DVC Studio support repository<\/a>.<\/p>",
                "Answer_score":20.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: concept and data drift; Content: hello i am new-ish to and still getting orientated. please could i ask - are there any specific tools or processes within studio for monitoring concept and data drift? or any plans in that area? thanks",
        "Question_original_content_gpt_summary":"The user is seeking tools or processes within Studio to monitor concept and data drift.",
        "Question_preprocessed_content":"Title: concept and data drift; Content: hello i am new ish to and still getting orientated. please could i ask are there any specific tools or processes within studio for monitoring concept and data drift? or any plans in that area? thanks",
        "Answer_original_content":"hi @peterbix if you are referring to statistically comparing old and new data distributions, studio does not have tools for this. if you would like to re-evaluate model performance on new dataset, so that you can re-train if needed, you can submit new experiments by selecting new dataset versions from the studio ui. you can set up your ci actions to re-run model evaluation stages, and you can generate metrics and plots as needed for comparing model performance. currently, studio does not track model deployment, which means it cannot track and evaluate real-time predictions. we are working on adding model management\/deployment features to studio. if you have suggestions around what would be useful for studio to support, could you please create a ticket in studio support repository.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi if you are referring to statistically comparing old and new data distributions, studio does not have tools for this. if you would like to re evaluate model performance on new dataset, so that you can re train if needed, you can submit new experiments by selecting new dataset versions from the studio ui. you can set up your ci actions to re run model evaluation stages, and you can generate metrics and plots as needed for comparing model performance. currently, studio does not track model deployment, which means it cannot track and evaluate real time predictions. we are working on adding model features to studio. if you have suggestions around what would be useful for studio to support, could you please create a ticket in studio support repository."
    },
    {
        "Question_id":61452211.0,
        "Question_title":"Kedro - how to pass nested parameters directly to node",
        "Question_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/tagged\/kedro\"><code>kedro<\/code><\/a> recommends storing parameters in <code>conf\/base\/parameters.yml<\/code>. Let's assume it looks like this:<\/p>\n\n<pre><code>step_size: 1\nmodel_params:\n    learning_rate: 0.01\n    test_data_ratio: 0.2\n    num_train_steps: 10000\n<\/code><\/pre>\n\n<p>And now imagine I have some <code>data_engineering<\/code> pipeline whose <code>nodes.py<\/code> has a function that looks something like this:<\/p>\n\n<pre><code>def some_pipeline_step(num_train_steps):\n    \"\"\"\n    Takes the parameter `num_train_steps` as argument.\n    \"\"\"\n    pass\n<\/code><\/pre>\n\n<p>How would I go about and pass that nested parameters straight to this function in <code>data_engineering\/pipeline.py<\/code>? I unsuccessfully tried:<\/p>\n\n<pre><code>from kedro.pipeline import Pipeline, node\n\nfrom .nodes import split_data\n\n\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                some_pipeline_step,\n                [\"params:model_params.num_train_steps\"],\n                dict(\n                    train_x=\"train_x\",\n                    train_y=\"train_y\",\n                ),\n            )\n        ]\n    )\n<\/code><\/pre>\n\n<p>I know that I could just pass all parameters into the function by using <code>['parameters']<\/code> or just pass all <code>model_params<\/code> parameters with <code>['params:model_params']<\/code> but it seems unelegant and I feel like there must be a way. Would appreciate any input!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1587965065520,
        "Question_favorite_count":1.0,
        "Question_score":2.0,
        "Question_view_count":1403.0,
        "Owner_creation_time":1525290575943,
        "Owner_last_access_time":1655397273760,
        "Owner_reputation":143.0,
        "Owner_up_votes":28.0,
        "Owner_down_votes":0.0,
        "Owner_views":17.0,
        "Answer_body":"<p>(Disclaimer: I'm part of the Kedro team)<\/p>\n\n<p>Thank you for your question. Current version of Kedro, unfortunately, does not support nested parameters. The interim solution would be to use top-level keys inside the node (as you already pointed out) or decorate your node function with some sort of a parameter filter, which is not elegant either.<\/p>\n\n<p>Probably the most viable solution would be to customise your <code>ProjectContext<\/code> (in <code>src\/&lt;package_name&gt;\/run.py<\/code>) class by overwriting <code>_get_feed_dict<\/code> method as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>class ProjectContext(KedroContext):\n    # ...\n\n\n    def _get_feed_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Get parameters and return the feed dictionary.\"\"\"\n        params = self.params\n        feed_dict = {\"parameters\": params}\n\n        def _add_param_to_feed_dict(param_name, param_value):\n            \"\"\"This recursively adds parameter paths to the `feed_dict`,\n            whenever `param_value` is a dictionary itself, so that users can\n            specify specific nested parameters in their node inputs.\n\n            Example:\n\n                &gt;&gt;&gt; param_name = \"a\"\n                &gt;&gt;&gt; param_value = {\"b\": 1}\n                &gt;&gt;&gt; _add_param_to_feed_dict(param_name, param_value)\n                &gt;&gt;&gt; assert feed_dict[\"params:a\"] == {\"b\": 1}\n                &gt;&gt;&gt; assert feed_dict[\"params:a.b\"] == 1\n            \"\"\"\n            key = \"params:{}\".format(param_name)\n            feed_dict[key] = param_value\n\n            if isinstance(param_value, dict):\n                for key, val in param_value.items():\n                    _add_param_to_feed_dict(\"{}.{}\".format(param_name, key), val)\n\n        for param_name, param_value in params.items():\n            _add_param_to_feed_dict(param_name, param_value)\n\n        return feed_dict\n<\/code><\/pre>\n\n<p>Please also note that this issue has already been <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/commit\/529606273e201a736f10338ada73ac6206081730\" rel=\"nofollow noreferrer\">addressed on develop<\/a> and will become available in the next release. The fix uses the approach from the snippet above.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1587979890916,
        "Answer_score":2.0,
        "Owner_location":"Toronto, ON, Canada",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61452211",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: - how to pass nested parameters directly to node; Content: recommends storing parameters in conf\/base\/parameters.yml. let's assume it looks like this: step_size: 1 model_params: learning_rate: 0.01 test_data_ratio: 0.2 num_train_steps: 10000 and now imagine i have some data_engineering pipeline whose nodes.py has a function that looks something like this: def some_pipeline_step(num_train_steps): \"\"\" takes the parameter `num_train_steps` as argument. \"\"\" pass how would i go about and pass that nested parameters straight to this function in data_engineering\/pipeline.py? i unsuccessfully tried: from .pipeline import pipeline, node from .nodes import split_data def create_pipeline(**kwargs): return pipeline( [ node( some_pipeline_step, [\"params:model_params.num_train_steps\"], dict( train_x=\"train_x\", train_y=\"train_y\", ), ) ] ) i know that i could just pass all parameters into the function by using ['parameters'] or just pass all model_params parameters with ['params:model_params'] but it seems unelegant and i feel like there must be a way. would appreciate any input!",
        "Question_original_content_gpt_summary":"The user is trying to figure out how to pass nested parameters directly to a node in a data engineering pipeline.",
        "Question_preprocessed_content":"Title: how to pass nested parameters directly to node; Content: recommends storing parameters in . let's assume it looks like this and now imagine i have some pipeline whose has a function that looks something like this how would i go about and pass that nested parameters straight to this function in ? i unsuccessfully tried i know that i could just pass all parameters into the function by using or just pass all parameters with but it seems unelegant and i feel like there must be a way. would appreciate any input!",
        "Answer_original_content":"(disclaimer: i'm part of the team) thank you for your question. current version of , unfortunately, does not support nested parameters. the interim solution would be to use top-level keys inside the node (as you already pointed out) or decorate your node function with some sort of a parameter filter, which is not elegant either. probably the most viable solution would be to customise your projectcontext (in src\/<package_name>\/run.py) class by overwriting _get_feed_dict method as follows: class projectcontext(context): # ... def _get_feed_dict(self) -> dict[str, any]: \"\"\"get parameters and return the feed dictionary.\"\"\" params = self.params feed_dict = {\"parameters\": params} def _add_param_to_feed_dict(param_name, param_value): \"\"\"this recursively adds parameter paths to the `feed_dict`, whenever `param_value` is a dictionary itself, so that users can specify specific nested parameters in their node inputs. example: >>> param_name = \"a\" >>> param_value = {\"b\": 1} >>> _add_param_to_feed_dict(param_name, param_value) >>> assert feed_dict[\"params:a\"] == {\"b\": 1} >>> assert feed_dict[\"params:a.b\"] == 1 \"\"\" key = \"params:{}\".format(param_name) feed_dict[key] = param_value if isinstance(param_value, dict): for key, val in param_value.items(): _add_param_to_feed_dict(\"{}.{}\".format(param_name, key), val) for param_name, param_value in params.items(): _add_param_to_feed_dict(param_name, param_value) return feed_dict please also note that this issue has already been addressed on develop and will become available in the next release. the fix uses the approach from the snippet above.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thank you for your question. current version of , unfortunately, does not support nested parameters. the interim solution would be to use top level keys inside the node or decorate your node function with some sort of a parameter filter, which is not elegant either. probably the most viable solution would be to customise your class by overwriting method as follows please also note that this issue has already been addressed on develop and will become available in the next release. the fix uses the approach from the snippet above."
    },
    {
        "Question_id":null,
        "Question_title":"NLP is hard",
        "Question_body":"Who else thinks NLP is the hardest subset of AI to build?",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1629067440000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":328.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/NLP-is-hard\/td-p\/167242\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-15T22:44:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Who else thinks NLP is the hardest subset of AI to build?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: nlp is hard; Content: who else thinks nlp is the hardest subset of ai to build?",
        "Question_original_content_gpt_summary":"The user finds Natural Language Processing (NLP) to be the most difficult subset of Artificial Intelligence (AI) to build.",
        "Question_preprocessed_content":"Title: nlp is hard; Content: who else thinks nlp is the hardest subset of ai to build?",
        "Answer_original_content":"who else thinks nlp is the hardest subset of ai to build?",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"who else thinks nlp is the hardest subset of ai to build?"
    },
    {
        "Question_id":null,
        "Question_title":"Access RStudio from Jupyterlab",
        "Question_body":"Hi,\n\nis it possible to launch RStudio from a Jupyterlab tile just like VS Code is in this tutorial? https:\/\/docs.dominodatalab.com\/en\/latest\/reference\/workspaces\/Accessing_multiple_hosted_applications_in_one_Workspace_session.html#\n\n\n\nthank you",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1643466307000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/tickets.dominodatalab.com\/hc\/en-us\/community\/posts\/4417672395924-Access-RStudio-from-Jupyterlab",
        "Tool":"Domino",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[

        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: access rstudio from jupyterlab; Content: hi, is it possible to launch rstudio from a jupyterlab tile just like vs code is in this tutorial? https:\/\/docs.datalab.com\/en\/latest\/reference\/workspaces\/accessing_multiple_hosted_applications_in_one_workspace_session.html# thank you",
        "Question_original_content_gpt_summary":"The user is looking to access RStudio from a JupyterLab tile, similar to how VS Code is accessed in a tutorial.",
        "Question_preprocessed_content":"Title: access rstudio from jupyterlab; Content: hi, is it possible to launch rstudio from a jupyterlab tile just like vs code is in this tutorial? thank you",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Can dvc support remote large DBs?",
        "Question_body":"<p>Hey all,<br>\nI tried DVC with small data set and I really liked it. The main thing DVC helped me with is controlling the data pipeline and versioning the data accordingly. In order to compare experiments we already use MLflow.<\/p>\n<p>My team have another project with much bigger data set which is stored on postgres (on AWS). Can we use DVC in order to version our tables? For example:<br>\nRaw_Table \u2192 One_hot_conversion_table \u2192 Normalized_one_hot_conversion_table<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1622532408317,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":626.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-dvc-support-remote-large-dbs\/770",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-01T09:05:38.536Z",
                "Answer_body":"<p>Hi, <a class=\"mention\" href=\"\/u\/nrk\">@nrk<\/a>. DVC cannot version database tables. One way would be to transform the table and track the transformed dataset itself.<\/p>\n<p>Another way might be to track processed datasets\/outputs in the pipelines, that are small rather than the large dataset itself. PTAL on the ongoing discussion for <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/3439\">function specific dependencies<\/a>.<\/p>",
                "Answer_score":27.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-01T09:32:33.413Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"skshetry\" data-post=\"2\" data-topic=\"770\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/s\/eb8c5e\/40.png\" class=\"avatar\"> skshetry:<\/div>\n<blockquote>\n<p>transform the table and track the transformed dataset itself<\/p>\n<\/blockquote>\n<\/aside>\n<p>Transform as in converting columns into rows and vice versa? It won\u2019t solve the size issue<\/p>",
                "Answer_score":16.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can support remote large dbs?; Content: hey all, i tried with small data set and i really liked it. the main thing helped me with is controlling the data pipeline and versioning the data accordingly. in order to compare experiments we already use mlflow. my team have another project with much bigger data set which is stored on postgres (on aws). can we use in order to version our tables? for example: raw_table \u2192 one_hot_conversion_table \u2192 normalized_one_hot_conversion_table",
        "Question_original_content_gpt_summary":"The user is looking to use a data pipeline and versioning system to manage a large database stored on Postgres on AWS.",
        "Question_preprocessed_content":"Title: can support remote large dbs?; Content: hey all, i tried with small data set and i really liked it. the main thing helped me with is controlling the data pipeline and versioning the data accordingly. in order to compare experiments we already use mlflow. my team have another project with much bigger data set which is stored on postgres . can we use in order to version our tables? for example",
        "Answer_original_content":"hi, @nrk. cannot version database tables. one way would be to transform the table and track the transformed dataset itself. another way might be to track processed datasets\/outputs in the pipelines, that are small rather than the large dataset itself. ptal on the ongoing discussion for function specific dependencies. skshetry: transform the table and track the transformed dataset itself transform as in converting columns into rows and vice versa? it wont solve the size issue",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, cannot version database tables. one way would be to transform the table and track the transformed dataset itself. another way might be to track processed in the pipelines, that are small rather than the large dataset itself. ptal on the ongoing discussion for function specific dependencies. skshetry transform the table and track the transformed dataset itself transform as in converting columns into rows and vice versa? it wont solve the size issue"
    },
    {
        "Question_id":70923438.0,
        "Question_title":"Azure ML Studio: Create DATASET via REST API",
        "Question_body":"<p>Please tell me how to create a Dataset via REST API.<\/p>\n<p>There is a way to <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/datastores\" rel=\"nofollow noreferrer\">create a Datastore<\/a>, but I can't find a Dataset.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1643619959523,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":126.0,
        "Owner_creation_time":1365773542310,
        "Owner_last_access_time":1663778656052,
        "Owner_reputation":131.0,
        "Owner_up_votes":123.0,
        "Owner_down_votes":1.0,
        "Owner_views":32.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70923438",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: studio: create dataset via rest api; Content: please tell me how to create a dataset via rest api. there is a way to create a datastore, but i can't find a dataset.",
        "Question_original_content_gpt_summary":"The user is struggling to find a way to create a dataset via the REST API.",
        "Question_preprocessed_content":"Title: studio create dataset via rest api; Content: please tell me how to create a dataset via rest api. there is a way to create a datastore, but i can't find a dataset.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Azure-ML>>ImportError: DLL load failed while importing win32file: The specified procedure could not be found.",
        "Question_body":"I have been using the Azure ML python SDK and it has been working fine until I started getting this error:\n\"ImportError: DLL load failed while importing win32file: The specified procedure could not be found.\" when I tried to access my workspace\n\nI am using a conda virtual environment with Python 3.9 and I was running all my codes in a jupyter notebook on a windows computer.\n\nIt's been many hours now and I cannot find a solution. Can you help?",
        "Question_answer_count":1,
        "Question_comment_count":5.0,
        "Question_creation_time":1648195247070,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/787222\/importerror-dll-load-failed-while-importing-win32f.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-31T12:51:13.093Z",
                "Answer_score":1,
                "Answer_body":"I had the same issue, but the fix here seems to have solved it.\n\nSpecifically:\n\n conda install pywin32",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure-ml>>importerror: dll load failed while importing win32file: the specified procedure could not be found.; Content: i have been using the python sdk and it has been working fine until i started getting this error: \"importerror: dll load failed while importing win32file: the specified procedure could not be found.\" when i tried to access my workspace i am using a conda virtual environment with python 3.9 and i was running all my codes in a jupyter notebook on a windows computer. it's been many hours now and i cannot find a solution. can you help?",
        "Question_original_content_gpt_summary":"The user is encountering an \"ImportError: DLL load failed while importing win32file: the specified procedure could not be found\" error while using the Azure ML Python SDK in a Conda virtual environment with Python 3.9 on a Windows computer.",
        "Question_preprocessed_content":"Title: >>importerror dll load failed while importing win file the specified procedure could not be found.; Content: i have been using the python sdk and it has been working fine until i started getting this error importerror dll load failed while importing win file the specified procedure could not be when i tried to access my workspace i am using a conda virtual environment with python and i was running all my codes in a jupyter notebook on a windows computer. it's been many hours now and i cannot find a solution. can you help?",
        "Answer_original_content":"i had the same issue, but the fix here seems to have solved it. specifically: conda install pywin32",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i had the same issue, but the fix here seems to have solved it. specifically conda install pywin"
    },
    {
        "Question_id":null,
        "Question_title":"Multiple machines setup for one repo",
        "Question_body":"<p>Hi,<\/p>\n<p>First of all - thanks for developing such a great tool! We are currently incorporating it in our deep learning workflow for different computer vision projects.<\/p>\n<p>I think we got a grasp of how it all should be set up and are able to run our pipelines properly on one machine. One big question that we still can\u2019t figure out is the best approach to experiment and version control with dvc simultaneuosly on several machines.<\/p>\n<p>Create separate branches per each machine and iterate there? But what about shared data (the very first dependency in our pipeline) then? Do we just <code>dvc add data<\/code> and commit those changes in one branch (e.g. with tag). And then <code>git checkout &lt;tag&gt; data.dvc<\/code> and <code>dvc checkout data.dvc<\/code> on other machines?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1603954361775,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":514.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/multiple-machines-setup-for-one-repo\/539",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-29T08:16:21.292Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/simon\">@simon<\/a>, good question!<\/p>\n<aside class=\"quote no-group\" data-username=\"simon\" data-post=\"1\" data-topic=\"539\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/s\/f07891\/40.png\" class=\"avatar\"> simon:<\/div>\n<blockquote>\n<p>And then <code>git checkout &lt;tag&gt; data.dvc<\/code> and <code>dvc checkout data.dvc<\/code> on other machines?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yep. This will work if you checkout <code>*.dvc<\/code> file and then do <code>dvc checkout<\/code>.<\/p>\n<p>Another way is to set up a <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\">data registry<\/a> in a separate repository. Users will be doing:<\/p>\n<pre><code class=\"lang-auto\">dvc import https:\/\/github.com\/iterative\/dataset-registry tutorial\/nlp\/Posts.xml.zip\n\n# something changed\ndvc update Posts.xml.zip.dvc\n\n# get a specific version\ndvc update -r my-tag-2.3 Posts.xml.zip.dvc\n<\/code><\/pre>\n<p>If you need optimizations you can investigate more advanced scenarios:<\/p>\n<ol>\n<li>\n<a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\">shared development server<\/a> scenarios - all users in a single machine with no data duplication<\/li>\n<li>Set up data cache in a shared NFS drive and symlinks on the user side - users will have a nice looking workspace with all the files (symlinks actually) but will be reading actual data from NFS.<\/li>\n<\/ol>",
                "Answer_score":42.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-29T23:57:31.456Z",
                "Answer_body":"<p>Thanks, <a class=\"mention\" href=\"\/u\/dmitry\">@dmitry<\/a> for your response, much appreciated! Data registries makes so much sense!<\/p>\n<p>We did set up shared cache on each of our GPU servers and experiment there as per shared development scenario, thanks! And we have our one remote for all projects on one of the servers too.<\/p>\n<p>All our servers can only be accesses through SSH due to security requirements which complicated things a bit. We ended up creating a special <code>dvcuser<\/code> with very restricted premissions (just to be able to push and pull data to remote). That way we can include its ssh credentials for that remote in <code>.dvc\/config<\/code> so everyone can clone any dvc project, get those credentials pulled and start experimenting. Scenario with accessing that ssh remote with their own credentials by including them in <code>.dvc\/config.local<\/code> was not considered safe enough.<\/p>",
                "Answer_score":12.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-30T05:09:24.102Z",
                "Answer_body":"<p>The design looks good. I hope the data registry will compliment your scenario.<br>\nPlease let us know if any other questions. We are always happy to help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: multiple machines setup for one repo; Content: hi, first of all - thanks for developing such a great tool! we are currently incorporating it in our deep learning workflow for different computer vision projects. i think we got a grasp of how it all should be set up and are able to run our pipelines properly on one machine. one big question that we still can\u2019t figure out is the best approach to experiment and version control with simultaneuosly on several machines. create separate branches per each machine and iterate there? but what about shared data (the very first dependency in our pipeline) then? do we just add data and commit those changes in one branch (e.g. with tag). and then git checkout <tag> data. and checkout data. on other machines?",
        "Question_original_content_gpt_summary":"The user is facing a challenge of setting up multiple machines for one repository while managing shared data and version control.",
        "Question_preprocessed_content":"Title: multiple machines setup for one repo; Content: hi, first of all thanks for developing such a great tool! we are currently incorporating it in our deep learning workflow for different computer vision projects. i think we got a grasp of how it all should be set up and are able to run our pipelines properly on one machine. one big question that we still cant figure out is the best approach to experiment and version control with simultaneuosly on several machines. create separate branches per each machine and iterate there? but what about shared data then? do we just and commit those changes in one branch . and then and on other machines?",
        "Answer_original_content":"hi @simon, good question! simon: and then git checkout <tag> data. and checkout data. on other machines? yep. this will work if you checkout *. file and then do checkout. another way is to set up a data registry in a separate repository. users will be doing: import https:\/\/github.com\/iterative\/dataset-registry tutorial\/nlp\/posts.xml.zip # something changed update posts.xml.zip. # get a specific version update -r my-tag-2.3 posts.xml.zip. if you need optimizations you can investigate more advanced scenarios: shared development server scenarios - all users in a single machine with no data duplication set up data cache in a shared nfs drive and symlinks on the user side - users will have a nice looking workspace with all the files (symlinks actually) but will be reading actual data from nfs. thanks, @dmitry for your response, much appreciated! data registries makes so much sense! we did set up shared cache on each of our gpu servers and experiment there as per shared development scenario, thanks! and we have our one remote for all projects on one of the servers too. all our servers can only be accesses through ssh due to security requirements which complicated things a bit. we ended up creating a special user with very restricted premissions (just to be able to push and pull data to remote). that way we can include its ssh credentials for that remote in .\/config so everyone can clone any project, get those credentials pulled and start experimenting. scenario with accessing that ssh remote with their own credentials by including them in .\/config.local was not considered safe enough. the design looks good. i hope the data registry will compliment your scenario. please let us know if any other questions. we are always happy to help",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi good question! simon and then and on other machines? yep. this will work if you checkout file and then do . another way is to set up a data registry in a separate repository. users will be doing if you need optimizations you can investigate more advanced scenarios shared development server scenarios all users in a single machine with no data duplication set up data cache in a shared nfs drive and symlinks on the user side users will have a nice looking workspace with all the files but will be reading actual data from nfs. thanks, for your response, much appreciated! data registries makes so much sense! we did set up shared cache on each of our gpu servers and experiment there as per shared development scenario, thanks! and we have our one remote for all projects on one of the servers too. all our servers can only be accesses through ssh due to security requirements which complicated things a bit. we ended up creating a special with very restricted premissions . that way we can include its ssh credentials for that remote in so everyone can clone any project, get those credentials pulled and start experimenting. scenario with accessing that ssh remote with their own credentials by including them in was not considered safe enough. the design looks good. i hope the data registry will compliment your scenario. please let us know if any other questions. we are always happy to help"
    },
    {
        "Question_id":69693666.0,
        "Question_title":"How to Deploy ML Recommender System on AWS",
        "Question_body":"<p>I'm dabbling with ML and was able to take a tutorial and get it to work for my needs.  It's a simple recommender system using TfidfVectorizer and linear_kernel.  I run into a problem with how I go about deploying it through Sagemaker with an end point.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nimport json\nimport csv\n\nwith open('data\/big_data.json') as json_file:\n    data = json.load(json_file)\n\nds = pd.DataFrame(data)\n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(ds['content'])\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\nresults = {}\n\nfor idx, row in ds.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n    similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices]\n\n    results[row['id']] = similar_items[1:]\n\ndef item(id):\n    return ds.loc[ds['id'] == id]['id'].tolist()[0]\n\ndef recommend(item_id, num):\n    print(&quot;Recommending &quot; + str(num) + &quot; products similar to &quot; + item(item_id) + &quot;...&quot;)\n    print(&quot;-------&quot;)\n    recs = results[item_id][:num]\n    for rec in recs:\n        print(&quot;Recommended: &quot; + item(rec[1]) + &quot; (score:&quot; + str(rec[0]) + &quot;)&quot;)\n\nrecommend(item_id='129035', num=5)\n<\/code><\/pre>\n<p>As a starting point I'm not sure if the output from <code>tf.fit_transform(ds['content'])<\/code> is considered the model or the output from <code>linear_kernel(tfidf_matrix, tfidf_matrix)<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1635046349497,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":63.0,
        "Owner_creation_time":1635045129020,
        "Owner_last_access_time":1650386337656,
        "Owner_reputation":53.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":11.0,
        "Answer_body":"<p>I came to the conclusion that I didn't need to deploy this through SageMaker.  Since the final linear_kernel output was a Dictionary I could do quick ID lookups to find correlations.<\/p>\n<p>I have it working on AWS with API Gateway\/Lambda, DynamoDB and an EC2 server to collect, process and plug the data into DynamoDB for fast lookups.  No expensive SageMaker endpoint needed.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1636075476147,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":1635173315200,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69693666",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to deploy ml recommender system on aws; Content: i'm dabbling with ml and was able to take a tutorial and get it to work for my needs. it's a simple recommender system using tfidfvectorizer and linear_kernel. i run into a problem with how i go about deploying it through with an end point. import pandas as pd from sklearn.feature_extraction.text import tfidfvectorizer from sklearn.metrics.pairwise import linear_kernel import json import csv with open('data\/big_data.json') as json_file: data = json.load(json_file) ds = pd.dataframe(data) tf = tfidfvectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english') tfidf_matrix = tf.fit_transform(ds['content']) cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix) results = {} for idx, row in ds.iterrows(): similar_indices = cosine_similarities[idx].argsort()[:-100:-1] similar_items = [(cosine_similarities[idx][i], ds['id'][i]) for i in similar_indices] results[row['id']] = similar_items[1:] def item(id): return ds.loc[ds['id'] == id]['id'].tolist()[0] def recommend(item_id, num): print(\"recommending \" + str(num) + \" products similar to \" + item(item_id) + \"...\") print(\"-------\") recs = results[item_id][:num] for rec in recs: print(\"recommended: \" + item(rec[1]) + \" (score:\" + str(rec[0]) + \")\") recommend(item_id='129035', num=5) as a starting point i'm not sure if the output from tf.fit_transform(ds['content']) is considered the model or the output from linear_kernel(tfidf_matrix, tfidf_matrix).",
        "Question_original_content_gpt_summary":"The user is facing a challenge in deploying a ML recommender system on AWS, as they are unsure if the output from tf.fit_transform(ds['content']) or linear_kernel(tfidf_matrix, tfidf_matrix) is considered the model.",
        "Question_preprocessed_content":"Title: how to deploy ml recommender system on aws; Content: i'm dabbling with ml and was able to take a tutorial and get it to work for my needs. it's a simple recommender system using tfidfvectorizer and i run into a problem with how i go about deploying it through with an end point. as a starting point i'm not sure if the output from is considered the model or the output from .",
        "Answer_original_content":"i came to the conclusion that i didn't need to deploy this through . since the final linear_kernel output was a dictionary i could do quick id lookups to find correlations. i have it working on aws with api gateway\/lambda, dynamodb and an ec2 server to collect, process and plug the data into dynamodb for fast lookups. no expensive endpoint needed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i came to the conclusion that i didn't need to deploy this through . since the final output was a dictionary i could do quick id lookups to find correlations. i have it working on aws with api dynamodb and an ec server to collect, process and plug the data into dynamodb for fast lookups. no expensive endpoint needed."
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML workspace- compute prices",
        "Question_body":"I was trying to create a compute instance on Azure ml workspace and found that few of the virtual machine sizes show blank in thee price column. Wondering if they are free to use or its a bug. I sorted by prices. So, you can see first few rows showing blanks under price column",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1606098165613,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/171550\/azure-ml-workspace-compute-prices.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-30T08:49:53.507Z",
                "Answer_score":0,
                "Answer_body":"@rawwar Thanks for the feedback. It's bug over free AzureML compute, We will update once the team fix this issue.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: workspace- compute prices; Content: i was trying to create a compute instance on workspace and found that few of the virtual machine sizes show blank in thee price column. wondering if they are free to use or its a bug. i sorted by prices. so, you can see first few rows showing blanks under price column",
        "Question_original_content_gpt_summary":"The user encountered a challenge while trying to create a compute instance on workspace, as some of the virtual machine sizes showed blank in the price column, leaving them unsure if they were free to use or if it was a bug.",
        "Question_preprocessed_content":"Title: workspace compute prices; Content: i was trying to create a compute instance on workspace and found that few of the virtual machine sizes show blank in thee price column. wondering if they are free to use or its a bug. i sorted by prices. so, you can see first few rows showing blanks under price column",
        "Answer_original_content":"@rawwar thanks for the feedback. it's bug over free compute, we will update once the team fix this issue.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the feedback. it's bug over free compute, we will update once the team fix this issue."
    },
    {
        "Question_id":null,
        "Question_title":"Hyperparameter search for RL(stable baseline3)",
        "Question_body":"<p>It seems the hyperparameter search guide in documentation is catered for supervised setting. Is there any guide on adapting it to reinforcement learning setting, specifically stable baseline3?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1667204781913,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":267.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hyperparameter-search-for-rl-stable-baseline3\/3349",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-02T13:58:39.731Z",
                "Answer_body":"<p>Hi Zeyu Xhang,<\/p>\n<p>Thanks for your question! Here you have an article about sweeps with RL. Also, I was wondering if you could explain me exactly in which step are you having issues to create the sweep and so I can help you better. Thanks!<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-07T16:09:59.737Z",
                "Answer_body":"<p>Hi Zeyu,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-30T08:27:02.346Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: hyperparameter search for rl(stable baseline3); Content: it seems the hyperparameter search guide in documentation is catered for supervised setting. is there any guide on adapting it to reinforcement learning setting, specifically stable baseline3?",
        "Question_original_content_gpt_summary":"The user is looking for guidance on how to adapt the hyperparameter search guide in the documentation to a reinforcement learning setting, specifically stable baseline3.",
        "Question_preprocessed_content":"Title: hyperparameter search for rl; Content: it seems the hyperparameter search guide in documentation is catered for supervised setting. is there any guide on adapting it to re rcement learning setting, specifically stable baseline ?",
        "Answer_original_content":"hi zeyu xhang, thanks for your question! here you have an article about sweeps with rl. also, i was wondering if you could explain me exactly in which step are you having issues to create the sweep and so i can help you better. thanks! best, luis hi zeyu, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi zeyu xhang, thanks for your question! here you have an article about sweeps with rl. also, i was wondering if you could explain me exactly in which step are you having issues to create the sweep and so i can help you better. thanks! best, luis hi zeyu, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. best, luis"
    },
    {
        "Question_id":null,
        "Question_title":"Import ML Model from ADLS to Azure ML using Databricks",
        "Question_body":"Hi,\nI have stored some ml model in my ADLS and I want to register the model to Azure ML using databricks.\nTried to use the following codes to register my ml model but keep encountering an error that the path cannot be found.\n\nimport urllib.request\nfrom azureml.core.model import Model\n\nRegister a model\n\n\n\nmodel = Model.register(model_path = 'dbfs:\/mnt\/machinelearning\/classifier.joblib',\nmodel_name = \"pretrained-classifier\",\ndescription = \"Pretrained Classifier\",\nworkspace=ws)",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1642414997297,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/697789\/import-ml-model-from-adls-to-azure-ml-using-databr.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-17T16:51:50.977Z",
                "Answer_score":1,
                "Answer_body":"@Yuzu-9670 Using the databricks file path for registering a model is not supported. When using the model.register() you need to download the model locally and then use the path of the model or the folder in which the model is present to register the same.\n\n\n\n\nmodel_path\n\n\nThe path on the local file system where the model assets are located. This can be a direct pointer to a single file or folder. If pointing to a folder, the child_paths parameter can be used to specify individual files to bundle together as the Model object, as opposed to using the entire contents of the folder.\n\nThis sample notebook should help you with using the method.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-01-18T06:21:46.863Z",
                "Answer_score":0,
                "Answer_body":"Hi @romungi-MSFT,\nThank you for your comment!\nI have shifted my ml model to a repo folder and it works now.\nThank you!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: import ml model from adls to using databricks; Content: i have stored one ml model in my adls and i want to register the model to using databricks. tried to use the following codes to register my ml model but keep encountering an error that the path cannot be found. i have mount the storage to my databricks. import urllib.request from .core.model import model # register a model model = model.register(model_path = 'dbfs:\/mnt\/machinelearning\/classifier.joblib', model_name = \"pretrained-classifier\", description = \"pretrained classifier\", workspace=ws) thanks in advance!",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to register a machine learning model stored in Azure Data Lake Storage to Azure Machine Learning using Databricks.",
        "Question_preprocessed_content":"Title: import ml model from adls to using databricks; Content: i have stored one ml model in my adls and i want to register the model to using databricks. tried to use the following codes to register my ml model but keep encountering an error that the path cannot be found. i have mount the storage to my databricks. thanks in advance!",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":56255154.0,
        "Question_title":"How to use a pretrained model from s3 to predict some data?",
        "Question_body":"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. <\/p>\n\n<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.<\/p>\n\n<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:<\/p>\n\n<p><a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker\" rel=\"noreferrer\">https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker<\/a>\n<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb<\/a><\/p>\n\n<p>My code is this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pipeline import PipelineModel\nfrom sagemaker.model import Model\n\ns3_model_bucket = 'bucket'\ns3_model_key_prefix = 'prefix'\ndata = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\nmodels = ss_model.create_model() # ss_model is my sagemaker.estimator\n\nmodel = PipelineModel(name=data, role=role, models= [models])\nss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1558522248223,
        "Question_favorite_count":1.0,
        "Question_score":6.0,
        "Question_view_count":7404.0,
        "Owner_creation_time":1558518328152,
        "Owner_last_access_time":1662111793340,
        "Owner_reputation":79.0,
        "Owner_up_votes":6.0,
        "Owner_down_votes":0.0,
        "Owner_views":16.0,
        "Answer_body":"<p>You can actually instantiate a Python SDK <code>model<\/code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:<\/p>\n\n<pre><code>trainedmodel = sagemaker.model.Model(\n    model_data='s3:\/\/...model path here..\/model.tar.gz',\n    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1\n    role=role)  # your role here; could be different name\n\ntrainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>\n\n<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:<\/p>\n\n<pre><code>predictor = sagemaker.predictor.RealTimePredictor(\n    endpoint='endpoint name here',\n    content_type='image\/jpeg',\n    accept='image\/png')\n<\/code><\/pre>\n\n<p>More on those abstractions:<\/p>\n\n<ul>\n<li><code>Model<\/code>: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html<\/a><\/li>\n<li><code>Predictor<\/code>:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1558621559712,
        "Answer_score":13.0,
        "Owner_location":"London, UK",
        "Question_last_edit_time":1558524094856,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56255154",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to use a pretrained model from s3 to predict some data?; Content: i have trained a semantic segmentation model using the and the out has been saved to a s3 bucket. i want to load this model from the s3 to predict some images in . i know how to predict if i leave the notebook instance running after the training as its just an easy deploy but doesn't really help if i want to use an older model. i have looked at these sources and been able to come up with something myself but it doesn't work hence me being here: https:\/\/course.fast.ai\/deployment_amzn_.html#deploy-to- https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-\/ https:\/\/.readthedocs.io\/en\/stable\/pipeline.html https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb my code is this: from .pipeline import pipelinemodel from .model import model s3_model_bucket = 'bucket' s3_model_key_prefix = 'prefix' data = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz') models = ss_model.create_model() # ss_model is my .estimator model = pipelinemodel(name=data, role=role, models= [models]) ss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')",
        "Question_original_content_gpt_summary":"The user is facing challenges in using a pretrained model from an S3 bucket to predict some data.",
        "Question_preprocessed_content":"Title: how to use a pretrained model from s to predict some data?; Content: i have trained a semantic segmentation model using the and the out has been saved to a s bucket. i want to load this model from the s to predict some images in . i know how to predict if i leave the notebook instance running after the training as its just an easy deploy but doesn't really help if i want to use an older model. i have looked at these sources and been able to come up with something myself but it doesn't work hence me being here my code is this",
        "Answer_original_content":"you can actually instantiate a python sdk model object from existing artifacts, and deploy it to an endpoint. this allows you to deploy a model from trained artifacts, without having to retrain in the notebook. for example, for the semantic segmentation model: trainedmodel = .model.model( model_data='s3:\/\/...model path here..\/model.tar.gz', image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest', # example path for the semantic segmentation in eu-west-1 role=role) # your role here; could be different name trainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge') and similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the sdk, with the following command: predictor = .predictor.realtimepredictor( endpoint='endpoint name here', content_type='image\/jpeg', accept='image\/png') more on those abstractions: model: https:\/\/.readthedocs.io\/en\/stable\/model.html predictor: https:\/\/.readthedocs.io\/en\/stable\/predictors.html",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"you can actually instantiate a python sdk object from existing artifacts, and deploy it to an endpoint. this allows you to deploy a model from trained artifacts, without having to retrain in the notebook. for example, for the semantic segmentation model and similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the sdk, with the following command more on those abstractions"
    },
    {
        "Question_id":null,
        "Question_title":"Dvc get git@github.com:*.git folder giving ERROR: unexpected error - Response payload is not completed",
        "Question_body":"<p>I have successfully added a folder with 2gb of files using DVC to an s3 bucket. After that I tried to get the folder using the dvc get command. It downloaded half the folder correctly and after that the download stopped and gave me this message: ERROR: Unexpected error - Response payload not completed<br>\nAny suggestions on how I can resolve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1634238227604,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":355.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-get-git-github-com-git-folder-giving-error-unexpected-error-response-payload-is-not-completed\/919",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-15T21:38:57.691Z",
                "Answer_body":"<p>For the record: this was discussed on discord <a href=\"https:\/\/discord.com\/channels\/485586884165107732\/485596304961962003\/898534803362701322\" class=\"inline-onebox\">Discord<\/a> and seems to have been a network issue.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: get git@github.com:*.git folder giving error: unexpected error - response payload is not completed; Content: i have successfully added a folder with 2gb of files using to an s3 bucket. after that i tried to get the folder using the get command. it downloaded half the folder correctly and after that the download stopped and gave me this message: error: unexpected error - response payload not completed any suggestions on how i can resolve this issue?",
        "Question_original_content_gpt_summary":"The user encountered an unexpected error while attempting to download a folder from an S3 bucket using the get command.",
        "Question_preprocessed_content":"Title: get folder giving error unexpected error response payload is not completed; Content: i have successfully added a folder with gb of files using to an s bucket. after that i tried to get the folder using the get command. it downloaded half the folder correctly and after that the download stopped and gave me this message error unexpected error response payload not completed any suggestions on how i can resolve this issue?",
        "Answer_original_content":"for the record: this was discussed on discord discord and seems to have been a network issue.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"for the record this was discussed on discord discord and seems to have been a network issue."
    },
    {
        "Question_id":null,
        "Question_title":"Vertex AI Training: Auto-packaged Custom Training Job Yields Very Large Docker Image",
        "Question_body":"Hello,I am trying to run a Custom Training Job in the Vertex AI Training service.The job is based on a tutorial for that fine-tuning a pre-trained BERT model (from HuggingFace).When I use the `gcloud` CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:$BASE_GPU_IMAGE=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\"\n$BUCKET_NAME = \"my-bucket\"gcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=\"--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier\" `\n--worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task\"... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.Granted the base image is around 6.5GB but where do the additional >10GB come from? Is there a way for me to avoid incurring the added size increase?Please note that my job loads the training data using the `datasets` Python package at run time and AFAIK does not include it in the auto-packaged docker image. Thanks,\nurig",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1645930620000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":441.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Training-Auto-packaged-Custom-Training-Job-Yields-Very\/td-p\/397685\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-17T08:03:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2022-04-07T10:36:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hi Urig,\n\nIs it possible that you have local files in the current directory such as data or log files that are getting picked up, specifically this line local-package-path=.\n\nIf this persists, I highly recommend for you to file a Public Issue\u00a0as you can high a private thread created to you and we would be able to further support you there."
            },
            {
                "Answer_creation_time":"2022-04-17T08:03:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: training: auto-packaged custom training job yields very large docker image; Content: hello,i am trying to run a custom training job in the training service.the job is based on a tutorial for that fine-tuning a pre-trained bert model (from huggingface).when i use the `gcloud` cli tool to auto-package my training code into a docker image and deploy it to the training service like so:$base_gpu_image=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\" $bucket_name = \"my-bucket\"gcloud ai custom-jobs create ` --region=us-central1 ` --display-name=fine_tune_bert ` --args=\"--job_dir=$bucket_name,--num-epochs=2,--model-name=finetuned-bert-classifier\" ` --worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=nvidia_tesla_v100,executor-image-uri=$base_gpu_image,local-package-path=.,python-module=trainer.task\"... i end up with a docker image that is roughly 18gb (!) and takes a very long time to upload to the gcp registry.granted the base image is around 6.5gb but where do the additional >10gb come from? is there a way for me to avoid incurring the added size increase?please note that my job loads the training data using the `datasets` python package at run time and afaik does not include it in the auto-packaged docker image. thanks, urig",
        "Question_original_content_gpt_summary":"The user encountered a challenge of a very large docker image when auto-packaging their custom training job for the training service, which was much larger than expected.",
        "Question_preprocessed_content":"Title: training auto packaged custom training job yields very large docker image; Content: hello,i am trying to run a custom training job in the training job is based on a tutorial for that fine tuning a pre trained bert model .when i use the cli tool to auto package my training code into a docker image and deploy it to the training service like my bucket gcloud ai custom jobs create i end up with a docker image that is roughly gb and takes a very long time to upload to the gcp the base image is around but where do the additional > gb come from? is there a way for me to avoid incurring the added size increase?please note that my job loads the training data using the python package at run time and afaik does not include it in the auto packaged docker image. thanks, urig",
        "Answer_original_content":"hello ismail, thank you for your help. i've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image. according to an answer that i've received on stackoverflow.com, it's likely that the 18gb size that i'm seeing is the size of my image after extraction. apparently the ~6.8gb size is for the image compressed. cheers, @urig view solution in original post",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello ismail, thank you for your help. i've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image. according to an answer that i've received on it's likely that the gb size that i'm seeing is the size of my image after extraction. apparently the size is for the image compressed. cheers, view solution in original post"
    },
    {
        "Question_id":43587399.0,
        "Question_title":"Azure ML Experiment Scheduling - No web service",
        "Question_body":"<p>I want to schedule an Azure ML experiment to run everyday without creating a web service. Is that possible. Is there no scheduler in Azure ML itself.<\/p>\n\n<p>I basically import the latest data from Azuresql and then export the predictions again into AzureSql.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1493034890893,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":116.0,
        "Owner_creation_time":1493034212608,
        "Owner_last_access_time":1499766917332,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/43587399",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: experiment scheduling - no web service; Content: i want to schedule an experiment to run everyday without creating a web service. is that possible. is there no scheduler in itself. i basically import the latest data from azuresql and then export the predictions again into azuresql.",
        "Question_original_content_gpt_summary":"The user is looking for a way to schedule an experiment to run everyday without creating a web service.",
        "Question_preprocessed_content":"Title: experiment scheduling no web service; Content: i want to schedule an experiment to run everyday without creating a web service. is that possible. is there no scheduler in itself. i basically import the latest data from azuresql and then export the predictions again into azuresql.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"why the ML data only result in 1 point?",
        "Question_body":"",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1629379772053,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/520581\/why-the-ml-data-only-result-in-1-point.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-02T08:43:51.05Z",
                "Answer_score":0,
                "Answer_body":"\u60a8\u597d\uff0c\n\n\u611f\u8c22\u60a8\u8054\u7cfb\u5fae\u8f6fAzure\u8bba\u575b\uff0c\u6211\u8bd5\u56fe\u91cd\u590d\u60a8\u6240\u8bf4\u7684bug\uff0c\u4f46\u662f\u5e76\u4e0d\u80fd\u5f97\u5230\u4efb\u4f55\u201c1 point\u201d\u7684\u7ed3\u679c\u3002\u5982\u679c\u60a8\u4ecd\u6709\u8fd9\u4e2a\u95ee\u9898\uff0c\u8bf7\u63d0\u4f9b\u66f4\u591a\u8be6\u60c5\uff0c\u6211\u4eec\u4f1a\u5c3d\u53ef\u80fd\u63d0\u4f9b\u6700\u5927\u5e2e\u52a9\u3002\n\n\u8c22\u8c22\u60a8\u3002\n\n\u5b87\u5f64",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why the ml data only result in 1 point?; Content:",
        "Question_original_content_gpt_summary":"The user is struggling to understand why their machine learning data is only resulting in a single point.",
        "Question_preprocessed_content":"Title: why the ml data only result in point?",
        "Answer_original_content":"azurebug1 point",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"azurebug point"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning (AutoML) export data to SharePoint",
        "Question_body":"I am using Azure Machine Learning Studio to design pipelines to analyze data.\nIs there any possibility to export data to sharepoint?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1631064917827,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/543361\/azure-machine-learning-automl-export-data-to-share.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-08T07:07:22.44Z",
                "Answer_score":2,
                "Answer_body":"Hi @MiaZhangWHQWistron-2092\nPer my research, there is no way to export data from Azure Machine Learning Studio to SharePoint directly.\n\nAs an alternative, you could export data to Azure SQL database first:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-sql-database\n\nThen export data from Azure SQL database to SharePoint list:\nhttps:\/\/social.technet.microsoft.com\/wiki\/contents\/articles\/39170.azure-sql-db-with-sharepoint-online-as-external-list-using-business-connectivity-services.aspx\n\n\nIf an Answer is helpful, please click \"Accept Answer\" and upvote it.\n\nNote: Please follow the steps in our documentation to enable e-mail notifications if you want to receive the related email notification for this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: (automl) export data to sharepoint; Content: i am using studio to design pipelines to analyze data. is there any possibility to export data to sharepoint?",
        "Question_original_content_gpt_summary":"The user is looking for a way to export data from Studio to SharePoint.",
        "Question_preprocessed_content":"Title: export data to sharepoint; Content: i am using studio to design pipelines to analyze data. is there any possibility to export data to sharepoint?",
        "Answer_original_content":"hi @miazhangwhqwistron-2092 per my research, there is no way to export data from studio to sharepoint directly. as an alternative, you could export data to azure sql database first: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-sql-database then export data from azure sql database to sharepoint list: https:\/\/social.technet.microsoft.com\/wiki\/contents\/articles\/39170.azure-sql-db-with-sharepoint-online-as-external-list-using-business-connectivity-services.aspx if an answer is helpful, please click \"accept answer\" and upvote it. note: please follow the steps in our documentation to enable e-mail notifications if you want to receive the related email notification for this thread.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi per my research, there is no way to export data from studio to sharepoint directly. as an alternative, you could export data to azure sql database first then export data from azure sql database to sharepoint list if an answer is helpful, please click accept answer and upvote it. note please follow the steps in our documentation to enable e mail notifications if you want to receive the related email notification for this thread."
    },
    {
        "Question_id":null,
        "Question_title":"Can't run a basic model in Azure ML",
        "Question_body":"Hello,\n\nI'm new to Azure Machine Learning, so I'm trying some tutorials on a free trial account.\n\nNone of the tutorials I've followed is running on my account, not even the most basic ones.\n\nFor example, the \"Flight Delay Prediction\" is running for several hours and then frustratingly it fails....\n\nThis is how the designer looks like:\n\n\n\n\n\n\nDoes this make sense?\nCan anyone help?\n\nThanks in advance!",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1607553548183,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/192735\/can39t-run-a-basic-model-in-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-10T07:45:59.39Z",
                "Answer_score":0,
                "Answer_body":"@nunonogueira From the screen shot it looks like your experiment is running with some modules run queued. Depending on the compute resources you have chosen the run time might vary and with a free trial account a lower compute power will slow down the run time. I have tried the same experiment with a STANDARD_DS2_V2 type with scaling option from 0-4 resources in the cluster and the run completed in around 23 minutes where most the time went to setup the compute since I do not have any running compute to run the experiment directly. You can try a similar setup with a higher compute and check if it runs faster.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can't run a basic model in ; Content: hello, i'm new to , so i'm trying some tutorials on a free trial account. none of the tutorials i've followed is running on my account, not even the most basic ones. for example, the \"flight delay prediction\" is running for several hours and then frustratingly it fails.... this is how the designer looks like: does this make sense? can anyone help? thanks in advance!",
        "Question_original_content_gpt_summary":"The user is encountering difficulty running even the most basic tutorials on their free trial account of , with the \"flight delay prediction\" tutorial running for several hours before failing.",
        "Question_preprocessed_content":"Title: can't run a basic model in; Content: hello, i'm new to , so i'm trying some tutorials on a free trial account. none of the tutorials i've followed is running on my account, not even the most basic ones. for example, the flight delay prediction is running for several hours and then frustratingly it this is how the designer looks like does this make sense? can anyone help? thanks in advance!",
        "Answer_original_content":"@nunonogueira from the screen shot it looks like your experiment is running with some modules run queued. depending on the compute resources you have chosen the run time might vary and with a free trial account a lower compute power will slow down the run time. i have tried the same experiment with a standard_ds2_v2 type with scaling option from 0-4 resources in the cluster and the run completed in around 23 minutes where most the time went to setup the compute since i do not have any running compute to run the experiment directly. you can try a similar setup with a higher compute and check if it runs faster.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"from the screen shot it looks like your experiment is running with some modules run queued. depending on the compute resources you have chosen the run time might vary and with a free trial account a lower compute power will slow down the run time. i have tried the same experiment with a type with scaling option from resources in the cluster and the run completed in around minutes where most the time went to setup the compute since i do not have any running compute to run the experiment directly. you can try a similar setup with a higher compute and check if it runs faster."
    },
    {
        "Question_id":60848427.0,
        "Question_title":"Clone private GitHub repo AWS Sagemaker NB instance using AWS CloudFormation",
        "Question_body":"<p>How do you set up a AWS Sagemaker Notebook instance, using CloudFormation, which is connected to one of your private GitHub repositories?<\/p>\n\n<p>Note: I have added <code>GitHub oauth<\/code> to a <code>ssm<\/code> parameter (called github) before launching the stack.<\/p>\n\n<p>So far I have tried to set up the aformentioned framework using <code>NotebookInstanceLifecycleConfig<\/code>. <\/p>\n\n<p>This example shows how I am able to clone a public repository:<\/p>\n\n<pre><code>  FastaiNotebookInstanceLifecycleConfig:\n    Type: \"AWS::SageMaker::NotebookInstanceLifecycleConfig\"\n    Properties:\n      NotebookInstanceLifecycleConfigName: !Sub ${AWS::StackName}-lifecycle-config\n      OnStart:\n        - Content: !Base64 |\n            #!\/bin\/bash\n\n            git clone https:\/\/github.com\/awslabs\/amazon-sagemaker-mlops-workshop.git \/home\/ec2-user\/SageMaker\/mlops_example\n\n<\/code><\/pre>\n\n<p>However, how can I customize <code>Lifecycleconfig<\/code> to clone a private repository instead? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1585137654783,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":1003.0,
        "Owner_creation_time":1549539224500,
        "Owner_last_access_time":1661856913008,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":5.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Amsterdam, Netherlands",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60848427",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: clone private github repo nb instance using aws cloudformation; Content: how do you set up a notebook instance, using cloudformation, which is connected to one of your private github repositories? note: i have added github oauth to a ssm parameter (called github) before launching the stack. so far i have tried to set up the aformentioned framework using notebookinstancelifecycleconfig. this example shows how i am able to clone a public repository: fastainotebookinstancelifecycleconfig: type: \"aws::::notebookinstancelifecycleconfig\" properties: notebookinstancelifecycleconfigname: !sub ${aws::stackname}-lifecycle-config onstart: - content: !base64 | #!\/bin\/bash git clone https:\/\/github.com\/awslabs\/amazon--mlops-workshop.git \/home\/ec2-user\/\/mlops_example however, how can i customize lifecycleconfig to clone a private repository instead?",
        "Question_original_content_gpt_summary":"The user is encountering challenges in setting up a notebook instance, using CloudFormation, which is connected to one of their private GitHub repositories.",
        "Question_preprocessed_content":"Title: clone private github repo nb instance using aws cloudformation; Content: how do you set up a notebook instance, using cloudformation, which is connected to one of your private github repositories? note i have added to a parameter before launching the stack. so far i have tried to set up the aformentioned framework using . this example shows how i am able to clone a public repository however, how can i customize to clone a private repository instead?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"MLflow 2.0.0rc0 release",
        "Question_body":"We're happy to announce a release candidate for MLflow 2.0:\nhttps:\/\/github.com\/mlflow\/mlflow\/releases\/tag\/v2.0.0rc0\n\nInstallation:\n===================================\n# Make sure python version is >=3.8\npip install mlflow==2.0.0rc0\n===================================\n\n\nDocumentation:\nMLflow 2.0.0rc0 documentation\n\nPlease report any issues with the release candidate in the issue tracker.",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1667264453000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":18.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/khwIVANuAKI",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[

        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: 2.0.0rc0 release; Content: we're happy to announce a release candidate for 2.0: https:\/\/github.com\/\/\/releases\/tag\/v2.0.0rc0 installation: =================================== # make sure python version is >=3.8 pip install ==2.0.0rc0 =================================== documentation: 2.0.0rc0 documentation please report any issues with the release candidate in the issue tracker.",
        "Question_original_content_gpt_summary":"The user encountered challenges with the 2.0.0rc0 release, including installation and documentation.",
        "Question_preprocessed_content":"Title: release; Content: we're happy to announce a release candidate for installation make sure python version is pip install documentation documentation please report any issues with the release candidate in the issue tracker.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Azure On-Demand ML cluster from a search in the data catalog",
        "Question_body":"I'm trying to implement a self-service solution in Azure so users can run a Jupyter or PySpark notebook on-Demand\/automatically with the dataset they found a search in the Azure Data Catalog. I visualize, once the user finds the data in a search, there will be a link that will take him\/her to a Notebook and the dataset can be used for analysis. Any suggestion would be very much appreciated!",
        "Question_answer_count":2,
        "Question_comment_count":1.0,
        "Question_creation_time":1619209726297,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/369952\/azure-on-demand-ml-cluster-from-a-search-in-the-da.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-26T10:22:10.247Z",
                "Answer_score":0,
                "Answer_body":"@JairoMelo-1657 Thanks for the question.Azure Purview can find, understand, and consume data sources. Please follow the Azure Purview documentation: https:\/\/docs.microsoft.com\/en-us\/azure\/purview\/\n\nand We have Azure Open Datasets where you can download a Notebook for AML, Databricks or Synapse that explores the data: Azure Open Datasets Catalog | Microsoft Azure. What are open datasets? Curated public datasets - Azure Open Datasets | Microsoft Docs.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-04-28T11:51:31.093Z",
                "Answer_score":1,
                "Answer_body":"Thank you very much for your response. I'll look into Azure Purview. Best! J.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure on-demand ml cluster from a search in the data catalog; Content: i'm trying to implement a self-service solution in azure so users can run a jupyter or pyspark notebook on-demand\/automatically with the dataset they found a search in the azure data catalog. i visualize, once the user finds the data in a search, there will be a link that will take him\/her to a notebook and the dataset can be used for analysis. any suggestion would be very much appreciated!",
        "Question_original_content_gpt_summary":"The user is trying to implement a self-service solution in Azure to enable users to run a Jupyter or PySpark notebook on-demand with the dataset they found in a search in the Azure Data Catalog.",
        "Question_preprocessed_content":"Title: azure on demand ml cluster from a search in the data catalog; Content: i'm trying to implement a self service solution in azure so users can run a jupyter or pyspark notebook with the dataset they found a search in the azure data catalog. i visualize, once the user finds the data in a search, there will be a link that will take to a notebook and the dataset can be used for analysis. any suggestion would be very much appreciated!",
        "Answer_original_content":"@jairomelo-1657 thanks for the question.azure purview can find, understand, and consume data sources. please follow the azure purview documentation: https:\/\/docs.microsoft.com\/en-us\/azure\/purview\/ and we have azure open datasets where you can download a notebook for aml, databricks or synapse that explores the data: azure open datasets catalog | microsoft azure. what are open datasets? curated public datasets - azure open datasets | microsoft docs.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the purview can find, understand, and consume data sources. please follow the azure purview documentation and we have azure open datasets where you can download a notebook for aml, databricks or synapse that explores the data azure open datasets catalog microsoft azure. what are open datasets? curated public datasets azure open datasets microsoft docs."
    },
    {
        "Question_id":null,
        "Question_title":"Tuning help for Time Series Forecasting model in Azure (AMLS \/ AutoML)",
        "Question_body":"Casting a wide here for some urgent assistance with ML Tuning. Looking for a specialist with time series forecasting model experience in azure... someone who knows the inner workings and can help tune things... needs to know how to alter the Alpha hyper-param in AMLS\/AutoML. The documentation is short in describing the tuning needed.",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1635468147490,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/608424\/tuning-help-for-time-series-forecasting-model-in-a.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-29T12:11:47.177Z",
                "Answer_score":0,
                "Answer_body":"@LarryYE-0750 Thanks for the question. Currently No control over hyperparameter tuning in Model Training.\n\u2022 DNN learners supported for forecasting\nPlease follow the below resources for understanding of Automated ML capabilities as well as forecasting using Automated ML.\n\nResources:\nAutomated ML Forecasting Blog\nAutomated ML Forecasting How-to\nAutomated ML Charts & Metrics\nGitHub Sample Notebooks\n\nHave questions or feedback? Let us know by reaching out to, AskAutomatedML@microsoft.com",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: tuning help for time series forecasting model in azure (amls \/ automl); Content: casting a wide here for some urgent assistance with ml tuning. looking for a specialist with time series forecasting model experience in azure... someone who knows the inner workings and can help tune things... needs to know how to alter the alpha hyper-param in amls\/automl. the documentation is short in describing the tuning needed.",
        "Question_original_content_gpt_summary":"The user is looking for a specialist with time series forecasting model experience in Azure who can help tune the alpha hyper-parameter in AMLS\/AutoML, as the documentation is short in describing the tuning needed.",
        "Question_preprocessed_content":"Title: tuning help for time series forecasting model in azure; Content: casting a wide here for some urgent assistance with ml tuning. looking for a specialist with time series forecasting model experience in someone who knows the inner workings and can help tune needs to know how to alter the alpha hyper param in the documentation is short in describing the tuning needed.",
        "Answer_original_content":"@larryye-0750 thanks for the question. currently no control over hyperparameter tuning in model training. dnn learners supported for forecasting please follow the below resources for understanding of automated ml capabilities as well as forecasting using automated ml. resources: automated ml forecasting blog automated ml forecasting how-to automated ml charts & metrics github sample notebooks have questions or feedback? let us know by reaching out to, askautomatedml@microsoft.com",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the question. currently no control over hyperparameter tuning in model training. dnn learners supported for forecasting please follow the below resources for understanding of automated ml capabilities as well as forecasting using automated ml. resources automated ml forecasting blog automated ml forecasting how to automated ml charts & metrics github sample notebooks have questions or feedback? let us know by reaching out to,"
    },
    {
        "Question_id":64709871.0,
        "Question_title":"AWS SageMaker studio CreateDomain Access error",
        "Question_body":"<p>I'm trying to setup sagemaker studio using quickstart method. I have IAM role with the <code>AmazonSageMakerFullAccess<\/code> added to it. After selecting the VPC\/subnet, I'm getting the below error,<\/p>\n<pre><code>AccessDeniedException\nUser: arn:aws:iam::1234567:user\/blahblah is not authorized to perform: \n      sagemaker:CreateDomain \n      on resource: arn:aws:sagemaker:us-east-1:2345678:domain\/d-hj4oh0jk7g6v\n<\/code><\/pre>\n<p>How do I fix this access issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1604644354467,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":1701.0,
        "Owner_creation_time":1535012930672,
        "Owner_last_access_time":1657552701700,
        "Owner_reputation":41.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Pune, Maharashtra, India",
        "Question_last_edit_time":1604692521470,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64709871",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: studio createdomain access error; Content: i'm trying to setup studio using quickstart method. i have iam role with the amazonfullaccess added to it. after selecting the vpc\/subnet, i'm getting the below error, accessdeniedexception user: arn:aws:iam::1234567:user\/blahblah is not authorized to perform: :createdomain on resource: arn:aws::us-east-1:2345678:domain\/d-hj4oh0jk7g6v how do i fix this access issue?",
        "Question_original_content_gpt_summary":"The user is encountering an \"accessdeniedexception\" when attempting to setup Studio using the Quickstart method.",
        "Question_preprocessed_content":"Title: studio createdomain access error; Content: i'm trying to setup studio using quickstart method. i have iam role with the added to it. after selecting the i'm getting the below error, how do i fix this access issue?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":36260727.0,
        "Question_title":"Equivalent of Subset in Azure machine learning studio",
        "Question_body":"<p>I have a dataset in azure machine learning (.csv), on the same dataset I have multiple models build, I want to subset data for each of the model based on a different column<\/p>\n\n<p>Input:<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>For the 1st model I want to retain all records where col1 not equal to None<\/p>\n\n<pre><code>ID col1 col2 col3\n2  5    45   0\n3  10   0    34\n4  12   1    3\n<\/code><\/pre>\n\n<p>Similarly for model 2<\/p>\n\n<pre><code>ID col1 col2 col3\n1  0    13   0\n2  5    45   0\n4  12   1    3\n<\/code><\/pre>\n\n<p>Hope it was clear<\/p>\n\n<p>The equivalent in R would be <\/p>\n\n<pre><code>df[!df$col1 == \"None\",] \n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1459161991533,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":243.0,
        "Owner_creation_time":1406266059940,
        "Owner_last_access_time":1663232189147,
        "Owner_reputation":1677.0,
        "Owner_up_votes":82.0,
        "Owner_down_votes":2.0,
        "Owner_views":221.0,
        "Answer_body":"<p>You can use the \"Execute R Script\" module and just plug in your R code there.<\/p>\n\n<pre><code>df &lt;- maml.mapInputPort(1)\ndf &lt;- df[!df$col1 == \"None\",] \nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1461479422230,
        "Answer_score":0.0,
        "Owner_location":"Link\u00f6ping, Sweden",
        "Question_last_edit_time":1459256566467,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36260727",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: equivalent of subset in studio; Content: i have a dataset in (.csv), on the same dataset i have multiple models build, i want to subset data for each of the model based on a different column input: id col1 col2 col3 1 0 13 0 2 5 45 0 3 10 0 34 4 12 1 3 for the 1st model i want to retain all records where col1 not equal to none id col1 col2 col3 2 5 45 0 3 10 0 34 4 12 1 3 similarly for model 2 id col1 col2 col3 1 0 13 0 2 5 45 0 4 12 1 3 hope it was clear the equivalent in r would be df[!df$col1 == \"none\",]",
        "Question_original_content_gpt_summary":"The user is looking for a way to subset their dataset in Studio based on different columns for multiple models.",
        "Question_preprocessed_content":"Title: equivalent of subset in studio; Content: i have a dataset in , on the same dataset i have multiple models build, i want to subset data for each of the model based on a different column input for the st model i want to retain all records where col not equal to none similarly for model hope it was clear the equivalent in r would be",
        "Answer_original_content":"you can use the \"execute r script\" module and just plug in your r code there. df <- maml.mapinputport(1) df <- df[!df$col1 == \"none\",] maml.mapoutputport(\"df\");",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"you can use the execute r script module and just plug in your r code there."
    },
    {
        "Question_id":null,
        "Question_title":"How to use custom environment defined in the custom environments tab in Azure Machine Learning Studio.",
        "Question_body":"In Azure Machine Learning Studio, in the Environments section's \"Custom environments\" tab, I defined a custom environment. I have done this once with a Conda yaml file, and once with a requirements.txt file, eg filling out this form as described here: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-environments-in-studio\n\nI can see that the environments have been created but I have no idea how to use them.\n\nI have tried using this code within an Azure ML Studio notebook, where the new environment I defined is called \"my_new_env\":\n\n from azureml.core import Workspace, Environment\n ws = Workspace.from_config()\n env = Environment.get(workspace=ws, name=\"my_new_env\")\n\n\n\nI also tried this within an Azure ML Studio notebook to see if I could define an environment without doing it in the environments menu.\n\n from azureml.core.environment import Environment\n my_new_env = Environment.from_conda_specification(name = \"myenv\", file_path = environment.yml)\n\n\n\nBoth execute without any warnings or errors, but I'm not sure they are running, or indeed what I have done.\n\nHaving run either of these two blocks of code, when I try to select a new environment in the Azure ML Studio notebook's drop down menu:\n\nThere is no evidence of my new environments.\n\nI'm new to Azure ML Studio. What I want to do is create a new Python virtual environment. Am I getting confused between virtual Python environments and some other more general type of environments? If I wanted to create my own stable Python virtual environment within Azure ML to use in notebooks that was not tied to a specific compute instance, how would I do it?\n\nThanks!",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1646779156963,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/764339\/how-to-use-custom-environment-defined-in-the-custo.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-09T10:54:29.947Z",
                "Answer_score":1,
                "Answer_body":"@ml-3263 In the first two cases the environments created from portal or the yaml file are used to train your model or score your model when it is deployed as a endpoint.\nThese are re-usable environments that can even be created on local machine or compute to develop your training script and can be used used on Azure compute for large scale training or deployment.\n\nThe different types of environments are curated, user-managed and system-managed. The first two environments you created fall under the user and system managed categories, whereas curated environments are offered by azure and are available by default in every workspace.\n\nThese environments are not tied to any of the compute instance and you can use them with any type of compute for training or inference.\n\nThe last type of environment that you have listed with the screen shot is actually a virtual environment that you have to setup on your compute instance to use it as your kernel on your notebook. This however is tied to your compute instance and if you need to use the same setup on a different compute then you need to set it up again on a different instance. I have explained the setup on one of the previous threads, which can be helpful if you need one.\n\nTo summarize, if you are looking to create environments to train your models and infer then you will have to use the curated, user\/system-managed environments in your experiments.\nIf you are looking to just use the notebooks then you can setup a custom kernel or virtual environment to run an experiment locally.\n\nA great way to start learning about Azure ML through notebooks is to clone this repo on your ml.azure.com notebooks and follow the steps or tutorials to create and run experiments.\n\nOnce the repo is cloned from sample tab it will be available under files tab to be run on available compute and kernel.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to use custom environment defined in the custom environments tab in studio.; Content: in studio, in the environments section's \"custom environments\" tab, i defined a custom environment. i have done this once with a conda yaml file, and once with a requirements.txt file, eg filling out this form as described here: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-environments-in-studio i can see that the environments have been created but i have no idea how to use them. i have tried using this code within an studio notebook, where the new environment i defined is called \"my_new_env\": from .core import workspace, environment ws = workspace.from_config() env = environment.get(workspace=ws, name=\"my_new_env\") i also tried this within an studio notebook to see if i could define an environment without doing it in the environments menu. from .core.environment import environment my_new_env = environment.from_conda_specification(name = \"myenv\", file_path = environment.yml) both execute without any warnings or errors, but i'm not sure they are running, or indeed what i have done. having run either of these two blocks of code, when i try to select a new environment in the studio notebook's drop down menu: there is no evidence of my new environments. i'm new to studio. what i want to do is create a new python virtual environment. am i getting confused between virtual python environments and some other more general type of environments? if i wanted to create my own stable python virtual environment within to use in notebooks that was not tied to a specific compute instance, how would i do it? thanks!",
        "Question_original_content_gpt_summary":"The user is struggling to understand how to use custom environments defined in the custom environments tab in Azure Machine Learning Studio, and is unsure of the difference between virtual python environments and other types of environments.",
        "Question_preprocessed_content":"Title: how to use custom environment defined in the custom environments tab in studio.; Content: in studio, in the environments section's custom environments tab, i defined a custom environment. i have done this once with a conda yaml file, and once with a file, eg filling out this form as described here i can see that the environments have been created but i have no idea how to use them. i have tried using this code within an studio notebook, where the new environment i defined is called from import workspace, environment ws env i also tried this within an studio notebook to see if i could define an environment without doing it in the environments menu. from import environment myenv , both execute without any warnings or errors, but i'm not sure they are running, or indeed what i have done. having run either of these two blocks of code, when i try to select a new environment in the studio notebook's drop down menu there is no evidence of my new environments. i'm new to studio. what i want to do is create a new python virtual environment. am i getting confused between virtual python environments and some other more general type of environments? if i wanted to create my own stable python virtual environment within to use in notebooks that was not tied to a specific compute instance, how would i do it? thanks!",
        "Answer_original_content":"@ml-3263 in the first two cases the environments created from portal or the yaml file are used to train your model or score your model when it is deployed as a endpoint. these are re-usable environments that can even be created on local machine or compute to develop your training script and can be used used on azure compute for large scale training or deployment. the different types of environments are curated, user-managed and system-managed. the first two environments you created fall under the user and system managed categories, whereas curated environments are offered by azure and are available by default in every workspace. these environments are not tied to any of the compute instance and you can use them with any type of compute for training or inference. the last type of environment that you have listed with the screen shot is actually a virtual environment that you have to setup on your compute instance to use it as your kernel on your notebook. this however is tied to your compute instance and if you need to use the same setup on a different compute then you need to set it up again on a different instance. i have explained the setup on one of the previous threads, which can be helpful if you need one. to summarize, if you are looking to create environments to train your models and infer then you will have to use the curated, user\/system-managed environments in your experiments. if you are looking to just use the notebooks then you can setup a custom kernel or virtual environment to run an experiment locally. a great way to start learning about through notebooks is to clone this repo on your ml.azure.com notebooks and follow the steps or tutorials to create and run experiments. once the repo is cloned from sample tab it will be available under files tab to be run on available compute and kernel. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"in the first two cases the environments created from portal or the yaml file are used to train your model or score your model when it is deployed as a endpoint. these are re usable environments that can even be created on local machine or compute to develop your training script and can be used used on azure compute for large scale training or deployment. the different types of environments are curated, user managed and system managed. the first two environments you created fall under the user and system managed categories, whereas curated environments are offered by azure and are available by default in every workspace. these environments are not tied to any of the compute instance and you can use them with any type of compute for training or inference. the last type of environment that you have listed with the screen shot is actually a virtual environment that you have to setup on your compute instance to use it as your kernel on your notebook. this however is tied to your compute instance and if you need to use the same setup on a different compute then you need to set it up again on a different instance. i have explained the setup on one of the previous threads, which can be helpful if you need one. to summarize, if you are looking to create environments to train your models and infer then you will have to use the curated, environments in your experiments. if you are looking to just use the notebooks then you can setup a custom kernel or virtual environment to run an experiment locally. a great way to start learning about through notebooks is to clone this repo on your notebooks and follow the steps or tutorials to create and run experiments. once the repo is cloned from sample tab it will be available under files tab to be run on available compute and kernel. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":53556558.0,
        "Question_title":"AWS Sagemaker | How to debug docker image | What the parameter we pass",
        "Question_body":"<p>We want to upload a docker image which have our custom code for tensorflow, now we followed this standard code \n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_bring_your_own\/tensorflow_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/tensorflow_bring_your_own\/tensorflow_bring_your_own.ipynb<\/a><\/p>\n\n<p>We are able to upload docker there with our dependency but we are not able to pass the S3 location to their method, now we are not sure if the S3 location is passing to the container or not so added print which is not printing on sagemaker. Can someone please help how to debug the docker as the custom log is also not available on cloudwatch.<\/p>\n\n<pre><code>018-11-30 09:55:17 Uploading - Uploading generated training model\n2018-11-30 09:55:17 Failed - Training job failed\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-17-5fc1c1e7ed65&gt; in &lt;module&gt;()\n     11                       hyperparameters=hyperparameters)\n     12 \n---&gt; 13 estimator.fit(data_location)\n     14 \n     15 # predictor = estimator.deploy(1, instance_type)\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in fit(self, inputs, wait, logs, job_name)\n    232         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    233         if wait:\n--&gt; 234             self.latest_training_job.wait(logs=logs)\n    235 \n    236     def _compilation_job_name(self):\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/estimator.py in wait(self, logs)\n    571     def wait(self, logs=True):\n    572         if logs:\n--&gt; 573             self.sagemaker_session.logs_for_job(self.job_name, wait=True)\n    574         else:\n    575             self.sagemaker_session.wait_for_job(self.job_name)\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in logs_for_job(self, job_name, wait, poll)\n   1121 \n   1122         if wait:\n-&gt; 1123             self._check_job_status(job_name, description, 'TrainingJobStatus')\n   1124             if dot:\n   1125                 print()\n\n~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/session.py in _check_job_status(self, job, desc, status_key_name)\n    821             reason = desc.get('FailureReason', '(No reason provided)')\n    822             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 823             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    824 \n    825     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Training job tensor-2018-11-30-09-52-12-964: Failed Reason: AlgorithmError: Exception during training: Return Code: 1, CMD: ['\/usr\/bin\/python', 'cifar10.py', '--model-dir', '\/opt\/ml\/model', '--train-steps', '100'], Err: b'\/usr\/local\/lib\/python3.5\/dist-packages\/h5py\/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\\n  from ._conv import register_converters as _register_converters\\nTraceback (most recent call last):\\n  File \"cifar10.py\", line 195, in &lt;module&gt;\\n    main()\\n  File \"cifar10.py\", line 188, in main\\n    interactions_processed, user_meta_processed, item_meta_processed, item_feats_set = process_data(interaction_data, interaction_cols, users_meta, users_meta_cols, items_meta, items_meta_cols, user_meta_filterlist=user_meta_list)\\n  File \"cifar10.py\", line 32, in process_data\\n    df=pd.read_csv(interaction_data, engine=\\'c\\', encoding=\\'latin1\\', usecols=interaction_cols).astype(str)\\n  File \"\/usr\/local\/lib\/python3.5\/d\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1543576936170,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1095.0,
        "Owner_creation_time":1501403168107,
        "Owner_last_access_time":1663685130870,
        "Owner_reputation":1370.0,
        "Owner_up_votes":94.0,
        "Owner_down_votes":1.0,
        "Owner_views":125.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Delhi, India",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53556558",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: | how to debug docker image | what the parameter we pass; Content: we want to upload a docker image which have our custom code for tensorflow, now we followed this standard code https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/advanced_functionality\/tensorflow_bring_your_own\/tensorflow_bring_your_own.ipynb we are able to upload docker there with our dependency but we are not able to pass the s3 location to their method, now we are not sure if the s3 location is passing to the container or not so added print which is not printing on . can someone please help how to debug the docker as the custom log is also not available on cloudwatch. 018-11-30 09:55:17 uploading - uploading generated training model 2018-11-30 09:55:17 failed - training job failed --------------------------------------------------------------------------- valueerror traceback (most recent call last) <ipython-input-17-5fc1c1e7ed65> in <module>() 11 hyperparameters=hyperparameters) 12 ---> 13 estimator.fit(data_location) 14 15 # predictor = estimator.deploy(1, instance_type) ~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/\/estimator.py in fit(self, inputs, wait, logs, job_name) 232 self.latest_training_job = _trainingjob.start_new(self, inputs) 233 if wait: --> 234 self.latest_training_job.wait(logs=logs) 235 236 def _compilation_job_name(self): ~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/\/estimator.py in wait(self, logs) 571 def wait(self, logs=true): 572 if logs: --> 573 self._session.logs_for_job(self.job_name, wait=true) 574 else: 575 self._session.wait_for_job(self.job_name) ~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/\/session.py in logs_for_job(self, job_name, wait, poll) 1121 1122 if wait: -> 1123 self._check_job_status(job_name, description, 'trainingjobstatus') 1124 if dot: 1125 print() ~\/anaconda3\/envs\/tensorflow_p36\/lib\/python3.6\/site-packages\/\/session.py in _check_job_status(self, job, desc, status_key_name) 821 reason = desc.get('failurereason', '(no reason provided)') 822 job_type = status_key_name.replace('jobstatus', ' job') --> 823 raise valueerror('error for {} {}: {} reason: {}'.format(job_type, job, status, reason)) 824 825 def wait_for_endpoint(self, endpoint, poll=5): valueerror: error for training job tensor-2018-11-30-09-52-12-964: failed reason: algorithmerror: exception during training: return code: 1, cmd: ['\/usr\/bin\/python', 'cifar10.py', '--model-dir', '\/opt\/ml\/model', '--train-steps', '100'], err: b'\/usr\/local\/lib\/python3.5\/dist-packages\/h5py\/__init__.py:36: futurewarning: conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. in future, it will be treated as `np.float64 == np.dtype(float).type`.\\n from ._conv import register_converters as _register_converters\\ntraceback (most recent call last):\\n file \"cifar10.py\", line 195, in <module>\\n main()\\n file \"cifar10.py\", line 188, in main\\n interactions_processed, user_meta_processed, item_meta_processed, item_feats_set = process_data(interaction_data, interaction_cols, users_meta, users_meta_cols, items_meta, items_meta_cols, user_meta_filterlist=user_meta_list)\\n file \"cifar10.py\", line 32, in process_data\\n df=pd.read_csv(interaction_data, engine=\\'c\\', encoding=\\'latin1\\', usecols=interaction_cols).astype(str)\\n file \"\/usr\/local\/lib\/python3.5\/d",
        "Question_original_content_gpt_summary":"The user is encountering challenges with debugging a Docker image and passing a S3 location to their method, as well as troubleshooting custom logs not appearing on Cloudwatch.",
        "Question_preprocessed_content":"Title: how to debug docker image what the parameter we pass; Content: we want to upload a docker image which have our custom code for tensorflow, now we followed this standard code we are able to upload docker there with our dependency but we are not able to pass the s location to their method, now we are not sure if the s location is passing to the container or not so added print which is not printing on . can someone please help how to debug the docker as the custom log is also not available on cloudwatch.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":66348183.0,
        "Question_title":"Azure DevOps release pipelines \"Conflict of operation error \" while running one job on microsoft-hosted agent",
        "Question_body":"<p>I am trying to implement an Azure DevOps Release pipeline to Deploy a machine learning model on Azure ML. I'm using 6 tasks:<\/p>\n<ul>\n<li>Use python version<\/li>\n<li>Azure CLI : <code>az extension add -n azure-cli-ml<\/code><\/li>\n<li>Azure CLI : <code>az ml model deploy -g &lt;RG&gt; -w &lt;WS&gt; -n &lt;SN&gt; -f ..\/metadata\/model.json --dc aciDeploymentConfig.yml --ic inferenceConfig.yml --overwrite<\/code><\/li>\n<li>Bash Task : pip install requirments.<\/li>\n<li>Azure CLI : Run some integration tests.<\/li>\n<li>Publish Test Results.<\/li>\n<\/ul>\n<p>My pipeline fails on the deployment task, with the error: <br>\n<code>Conflict of operation, another operation on same entity is already running in workspace mlops-wrksp.<\/code><br>\n<code>Script failed with exit code: 1<\/code>\n<br>\nNo other pipeline is running, I'm using the Microsoft-hosted agent.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_creation_time":1614159396837,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":409.0,
        "Owner_creation_time":1581516516116,
        "Owner_last_access_time":1645484908603,
        "Owner_reputation":36.0,
        "Owner_up_votes":5.0,
        "Owner_down_votes":0.0,
        "Owner_views":6.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Morocco",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66348183",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: azure devops release pipelines \"conflict of operation error \" while running one job on microsoft-hosted agent; Content: i am trying to implement an azure devops release pipeline to deploy a machine learning model on . i'm using 6 tasks: use python version azure cli : az extension add -n azure-cli-ml azure cli : az ml model deploy -g <rg> -w <ws> -n <sn> -f ..\/metadata\/model.json --dc acideploymentconfig.yml --ic inferenceconfig.yml --overwrite bash task : pip install requirments. azure cli : run some integration tests. publish test results. my pipeline fails on the deployment task, with the error: conflict of operation, another operation on same entity is already running in workspace mlops-wrksp. script failed with exit code: 1 no other pipeline is running, i'm using the microsoft-hosted agent.",
        "Question_original_content_gpt_summary":"The user encountered a \"conflict of operation error\" while running a job on a Microsoft-hosted agent in an Azure DevOps Release Pipeline to deploy a machine learning model.",
        "Question_preprocessed_content":"Title: azure devops release pipelines conflict of operation error while running one job on microsoft hosted agent; Content: i am trying to implement an azure devops release pipeline to deploy a machine learning model on . i'm using tasks use python version azure cli azure cli bash task pip install requirments. azure cli run some integration tests. publish test results. my pipeline fails on the deployment task, with the error no other pipeline is running, i'm using the microsoft hosted agent.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":63230793.0,
        "Question_title":"How to handle errors in MLflow when a model has been served using \"mlflow models serve\"?",
        "Question_body":"<p>During training, it is possible to use tags as a way to handle exceptions according to <a href=\"https:\/\/stackoverflow.com\/questions\/59856641\/how-can-i-throw-an-exception-from-within-an-mlflow-project\">this question<\/a>.<\/p>\n<p>If a model has been created using <code>mlflow.pyfunc.PythonModel<\/code>, is it possible to throw exceptions? Is there a way to allow error handling for a model that has been served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1596462943310,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":266.0,
        "Owner_creation_time":1472932425400,
        "Owner_last_access_time":1623748857056,
        "Owner_reputation":3.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Pune, Maharashtra, India",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63230793",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to handle errors in when a model has been served using \" models serve\"?; Content: during training, it is possible to use tags as a way to handle exceptions according to this question. if a model has been created using .pyfunc.pythonmodel, is it possible to throw exceptions? is there a way to allow error handling for a model that has been served?",
        "Question_original_content_gpt_summary":"The user is looking for a way to handle errors when a model has been served using \"models serve\".",
        "Question_preprocessed_content":"Title: how to handle errors in when a model has been served using models serve ?; Content: during training, it is possible to use tags as a way to handle exceptions according to this question. if a model has been created using , is it possible to throw exceptions? is there a way to allow error handling for a model that has been served?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":52686841.0,
        "Question_title":"Support for distance package in Azure Machine Learning Studio",
        "Question_body":"<p>I'm checking this url <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/r-packages-supported-by-azure-machine-learning#bkmk_D\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/r-packages-supported-by-azure-machine-learning#bkmk_D<\/a><\/p>\n\n<p>to confirm wheter or not this supports the Distance package <a href=\"http:\/\/distancesampling.org\/R\/\" rel=\"nofollow noreferrer\">http:\/\/distancesampling.org\/R\/<\/a><\/p>\n\n<p>Seems not, but I'm not 100% sure this is the right place to check it. Am I doing it right here?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1538901712943,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":71.0,
        "Owner_creation_time":1279805632796,
        "Owner_last_access_time":1663939443996,
        "Owner_reputation":473.0,
        "Owner_up_votes":19.0,
        "Owner_down_votes":0.0,
        "Owner_views":61.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Banyoles, Spain",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52686841",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: support for distance package in studio; Content: i'm checking this url https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/r-packages-supported-by-azure-machine-learning#bkmk_d to confirm wheter or not this supports the distance package http:\/\/distancesampling.org\/r\/ seems not, but i'm not 100% sure this is the right place to check it. am i doing it right here?",
        "Question_original_content_gpt_summary":"The user is trying to determine if the distance package is supported by Azure Machine Learning Studio.",
        "Question_preprocessed_content":"Title: support for distance package in studio; Content: i'm checking this url to confirm wheter or not this supports the distance package seems not, but i'm not % sure this is the right place to check it. am i doing it right here?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":73219841.0,
        "Question_title":"How can I set hidden_units to a list in Vertex AI?",
        "Question_body":"<p>I am following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/02-experimentation.ipynb\" rel=\"nofollow noreferrer\">this<\/a> notebook '02 ML Experimentation with Custom Model'.<\/p>\n<p>When i try <code>vertex_ai.log_params(hyperparams)<\/code>, I get:<\/p>\n<pre><code>TypeError: Value for key hidden_units is of type list but must be one of float, int, str\n<\/code><\/pre>\n<p>but the next step <code>classifier = trainer.train<\/code> needs <code>hidden_units<\/code> to be a list.<\/p>\n<p>(My version of <code>google-cloud-aiplatform<\/code> is <code>1.16.0<\/code>.)\nAny help is appreciated.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1659521125980,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":49.0,
        "Owner_creation_time":1351154914716,
        "Owner_last_access_time":1663927832783,
        "Owner_reputation":2564.0,
        "Owner_up_votes":304.0,
        "Owner_down_votes":8.0,
        "Owner_views":451.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73219841",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i set hidden_units to a list in ?; Content: i am following this notebook '02 ml experimentation with custom model'. when i try vertex_ai.log_params(hyperparams), i get: typeerror: value for key hidden_units is of type list but must be one of float, int, str but the next step classifier = trainer.train needs hidden_units to be a list. (my version of google-cloud-aiplatform is 1.16.0.) any help is appreciated.",
        "Question_original_content_gpt_summary":"The user is encountering a TypeError when trying to log hyperparameters for a custom model in Google Cloud AI Platform, as the value for the key 'hidden_units' must be one of float, int, or str, but the next step requires it to be a list.",
        "Question_preprocessed_content":"Title: how can i set to a list in ?; Content: i am following this notebook ' ml experimentation with custom model'. when i try , i get but the next step needs to be a list. my version of is . any help is appreciated.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":72011565.0,
        "Question_title":"Predictions into the future Azure Machine Learning Studio Designer",
        "Question_body":"<p>I am currently developing an automated mechanism where I use the Azure Machine Learning Designer (AMLD). During development i used an 80\/20 Split to test the efficency of my predictions.\nNow i want to go live but I've missed the point where i can actually predict into the future.<\/p>\n<p>I currently get a prediction for the last 20% of my data so i can compare them to the actual data. How do i change it so that the prediction actually starts at the end of my data?<\/p>\n<p>A part of my prediction process is attached:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eh7Rv.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eh7Rv.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5.0,
        "Question_creation_time":1650965432403,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":39.0,
        "Owner_creation_time":1592226417127,
        "Owner_last_access_time":1663873257356,
        "Owner_reputation":13.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Germany",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72011565",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: predictions into the future studio designer; Content: i am currently developing an automated mechanism where i use the designer (amld). during development i used an 80\/20 split to test the efficency of my predictions. now i want to go live but i've missed the point where i can actually predict into the future. i currently get a prediction for the last 20% of my data so i can compare them to the actual data. how do i change it so that the prediction actually starts at the end of my data? a part of my prediction process is attached:",
        "Question_original_content_gpt_summary":"The user is facing a challenge in developing an automated mechanism using the designer (AMLD) to predict into the future, as they are currently only able to predict for the last 20% of their data.",
        "Question_preprocessed_content":"Title: predictions into the future studio designer; Content: i am currently developing an automated mechanism where i use the designer . during development i used an split to test the efficency of my predictions. now i want to go live but i've missed the point where i can actually predict into the future. i currently get a prediction for the last % of my data so i can compare them to the actual data. how do i change it so that the prediction actually starts at the end of my data? a part of my prediction process is attached",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"regression model",
        "Question_body":"Why we can't draw line (like jig jag line) which connect all the data points, in case of regression. Why we draw straight line?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1608745621877,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/209043\/regression-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-24T13:58:51.993Z",
                "Answer_score":0,
                "Answer_body":"@SanniddhaChakrabarti-9451 Thanks, vector valued regression i.e. to treat the dependent variables as a vector.\nAnother approach is multi-task learning which means that you treat each dependent variable as a task but you learn the two tasks jointly together. In many cases this outperforms learning the tasks separately.\n\nPlease follow the Regression.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: regression model; Content: why we can't draw line (like jig jag line) which connect all the data points, in case of regression. why we draw straight line?",
        "Question_original_content_gpt_summary":"The user is questioning why a straight line is used in regression models instead of a jagged line that connects all the data points.",
        "Question_preprocessed_content":"Title: regression model; Content: why we can't draw line which connect all the data points, in case of regression. why we draw straight line?",
        "Answer_original_content":"@sanniddhachakrabarti-9451 thanks, vector valued regression i.e. to treat the dependent variables as a vector. another approach is multi-task learning which means that you treat each dependent variable as a task but you learn the two tasks jointly together. in many cases this outperforms learning the tasks separately. please follow the regression.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks, vector valued regression to treat the dependent variables as a vector. another approach is multi task learning which means that you treat each dependent variable as a task but you learn the two tasks jointly together. in many cases this outperforms learning the tasks separately. please follow the regression."
    },
    {
        "Question_id":null,
        "Question_title":"Conda environment locked by another AzureML job",
        "Question_body":"I tried to run an experiments. There was an error in my first submit and the run did not go through. However, a lock has been created which is preventing me from submitting further runs. I am getting the following error.\n\n\"The conda environment is currently locked by another AzureML job. Further job submission will wait until the other process finishes. If there are no other jobs running, please delete \/home\/azureuser\/.azureml\/locks\/azureml_conda_lock\"\n\nI tried to use:\naz ml run cancel -r exp_id\n\nin CLI. However, this gives me an error:\nError, default experiment not set and experiment name parameter not provided.\\nPlease provide a value for the experiment name parameter.",
        "Question_answer_count":1,
        "Question_comment_count":4.0,
        "Question_creation_time":1611713493223,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/246501\/conda-environment-locked-by-another-azureml-job.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-27T12:23:43.017Z",
                "Answer_score":0,
                "Answer_body":"@KanyanLawrence-5964 Thanks for the question. Could you please add more details about the steps\/link to the code that you are trying.\nHere is the doc to configure and submit training runs.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#persistent",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: conda environment locked by another job; Content: i tried to run an experiments. there was an error in my first submit and the run did not go through. however, a lock has been created which is preventing me from submitting further runs. i am getting the following error. \"the conda environment is currently locked by another job. further job submission will wait until the other process finishes. if there are no other jobs running, please delete \/home\/azureuser\/.\/locks\/_conda_lock\" i tried to use: az ml run cancel -r exp_id in cli. however, this gives me an error: error, default experiment not set and experiment name parameter not provided.\\nplease provide a value for the experiment name parameter.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge where their conda environment is locked by another job, preventing them from submitting further runs, and they are unable to cancel the job using the CLI.",
        "Question_preprocessed_content":"Title: conda environment locked by another job; Content: i tried to run an experiments. there was an error in my first submit and the run did not go through. however, a lock has been created which is preventing me from submitting further runs. i am getting the following error. the conda environment is currently locked by another job. further job submission will wait until the other process finishes. if there are no other jobs running, please delete i tried to use az ml run cancel r in cli. however, this gives me an error error, default experiment not set and experiment name parameter not provide a value for the experiment name parameter.",
        "Answer_original_content":"@kanyanlawrence-5964 thanks for the question. could you please add more details about the steps\/link to the code that you are trying. here is the doc to configure and submit training runs. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#persistent",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the question. could you please add more details about the to the code that you are trying. here is the doc to configure and submit training runs."
    },
    {
        "Question_id":73658032.0,
        "Question_title":"How to enable server-side batching on SageMaker PyTorch TorchServe endpoints?",
        "Question_body":"<p>How do I enable server-side batching on SageMaker PyTorch TorchServe endpoints?\nCan't seem to find relevant documentation about this around.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1662704254693,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":19.0,
        "Owner_creation_time":1412669622830,
        "Owner_last_access_time":1663944305230,
        "Owner_reputation":26.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73658032",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to enable server-side batching on pytorch torchserve endpoints?; Content: how do i enable server-side batching on pytorch torchserve endpoints? can't seem to find relevant documentation about this around.",
        "Question_original_content_gpt_summary":"The user is trying to enable server-side batching on PyTorch TorchServe endpoints, but is having difficulty finding relevant documentation.",
        "Question_preprocessed_content":"Title: how to enable server side batching on pytorch torchserve endpoints?; Content: how do i enable server side batching on pytorch torchserve endpoints? can't seem to find relevant documentation about this around.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML deploy new model",
        "Question_body":"I have created an Azure ML model using Azure ML studios notebooks.\n\nIm trying to build and environment with the needed packages, but im having some problems get my model deployed to the environment.\n\nIm pretty new to this whole process so im following the Microsofts guides, but im finding them hard to use when running into issues.\n\nAre there some blogs\/post with real life examples on how to deploy and troubleshoot deploying ML models on Azure?",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1667478071347,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1074346\/azure-ml-deploy-new-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-15T19:55:09.683Z",
                "Answer_score":0,
                "Answer_body":"Hello @JacobBrockHansen-5654\n\nWe have not hear from you. I hope you have solved your problem but I still want to share more information I got for your issue for reference.\n\nFor your question - real life trouble shooting guidance, you can post any issue you have in this forum, engineers and community will help you out of it.\n\nOfficial troubleshooting guidance we have two -\nTroubleshooting online endpoints deployment and scoring: https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-online-endpoints?tabs=cli\nTroubleshooting remote model deployment: https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-troubleshoot-deployment?tabs=azcli\n\nHow guidance for new users -\nHow to deploy models, please see - https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-managed-online-endpoints?tabs=azure-cli\nThere is also a sample for how to deploy models in designer - https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy\n\nYou can post any issue you face during your exploration, we are happy to help.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deploy new model; Content: i have created an model using studios notebooks. im trying to build and environment with the needed packages, but im having some problems get my model deployed to the environment. im pretty new to this whole process so im following the microsofts guides, but im finding them hard to use when running into issues. are there some blogs\/post with real life examples on how to deploy and troubleshoot deploying ml models on azure?",
        "Question_original_content_gpt_summary":"The user is having difficulty deploying a new model to an environment with the necessary packages, and is finding the Microsoft guides difficult to use when running into issues.",
        "Question_preprocessed_content":"Title: deploy new model; Content: i have created an model using studios notebooks. im trying to build and environment with the needed packages, but im having some problems get my model deployed to the environment. im pretty new to this whole process so im following the microsofts guides, but im finding them hard to use when running into issues. are there some with real life examples on how to deploy and troubleshoot deploying ml models on azure?",
        "Answer_original_content":"hello @jacobbrockhansen-5654 we have not hear from you. i hope you have solved your problem but i still want to share more information i got for your issue for reference. for your question - real life trouble shooting guidance, you can post any issue you have in this forum, engineers and community will help you out of it. official troubleshooting guidance we have two - troubleshooting online endpoints deployment and scoring: https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-online-endpoints?tabs=cli troubleshooting remote model deployment: https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/v1\/how-to-troubleshoot-deployment?tabs=azcli how guidance for new users - how to deploy models, please see - https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-managed-online-endpoints?tabs=azure-cli there is also a sample for how to deploy models in designer - https:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy you can post any issue you face during your exploration, we are happy to help. regards, yutong -please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello we have not hear from you. i hope you have solved your problem but i still want to share more i got for your issue for reference. for your question real life trouble shooting guidance, you can post any issue you have in this forum, engineers and community will help you out of it. official troubleshooting guidance we have two troubleshooting online endpoints deployment and scoring troubleshooting remote model deployment how guidance for new users how to deploy models, please see there is also a sample for how to deploy models in designer you can post any issue you face during your exploration, we are happy to help. regards, yutong please kindly accept the answer if you feel helpful to support the community, thanks a lot."
    },
    {
        "Question_id":null,
        "Question_title":"After deploying getting error 502 while generating forecast in Azure Ml through Code",
        "Question_body":"Hi,\nAfter doing the deployment task, i am not able to generate the forecast through code pipeline of Azure Ml.\nthis is my run.py file:\n\ndef run(df, url, api_key):\nallowSelfSignedHttps(True)\nif api_key == 'uu':\napi_key = ''\ndata = []\nfor index, row in list(df.iterrows()):\ndata.append(dict(row))\n\n data = {\n     \"data\": data\n }\n body = str.encode(json.dumps(data))\n headers = {'Content-Type': 'application\/json',\n            'Authorization': ('Bearer ' + api_key)}\n req = urllib.request.Request(url, body, headers)\n print(url, api_key)\n try:\n     response = urllib.request.urlopen(req)\n     result = response.read()\n     print(result)\n except urllib.error.HTTPError as error:\n     print(\"The request failed with status code: \" + str(error.code))\n     # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n     print(error.info())\n     # print(json.loads(error.read().decode(\"utf8\", 'ignore')))\n data = json.loads(json.load(response))\n df= pd.json_normalize(data[result])\n data = data.get('forecast', None)\n return data\n\n\n\n\nError: 502",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1644405738470,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/728627\/after-deploying-getting-error-502-while-generating.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: after deploying getting error 502 while generating forecast in through code; Content: hi, after doing the deployment task, i am not able to generate the forecast through code pipeline of . this is my run.py file: def run(df, url, api_key): allowselfsignedhttps(true) if api_key == 'uu': api_key = '' data = [] for index, row in list(df.iterrows()): data.append(dict(row)) data = { \"data\": data } body = str.encode(json.dumps(data)) headers = {'content-type': 'application\/json', 'authorization': ('bearer ' + api_key)} req = urllib.request.request(url, body, headers) print(url, api_key) try: response = urllib.request.urlopen(req) result = response.read() print(result) except urllib.error.httperror as error: print(\"the request failed with status code: \" + str(error.code)) # print the headers - they include the requert id and the timestamp, which are useful for debugging the failure print(error.info()) # print(json.loads(error.read().decode(\"utf8\", 'ignore'))) data = json.loads(json.load(response)) df= pd.json_normalize(data[result]) data = data.get('forecast', none) return data error: 502",
        "Question_original_content_gpt_summary":"The user encountered a 502 error while attempting to generate a forecast through code pipeline after deploying.",
        "Question_preprocessed_content":"Title: after deploying getting error while generating forecast in through code; Content: hi, after doing the deployment task, i am not able to generate the forecast through code pipeline of . this is my file def run allowselfsignedhttps if 'uu' '' data for index, row in data body headers req body, headers print try response result print except as error print print the headers they include the requert id and the timestamp, which are useful for debugging the failure 'ignore' data df data none return data error",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"SDK v1 or V2",
        "Question_body":"We are planing for next gen of product. Will V2 provide way more changes than V1?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1661977244390,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/989368\/sdk-v1-or-v2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-31T21:32:40.817Z",
                "Answer_score":0,
                "Answer_body":"Hello @nam-4027\n\nThanks for using Microsoft Q&A. I will recommend you keeping in V1 at this moment.\n\nSDK v2 is currently in public preview. The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see Supplemental Terms of Use for Microsoft Azure Previews.\n\nhttps:\/\/azure.microsoft.com\/support\/legal\/preview-supplemental-terms\/\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sdk v1 or v2; Content: we are planing for next gen of product. will v2 provide way more changes than v1?",
        "Question_original_content_gpt_summary":"The user is considering whether to use SDK v1 or v2 for their next product, and is wondering if v2 will provide more changes than v1.",
        "Question_preprocessed_content":"Title: sdk v or v; Content: we are planing for next gen of product. will v provide way more changes than v ?",
        "Answer_original_content":"hello @nam-4027 thanks for using microsoft q&a. i will recommend you keeping in v1 at this moment. sdk v2 is currently in public preview. the preview version is provided without a service level agreement, and it's not recommended for production workloads. certain features might not be supported or might have constrained capabilities. for more information, see supplemental terms of use for microsoft azure previews. https:\/\/azure.microsoft.com\/support\/legal\/preview-supplemental-terms\/ i hope this helps. regards, yutong -please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello thanks for using microsoft q&a. i will recommend you keeping in v at this moment. sdk v is currently in public preview. the preview version is provided without a service level agreement, and it's not recommended for production workloads. certain features might not be supported or might have constrained capabilities. for more , see supplemental terms of use for microsoft azure previews. i hope this helps. regards, yutong please kindly accept the answer if you feel helpful to support the community, thanks a lot."
    },
    {
        "Question_id":null,
        "Question_title":"Automated streaming integration and multiple requests for SageMaker endpoint",
        "Question_body":"A data scientist is looking to host a Tensorflow model in SageMaker and process low volume streaming event data (~2-3 per second) to collect inferences about each event. Data scientist is looking at having the SageMaker inference model plugged in as a Kinesis Data Analytics Application but Kinesis Data Analytics currently only supports SQL or Flink.\n\nOne option to set up an ECS or Lambda service to consume data from Kinesis or SNS and invoke the SageMaker inference endpoint per message, but if there is a more automated and optimal solution available for these kind of workflows.\n\nIt is not possible to pass multiple requests currently to a SageMaker endpoint, yet Tensorflow models tend to perform much better on batches of data rather than multiple single invocations so some windowing would be beneficial. Ideally the client would want to react to an inference within 10-15 seconds of the event being processed so an S3 based batch approach is probably too slow.\n\nIs there anything you can recommend for handling this sort of workload?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1599656695000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":88.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6E1eYARES123bRYAe2B0Ag\/automated-streaming-integration-and-multiple-requests-for-sage-maker-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-15T05:59:33.000Z",
                "Answer_score":0,
                "Answer_body":"To build integration between SageMaker endpoints and Kinesis Data Application use this blog - https:\/\/aws.amazon.com\/blogs\/architecture\/realtime-in-stream-inference-kinesis-sagemaker-flink\/. It help to setup serverless service to invoke the SageMaker inference endpoint.\n\nTo use batching. The Tensorflow documentation mentions the following:\n\nThis link mentions that you can include multiple instances in your predict request (or multiple examples in classify\/regress requests) to get multiple prediction results in one request to your Endpoint.\nThis link mentions that you can configure SageMaker TensorFlow Serving Container to batch multiple records together before performing an inference\n\nYou would still have to handle the logic internally in ECS\/Lambda to control how many records you consume from your stream in one batch, but at least you will be able to infer on the whole batch on the SageMaker endpoint end based on the above.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: automated streaming integration and multiple requests for endpoint; Content: a data scientist is looking to host a tensorflow model in and process low volume streaming event data (~2-3 per second) to collect inferences about each event. data scientist is looking at having the inference model plugged in as a kinesis data analytics application but kinesis data analytics currently only supports sql or flink. one option to set up an ecs or lambda service to consume data from kinesis or sns and invoke the inference endpoint per message, but if there is a more automated and optimal solution available for these kind of workflows. it is not possible to pass multiple requests currently to a endpoint, yet tensorflow models tend to perform much better on batches of data rather than multiple single invocations so some windowing would be beneficial. ideally the client would want to react to an inference within 10-15 seconds of the event being processed so an s3 based batch approach is probably too slow. is there anything you can recommend for handling this sort of workload?",
        "Question_original_content_gpt_summary":"The user is facing challenges with automated streaming integration and multiple requests for an endpoint, as they need to process low volume streaming event data (~2-3 per second) to collect inferences about each event, while also needing to react to an inference within 10-15 seconds of the event being processed.",
        "Question_preprocessed_content":"Title: automated streaming integration and multiple requests for endpoint; Content: a data scientist is looking to host a tensorflow model in and process low volume streaming event data to collect inferences about each event. data scientist is looking at having the inference model plugged in as a kinesis data analytics application but kinesis data analytics currently only supports sql or flink. one option to set up an ecs or lambda service to consume data from kinesis or sns and invoke the inference endpoint per message, but if there is a more automated and optimal solution available for these kind of workflows. it is not possible to pass multiple requests currently to a endpoint, yet tensorflow models tend to perform much better on batches of data rather than multiple single invocations so some windowing would be beneficial. ideally the client would want to react to an inference within seconds of the event being processed so an s based batch approach is probably too slow. is there anything you can recommend for handling this sort of workload?",
        "Answer_original_content":"to build integration between endpoints and kinesis data application use this blog - https:\/\/aws.amazon.com\/blogs\/architecture\/realtime-in-stream-inference-kinesis--flink\/. it help to setup serverless service to invoke the inference endpoint. to use batching. the tensorflow documentation mentions the following: this link mentions that you can include multiple instances in your predict request (or multiple examples in classify\/regress requests) to get multiple prediction results in one request to your endpoint. this link mentions that you can configure tensorflow serving container to batch multiple records together before performing an inference you would still have to handle the logic internally in ecs\/lambda to control how many records you consume from your stream in one batch, but at least you will be able to infer on the whole batch on the endpoint end based on the above.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"to build integration between endpoints and kinesis data application use this blog it help to setup serverless service to invoke the inference endpoint. to use batching. the tensorflow documentation mentions the following this link mentions that you can include multiple instances in your predict request to get multiple prediction results in one request to your endpoint. this link mentions that you can configure tensorflow serving container to batch multiple records together before performing an inference you would still have to handle the logic internally in to control how many records you consume from your stream in one batch, but at least you will be able to infer on the whole batch on the endpoint end based on the above."
    },
    {
        "Question_id":71612603.0,
        "Question_title":"How does one invert an encoded prediction in Keras for model serving?",
        "Question_body":"<p>I have a Keras model in which i have successfully added a <code>StringLookUp<\/code> pre-processing step as part of the model definition. This is generally a good practice because i can then feed it the raw data to get back a prediction.<\/p>\n<p>I am feeding the model string words that are mapped to an integer. The Y values are also string words that have been mapped to an integer.<\/p>\n<p>Here is the implementation of the encoder and decoders:<\/p>\n<pre><code>#generate the encoder and decoders\nencoder = tf.keras.layers.StringLookup(vocabulary=vocab, )\ndecoder = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode=&quot;int&quot;, invert=True)\n<\/code><\/pre>\n<p>Here is the some of the code that makes the inference model<\/p>\n<pre><code># For inference, you can export a model that accepts strings as input\ninputs = Input(shape=(6,), dtype=&quot;string&quot;)\nx = encoder(inputs)\noutputs = keras_model(x)\ninference_model = Model(inputs, outputs)\n\ninference_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \ninference_model.summary()\n<\/code><\/pre>\n<p>The <code>encoder<\/code> above is just a function that implements <code>tf.keras.layers.StringLookup<\/code><\/p>\n<p>Now, inside the notebook, I can easily convert the predictions back to the Original String representations by using a <code>decoder<\/code> which implements the reverse of <code>StringLookUp<\/code>.<\/p>\n<p><em><strong>Here's my problem<\/strong><\/em>\nWhile this works fine inside the notebook, this isn't very practical for deploying the model as a REST API because the calling program has no way of knowing how the encoded integer maps back to the original string representation.<\/p>\n<p><em><strong>So the question is what strategy should I use to implement the keras predict so that it returns the original string which I can then serialize using mlflow &amp; cloudpickle to deploy it as a servable model in databricks<\/strong><\/em><\/p>\n<p>Any guidance would be very much appreciated. I've seen a lot of example of Keras, but none that show how to do enact this kind of behavior for model deployment.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1648186526013,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":176.0,
        "Owner_creation_time":1427492676943,
        "Owner_last_access_time":1663998407848,
        "Owner_reputation":61.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":18.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71612603",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how does one invert an encoded prediction in keras for model serving?; Content: i have a keras model in which i have successfully added a stringlookup pre-processing step as part of the model definition. this is generally a good practice because i can then feed it the raw data to get back a prediction. i am feeding the model string words that are mapped to an integer. the y values are also string words that have been mapped to an integer. here is the implementation of the encoder and decoders: #generate the encoder and decoders encoder = tf.keras.layers.stringlookup(vocabulary=vocab, ) decoder = tf.keras.layers.stringlookup(vocabulary=vocab, output_mode=\"int\", invert=true) here is the some of the code that makes the inference model # for inference, you can export a model that accepts strings as input inputs = input(shape=(6,), dtype=\"string\") x = encoder(inputs) outputs = keras_model(x) inference_model = model(inputs, outputs) inference_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) inference_model.summary() the encoder above is just a function that implements tf.keras.layers.stringlookup now, inside the notebook, i can easily convert the predictions back to the original string representations by using a decoder which implements the reverse of stringlookup. here's my problem while this works fine inside the notebook, this isn't very practical for deploying the model as a rest api because the calling program has no way of knowing how the encoded integer maps back to the original string representation. so the question is what strategy should i use to implement the keras predict so that it returns the original string which i can then serialize using & cloudpickle to deploy it as a servable model in databricks any guidance would be very much appreciated. i've seen a lot of example of keras, but none that show how to do enact this kind of behavior for model deployment.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in deploying a Keras model as a REST API, as they need to invert the encoded prediction to the original string representation for serialization.",
        "Question_preprocessed_content":"Title: how does one invert an encoded prediction in keras for model serving?; Content: i have a keras model in which i have successfully added a pre processing step as part of the model definition. this is generally a good practice because i can then feed it the raw data to get back a prediction. i am feeding the model string words that are mapped to an integer. the y values are also string words that have been mapped to an integer. here is the implementation of the encoder and decoders here is the some of the code that makes the inference model the above is just a function that implements now, inside the notebook, i can easily convert the predictions back to the original string representations by using a which implements the reverse of . here's my problem while this works fine inside the notebook, this isn't very practical for deploying the model as a rest api because the calling program has no way of knowing how the encoded integer maps back to the original string representation. so the question is what strategy should i use to implement the keras predict so that it returns the original string which i can then serialize using & cloudpickle to deploy it as a servable model in databricks any guidance would be very much appreciated. i've seen a lot of example of keras, but none that show how to do enact this kind of behavior for model deployment.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Giving weights to event types in amazon personalize",
        "Question_body":"For the VIDEO_ON_DEMAND domain, some use cases include multiple event types. For example, the 'Top picks for you' use case includes two event types 'watch' and 'click'. Is 'watch' given more weight than 'click' when training the model? In general, when there is more than one event type, do domain recommenders give more weight to some event types?\n\nIn our use case, we have a platform that recommends video content. However, we have multiple event types, and some events need to be given more weight than others. Below is the list of our event types in the order of their importance:\n\nSHARE > LIKE > WATCH_COMPLETE > WATCH_PARTIAL > STARTED > SKIP\n\nSo when training the model, we would want 'SHARE' to have more weight than 'LIKE', and 'LIKE' to have more weight than 'WATCH_COMPLETE' and so on.\n\nI was looking into custom solutions. It looks like there is no way to give weights when using Personalize's custom solutions as mentioned in this post...\n\nSo when using Amazon Personalize, should we use domain recommenders or build custom solutions for our use case?\n\n**If we cannot give weights to different event types using Personalize, then what are alternatives? **Should we use Amazon SageMaker and build models from scratch? Open to any and all suggestions.\n\nThank you!",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1639825094332,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":174.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUSogRKFlfRzC5b8afIwPybQ\/giving-weights-to-event-types-in-amazon-personalize",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-21T19:25:06.908Z",
                "Answer_score":0,
                "Answer_body":"The VOD recommender for \"Top picks for you\" uses the same underlying HRNN-based algorithm noted in the StackOverflow answer you linked. Therefore the answer still applies with respect to weighting event types. That is, Personalize does not support weighting specific event types or specific interactions more than others. Instead, the Top picks for you recommender (as well as the underlying user-personalization recipe) builds sequence models from user sessions which are used to learn each user's interest based on a sequence of events rather than specific event types.\n\nGiven your event taxonomy, including interactions for SHARE, LIKE, and WATCH_COMPLETE in your interactions dataset are good choices since they indicate positive intent by the user. It may make sense to include WATCH_PARTIAL interactions as well (particularly if they represent the user watching the majority of the content, there is not a subsequent WATCH_COMPLETE for the user for the video, and\/or you do not have a sufficient number of WATCH_COMPLETE events across your user base). Otherwise, use WATCH_COMPLETE. If using one of the VOD recommenders, you will need to map your WATCH_COMPLETE events to the required Watch type and you could map the STARTED events to View. The SKIP events could be used as impressions if they can be correlated to a WATCH_COMPLETE or WATCH_PARTIAL event for a video that the user eventually watched (e.g., the user skips through the first 3 videos in a sequence and watches the 4th video could be expressed in a PutEvents call with the 4 videos as impressions and the 4th video as the ItemId that the user Watched).\n\nI suggest not basing the choice of whether to use Personalize or a custom SageMaker model on whether event type weighting is supported. Rather, the choice should be based on the approach that drives the most impact to your business metric (CTR, watch time, etc) with online testing.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: giving weights to event types in amazon personalize; Content: for the video_on_demand domain, some use cases include multiple event types. for example, the 'top picks for you' use case includes two event types 'watch' and 'click'. is 'watch' given more weight than 'click' when training the model? in general, when there is more than one event type, do domain recommenders give more weight to some event types? in our use case, we have a platform that recommends video content. however, we have multiple event types, and some events need to be given more weight than others. below is the list of our event types in the order of their importance: share > like > watch_complete > watch_partial > started > skip so when training the model, we would want 'share' to have more weight than 'like', and 'like' to have more weight than 'watch_complete' and so on. i was looking into custom solutions. it looks like there is no way to give weights when using personalize's custom solutions as mentioned in this post... so when using amazon personalize, should we use domain recommenders or build custom solutions for our use case? **if we cannot give weights to different event types using personalize, then what are alternatives? **should we use and build models from scratch? open to any and all suggestions. thank you!",
        "Question_original_content_gpt_summary":"The user is facing the challenge of giving weights to different event types when using Amazon Personalize, and is looking for alternatives to custom solutions.",
        "Question_preprocessed_content":"Title: giving weights to event types in amazon personalize; Content: for the domain, some use cases include multiple event types. for example, the 'top picks for you' use case includes two event types 'watch' and 'click'. is 'watch' given more weight than 'click' when training the model? in general, when there is more than one event type, do domain recommenders give more weight to some event types? in our use case, we have a platform that recommends video content. however, we have multiple event types, and some events need to be given more weight than others. below is the list of our event types in the order of their importance share > like > > > started > skip so when training the model, we would want 'share' to have more weight than 'like', and 'like' to have more weight than and so on. i was looking into custom solutions. it looks like there is no way to give weights when using personalize's custom solutions as mentioned in this so when using amazon personalize, should we use domain recommenders or build custom solutions for our use case? if we cannot give weights to different event types using personalize, then what are alternatives? should we use and build models from scratch? open to any and all suggestions. thank you!",
        "Answer_original_content":"the vod recommender for \"top picks for you\" uses the same underlying hrnn-based algorithm noted in the stackoverflow answer you linked. therefore the answer still applies with respect to weighting event types. that is, personalize does not support weighting specific event types or specific interactions more than others. instead, the top picks for you recommender (as well as the underlying user-personalization recipe) builds sequence models from user sessions which are used to learn each user's interest based on a sequence of events rather than specific event types. given your event taxonomy, including interactions for share, like, and watch_complete in your interactions dataset are good choices since they indicate positive intent by the user. it may make sense to include watch_partial interactions as well (particularly if they represent the user watching the majority of the content, there is not a subsequent watch_complete for the user for the video, and\/or you do not have a sufficient number of watch_complete events across your user base). otherwise, use watch_complete. if using one of the vod recommenders, you will need to map your watch_complete events to the required watch type and you could map the started events to view. the skip events could be used as impressions if they can be correlated to a watch_complete or watch_partial event for a video that the user eventually watched (e.g., the user skips through the first 3 videos in a sequence and watches the 4th video could be expressed in a putevents call with the 4 videos as impressions and the 4th video as the itemid that the user watched). i suggest not basing the choice of whether to use personalize or a custom model on whether event type weighting is supported. rather, the choice should be based on the approach that drives the most impact to your business metric (ctr, watch time, etc) with online testing.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"the vod recommender for top picks for you uses the same underlying hrnn based algorithm noted in the stackoverflow answer you linked. therefore the answer still applies with respect to weighting event types. that is, personalize does not support weighting specific event types or specific interactions more than others. instead, the top picks for you recommender builds sequence models from user sessions which are used to learn each user's interest based on a sequence of events rather than specific event types. given your event taxonomy, including interactions for share, like, and in your interactions dataset are good choices since they indicate positive intent by the user. it may make sense to include interactions as well . otherwise, use if using one of the vod recommenders, you will need to map your events to the required watch type and you could map the started events to view. the skip events could be used as impressions if they can be correlated to a or event for a video that the user eventually watched . i suggest not basing the choice of whether to use personalize or a custom model on whether event type weighting is supported. rather, the choice should be based on the approach that drives the most impact to your business metric with online testing."
    },
    {
        "Question_id":67020040.0,
        "Question_title":"Sagemaker Pytorch model - An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4):",
        "Question_body":"<p>I am facing an issue while invoking the Pytorch model Endpoint. Please check the below error for detail.<\/p>\n<p>Error Message:<\/p>\n<blockquote>\n<p>An error occurred (InternalFailure) when calling the InvokeEndpoint operation (reached max retries: 4): An exception occurred while sending request to model. Please contact customer support regarding request 9d4f143b-497f-47ce-9d45-88c697c4b0c4.<\/p>\n<\/blockquote>\n<p>Automatically restarted the Endpoint after this error. No specific log in cloud watch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1617966706043,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":407.0,
        "Owner_creation_time":1582178023440,
        "Owner_last_access_time":1648464697612,
        "Owner_reputation":3.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":4.0,
        "Answer_body":"<p>There may be a few issues here we can explore the paths and ways to resolve.<\/p>\n<ol>\n<li>Inference Code Error\nSometimes these errors occur when your payload or what you're feeding your endpoint is not in the appropriate format. When invoking the endpoint you want to make sure your data is in the correct format\/encoded properly. For this you can use the serializer SageMaker provides when creating the endpoint. The serializer takes care of encoding for you and sends data in the appropriate format. Look at the following code snippet.<\/li>\n<\/ol>\n<pre><code>from sagemaker.predictor import csv_serializer\nrf_pred = rf.deploy(1, &quot;ml.m4.xlarge&quot;, serializer=csv_serializer)\nprint(rf_pred.predict(payload).decode('utf-8'))\n<\/code><\/pre>\n<p>For more information about the different serializers based off the type of data you are feeding in check the following link.\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html<\/a><\/p>\n<ol start=\"2\">\n<li>Throttling Limits Reached\nSometimes the payload you are feeding in may be too large or the API request rate may have been exceeded for the endpoint so experiment with a more compute heavy instance or increase retries in your boto3 configuration. Here is a link for an example of what retries are and configuring them for your endpoint.<\/li>\n<\/ol>\n<p><a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-python-throttlingexception\/<\/a><\/p>\n<p>I work for AWS &amp; my opinions are my own<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1626976733376,
        "Answer_score":0.0,
        "Owner_location":"Palanpur, Gujarat, India",
        "Question_last_edit_time":1631708929472,
        "Answer_last_edit_time":1626979329227,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67020040",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pytorch model - an error occurred (internalfailure) when calling the invokeendpoint operation (reached max retries: 4):; Content: i am facing an issue while invoking the pytorch model endpoint. please check the below error for detail. error message: an error occurred (internalfailure) when calling the invokeendpoint operation (reached max retries: 4): an exception occurred while sending request to model. please contact customer support regarding request 9d4f143b-497f-47ce-9d45-88c697c4b0c4. automatically restarted the endpoint after this error. no specific log in cloud watch.",
        "Question_original_content_gpt_summary":"The user is encountering an issue while invoking a pytorch model endpoint, resulting in an internal failure error when reaching the maximum retries of 4.",
        "Question_preprocessed_content":"Title: pytorch model an error occurred when calling the invokeendpoint operation; Content: i am facing an issue while invoking the pytorch model endpoint. please check the below error for detail. error message an error occurred when calling the invokeendpoint operation an exception occurred while sending request to model. please contact customer support regarding request d f b f ce d c c b c . automatically restarted the endpoint after this error. no specific log in cloud watch.",
        "Answer_original_content":"there may be a few issues here we can explore the paths and ways to resolve. inference code error sometimes these errors occur when your payload or what you're feeding your endpoint is not in the appropriate format. when invoking the endpoint you want to make sure your data is in the correct format\/encoded properly. for this you can use the serializer provides when creating the endpoint. the serializer takes care of encoding for you and sends data in the appropriate format. look at the following code snippet. from .predictor import csv_serializer rf_pred = rf.deploy(1, \"ml.m4.xlarge\", serializer=csv_serializer) print(rf_pred.predict(payload).decode('utf-8')) for more information about the different serializers based off the type of data you are feeding in check the following link. https:\/\/.readthedocs.io\/en\/stable\/api\/inference\/serializers.html throttling limits reached sometimes the payload you are feeding in may be too large or the api request rate may have been exceeded for the endpoint so experiment with a more compute heavy instance or increase retries in your boto3 configuration. here is a link for an example of what retries are and configuring them for your endpoint. https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/-python-throttlingexception\/ i work for aws & my opinions are my own",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"there may be a few issues here we can explore the paths and ways to resolve. inference code error sometimes these errors occur when your payload or what you're feeding your endpoint is not in the appropriate format. when invoking the endpoint you want to make sure your data is in the correct properly. for this you can use the serializer provides when creating the endpoint. the serializer takes care of encoding for you and sends data in the appropriate format. look at the following code snippet. for more about the different serializers based off the type of data you are feeding in check the following link. throttling limits reached sometimes the payload you are feeding in may be too large or the api request rate may have been exceeded for the endpoint so experiment with a more compute heavy instance or increase retries in your boto configuration. here is a link for an example of what retries are and configuring them for your endpoint. i work for aws & my opinions are my own"
    },
    {
        "Question_id":null,
        "Question_title":"Having problems with LaTeX reports",
        "Question_body":"<p>When I try to download a LaTeX report the download spinning wheel starts, but never stops.<br>\nDoes it take so long or is this an issue with my setup?<\/p>\n<p>I\u2019m working on a MacBook accessing wandb from Apple Safari browser.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1639306013787,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":183.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/having-problems-with-latex-reports\/1514",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-14T12:07:48.666Z",
                "Answer_body":"<p>Hey J\u00fcrgen, can you send the link to the report?<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-14T14:04:59.000Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>This report is not downloadable as LaTeX version: <a href=\"https:\/\/wandb.ai\/thetaphipsi\/CNEP\/reports\/Training-CNEP-with-different-LR-Warmup--VmlldzoxMzMxNTg2?accessToken=d19fhd2vegr1y65xnjkaqqnc758fsav3edcsmc264gsmen4k1ngppz6v4ii4kbpq\">https:\/\/wandb.ai\/thetaphipsi\/CNEP\/reports\/Training-CNEP-with-different-LR-Warmup\u2013VmlldzoxMzMxNTg2?accessToken=d19fhd2vegr1y65xnjkaqqnc758fsav3edcsmc264gsmen4k1ngppz6v4ii4kbpq<\/a><\/p>\n<p>I created a different one, without any table of contents and it works, don\u2019t know what the issues is in that case.<\/p>\n<p>Regards<br>\nJ\u00fcrgen<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-23T14:36:26.666Z",
                "Answer_body":"<p>Hey J\u00fcrgen.<\/p>\n<p>There seems to be a bug on our end. We\u2019ll need to investigate this. I have created a ticket.<br>\nSorry about the inconvenience.<\/p>\n<p>Best,<br>\nArman<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-12T14:05:04.249Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: having problems with latex reports; Content: when i try to download a latex report the download spinning wheel starts, but never stops. does it take so long or is this an issue with my setup? i\u2019m working on a macbook accessing from apple safari browser.",
        "Question_original_content_gpt_summary":"The user is having difficulty downloading a LaTeX report, as the download spinning wheel starts but never stops when accessed from an Apple Safari browser on a Macbook.",
        "Question_preprocessed_content":"Title: having problems with latex reports; Content: when i try to download a latex report the download spinning wheel starts, but never stops. does it take so long or is this an issue with my setup? im working on a macbook accessing from apple safari browser.",
        "Answer_original_content":"hey jrgen, can you send the link to the report? hi, this report is not downloadable as latex version: https:\/\/.ai\/thetaphipsi\/cnep\/reports\/training-cnep-with-different-lr-warmupvmlldzoxmzmxntg2?accesstoken=d19fhd2vegr1y65xnjkaqqnc758fsav3edcsmc264gsmen4k1ngppz6v4ii4kbpq i created a different one, without any table of contents and it works, dont know what the issues is in that case. regards jrgen hey jrgen. there seems to be a bug on our end. well need to investigate this. i have created a ticket. sorry about the inconvenience. best, arman this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hey jrgen, can you send the link to the report? hi, this report is not downloadable as latex version i created a different one, without any table of contents and it works, dont know what the issues is in that case. regards jrgen hey jrgen. there seems to be a bug on our end. well need to investigate this. i have created a ticket. sorry about the inconvenience. best, arman this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Organization - NetObjex uses MLFlow",
        "Question_body":"Attached is our logo\nhttp:\/\/www.netobjex.com",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1614645394000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":18.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/oqewetS7cuY",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[

        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: organization - netobjex uses ; Content: attached is our logo http:\/\/www.netobjex.com",
        "Question_original_content_gpt_summary":"The user encountered a challenge of organizing their logo for their website.",
        "Question_preprocessed_content":"Title: organization netobjex uses; Content: attached is our logo",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":61090530.0,
        "Question_title":"Using tidyverse to read data from s3 bucket",
        "Question_body":"<p>I'm trying to read a <code>.csv<\/code> file stored in an s3 bucket, and I'm getting errors. I'm following the instructions <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/r_kernel\/using_r_with_amazon_sagemaker.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, but either it does not work or I am making a mistake and I'm not getting what I'm doing wrong.<\/p>\n\n<p>Here's what I'm trying to do:<\/p>\n\n<pre><code># I'm working on a SageMaker notebook instance\nlibrary(reticulate)\nlibrary(tidyverse)\n\nsagemaker &lt;- import('sagemaker')\nsagemaker.session &lt;- sagemaker$Session()\n\nregion &lt;- sagemaker.session$boto_region_name\nbucket &lt;- \"my-bucket\"\nprefix &lt;- \"data\/staging\"\nbucket.path &lt;- sprintf(\"https:\/\/s3-%s.amazonaws.com\/%s\", region, bucket)\nrole &lt;- sagemaker$get_execution_role()\n\nclient &lt;- sagemaker.session$boto_session$client('s3')\nkey &lt;- sprintf(\"%s\/%s\", prefix, 'my_file.csv')\n\nmy.obj &lt;- client$get_object(Bucket=bucket, Key=key)\n\nmy.df &lt;- read_csv(my.obj$Body) # This is where it all breaks down:\n## \n## Error: `file` must be a string, raw vector or a connection.\n## Traceback:\n## \n## 1. read_csv(my.obj$Body)\n## 2. read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, \n##  .     locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, n_max = n_max, guess_max = guess_max, \n##  .     progress = progress)\n## 3. col_spec_standardise(data, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, guess_max = guess_max, col_names = col_names, \n##  .     col_types = col_types, tokenizer = tokenizer, locale = locale)\n## 4. datasource(file, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment)\n## 5. stop(\"`file` must be a string, raw vector or a connection.\", \n##  .     call. = FALSE)\n<\/code><\/pre>\n\n<p>When working with Python, I can read a CSV file using someting like this:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import pandas as pd\n# ... Lots of boilerplate code\nmy_data = pd.read_csv(client.get_object(Bucket=bucket, Key=key)['Body'])\n<\/code><\/pre>\n\n<p>This is very similar to what I'm trying to do in R, and it works with Python... so why does it not work on R?<\/p>\n\n<p>Can you point me in the right path?<\/p>\n\n<p><strong>Note:<\/strong> Although I could use a Python kernel for this, I'd like to stick to R, because I'm more fluent with it than with Python, at least when it comes to dataframe crunching.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1586299117960,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1735.0,
        "Owner_creation_time":1275433277096,
        "Owner_last_access_time":1664032713307,
        "Owner_reputation":20017.0,
        "Owner_up_votes":1826.0,
        "Owner_down_votes":1932.0,
        "Owner_views":2754.0,
        "Answer_body":"<p>I'd recommend trying the <code>aws.s3<\/code> package instead:<\/p>\n\n<p><a href=\"https:\/\/github.com\/cloudyr\/aws.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/aws.s3<\/a><\/p>\n\n<p>Pretty simple - set your env variables:<\/p>\n\n<pre><code>Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"mykey\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"mysecretkey\",\n           \"AWS_DEFAULT_REGION\" = \"us-east-1\",\n           \"AWS_SESSION_TOKEN\" = \"mytoken\")\n<\/code><\/pre>\n\n<p>and then once that is out of the way:<\/p>\n\n<p><code>aws.s3::s3read_using(read.csv, object = \"s3:\/\/bucket\/folder\/data.csv\")<\/code><\/p>\n\n<p>Update: I see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here:\n<a href=\"https:\/\/github.com\/cloudyr\/roto.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/roto.s3<\/a><\/p>\n\n<p>Looks like it has a great api for example the variable layout you're aiming to use:<\/p>\n\n<pre><code>download_file(\n  bucket = \"is.rud.test\", \n  key = \"mtcars.csv\", \n  filename = \"\/tmp\/mtcars-again.csv\", \n  profile_name = \"personal\"\n)\n\nread_csv(\"\/tmp\/mtcars-again.csv\")\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1586562277483,
        "Answer_score":1.0,
        "Owner_location":"Mexico",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1586740595876,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61090530",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: using tidyverse to read data from s3 bucket; Content: i'm trying to read a .csv file stored in an s3 bucket, and i'm getting errors. i'm following the instructions here, but either it does not work or i am making a mistake and i'm not getting what i'm doing wrong. here's what i'm trying to do: # i'm working on a notebook instance library(reticulate) library(tidyverse) <- import('') .session <- $session() region <- .session$boto_region_name bucket <- \"my-bucket\" prefix <- \"data\/staging\" bucket.path <- sprintf(\"https:\/\/s3-%s.amazonaws.com\/%s\", region, bucket) role <- $get_execution_role() client <- .session$boto_session$client('s3') key <- sprintf(\"%s\/%s\", prefix, 'my_file.csv') my.obj <- client$get_object(bucket=bucket, key=key) my.df <- read_csv(my.obj$body) # this is where it all breaks down: ## ## error: `file` must be a string, raw vector or a connection. ## traceback: ## ## 1. read_csv(my.obj$body) ## 2. read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, ## . locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, ## . comment = comment, n_max = n_max, guess_max = guess_max, ## . progress = progress) ## 3. col_spec_standardise(data, skip = skip, skip_empty_rows = skip_empty_rows, ## . comment = comment, guess_max = guess_max, col_names = col_names, ## . col_types = col_types, tokenizer = tokenizer, locale = locale) ## 4. datasource(file, skip = skip, skip_empty_rows = skip_empty_rows, ## . comment = comment) ## 5. stop(\"`file` must be a string, raw vector or a connection.\", ## . call. = false) when working with python, i can read a csv file using someting like this: import pandas as pd # ... lots of boilerplate code my_data = pd.read_csv(client.get_object(bucket=bucket, key=key)['body']) this is very similar to what i'm trying to do in r, and it works with python... so why does it not work on r? can you point me in the right path? note: although i could use a python kernel for this, i'd like to stick to r, because i'm more fluent with it than with python, at least when it comes to dataframe crunching.",
        "Question_original_content_gpt_summary":"The user is encountering challenges with using the tidyverse library to read a .csv file stored in an s3 bucket, and is unable to figure out why the same code works in Python but not in R.",
        "Question_preprocessed_content":"Title: using tidyverse to read data from s bucket; Content: i'm trying to read a file stored in an s bucket, and i'm getting errors. i'm following the instructions here, but either it does not work or i am making a mistake and i'm not getting what i'm doing wrong. here's what i'm trying to do when working with python, i can read a csv file using someting like this this is very similar to what i'm trying to do in r, and it works with so why does it not work on r? can you point me in the right path? note although i could use a python kernel for this, i'd like to stick to r, because i'm more fluent with it than with python, at least when it comes to dataframe crunching.",
        "Answer_original_content":"i'd recommend trying the aws.s3 package instead: https:\/\/github.com\/cloudyr\/aws.s3 pretty simple - set your env variables: sys.setenv(\"aws_access_key_id\" = \"mykey\", \"aws_secret_access_key\" = \"mysecretkey\", \"aws_default_region\" = \"us-east-1\", \"aws_session_token\" = \"mytoken\") and then once that is out of the way: aws.s3::s3read_using(read.csv, object = \"s3:\/\/bucket\/folder\/data.csv\") update: i see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here: https:\/\/github.com\/cloudyr\/roto.s3 looks like it has a great api for example the variable layout you're aiming to use: download_file( bucket = \"is.rud.test\", key = \"mtcars.csv\", filename = \"\/tmp\/mtcars-again.csv\", profile_name = \"personal\" ) read_csv(\"\/tmp\/mtcars-again.csv\")",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i'd recommend trying the package instead pretty simple set your env variables and then once that is out of the way update i see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here looks like it has a great api for example the variable layout you're aiming to use"
    },
    {
        "Question_id":null,
        "Question_title":"Wanb.watch(model) causing CUDA OOM",
        "Question_body":"<p>I am trying to use wandb gradient visualization to debug the gradient flow in my neural net on Google Colab. Without wandb logging, the training runs without error, taking up 11Gb\/16GB on the p100 gpu. However, adding this line <code>wandb.watch(model, log='all', log_freq=3)<\/code> causes a cuda out of memory error. How does wandb logging create extra gpu memory overhead? Is there some way to reduce the overhead? Thank you for your help.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1631287205142,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":616.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wanb-watch-model-causing-cuda-oom\/499",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-10T20:21:45.773Z",
                "Answer_body":"<p>Hello and welcome to the forums <a class=\"mention\" href=\"\/u\/ambrose\">@ambrose<\/a>! <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wave.png?v=10\" title=\":wave:\" class=\"emoji\" alt=\":wave:\"><\/p>\n<p>Please do introduce yourself in the <a class=\"hashtag\" href=\"\/c\/start-here\/2\">#<span>start-here<\/span><\/a> category if you\u2019d like to!<\/p>\n<p>Please allow me to replicate this issue, and ask the team for help.<br>\nI\u2019ll get back once I\u2019m able to replicate the issue, Thanks for the Q! <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":12.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-11T00:52:29.061Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/bhutanisanyam1\">@bhutanisanyam1<\/a>,<\/p>\n<p>Thank you for your reply and welcome! I am quite excited to use WandB and join the community.<\/p>\n<p>Ambrose<\/p>",
                "Answer_score":7.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-11T19:32:36.707Z",
                "Answer_body":"<p>Hmm I think WandB is creating extra copies of the gradients during the logging. In case it helps, here is the error traceback:<\/p>\n<pre><code class=\"lang-auto\">---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-11-13de83557b55&gt; in &lt;module&gt;()\n     60         get_ipython().system(\"nvidia-smi | grep MiB | awk '{print $9 $10 $11}'\")\n     61 \n---&gt; 62         loss.backward()\n     63 \n     64         print('check 10')\n\n4 frames\n\/usr\/local\/lib\/python3.7\/dist-packages\/torch\/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)\n    253                 create_graph=create_graph,\n    254                 inputs=inputs)\n--&gt; 255         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n    256 \n    257     def register_hook(self, hook):\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    147     Variable._execution_engine.run_backward(\n    148         tensors, grad_tensors_, retain_graph, create_graph, inputs,\n--&gt; 149         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n    150 \n    151 \n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in &lt;lambda&gt;(grad)\n    283             self.log_tensor_stats(grad.data, name)\n    284 \n--&gt; 285         handle = var.register_hook(lambda grad: _callback(grad, log_track))\n    286         self._hook_handles[name] = handle\n    287         return handle\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in _callback(grad, log_track)\n    281             if not log_track_update(log_track):\n    282                 return\n--&gt; 283             self.log_tensor_stats(grad.data, name)\n    284 \n    285         handle = var.register_hook(lambda grad: _callback(grad, log_track))\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in log_tensor_stats(self, tensor, name)\n    219         # Remove nans from tensor. There's no good way to represent that in histograms.\n    220         flat = flat[~torch.isnan(flat)]\n--&gt; 221         flat = flat[~torch.isinf(flat)]\n    222         if flat.shape == torch.Size([0]):\n    223             # Often the whole tensor is nan or inf. Just don't log it in that case.\n\nRuntimeError: CUDA out of memory. Tried to allocate 4.65 GiB (GPU 0; 15.90 GiB total capacity; 10.10 GiB already allocated; 717.75 MiB free; 14.27 GiB reserved in total by PyTorch)\n<\/code><\/pre>",
                "Answer_score":27.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-11T20:00:25.419Z",
                "Answer_body":"<p>Indeed, commenting out the offending line <code>flat = flat[~torch.isinf(flat)]<\/code> gets the WandB log step to just barely fit into the GPU memory. This is not a great solution though.<\/p>",
                "Answer_score":17.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:07.124Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":10.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: wanb.watch(model) causing cuda oom; Content: i am trying to use gradient visualization to debug the gradient flow in my neural net on google colab. without logging, the training runs without error, taking up 11gb\/16gb on the p100 gpu. however, adding this line .watch(model, log='all', log_freq=3) causes a cuda out of memory error. how does logging create extra gpu memory overhead? is there some way to reduce the overhead? thank you for your help.",
        "Question_original_content_gpt_summary":"The user is encountering a Cuda Out of Memory error when attempting to debug the gradient flow in their neural net on Google Colab, and is wondering how logging creates extra GPU memory overhead and if there is a way to reduce it.",
        "Question_preprocessed_content":"Title: causing cuda oom; Content: i am trying to use gradient visualization to debug the gradient flow in my neural net on google colab. without logging, the training runs without error, taking up on the p gpu. however, adding this line causes a cuda out of memory error. how does logging create extra gpu memory overhead? is there some way to reduce the overhead? thank you for your help.",
        "Answer_original_content":"hello and welcome to the forums @ambrose! please do introduce yourself in the #start-here category if youd like to! please allow me to replicate this issue, and ask the team for help. ill get back once im able to replicate the issue, thanks for the q! hi @bhutanisanyam1, thank you for your reply and welcome! i am quite excited to use and join the community. ambrose hmm i think is creating extra copies of the gradients during the logging. in case it helps, here is the error traceback: --------------------------------------------------------------------------- runtimeerror traceback (most recent call last) <ipython-input-11-13de83557b55> in <module>() 60 get_ipython().system(\"nvidia-smi | grep mib | awk '{print $9 $10 $11}'\") 61 ---> 62 loss.backward() 63 64 print('check 10') 4 frames \/usr\/local\/lib\/python3.7\/dist-packages\/torch\/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs) 253 create_graph=create_graph, 254 inputs=inputs) --> 255 torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs) 256 257 def register_hook(self, hook): \/usr\/local\/lib\/python3.7\/dist-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs) 147 variable._execution_engine.run_backward( 148 tensors, grad_tensors_, retain_graph, create_graph, inputs, --> 149 allow_unreachable=true, accumulate_grad=true) # allow_unreachable flag 150 151 \/usr\/local\/lib\/python3.7\/dist-packages\/\/_torch.py in <lambda>(grad) 283 self.log_tensor_stats(grad.data, name) 284 --> 285 handle = var.register_hook(lambda grad: _callback(grad, log_track)) 286 self._hook_handles[name] = handle 287 return handle \/usr\/local\/lib\/python3.7\/dist-packages\/\/_torch.py in _callback(grad, log_track) 281 if not log_track_update(log_track): 282 return --> 283 self.log_tensor_stats(grad.data, name) 284 285 handle = var.register_hook(lambda grad: _callback(grad, log_track)) \/usr\/local\/lib\/python3.7\/dist-packages\/\/_torch.py in log_tensor_stats(self, tensor, name) 219 # remove nans from tensor. there's no good way to represent that in histograms. 220 flat = flat[~torch.isnan(flat)] --> 221 flat = flat[~torch.isinf(flat)] 222 if flat.shape == torch.size([0]): 223 # often the whole tensor is nan or inf. just don't log it in that case. runtimeerror: cuda out of memory. tried to allocate 4.65 gib (gpu 0; 15.90 gib total capacity; 10.10 gib already allocated; 717.75 mib free; 14.27 gib reserved in total by pytorch) indeed, commenting out the offending line flat = flat[~torch.isinf(flat)] gets the log step to just barely fit into the gpu memory. this is not a great solution though. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello and welcome to the forums please do introduce yourself in the start here category if youd like to! please allow me to replicate this issue, and ask the team for help. ill get back once im able to replicate the issue, thanks for the q! hi thank you for your reply and welcome! i am quite excited to use and join the community. ambrose hmm i think is creating extra copies of the gradients during the logging. in case it helps, here is the error traceback indeed, commenting out the offending line gets the log step to just barely fit into the gpu memory. this is not a great solution though. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":56843216.0,
        "Question_title":"How do I schedule a ipnyb notebook in Sagemaker using AWS lambda?",
        "Question_body":"<p>I need to schedule AWS Lambda to open\/run a Jupyter Notebook I have inside Sagemaker to produce a csv file once a day.<\/p>\n\n<p>I have already created my notebook instance (let's call it Model_v1) and the Lifecycle configuration needed inside Sagemaker. I can Start the instance, run the code (R) inside the Notebook, and the code writes the CSV file I require.<\/p>\n\n<p>I have read many posts about how to use Sagemaker with Lambda, but I'm not formally using a \"training job\" or a \"model\" \/ endpoint etc etc. I literally just want Lambda to\n1) Start Notebook instance\n2) Run .ipnyb code which generates CSV<\/p>\n\n<p>If there is an easier way to make Sagemaker run this script once a day with another tool (instead of lambda), more than happy to change!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1562020342263,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":3041.0,
        "Owner_creation_time":1562014251963,
        "Owner_last_access_time":1598480552608,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56843216",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i schedule a ipnyb notebook in using aws lambda?; Content: i need to schedule aws lambda to open\/run a jupyter notebook i have inside to produce a csv file once a day. i have already created my notebook instance (let's call it model_v1) and the lifecycle configuration needed inside . i can start the instance, run the code (r) inside the notebook, and the code writes the csv file i require. i have read many posts about how to use with lambda, but i'm not formally using a \"training job\" or a \"model\" \/ endpoint etc etc. i literally just want lambda to 1) start notebook instance 2) run .ipnyb code which generates csv if there is an easier way to make run this script once a day with another tool (instead of lambda), more than happy to change!",
        "Question_original_content_gpt_summary":"The user is facing a challenge of scheduling an IPython notebook in AWS Lambda to produce a CSV file once a day.",
        "Question_preprocessed_content":"Title: how do i schedule a ipnyb notebook in using aws lambda?; Content: i need to schedule aws lambda to a jupyter notebook i have inside to produce a csv file once a day. i have already created my notebook instance and the lifecycle configuration needed inside . i can start the instance, run the code inside the notebook, and the code writes the csv file i require. i have read many posts about how to use with lambda, but i'm not formally using a training job or a model \/ endpoint etc etc. i literally just want lambda to start notebook instance run .ipnyb code which generates csv if there is an easier way to make run this script once a day with another tool , more than happy to change!",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":68180770.0,
        "Question_title":"Amazon S3 files access in Sagemaker instance",
        "Question_body":"<p>How exactly is file in my S3 bucket is accessible in my Sagemaker instance?\nGiven that I am not adding any access provision...what exactly is happening in the backend?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1624977119120,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":1234.0,
        "Owner_creation_time":1623856546760,
        "Owner_last_access_time":1625453055656,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68180770",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: amazon s3 files access in instance; Content: how exactly is file in my s3 bucket is accessible in my instance? given that i am not adding any access provision...what exactly is happening in the backend?",
        "Question_original_content_gpt_summary":"The user is trying to understand how files in their S3 bucket can be accessed in their instance without adding any access provision.",
        "Question_preprocessed_content":"Title: amazon s files access in instance; Content: how exactly is file in my s bucket is accessible in my instance? given that i am not adding any access exactly is happening in the backend?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70268372.0,
        "Question_title":"How to adjust feature importance in Azure AutoML",
        "Question_body":"<p>I am hoping to have some <strong>low code model<\/strong> using Azure AutoML, which is really just going to the AutoML tab, running a classification experiment with my dataset, after it's done, I deploy the best selected model.<\/p>\n<p>The model kinda works (meaning, I publish the endpoint and then I do some manual validation, seems accurate), however, I am not confident enough, because when I am looking at the explanation, I can see something like this:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qM51x.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qM51x.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>4 top features are not really closely important. The most &quot;important&quot; one is really not the one I prefer it to use. I am hoping it will use the <code>Title<\/code> feature more.<\/p>\n<p>Is there such a thing I can adjust the importance of individual features, like ranking all features before it starts the experiment?<\/p>\n<p>I would love to do more reading, but I only found this:<\/p>\n<p><a href=\"https:\/\/stackoverflow.com\/questions\/52484233\/increase-feature-importance\">Increase feature importance<\/a><\/p>\n<p>The only answer seems to be about how to measure if a feature is important.<\/p>\n<p>Hence, does it mean, if I want to customize the experiment, such as selecting which features to &quot;focus&quot;, I should learn how to use the &quot;designer&quot; part in Azure ML? Or is it something I can't do, even with the designer. I guess my confusion is, with ML being such a big topic, I am looking for a direction of learning, in this case of what I am having, so I can improve my current model.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1638921326880,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":119.0,
        "Owner_creation_time":1296571549840,
        "Owner_last_access_time":1663809103808,
        "Owner_reputation":1021.0,
        "Owner_up_votes":79.0,
        "Owner_down_votes":7.0,
        "Owner_views":138.0,
        "Answer_body":"<p>Here is <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-automated-ml#customize-featurization\" rel=\"nofollow noreferrer\">link<\/a> to the document for feature customization.<\/p>\n<p>Using the SDK you can specify &quot;feauturization&quot;: 'auto' \/ 'off' \/ 'FeaturizationConfig' in your <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py\" rel=\"nofollow noreferrer\">AutoMLConfig<\/a> object. Learn more about <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-features\" rel=\"nofollow noreferrer\">enabling featurization<\/a>.<\/p>\n<p>Automated ML tries out different ML models that have different settings which control for overfitting.  Automated ML will pick which overfitting parameter configuration is best based on the best score (e.g. accuracy) it gets from hold-out data.  The kind of overfitting settings these models has includes:<\/p>\n<ul>\n<li>Explicitly penalizing overly-complex models in the loss function that the ML model is optimizing<\/li>\n<li>Limiting model complexity before training, for example by limiting the size of trees in an ensemble tree learning model (e.g. gradient boosting trees or random forest)<\/li>\n<\/ul>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1641210936728,
        "Answer_score":1.0,
        "Owner_location":"Seattle, WA",
        "Question_last_edit_time":1638925038328,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70268372",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to adjust feature importance in azure automl; Content: i am hoping to have some low code model using azure automl, which is really just going to the automl tab, running a classification experiment with my dataset, after it's done, i deploy the best selected model. the model kinda works (meaning, i publish the endpoint and then i do some manual validation, seems accurate), however, i am not confident enough, because when i am looking at the explanation, i can see something like this: 4 top features are not really closely important. the most \"important\" one is really not the one i prefer it to use. i am hoping it will use the title feature more. is there such a thing i can adjust the importance of individual features, like ranking all features before it starts the experiment? i would love to do more reading, but i only found this: increase feature importance the only answer seems to be about how to measure if a feature is important. hence, does it mean, if i want to customize the experiment, such as selecting which features to \"focus\", i should learn how to use the \"designer\" part in ? or is it something i can't do, even with the designer. i guess my confusion is, with ml being such a big topic, i am looking for a direction of learning, in this case of what i am having, so i can improve my current model.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in adjusting the feature importance in Azure Automl, and is looking for a direction of learning to improve their current model.",
        "Question_preprocessed_content":"Title: how to adjust feature importance in azure automl; Content: i am hoping to have some low code model using azure automl, which is really just going to the automl tab, running a classification experiment with my dataset, after it's done, i deploy the best selected model. the model kinda works , however, i am not confident enough, because when i am looking at the explanation, i can see something like this top features are not really closely important. the most important one is really not the one i prefer it to use. i am hoping it will use the feature more. is there such a thing i can adjust the importance of individual features, like ranking all features before it starts the experiment? i would love to do more reading, but i only found this increase feature importance the only answer seems to be about how to measure if a feature is important. hence, does it mean, if i want to customize the experiment, such as selecting which features to focus , i should learn how to use the designer part in ? or is it something i can't do, even with the designer. i guess my confusion is, with ml being such a big topic, i am looking for a direction of learning, in this case of what i am having, so i can improve my current model.",
        "Answer_original_content":"here is link to the document for feature customization. using the sdk you can specify \"feauturization\": 'auto' \/ 'off' \/ 'featurizationconfig' in your automlconfig object. learn more about enabling featurization. automated ml tries out different ml models that have different settings which control for overfitting. automated ml will pick which overfitting parameter configuration is best based on the best score (e.g. accuracy) it gets from hold-out data. the kind of overfitting settings these models has includes: explicitly penalizing overly-complex models in the loss function that the ml model is optimizing limiting model complexity before training, for example by limiting the size of trees in an ensemble tree learning model (e.g. gradient boosting trees or random forest) https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"here is link to the document for feature customization. using the sdk you can specify feauturization 'auto' \/ 'off' \/ 'featurizationconfig' in your automlconfig object. learn more about enabling featurization. automated ml tries out different ml models that have different settings which control for overfitting. automated ml will pick which overfitting parameter configuration is best based on the best score it gets from hold out data. the kind of overfitting settings these models has includes explicitly penalizing overly complex models in the loss function that the ml model is optimizing limiting model complexity before training, for example by limiting the size of trees in an ensemble tree learning model"
    },
    {
        "Question_id":null,
        "Question_title":"Modify code during sweep",
        "Question_body":"<p>If I run a sweep, and while the sweep is running I modify code that gets called during the sweep, will it affect the sweep or does it save the state of the code when the sweep starts?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1653313397712,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/modify-code-during-sweep\/2484",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-23T22:20:03.995Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/apjansen\">@apjansen<\/a> ,<\/p>\n<p>Sweep configs are immutable once started. Starting a new sweep would be the best method to making any modifications. Please let me know if you have any additional questions.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-27T00:07:30.379Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/apjansen\">@apjansen<\/a> ,<\/p>\n<p>As we have not heard back from you, I will be closing out this support request. Please do reach back out again if you require further assistance or have any questions.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-26T00:08:17.610Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: modify code during sweep; Content: if i run a sweep, and while the sweep is running i modify code that gets called during the sweep, will it affect the sweep or does it save the state of the code when the sweep starts?",
        "Question_original_content_gpt_summary":"The user is wondering if modifying code during a sweep will affect the sweep or if the state of the code will be saved when the sweep starts.",
        "Question_preprocessed_content":"Title: modify code during sweep; Content: if i run a sweep, and while the sweep is running i modify code that gets called during the sweep, will it affect the sweep or does it save the state of the code when the sweep starts?",
        "Answer_original_content":"hi @apjansen , sweep configs are immutable once started. starting a new sweep would be the best method to making any modifications. please let me know if you have any additional questions. regards, mohammad hi @apjansen , as we have not heard back from you, i will be closing out this support request. please do reach back out again if you require further assistance or have any questions. this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi , sweep configs are immutable once started. starting a new sweep would be the best method to making any modifications. please let me know if you have any additional questions. regards, mohammad hi , as we have not heard back from you, i will be closing out this support request. please do reach back out again if you require further assistance or have any questions. this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":61270842.0,
        "Question_title":"Azure ML SDK to DataIKU (python Recepie) integration",
        "Question_body":"<p>WE are installing Azure Machine Learning SDK in Python Code Recipe in DataIKU.<\/p>\n\n<p>The Model coding is more manual when using SDK, Is there a way where we can create models in ML Studio (Drag and Drop) and use it's service in Python to get the output. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1587123889007,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":62.0,
        "Owner_creation_time":1578649477907,
        "Owner_last_access_time":1590997946847,
        "Owner_reputation":41.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":11.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61270842",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sdk to dataiku (python recepie) integration; Content: we are installing sdk in python code recipe in dataiku. the model coding is more manual when using sdk, is there a way where we can create models in ml studio (drag and drop) and use it's service in python to get the output.",
        "Question_original_content_gpt_summary":"The user is attempting to integrate an SDK into a Python recipe in Dataiku, and is looking for a way to create models in ML Studio and use its services in Python to get the output.",
        "Question_preprocessed_content":"Title: sdk to dataiku integration; Content: we are installing sdk in python code recipe in dataiku. the model coding is more manual when using sdk, is there a way where we can create models in ml studio and use it's service in python to get the output.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":71410791.0,
        "Question_title":"How can I build a multi model endpoint for ensemble modeling with using my own model containers?",
        "Question_body":"<p>I'm trying to deploy a multi model endpoint on Amazon Sagemaker, and am working with my own model containers which I created using <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/main\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example. I can train and create endpoint for each of them separately but for example when I try to collect mlp and cart together in multi model endpoint, I get an error which says &quot;The cart,mlp for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint&quot;. When I check CloudWatch logs I cannot see anything unusual. <strong>Should I change the container structure for multi model endpoints ?<\/strong><\/p>\n<pre><code>from time import gmtime, strftime\nimport os\nimport boto3\nimport time\nimport re\nimport sagemaker\n\nmodel_name = &quot;efe-test-model-ensemble-modeling-&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n\ncart_hosting_container = {\n    &quot;Image&quot;: &quot;097916623002.dkr.ecr.eu-central-1.amazonaws.com\/snop-mm-cart:latest&quot;,\n    &quot;ContainerHostname&quot;: &quot;cart&quot;,\n    &quot;ModelDataUrl&quot;: &quot;s3:\/\/sagemaker-eu-central-1-097916623002\/output\/snop-mm-cart-2022-03-09-12-47-04-881\/output\/model.tar.gz&quot;,\n}\n\nmlp_hosting_container = {\n    &quot;Image&quot;: &quot;097916623002.dkr.ecr.eu-central-1.amazonaws.com\/snop-mm-mlp:latest&quot;,\n    &quot;ContainerHostname&quot;: &quot;mlp&quot;,\n    &quot;ModelDataUrl&quot;: &quot;s3:\/\/sagemaker-eu-central-1-097916623002\/output\/snop-mm-mlp-2022-03-09-12-52-09-267\/output\/model.tar.gz&quot;,\n}\n\nrole = sagemaker.get_execution_role()\nsm = boto3.client(&quot;sagemaker&quot;)\n\ninferenceExecutionConfig = {&quot;Mode&quot;: &quot;Direct&quot;}\n\ncreate_model_response = sm.create_model(\n    ModelName=model_name,\n    InferenceExecutionConfig=inferenceExecutionConfig,\n    ExecutionRoleArn=role,\n    Containers=[cart_hosting_container, mlp_hosting_container],\n)\n\nendpoint_config_name = &quot;TEST-config-ensemble-modelling-&quot; + strftime(\n    &quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime()\n)\nprint(endpoint_config_name)\ncreate_endpoint_config_response = sm.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;InitialVariantWeight&quot;: 1,\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;AllTraffic&quot;,\n        }\n    ],\n)\n\nprint(&quot;Endpoint Config Arn: &quot; + create_endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n%%time\nimport time\n\nendpoint_name = &quot;TEST-endpoint-ensemble-modelling-&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(endpoint_name)\ncreate_endpoint_response = sm.create_endpoint(\n    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n)\nprint(create_endpoint_response[&quot;EndpointArn&quot;])\n\nresp = sm.describe_endpoint(EndpointName=endpoint_name)\nstatus = resp[&quot;EndpointStatus&quot;]\nprint(&quot;Status: &quot; + status)\n\nwhile status == &quot;Creating&quot;:\n    time.sleep(60)\n    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n    status = resp[&quot;EndpointStatus&quot;]\n    print(&quot;Status: &quot; + status)\n\nprint(&quot;Arn: &quot; + resp[&quot;EndpointArn&quot;])\nprint(&quot;Status: &quot; + status)\n<\/code><\/pre>\n<p>It creates two folders in CloudWatch<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wCNOC.png\" rel=\"nofollow noreferrer\">log groups<\/a><\/p>\n<p>mlp is:\n<a href=\"https:\/\/i.stack.imgur.com\/O7qeA.png\" rel=\"nofollow noreferrer\">mlp log<\/a><\/p>\n<p>cart is:\n<a href=\"https:\/\/i.stack.imgur.com\/XhiDC.png\" rel=\"nofollow noreferrer\">cart log<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1646835413127,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":87.0,
        "Owner_creation_time":1645732047140,
        "Owner_last_access_time":1654867064967,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71410791",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i build a multi model endpoint for ensemble modeling with using my own model containers?; Content: i'm trying to deploy a multi model endpoint on , and am working with my own model containers which i created using scikit_bring_your_own example. i can train and create endpoint for each of them separately but for example when i try to collect mlp and cart together in multi model endpoint, i get an error which says \"the cart,mlp for production variant alltraffic did not pass the ping health check. please check cloudwatch logs for this endpoint\". when i check cloudwatch logs i cannot see anything unusual. should i change the container structure for multi model endpoints ? from time import gmtime, strftime import os import boto3 import time import re import model_name = \"efe-test-model-ensemble-modeling-\" + strftime(\"%y-%m-%d-%h-%m-%s\", gmtime()) cart_hosting_container = { \"image\": \"097916623002.dkr.ecr.eu-central-1.amazonaws.com\/snop-mm-cart:latest\", \"containerhostname\": \"cart\", \"modeldataurl\": \"s3:\/\/-eu-central-1-097916623002\/output\/snop-mm-cart-2022-03-09-12-47-04-881\/output\/model.tar.gz\", } mlp_hosting_container = { \"image\": \"097916623002.dkr.ecr.eu-central-1.amazonaws.com\/snop-mm-mlp:latest\", \"containerhostname\": \"mlp\", \"modeldataurl\": \"s3:\/\/-eu-central-1-097916623002\/output\/snop-mm-mlp-2022-03-09-12-52-09-267\/output\/model.tar.gz\", } role = .get_execution_role() sm = boto3.client(\"\") inferenceexecutionconfig = {\"mode\": \"direct\"} create_model_response = sm.create_model( modelname=model_name, inferenceexecutionconfig=inferenceexecutionconfig, executionrolearn=role, containers=[cart_hosting_container, mlp_hosting_container], ) endpoint_config_name = \"test-config-ensemble-modelling-\" + strftime( \"%y-%m-%d-%h-%m-%s\", gmtime() ) print(endpoint_config_name) create_endpoint_config_response = sm.create_endpoint_config( endpointconfigname=endpoint_config_name, productionvariants=[ { \"instancetype\": \"ml.m5.large\", \"initialinstancecount\": 1, \"initialvariantweight\": 1, \"modelname\": model_name, \"variantname\": \"alltraffic\", } ], ) print(\"endpoint config arn: \" + create_endpoint_config_response[\"endpointconfigarn\"]) %%time import time endpoint_name = \"test-endpoint-ensemble-modelling-\" + strftime(\"%y-%m-%d-%h-%m-%s\", gmtime()) print(endpoint_name) create_endpoint_response = sm.create_endpoint( endpointname=endpoint_name, endpointconfigname=endpoint_config_name ) print(create_endpoint_response[\"endpointarn\"]) resp = sm.describe_endpoint(endpointname=endpoint_name) status = resp[\"endpointstatus\"] print(\"status: \" + status) while status == \"creating\": time.sleep(60) resp = sm.describe_endpoint(endpointname=endpoint_name) status = resp[\"endpointstatus\"] print(\"status: \" + status) print(\"arn: \" + resp[\"endpointarn\"]) print(\"status: \" + status) it creates two folders in cloudwatch log groups mlp is: mlp log cart is: cart log",
        "Question_original_content_gpt_summary":"The user is encountering challenges when attempting to deploy a multi model endpoint using their own model containers created with the scikit_bring_your_own example.",
        "Question_preprocessed_content":"Title: how can i build a multi model endpoint for ensemble modeling with using my own model containers?; Content: i'm trying to deploy a multi model endpoint on , and am working with my own model containers which i created using example. i can train and create endpoint for each of them separately but for example when i try to collect mlp and cart together in multi model endpoint, i get an error which says the cart,mlp for production variant alltraffic did not pass the ping health check. please check cloudwatch logs for this endpoint . when i check cloudwatch logs i cannot see anything unusual. should i change the container structure for multi model endpoints ? it creates two folders in cloudwatch log groups mlp is mlp log cart is cart log",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":63508876.0,
        "Question_title":"Do Sagemaker ML instance types use Nitro system based on instance type pattern?",
        "Question_body":"<p>Only certain EC2 instance types are build on the <a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/instance-types.html#ec2-nitro-instances\" rel=\"nofollow noreferrer\">Nitro System<\/a><\/p>\n<p>Sagemaker <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"nofollow noreferrer\">instance types<\/a> follow the pattern of <code>ml.{ec2 instance type}<\/code><\/p>\n<p>If the <code>{ec2 instance type}<\/code> for an Amazon Sagemaker training instance was built on the <a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/instance-types.html#ec2-nitro-instances\" rel=\"nofollow noreferrer\">Nitro System<\/a>, can I deduce that the model training instance was <em>also<\/em> built on the Nitro system?<\/p>\n<p>For example, M5 EC2 instances were built on the Nitro System. Does this mean that a Sagemaker ml.m5.xlarge instance is <em>also<\/em> built on the Nitro System?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1597939017983,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":72.0,
        "Owner_creation_time":1348841548500,
        "Owner_last_access_time":1663964727910,
        "Owner_reputation":5182.0,
        "Owner_up_votes":1902.0,
        "Owner_down_votes":7.0,
        "Owner_views":315.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63508876",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: do ml instance types use nitro system based on instance type pattern?; Content: only certain ec2 instance types are build on the nitro system instance types follow the pattern of ml.{ec2 instance type} if the {ec2 instance type} for an training instance was built on the nitro system, can i deduce that the model training instance was also built on the nitro system? for example, m5 ec2 instances were built on the nitro system. does this mean that a ml.m5.xlarge instance is also built on the nitro system?",
        "Question_original_content_gpt_summary":"The user is trying to determine if ML instance types use the Nitro System based on the instance type pattern.",
        "Question_preprocessed_content":"Title: do ml instance types use nitro system based on instance type pattern?; Content: only certain ec instance types are build on the nitro system instance types follow the pattern of if the for an training instance was built on the nitro system, can i deduce that the model training instance was also built on the nitro system? for example, m ec instances were built on the nitro system. does this mean that a instance is also built on the nitro system?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Performance issue of UI with O(100) runs?",
        "Question_body":"Hi all,\n\nwe have been using MLFlow (only the tracking API) for some time now and\nhave recently upgraded to MLFlow 1.0.0 with a MySQL tracking store. Over\ntime, we have accumulated a (still moderate) number of runs in our\nlarger experiments, and the first signs of a performance issue start to\nshow up:\n\nWe have experiments with typically ~100 runs in them. This is enough so\nthat it takes the web UI several seconds (> 5s) to display the list of\nruns initially (before I can even add search queries). I have not done\nmore detailed profiling so far, but it seems strange to me to see such a\nresponse time in a still rather small setup.\n\nThus my question: Are there people around with large production setups\n(I would expect that larger databases with thousands of runs in a single\nexperiment could be quite common)? How well can MLFlow handle this? Or\nare you keeping single experiments small with only few runs (which does\nnot seem viable to me, since I cannot compare runs between different\nexperiments in the UI)?\n\nAny experience would be highly appreciated! Could well be that we just\nhave a badly configured SQL server, but I do not really know what to expect.\n\nThanks a lot!\n\nDa",
        "Question_answer_count":6,
        "Question_comment_count":null,
        "Question_creation_time":1560261092000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":67.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/Nh23E3ncxzw",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-06-22T04:15:32",
                "Answer_body":"Is there really nobody who has used MLFlow at this scale? Maybe one of the developers can comment whether MLFlow is at all intended to be used like that? Any experience would be highly appreciated.\n\n\nThanks,\n\n\nDa\n\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-06-26T01:50:00",
                "Answer_body":"Hi Da,\n\n\nIt should definitely be able to handle hundreds of runs, and I\u2019ve seen installations that have many more. The UI only displays the first 1000 runs or so. Do you have a lot of metrics and parameters per run? How many? It would also be nice to do some profiling and figure out whether the issue is on the browser side or on the server side (maybe DB connection or something like that).\n\n\nMatei\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/30e8cd64-75a7-4b3f-99b6-be97a5801c11%40googlegroups.com.\nFor more options, visit https:\/\/groups.google.com\/d\/optout."
            },
            {
                "Answer_creation_time":"2019-06-30T05:37:10",
                "Answer_body":"Hi Matei,\n\nwe have typically 10-20 parameters logged per run, and about 10\ndifferent metrics. Is this a number that you would expect to be handled\nsmoothly? And should the number of logged values per metric impact the\nperformance of the initial query for listing all runs, or only the\ndetailed graph display of a single run?\n\nI am happy to provide more detailed profiling information. Do you have a\nsuggestion how to profile this? So far, what I have seen is that while\nloading the MLFlow page in the browser, I see full CPU load by two\ngunicorn workers on the server, over basically the whole waiting time.\nThus I believe it is more likely a server than a browser issue.\n\nDoes MLFlow write any useful logging (and where?) that could help us\nfurther?\n\nThanks a lot,\nDa"
            },
            {
                "Answer_creation_time":"2019-07-01T08:43:44",
                "Answer_body":"This seems like it should work well. It\u2019s probably best to profile the gunicorn workers then. Are you using the file store or the database store? Switching to the database store will likely improve performance if you aren\u2019t using it already because it requires less I\/O and less data parsing in the workers. I\u2019d probably start with that \u2014 see https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#mlflow-tracking-servers\u00a0for how to set it up using a SQLAlchemy URI.\n\n\nMatei\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/6972f756-a56a-4e90-a698-3d7e33c1dd39%40googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-07-01T13:47:00",
                "Answer_body":"Thanks, Matei!\n\n\nWe are already using the database store with a MySQL database running on the same host as MLFlow. Do you have a suggestion how I could profile the gunicorn workers? I have very limited understanding about the inner workings of the MLFlow server so far...\n\n\nThanks again,\nDa\n\n\nAm Montag, 1. Juli 2019 14:43:44 UTC+2 schrieb Matei Zaharia:\nThis seems like it should work well. It\u2019s probably best to profile the gunicorn workers then. Are you using the file store or the database store? Switching to the database store will likely improve performance if you aren\u2019t using it already because it requires less I\/O and less data parsing in the workers. I\u2019d probably start with that \u2014 see https:\/\/www.mlflow.org\/docs\/latest\/tracking.html#mlflow-tracking-servers\u00a0for how to set it up using a SQLAlchemy URI.\n\n\nMatei\n\n\n\nOn Jun 30, 2019, at 11:37 AM, Da Joghurt <dajo...@gmail.com> wrote:\n\n\nHi Matei,\n\nwe have typically 10-20 parameters logged per run, and about 10\ndifferent metrics. Is this a number that you would expect to be handled\nsmoothly? And should the number of logged values per metric impact the\nperformance of the initial query for listing all runs, or only the\ndetailed graph display of a single run?\n\nI am happy to provide more detailed profiling information. Do you have a\nsuggestion how to profile this? So far, what I have seen is that while\nloading the MLFlow page in the browser, I see full CPU load by two\ngunicorn workers on the server, over basically the whole waiting time.\nThus I believe it is more likely a server than a browser issue.\n\nDoes MLFlow write any useful logging (and where?) that could help us\nfurther?\n\nThanks a lot,\nDa\n\n\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\n\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow...@googlegroups.com.\n\ue5d3"
            },
            {
                "Answer_creation_time":"2019-07-02T07:30:36",
                "Answer_body":"I think the easiest way would be to add some cProfile calls into the server code:\u00a0https:\/\/docs.python.org\/3\/library\/profile.html\u00a0(for each request). There may also be modules to add profiling to Flask as a whole.\n\n\nMatei\n\n\n\n\ue5d3\n\ue5d3\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\n\nTo post to this group, send email to mlflow...@googlegroups.com.\n\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/2697cff6-b35f-40e0-b7cb-be1a4591612c%40googlegroups.com.\n\ue5d3"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: performance issue of ui with o(100) runs?; Content: hi all, we have been using (only the tracking api) for some time now and have recently upgraded to 1.0.0 with a mysql tracking store. over time, we have accumulated a (still moderate) number of runs in our larger experiments, and the first signs of a performance issue start to show up: we have experiments with typically ~100 runs in them. this is enough so that it takes the web ui several seconds (> 5s) to display the list of runs initially (before i can even add search queries). i have not done more detailed profiling so far, but it seems strange to me to see such a response time in a still rather small setup. thus my question: are there people around with large production setups (i would expect that larger databases with thousands of runs in a single experiment could be quite common)? how well can handle this? or are you keeping single experiments small with only few runs (which does not seem viable to me, since i cannot compare runs between different experiments in the ui)? any experience would be highly appreciated! could well be that we just have a badly configured sql server, but i do not really know what to expect. thanks a lot! da",
        "Question_original_content_gpt_summary":"The user is experiencing a performance issue with their web UI taking several seconds to display a list of runs in an experiment with ~100 runs, and is looking for advice from people with larger production setups on how to handle this.",
        "Question_preprocessed_content":"Title: performance issue of ui with o runs?; Content: hi all, we have been using for some time now and have recently upgraded to with a mysql tracking store. over time, we have accumulated a number of runs in our larger experiments, and the first signs of a performance issue start to show up we have experiments with typically runs in them. this is enough so that it takes the web ui several seconds to display the list of runs initially . i have not done more detailed profiling so far, but it seems strange to me to see such a response time in a still rather small setup. thus my question are there people around with large production setups i would expect that larger databases with thousands of runs in a single experiment could be quite common ? how well can handle this? or are you keeping single experiments small with only few runs ? any experience would be highly appreciated! could well be that we just have a badly configured sql server, but i do not really know what to expect. thanks a lot! da",
        "Answer_original_content":"is there really nobody who has used at this scale? maybe one of the developers can comment whether is at all intended to be used like that? any experience would be highly appreciated. thanks, da hi da, it should definitely be able to handle hundreds of runs, and ive seen installations that have many more. the ui only displays the first 1000 runs or so. do you have a lot of metrics and parameters per run? how many? it would also be nice to do some profiling and figure out whether the issue is on the browser side or on the server side (maybe db connection or something like that). matei -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to post to this group, send email to ...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/30e8cd64-75a7-4b3f-99b6-be97a5801c11%40googlegroups.com. for more options, visit https:\/\/groups.google.com\/d\/optout. hi matei, we have typically 10-20 parameters logged per run, and about 10 different metrics. is this a number that you would expect to be handled smoothly? and should the number of logged values per metric impact the performance of the initial query for listing all runs, or only the detailed graph display of a single run? i am happy to provide more detailed profiling information. do you have a suggestion how to profile this? so far, what i have seen is that while loading the page in the browser, i see full cpu load by two gunicorn workers on the server, over basically the whole waiting time. thus i believe it is more likely a server than a browser issue. does write any useful logging (and where?) that could help us further? thanks a lot, da this seems like it should work well. its probably best to profile the gunicorn workers then. are you using the file store or the database store? switching to the database store will likely improve performance if you arent using it already because it requires less i\/o and less data parsing in the workers. id probably start with that see https:\/\/www..org\/docs\/latest\/tracking.html#-tracking-serversfor how to set it up using a sqlalchemy uri. matei -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to post to this group, send email to ...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/6972f756-a56a-4e90-a698-3d7e33c1dd39%40googlegroups.com. thanks, matei! we are already using the database store with a mysql database running on the same host as . do you have a suggestion how i could profile the gunicorn workers? i have very limited understanding about the inner workings of the server so far... thanks again, da am montag, 1. juli 2019 14:43:44 utc+2 schrieb matei zaharia: this seems like it should work well. its probably best to profile the gunicorn workers then. are you using the file store or the database store? switching to the database store will likely improve performance if you arent using it already because it requires less i\/o and less data parsing in the workers. id probably start with that see https:\/\/www..org\/docs\/latest\/tracking.html#-tracking-serversfor how to set it up using a sqlalchemy uri. matei on jun 30, 2019, at 11:37 am, da joghurt wrote: hi matei, we have typically 10-20 parameters logged per run, and about 10 different metrics. is this a number that you would expect to be handled smoothly? and should the number of logged values per metric impact the performance of the initial query for listing all runs, or only the detailed graph display of a single run? i am happy to provide more detailed profiling information. do you have a suggestion how to profile this? so far, what i have seen is that while loading the page in the browser, i see full cpu load by two gunicorn workers on the server, over basically the whole waiting time. thus i believe it is more likely a server than a browser issue. does write any useful logging (and where?) that could help us further? thanks a lot, da -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to ...@googlegroups.com. i think the easiest way would be to add some cprofile calls into the server code:https:\/\/docs.python.org\/3\/library\/profile.html(for each request). there may also be modules to add profiling to flask as a whole. matei to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to post to this group, send email to ...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/2697cff6-b35f-40e0-b7cb-be1a4591612c%40googlegroups.com.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"is there really nobody who has used at this scale? maybe one of the developers can comment whether is at all intended to be used like that? any experience would be highly appreciated. thanks, da hi da, it should definitely be able to handle hundreds of runs, and ive seen installations that have many more. the ui only displays the first runs or so. do you have a lot of metrics and parameters per run? how many? it would also be nice to do some profiling and figure out whether the issue is on the browser side or on the server side . matei you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to to post to this group, send email to to view this discussion on the web visit for more options, visit hi matei, we have typically parameters logged per run, and about different metrics. is this a number that you would expect to be handled smoothly? and should the number of logged values per metric impact the performance of the initial query for listing all runs, or only the detailed graph display of a single run? i am happy to provide more detailed profiling . do you have a suggestion how to profile this? so far, what i have seen is that while loading the page in the browser, i see full cpu load by two gunicorn workers on the server, over basically the whole waiting time. thus i believe it is more likely a server than a browser issue. does write any useful logging that could help us further? thanks a lot, da this seems like it should work well. its probably best to profile the gunicorn workers then. are you using the file store or the database store? switching to the database store will likely improve performance if you arent using it already because it requires less and less data parsing in the workers. id probably start with that see how to set it up using a sqlalchemy uri. matei you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to to post to this group, send email to to view this discussion on the web visit thanks, matei! we are already using the database store with a mysql database running on the same host as . do you have a suggestion how i could profile the gunicorn workers? i have very limited understanding about the inner workings of the server so thanks again, da am montag, . juli utc+ schrieb matei zaharia this seems like it should work well. its probably best to profile the gunicorn workers then. are you using the file store or the database store? switching to the database store will likely improve performance if you arent using it already because it requires less and less data parsing in the workers. id probably start with that see how to set it up using a sqlalchemy uri. matei on jun , , at am, da joghurt wrote hi matei, we have typically parameters logged per run, and about different metrics. is this a number that you would expect to be handled smoothly? and should the number of logged values per metric impact the performance of the initial query for listing all runs, or only the detailed graph display of a single run? i am happy to provide more detailed profiling . do you have a suggestion how to profile this? so far, what i have seen is that while loading the page in the browser, i see full cpu load by two gunicorn workers on the server, over basically the whole waiting time. thus i believe it is more likely a server than a browser issue. does write any useful logging that could help us further? thanks a lot, da you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to i think the easiest way would be to add some cprofile calls into the server each request . there may also be modules to add profiling to flask as a whole. matei to unsubscribe from this group and stop receiving emails from it, send an email to to post to this group, send email to to view this discussion on the web visit"
    },
    {
        "Question_id":72805580.0,
        "Question_title":"Is it possible to modify an existing AWS implementation of a deep learning model?",
        "Question_body":"<p>I wish to modify an existing model implementation in order to add an additional upsampling layer to a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">semantic segmentation algorithm that has previously been implemented in AWS<\/a>.<\/p>\n<p>It appears that Sagemaker refers to <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\/tree\/4e1d44d718ca998523527aa2039cde2184d31296\" rel=\"nofollow noreferrer\">this repo<\/a>, and I'm hoping to modify <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\/blob\/4e1d44d718ca998523527aa2039cde2184d31296\/gluoncv\/model_zoo\/deeplabv3.py\" rel=\"nofollow noreferrer\">the deeplab model<\/a> to add a final additional upsampling layer that is higher resolution than the initial input layer in order to boost the resolution of the output image (i.e., statistically downscale the original imagery).<\/p>\n<p>(<a href=\"https:\/\/www.researchgate.net\/publication\/349804789_A_Deep_Learning_Approach_to_Downscale_Geostationary_Satellite_Imagery_for_Decision_Support_in_High_Impact_Wildfires\" rel=\"nofollow noreferrer\">This technique has been demonstrated with UNET architectures.<\/a>)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1656523696723,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":27.0,
        "Owner_creation_time":1585530660043,
        "Owner_last_access_time":1663135350527,
        "Owner_reputation":59.0,
        "Owner_up_votes":7.0,
        "Owner_down_votes":0.0,
        "Owner_views":11.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72805580",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: is it possible to modify an existing aws implementation of a deep learning model?; Content: i wish to modify an existing model implementation in order to add an additional upsampling layer to a semantic segmentation algorithm that has previously been implemented in aws. it appears that refers to this repo, and i'm hoping to modify the deeplab model to add a final additional upsampling layer that is higher resolution than the initial input layer in order to boost the resolution of the output image (i.e., statistically downscale the original imagery). (this technique has been demonstrated with unet architectures.)",
        "Question_original_content_gpt_summary":"The user is attempting to modify an existing AWS implementation of a semantic segmentation algorithm to add an additional upsampling layer to boost the resolution of the output image.",
        "Question_preprocessed_content":"Title: is it possible to modify an existing aws implementation of a deep learning model?; Content: i wish to modify an existing model implementation in order to add an additional upsampling layer to a semantic segmentation algorithm that has previously been implemented in aws. it appears that refers to this repo, and i'm hoping to modify the deeplab model to add a final additional upsampling layer that is higher resolution than the initial input layer in order to boost the resolution of the output image . this technique has been demonstrated with unet",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"How do I orchestrate ML model retraining periodically?",
        "Question_body":"I have to retrain every month or so a PyTorch Model trained on data obtained from processing tables sitting in Azure Data Lake Storage gen 1.\n\nSo far, I have the following building blocks:\n\nA Databricks notebook that does the ETL job of transforming the ADLS gen 1 tables into train\/validation files that are written in blob storage\n\n\nPython scripts that I can execute locally to run in an AzureML workspace an experiment so to train the PyTorch model using a ScriptRunConfig + training script as in https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-pytorch mounting blob to get the training data.\n\nHow can I schedule steps 1. and 2. to be run in sequence in a pipeline? Azure Data Factory seems a possible way to go, but what should I use as activities in ADF?\n\nI see a few alternatives:\n\nStays surely a Databricks notebook\n2a. Databricks python script calling the azureml-sdk classes (?)\n\nAlternative for step 2a could be\n\n2b. a Batch Service custom activity calling the azureml-sdk classes - seems overkill to me\n2c. use a AzureML execute pipeline as ADF activity https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines (not sure how...)\n2d. use a Python script Databricks activity train a PyTorch model with Databricks https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/tracking-ex-pytorch instead of calling the azureml-sdk classes\n\nCan someone point me to the current best practice for this?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1610148728983,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/224481\/how-do-i-orchestrate-ml-model-retraining-periodica.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-11T08:57:22.71Z",
                "Answer_score":0,
                "Answer_body":"@DavideFiocco-7346 Thanks for the question. Activate a pipeline to retrain the model using AML pipeline. We have a functional repo with training \/ retraining available right here. The link below explains how to use Azure DevOps Project for build and release\/deployment pipelines along with Azure ML services for model retraining pipeline, model management and operationalization. Can you please add more details about the use case.\nHere are some links you might find useful:\n- https:\/\/github.com\/microsoft\/MLOps\n- https:\/\/github.com\/microsoft\/MLOpsPython\n- https:\/\/github.com\/csiebler\/azureml-workshop-2020\/blob\/master\/3-mlops\/MLOps_basic_example.md\n\n\n\n\nSample AI Reference Architectures.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how do i orchestrate ml model retraining periodically?; Content: i have to retrain every month or so a pytorch model trained on data obtained from processing tables sitting in azure data lake storage gen 1. so far, i have the following building blocks: a databricks notebook that does the etl job of transforming the adls gen 1 tables into train\/validation files that are written in blob storage python scripts that i can execute locally to run in an workspace an experiment so to train the pytorch model using a scriptrunconfig + training script as in https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-pytorch mounting blob to get the training data. how can i schedule steps 1. and 2. to be run in sequence in a pipeline? azure data factory seems a possible way to go, but what should i use as activities in adf? i see a few alternatives: stays surely a databricks notebook 2a. databricks python script calling the -sdk classes (?) alternative for step 2a could be 2b. a batch service custom activity calling the -sdk classes - seems overkill to me 2c. use a execute pipeline as adf activity https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-ml-pipelines (not sure how...) 2d. use a python script databricks activity train a pytorch model with databricks https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/tracking-ex-pytorch instead of calling the -sdk classes can someone point me to the current best practice for this?",
        "Question_original_content_gpt_summary":"The user is facing the challenge of orchestrating the periodic retraining of a PyTorch model trained on data obtained from Azure Data Lake Storage Gen 1, and is considering various alternatives for scheduling the necessary steps.",
        "Question_preprocessed_content":"Title: how do i orchestrate ml model retraining periodically?; Content: i have to retrain every month or so a pytorch model trained on data obtained from processing tables sitting in azure data lake storage gen . so far, i have the following building blocks a databricks notebook that does the etl job of transforming the adls gen tables into files that are written in blob storage python scripts that i can execute locally to run in an workspace an experiment so to train the pytorch model using a scriptrunconfig + training script as in mounting blob to get the training data. how can i schedule steps . and . to be run in sequence in a pipeline? azure data factory seems a possible way to go, but what should i use as activities in adf? i see a few alternatives stays surely a databricks notebook a. databricks python script calling the sdk classes alternative for step a could be b. a batch service custom activity calling the sdk classes seems overkill to me c. use a execute pipeline as adf activity d. use a python script databricks activity train a pytorch model with databricks instead of calling the sdk classes can someone point me to the current best practice for this?",
        "Answer_original_content":"@davidefiocco-7346 thanks for the question. activate a pipeline to retrain the model using aml pipeline. we have a functional repo with training \/ retraining available right here. the link below explains how to use azure devops project for build and release\/deployment pipelines along with services for model retraining pipeline, model management and operationalization. can you please add more details about the use case. here are some links you might find useful: - https:\/\/github.com\/microsoft\/mlops - https:\/\/github.com\/microsoft\/mlopspython - https:\/\/github.com\/csiebler\/-workshop-2020\/blob\/master\/3-mlops\/mlops_basic_example.md sample ai reference architectures.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the question. activate a pipeline to retrain the model using aml pipeline. we have a functional repo with training \/ retraining available right here. the link below explains how to use azure devops project for build and pipelines along with services for model retraining pipeline, model management and operationalization. can you please add more details about the use case. here are some links you might find useful sample ai reference architectures."
    },
    {
        "Question_id":72833918.0,
        "Question_title":"Eval_Metrics not recognized\/invalid in AWS XGBoost model",
        "Question_body":"<pre><code>xgb.set_hyperparameters(objective='binary:logistic',num_round=100)\nxgb.fit({'train': s3_input_train})\n\n...\n\n\nfrom sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n                         'min_child_weight': ContinuousParameter(1, 10),\n                         'alpha': ContinuousParameter(0, 2),\n                         'max_depth': IntegerParameter(1, 10),\n                         'num_round': IntegerParameter(1, 300),\n                        'gamma': ContinuousParameter(0, 5),\n                        'lambda': ContinuousParameter(0, 1000),\n                        'max_delta_step':IntegerParameter(1, 10),\n                        'colsample_bylevel':ContinuousParameter(0.1, 1),\n                        'colsample_bytree':ContinuousParameter(0.5, 1),\n                        'subsample':ContinuousParameter(0.5, 1)}\n\n\nobjective_metric_name = 'validation:aucpr'\n\ntuner = HyperparameterTuner(xgb,\n                            objective_metric_name,\n                            hyperparameter_ranges,\n                            max_jobs=50,\n                            max_parallel_jobs=3)\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_val}, include_cls_metadata=False, wait=False)\n<\/code><\/pre>\n<p>Returns the error:<\/p>\n<pre><code>\nAn error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The objective metric for the hyperparameter tuning job, [validation:aucpr], isn\u2019t valid for the [811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest] algorithm. Choose a valid objective metric.\n<\/code><\/pre>\n<p>The same applies when replacing aucpr with f1 and logloss. They are clearly defined as evaluation metrics in the documentation for classification purposes. <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a><\/p>\n<p>What can I do to allow the f1, aucpr and logloss evaluation metrics?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1656704152423,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":77.0,
        "Owner_creation_time":1633542845252,
        "Owner_last_access_time":1663696816443,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72833918",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: eval_metrics not recognized\/invalid in aws xgboost model; Content: xgb.set_hyperparameters(objective='binary:logistic',num_round=100) xgb.fit({'train': s3_input_train}) ... from .tuner import integerparameter, categoricalparameter, continuousparameter, hyperparametertuner hyperparameter_ranges = {'eta': continuousparameter(0, 1), 'min_child_weight': continuousparameter(1, 10), 'alpha': continuousparameter(0, 2), 'max_depth': integerparameter(1, 10), 'num_round': integerparameter(1, 300), 'gamma': continuousparameter(0, 5), 'lambda': continuousparameter(0, 1000), 'max_delta_step':integerparameter(1, 10), 'colsample_bylevel':continuousparameter(0.1, 1), 'colsample_bytree':continuousparameter(0.5, 1), 'subsample':continuousparameter(0.5, 1)} objective_metric_name = 'validation:aucpr' tuner = hyperparametertuner(xgb, objective_metric_name, hyperparameter_ranges, max_jobs=50, max_parallel_jobs=3) tuner.fit({'train': s3_input_train, 'validation': s3_input_val}, include_cls_metadata=false, wait=false) returns the error: an error occurred (validationexception) when calling the createhyperparametertuningjob operation: the objective metric for the hyperparameter tuning job, [validation:aucpr], isn\u2019t valid for the [811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest] algorithm. choose a valid objective metric. the same applies when replacing aucpr with f1 and logloss. they are clearly defined as evaluation metrics in the documentation for classification purposes. https:\/\/docs.aws.amazon.com\/\/latest\/dg\/xgboost-tuning.html what can i do to allow the f1, aucpr and logloss evaluation metrics?",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to use the evaluation metrics f1, aucpr, and logloss in an AWS XGBoost model.",
        "Question_preprocessed_content":"Title: not in aws xgboost model; Content: returns the error the same applies when replacing aucpr with f and logloss. they are clearly defined as evaluation metrics in the documentation for classification purposes. what can i do to allow the f , aucpr and logloss evaluation metrics?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Compare command not working with hiplot on Windows 10",
        "Question_body":"<p>I am able to use <code>hiplot<\/code> in Jupyter notebook and also start using it via command line using <code>python -m hiplot<\/code>. But when I try to use it with <code>guild compare<\/code> I get invalid windows application error.<\/p>\n<pre><code>C:\\Users\\sarat.chinni\\Codes_sequencing\\hiplot&gt;guild compare --tool hiplot\nPreparing data for compare\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sarat.chinni\\Codes_sequencing\\biobench\\sandbox\\Sarat\\supervised_sequencing\\.venv\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    _main()\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\decorators.py\", line 21, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare.py\", line 224, in compare\n    compare_impl.main(args, ctx)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 73, in main\n    _compare_with_tool(args)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 566, in _compare_with_tool\n    _compare_with_hiplot(args)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 579, in _compare_with_hiplot\n    hiplot.compare_runs(get_data_cb)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 36, in compare_runs\n    _handle_default(hiplot, data)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 52, in _handle_default\n    _generate_hiplot_html(hiplot, csv_path, html_path)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 100, in _generate_hiplot_html\n    html = subprocess.check_output([hiplot_include, \"--format\", \"html\", csv_path])\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 411, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 489, in run\n    with Popen(*popenargs, **kwargs) as process:\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 854, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 1307, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\nOSError: [WinError 193] %1 is not a valid Win32 application\n<\/code><\/pre>\n<p>Does this has to do anything with <code>hiplot<\/code> is python script and not windows executable?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":null,
        "Question_creation_time":1610356086236,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":270.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/compare-command-not-working-with-hiplot-on-windows-10\/514",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-11T15:13:45.508Z",
                "Answer_body":"<p>I\u2019m able to recreate. Yes it\u2019s a fault with Guild in assuming that HiPlot provides a Windows script for the library (and lack of tests on Windows). This will be fixed in the next release.<\/p>\n<p>There\u2019s unfortunately no easy work around for this, beyond running in a Linux VM.<\/p>\n<p>I\u2019ll update here with progress.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T07:19:13.577Z",
                "Answer_body":"<p>Thanks for addressing this issue, I am able to use <code>hiplot<\/code> on windows with <code>guildai-0.7.3.dev1<\/code>.<\/p>\n<p>I am not able to install <code>guildai-0.7.2<\/code>(latest stable release) with python-3.8 in windows-10, looks like wheel files are missing in <a href=\"https:\/\/pypi.org\/project\/guildai\/#files\" rel=\"noopener nofollow ugc\">PyPi website<\/a>.<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T14:53:54.449Z",
                "Answer_body":"<p>Yes indeed \u2014 good catch! I\u2019m working to get the missing Windows platforms built now.<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T16:25:08.779Z",
                "Answer_body":"<p>All fixed - thanks for the note!<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: compare command not working with hiplot on windows 10; Content: i am able to use hiplot in jupyter notebook and also start using it via command line using python -m hiplot. but when i try to use it with guild compare i get invalid windows application error. c:\\users\\sarat.chinni\\codes_sequencing\\hiplot>guild compare --tool hiplot preparing data for compare traceback (most recent call last): file \"c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 194, in _run_module_as_main return _run_code(code, main_globals, none, file \"c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code exec(code, run_globals) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\scripts\\guild.exe\\__main__.py\", line 7, in <module> file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main _main() file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main guild.main.main() file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main.py\", line 33, in main main_cmd.main(standalone_mode=false) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 829, in __call__ return self.main(*args, **kwargs) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 782, in main rv = self.invoke(ctx) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 1259, in invoke return _process_result(sub_ctx.command.invoke(sub_ctx)) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 1066, in invoke return ctx.invoke(self.callback, **ctx.params) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 610, in invoke return callback(*args, **kwargs) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\decorators.py\", line 21, in new_func return f(get_current_context(), *args, **kwargs) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn return fn0(*(args + (args(**kw),))) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare.py\", line 224, in compare compare_impl.main(args, ctx) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 73, in main _compare_with_tool(args) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 566, in _compare_with_tool _compare_with_hiplot(args) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 579, in _compare_with_hiplot hiplot.compare_runs(get_data_cb) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 36, in compare_runs _handle_default(hiplot, data) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 52, in _handle_default _generate_hiplot_html(hiplot, csv_path, html_path) file \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 100, in _generate_hiplot_html html = subprocess.check_output([hiplot_include, \"--format\", \"html\", csv_path]) file \"c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 411, in check_output return run(*popenargs, stdout=pipe, timeout=timeout, check=true, file \"c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 489, in run with popen(*popenargs, **kwargs) as process: file \"c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 854, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, file \"c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 1307, in _execute_child hp, ht, pid, tid = _winapi.createprocess(executable, args, oserror: [winerror 193] %1 is not a valid win32 application does this has to do anything with hiplot is python script and not windows executable?",
        "Question_original_content_gpt_summary":"The user is encountering an issue with using the 'guild compare' command with hiplot on Windows 10, resulting in an \"invalid windows application error\".",
        "Question_preprocessed_content":"Title: compare command not working with hiplot on windows; Content: i am able to use in jupyter notebook and also start using it via command line using . but when i try to use it with i get invalid windows application error. does this has to do anything with is python script and not windows executable?",
        "Answer_original_content":"im able to recreate. yes its a fault with guild in assuming that hiplot provides a windows script for the library (and lack of tests on windows). this will be fixed in the next release. theres unfortunately no easy work around for this, beyond running in a linux vm. ill update here with progress. thanks for addressing this issue, i am able to use hiplot on windows with guildai-0.7.3.dev1. i am not able to install guildai-0.7.2(latest stable release) with python-3.8 in windows-10, looks like wheel files are missing in pypi website. yes indeed good catch! im working to get the missing windows platforms built now. all fixed - thanks for the note!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"im able to recreate. yes its a fault with guild in assuming that hiplot provides a windows script for the library . this will be fixed in the next release. theres unfortunately no easy work around for this, beyond running in a linux vm. ill update here with progress. thanks for addressing this issue, i am able to use on windows with . i am not able to install with in windows , looks like wheel files are missing in pypi website. yes indeed good catch! im working to get the missing windows platforms built now. all fixed thanks for the note!"
    },
    {
        "Question_id":null,
        "Question_title":"Pickle support?",
        "Question_body":"Would it be reasonable to have a custom pickling magic method? Something like this:\n\nclass DataFrameSummary:\n    ...\n\n    def __getstate__(self):\n         return {'length': self.length, \n                 'column_stats': self.column_stats,\n                 'corr': self.corr}\n   \n    def __setstate__(self, state):\n        self.length = state['length']\n        self.column_stats = state['column_stats']\n        self.corr = state['corr']\n\nI guess that usually, you don't need to store df property because it is probably already stored somewhere. Or it could be an optional behavior depending on __init__ parameters.\n\nWhat do you think?",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1548256656000,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1493",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2019-01-24T20:00:24Z",
                "Answer_score":1,
                "Answer_body":"I am not sure what would be the use case? Faster access or caching?"
            },
            {
                "Answer_creation_time":"2019-01-25T09:53:45Z",
                "Answer_score":1,
                "Answer_body":"In my case, it was for the purpose of restoring the state from the previous run. I am using Jupyter notebooks to carry out some data analysis, and the dataset is huge enough to take a few minutes to compute summary statistics. So I am doing something like this:\n\ntemp_summary = temp_dir\/'summary.pickle'\nif temp_summary.exists():\n    print(f'Loading previously saved summary: {temp_summary}')\n    summary = pickle.load(temp_summary.open('rb'))\nelse:\n    print('Generating summary statistics')\n    summary = DataFrameSummary(data)\n    print('Saving summary into pickle file...')\n    summary.df = None  # don't need to save the dataset itself\n    with temp_summary.open('wb') as file:\n        pickle.dump(summary, file)\n\nNot a bit deal but requires additional step to save summary onto disk :) Or even better, it could be something like summary.to_pickle() and summary.read_pickle(). Does it make sense?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":1492.0,
        "Question_original_content":"Title: pickle support?; Content: would it be reasonable to have a custom pickling magic method? something like this: class dataframesummary: ... def __getstate__(self): return {'length': self.length, 'column_stats': self.column_stats, 'corr': self.corr} def __setstate__(self, state): self.length = state['length'] self.column_stats = state['column_stats'] self.corr = state['corr'] i guess that usually, you don't need to store df property because it is probably already stored somewhere. or it could be an optional behavior depending on __init__ parameters. what do you think?",
        "Question_original_content_gpt_summary":"The user is questioning whether it would be reasonable to have a custom pickling magic method for a class dataframesummary.",
        "Question_preprocessed_content":"Title: pickle support?; Content: would it be reasonable to have a custom pickling magic method? something like this class dataframesummary def return def state state state i guess that usually, you don't need to store df property because it is probably already stored somewhere. or it could be an optional behavior depending on parameters. what do you think?",
        "Answer_original_content":"i am not sure what would be the use case? faster access or caching? in my case, it was for the purpose of restoring the state from the previous run. i am using jupyter notebooks to carry out some data analysis, and the dataset is huge enough to take a few minutes to compute summary statistics. so i am doing something like this: temp_summary = temp_dir\/'summary.pickle' if temp_summary.exists(): print(f'loading previously saved summary: {temp_summary}') summary = pickle.load(temp_summary.open('rb')) else: print('generating summary statistics') summary = dataframesummary(data) print('saving summary into pickle file...') summary.df = none # don't need to save the dataset itself with temp_summary.open('wb') as file: pickle.dump(summary, file) not a bit deal but requires additional step to save summary onto disk :) or even better, it could be something like summary.to_pickle() and summary.read_pickle(). does it make sense?",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i am not sure what would be the use case? faster access or caching? in my case, it was for the purpose of restoring the state from the previous run. i am using jupyter notebooks to carry out some data analysis, and the dataset is huge enough to take a few minutes to compute summary statistics. so i am doing something like this if print summary else print summary dataframesummary print none don't need to save the dataset itself with as file file not a bit deal but requires additional step to save summary onto disk or even better, it could be something like and does it make sense?"
    },
    {
        "Question_id":57966245.0,
        "Question_title":"How to hyperparametrize Amazon SageMaker Training Jobs Console",
        "Question_body":"<p>I'm trying to use de AWS SageMaker Training Jobs console to train a model with H2o.AutoMl.<\/p>\n\n<p>I got stuck trying to set up Hyperparameters, specifically setting up the 'training' field.<\/p>\n\n<pre><code>{'classification': true, 'categorical_columns':'', 'target': 'label'}\n<\/code><\/pre>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9uR6Y.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I'm trying to set up a classification training job (1\/0), and I believe that everything else on the setup page I can cope, but I don't know how to set up the 'training' field. My data is stored on S3 as a CSV file, as the algorithm requires.<\/p>\n\n<p>My data has around 250000 columns, 4 out of them are categorical, one of them is the target, and the remainder is continuous variables (800 MB)<\/p>\n\n<pre><code>target column name = 'y'\ncategorical columns name = 'SIT','HOL','CTH','YTT'\n<\/code><\/pre>\n\n<p>I hope someone could help me.<\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1568683965203,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Owner_creation_time":1509012479112,
        "Owner_last_access_time":1632253986416,
        "Owner_reputation":97.0,
        "Owner_up_votes":15.0,
        "Owner_down_votes":0.0,
        "Owner_views":25.0,
        "Answer_body":"<p>After I asked I came across an explanation from <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/aws_marketplace\/using_algorithms\/automl\/AutoML_-_Train_multiple_models_in_parallel.ipynb\" rel=\"nofollow noreferrer\">SageMaker examples.<\/a><\/p>\n\n<p>{classification': 'true', 'categorical_columns': 'SIT','HOL','CTH','YTT','target': 'y'}.<\/p>\n\n<p>Problem solved!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1568727043932,
        "Answer_score":0.0,
        "Owner_location":"Belo Horizonte, MG, Brasil",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57966245",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to hyperparametrize training jobs console; Content: i'm trying to use de training jobs console to train a model with h2o.automl. i got stuck trying to set up hyperparameters, specifically setting up the 'training' field. {'classification': true, 'categorical_columns':'', 'target': 'label'} i'm trying to set up a classification training job (1\/0), and i believe that everything else on the setup page i can cope, but i don't know how to set up the 'training' field. my data is stored on s3 as a csv file, as the algorithm requires. my data has around 250000 columns, 4 out of them are categorical, one of them is the target, and the remainder is continuous variables (800 mb) target column name = 'y' categorical columns name = 'sit','hol','cth','ytt' i hope someone could help me. thank you!",
        "Question_original_content_gpt_summary":"The user is struggling to set up hyperparameters for a training job using the H2O.ai Automl console, specifically the 'training' field, for a classification job with 250000 columns, 4 of which are categorical and one of which is the target.",
        "Question_preprocessed_content":"Title: how to hyperparametrize training jobs console; Content: i'm trying to use de training jobs console to train a model with i got stuck trying to set up hyperparameters, specifically setting up the 'training' field. i'm trying to set up a classification training job , and i believe that everything else on the setup page i can cope, but i don't know how to set up the 'training' field. my data is stored on s as a csv file, as the algorithm requires. my data has around columns, out of them are categorical, one of them is the target, and the remainder is continuous variables i hope someone could help me. thank you!",
        "Answer_original_content":"after i asked i came across an explanation from examples. {classification': 'true', 'categorical_columns': 'sit','hol','cth','ytt','target': 'y'}. problem solved!",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"after i asked i came across an explanation from examples. classification' 'true', 'sit','hol','cth','ytt','target' 'y' . problem solved!"
    },
    {
        "Question_id":null,
        "Question_title":"Vertex AI deploy custom model error - Model server terminated: model server container terminated:",
        "Question_body":"Hi, I'm stuck at following error message when I try to deploy custom model to vertex-ai endpoint.Command:  ",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1668750420000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":50.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-deploy-custom-model-error-Model-server-terminated\/td-p\/490796\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-21T21:34:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"You may check here some things you can check further when deploying your model. If this does not work, it would be helpful to file a 1:1 support case since they can check your internal resources."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deploy custom model error - model server terminated: model server container terminated:; Content: hi, i'm stuck at following error message when i try to deploy custom model to vertex-ai endpoint.command:",
        "Question_original_content_gpt_summary":"The user is encountering an error when attempting to deploy a custom model to a Vertex-AI endpoint.",
        "Question_preprocessed_content":"Title: deploy custom model error model server terminated model server container terminated; Content: hi, i'm stuck at following error message when i try to deploy custom model to",
        "Answer_original_content":"you may check here some things you can check further when deploying your model. if this does not work, it would be helpful to file a 1:1 support case since they can check your internal resources.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"you may check here some things you can check further when deploying your model. if this does not work, it would be helpful to file a support case since they can check your internal resources."
    },
    {
        "Question_id":67450031.0,
        "Question_title":"How to set-up MLFlow artifact sftp store on a remote Linux machine?",
        "Question_body":"<p>My goal is to configure ML Flow to run on a remote Linux server, with logs stored in the PostgreSQL database and artifacts in \/home\/aw\/mlfow\/mllogs, where &quot;aw&quot; is my user name with root privileges.<\/p>\n<p>When I run my simple python code (see section below):<\/p>\n<ol>\n<li><p>when it's last line (&quot;mlflow.log_artifact(&quot;features.txt&quot;)) is commented out: I get no errors in Python, but when I enter log details via browser, in the artifact section I get the warning: <code>Unable to list artifacts stored undersftp:\/\/mlflow_user@194.39.141.27:~\/mlflow\/mlruns\/0\/d1eb9ce83b6b4ede96a9ea5203c097da\/artifacts<\/code><\/p>\n<\/li>\n<li><p>in case the last line is active, python compiler returns a long list of errors, ending with <code>ValueError: Port could not be cast to integer value as '~'<\/code>\nI tried running the server from the CLI in many different ways, each time changing the --default-artifact-root parameter (see section below). Interestingly, the printout of the mlflow.get_artifact_uri() variable from the Python code remains the same, despite changes in the CLI server parameters: it always shows up as\n<code>sftp:\/\/mlflow_user@94.39.141.27:~\/mlflow\/mlruns\/0\/c613c110839946a3adc198377cc82c0c\/artifacts<\/code><\/p>\n<\/li>\n<\/ol>\n<p>So, it looks like there is a problem of setting this parameter up during MLFlow server run from CLI. Maybe it's cached somewhere? Linux server reboot doesn't help.<\/p>\n<p>Code to reproduce issue\nServer CLI run (credentials are just examples):<\/p>\n<p>option 1 (based on <a href=\"https:\/\/towardsdatascience.com\/setup-mlflow-in-production-d72aecde7fef\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/setup-mlflow-in-production-d72aecde7fef<\/a>):\n<code>mlflow server --backend-store-uri postgresql:\/\/mlflow_user:mlflow321@localhost\/mlflow_db --default-artifact-root sftp:\/\/mlflow_user@194.39.141.27:~\/mlflow\/mlruns -h 0.0.0.0 -p 8000&amp;<\/code><\/p>\n<p>options 2.1 and 2.2 (aw is my user name on the machine, with root privileges):\n<code>mlflow server --backend-store-uri postgresql:\/\/mlflow_user:mlflow321@localhost\/mlflow_db --default-artifact-root sftp:\/\/aw:aw_pass@194.39.141.27:~\/mlflow\/mlruns -h 0.0.0.0 -p 8000&amp;<\/code><\/p>\n<p><code>mlflow server --backend-store-uri postgresql:\/\/mlflow_user:mlflow321@localhost\/mlflow_db --default-artifact-root sftp:\/\/aw:@194.39.141.27:~\/home\/aw\/mlflow\/mlruns -h 0.0.0.0 -p 8000&amp;<\/code><\/p>\n<p>option 3:\n<code>mlflow server --backend-store-uri postgresql:\/\/mlflow_user:mlflow321@localhost\/mlflow_db --default-artifact-root sftp:\/\/mlflow_user:mlflow_pass#@194.39.141.27:~\/mlflow\/mlruns -h 0.0.0.0 -p 8000&amp;<\/code><\/p>\n<p>Python code:<\/p>\n<pre><code>import mlflow\n\nif __name__ == &quot;__main__&quot;:\n    mlflow.set_tracking_uri(&quot;http:\/\/194.39.141.27:8000&quot;) #hostname IP here is just an example\n\n    features = &quot;rooms, zipcode, median_price, school_rating, transport&quot;\n    with open(&quot;features.txt&quot;, 'w') as f:\n        f.write(features)\n\n    with mlflow.start_run():\n\n        tracking_uri = mlflow.get_tracking_uri()\n        artifact_uri = mlflow.get_artifact_uri()\n        print(&quot;Tracking uri: {}&quot;.format(tracking_uri))\n        print(&quot;Artifact uri: {}&quot;.format(artifact_uri))\n\n        mlflow.log_artifact(&quot;features.txt&quot;)\n<\/code><\/pre>\n<p>Other info \/ logs\nPython compiler log:<\/p>\n<pre><code>File &quot;1.py&quot;, line 18, in\nmlflow.log_artifact(&quot;features.txt&quot;)\nFile &quot;\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 544, in log_artifact\nMlflowClient().log_artifact(run_id, local_path, artifact_path)\nFile &quot;\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/tracking\/client.py&quot;, line 903, in log_artifact\nself._tracking_client.log_artifact(run_id, local_path, artifact_path)\nFile &quot;\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 271, in log_artifact\nartifact_repo = self._get_artifact_repo(run_id)\nFile &quot;\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 262, in _get_artifact_repo\nreturn get_artifact_repository(artifact_uri)\nFile &quot;\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/store\/artifact\/artifact_repository_registry.py&quot;, line 102, in get_artifact_repository\nreturn _artifact_repository_registry.get_artifact_repository(artifact_uri)\nFile &quot;\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/store\/artifact\/artifact_repository_registry.py&quot;, line 71, in get_artifact_repository\nreturn repository(artifact_uri)\nFile &quot;\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/mlflow\/store\/artifact\/sftp_artifact_repo.py&quot;, line 32, in init\n&quot;port&quot;: parsed.port,\nFile &quot;\/home\/aw\/anaconda3\/lib\/python3.8\/urllib\/parse.py&quot;, line 174, in port\nraise ValueError(message) from None\nValueError: Port could not be cast to integer value as '~'\n<\/code><\/pre>\n<p>To conclude: My main goal is to store MLFLow artifacts on a remote Linux server. I would really appreciate any help on how to do this?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1620492790340,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":498.0,
        "Owner_creation_time":1571767378056,
        "Owner_last_access_time":1663412912792,
        "Owner_reputation":95.0,
        "Owner_up_votes":4.0,
        "Owner_down_votes":0.0,
        "Owner_views":14.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Lublin, Polska",
        "Question_last_edit_time":1620493211583,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67450031",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to set-up artifact sftp store on a remote linux machine?; Content: my goal is to configure ml flow to run on a remote linux server, with logs stored in the postgresql database and artifacts in \/home\/aw\/mlfow\/mllogs, where \"aw\" is my user name with root privileges. when i run my simple python code (see section below): when it's last line (\".log_artifact(\"features.txt\")) is commented out: i get no errors in python, but when i enter log details via browser, in the artifact section i get the warning: unable to list artifacts stored undersftp:\/\/_user@194.39.141.27:~\/\/mlruns\/0\/d1eb9ce83b6b4ede96a9ea5203c097da\/artifacts in case the last line is active, python compiler returns a long list of errors, ending with valueerror: port could not be cast to integer value as '~' i tried running the server from the cli in many different ways, each time changing the --default-artifact-root parameter (see section below). interestingly, the printout of the .get_artifact_uri() variable from the python code remains the same, despite changes in the cli server parameters: it always shows up as sftp:\/\/_user@94.39.141.27:~\/\/mlruns\/0\/c613c110839946a3adc198377cc82c0c\/artifacts so, it looks like there is a problem of setting this parameter up during server run from cli. maybe it's cached somewhere? linux server reboot doesn't help. code to reproduce issue server cli run (credentials are just examples): option 1 (based on https:\/\/towardsdatascience.com\/setup--in-production-d72aecde7fef): server --backend-store-uri postgresql:\/\/_user:321@localhost\/_db --default-artifact-root sftp:\/\/_user@194.39.141.27:~\/\/mlruns -h 0.0.0.0 -p 8000& options 2.1 and 2.2 (aw is my user name on the machine, with root privileges): server --backend-store-uri postgresql:\/\/_user:321@localhost\/_db --default-artifact-root sftp:\/\/aw:aw_pass@194.39.141.27:~\/\/mlruns -h 0.0.0.0 -p 8000& server --backend-store-uri postgresql:\/\/_user:321@localhost\/_db --default-artifact-root sftp:\/\/aw:@194.39.141.27:~\/home\/aw\/\/mlruns -h 0.0.0.0 -p 8000& option 3: server --backend-store-uri postgresql:\/\/_user:321@localhost\/_db --default-artifact-root sftp:\/\/_user:_pass#@194.39.141.27:~\/\/mlruns -h 0.0.0.0 -p 8000& python code: import if __name__ == \"__main__\": .set_tracking_uri(\"http:\/\/194.39.141.27:8000\") #hostname ip here is just an example features = \"rooms, zipcode, median_price, school_rating, transport\" with open(\"features.txt\", 'w') as f: f.write(features) with .start_run(): tracking_uri = .get_tracking_uri() artifact_uri = .get_artifact_uri() print(\"tracking uri: {}\".format(tracking_uri)) print(\"artifact uri: {}\".format(artifact_uri)) .log_artifact(\"features.txt\") other info \/ logs python compiler log: file \"1.py\", line 18, in .log_artifact(\"features.txt\") file \"\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/\/tracking\/fluent.py\", line 544, in log_artifact client().log_artifact(run_id, local_path, artifact_path) file \"\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/\/tracking\/client.py\", line 903, in log_artifact self._tracking_client.log_artifact(run_id, local_path, artifact_path) file \"\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 271, in log_artifact artifact_repo = self._get_artifact_repo(run_id) file \"\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/\/tracking\/_tracking_service\/client.py\", line 262, in _get_artifact_repo return get_artifact_repository(artifact_uri) file \"\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/\/store\/artifact\/artifact_repository_registry.py\", line 102, in get_artifact_repository return _artifact_repository_registry.get_artifact_repository(artifact_uri) file \"\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/\/store\/artifact\/artifact_repository_registry.py\", line 71, in get_artifact_repository return repository(artifact_uri) file \"\/home\/aw\/anaconda3\/lib\/python3.8\/site-packages\/\/store\/artifact\/sftp_artifact_repo.py\", line 32, in init \"port\": parsed.port, file \"\/home\/aw\/anaconda3\/lib\/python3.8\/urllib\/parse.py\", line 174, in port raise valueerror(message) from none valueerror: port could not be cast to integer value as '~' to conclude: my main goal is to store artifacts on a remote linux server. i would really appreciate any help on how to do this?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with setting up an artifact sftp store on a remote Linux machine for ML Flow to run on, with logs stored in a Postgresql database.",
        "Question_preprocessed_content":"Title: how to set up artifact sftp store on a remote linux machine?; Content: my goal is to configure ml flow to run on a remote linux server, with logs stored in the postgresql database and artifacts in where aw is my user name with root privileges. when i run my simple python code when it's last line is commented out i get no errors in python, but when i enter log details via browser, in the artifact section i get the warning in case the last line is active, python compiler returns a long list of errors, ending with i tried running the server from the cli in many different ways, each time changing the default artifact root parameter . interestingly, the printout of the variable from the python code remains the same, despite changes in the cli server parameters it always shows up as so, it looks like there is a problem of setting this parameter up during server run from cli. maybe it's cached somewhere? linux server reboot doesn't help. code to reproduce issue server cli run option options and option python code other \/ logs python compiler log to conclude my main goal is to store artifacts on a remote linux server. i would really appreciate any help on how to do this?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":62569747.0,
        "Question_title":"ML Model pod keeps restarting in Seldon deployment",
        "Question_body":"<p>I have a Seldon deployment like this:<\/p>\n<pre><code>apiVersion: machinelearning.seldon.io\/v1alpha2\nkind: SeldonDeployment\nmetadata:\n  name: mlflow\nspec:\n  name: wines\n  predictors:\n    - graph:\n        children: []\n        implementation: MLFLOW_SERVER\n        modelUri: gs:\/\/seldon-models\/mlflow\/elasticnet_wine\n        name: classifier\n      name: default\n      replicas: 1     \n<\/code><\/pre>\n<p>Model is downloaded successfully from the server, but, after a while, pods go to state <code>crashloop<\/code> and restart again and again.<\/p>\n<p>When I see the logs, there is no errors since logs have re-started and I can only see how python packages are being downloaded.<\/p>\n<pre><code>PS C:\\Users\\xxx\\mlflow&gt; kubectl logs -p -c wines-classifier model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp\n<\/code><\/pre>\n<pre><code>Executing before-run script\n---&gt; Creating environment with Conda...\nINFO:root:Copying contents of \/mnt\/models to local\nINFO:root:Reading MLmodel file\nINFO:root:Creating Conda environment 'mlflow' from conda.yaml\nWarning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\n\nDownloading and Extracting Packages\n_libgcc_mutex-0.1    | 3 KB      | ########## | 100%\nreadline-7.0         | 324 KB    | ########## | 100%\nncurses-6.2          | 817 KB    | ########## | 100%\ntbb4py-2020.0        | 209 KB    | ########## | 100%\nscipy-1.1.0          | 13.2 MB   | ########## | 100%\nzlib-1.2.11          | 103 KB    | ########## | 100%\nxz-5.2.5             | 341 KB    | ########## | 100%\nopenssl-1.1.1g       | 2.5 MB    | ########## | 100%\nmkl_fft-1.0.6        | 135 KB    | ########## | 100%\nblas-1.0             | 6 KB      | ########## | 100%\npip-20.1.1           | 1.8 MB    | ########## | 100%\nwheel-0.34.2         | 51 KB     | ########## | 100%\nlibffi-3.2.1         | 40 KB     | ########## | 100%\nscikit-learn-0.19.1  | 3.9 MB    | ########## | 100%\nlibgfortran-ng-7.3.0 | 1006 KB   | ########## | 100%\nsqlite-3.32.3        | 1.1 MB    | ########## | 100%\nnumpy-1.15.4         | 34 KB     | ########## | 100%\ntk-8.6.10            | 3.0 MB    | ########## | 100%\nlibgcc-ng-9.1.0      | 5.1 MB    | ########## | 100%\nsetuptools-47.3.1    | 514 KB    | ########## | 100%\nmkl_random-1.0.1     | 324 KB    | ########## | 100%\npython-3.6.9         | 30.2 MB   | ########## | 100%\ncertifi-2020.6.20    | 156 KB    | ########## | 100%\nnumpy-base-1.15.4    | 3.4 MB    | ########## | 100%\nintel-openmp-2019.4  | 729 KB    | ########## | 100%\nlibedit-3.1.20191231 | 167 KB    | ########## | 100%\nlibstdcxx-ng-9.1.0   | 3.1 MB    | ########## | 100%\ntbb-2020.0           | 1.1 MB    | ########## | 100%\nmkl-2018.0.3         | 126.9 MB  | #########  |  91%\n<\/code><\/pre>\n<p>Now, trying with <code>-p<\/code> parameter as proposed by @arghya-sadhu:<\/p>\n<pre><code>PS C:\\Users\\xxx\\mlflow&gt; kubectl logs -p model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp wines-classifier\n<\/code><\/pre>\n<pre><code>---&gt; Creating environment with Conda...\nINFO:root:Copying contents of \/mnt\/models to local\nINFO:root:Reading MLmodel file\nINFO:root:Creating Conda environment 'mlflow' from conda.yaml\nWarning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\n\nDownloading and Extracting Packages\nscikit-learn-0.19.1  | 3.9 MB    | ########## | 100%\nncurses-6.2          | 817 KB    | ########## | 100%\n_libgcc_mutex-0.1    | 3 KB      | ########## | 100%\nzlib-1.2.11          | 103 KB    | ########## | 100%\ntbb4py-2020.0        | 209 KB    | ########## | 100%\nsetuptools-47.3.1    | 514 KB    | ########## | 100%\nlibedit-3.1.20191231 | 167 KB    | ########## | 100%\ntbb-2020.0           | 1.1 MB    | ########## | 100%\nxz-5.2.5             | 341 KB    | ########## | 100%\nmkl_random-1.0.1     | 324 KB    | ########## | 100%\nlibgcc-ng-9.1.0      | 5.1 MB    | ########## | 100%\npython-3.6.9         | 30.2 MB   | ########## | 100%\nlibgfortran-ng-7.3.0 | 1006 KB   | ########## | 100%\nlibffi-3.2.1         | 40 KB     | ########## | 100%\nmkl-2018.0.3         | 126.9 MB  | ########## | 100%\nlibstdcxx-ng-9.1.0   | 3.1 MB    | ########## | 100%\nreadline-7.0         | 324 KB    | ########## | 100%\nintel-openmp-2019.4  | 729 KB    | ########## | 100%\ntk-8.6.10            | 3.0 MB    | ########## | 100%\npip-20.1.1           | 1.8 MB    | ########## | 100%\nnumpy-base-1.15.4    | 3.4 MB    | ########## | 100%\nwheel-0.34.2         | 51 KB     | ########## | 100%\nscipy-1.1.0          | 13.2 MB   | #########3 |  93%\n<\/code><\/pre>\n<p>And the description of the pod:<\/p>\n<pre><code>PS C:\\Users\\ivarea\\repo\\smartgraph\\mlflow-v2&gt; kubectl describe pod model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp\n<\/code><\/pre>\n<pre><code>Name:         model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp\nNamespace:    default\nPriority:     0\nNode:         mlops-control-plane\/172.19.0.2\nStart Time:   Thu, 25 Jun 2020 10:08:20 +0200\nLabels:       app=model-a-wines-classifier-0-wines-classifier\n              fluentd=true\n              pod-template-hash=5b8bc7889d\n              seldon-app=model-a-wines-classifier\n              seldon-app-svc=model-a-wines-classifier-wines-classifier\n              seldon-deployment-id=model-a\n              version=wines-classifier\nAnnotations:  prometheus.io\/path: \/prometheus\n              prometheus.io\/scrape: true\nStatus:       Running\nIP:           10.244.0.17\nIPs:\n  IP:           10.244.0.17\nControlled By:  ReplicaSet\/model-a-wines-classifier-0-wines-classifier-5b8bc7889d\nInit Containers:\n  wines-classifier-model-initializer:\n    Container ID:  containerd:\/\/6a3b158cf4218f8c177f6d18eb5d0387946bf9cc36f1173754b68a029483da8b\n    Image:         gcr.io\/kfserving\/storage-initializer:0.2.2\n    Image ID:      gcr.io\/kfserving\/storage-initializer@sha256:7a7d3cf4c5121a3e6bad0acc9e88bbdfa9c7f774d80bd64d8e35a84dcfef8890\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Args:\n      gs:\/\/seldon-models\/mlflow\/model-a\n      \/mnt\/models\n    State:          Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Thu, 25 Jun 2020 10:08:24 +0200\n      Finished:     Thu, 25 Jun 2020 10:08:47 +0200\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      cpu:     1\n      memory:  1Gi\n    Requests:\n      cpu:        100m\n      memory:     100Mi\n    Environment:  &lt;none&gt;\n    Mounts:\n      \/mnt\/models from wines-classifier-provision-location (rw)\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from default-token-6vqwk (ro)\nContainers:\n  wines-classifier:\n    Container ID:   containerd:\/\/536753d25877994a17d1f1a63bbaf8717dc9180b80f061152688e4c8504c8468\n    Image:          seldonio\/mlflowserver_rest:0.5\n    Image ID:       docker.io\/seldonio\/mlflowserver_rest@sha256:0fd54a0a314fafc82c490c91df0c4776be454702a307b4b76e12ed6958b4ee00\n    Ports:          6000\/TCP, 9000\/TCP\n    Host Ports:     0\/TCP, 0\/TCP\n    State:          Running\n      Started:      Thu, 25 Jun 2020 10:23:28 +0200\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    137\n      Started:      Thu, 25 Jun 2020 10:19:09 +0200\n      Finished:     Thu, 25 Jun 2020 10:20:41 +0200\n    Ready:          False\n    Restart Count:  7\n    Liveness:       tcp-socket :http delay=60s timeout=1s period=5s #success=1 #failure=3\n    Readiness:      tcp-socket :http delay=20s timeout=1s period=5s #success=1 #failure=3\n    Environment:\n      PREDICTIVE_UNIT_SERVICE_PORT:          9000\n      PREDICTIVE_UNIT_ID:                    wines-classifier\n      PREDICTIVE_UNIT_IMAGE:                 seldonio\/mlflowserver_rest:0.5\n      PREDICTOR_ID:                          wines-classifier\n      PREDICTOR_LABELS:                      {&quot;version&quot;:&quot;wines-classifier&quot;}\n      SELDON_DEPLOYMENT_ID:                  model-a\n      PREDICTIVE_UNIT_METRICS_SERVICE_PORT:  6000\n      PREDICTIVE_UNIT_METRICS_ENDPOINT:      \/prometheus\n      PREDICTIVE_UNIT_PARAMETERS:            [{&quot;name&quot;:&quot;model_uri&quot;,&quot;value&quot;:&quot;\/mnt\/models&quot;,&quot;type&quot;:&quot;STRING&quot;}]\n    Mounts:\n      \/etc\/podinfo from podinfo (rw)\n      \/mnt\/models from wines-classifier-provision-location (ro)\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from default-token-6vqwk (ro)\n  seldon-container-engine:\n    Container ID:  containerd:\/\/938e8f7e3ac23355c8a7a475b71ab54b858aff5ca485f26b99feaba09bb60069\n    Image:         docker.io\/seldonio\/seldon-core-executor:1.1.0\n    Image ID:      docker.io\/seldonio\/seldon-core-executor@sha256:661173fcbc6cb4e9b56db353b19e97d04d9c086e9dc445217f84dc1721bdf894\n    Ports:         8000\/TCP, 8000\/TCP, 5001\/TCP\n    Host Ports:    0\/TCP, 0\/TCP, 0\/TCP\n    Args:\n      --sdep\n      model-a\n      --namespace\n      default\n      --predictor\n      wines-classifier\n      --http_port\n      8000\n      --grpc_port\n      5001\n      --transport\n      rest\n      --protocol\n      seldon\n      --prometheus_path\n      \/prometheus\n    State:          Running\n      Started:      Thu, 25 Jun 2020 10:08:51 +0200\n    Ready:          False\n    Restart Count:  0\n    Requests:\n      cpu:      100m\n    Liveness:   http-get http:\/\/:8000\/live delay=20s timeout=60s period=5s #success=1 #failure=3\n    Readiness:  http-get http:\/\/:8000\/ready delay=20s timeout=60s period=5s #success=1 #failure=3\n    Environment:\n      ENGINE_PREDICTOR:  &lt;binary ommited&gt;\n      REQUEST_LOGGER_DEFAULT_ENDPOINT_PREFIX:  http:\/\/default-broker.\n      SELDON_LOG_MESSAGES_EXTERNALLY:          false\n    Mounts:\n      \/etc\/podinfo from podinfo (rw)\n      \/var\/run\/secrets\/kubernetes.io\/serviceaccount from default-token-6vqwk (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             False\n  ContainersReady   False\n  PodScheduled      True\nVolumes:\n  podinfo:\n    Type:  DownwardAPI (a volume populated by information about the pod)\n    Items:\n      metadata.annotations -&gt; annotations\n  wines-classifier-provision-location:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:\n    SizeLimit:  &lt;unset&gt;\n  default-token-6vqwk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-6vqwk\n    Optional:    false\nQoS Class:       Burstable\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io\/not-ready:NoExecute for 300s\n                 node.kubernetes.io\/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason     Age                  From                          Message\n  ----     ------     ----                 ----                          -------\n  Normal   Scheduled  &lt;unknown&gt;            default-scheduler             Successfully assigned default\/model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp to mlops-control-plane\n  Normal   Pulled     15m                  kubelet, mlops-control-plane  Container image &quot;gcr.io\/kfserving\/storage-initializer:0.2.2&quot; already present on machine\n  Normal   Created    15m                  kubelet, mlops-control-plane  Created container wines-classifier-model-initializer\n  Normal   Started    15m                  kubelet, mlops-control-plane  Started container wines-classifier-model-initializer\n  Normal   Pulled     15m                  kubelet, mlops-control-plane  Container image &quot;seldonio\/mlflowserver_rest:0.5&quot; already present on machine\n  Normal   Created    15m                  kubelet, mlops-control-plane  Created container wines-classifier\n  Normal   Started    15m                  kubelet, mlops-control-plane  Started container wines-classifier\n  Normal   Pulled     15m                  kubelet, mlops-control-plane  Container image &quot;docker.io\/seldonio\/seldon-core-executor:1.1.0&quot; already present on machine\n  Normal   Created    14m                  kubelet, mlops-control-plane  Created container seldon-container-engine\n  Normal   Started    14m                  kubelet, mlops-control-plane  Started container seldon-container-engine\n  Warning  Unhealthy  14m (x8 over 14m)    kubelet, mlops-control-plane  Readiness probe failed: dial tcp 10.244.0.17:9000: connect: connection refused\n  Warning  Unhealthy  28s (x171 over 14m)  kubelet, mlops-control-plane  Readiness probe failed: HTTP probe failed with statuscode: 503\n<\/code><\/pre>\n<p>How can I disable restarting so I can inspect logs to see the actual error?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1593068980727,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":701.0,
        "Owner_creation_time":1362044275996,
        "Owner_last_access_time":1641213346230,
        "Owner_reputation":2570.0,
        "Owner_up_votes":183.0,
        "Owner_down_votes":2.0,
        "Owner_views":106.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Spain",
        "Question_last_edit_time":1593074453296,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62569747",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: ml model pod keeps restarting in seldon deployment; Content: i have a seldon deployment like this: apiversion: machinelearning.seldon.io\/v1alpha2 kind: seldondeployment metadata: name: spec: name: wines predictors: - graph: children: [] implementation: _server modeluri: gs:\/\/seldon-models\/\/elasticnet_wine name: classifier name: default replicas: 1 model is downloaded successfully from the server, but, after a while, pods go to state crashloop and restart again and again. when i see the logs, there is no errors since logs have re-started and i can only see how python packages are being downloaded. ps c:\\users\\xxx\\> kubectl logs -p -c wines-classifier model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp executing before-run script ---> creating environment with conda... info:root:copying contents of \/mnt\/models to local info:root:reading mlmodel file info:root:creating conda environment '' from conda.yaml warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies. conda may not use the correct pip to install your packages, and they may end up in the wrong place. please add an explicit pip dependency. i'm adding one for you, but still nagging you. collecting package metadata (repodata.json): ...working... done solving environment: ...working... done downloading and extracting packages _libgcc_mutex-0.1 | 3 kb | ########## | 100% readline-7.0 | 324 kb | ########## | 100% ncurses-6.2 | 817 kb | ########## | 100% tbb4py-2020.0 | 209 kb | ########## | 100% scipy-1.1.0 | 13.2 mb | ########## | 100% zlib-1.2.11 | 103 kb | ########## | 100% xz-5.2.5 | 341 kb | ########## | 100% openssl-1.1.1g | 2.5 mb | ########## | 100% mkl_fft-1.0.6 | 135 kb | ########## | 100% blas-1.0 | 6 kb | ########## | 100% pip-20.1.1 | 1.8 mb | ########## | 100% wheel-0.34.2 | 51 kb | ########## | 100% libffi-3.2.1 | 40 kb | ########## | 100% scikit-learn-0.19.1 | 3.9 mb | ########## | 100% libgfortran-ng-7.3.0 | 1006 kb | ########## | 100% sqlite-3.32.3 | 1.1 mb | ########## | 100% numpy-1.15.4 | 34 kb | ########## | 100% tk-8.6.10 | 3.0 mb | ########## | 100% libgcc-ng-9.1.0 | 5.1 mb | ########## | 100% setuptools-47.3.1 | 514 kb | ########## | 100% mkl_random-1.0.1 | 324 kb | ########## | 100% python-3.6.9 | 30.2 mb | ########## | 100% certifi-2020.6.20 | 156 kb | ########## | 100% numpy-base-1.15.4 | 3.4 mb | ########## | 100% intel-openmp-2019.4 | 729 kb | ########## | 100% libedit-3.1.20191231 | 167 kb | ########## | 100% libstdcxx-ng-9.1.0 | 3.1 mb | ########## | 100% tbb-2020.0 | 1.1 mb | ########## | 100% mkl-2018.0.3 | 126.9 mb | ######### | 91% now, trying with -p parameter as proposed by @arghya-sadhu: ps c:\\users\\xxx\\> kubectl logs -p model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp wines-classifier ---> creating environment with conda... info:root:copying contents of \/mnt\/models to local info:root:reading mlmodel file info:root:creating conda environment '' from conda.yaml warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies. conda may not use the correct pip to install your packages, and they may end up in the wrong place. please add an explicit pip dependency. i'm adding one for you, but still nagging you. collecting package metadata (repodata.json): ...working... done solving environment: ...working... done downloading and extracting packages scikit-learn-0.19.1 | 3.9 mb | ########## | 100% ncurses-6.2 | 817 kb | ########## | 100% _libgcc_mutex-0.1 | 3 kb | ########## | 100% zlib-1.2.11 | 103 kb | ########## | 100% tbb4py-2020.0 | 209 kb | ########## | 100% setuptools-47.3.1 | 514 kb | ########## | 100% libedit-3.1.20191231 | 167 kb | ########## | 100% tbb-2020.0 | 1.1 mb | ########## | 100% xz-5.2.5 | 341 kb | ########## | 100% mkl_random-1.0.1 | 324 kb | ########## | 100% libgcc-ng-9.1.0 | 5.1 mb | ########## | 100% python-3.6.9 | 30.2 mb | ########## | 100% libgfortran-ng-7.3.0 | 1006 kb | ########## | 100% libffi-3.2.1 | 40 kb | ########## | 100% mkl-2018.0.3 | 126.9 mb | ########## | 100% libstdcxx-ng-9.1.0 | 3.1 mb | ########## | 100% readline-7.0 | 324 kb | ########## | 100% intel-openmp-2019.4 | 729 kb | ########## | 100% tk-8.6.10 | 3.0 mb | ########## | 100% pip-20.1.1 | 1.8 mb | ########## | 100% numpy-base-1.15.4 | 3.4 mb | ########## | 100% wheel-0.34.2 | 51 kb | ########## | 100% scipy-1.1.0 | 13.2 mb | #########3 | 93% and the description of the pod: ps c:\\users\\ivarea\\repo\\smartgraph\\-v2> kubectl describe pod model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp name: model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp namespace: default priority: 0 node: mlops-control-plane\/172.19.0.2 start time: thu, 25 jun 2020 10:08:20 +0200 labels: app=model-a-wines-classifier-0-wines-classifier fluentd=true pod-template-hash=5b8bc7889d seldon-app=model-a-wines-classifier seldon-app-svc=model-a-wines-classifier-wines-classifier seldon-deployment-id=model-a version=wines-classifier annotations: prometheus.io\/path: \/prometheus prometheus.io\/scrape: true status: running ip: 10.244.0.17 ips: ip: 10.244.0.17 controlled by: replicaset\/model-a-wines-classifier-0-wines-classifier-5b8bc7889d init containers: wines-classifier-model-initializer: container id: containerd:\/\/6a3b158cf4218f8c177f6d18eb5d0387946bf9cc36f1173754b68a029483da8b image: gcr.io\/kfserving\/storage-initializer:0.2.2 image id: gcr.io\/kfserving\/storage-initializer@sha256:7a7d3cf4c5121a3e6bad0acc9e88bbdfa9c7f774d80bd64d8e35a84dcfef8890 port: <none> host port: <none> args: gs:\/\/seldon-models\/\/model-a \/mnt\/models state: terminated reason: completed exit code: 0 started: thu, 25 jun 2020 10:08:24 +0200 finished: thu, 25 jun 2020 10:08:47 +0200 ready: true restart count: 0 limits: cpu: 1 memory: 1gi requests: cpu: 100m memory: 100mi environment: <none> mounts: \/mnt\/models from wines-classifier-provision-location (rw) \/var\/run\/secrets\/kubernetes.io\/serviceaccount from default-token-6vqwk (ro) containers: wines-classifier: container id: containerd:\/\/536753d25877994a17d1f1a63bbaf8717dc9180b80f061152688e4c8504c8468 image: seldonio\/server_rest:0.5 image id: docker.io\/seldonio\/server_rest@sha256:0fd54a0a314fafc82c490c91df0c4776be454702a307b4b76e12ed6958b4ee00 ports: 6000\/tcp, 9000\/tcp host ports: 0\/tcp, 0\/tcp state: running started: thu, 25 jun 2020 10:23:28 +0200 last state: terminated reason: error exit code: 137 started: thu, 25 jun 2020 10:19:09 +0200 finished: thu, 25 jun 2020 10:20:41 +0200 ready: false restart count: 7 liveness: tcp-socket :http delay=60s timeout=1s period=5s #success=1 #failure=3 readiness: tcp-socket :http delay=20s timeout=1s period=5s #success=1 #failure=3 environment: predictive_unit_service_port: 9000 predictive_unit_id: wines-classifier predictive_unit_image: seldonio\/server_rest:0.5 predictor_id: wines-classifier predictor_labels: {\"version\":\"wines-classifier\"} seldon_deployment_id: model-a predictive_unit_metrics_service_port: 6000 predictive_unit_metrics_endpoint: \/prometheus predictive_unit_parameters: [{\"name\":\"model_uri\",\"value\":\"\/mnt\/models\",\"type\":\"string\"}] mounts: \/etc\/podinfo from podinfo (rw) \/mnt\/models from wines-classifier-provision-location (ro) \/var\/run\/secrets\/kubernetes.io\/serviceaccount from default-token-6vqwk (ro) seldon-container-engine: container id: containerd:\/\/938e8f7e3ac23355c8a7a475b71ab54b858aff5ca485f26b99feaba09bb60069 image: docker.io\/seldonio\/seldon-core-executor:1.1.0 image id: docker.io\/seldonio\/seldon-core-executor@sha256:661173fcbc6cb4e9b56db353b19e97d04d9c086e9dc445217f84dc1721bdf894 ports: 8000\/tcp, 8000\/tcp, 5001\/tcp host ports: 0\/tcp, 0\/tcp, 0\/tcp args: --sdep model-a --namespace default --predictor wines-classifier --http_port 8000 --grpc_port 5001 --transport rest --protocol seldon --prometheus_path \/prometheus state: running started: thu, 25 jun 2020 10:08:51 +0200 ready: false restart count: 0 requests: cpu: 100m liveness: http-get http:\/\/:8000\/live delay=20s timeout=60s period=5s #success=1 #failure=3 readiness: http-get http:\/\/:8000\/ready delay=20s timeout=60s period=5s #success=1 #failure=3 environment: engine_predictor: <binary ommited> request_logger_default_endpoint_prefix: http:\/\/default-broker. seldon_log_messages_externally: false mounts: \/etc\/podinfo from podinfo (rw) \/var\/run\/secrets\/kubernetes.io\/serviceaccount from default-token-6vqwk (ro) conditions: type status initialized true ready false containersready false podscheduled true volumes: podinfo: type: downwardapi (a volume populated by information about the pod) items: metadata.annotations -> annotations wines-classifier-provision-location: type: emptydir (a temporary directory that shares a pod's lifetime) medium: sizelimit: <unset> default-token-6vqwk: type: secret (a volume populated by a secret) secretname: default-token-6vqwk optional: false qos class: burstable node-selectors: <none> tolerations: node.kubernetes.io\/not-ready:noexecute for 300s node.kubernetes.io\/unreachable:noexecute for 300s events: type reason age from message ---- ------ ---- ---- ------- normal scheduled <unknown> default-scheduler successfully assigned default\/model-a-wines-classifier-0-wines-classifier-5b8bc7889d-5t7wp to mlops-control-plane normal pulled 15m kubelet, mlops-control-plane container image \"gcr.io\/kfserving\/storage-initializer:0.2.2\" already present on machine normal created 15m kubelet, mlops-control-plane created container wines-classifier-model-initializer normal started 15m kubelet, mlops-control-plane started container wines-classifier-model-initializer normal pulled 15m kubelet, mlops-control-plane container image \"seldonio\/server_rest:0.5\" already present on machine normal created 15m kubelet, mlops-control-plane created container wines-classifier normal started 15m kubelet, mlops-control-plane started container wines-classifier normal pulled 15m kubelet, mlops-control-plane container image \"docker.io\/seldonio\/seldon-core-executor:1.1.0\" already present on machine normal created 14m kubelet, mlops-control-plane created container seldon-container-engine normal started 14m kubelet, mlops-control-plane started container seldon-container-engine warning unhealthy 14m (x8 over 14m) kubelet, mlops-control-plane readiness probe failed: dial tcp 10.244.0.17:9000: connect: connection refused warning unhealthy 28s (x171 over 14m) kubelet, mlops-control-plane readiness probe failed: http probe failed with statuscode: 503 how can i disable restarting so i can inspect logs to see the actual error?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with their Seldon deployment, where the model pod keeps restarting and the logs do not show any errors, preventing them from being able to identify the cause of the issue.",
        "Question_preprocessed_content":"Title: ml model pod keeps restarting in seldon deployment; Content: i have a seldon deployment like this model is downloaded successfully from the server, but, after a while, pods go to state and restart again and again. when i see the logs, there is no errors since logs have re started and i can only see how python packages are being downloaded. now, trying with parameter as proposed by and the description of the pod how can i disable restarting so i can inspect logs to see the actual error?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Vertex AI image classification models lose accuracy when being placed in a python dictionary",
        "Question_body":"I have made a  model using vertex AI's image classification. Exported as EdgeTPU tflite model to my Raspberry pi 4 with Coral USB accelerator. When I used the Pycoral's example code https:\/\/github.com\/google-coral\/pycoral\/blob\/master\/examples\/classify_image.py  to run my model, I get a perfect prediction result. But when I passed them to a python dictionary in my script, the prediction accuracy is way off. https:\/\/github.com\/hillyuyichu\/Pycoral-python-API\/blob\/main\/pycoral_classification.py   Here is a screenshot of the prediction results on my python classification.py:The label in row 1 is always the most active. The one in the last rows are the least active and most inaccurate.ex: In picture 2, the label empty_pan barely ever cross 0.10 mark when it should have been more than 0.50",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1666779180000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":106.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-image-classification-models-lose-accuracy-when-being\/td-p\/482475\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-31T12:54:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"It seems Vertex AI is not supposed to be placed in a Python directory.\n\nIf this is impacting your application or your business, you can file a feature request using the following link. File the feature request, and they could assist you with the feature you are trying to implement."
            },
            {
                "Answer_creation_time":"2022-11-02T09:30:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi Comaro!\n\nThanks for the reply! I was able to find a way to work around it."
            },
            {
                "Answer_creation_time":"2022-11-02T11:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Could you share the workaround, Hillyu?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: image classification models lose accuracy when being placed in a python dictionary; Content: i have made a model using 's image classification. exported as edgetpu tflite model to my raspberry pi 4 with coral usb accelerator. when i used the pycoral's example code https:\/\/github.com\/google-coral\/pycoral\/blob\/master\/examples\/classify_image.py to run my model, i get a perfect prediction result. but when i passed them to a python dictionary in my script, the prediction accuracy is way off. https:\/\/github.com\/hillyuyichu\/pycoral-python-api\/blob\/main\/pycoral_classification.py here is a screenshot of the prediction results on my python classification.py:the label in row 1 is always the most active. the one in the last rows are the least active and most inaccurate.ex: in picture 2, the label empty_pan barely ever cross 0.10 mark when it should have been more than 0.50",
        "Question_original_content_gpt_summary":"The user encountered a challenge where their image classification models lost accuracy when being placed in a Python dictionary.",
        "Question_preprocessed_content":"Title: image classification models lose accuracy when being placed in a python dictionary; Content: i have made a model using 's image classification. exported as edgetpu tflite model to my raspberry pi with coral usb accelerator. when i used the pycoral's example code to run my model, i get a perfect prediction result. but when i passed them to a python dictionary in my script, the prediction accuracy is way off. here is a screenshot of the prediction results on my python label in row is always the most active. the one in the last rows are the least active and most in picture , the label barely ever cross mark when it should have been more than",
        "Answer_original_content":"it seems is not supposed to be placed in a python directory. if this is impacting your application or your business, you can file a feature request using the following link. file the feature request, and they could assist you with the feature you are trying to implement. hi comaro! thanks for the reply! i was able to find a way to work around it. could you share the workaround, hillyu?",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"it seems is not supposed to be placed in a python directory. if this is impacting your application or your business, you can file a feature request using the following link. file the feature request, and they could assist you with the feature you are trying to implement. hi comaro! thanks for the reply! i was able to find a way to work around it. could you share the workaround, hillyu?"
    },
    {
        "Question_id":null,
        "Question_title":"When Analyzing using Explanations is it possible to unselect the first value?",
        "Question_body":"When working with Azure Machine Learning AutoML I am unable to unselect the first value in the list when trying to create a new cohort using Explanations. For example in the below I am unable to unselect Female if I wish to only see how a factor affected the Male demographic. \n\nIs there a way to unselect this value?",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_creation_time":1614091600373,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/285095\/when-analyzing-using-explanations-is-it-possible-t.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: when analyzing using explanations is it possible to unselect the first value?; Content: when working with automl i am unable to unselect the first value in the list when trying to create a new cohort using explanations. for example in the below i am unable to unselect female if i wish to only see how a factor affected the male demographic. is there a way to unselect this value?",
        "Question_original_content_gpt_summary":"The user is unable to unselect the first value in the list when trying to create a new cohort using explanations in Automl.",
        "Question_preprocessed_content":"Title: when analyzing using explanations is it possible to unselect the first value?; Content: when working with automl i am unable to unselect the first value in the list when trying to create a new cohort using explanations. for example in the below i am unable to unselect female if i wish to only see how a factor affected the male demographic. is there a way to unselect this value?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Traceback error",
        "Question_body":"<p>Hey guys,<\/p>\n<p>I am totally new to W&amp;B. I am getting a Traceback error when I want to run \u201cwandb.init(project=\u201d\u2026\u201c)\u201d. Last week it still did work. Any tips what to do?? Thank you so much.<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 999, in init\n    run = wi.init()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 651, in init\n    backend.cleanup()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/backend\/backend.py\", line 246, in cleanup\n    self.interface.join()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 475, in join\n    super().join()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 666, in join\n    _ = self._communicate_shutdown()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 472, in _communicate_shutdown\n    _ = self._communicate(record)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 226, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 231, in _communicate_async\n    raise Exception(\"The wandb backend process has shutdown\")\nException: The wandb backend process has shutdown\nwandb: ERROR Abnormal program exit\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n    998         try:\n--&gt; 999             run = wi.init()\n   1000             except_exit = wi.settings._except_exit\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py in init(self)\n    650                     # we don't need to do console cleanup at this point\n--&gt; 651                     backend.cleanup()\n    652                     self.teardown()\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/backend\/backend.py in cleanup(self)\n    245         if self.interface:\n--&gt; 246             self.interface.join()\n    247         if self.wandb_process:\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in join(self)\n    474     def join(self) -&gt; None:\n--&gt; 475         super().join()\n    476 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface.py in join(self)\n    665             return\n--&gt; 666         _ = self._communicate_shutdown()\n    667 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in _communicate_shutdown(self)\n    471         record = self._make_record(request=request)\n--&gt; 472         _ = self._communicate(record)\n    473 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in _communicate(self, rec, timeout, local)\n    225     ) -&gt; Optional[pb.Result]:\n--&gt; 226         return self._communicate_async(rec, local=local).get(timeout=timeout)\n    227 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in _communicate_async(self, rec, local)\n    230         if self._process_check and self._process and not self._process.is_alive():\n--&gt; 231             raise Exception(\"The wandb backend process has shutdown\")\n    232         future = self._router.send_and_receive(rec, local=local)\n\nException: The wandb backend process has shutdown\n\nThe above exception was the direct cause of the following exception:\n\nException                                 Traceback (most recent call last)\n&lt;ipython-input-49-e3734aa09c65&gt; in &lt;module&gt;\n      1 #Login to wandb\n      2 # #! wandb login config_dict[\"wandb_key\"]\n----&gt; 3 wandb.init()\n      4 #run_name = wandb.run.name\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n   1035             if except_exit:\n   1036                 os._exit(-1)\n-&gt; 1037             raise Exception(\"problem\") from error_seen\n   1038     return run\n\nException: problem\n<\/code><\/pre>",
        "Question_answer_count":7,
        "Question_comment_count":null,
        "Question_creation_time":1661465657943,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":1364.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/traceback-error\/3008",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-29T23:11:28.940Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/pthielge\">@pthielge<\/a>!<\/p>\n<p>Thanks for writing in, and I am sorry you are facing issues over here. There could be a few issues that could cause this error to show up in your program. Could you share some more information with me so that we could narrow down the scope of this error?<\/p>\n<ul>\n<li>What version of <code>wandb<\/code> are you using?<\/li>\n<li>What operating system are you running this script on?<\/li>\n<li>There should be a folder called <code>wandb<\/code> in your working directory with multiple sub-folders of the format <code>run-&lt;DATETIME&gt;-&lt;ID&gt;<\/code>, could you share the <code>debug.log<\/code> and <code>debug-internal.log<\/code> files from the folder corresponding to this run ID?<\/li>\n<\/ul>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":38.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-30T08:53:15.297Z",
                "Answer_body":"<p>Hi Ramit,<\/p>\n<p>thanks for your reply.  I\u2019m using version \u20180.13.2\u2019 in JupyterLab in Windows 11. Somehow, there is only a debug.log and no debug-internal.log file, so I will append it here.<\/p>\n<p>Thanks,<\/p>\n<p>Philip<\/p>\n<pre><code class=\"lang-auto\">2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Configure stats pid to 420\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Loading settings from \/home\/p\/pthielge\/.config\/wandb\/settings\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Loading settings from \/home\/p\/pthielge\/3D\/S2_Top\/wandb\/settings\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'_require_service': 'True'}\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program': '&lt;python with no main file&gt;'}\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_init.py:_log_setup():461] Logging user logs to \/home\/p\/pthielge\/3D\/S2_Top\/wandb\/run-20220830_104449-5v6f9ano\/logs\/debug.log\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:_log_setup():462] Logging internal logs to \/home\/p\/pthielge\/3D\/S2_Top\/wandb\/run-20220830_104449-5v6f9ano\/logs\/debug-internal.log\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:init():495] calling init triggers\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:init():499] wandb.init called with sweep_config: {}\nconfig: {}\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:init():548] starting backend\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:init():552] setting up manager\n2022-08-30 10:44:49,631 ERROR   MainThread:420 [wandb_init.py:init():1070] error\nTraceback (most recent call last):\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1043, in init\n    run = wi.init()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 553, in init\n    manager._inform_init(settings=self.settings, run_id=self.settings.run_id)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_manager.py\", line 161, in _inform_init\n    svc_iface._svc_inform_init(settings=settings, run_id=run_id)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/service\/service_sock.py\", line 39, in _svc_inform_init\n    self._sock_client.send(inform_init=inform_init)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 140, in send\n    self.send_server_request(server_req)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\n    self._send_message(msg)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\n    sent = self._sock.send(data[total_sent:])\nBrokenPipeError: [Errno 32] Broken pipe\n<\/code><\/pre>",
                "Answer_score":72.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-14T16:28:02.005Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/pthielge\">@pthielge<\/a>,<\/p>\n<p>Thanks for the logs! Is this an error that you see consistently across the runs or is this just a one-off event? Looks like the communication we set up between your program and our server was dropped, so I don\u2019t expect this to happen consistently.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":62.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-20T17:07:33.911Z",
                "Answer_body":"<p>\u200bHi <a class=\"mention\" href=\"\/u\/pthielge\">@pthielge<\/a>,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_score":12.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-20T17:23:57.681Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>thanks for your reply <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a>. It seems you\u2019re right, some temporary connection problem. I had this issue for 2 days, and then suddenly it was no longer there, without changing the code.<\/p>\n<p>Thank you so much. Best regards!<\/p>",
                "Answer_score":27.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-20T17:28:03.479Z",
                "Answer_body":"<p>Got it! In that case, I\u2019ll go ahead and close out this support request. In case this issue shows up again, please let us know!<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":37.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-19T17:28:32.595Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: traceback error; Content: hey guys, i am totally new to w&b. i am getting a traceback error when i want to run \u201c.init(project=\u201d\u2026\u201c)\u201d. last week it still did work. any tips what to do?? thank you so much. traceback (most recent call last): file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/_init.py\", line 999, in init run = wi.init() file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/_init.py\", line 651, in init backend.cleanup() file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/backend\/backend.py\", line 246, in cleanup self.interface.join() file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface_shared.py\", line 475, in join super().join() file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface.py\", line 666, in join _ = self._communicate_shutdown() file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface_shared.py\", line 472, in _communicate_shutdown _ = self._communicate(record) file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface_shared.py\", line 226, in _communicate return self._communicate_async(rec, local=local).get(timeout=timeout) file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface_shared.py\", line 231, in _communicate_async raise exception(\"the backend process has shutdown\") exception: the backend process has shutdown : error abnormal program exit --------------------------------------------------------------------------- exception traceback (most recent call last) ~\/.local\/lib\/python3.6\/site-packages\/\/sdk\/_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings) 998 try: --> 999 run = wi.init() 1000 except_exit = wi.settings._except_exit ~\/.local\/lib\/python3.6\/site-packages\/\/sdk\/_init.py in init(self) 650 # we don't need to do console cleanup at this point --> 651 backend.cleanup() 652 self.teardown() ~\/.local\/lib\/python3.6\/site-packages\/\/sdk\/backend\/backend.py in cleanup(self) 245 if self.interface: --> 246 self.interface.join() 247 if self._process: ~\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface_shared.py in join(self) 474 def join(self) -> none: --> 475 super().join() 476 ~\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface.py in join(self) 665 return --> 666 _ = self._communicate_shutdown() 667 ~\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface_shared.py in _communicate_shutdown(self) 471 record = self._make_record(request=request) --> 472 _ = self._communicate(record) 473 ~\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface_shared.py in _communicate(self, rec, timeout, local) 225 ) -> optional[pb.result]: --> 226 return self._communicate_async(rec, local=local).get(timeout=timeout) 227 ~\/.local\/lib\/python3.6\/site-packages\/\/sdk\/interface\/interface_shared.py in _communicate_async(self, rec, local) 230 if self._process_check and self._process and not self._process.is_alive(): --> 231 raise exception(\"the backend process has shutdown\") 232 future = self._router.send_and_receive(rec, local=local) exception: the backend process has shutdown the above exception was the direct cause of the following exception: exception traceback (most recent call last) <ipython-input-49-e3734aa09c65> in <module> 1 #login to 2 # #! login config_dict[\"_key\"] ----> 3 .init() 4 #run_name = .run.name ~\/.local\/lib\/python3.6\/site-packages\/\/sdk\/_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings) 1035 if except_exit: 1036 os._exit(-1) -> 1037 raise exception(\"problem\") from error_seen 1038 return run exception: problem",
        "Question_original_content_gpt_summary":"The user is encountering a traceback error when attempting to run \".init(project=\u201d\u2026\u201c)\u201d, which was working last week.",
        "Question_preprocessed_content":"Title: traceback error; Content: hey guys, i am totally new to w&b. i am getting a traceback error when i want to run last week it still did work. any tips what to do?? thank you so much.",
        "Answer_original_content":"hi @pthielge! thanks for writing in, and i am sorry you are facing issues over here. there could be a few issues that could cause this error to show up in your program. could you share some more information with me so that we could narrow down the scope of this error? what version of are you using? what operating system are you running this script on? there should be a folder called in your working directory with multiple sub-folders of the format run-<datetime>-<id>, could you share the debug.log and debug-internal.log files from the folder corresponding to this run id? thanks, ramit hi ramit, thanks for your reply. im using version 0.13.2 in jupyterlab in windows 11. somehow, there is only a debug.log and no debug-internal.log file, so i will append it here. thanks, philip 2022-08-30 10:44:49,612 info mainthread:420 [_setup.py:_flush():76] configure stats pid to 420 2022-08-30 10:44:49,612 info mainthread:420 [_setup.py:_flush():76] loading settings from \/home\/p\/pthielge\/.config\/\/settings 2022-08-30 10:44:49,612 info mainthread:420 [_setup.py:_flush():76] loading settings from \/home\/p\/pthielge\/3d\/s2_top\/\/settings 2022-08-30 10:44:49,612 info mainthread:420 [_setup.py:_flush():76] loading settings from environment variables: {'_require_service': 'true'} 2022-08-30 10:44:49,612 info mainthread:420 [_setup.py:_flush():76] inferring run settings from compute environment: {'program': '<python with no main file>'} 2022-08-30 10:44:49,612 info mainthread:420 [_init.py:_log_setup():461] logging user logs to \/home\/p\/pthielge\/3d\/s2_top\/\/run-20220830_104449-5v6f9ano\/logs\/debug.log 2022-08-30 10:44:49,613 info mainthread:420 [_init.py:_log_setup():462] logging internal logs to \/home\/p\/pthielge\/3d\/s2_top\/\/run-20220830_104449-5v6f9ano\/logs\/debug-internal.log 2022-08-30 10:44:49,613 info mainthread:420 [_init.py:init():495] calling init triggers 2022-08-30 10:44:49,613 info mainthread:420 [_init.py:init():499] .init called with sweep_config: {} config: {} 2022-08-30 10:44:49,613 info mainthread:420 [_init.py:init():548] starting backend 2022-08-30 10:44:49,613 info mainthread:420 [_init.py:init():552] setting up manager 2022-08-30 10:44:49,631 error mainthread:420 [_init.py:init():1070] error traceback (most recent call last): file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/_init.py\", line 1043, in init run = wi.init() file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/_init.py\", line 553, in init manager._inform_init(settings=self.settings, run_id=self.settings.run_id) file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/_manager.py\", line 161, in _inform_init svc_iface._svc_inform_init(settings=settings, run_id=run_id) file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/service\/service_sock.py\", line 39, in _svc_inform_init self._sock_client.send(inform_init=inform_init) file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/lib\/sock_client.py\", line 140, in send self.send_server_request(server_req) file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/lib\/sock_client.py\", line 84, in send_server_request self._send_message(msg) file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/lib\/sock_client.py\", line 81, in _send_message self._sendall_with_error_handle(header + data) file \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle sent = self._sock.send(data[total_sent:]) brokenpipeerror: [errno 32] broken pipe hey @pthielge, thanks for the logs! is this an error that you see consistently across the runs or is this just a one-off event? looks like the communication we set up between your program and our server was dropped, so i dont expect this to happen consistently. thanks, ramit hi @pthielge, we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. hi, thanks for your reply @ramit_goolry. it seems youre right, some temporary connection problem. i had this issue for 2 days, and then suddenly it was no longer there, without changing the code. thank you so much. best regards! got it! in that case, ill go ahead and close out this support request. in case this issue shows up again, please let us know! thanks, ramit this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi thanks for writing in, and i am sorry you are facing issues over here. there could be a few issues that could cause this error to show up in your program. could you share some more with me so that we could narrow down the scope of this error? what version of are you using? what operating system are you running this script on? there should be a folder called in your working directory with multiple sub folders of the format , could you share the and files from the folder corresponding to this run id? thanks, ramit hi ramit, thanks for your reply. im using version in jupyterlab in windows . somehow, there is only a and no file, so i will append it here. thanks, philip hey thanks for the logs! is this an error that you see consistently across the runs or is this just a one off event? looks like the communication we set up between your program and our server was dropped, so i dont expect this to happen consistently. thanks, ramit hi we wanted to follow up with you regarding your support request as we have not heard back from you. please let us know if we can be of further assistance or if your issue has been resolved. hi, thanks for your reply it seems youre right, some temporary connection problem. i had this issue for days, and then suddenly it was no longer there, without changing the code. thank you so much. best regards! got it! in that case, ill go ahead and close out this support request. in case this issue shows up again, please let us know! thanks, ramit this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":58572657.0,
        "Question_title":"Can a Sagemaker endpoint support nested data?",
        "Question_body":"<p>This has been puzzling me for a while and I may be 'barking up the wrong tree'.<\/p>\n\n<p>We currently use Sagemaker to make predictions on component failures for certain products in a basic way. This is done fairly simply by training the model and passing \"modelcode, manufacture_date, component_code, failure_type\" to the endpoint.<\/p>\n\n<p>The issue is that certain products have trends in component failures and passing the above doesn't include the historic issues with a product in question. e.g. the product may have had 2 component failures that we would predict would lead to a 3rd component failure as other products have had the same issues\/trend.<\/p>\n\n<p>Ideally we would pass nested JSON into the endpoint as follows:<\/p>\n\n<pre><code>{\n    \"modelcode\": \"XX001\",\n    \"manufacturedate\": \"2008.10.08\",\n    \"component_failures\":[\n     {\n           \"component_code\":\"CC001\",\n           \"failure_type\":\"shattered\",\n           \"failure_date\":\"2010.01.01\",\n     }\n     {\n           \"component_code\":\"CC012\",\n           \"failure_type\":\"cracked\",\n           \"failure_date\":\"2012.12.19\",\n     }\n     ]\n}\n<\/code><\/pre>\n\n<p>Is this possible using AWS Sagemaker or would I have to use an alternative product?<\/p>\n\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1572107649950,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":63.0,
        "Owner_creation_time":1572106621756,
        "Owner_last_access_time":1584377003532,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":1.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"UK",
        "Question_last_edit_time":1572110500476,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58572657",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: can a endpoint support nested data?; Content: this has been puzzling me for a while and i may be 'barking up the wrong tree'. we currently use to make predictions on component failures for certain products in a basic way. this is done fairly simply by training the model and passing \"modelcode, manufacture_date, component_code, failure_type\" to the endpoint. the issue is that certain products have trends in component failures and passing the above doesn't include the historic issues with a product in question. e.g. the product may have had 2 component failures that we would predict would lead to a 3rd component failure as other products have had the same issues\/trend. ideally we would pass nested json into the endpoint as follows: { \"modelcode\": \"xx001\", \"manufacturedate\": \"2008.10.08\", \"component_failures\":[ { \"component_code\":\"cc001\", \"failure_type\":\"shattered\", \"failure_date\":\"2010.01.01\", } { \"component_code\":\"cc012\", \"failure_type\":\"cracked\", \"failure_date\":\"2012.12.19\", } ] } is this possible using or would i have to use an alternative product? thanks.",
        "Question_original_content_gpt_summary":"The user is trying to determine if they can use an endpoint to support nested data in order to make predictions on component failures for certain products.",
        "Question_preprocessed_content":"Title: can a endpoint support nested data?; Content: this has been puzzling me for a while and i may be 'barking up the wrong tree'. we currently use to make predictions on component failures for certain products in a basic way. this is done fairly simply by training the model and passing modelcode, to the endpoint. the issue is that certain products have trends in component failures and passing the above doesn't include the historic issues with a product in question. the product may have had component failures that we would predict would lead to a rd component failure as other products have had the same ideally we would pass nested json into the endpoint as follows is this possible using or would i have to use an alternative product? thanks.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"\" Object of type 'int64' is not JSON serializable\" when running automl time series",
        "Question_body":"I am trying to use the Online ML studio and running an \"Automated ML\". I upload my dataset (see simple example below) which passes fine and then I start a automl experiment selecting \"time series forecasting\". I select all the revelant fields and everything starts without any issues.\n\nShortly after the process fails and the error given is:\n\n\"User error: User program failed with TypeError: Object of type 'int64' is not JSON serializable\"\n\nDigging into the logs the only log with any useful information appears to be the driver_log which has these lines with no more detail about the error unless the INFO about streaming is actually an error not information:\n\n2020-09-08 11:17:01.734 - INFO - Successfully retrieved data using dataprep.\n2020-09-08 11:17:01.734 - INFO - Streaming is not conducive due to incompatible settings. Reason[s]: [Forecasting is not supported, 'n_cross_validations' was non-empty]\n2020-09-08 11:17:01.734 - INFO - Service responded with streaming disabled\n2020-09-08 11:17:01.734 - INFO - Inferring type for feature columns.\n2020-09-08 11:17:12.669 - INFO - Error in setup_wrapper.\n2020-09-08 11:17:12.670 - ERROR - Marking Run AutoML_f5a7c759-653c-4314-98a9-c2afbcecff55_setup as Failed.\n\n\n\n\nCan anyone suggest an answer or recommend some ways to debug this?\n\n][1]",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1599571606537,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/89272\/34-object-of-type-39int6439-is-not-json-serializab.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-08T13:41:33.393Z",
                "Answer_score":0,
                "Answer_body":"found the more detailed stacktrace\n\n\"debugInfo\": {\n\"type\": \"TypeError\",\n\"message\": \"Object of type 'int64' is not JSON serializable\",\n\"stackTrace\": \" File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/brian-ml-learning\/azureml\/automl_f5a7c759-653c-4314-98a9-c2afbcecff55_setup\/mounts\/workspaceblobstore\/azureml\/AutoML_f5a7c759-653c-4314-98a9-c2afbcecff55_setup\/azureml-setup\/context_manager_injector.py\\\", line 166, in execute_with_context\\n runpy.run_path(sys.argv[0], globals(), run_name=\\\"main\\\")\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/runpy.py\\\", line 263, in run_path\\n pkg_name=pkg_name, script_name=fname)\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/runpy.py\\\", line 96, in _run_module_code\\n mod_name, mod_spec, pkg_name, script_name)\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/runpy.py\\\", line 85, in _run_code\\n exec(code, run_globals)\\n File \\\"setup_AutoML_f5a7c759-653c-4314-98a9-c2afbcecff55.py\\\", line 731, in <module>\\n result = setup_run()\\n File \\\"setup_AutoML_f5a7c759-653c-4314-98a9-c2afbcecff55.py\\\", line 725, in setup_run\\n prep_type=preparation_type\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/runtime\/_remote_script.py\\\", line 578, in setup_wrapper\\n setup_run._fail_with_error(e)\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/run.py\\\", line 1258, in _fail_with_error\\n logging_utilities.log_traceback(exception, logger)\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/shared\/logging_utilities.py\\\", line 212, in log_traceback\\n error_msg_without_pii = _get_pii_free_message(exception)\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/shared\/logging_utilities.py\\\", line 140, in _get_pii_free_message\\n return exception.get_pii_free_exception_msg_format()\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/azureml\/automl\/core\/shared\/exceptions.py\\\", line 151, in get_pii_free_exception_msg_format\\n error_dict = json.loads(self.serialize_json())\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/azureml\/common\/exceptions.py\\\", line 181, in serialize_json\\n return json.dumps(error_ret, indent=indent)\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/json\/init.py\\\", line 231, in dumps\\n return default_encoder.encode(obj)\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/json\/encoder.py\\\", line 199, in encode\\n chunks = self.iterencode(o, one_shot=True)\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/json\/encoder.py\\\", line 257, in iterencode\\n return iterencode(o, 0)\\n File \\\"\/azureml-envs\/azureml_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/json\/encoder.py\\\", line 180, in default\\n o.class.name)\\n\",\n\"innerException\": null,\n\"data\": null,\n\"errorResponse\": null\n}",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: \" object of type 'int64' is not json serializable\" when running automl time series; Content: i am trying to use the online ml studio and running an \"automated ml\". i upload my dataset (see simple example below) which passes fine and then i start a automl experiment selecting \"time series forecasting\". i select all the revelant fields and everything starts without any issues. shortly after the process fails and the error given is: \"user error: user program failed with typeerror: object of type 'int64' is not json serializable\" digging into the logs the only log with any useful information appears to be the driver_log which has these lines with no more detail about the error unless the info about streaming is actually an error not information: 2020-09-08 11:17:01.734 - info - successfully retrieved data using dataprep. 2020-09-08 11:17:01.734 - info - streaming is not conducive due to incompatible settings. reason[s]: [forecasting is not supported, 'n_cross_validations' was non-empty] 2020-09-08 11:17:01.734 - info - service responded with streaming disabled 2020-09-08 11:17:01.734 - info - inferring type for feature columns. 2020-09-08 11:17:12.669 - info - error in setup_wrapper. 2020-09-08 11:17:12.670 - error - marking run automl_f5a7c759-653c-4314-98a9-c2afbcecff55_setup as failed. can anyone suggest an answer or recommend some ways to debug this? ][1]",
        "Question_original_content_gpt_summary":"The user is encountering an error of \"object of type 'int64' is not json serializable\" when running an automated ML time series experiment in the online ML Studio.",
        "Question_preprocessed_content":"Title: object of type 'int ' is not json serializable when running automl time series; Content: i am trying to use the online ml studio and running an automated ml . i upload my dataset which passes fine and then i start a automl experiment selecting time series forecasting . i select all the revelant fields and everything starts without any issues. shortly after the process fails and the error given is user error user program failed with typeerror object of type 'int ' is not json serializable digging into the logs the only log with any useful appears to be the which has these lines with no more detail about the error unless the about streaming is actually an error not successfully retrieved data using dataprep. streaming is not conducive due to incompatible settings. reason service responded with streaming disabled inferring type for feature columns. error in error marking run as failed. can anyone suggest an answer or recommend some ways to debug this?",
        "Answer_original_content":"found the more detailed stacktrace \"debuginfo\": { \"type\": \"typeerror\", \"message\": \"object of type 'int64' is not json serializable\", \"stacktrace\": \" file \\\"\/mnt\/batch\/tasks\/shared\/ls_root\/jobs\/brian-ml-learning\/\/automl_f5a7c759-653c-4314-98a9-c2afbcecff55_setup\/mounts\/workspaceblobstore\/\/automl_f5a7c759-653c-4314-98a9-c2afbcecff55_setup\/-setup\/context_manager_injector.py\\\", line 166, in execute_with_context\\n runpy.run_path(sys.argv[0], globals(), run_name=\\\"main\\\")\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/runpy.py\\\", line 263, in run_path\\n pkg_name=pkg_name, script_name=fname)\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/runpy.py\\\", line 96, in _run_module_code\\n mod_name, mod_spec, pkg_name, script_name)\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/runpy.py\\\", line 85, in _run_code\\n exec(code, run_globals)\\n file \\\"setup_automl_f5a7c759-653c-4314-98a9-c2afbcecff55.py\\\", line 731, in \\n result = setup_run()\\n file \\\"setup_automl_f5a7c759-653c-4314-98a9-c2afbcecff55.py\\\", line 725, in setup_run\\n prep_type=preparation_type\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/\/train\/automl\/runtime\/_remote_script.py\\\", line 578, in setup_wrapper\\n setup_run._fail_with_error(e)\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/\/train\/automl\/run.py\\\", line 1258, in _fail_with_error\\n logging_utilities.log_traceback(exception, logger)\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/\/automl\/core\/shared\/logging_utilities.py\\\", line 212, in log_traceback\\n error_msg_without_pii = _get_pii_free_message(exception)\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/\/automl\/core\/shared\/logging_utilities.py\\\", line 140, in _get_pii_free_message\\n return exception.get_pii_free_exception_msg_format()\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/\/automl\/core\/shared\/exceptions.py\\\", line 151, in get_pii_free_exception_msg_format\\n error_dict = json.loads(self.serialize_json())\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/site-packages\/\/common\/exceptions.py\\\", line 181, in serialize_json\\n return json.dumps(error_ret, indent=indent)\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/json\/init.py\\\", line 231, in dumps\\n return default_encoder.encode(obj)\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/json\/encoder.py\\\", line 199, in encode\\n chunks = self.iterencode(o, one_shot=true)\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/json\/encoder.py\\\", line 257, in iterencode\\n return iterencode(o, 0)\\n file \\\"\/-envs\/_63ddfcee3da4413e556b059b99c2fb63\/lib\/python3.6\/json\/encoder.py\\\", line 180, in default\\n o.class.name)\\n\", \"innerexception\": null, \"data\": null, \"errorresponse\": null }",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"found the more detailed stacktrace debug"
    },
    {
        "Question_id":69450547.0,
        "Question_title":"WebserviceException: Unable to deploy a model with aks and azure machine learning",
        "Question_body":"<p>I tried to deploy a new model in azure databricks notebook.\nThis morning it was working and now I have the following error:<\/p>\n<p>After<\/p>\n<pre><code>service.wait_for_deployment(show_output=True)\nprint(service.state)\nprint(service.get_logs())\n<\/code><\/pre>\n<p>I have:<\/p>\n<pre><code>&quot;message&quot;: &quot;Timed out waiting for AKS deployment to complete. pollTimeout : 00:20:00 serviceName: simdev serviceId: ...&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;DeploymentTimedOut&quot;,\n      &quot;message&quot;: &quot;Your container endpoint is not available. Please follow the steps to debug:\n    1. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. Please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information.\n    2. You can also interactively debug your scoring file locally. Please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information.\n    3. View the diagnostic events to check status of container, it may help you to debug the issue.\n{&quot;InvolvedObject&quot;:&quot;simdev-757df4f999-rbcws&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Warning&quot;,&quot;Reason&quot;:&quot;FailedScheduling&quot;,&quot;Message&quot;:&quot;0\/2 nodes are available: 2 Insufficient nvidia.com\/gpu.&quot;,&quot;LastTimestamp&quot;:null}\n{&quot;InvolvedObject&quot;:&quot;simdev-757df4f999-rbcws&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Warning&quot;,&quot;Reason&quot;:&quot;FailedScheduling&quot;,&quot;Message&quot;:&quot;0\/2 nodes are available: 2 Insufficient nvidia.com\/gpu.&quot;,&quot;LastTimestamp&quot;:null}\n{&quot;InvolvedObject&quot;:&quot;simdev-757df4f999-rbcws&quot;,&quot;InvolvedKind&quot;:&quot;Pod&quot;,&quot;Type&quot;:&quot;Normal&quot;,&quot;Reason&quot;:&quot;Scheduled&quot;,&quot;Message&quot;:&quot;Successfully assigned azureml-train-aml-001-dev\/simdev-757df4f999-rbcws to aks-agentpool-34690879-vmss000000&quot;,&quot;LastTimestamp&quot;:null}\n<\/code><\/pre>\n<p>Yesterday it didn't work. This morning yes, and now no.<\/p>\n<p>Here is aks config:<\/p>\n<pre><code>aks_config = AksWebservice.deploy_configuration(cpu_cores=0.7,\n                                                memory_gb=0.7,\n                                                gpu_cores=1,\n                                                period_seconds=1800,\n                                                failure_threshold=10,\n                                                timeout_seconds=60,\n                                                max_request_wait_time=300000,\n                                                scoring_timeout_ms=300000,)\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1633437212317,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":178.0,
        "Owner_creation_time":1606642099552,
        "Owner_last_access_time":1654258609767,
        "Owner_reputation":371.0,
        "Owner_up_votes":65.0,
        "Owner_down_votes":0.0,
        "Owner_views":55.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1633437593747,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69450547",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: webserviceexception: unable to deploy a model with aks and ; Content: i tried to deploy a new model in azure databricks notebook. this morning it was working and now i have the following error: after service.wait_for_deployment(show_output=true) print(service.state) print(service.get_logs()) i have: \"message\": \"timed out waiting for aks deployment to complete. polltimeout : 00:20:00 servicename: simdev serviceid: ...\", \"details\": [ { \"code\": \"deploymenttimedout\", \"message\": \"your container endpoint is not available. please follow the steps to debug: 1. from the aml sdk, you can run print(service.get_logs()) if you have service object to fetch the logs. please refer to https:\/\/aka.ms\/debugimage#dockerlog for more information. 2. you can also interactively debug your scoring file locally. please refer to https:\/\/docs.microsoft.com\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments for more information. 3. view the diagnostic events to check status of container, it may help you to debug the issue. {\"involvedobject\":\"simdev-757df4f999-rbcws\",\"involvedkind\":\"pod\",\"type\":\"warning\",\"reason\":\"failedscheduling\",\"message\":\"0\/2 nodes are available: 2 insufficient nvidia.com\/gpu.\",\"lasttimestamp\":null} {\"involvedobject\":\"simdev-757df4f999-rbcws\",\"involvedkind\":\"pod\",\"type\":\"warning\",\"reason\":\"failedscheduling\",\"message\":\"0\/2 nodes are available: 2 insufficient nvidia.com\/gpu.\",\"lasttimestamp\":null} {\"involvedobject\":\"simdev-757df4f999-rbcws\",\"involvedkind\":\"pod\",\"type\":\"normal\",\"reason\":\"scheduled\",\"message\":\"successfully assigned -train-aml-001-dev\/simdev-757df4f999-rbcws to aks-agentpool-34690879-vmss000000\",\"lasttimestamp\":null} yesterday it didn't work. this morning yes, and now no. here is aks config: aks_config = akswebservice.deploy_configuration(cpu_cores=0.7, memory_gb=0.7, gpu_cores=1, period_seconds=1800, failure_threshold=10, timeout_seconds=60, max_request_wait_time=300000, scoring_timeout_ms=300000,)",
        "Question_original_content_gpt_summary":"The user encountered a webserviceexception when attempting to deploy a model with AKS, resulting in a timeout error.",
        "Question_preprocessed_content":"Title: webserviceexception unable to deploy a model with aks and; Content: i tried to deploy a new model in azure databricks notebook. this morning it was working and now i have the following error after i have yesterday it didn't work. this morning yes, and now no. here is aks config",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":50281188.0,
        "Question_title":"Sagemaker Java client generate IOrecord",
        "Question_body":"<p>I am trying to build a training set for Sagemaker using the Linear Learner algorithm. This algorithm supports recordIO wrapped protobuf and csv as format for the training data. As the training data is generated using spark I am having issues to generate a csv file from a dataframe (this seem broken for now), so I am trying to use protobuf. <\/p>\n\n<p>I managed to create a binary file for the training dataset using Protostuff which is a library that allows to generate protobuf messages from POJO objects. The problem is when triggering the training job I receive that message from SageMaker:\nClientError: No training data processed. Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host.<\/p>\n\n<p>The training file is certainly not null. I suspect the way I generate the training data to be incorrect as I am able to train models using the libsvm format. Is there a way to generate IOrecord using the Sagemaker java client ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1525984750483,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":222.0,
        "Owner_creation_time":1428454496052,
        "Owner_last_access_time":1570063575876,
        "Owner_reputation":35.0,
        "Owner_up_votes":1.0,
        "Owner_down_votes":0.0,
        "Owner_views":10.0,
        "Answer_body":"<p>Answering my own question. It was an issue in the algorithm configuration. I reduced mini batch size and it worked fine.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1526653073368,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50281188",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: java client generate iorecord; Content: i am trying to build a training set for using the linear learner algorithm. this algorithm supports recordio wrapped protobuf and csv as format for the training data. as the training data is generated using spark i am having issues to generate a csv file from a dataframe (this seem broken for now), so i am trying to use protobuf. i managed to create a binary file for the training dataset using protostuff which is a library that allows to generate protobuf messages from pojo objects. the problem is when triggering the training job i receive that message from : clienterror: no training data processed. either the training channel is empty or the mini-batch size is too high. verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host. the training file is certainly not null. i suspect the way i generate the training data to be incorrect as i am able to train models using the libsvm format. is there a way to generate iorecord using the java client ?",
        "Question_original_content_gpt_summary":"The user is encountering challenges generating a training set for the linear learner algorithm using a dataframe, and is having difficulty generating an iorecord using the Java client.",
        "Question_preprocessed_content":"Title: java client generate iorecord; Content: i am trying to build a training set for using the linear learner algorithm. this algorithm supports recordio wrapped protobuf and csv as format for the training data. as the training data is generated using spark i am having issues to generate a csv file from a dataframe , so i am trying to use protobuf. i managed to create a binary file for the training dataset using protostuff which is a library that allows to generate protobuf messages from pojo objects. the problem is when triggering the training job i receive that message from clienterror no training data processed. either the training channel is empty or the mini batch size is too high. verify that training data contains non empty files and the mini batch size is less than the number of records per training host. the training file is certainly not null. i suspect the way i generate the training data to be incorrect as i am able to train models using the libsvm format. is there a way to generate iorecord using the java client ?",
        "Answer_original_content":"answering my own question. it was an issue in the algorithm configuration. i reduced mini batch size and it worked fine.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"answering my own question. it was an issue in the algorithm configuration. i reduced mini batch size and it worked fine."
    },
    {
        "Question_id":null,
        "Question_title":"Rewriting git history and moving binaries to dvc",
        "Question_body":"<p>Are there any tools or guides for rewriting a git repo containing binaries in the git history to dvc based storage?<\/p>\n<p>I can delete the existing binary files and use dvc and S3 to store them but would like to be able to rewrite the git history.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":null,
        "Question_creation_time":1636163254101,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":192.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/rewriting-git-history-and-moving-binaries-to-dvc\/953",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-06T01:53:09.803Z",
                "Answer_body":"<p>For myself, I would use <code>git rebase -i &lt;references&gt;<\/code> and manually modify each of the commits.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-06T02:01:54.088Z",
                "Answer_body":"<p>You might want to take a look at the tool like this - <a href=\"https:\/\/rtyley.github.io\/bfg-repo-cleaner\/\" class=\"inline-onebox\">BFG Repo-Cleaner by rtyley<\/a> .Or this link covers a few other options I think - <a href=\"https:\/\/stackoverflow.com\/questions\/2100907\/how-to-remove-delete-a-large-file-from-commit-history-in-the-git-repository\" class=\"inline-onebox\">version control - How to remove\/delete a large file from commit history in the Git repository? - Stack Overflow<\/a><\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-03T17:26:39.212Z",
                "Answer_body":"<p>Thanks for your suggestions. BFG has been superseded by git filter-repo.<br>\nI\u2019ve used BFG and it was a pain compared to git filter-repo, so I strongly suggest that everyone use git filter-repo.<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/github.com\/newren\/git-filter-repo\">\n  <header class=\"source\">\n      <img src=\"https:\/\/github.githubassets.com\/favicons\/favicon.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/github.com\/newren\/git-filter-repo\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/345;\"><img src=\"https:\/\/opengraph.githubassets.com\/79ebc087cf5cd17f3665fd0475d223fc4c588ec2b85cacb7ba8cef90890ad6af\/newren\/git-filter-repo\" class=\"thumbnail\" width=\"690\" height=\"345\"><\/div>\n\n<h3><a href=\"https:\/\/github.com\/newren\/git-filter-repo\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - newren\/git-filter-repo: Quickly rewrite git repository history...<\/a><\/h3>\n\n  <p>Quickly rewrite git repository history (filter-branch replacement) - GitHub - newren\/git-filter-repo: Quickly rewrite git repository history (filter-branch replacement)<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>git filter-repo has the ability to be extended so there is probably a way to use it to find large blobs matching a file path or file pattern and execute the git commands to remove the blob, add them to dvc, and move on. But I haven\u2019t done that yet.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: rewriting git history and moving binaries to ; Content: are there any tools or guides for rewriting a git repo containing binaries in the git history to based storage? i can delete the existing binary files and use and s3 to store them but would like to be able to rewrite the git history.",
        "Question_original_content_gpt_summary":"The user is looking for tools or guides to help them rewrite a git repository containing binaries in the git history to a cloud-based storage solution.",
        "Question_preprocessed_content":"Title: rewriting git history and moving binaries to; Content: are there any tools or guides for rewriting a git repo containing binaries in the git history to based storage? i can delete the existing binary files and use and s to store them but would like to be able to rewrite the git history.",
        "Answer_original_content":"for myself, i would use git rebase -i <references> and manually modify each of the commits. you might want to take a look at the tool like this - bfg repo-cleaner by rtyley .or this link covers a few other options i think - version control - how to remove\/delete a large file from commit history in the git repository? - stack overflow thanks for your suggestions. bfg has been superseded by git filter-repo. ive used bfg and it was a pain compared to git filter-repo, so i strongly suggest that everyone use git filter-repo. github github - newren\/git-filter-repo: quickly rewrite git repository history... quickly rewrite git repository history (filter-branch replacement) - github - newren\/git-filter-repo: quickly rewrite git repository history (filter-branch replacement) git filter-repo has the ability to be extended so there is probably a way to use it to find large blobs matching a file path or file pattern and execute the git commands to remove the blob, add them to , and move on. but i havent done that yet.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"for myself, i would use and manually modify each of the commits. you might want to take a look at the tool like this bfg repo cleaner by rtyley .or this link covers a few other options i think version control how to a large file from commit history in the git repository? stack overflow thanks for your suggestions. bfg has been superseded by git filter repo. ive used bfg and it was a pain compared to git filter repo, so i strongly suggest that everyone use git filter repo. github github quickly rewrite git repository quickly rewrite git repository history github quickly rewrite git repository history git filter repo has the ability to be extended so there is probably a way to use it to find large blobs matching a file path or file pattern and execute the git commands to remove the blob, add them to , and move on. but i havent done that yet."
    },
    {
        "Question_id":72729259.0,
        "Question_title":"wandb.Table raises error: AssertionError: columns argument expects a `list` object",
        "Question_body":"<p>I'm very beginner with wandb , so this is very basic question.\nI have dataframe which has my x features and y values.\nI'm tryin to follow <a href=\"https:\/\/docs.wandb.ai\/examples\" rel=\"nofollow noreferrer\">this tutorial<\/a>  to train model from my pandas dataframe . However, when I try to create wandb table from my pandas dataframe, I get an error:<\/p>\n<pre><code>\nwandb.init(project='my-xgb', config={'lr': 0.01})\n\n#the log didn't work  so I haven't run it at the moment (the log 'loss') \n#wandb.log({'loss': loss, ...})\n\n\n# Create a W&amp;B Table with your pandas dataframe\ntable = wandb.Table(df1)\n<\/code><\/pre>\n<blockquote>\n<p>AssertionError: columns argument expects a <code>list<\/code> object<\/p>\n<\/blockquote>\n<p>I have no idea why is this happen, and why it excpect a list. In the tutorial it doesn't look like the dataframe is list.<\/p>\n<p>My end goal - to be able to create wandb table.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1655983093280,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":63.0,
        "Owner_creation_time":1572256318027,
        "Owner_last_access_time":1663676968963,
        "Owner_reputation":1387.0,
        "Owner_up_votes":955.0,
        "Owner_down_votes":16.0,
        "Owner_views":224.0,
        "Answer_body":"<p><strong>Short answer<\/strong>: <code>table = wandb.Table(dataframe=my_df)<\/code>.<\/p>\n<p>The explanation of your specific case is at the bottom.<\/p>\n<hr \/>\n<p><strong>Minimal example<\/strong> of using <code>wandb.Table<\/code> with a DataFrame:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import wandb\nimport pandas as pd\n\niris_path = 'https:\/\/raw.githubusercontent.com\/mwaskom\/seaborn-data\/master\/iris.csv'\niris = pd.read_csv(iris_path)\ntable = wandb.Table(dataframe=iris)\nwandb.log({'dataframe_in_table': table})\n<\/code><\/pre>\n<p>(Here the dataset is called the Iris dataset that consists of &quot;3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray&quot;)<\/p>\n<p>There are two ways of creating W&amp;B <code>Table<\/code>s according to <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\" rel=\"nofollow noreferrer\">the official documentation<\/a>:<\/p>\n<ul>\n<li><strong>List of Rows<\/strong>: Log named columns and rows of data. For example: <code>wandb.Table(columns=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], data=[[&quot;1a&quot;, &quot;1b&quot;, &quot;1c&quot;], [&quot;2a&quot;, &quot;2b&quot;, &quot;2c&quot;]])<\/code> generates a table with two rows and three columns.<\/li>\n<li><strong>Pandas DataFrame<\/strong>: Log a DataFrame using <code>wandb.Table(dataframe=my_df)<\/code>. Column names will be extracted from the DataFrame.<\/li>\n<\/ul>\n<hr \/>\n<p><strong>Explanation<\/strong>: Why <code>table = wandb.Table(my_df)<\/code> gives error &quot;columns argument expects a <code>list<\/code> object&quot;? Because <code>wandb.Table<\/code>'s init function looks like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def __init__(\n        self,\n        columns=None,\n        data=None,\n        rows=None,\n        dataframe=None,\n        dtype=None,\n        optional=True,\n        allow_mixed_types=False,\n    ):\n<\/code><\/pre>\n<p>If one passes a DataFrame without telling it's a DataFrame, <code>wandb.Table<\/code> will assume the argument is <code>columns<\/code>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1655983680023,
        "Answer_score":2.0,
        "Owner_location":"Israel",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1655985859240,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72729259",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: .table raises error: assertionerror: columns argument expects a `list` object; Content: i'm very beginner with , so this is very basic question. i have dataframe which has my x features and y values. i'm tryin to follow this tutorial to train model from my pandas dataframe . however, when i try to create table from my pandas dataframe, i get an error: .init(project='my-xgb', config={'lr': 0.01}) #the log didn't work so i haven't run it at the moment (the log 'loss') #.log({'loss': loss, ...}) # create a w&b table with your pandas dataframe table = .table(df1) assertionerror: columns argument expects a list object i have no idea why is this happen, and why it excpect a list. in the tutorial it doesn't look like the dataframe is list. my end goal - to be able to create table.",
        "Question_original_content_gpt_summary":"The user is encountering an assertionerror when attempting to create a table from their pandas dataframe, as the columns argument expects a `list` object.",
        "Question_preprocessed_content":"Title: raises error assertionerror columns argument expects a object; Content: i'm very beginner with , so this is very basic question. i have dataframe which has my x features and y values. i'm tryin to follow this tutorial to train model from my pandas dataframe . however, when i try to create table from my pandas dataframe, i get an error assertionerror columns argument expects a object i have no idea why is this happen, and why it excpect a list. in the tutorial it doesn't look like the dataframe is list. my end goal to be able to create table.",
        "Answer_original_content":"short answer: table = .table(dataframe=my_df). the explanation of your specific case is at the bottom. minimal example of using .table with a dataframe: import import pandas as pd iris_path = 'https:\/\/raw.githubusercontent.com\/mwaskom\/seaborn-data\/master\/iris.csv' iris = pd.read_csv(iris_path) table = .table(dataframe=iris) .log({'dataframe_in_table': table}) (here the dataset is called the iris dataset that consists of \"3 different types of irises (setosa, versicolour, and virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\") there are two ways of creating w&b tables according to the official documentation: list of rows: log named columns and rows of data. for example: .table(columns=[\"a\", \"b\", \"c\"], data=[[\"1a\", \"1b\", \"1c\"], [\"2a\", \"2b\", \"2c\"]]) generates a table with two rows and three columns. pandas dataframe: log a dataframe using .table(dataframe=my_df). column names will be extracted from the dataframe. explanation: why table = .table(my_df) gives error \"columns argument expects a list object\"? because .table's init function looks like this: def __init__( self, columns=none, data=none, rows=none, dataframe=none, dtype=none, optional=true, allow_mixed_types=false, ): if one passes a dataframe without telling it's a dataframe, .table will assume the argument is columns.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"short answer . the explanation of your specific case is at the bottom. minimal example of using with a dataframe here the dataset is called the iris dataset that consists of different types of irises petal and sepal length, stored in a x there are two ways of creating w&b s according to the official documentation list of rows log named columns and rows of data. for example generates a table with two rows and three columns. pandas dataframe log a dataframe using . column names will be extracted from the dataframe. explanation why gives error columns argument expects a object ? because 's init function looks like this if one passes a dataframe without telling it's a dataframe, will assume the argument is ."
    },
    {
        "Question_id":71859083.0,
        "Question_title":"Run script inside Docker container using Azure Machine Learning",
        "Question_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/overview-what-is-azure-machine-learning\" rel=\"nofollow noreferrer\">Azure Machine Learning<\/a> provides provides encapsulation of the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments\" rel=\"nofollow noreferrer\">environment<\/a> for your code to run. As far as I know you can specify custom Docker images and Dockerfiles to create an environment.<\/p>\n<p>But in my specific use case, I want to run the script inside a specific Docker container. Does Azure ML provide some ways to run a script inside a custom Docker container?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1649859511153,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":97.0,
        "Owner_creation_time":1458750541110,
        "Owner_last_access_time":1663754983956,
        "Owner_reputation":360.0,
        "Owner_up_votes":38.0,
        "Owner_down_votes":1.0,
        "Owner_views":35.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Essen, Germany",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71859083",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: run script inside docker container using ; Content: provides provides encapsulation of the environment for your code to run. as far as i know you can specify custom docker images and dockerfiles to create an environment. but in my specific use case, i want to run the script inside a specific docker container. does provide some ways to run a script inside a custom docker container?",
        "Question_original_content_gpt_summary":"The user is looking for a way to run a script inside a custom Docker container using Azure Machine Learning.",
        "Question_preprocessed_content":"Title: run script inside docker container using; Content: provides provides encapsulation of the environment for your code to run. as far as i know you can specify custom docker images and dockerfiles to create an environment. but in my specific use case, i want to run the script inside a specific docker container. does provide some ways to run a script inside a custom docker container?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Jupyter notebook",
        "Question_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1606707121000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-30T04:05:24.000Z",
                "Answer_score":0,
                "Answer_body":"Some useful points:\n\nThe typical arguments of cloud vs local will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\nSageMaker notebooks already run in an explicit IAM context (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run sagemaker.get_execution_role()\nPre-built environments for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\nLinux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\nIf you started using SageMaker Studio, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: jupyter notebook; Content: what are the advantages of using jupyter instance instead of running it locally? is there a special integration with that we lose it if we do not use jupyer instance?",
        "Question_original_content_gpt_summary":"The user is questioning the advantages of using a Jupyter instance instead of running it locally, and whether there is a special integration with Jupyter that would be lost if they do not use the instance.",
        "Question_preprocessed_content":"Title: jupyter notebook; Content: what are the advantages of using jupyter instance instead of running it locally? is there a special integration with that we lose it if we do not use jupyer instance?",
        "Answer_original_content":"some useful points: the typical arguments of cloud vs local will apply (as with e.g. cloud9, workspaces, etc): can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc. notebooks already run in an explicit iam context (via assigned execution role) - so you don't need to log in e.g. as you would through the cli on local machine... can just run .get_execution_role() pre-built environments for a range of use-cases (e.g. generic data science, tensorflow, pytorch, mxnet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop. linux-based environments, which typically makes for a shorter path to production code than mac\/windows. if you started using studio, then yes there are some native integrations such as the uis for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"some useful points the typical arguments of cloud vs local will apply can de couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right size the environment for what workloads you need to do on a given day, etc. notebooks already run in an explicit iam context so you don't need to log in as you would through the cli on local can just run pre built environments for a range of use cases with libraries already installed, and easy of the environment by stopping & starting the instance no more environment soup on your local laptop. linux based environments, which typically makes for a shorter path to production code than if you started using studio, then yes there are some native integrations such as the uis for experiment tracking and endpoint easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks."
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML: Creating a Siamese network, I have the reference data stored in one blob storage and the user input stored in another blob storage. How can I connect the two blob storage to my batch endpoint. If not, is there a workaround?",
        "Question_body":"Azure ML: Creating a Siamese network, I have the reference data stored in one blob storage and the user input stored in another blob storage. How can I connect the two blob storage to my batch endpoint. If not, is there a workaround? The setup looks like this.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1662886746297,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1002087\/azure-ml-creating-a-siamese-network-i-have-the-ref.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-12T12:04:20.93Z",
                "Answer_score":0,
                "Answer_body":"@SamarjeetSinghPatil-6739 I think this should be possible where your input could be a different storage account and your output could be the default datastore.\n\n export OUTPUT_FILE_NAME=predictions_`echo $RANDOM`.csv\n JOB_NAME=$(az ml batch-endpoint invoke --name $ENDPOINT_NAME --input https:\/\/pipelinedata.blob.core.windows.net\/sampledata\/mnist --input-type uri_folder --output-path azureml:\/\/datastores\/workspaceblobstore\/paths\/$ENDPOINT_NAME --set output_file_name=$OUTPUT_FILE_NAME --mini-batch-size 20 --instance-count 5 --query name -o tsv)\n\n\n\nRef: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-batch-endpoint#configure-the-output-location-and-overwrite-settings\n\nPlease refer some other scenarios that you can set with the input and output path while using the batch endpoint invoke command.\n\n az ml batch-endpoint invoke --help",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: : creating a siamese network, i have the reference data stored in one blob storage and the user input stored in another blob storage. how can i connect the two blob storage to my batch endpoint. if not, is there a workaround?; Content: : creating a siamese network, i have the reference data stored in one blob storage and the user input stored in another blob storage. how can i connect the two blob storage to my batch endpoint. if not, is there a workaround? the setup looks like this.",
        "Question_original_content_gpt_summary":"The user is facing a challenge of connecting two blob storages to a batch endpoint in order to create a siamese network.",
        "Question_preprocessed_content":"Title: creating a siamese network, i have the reference data stored in one blob storage and the user input stored in another blob storage. how can i connect the two blob storage to my batch endpoint. if not, is there a workaround?; Content: creating a siamese network, i have the reference data stored in one blob storage and the user input stored in another blob storage. how can i connect the two blob storage to my batch endpoint. if not, is there a workaround? the setup looks like this.",
        "Answer_original_content":"@samarjeetsinghpatil-6739 i think this should be possible where your input could be a different storage account and your output could be the default datastore. export output_file_name=predictions_`echo $random`.csv job_name=$(az ml batch-endpoint invoke --name $endpoint_name --input https:\/\/pipelinedata.blob.core.windows.net\/sampledata\/mnist --input-type uri_folder --output-path :\/\/datastores\/workspaceblobstore\/paths\/$endpoint_name --set output_file_name=$output_file_name --mini-batch-size 20 --instance-count 5 --query name -o tsv) ref: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-batch-endpoint#configure-the-output-location-and-overwrite-settings please refer some other scenarios that you can set with the input and output path while using the batch endpoint invoke command. az ml batch-endpoint invoke --help",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i think this should be possible where your input could be a different storage account and your output could be the default datastore. export ml batch endpoint invoke name input input type output path set mini batch size instance count query name o tsv ref please refer some other scenarios that you can set with the input and output path while using the batch endpoint invoke command. az ml batch endpoint invoke help"
    },
    {
        "Question_id":null,
        "Question_title":"Why my testing always failed in Custom Speech?",
        "Question_body":"",
        "Question_answer_count":0,
        "Question_comment_count":3.0,
        "Question_creation_time":1608793221783,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/209520\/why-my-testing-always-failed-in-custom-speech.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[

        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why my testing always failed in custom speech?; Content:",
        "Question_original_content_gpt_summary":"The user encountered challenges with their custom speech tests failing consistently.",
        "Question_preprocessed_content":"Title: why my testing always failed in custom speech?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":65421005.0,
        "Question_title":"how to save uncompressed outputs from a training job in using aws Sagemaker python SDK?",
        "Question_body":"<p>I'm trying to upload training job artifacts to S3 in a non-compressed manner.<\/p>\n<p>I am familiar with the output_dir one can provide to a sagemaker Estimator, then everything saved under \/opt\/ml\/output is uploaded compressed to the S3 output dir.<\/p>\n<p>I want to have the option to access a specific artifact without having to decompress the output every time. Is there a clean way to go about it? if not any workaround in mind?\nThe artifacts of my interest are small meta-data files .txt or .csv, while in my case the rest of the artifacts can be ~1GB so downloading and decompressing is quite excessive.<\/p>\n<p>any help would be appreciated<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1608711307120,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":313.0,
        "Owner_creation_time":1557646363768,
        "Owner_last_access_time":1663926361952,
        "Owner_reputation":51.0,
        "Owner_up_votes":34.0,
        "Owner_down_votes":0.0,
        "Owner_views":14.0,
        "Answer_body":"<p>I ended up using the checkpoint path that is by default being synced with the specified S3 path in an uncompressed manner.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1612085605603,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65421005",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to save uncompressed outputs from a training job in using python sdk?; Content: i'm trying to upload training job artifacts to s3 in a non-compressed manner. i am familiar with the output_dir one can provide to a estimator, then everything saved under \/opt\/ml\/output is uploaded compressed to the s3 output dir. i want to have the option to access a specific artifact without having to decompress the output every time. is there a clean way to go about it? if not any workaround in mind? the artifacts of my interest are small meta-data files .txt or .csv, while in my case the rest of the artifacts can be ~1gb so downloading and decompressing is quite excessive. any help would be appreciated",
        "Question_original_content_gpt_summary":"The user is looking for a way to save uncompressed outputs from a training job in Python SDK, as downloading and decompressing the artifacts of interest (small meta-data files .txt or .csv) is quite excessive.",
        "Question_preprocessed_content":"Title: how to save uncompressed outputs from a training job in using python sdk?; Content: i'm trying to upload training job artifacts to s in a non compressed manner. i am familiar with the one can provide to a estimator, then everything saved under is uploaded compressed to the s output dir. i want to have the option to access a specific artifact without having to decompress the output every time. is there a clean way to go about it? if not any workaround in mind? the artifacts of my interest are small meta data files .txt or .csv, while in my case the rest of the artifacts can be gb so downloading and decompressing is quite excessive. any help would be appreciated",
        "Answer_original_content":"i ended up using the checkpoint path that is by default being synced with the specified s3 path in an uncompressed manner.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"i ended up using the checkpoint path that is by default being synced with the specified s path in an uncompressed manner."
    },
    {
        "Question_id":null,
        "Question_title":"how to share project to my team",
        "Question_body":"I want to share my whole project to my team in CLI, how to do that, can\u2019t find any document about that.",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1669425813897,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1105193\/how-to-share-project-to-my-team.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-26T10:40:35.147Z",
                "Answer_score":0,
                "Answer_body":"Hello @sursalaterrano-8475\n\nThanks for reaching out to us, which version you are working on?\n\nFor Azure Machine Learning CLI V1, you should use az ml workspace share commands\n\nFor Azure Machine Learning CLI V2, you should use az role assignment create commands\n\nPlease refer to below document -\n\nhttps:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml\/workspace?view=azure-cli-latest\n\nhttps:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml(v1)\/workspace?view=azure-cli-latest\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to share project to my team; Content: i want to share my whole project to my team in cli, how to do that, can\u2019t find any document about that.",
        "Question_original_content_gpt_summary":"The user is looking for a way to share their project with their team using the command line interface, but is having difficulty finding any documentation on how to do so.",
        "Question_preprocessed_content":"Title: how to share project to my team; Content: i want to share my whole project to my team in cli, how to do that, cant find any document about that.",
        "Answer_original_content":"hello @sursalaterrano-8475 thanks for reaching out to us, which version you are working on? for cli v1, you should use az ml workspace share commands for cli v2, you should use az role assignment create commands please refer to below document - https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml\/workspace?view=azure-cli-latest https:\/\/learn.microsoft.com\/en-us\/cli\/azure\/ml(v1)\/workspace?view=azure-cli-latest i hope this helps. regards, yutong -please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello thanks for reaching out to us, which version you are working on? for cli v , you should use az ml workspace share commands for cli v , you should use az role assignment create commands please refer to below document i hope this helps. regards, yutong please kindly accept the answer if you feel helpful to support the community, thanks a lot."
    },
    {
        "Question_id":null,
        "Question_title":"What are the recommended practices on how to use DDP with wandb?",
        "Question_body":"<p>I often use distributed data laoders in pytorch DDP and often just check the rank and have only rank 0 log. Is that the recommended way to use DDP and wandb?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1631289824937,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":353.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/what-are-the-recommended-practices-on-how-to-use-ddp-with-wandb\/502",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-13T17:17:20.017Z",
                "Answer_body":"<p><a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/distributed-training#logging-distributed-training-experiments-with-w-and-b\">Here<\/a> is link to the docs sharing the relevant suggestions. Please let me know incase you have any follow up Qs <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":32.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:05.406Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: what are the recommended practices on how to use ddp with ?; Content: i often use distributed data laoders in pytorch ddp and often just check the rank and have only rank 0 log. is that the recommended way to use ddp and ? thanks!",
        "Question_original_content_gpt_summary":"The user is seeking advice on the recommended practices for using distributed data loaders in PyTorch DDP.",
        "Question_preprocessed_content":"Title: what are the recommended practices on how to use ddp with ?; Content: i often use distributed data laoders in pytorch ddp and often just check the rank and have only rank log. is that the recommended way to use ddp and ? thanks!",
        "Answer_original_content":"here is link to the docs sharing the relevant suggestions. please let me know incase you have any follow up qs this topic was automatically closed 60 days after the last reply. new replies are no longer allowed.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"here is link to the docs sharing the relevant suggestions. please let me know incase you have any follow up qs this topic was automatically closed days after the last reply. new replies are no longer allowed."
    },
    {
        "Question_id":null,
        "Question_title":"Serverless Inference - Limit number of workers",
        "Question_body":"We've deployed a HuggingFace model to Sagemaker as a serverless endpoint. We set memory to be 6GB and max concurrency to be 1. With these settings, we keep getting errors when we call invoke_endpoint. Not all the time, but about 60% of the time...\n\nWhen we check the logs and metrics, we see that the memory has gone up to almost 100%. We also see that, since the machine has 6 CPUs, if starts 6 workers. We believe this could be the cause of the problem. How can se set the number of workers?\n\nThanks!",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1642602434394,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":161.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWYP78UdYQseoErcj4kjiug\/serverless-inference-limit-number-of-workers",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-19T17:44:50.854Z",
                "Answer_score":0,
                "Answer_body":"From \u201csagemaker.pytorch.model.PyTorchModel\u201d documentation:\n\nmodel_server_workers (int) \u2013 Optional. The number of worker processes used by the inference server. If None, server will use one worker per vCPU.\n\nYou can see this example on how to set \u201cMODEL_SERVER_WORKERS\u201d environment variable to set number of workers.\n\nenv={\n    \"MODEL_SERVER_WORKERS\":\"2\"\n    }\n\nlocal_regressor = Estimator(\n    image,\n    role,\n    instance_count=1,\n    instance_type=\"local\")\n\ntrain_location = 'file:\/\/'+local_train\nvalidation_location = 'file:\/\/'+local_validation\nlocal_regressor.fit({'train':train_location, 'validation': validation_location}, logs=True)\n\npredictor = local_regressor.deploy(1, 'local', serializer=csv_serializer, env=env)\n\n\nHope it helps.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-19T16:04:33.089Z",
                "Answer_score":0,
                "Answer_body":"Eitan, thanks for replying.\n\nI'm not sure if this worked or not, as not the cloudwatch logs are not showing the number of workers anymore! The performance seems to be the same, however. It's failing more often than it's responding. And still reaching almost 100% memory.\n\nInstead of your code, I used the following, as I'm deploying a Hugging Face model:\n\nhuggingface_model = HuggingFaceModel(\n    name=model_name,\n    model_data=os.path.join(\"s3:\/\/\" + tar_bucket_name, tarfile_name),\n    env={\n        'HF_TASK': 'text-classification',\n        'MODEL_SERVER_WORKERS': '1',\n        'MODEL_SERVER_TIMEOUT': '300'\n    },\n    role=sagemaker.get_execution_role(),\n    entry_point='inference.py',\n    transformers_version='4.12.3',\n    pytorch_version='1.9.1',\n    py_version='py38'\n)\n\n\nTwo follow up questions then, if you don't mind:\n\nHow can I see if the serverless function actually created only one worker per instance?\nWhere can I find all the different environment variables accepted by SageMaker?\n\nMany thanks!\n\nRogerio",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: serverless inference - limit number of workers; Content: we've deployed a huggingface model to as a serverless endpoint. we set memory to be 6gb and max concurrency to be 1. with these settings, we keep getting errors when we call invoke_endpoint. not all the time, but about 60% of the time... when we check the logs and metrics, we see that the memory has gone up to almost 100%. we also see that, since the machine has 6 cpus, if starts 6 workers. we believe this could be the cause of the problem. how can se set the number of workers? thanks!",
        "Question_original_content_gpt_summary":"The user is encountering challenges with their serverless inference deployment, as they are receiving errors when calling invoke_endpoint and seeing that the memory has gone up to almost 100%, likely due to the machine starting 6 workers.",
        "Question_preprocessed_content":"Title: serverless inference limit number of workers; Content: we've deployed a huggingface model to as a serverless endpoint. we set memory to be gb and max concurrency to be . with these settings, we keep getting errors when we call not all the time, but about % of the when we check the logs and metrics, we see that the memory has gone up to almost %. we also see that, since the machine has cpus, if starts workers. we believe this could be the cause of the problem. how can se set the number of workers? thanks!",
        "Answer_original_content":"from .pytorch.model.pytorchmodel documentation: model_server_workers (int) optional. the number of worker processes used by the inference server. if none, server will use one worker per vcpu. you can see this example on how to set model_server_workers environment variable to set number of workers. env={ \"model_server_workers\":\"2\" } local_regressor = estimator( image, role, instance_count=1, instance_type=\"local\") train_location = 'file:\/\/'+local_train validation_location = 'file:\/\/'+local_validation local_regressor.fit({'train':train_location, 'validation': validation_location}, logs=true) predictor = local_regressor.deploy(1, 'local', serializer=csv_serializer, env=env) hope it helps. eitan, thanks for replying. i'm not sure if this worked or not, as not the cloudwatch logs are not showing the number of workers anymore! the performance seems to be the same, however. it's failing more often than it's responding. and still reaching almost 100% memory. instead of your code, i used the following, as i'm deploying a hugging face model: huggingface_model = huggingfacemodel( name=model_name, model_data=os.path.join(\"s3:\/\/\" + tar_bucket_name, tarfile_name), env={ 'hf_task': 'text-classification', 'model_server_workers': '1', 'model_server_timeout': '300' }, role=.get_execution_role(), entry_point='inference.py', transformers_version='4.12.3', pytorch_version='1.9.1', py_version='py38' ) two follow up questions then, if you don't mind: how can i see if the serverless function actually created only one worker per instance? where can i find all the different environment variables accepted by ? many thanks! rogerio",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"from documentation optional. the number of worker processes used by the inference server. if none, server will use one worker per vcpu. you can see this example on how to set environment variable to set number of workers. env estimator 'validation' logs true predictor 'local', env env hope it helps. eitan, thanks for replying. i'm not sure if this worked or not, as not the cloudwatch logs are not showing the number of workers anymore! the performance seems to be the same, however. it's failing more often than it's responding. and still reaching almost % memory. instead of your code, i used the following, as i'm deploying a hugging face model huggingfacemodel , env 'text classification', ' ', ' ' , two follow up questions then, if you don't mind how can i see if the serverless function actually created only one worker per instance? where can i find all the different environment variables accepted by ? many thanks! rogerio"
    },
    {
        "Question_id":null,
        "Question_title":"How to export tresained models to ECR as container image",
        "Question_body":"I want to train and build the model in Sagemaker studio and then be able to export the model as a container image to ECR, so I can use the model in external platform by sharing the ECR image to another account where I Can create container with the image from ECR",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1663258467464,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZHWz5-hpSc-80dEIkuxwQw\/how-to-export-tresained-models-to-ecr-as-container-image",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-16T23:05:33.114Z",
                "Answer_score":0,
                "Answer_body":"The models you train in SageMaker are stored in S3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). If you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any AWS account.\n\nIf you are using a custom training image (docs here), you can push this image to ECR and allow a second account to pull the image and then use the image with the model that you have trained. However, note that Studio at this time does not support building Docker images out of the box. You can use SageMaker Notebook Instances instead.\n\nI would recommend keeping the model (.tar.gz) and the image (Docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to export tresained models to ecr as container image; Content: i want to train and build the model in studio and then be able to export the model as a container image to ecr, so i can use the model in external platform by sharing the ecr image to another account where i can create container with the image from ecr",
        "Question_original_content_gpt_summary":"The user wants to train and build a model in Studio and then export it as a container image to ECR, so they can use the model in an external platform by sharing the ECR image to another account.",
        "Question_preprocessed_content":"Title: how to export tresained models to ecr as container image; Content: i want to train and build the model in studio and then be able to export the model as a container image to ecr, so i can use the model in external platform by sharing the ecr image to another account where i can create container with the image from ecr",
        "Answer_original_content":"the models you train in are stored in s3 as .tar.gz files that you can use to deploy to an endpoint, or even test locally (extracting the model file from the tar file). if you are using a built-in algorithm, you can share the .tar.gz file to the second account and deploy the model in the second account, since built-in algorithm containers can be accessed from any aws account. if you are using a custom training image (docs here), you can push this image to ecr and allow a second account to pull the image and then use the image with the model that you have trained. however, note that studio at this time does not support building docker images out of the box. you can use notebook instances instead. i would recommend keeping the model (.tar.gz) and the image (docker) separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"the models you train in are stored in s as files that you can use to deploy to an endpoint, or even test locally . if you are using a built in algorithm, you can share the file to the second account and deploy the model in the second account, since built in algorithm containers can be accessed from any aws account. if you are using a custom training image , you can push this image to ecr and allow a second account to pull the image and then use the image with the model that you have trained. however, note that studio at this time does not support building docker images out of the box. you can use notebook instances instead. i would recommend keeping the model and the image separate, since you can easily retrain and deploy the newer versions of models without updating the image every single time."
    },
    {
        "Question_id":50253681.0,
        "Question_title":"Amazon SageMaker BlazingText",
        "Question_body":"<p>I am working on a word embedding project. I am using Amazon SageMaker for this purpose. The BlazingText algorithm in the Amazon SageMaker produced fast result than the other options. But I don't see any facility to get the prediction model or the weights. The output consists only the vectors file from which I cannot generate the model.\nIs there any way by which I can get the model with the vector file? I need this to predict new words. Thanks in advance.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3.0,
        "Question_creation_time":1525869601293,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":2223.0,
        "Owner_creation_time":1480655238883,
        "Owner_last_access_time":1638959948943,
        "Owner_reputation":27.0,
        "Owner_up_votes":5.0,
        "Owner_down_votes":0.0,
        "Owner_views":12.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Sri Lanka",
        "Question_last_edit_time":1525869863667,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50253681",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: blazingtext; Content: i am working on a word embedding project. i am using for this purpose. the blazingtext algorithm in the produced fast result than the other options. but i don't see any facility to get the prediction model or the weights. the output consists only the vectors file from which i cannot generate the model. is there any way by which i can get the model with the vector file? i need this to predict new words. thanks in advance.",
        "Question_original_content_gpt_summary":"The user is encountering a challenge with the BlazingText algorithm, as they are unable to generate a prediction model or weights from the output vector file.",
        "Question_preprocessed_content":"Title: blazingtext; Content: i am working on a word embedding project. i am using for this purpose. the blazingtext algorithm in the produced fast result than the other options. but i don't see any facility to get the prediction model or the weights. the output consists only the vectors file from which i cannot generate the model. is there any way by which i can get the model with the vector file? i need this to predict new words. thanks in advance.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":66932214.0,
        "Question_title":"why can't I see Train Model results in Azure ML Designer?",
        "Question_body":"<p>I created data from 1000 sessions of a board game simulator I ran. I'm trying to figure out what the winning strategies are and tracked several features in the data.<\/p>\n<p>I loaded the result in a Azure Machine Learning diagram and connected the data set to a model that uses linear regression.<\/p>\n<p>I click the &quot;Train Model&quot; and go to &quot;View Output&quot;. After clicking through the ensuing links, I seem to be able to locate 9 files. I don't see anything that looks like, &quot;column 9 is best predictor of column 1&quot; or something like that.<\/p>\n<p>Instead I see an iLearner file with a lot of binary I can't read. I see a schema file. There's also a lot of meta files about what version of conda ran it and data types and stuff.<\/p>\n<p>How do I see which features best indicated the label I indicated?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wO0eW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wO0eW.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><strong>EDIT:<\/strong><\/p>\n<p>As suggested, I added score model and evaluate model.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/emFsG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/emFsG.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I did see some error metrics in the evaluate results -&gt; visualize.<\/p>\n<p>Train model had a view output and a view log, but no visualize for me. When I went to &quot;view output&quot; there were a lot of files like convert_to_dataset.yaml and boosted_decision_tree_regression.yaml. Also there was a directory there called trained model which had files with names like data_type.json and score.py. It seemed like it was all meta data and nothing like, &quot;Column 1 best predicted X ...&quot;.<\/p>\n<p>I am still not seeing anything that indicates what best predicts the outcome.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/knkUR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/knkUR.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z8E6y.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z8E6y.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1617459748413,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":354.0,
        "Owner_creation_time":1304399536083,
        "Owner_last_access_time":1663946253027,
        "Owner_reputation":2032.0,
        "Owner_up_votes":996.0,
        "Owner_down_votes":3.0,
        "Owner_views":515.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Raleigh, NC",
        "Question_last_edit_time":1618582754150,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66932214",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: why can't i see train model results in designer?; Content: i created data from 1000 sessions of a board game simulator i ran. i'm trying to figure out what the winning strategies are and tracked several features in the data. i loaded the result in a diagram and connected the data set to a model that uses linear regression. i click the \"train model\" and go to \"view output\". after clicking through the ensuing links, i seem to be able to locate 9 files. i don't see anything that looks like, \"column 9 is best predictor of column 1\" or something like that. instead i see an ilearner file with a lot of binary i can't read. i see a schema file. there's also a lot of meta files about what version of conda ran it and data types and stuff. how do i see which features best indicated the label i indicated? edit: as suggested, i added score model and evaluate model. i did see some error metrics in the evaluate results -> visualize. train model had a view output and a view log, but no visualize for me. when i went to \"view output\" there were a lot of files like convert_to_dataset.yaml and boosted_decision_tree_regression.yaml. also there was a directory there called trained model which had files with names like data_type.json and score.py. it seemed like it was all meta data and nothing like, \"column 1 best predicted x ...\". i am still not seeing anything that indicates what best predicts the outcome.",
        "Question_original_content_gpt_summary":"The user is encountering challenges in trying to identify which features best indicate the label they indicated in their data set, using a linear regression model in Designer.",
        "Question_preprocessed_content":"Title: why can't i see train model results in designer?; Content: i created data from sessions of a board game simulator i ran. i'm trying to figure out what the winning strategies are and tracked several features in the data. i loaded the result in a diagram and connected the data set to a model that uses linear regression. i click the train model and go to view output . after clicking through the ensuing links, i seem to be able to locate files. i don't see anything that looks like, column is best predictor of column or something like that. instead i see an ilearner file with a lot of binary i can't read. i see a schema file. there's also a lot of meta files about what version of conda ran it and data types and stuff. how do i see which features best indicated the label i indicated? edit as suggested, i added score model and evaluate model. i did see some error metrics in the evaluate results > visualize. train model had a view output and a view log, but no visualize for me. when i went to view output there were a lot of files like and also there was a directory there called trained model which had files with names like and it seemed like it was all meta data and nothing like, column best predicted x i am still not seeing anything that indicates what best predicts the outcome.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"How to Delete Data Backing a Dataset",
        "Question_body":"How do I delete the data backing a dataset?\n\nI've got a ton of datasets our data scientist created. Some of these are good, but most are no longer relevant. These are growing and I want to delete these since they cost money to store.\n\nI think these files located in \"blobstore-<UUID>\/UI\" may be backing files. There are *.csv files.\n\n\n\n\n\nWhat are these files located in \"blobstore-<UUID>\/azureml?",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1619800286227,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/379022\/how-to-delete-data-backing-a-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-03T13:45:33.607Z",
                "Answer_score":0,
                "Answer_body":"@JeremiahAdams-0775 Thanks for the question. If you wish to delete the file in Blob storage, you can do that via Azure Storage Explorer\/the portal provided you have the appropriate permissions to that storage service.\n\nThis is in fact by design that \u201cunregister\u201d doesn\u2019t actually delete your underlying storage; since an Azure ML Dataset is a reference point to your data in storage, this means we don\u2019t copy your data to your workspace so no extra storage cost is incurred. This also helps safeguard against accidentally deleting files in storage when cleaning up assets in an Azure ML workspace.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to delete data backing a dataset; Content: how do i delete the data backing a dataset? i've got a ton of datasets our data scientist created. some of these are good, but most are no longer relevant. these are growing and i want to delete these since they cost money to store. i think these files located in \"blobstore-\/ui\" may be backing files. there are *.csv files. what are these files located in \"blobstore-\/?",
        "Question_original_content_gpt_summary":"The user is looking to delete a large number of datasets created by a data scientist, which are no longer relevant and are costing money to store, and is unsure of the files located in \"blobstore-\/ui\".",
        "Question_preprocessed_content":"Title: how to delete data backing a dataset; Content: how do i delete the data backing a dataset? i've got a ton of datasets our data scientist created. some of these are good, but most are no longer relevant. these are growing and i want to delete these since they cost money to store. i think these files located in may be backing files. there are files. what are these files located in",
        "Answer_original_content":"@jeremiahadams-0775 thanks for the question. if you wish to delete the file in blob storage, you can do that via azure storage explorer\/the portal provided you have the appropriate permissions to that storage service. this is in fact by design that unregister doesnt actually delete your underlying storage; since an dataset is a reference point to your data in storage, this means we dont copy your data to your workspace so no extra storage cost is incurred. this also helps safeguard against accidentally deleting files in storage when cleaning up assets in an workspace.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"thanks for the question. if you wish to delete the file in blob storage, you can do that via azure storage portal provided you have the appropriate permissions to that storage service. this is in fact by design that unregister doesnt actually delete your underlying storage; since an dataset is a reference point to your data in storage, this means we dont copy your data to your workspace so no extra storage cost is incurred. this also helps safeguard against accidentally deleting files in storage when cleaning up assets in an workspace."
    },
    {
        "Question_id":56269391.0,
        "Question_title":"Readding missing files to DVC",
        "Question_body":"<p>A ran into problem with DVC when some files are missing in remote. For example when I execute <code>dvc pull<\/code> I get the output<\/p>\n\n<pre><code>[##############################] 100% Analysing status.\nWARNING: Cache 'c31bcdd6910977a0e3a86446f2f3bdaa' not found. File 'data\/2.mp4' won't be created.\nWARNING: Cache '77186c4596da7dbc85fefec6d0779049' not found. File 'data\/3.mp4' won't be created.\n<\/code><\/pre>\n\n<p>The <code>dvc status<\/code> command gives me:<\/p>\n\n<pre><code>data\/2.mp4.dvc:\n    changed outs:\n        not in cache:       data\/2.mp4\ndata\/3.mp4.dvc:\n    changed outs:\n        not in cache:       data\/3.mp4\n<\/code><\/pre>\n\n<p>It seems that <code>2.mp4<\/code> and <code>3.mp4<\/code> where added under dvc control but <code>dvc push<\/code> command has not been executed.<\/p>\n\n<p>I have access to the original mp4 files and I have tried to readd them. I copied mp4 files to data folder and executed the command:<\/p>\n\n<pre><code>dvc remove data\/2.mp4.dvc\ndvc remove data\/3.mp4.dvc\n\ndvc add data\/2.mp4 \ndvc add data\/3.mp4 \n<\/code><\/pre>\n\n<p>But there is no effect. How can I remove files from under dvc control and add them again?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1558593719503,
        "Question_favorite_count":null,
        "Question_score":4.0,
        "Question_view_count":1065.0,
        "Owner_creation_time":1522254698710,
        "Owner_last_access_time":1663927713687,
        "Owner_reputation":784.0,
        "Owner_up_votes":32.0,
        "Owner_down_votes":0.0,
        "Owner_views":77.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Russia",
        "Question_last_edit_time":1558637203487,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56269391",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: readding missing files to ; Content: a ran into problem with when some files are missing in remote. for example when i execute pull i get the output [##############################] 100% analysing status. warning: cache 'c31bcdd6910977a0e3a86446f2f3bdaa' not found. file 'data\/2.mp4' won't be created. warning: cache '77186c4596da7dbc85fefec6d0779049' not found. file 'data\/3.mp4' won't be created. the status command gives me: data\/2.mp4.: changed outs: not in cache: data\/2.mp4 data\/3.mp4.: changed outs: not in cache: data\/3.mp4 it seems that 2.mp4 and 3.mp4 where added under control but push command has not been executed. i have access to the original mp4 files and i have tried to readd them. i copied mp4 files to data folder and executed the command: remove data\/2.mp4. remove data\/3.mp4. add data\/2.mp4 add data\/3.mp4 but there is no effect. how can i remove files from under control and add them again?",
        "Question_original_content_gpt_summary":"The user encountered a challenge when some files were missing in their remote repository, and they were unable to readd them to the repository.",
        "Question_preprocessed_content":"Title: readding missing files to; Content: a ran into problem with when some files are missing in remote. for example when i execute i get the output the command gives me it seems that and where added under control but command has not been executed. i have access to the original mp files and i have tried to readd them. i copied mp files to data folder and executed the command but there is no effect. how can i remove files from under control and add them again?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Website section \"organizations using MLflow\"",
        "Question_body":"Hi,\n\n\nI am from https:\/\/mlcertific.com which provides the certification on machine learning\u00a0 we are using mlflow in our organisation and want to contribute to it. Please add our organization\u00a0in the list\u00a0 .\n\n\nThanks,\nhttps:\/\/mlcertific.com\/",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1604036380000,
        "Question_favorite_count":null,
        "Question_score":null,
        "Question_view_count":24.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/groups.google.com\/g\/mlflow-users\/c\/AL3eITG_MP8",
        "Tool":"MLFlow",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-05T14:10:15",
                "Answer_body":"Hello Sweta,\n\n\nWe will add it to the list. Thanks for using MLflow for your certification.\n\n\ncheers\nJules\n\n\n\n\n\n\n\u2013\u2013\n\nThe Best Ideas are Simple\n\nJules S. Damji\n\nSr. Developer Advocate\n\nDatabricks, Inc.\n\nju...@databricks.com\n\n(510) 304-7686\n\n\n\n\n\n\n\n\n\n\n\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\ue5d3\n\ue5d3\n--\nYou received this message because you are subscribed to the Google Groups \"mlflow-users\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to mlflow-users...@googlegroups.com.\nTo view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/mlflow-users\/CAOUcoWx%3D2x-BaMJBq9PkRbko%3DPQ3bnfsSszpZiCPA6RvAS%2BbJQ%40mail.gmail.com."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: website section \"organizations using \"; Content: hi, i am from https:\/\/mlcertific.com which provides the certification on machine learning we are using in our organisation and want to contribute to it. please add our organization in the list . thanks, https:\/\/mlcertific.com\/",
        "Question_original_content_gpt_summary":"The user is requesting to have their organization added to the list of organizations using the certification on machine learning provided by https:\/\/mlcertific.com.",
        "Question_preprocessed_content":"Title: website section organizations using; Content: hi, i am from which provides the certification on machine learning we are using in our organisation and want to contribute to it. please add our organizationin the list . thanks,",
        "Answer_original_content":"hello sweta, we will add it to the list. thanks for using for your certification. cheers jules the best ideas are simple jules s. damji sr. developer advocate databricks, inc. ju...@databricks.com (510) 304-7686 -- you received this message because you are subscribed to the google groups \"-users\" group. to unsubscribe from this group and stop receiving emails from it, send an email to -users...@googlegroups.com. to view this discussion on the web visit https:\/\/groups.google.com\/d\/msgid\/-users\/caoucowx%3d2x-bamjbq9pkrbko%3dpq3bnfssszpzicpa6rvas%2bbjq%40mail.gmail.com.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hello sweta, we will add it to the list. thanks for using for your certification. cheers jules the best ideas are simple jules s. damji sr. developer advocate databricks, inc. you received this message because you are subscribed to the google groups users group. to unsubscribe from this group and stop receiving emails from it, send an email to to view this discussion on the web visit"
    },
    {
        "Question_id":69782294.0,
        "Question_title":"AWS Sagemaker output how to read file with multiple json objects spread out over multiple lines",
        "Question_body":"<p>I have a bunch of json files that look like this<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n<\/code><\/pre>\n<p>Which I can read in with<\/p>\n<pre><code>f = open(file_name)\ndata = []\nfor line in f:\n   data.append(json.dumps(line))\n<\/code><\/pre>\n<p>But I have another file with output like this<\/p>\n<pre><code>{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n<\/code><\/pre>\n<p>I.e. the json is formatted over several lines, so I can't simply read the json in line for line. Is there an easy way to parse this? Or do I have to write something that stitches together each json object line by line and the does json.loads?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1635629350997,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":248.0,
        "Owner_creation_time":1421343783700,
        "Owner_last_access_time":1661295265603,
        "Owner_reputation":1387.0,
        "Owner_up_votes":51.0,
        "Owner_down_votes":1.0,
        "Owner_views":153.0,
        "Answer_body":"<p>Hmm,  as far as I know there's unfortunately no way to load a <a href=\"https:\/\/jsonlines.org\/\" rel=\"nofollow noreferrer\">JSONL<\/a> format data using <code>json.loads<\/code>. One option though, is to come up with a helper function that can convert it to a valid JSON string, as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\n\nstring = &quot;&quot;&quot;\n{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n    # replace the first occurrence of '{'\n    s = s.replace('{', '[{', 1)\n\n    # replace the last occurrence of '}\n    s = s.rsplit('}', 1)[0] + '}]'\n\n    # now go in and replace all occurrences of '}' immediately followed\n    # by newline with a '},'\n    s = s.replace('}\\n', '},\\n')\n\n    return s\n\n\nprint(json.loads(json_lines_to_json(string)))\n<\/code><\/pre>\n<p>Prints:<\/p>\n<pre><code>[{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}]\n<\/code><\/pre>\n<p><strong>Note:<\/strong> your first example actually doesn't seem like valid JSON (or at least JSON lines from my understanding). In particular, this part appears to be invalid due to a trailing comma after the last array element:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...}\n<\/code><\/pre>\n<p>To ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...},\n<\/code><\/pre>\n<hr \/>\n<p>There also appears to be a <a href=\"https:\/\/stackoverflow.com\/questions\/50475635\/loading-jsonl-file-as-json-objects\/50475669\">similar question<\/a> where they suggest splitting on newlines and calling <code>json.loads<\/code> on each line; actually it should be (slightly) less performant to call <code>json.loads<\/code> multiple times on each object, rather than once on the list, as I show below.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from timeit import timeit\nimport json\n\n\nstring = &quot;&quot;&quot;\\\n{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], &quot;word&quot;: &quot;blah blah blah&quot;}\\\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n\n    # Strip newlines from end, then replace all occurrences of '}' followed\n    # by a newline, by a '},' followed by a newline.\n    s = s.rstrip('\\n').replace('}\\n', '},\\n')\n\n    # return string value wrapped in brackets (list)\n    return f'[{s}]'\n\n\nn = 10_000\n\nprint('string replace:        ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals()))\nprint('json.loads each line:  ', timeit(r'[json.loads(line) for line in string.split(&quot;\\n&quot;)]', number=n, globals=globals()))\n<\/code><\/pre>\n<p>Result:<\/p>\n<pre><code>string replace:         0.07599360000000001\njson.loads each line:   0.1078384\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_creation_time":1635633656723,
        "Answer_score":1.0,
        "Owner_location":null,
        "Question_last_edit_time":1635655049276,
        "Answer_last_edit_time":1635655742576,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69782294",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: output how to read file with multiple json objects spread out over multiple lines; Content: i have a bunch of json files that look like this {\"vector\": [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], \"word\": \"blah blah blah\"} {\"vector\": [0.01027186680585146, 0.04181386157870293, -0.07363887131214142, ], \"word\": \"blah blah blah\"} {\"vector\": [0.011699287220835686, 0.04741542786359787, -0.07899319380521774, ], \"word\": \"blah blah blah\"} which i can read in with f = open(file_name) data = [] for line in f: data.append(json.dumps(line)) but i have another file with output like this { \"predictions\": [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777] ] } { \"predictions\": [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0] ] } { \"predictions\": [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0] ] } i.e. the json is formatted over several lines, so i can't simply read the json in line for line. is there an easy way to parse this? or do i have to write something that stitches together each json object line by line and the does json.loads?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge of reading a file with multiple JSON objects spread out over multiple lines.",
        "Question_preprocessed_content":"Title: output how to read file with multiple json objects spread out over multiple lines; Content: i have a bunch of json files that look like this which i can read in with but i have another file with output like this the json is formatted over several lines, so i can't simply read the json in line for line. is there an easy way to parse this? or do i have to write something that stitches together each json object line by line and the does",
        "Answer_original_content":"hmm, as far as i know there's unfortunately no way to load a jsonl format data using json.loads. one option though, is to come up with a helper function that can convert it to a valid json string, as below: import json string = \"\"\" { \"predictions\": [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777] ] } { \"predictions\": [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0] ] } { \"predictions\": [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0] ] } \"\"\" def json_lines_to_json(s: str) -> str: # replace the first occurrence of '{' s = s.replace('{', '[{', 1) # replace the last occurrence of '} s = s.rsplit('}', 1)[0] + '}]' # now go in and replace all occurrences of '}' immediately followed # by newline with a '},' s = s.replace('}\\n', '},\\n') return s print(json.loads(json_lines_to_json(string))) prints: [{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}] note: your first example actually doesn't seem like valid json (or at least json lines from my understanding). in particular, this part appears to be invalid due to a trailing comma after the last array element: {\"vector\": [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...} to ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format: {\"vector\": [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...}, there also appears to be a similar question where they suggest splitting on newlines and calling json.loads on each line; actually it should be (slightly) less performant to call json.loads multiple times on each object, rather than once on the list, as i show below. from timeit import timeit import json string = \"\"\"\\ {\"vector\": [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], \"word\": \"blah blah blah\"} {\"vector\": [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], \"word\": \"blah blah blah\"} {\"vector\": [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], \"word\": \"blah blah blah\"}\\ \"\"\" def json_lines_to_json(s: str) -> str: # strip newlines from end, then replace all occurrences of '}' followed # by a newline, by a '},' followed by a newline. s = s.rstrip('\\n').replace('}\\n', '},\\n') # return string value wrapped in brackets (list) return f'[{s}]' n = 10_000 print('string replace: ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals())) print('json.loads each line: ', timeit(r'[json.loads(line) for line in string.split(\"\\n\")]', number=n, globals=globals())) result: string replace: 0.07599360000000001 json.loads each line: 0.1078384",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hmm, as far as i know there's unfortunately no way to load a jsonl format data using . one option though, is to come up with a helper function that can convert it to a valid json string, as below prints note your first example actually doesn't seem like valid json . in particular, this part appears to be invalid due to a trailing comma after the last array element to ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format there also appears to be a similar question where they suggest splitting on newlines and calling on each line; actually it should be less performant to call multiple times on each object, rather than once on the list, as i show below. result"
    },
    {
        "Question_id":62158009.0,
        "Question_title":"Cannot create inference because there is no model on this pipeline",
        "Question_body":"<p>I have a pipeline in the Designer tool from Azure Machine Learning Studio Preview. I am trying to run this pipeline as a batch prediction, due to the size of my input\/output datasets.<\/p>\n\n<p>The problem is that when I click on \"Create Inference Pipeline\"->\"Batch Inference Pipeline\" it returns the below error message (highlighted in red):\n<a href=\"https:\/\/i.stack.imgur.com\/ZLocy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZLocy.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I tried to check the microsoft documentation (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-run-batch-predictions-designer\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-run-batch-predictions-designer<\/a>), but I didn't find nothing related to that error.<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":0.0,
        "Question_creation_time":1591119509073,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":322.0,
        "Owner_creation_time":1418648544296,
        "Owner_last_access_time":1614789826136,
        "Owner_reputation":25.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":6.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1592073124440,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62158009",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: cannot create inference because there is no model on this pipeline; Content: i have a pipeline in the designer tool from studio preview. i am trying to run this pipeline as a batch prediction, due to the size of my input\/output datasets. the problem is that when i click on \"create inference pipeline\"->\"batch inference pipeline\" it returns the below error message (highlighted in red): i tried to check the microsoft documentation (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-run-batch-predictions-designer), but i didn't find nothing related to that error.",
        "Question_original_content_gpt_summary":"The user is unable to create an inference pipeline for batch prediction due to an error message that they cannot find a solution for in the Microsoft documentation.",
        "Question_preprocessed_content":"Title: cannot create inference because there is no model on this pipeline; Content: i have a pipeline in the designer tool from studio preview. i am trying to run this pipeline as a batch prediction, due to the size of my datasets. the problem is that when i click on create inference pipeline > batch inference pipeline it returns the below error message i tried to check the microsoft documentation , but i didn't find nothing related to that error.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":66830113.0,
        "Question_title":"Run ML pipeline using AWS step function for entire dataset?",
        "Question_body":"<p>I have a step function setup which calls preprocessing lambda and inference lambda for a data item. Now, I need to do this process on the entire dataset(over 10000 items). One way is to invoke step function parallelly for each input. Is there a better alternative to this approach?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1616839735833,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":47.0,
        "Owner_creation_time":1616737438963,
        "Owner_last_access_time":1648106596832,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66830113",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: run ml pipeline using aws step function for entire dataset?; Content: i have a step function setup which calls preprocessing lambda and inference lambda for a data item. now, i need to do this process on the entire dataset(over 10000 items). one way is to invoke step function parallelly for each input. is there a better alternative to this approach?",
        "Question_original_content_gpt_summary":"The user is looking for a better alternative to invoking a step function in parallel for each input in order to run an ML pipeline using AWS Step Function for an entire dataset of over 10,000 items.",
        "Question_preprocessed_content":"Title: run ml pipeline using aws step function for entire dataset?; Content: i have a step function setup which calls preprocessing lambda and inference lambda for a data item. now, i need to do this process on the entire dataset . one way is to invoke step function parallelly for each input. is there a better alternative to this approach?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70909916.0,
        "Question_title":"No GPU detected on AWS SageMaker pytorch-1.8-gpu-py36 instance",
        "Question_body":"<p>I've got a pytorch-1.8-gpu-py36 instance running on AWS SageMaker Studio.<\/p>\n<p>If I'm in a notebook in and I enter:<\/p>\n<pre><code>!nvidia-smi -L\n<\/code><\/pre>\n<p>I get:<\/p>\n<pre><code>GPU 0: Tesla T4 (UUID: GPU-786d298a-2648-3506-6c3a-f541fa46d777)\n<\/code><\/pre>\n<p>But if I open a terminal and enter:<\/p>\n<pre><code>nvidia-smi -L\n<\/code><\/pre>\n<p>I get command not found, and if I try to run a .py script that requires a GPU I get this error from PyTorch:<\/p>\n<pre><code>pytorch_lightning.utilities.exceptions.MisconfigurationException: \nYou requested GPUs: [0]\nBut your machine only has: []\n<\/code><\/pre>\n<p>Do the terminal windows and notebooks run off of separate instances even if they're in the same folder? Is there a way to get the terminal to be part of the same instance as the notebook?<\/p>\n<p>I can't simply run the command line from the notebook as I require a Conda environment that can't be activated from the notebook interface.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1643491948780,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":278.0,
        "Owner_creation_time":1369310121732,
        "Owner_last_access_time":1661886805676,
        "Owner_reputation":931.0,
        "Owner_up_votes":7.0,
        "Owner_down_votes":0.0,
        "Owner_views":48.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":1643519980470,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70909916",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: no gpu detected on pytorch-1.8-gpu-py36 instance; Content: i've got a pytorch-1.8-gpu-py36 instance running on studio. if i'm in a notebook in and i enter: !nvidia-smi -l i get: gpu 0: tesla t4 (uuid: gpu-786d298a-2648-3506-6c3a-f541fa46d777) but if i open a terminal and enter: nvidia-smi -l i get command not found, and if i try to run a .py script that requires a gpu i get this error from pytorch: pytorch_lightning.utilities.exceptions.misconfigurationexception: you requested gpus: [0] but your machine only has: [] do the terminal windows and notebooks run off of separate instances even if they're in the same folder? is there a way to get the terminal to be part of the same instance as the notebook? i can't simply run the command line from the notebook as i require a conda environment that can't be activated from the notebook interface.",
        "Question_original_content_gpt_summary":"The user is facing challenges in getting their GPU to be detected by their pytorch-1.8-gpu-py36 instance, as the terminal window and notebook appear to be running off of separate instances.",
        "Question_preprocessed_content":"Title: no gpu detected on instance; Content: i've got a instance running on studio. if i'm in a notebook in and i enter i get but if i open a terminal and enter i get command not found, and if i try to run a .py script that requires a gpu i get this error from pytorch do the terminal windows and notebooks run off of separate instances even if they're in the same folder? is there a way to get the terminal to be part of the same instance as the notebook? i can't simply run the command line from the notebook as i require a conda environment that can't be activated from the notebook interface.",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":71009191.0,
        "Question_title":"Run SKLearnProcessor from AWS Lambda",
        "Question_body":"<p>I have below code written in AWS Sagemaker Jupyter Notebook. But I would like to <strong>run it from AWS Lambda (or AWS Step Functions)<\/strong> to enable automated execution.<\/p>\n<p>There is a <em>CreateProcessingJob<\/em> in Step Functions and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html?highlight=frameworkprocessor#sagemaker.processing.ProcessingJob\" rel=\"nofollow noreferrer\"><em>sagemaker.processing.ProcessingJob<\/em><\/a>  in <code>sagemaker<\/code> API as well as <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_processing_job\" rel=\"nofollow noreferrer\"><code>create_processing_job<\/code><\/a> in <code>boto3<\/code> - that's the closest I could find...<\/p>\n<p>Is it possible to achieve? What would be the point of creating all of these Sagemaker functionalities if they have to be executed manually from the notebook..?<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.processing import FrameworkProcessor\nfrom sagemaker.processing import ProcessingOutput\n\nregion = boto3.session.Session().region_name    \nrole = get_execution_role()\n\nest_cls = sagemaker.sklearn.estimator.SKLearn\nframework_version_str=&quot;0.23-1&quot;\n\nscript_processor = FrameworkProcessor(\n    role=role,\n    instance_count=8,\n    instance_type=&quot;ml.r5.8xlarge&quot;,\n    volume_size_in_gb=120,\n    max_runtime_in_seconds=432000,\n    estimator_cls=est_cls,\n    framework_version=framework_version_str\n)\n\noutput_folder = 's3:\/\/bucket\/out'\n\nscript_processor.run(\n    code=&quot;preprocessing.py&quot;,\n    source_dir = &quot;code&quot;,\n    outputs=[\n        ProcessingOutput(output_name='preprocessed_data', source=&quot;\/opt\/ml\/processing\/train&quot;),\n    ],\n    arguments=[&quot;--bucket&quot;, &quot;bucket&quot;, &quot;--subfolder&quot;, &quot;Training_data\/&quot;],\n)\n\nscript_processor_job_description = script_processor.jobs[-1].describe()\nprint(script_processor_job_description)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1644164386583,
        "Question_favorite_count":2.0,
        "Question_score":0.0,
        "Question_view_count":274.0,
        "Owner_creation_time":1328189435160,
        "Owner_last_access_time":1663948840663,
        "Owner_reputation":10534.0,
        "Owner_up_votes":888.0,
        "Owner_down_votes":311.0,
        "Owner_views":984.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Katowice, Poland",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71009191",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: run sklearnprocessor from aws lambda; Content: i have below code written in jupyter notebook. but i would like to run it from aws lambda (or aws step functions) to enable automated execution. there is a createprocessingjob in step functions and .processing.processingjob in api as well as create_processing_job in boto3 - that's the closest i could find... is it possible to achieve? what would be the point of creating all of these functionalities if they have to be executed manually from the notebook..? import boto3 import from import get_execution_role from .sklearn.processing import sklearnprocessor from .processing import frameworkprocessor from .processing import processingoutput region = boto3.session.session().region_name role = get_execution_role() est_cls = .sklearn.estimator.sklearn framework_version_str=\"0.23-1\" script_processor = frameworkprocessor( role=role, instance_count=8, instance_type=\"ml.r5.8xlarge\", volume_size_in_gb=120, max_runtime_in_seconds=432000, estimator_cls=est_cls, framework_version=framework_version_str ) output_folder = 's3:\/\/bucket\/out' script_processor.run( code=\"preprocessing.py\", source_dir = \"code\", outputs=[ processingoutput(output_name='preprocessed_data', source=\"\/opt\/ml\/processing\/train\"), ], arguments=[\"--bucket\", \"bucket\", \"--subfolder\", \"training_data\/\"], ) script_processor_job_description = script_processor.jobs[-1].describe() print(script_processor_job_description)",
        "Question_original_content_gpt_summary":"The user is facing challenges in running a sklearnprocessor from AWS Lambda or AWS Step Functions to enable automated execution.",
        "Question_preprocessed_content":"Title: run sklearnprocessor from aws lambda; Content: i have below code written in jupyter notebook. but i would like to run it from aws lambda to enable automated execution. there is a createprocessingjob in step functions and in api as well as in that's the closest i could is it possible to achieve? what would be the point of creating all of these functionalities if they have to be executed manually from the",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":73724864.0,
        "Question_title":"How to get the Endpoint for a model built in my machine",
        "Question_body":"<p>I have built a model in my machine and want to deploy the model (catboost classifier) in the amazon sagemaker. Not able to get the endpoint of the model. Looking for a code\/ process to get the end point. I tried with deploy function, but it is not giving the endpoint, the message is deploy function don't exist for catboostclassifier()<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1663207497110,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":34.0,
        "Owner_creation_time":1663206976432,
        "Owner_last_access_time":1663701306920,
        "Owner_reputation":1.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73724864",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how to get the endpoint for a model built in my machine; Content: i have built a model in my machine and want to deploy the model (catboost classifier) in the . not able to get the endpoint of the model. looking for a code\/ process to get the end point. i tried with deploy function, but it is not giving the endpoint, the message is deploy function don't exist for catboostclassifier()",
        "Question_original_content_gpt_summary":"The user is looking for a code or process to get the endpoint of a model built in their machine using a CatBoostClassifier, but is having difficulty finding a deploy function that works.",
        "Question_preprocessed_content":"Title: how to get the endpoint for a model built in my machine; Content: i have built a model in my machine and want to deploy the model in the . not able to get the endpoint of the model. looking for a code\/ process to get the end point. i tried with deploy function, but it is not giving the endpoint, the message is deploy function don't exist for catboostclassifier",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?",
        "Question_body":"<p>Hi, I created a sweep from existing runs, but the panel Parallel Coordinates are empty, is this an intended behaviour or a bug?<\/p>\n<p>Here is what I did:<\/p>\n<ul>\n<li>populate projects with many runs (using ray\u2019s wandb_mixin)<\/li>\n<li>create a sweep following <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs<\/a>\n<\/li>\n<li>the panel at \u201cSweeps &gt; [2]\u201d contains only 1 run, should contains all 42 runs.<\/li>\n<\/ul>\n<p>The sweep is at <a href=\"https:\/\/wandb.ai\/inc\/try_ray_tune\/sweeps\/smh3d0wg\" class=\"inline-onebox\">Weights &amp; Biases<\/a>, if any one is interested.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":null,
        "Question_creation_time":1640245639181,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":201.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-from-existing-runs-not-showing-up-in-parallel-coordinates-is-this-intended-or-a-bug\/1601",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-23T21:43:17.019Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>Checking the link for your run, I see all 42 runs, grouped together as one single grouped run. Are you still facing this issue?<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-24T01:21:21.696Z",
                "Answer_body":"<p>I can see all 42 runs no problem, but I expect to see them on the sweeps\u2019 parallel coordinates panel:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7417aff14e55470287a1e936710606abe4488cbf.png\" data-download-href=\"\/uploads\/short-url\/gz08s2oOYUdETsRiUUbSDUf6ZKD.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_690x242.png\" alt=\"image\" data-base62-sha1=\"gz08s2oOYUdETsRiUUbSDUf6ZKD\" width=\"690\" height=\"242\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_690x242.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_1035x363.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_1380x484.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/7417aff14e55470287a1e936710606abe4488cbf_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1793\u00d7631 35.9 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Thanks for the reply.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-24T01:39:21.154Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>You should be able to see all 42 runs on your parallel coordinates plot by ungrouping the runs. Grouping runs groups them for charts on your workspace as well.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-12-24T02:54:06.812Z",
                "Answer_body":"<p>Thanks, that solved it.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-22T02:54:52.566Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?; Content: hi, i created a sweep from existing runs, but the panel parallel coordinates are empty, is this an intended behaviour or a bug? here is what i did: populate projects with many runs (using ray\u2019s _mixin) create a sweep following https:\/\/docs..ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs the panel at \u201csweeps > [2]\u201d contains only 1 run, should contains all 42 runs. the sweep is at weights & biases, if any one is interested.",
        "Question_original_content_gpt_summary":"The user encountered a challenge where a sweep from existing runs was not showing up in the parallel coordinates panel, and was unsure if this was an intended behaviour or a bug.",
        "Question_preprocessed_content":"Title: sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?; Content: hi, i created a sweep from existing runs, but the panel parallel coordinates are empty, is this an intended behaviour or a bug? here is what i did populate projects with many runs create a sweep following the panel at sweeps > contains only run, should contains all runs. the sweep is at , if any one is interested.",
        "Answer_original_content":"hi @inc, you should be able to see all 42 runs on your parallel coordinates plot by ungrouping the runs. grouping runs groups them for charts on your workspace as well. thanks, ramit",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi you should be able to see all runs on your parallel coordinates plot by ungrouping the runs. grouping runs groups them for charts on your workspace as well. thanks, ramit"
    },
    {
        "Question_id":null,
        "Question_title":"Deep learning",
        "Question_body":"Which algorithm is most suitable for face reorganization attendance system?",
        "Question_answer_count":1,
        "Question_comment_count":1.0,
        "Question_creation_time":1647180028657,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/770128\/deep-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-14T08:08:38.477Z",
                "Answer_score":0,
                "Answer_body":"@ARCHANAMACHHOYA-1358 A great way to start learning about the various algorithms available with Azure ML is to lookup the cheat sheet document which summarizes which algorithm will help you.\n\nFor this use case I would recommend looking at Image classification ResNet & DenseNet modules in the designer studio. Thanks.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deep learning; Content: which algorithm is most suitable for face reorganization attendance system?",
        "Question_original_content_gpt_summary":"The user is trying to determine which deep learning algorithm is most suitable for a face reorganization attendance system.",
        "Question_preprocessed_content":"Title: deep learning; Content: which algorithm is most suitable for face reorganization attendance system?",
        "Answer_original_content":"@archanamachhoya-1358 a great way to start learning about the various algorithms available with is to lookup the cheat sheet document which summarizes which algorithm will help you. for this use case i would recommend looking at image classification resnet & densenet modules in the designer studio. thanks. if an answer is helpful, please click on or upvote which might help other community members reading this thread.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"a great way to start learning about the various algorithms available with is to lookup the cheat sheet document which summarizes which algorithm will help you. for this use case i would recommend looking at image classification resnet & densenet modules in the designer studio. thanks. if an answer is helpful, please click on or upvote which might help other community members reading this thread."
    },
    {
        "Question_id":70306493.0,
        "Question_title":"View train error metrics for Hugging Face Sagemaker model",
        "Question_body":"<p>I have trained a model using Hugging Face's integration with Amazon Sagemaker <a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/train\" rel=\"nofollow noreferrer\">and their Hello World example<\/a>.<\/p>\n<p>I can easily calculate and view the metrics generated on the evaluation test set: accuracy, f-score, precision, recall etc. by calling <code>training_job_analytics<\/code> on the trained model: <code>huggingface_estimator.training_job_analytics.dataframe()<\/code><\/p>\n<p>How can I also see the same metrics on training sets (or even training error for each epoch)?<\/p>\n<p>Training code is basically the same as the link with extra parts of the docs added:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.huggingface import HuggingFace\n\n# optionally parse logs for key metrics\n# from the docs: https:\/\/huggingface.co\/docs\/sagemaker\/train#sagemaker-metrics\nmetric_definitions = [\n    {'Name': 'loss', 'Regex': &quot;'loss': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'learning_rate', 'Regex': &quot;'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_loss', 'Regex': &quot;'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_accuracy', 'Regex': &quot;'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_f1', 'Regex': &quot;'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_precision', 'Regex': &quot;'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_recall', 'Regex': &quot;'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_runtime', 'Regex': &quot;'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'eval_samples_per_second', 'Regex': &quot;'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?&quot;},\n    {'Name': 'epoch', 'Regex': &quot;'epoch': ([0-9]+(.|e\\-)[0-9]+),?&quot;}\n]\n\n# hyperparameters, which are passed into the training job\nhyperparameters={\n    'epochs': 5,\n    'train_batch_size': batch_size,\n    'model_name': model_checkpoint,\n    'task': task,\n}\n\n# init the model (but not yet trained)\nhuggingface_estimator = HuggingFace(\n    entry_point='train.py',\n    source_dir='.\/scripts',\n    instance_type='ml.p3.2xlarge',\n    instance_count=1,\n    role=role,\n    transformers_version='4.6',\n    pytorch_version='1.7',\n    py_version='py36',\n    hyperparameters = hyperparameters,\n    metric_definitions=metric_definitions\n)\n# starting the train job with our uploaded datasets as input\nhuggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})\n\n# does not return metrics on training - only on eval!\nhuggingface_estimator.training_job_analytics.dataframe()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1639148719370,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":184.0,
        "Owner_creation_time":1437078651387,
        "Owner_last_access_time":1663950070696,
        "Owner_reputation":901.0,
        "Owner_up_votes":150.0,
        "Owner_down_votes":10.0,
        "Owner_views":145.0,
        "Answer_body":"<p>This can be solved by increasing the number of epochs in training to a more realistic value.<\/p>\n<p>Currently, the model trains in fewer than 300 seconds (which is when the following timestamp would be recorded) and presumably the loss function.<\/p>\n<p>Changes to make:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>hyperparameters={\n    'epochs': 100, # increase the number of epochs to realistic value!\n    'train_batch_size': batch_size,\n    'model_name': model_checkpoint,\n    'task': task,\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1639502011652,
        "Answer_score":0.0,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1645119168563,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70306493",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: view train error metrics for hugging face model; Content: i have trained a model using hugging face's integration with and their hello world example. i can easily calculate and view the metrics generated on the evaluation test set: accuracy, f-score, precision, recall etc. by calling training_job_analytics on the trained model: huggingface_estimator.training_job_analytics.dataframe() how can i also see the same metrics on training sets (or even training error for each epoch)? training code is basically the same as the link with extra parts of the docs added: from .huggingface import huggingface # optionally parse logs for key metrics # from the docs: https:\/\/huggingface.co\/docs\/\/train#-metrics metric_definitions = [ {'name': 'loss', 'regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"}, {'name': 'learning_rate', 'regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"}, {'name': 'eval_loss', 'regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"}, {'name': 'eval_accuracy', 'regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"}, {'name': 'eval_f1', 'regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"}, {'name': 'eval_precision', 'regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"}, {'name': 'eval_recall', 'regex': \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\"}, {'name': 'eval_runtime', 'regex': \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"}, {'name': 'eval_samples_per_second', 'regex': \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"}, {'name': 'epoch', 'regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"} ] # hyperparameters, which are passed into the training job hyperparameters={ 'epochs': 5, 'train_batch_size': batch_size, 'model_name': model_checkpoint, 'task': task, } # init the model (but not yet trained) huggingface_estimator = huggingface( entry_point='train.py', source_dir='.\/scripts', instance_type='ml.p3.2xlarge', instance_count=1, role=role, transformers_version='4.6', pytorch_version='1.7', py_version='py36', hyperparameters = hyperparameters, metric_definitions=metric_definitions ) # starting the train job with our uploaded datasets as input huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path}) # does not return metrics on training - only on eval! huggingface_estimator.training_job_analytics.dataframe()",
        "Question_original_content_gpt_summary":"The user is encountering a challenge in viewing train error metrics for a model trained using Hugging Face's integration with their Hello World example.",
        "Question_preprocessed_content":"Title: view train error metrics for hugging face model; Content: i have trained a model using hugging face's integration with and their hello world example. i can easily calculate and view the metrics generated on the evaluation test set accuracy, f score, precision, recall etc. by calling on the trained model how can i also see the same metrics on training sets ? training code is basically the same as the link with extra parts of the docs added",
        "Answer_original_content":"this can be solved by increasing the number of epochs in training to a more realistic value. currently, the model trains in fewer than 300 seconds (which is when the following timestamp would be recorded) and presumably the loss function. changes to make: hyperparameters={ 'epochs': 100, # increase the number of epochs to realistic value! 'train_batch_size': batch_size, 'model_name': model_checkpoint, 'task': task, }",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"this can be solved by increasing the number of epochs in training to a more realistic value. currently, the model trains in fewer than seconds and presumably the loss function. changes to make"
    },
    {
        "Question_id":null,
        "Question_title":"Debugging and profiling guild",
        "Question_body":"<p>I sometimes see <code>guild<\/code> taking a long time to start a run compared to just running the command that I get from <code>--print-cmd<\/code>. I realise this is because <code>guild<\/code> has to resolve dependencies etc., but I would like to understand if there is an easy way to debug and especially profile what steps \/ operations that is expensive in the <code>guild<\/code> command.<\/p>\n<p>I am aware of the <code>guild --debug<\/code> flag, but in my particular case it doesn\u2019t provide much info about what is taking a long time.<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":null,
        "Question_creation_time":1607965561395,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":512.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/debugging-and-profiling-guild\/500",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-15T19:47:26.311Z",
                "Answer_body":"<p>Run your command with <code>PROFILE=1<\/code> env var like this:<\/p>\n<pre><code class=\"lang-command\">PROFILE=1 guild run ...\n<\/code><\/pre>\n<p>You\u2019ll get a couple of Python profile stats written.<\/p>\n<p>A nice way to view these files is with <a href=\"https:\/\/jiffyclub.github.io\/snakeviz\/\">SnakeViz<\/a>. Guild prints the instructions for running <code>snakeviz<\/code> when you run with the profile flag.<\/p>\n<p>If you need help interpreting anything just attach both stat files and I\u2019ll take a look.<\/p>\n<p>The shorter you can make your code the better. Otherwise the startup cost\/time will be overshadowed by the actual run time.<\/p>",
                "Answer_score":3.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T22:33:34.706Z",
                "Answer_body":"<p>That\u2019s pretty sweet!<\/p>\n<p>I\u2019ve profiled the run that takes a long time. You can find the results <a href=\"https:\/\/drive.google.com\/file\/d\/1-OBwznTVA_KtbJGZo8yUgi8RLIEUPaMo\/view?usp=sharing\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>\n<p>As you can see it takes a looong time. This happened after I did a big refactoring of my <code>guild<\/code> file into multiple <code>guild<\/code> files using inheritance.<\/p>\n<p>Is the above happening because it is looking for source code in the root dir?<\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T22:55:26.025Z",
                "Answer_body":"<p>It looks like you have a directory with a lot of files - over 1M. Guild is example those files to see if they\u2019re candidates for source code copy. By default Guild only looks at I think around 100 files unless you\u2019ve configured the <code>sourcecode<\/code> attr for the operation.<\/p>\n<p>You can see what\u2019s going on by running:<\/p>\n<pre><code class=\"lang-command\">guild run &lt;op&gt; --test-sourcecode\n<\/code><\/pre>\n<p>This should take all that time but you\u2019ll see where the files are.<\/p>\n<p>You can remove a directory from consideration (Guild won\u2019t scan it) this way:<\/p>\n<pre><code class=\"lang-yaml\">op:\n  sourcecode:\n    - exclude:\n        dir: &lt;dir containing lots of files&gt;\n<\/code><\/pre>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T23:30:50.935Z",
                "Answer_body":"<p>So I have the <code>sourcecode<\/code> attribute specified, but I guess not it in the right way.<\/p>\n<p>My current folder structure looks like this:<\/p>\n<pre><code>training\/\nscripts\/\nguild\/\n   flags\/\n           classification.yml\n           common.yml\n           segmentation.yml\n    base_model.yml\n    classification_model.yml\n    segmentation_model.yml\n    utils.yml      \nguild.yml\n<\/code><\/pre>\n<p>My main <code>guild.yml<\/code> looks like this:<\/p>\n<pre><code>- include: guild\/segmentation_model.yml\n- include: guild\/classification_model.yml\n<\/code><\/pre>\n<p>These two <code>guild<\/code> files in turn looks like this:<\/p>\n<pre><code>guild\/segmentation_model.yml\n-----------------------------\n- include:\n    - base_model.yml\n    - utils.yml\n    - flags\/segmentation.yml\n    - flags\/common.yml\n\n- model: segmentation_model\n  sourcecode:\n    - scripts\n    - training\n    - guild.yml\n  extends:\n    - base_model\n    - utils\n  operations:\n    convert_to_onnx:\n      flags:\n        $include: onnx_flags_segmentation\n    train:\n      flags:\n        batch_size: 1\n        $include:\n          - segmentation_flags\n          - train_flags\n          - common_flags\n    test:\n      flags:\n        $include:\n          - common_flags\n          - test_flags\n<\/code><\/pre>\n<pre><code>guild\/classification_model.yml\n-------------------------------\n- include:\n    - base_model.yml\n    - utils.yml\n    - flags\/classification.yml\n    - flags\/common.yml\n\n- model: classification_model\n  sourcecode:\n    - scripts\n    - training\n    - guild.yml\n  extends:\n    - base_model\n    - utils\n  operations:\n    train:\n      flags:\n        batch_size: 0\n        $include:\n          - classification_flags\n          - train_flags\n          - common_flags\n    convert_to_onnx:\n      flags:\n        $include: onnx_flags_classification\n<\/code><\/pre>\n<p>The <code>base_model.yml<\/code> looks like this:<\/p>\n<pre><code>base_model.yml\n--------------------------------\n- config: base_model\n  sourcecode:\n    select:\n      - scripts\n      - training\n      - guild.yml\n  operations:\n    train:\n      main: scripts\/training\/train_model --input_database ...\n      requires: prepared_data\n    test:\n      main: scripts\/training\/test_model --input_database ...\n      requires:\n        - operation: train\n        - prepared_data\n  resources:\n    prepared_data:\n      sources: ...\n<\/code><\/pre>\n<p>The command<\/p>\n<pre><code>guild run classification_model:train\n<\/code><\/pre>\n<p>Is what takes a long time. The interesting thing is that the <code>sourcecode<\/code> directory in the run directory only contains the sourcecode that I have specified.<\/p>\n<pre><code>$ ls ~\/...\/.guild\/runs\/5d239a67d97d4bd4952e2b1cc2b10083\/.guild\/sourcecode\/\nguild.yml  scripts  training\n<\/code><\/pre>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T23:34:01.459Z",
                "Answer_body":"<p>It\u2019s the scanning\/testing of a large number of files that\u2019s taking time.<\/p>\n<p>What does this command reveal?<\/p>\n<pre><code class=\"lang-command\">guild run classification_model:train --test-sourcecode\n<\/code><\/pre>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T23:35:24.171Z",
                "Answer_body":"<p>It scans through the entire directory:<\/p>\n<pre><code>training\/\ndata\/\n3rd_party_lib\/\nscripts\/\nguild\/\nguild.yml\n<\/code><\/pre>\n<p>So also <code>data<\/code> and <code>3rd_part_lib<\/code> which are the heavy folders.<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-15T23:37:28.584Z",
                "Answer_body":"<p>Refer the example I provided above. You need to explicitly exclude any directories containing large numbers of files - unless you want those scanned for consideration as source code files. This is what\u2019s taking time. The code snippet above will address that.<\/p>",
                "Answer_score":7.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-16T00:39:39.430Z",
                "Answer_body":"<p>I see - thank you! It works now <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":46.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: debugging and profiling guild; Content: i sometimes see guild taking a long time to start a run compared to just running the command that i get from --print-cmd. i realise this is because guild has to resolve dependencies etc., but i would like to understand if there is an easy way to debug and especially profile what steps \/ operations that is expensive in the guild command. i am aware of the guild --debug flag, but in my particular case it doesn\u2019t provide much info about what is taking a long time.",
        "Question_original_content_gpt_summary":"The user is looking for an easy way to debug and profile the expensive steps and operations that are taking a long time when running a Guild command.",
        "Question_preprocessed_content":"Title: debugging and profiling guild; Content: i sometimes see taking a long time to start a run compared to just running the command that i get from . i realise this is because has to resolve dependencies but i would like to understand if there is an easy way to debug and especially profile what steps \/ operations that is expensive in the command. i am aware of the flag, but in my particular case it doesnt provide much about what is taking a long time.",
        "Answer_original_content":"run your command with profile=1 env var like this: profile=1 guild run ... youll get a couple of python profile stats written. a nice way to view these files is with snakeviz. guild prints the instructions for running snakeviz when you run with the profile flag. if you need help interpreting anything just attach both stat files and ill take a look. the shorter you can make your code the better. otherwise the startup cost\/time will be overshadowed by the actual run time. thats pretty sweet! ive profiled the run that takes a long time. you can find the results here. as you can see it takes a looong time. this happened after i did a big refactoring of my guild file into multiple guild files using inheritance. is the above happening because it is looking for source code in the root dir? it looks like you have a directory with a lot of files - over 1m. guild is example those files to see if theyre candidates for source code copy. by default guild only looks at i think around 100 files unless youve configured the sourcecode attr for the operation. you can see whats going on by running: guild run <op> --test-sourcecode this should take all that time but youll see where the files are. you can remove a directory from consideration (guild wont scan it) this way: op: sourcecode: - exclude: dir: <dir containing lots of files> so i have the sourcecode attribute specified, but i guess not it in the right way. my current folder structure looks like this: training\/ scripts\/ guild\/ flags\/ classification.yml common.yml segmentation.yml base_model.yml classification_model.yml segmentation_model.yml utils.yml guild.yml my main guild.yml looks like this: - include: guild\/segmentation_model.yml - include: guild\/classification_model.yml these two guild files in turn looks like this: guild\/segmentation_model.yml ----------------------------- - include: - base_model.yml - utils.yml - flags\/segmentation.yml - flags\/common.yml - model: segmentation_model sourcecode: - scripts - training - guild.yml extends: - base_model - utils operations: convert_to_onnx: flags: $include: onnx_flags_segmentation train: flags: batch_size: 1 $include: - segmentation_flags - train_flags - common_flags test: flags: $include: - common_flags - test_flags guild\/classification_model.yml ------------------------------- - include: - base_model.yml - utils.yml - flags\/classification.yml - flags\/common.yml - model: classification_model sourcecode: - scripts - training - guild.yml extends: - base_model - utils operations: train: flags: batch_size: 0 $include: - classification_flags - train_flags - common_flags convert_to_onnx: flags: $include: onnx_flags_classification the base_model.yml looks like this: base_model.yml -------------------------------- - config: base_model sourcecode: select: - scripts - training - guild.yml operations: train: main: scripts\/training\/train_model --input_database ... requires: prepared_data test: main: scripts\/training\/test_model --input_database ... requires: - operation: train - prepared_data resources: prepared_data: sources: ... the command guild run classification_model:train is what takes a long time. the interesting thing is that the sourcecode directory in the run directory only contains the sourcecode that i have specified. $ ls ~\/...\/.guild\/runs\/5d239a67d97d4bd4952e2b1cc2b10083\/.guild\/sourcecode\/ guild.yml scripts training its the scanning\/testing of a large number of files thats taking time. what does this command reveal? guild run classification_model:train --test-sourcecode it scans through the entire directory: training\/ data\/ 3rd_party_lib\/ scripts\/ guild\/ guild.yml so also data and 3rd_part_lib which are the heavy folders. refer the example i provided above. you need to explicitly exclude any directories containing large numbers of files - unless you want those scanned for consideration as source code files. this is whats taking time. the code snippet above will address that. i see - thank you! it works now",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"run your command with env var like this youll get a couple of python profile stats written. a nice way to view these files is with snakeviz. guild prints the instructions for running when you run with the profile flag. if you need help interpreting anything just attach both stat files and ill take a look. the shorter you can make your code the better. otherwise the startup will be overshadowed by the actual run time. thats pretty sweet! ive profiled the run that takes a long time. you can find the results here. as you can see it takes a looong time. this happened after i did a big refactoring of my file into multiple files using inheritance. is the above happening because it is looking for source code in the root dir? it looks like you have a directory with a lot of files over m. guild is example those files to see if theyre candidates for source code copy. by default guild only looks at i think around files unless youve configured the attr for the operation. you can see whats going on by running this should take all that time but youll see where the files are. you can remove a directory from consideration this way so i have the attribute specified, but i guess not it in the right way. my current folder structure looks like this my main looks like this these two files in turn looks like this the looks like this the command is what takes a long time. the interesting thing is that the directory in the run directory only contains the sourcecode that i have specified. its the of a large number of files thats taking time. what does this command reveal? it scans through the entire directory so also and which are the heavy folders. refer the example i provided above. you need to explicitly exclude any directories containing large numbers of files unless you want those scanned for consideration as source code files. this is whats taking time. the code snippet above will address that. i see thank you! it works now"
    },
    {
        "Question_id":58019308.0,
        "Question_title":"ScriptRunConfig with datastore reference on AML",
        "Question_body":"<p>When trying to run a ScriptRunConfig, using :<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>src = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', ds.as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>\n\n<p>It doesn't work and breaks with this when I submit the job : <\/p>\n\n<pre><code>... lots of things... and then\nTypeError: Object of type 'DataReference' is not JSON serializable\n<\/code><\/pre>\n\n<p>However if I run it with the Estimator, it works. One of the differences is the fact that with a <code>ScriptRunConfig<\/code> we're using a list for parameters and the other is a dictionary.<\/p>\n\n<p>Thanks for any pointers!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0.0,
        "Question_creation_time":1568929720367,
        "Question_favorite_count":null,
        "Question_score":2.0,
        "Question_view_count":1541.0,
        "Owner_creation_time":1538275960603,
        "Owner_last_access_time":1658458641830,
        "Owner_reputation":381.0,
        "Owner_up_votes":75.0,
        "Owner_down_votes":2.0,
        "Owner_views":50.0,
        "Answer_body":"<p>Being able to use <code>DataReference<\/code> in <code>ScriptRunConfig<\/code> is a bit more involved than doing just <code>ds.as_mount()<\/code>. You will need to convert it into a string in <code>arguments<\/code> and then update the <code>RunConfiguration<\/code>'s <code>data_references<\/code> section with the <code>DataReferenceConfiguration<\/code> created from <code>ds<\/code>. Please <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training\/train-on-remote-vm\/train-on-remote-vm.ipynb\" rel=\"nofollow noreferrer\">see here<\/a> for an example notebook on how to do that.<\/p>\n<p>If you are just reading from the input location and not doing any writes to it, please check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-register-datasets\" rel=\"nofollow noreferrer\"><code>Dataset<\/code><\/a>. It allows you to do exactly what you are doing without doing anything extra. <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">Here is an example notebook<\/a> that shows this in action.<\/p>\n<p>Below is a short version of the notebook<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\n\n# more imports and code\n\nds = Datastore(workspace, 'mydatastore')\ndataset = Dataset.File.from_files(path=(ds, 'path\/to\/input-data\/within-datastore'))\n\nsrc = ScriptRunConfig(source_directory=project_folder, \n                      script='train.py', \n                      arguments=['--input-data-dir', dataset.as_named_input('input').as_mount(),\n                                 '--reg', '0.99'],\n                      run_config=run_config) \nrun = experiment.submit(config=src)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_creation_time":1568945686667,
        "Answer_score":4.0,
        "Owner_location":"Montreal, QC, Canada",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":1595974462436,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58019308",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: scriptrunconfig with datastore reference on aml; Content: when trying to run a scriptrunconfig, using : src = scriptrunconfig(source_directory=project_folder, script='train.py', arguments=['--input-data-dir', ds.as_mount(), '--reg', '0.99'], run_config=run_config) run = experiment.submit(config=src) it doesn't work and breaks with this when i submit the job : ... lots of things... and then typeerror: object of type 'datareference' is not json serializable however if i run it with the estimator, it works. one of the differences is the fact that with a scriptrunconfig we're using a list for parameters and the other is a dictionary. thanks for any pointers!",
        "Question_original_content_gpt_summary":"The user is encountering a TypeError when trying to run a scriptrunconfig with a datastore reference on Azure Machine Learning, but the same code works when using an estimator.",
        "Question_preprocessed_content":"Title: scriptrunconfig with datastore reference on aml; Content: when trying to run a scriptrunconfig, using it doesn't work and breaks with this when i submit the job however if i run it with the estimator, it works. one of the differences is the fact that with a we're using a list for parameters and the other is a dictionary. thanks for any pointers!",
        "Answer_original_content":"being able to use datareference in scriptrunconfig is a bit more involved than doing just ds.as_mount(). you will need to convert it into a string in arguments and then update the runconfiguration's data_references section with the datareferenceconfiguration created from ds. please see here for an example notebook on how to do that. if you are just reading from the input location and not doing any writes to it, please check out dataset. it allows you to do exactly what you are doing without doing anything extra. here is an example notebook that shows this in action. below is a short version of the notebook from .core import dataset # more imports and code ds = datastore(workspace, 'mydatastore') dataset = dataset.file.from_files(path=(ds, 'path\/to\/input-data\/within-datastore')) src = scriptrunconfig(source_directory=project_folder, script='train.py', arguments=['--input-data-dir', dataset.as_named_input('input').as_mount(), '--reg', '0.99'], run_config=run_config) run = experiment.submit(config=src)",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"being able to use in is a bit more involved than doing just . you will need to convert it into a string in and then update the 's section with the created from . please see here for an example notebook on how to do that. if you are just reading from the input location and not doing any writes to it, please check out . it allows you to do exactly what you are doing without doing anything extra. here is an example notebook that shows this in action. below is a short version of the notebook"
    },
    {
        "Question_id":null,
        "Question_title":"Create video tutorials for the new users",
        "Question_body":"We have been using Polyaxon for years as the ML Platform team. One feedback we got from ML engineers is the onboarding cost. One idea to help the onboarding is to create video tutorials for new users.",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1649007252000,
        "Question_favorite_count":null,
        "Question_score":3.0,
        "Question_view_count":null,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/github.com\/orgs\/polyaxon\/discussions\/1459",
        "Tool":"Polyaxon",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":null,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-03T18:51:28Z",
                "Answer_score":2,
                "Answer_body":"Hi @shotarok ,\n\nI already started some work around some recurrent use-cases that I see in demos: https:\/\/www.youtube.com\/channel\/UCbzCxshQWTxCZPWZwgM0ePw\nMore short content around specific use-cases is under work.\nIf you have specific use-cases that you would like to see and share with the rest of your team as an educational or on-boarding material, please let me know."
            },
            {
                "Answer_creation_time":"2022-04-03T18:55:19Z",
                "Answer_score":1,
                "Answer_body":"Another idea that I was thinking about is starting a bi-weekly or a monthly community call, where anyone can join and we can either demo specific aspects of the platform or answer questions."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":-1.0,
        "Question_original_content":"Title: create video tutorials for the new users; Content: we have been using for years as the ml platform team. one feedback we got from ml engineers is the onboarding cost. one idea to help the onboarding is to create video tutorials for new users.",
        "Question_original_content_gpt_summary":"The user has encountered the challenge of creating video tutorials to reduce the onboarding cost for new users of the ML platform they have been using for years.",
        "Question_preprocessed_content":"Title: create video tutorials for the new users; Content: we have been using for years as the ml platform team. one feedback we got from ml engineers is the onboarding cost. one idea to help the onboarding is to create video tutorials for new users.",
        "Answer_original_content":"hi @shotarok , i already started some work around some recurrent use-cases that i see in demos: https:\/\/www.youtube.com\/channel\/ucbzcxshqwtxczpwzwgm0epw more short content around specific use-cases is under work. if you have specific use-cases that you would like to see and share with the rest of your team as an educational or on-boarding material, please let me know. another idea that i was thinking about is starting a bi-weekly or a monthly community call, where anyone can join and we can either demo specific aspects of the platform or answer questions.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi , i already started some work around some recurrent use cases that i see in demos more short content around specific use cases is under work. if you have specific use cases that you would like to see and share with the rest of your team as an educational or on boarding material, please let me know. another idea that i was thinking about is starting a bi weekly or a monthly community call, where anyone can join and we can either demo specific aspects of the platform or answer questions."
    },
    {
        "Question_id":58487710.0,
        "Question_title":"User Docker Hub registry containers in AWS Sagemaker",
        "Question_body":"<p>Is there any way to load containers stored in docker hub registry in Amazon Sagemaker.\nAccording to some documentation, it should be possible, but I have not been able to find any relevan example or guide for it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0.0,
        "Question_creation_time":1571665775480,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":135.0,
        "Owner_creation_time":1382876091092,
        "Owner_last_access_time":1640773246956,
        "Owner_reputation":149.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":26.0,
        "Answer_body":"<p>While you can use any registry when working with Docker on a SageMaker notebook, as of this writing other SageMaker components presently only support images from Amazon ECR repositories.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_creation_time":1582242007587,
        "Answer_score":0.0,
        "Owner_location":"Valencia, Espa\u00f1a",
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58487710",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: user docker hub registry containers in ; Content: is there any way to load containers stored in docker hub registry in . according to some documentation, it should be possible, but i have not been able to find any relevan example or guide for it.",
        "Question_original_content_gpt_summary":"The user is attempting to load containers stored in Docker Hub Registry in order to complete a task, but has been unable to find any relevant examples or guides.",
        "Question_preprocessed_content":"Title: user docker hub registry containers in; Content: is there any way to load containers stored in docker hub registry in . according to some documentation, it should be possible, but i have not been able to find any relevan example or guide for it.",
        "Answer_original_content":"while you can use any registry when working with docker on a notebook, as of this writing other components presently only support images from amazon ecr repositories.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"while you can use any registry when working with docker on a notebook, as of this writing other components presently only support images from amazon ecr repositories."
    },
    {
        "Question_id":null,
        "Question_title":"Deploying multiple Comprehend Custom Classifiers (multi-label mode)",
        "Question_body":"I want to train and deploy multiple comprehend custom classifiers (for example 50 models). I want to be able to classify my documents in near real-time (a couple of seconds are fine) 24\/7. The problem is that deploying one end-point for each classifier is very expensive, especially that one or two IU would be enough for all my models combined (I am expecting to process around 10 document a minute total\/length of one document is around 1000 characters ). Is there a way where I can deploy multiple models behind the same endpoint (similar to the multi-model endpoint in SageMaker)? Or maybe do an asynchronous approach and somewho make sure I get the response within seconds?",
        "Question_answer_count":1,
        "Question_comment_count":null,
        "Question_creation_time":1664215795645,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":36.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUEQiFVzOhR5q1XYhjlltO7w\/deploying-multiple-comprehend-custom-classifiers-multi-label-mode",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":1.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-24T19:34:11.311Z",
                "Answer_score":0,
                "Answer_body":"No , Comprehend don't support hosting multiple models with the same endpoint right now. Thanks for your suggestions . We will take them into consideration .",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: deploying multiple comprehend custom classifiers (multi-label mode); Content: i want to train and deploy multiple comprehend custom classifiers (for example 50 models). i want to be able to classify my documents in near real-time (a couple of seconds are fine) 24\/7. the problem is that deploying one end-point for each classifier is very expensive, especially that one or two iu would be enough for all my models combined (i am expecting to process around 10 document a minute total\/length of one document is around 1000 characters ). is there a way where i can deploy multiple models behind the same endpoint (similar to the multi-model endpoint in )? or maybe do an asynchronous approach and somewho make sure i get the response within seconds?",
        "Question_original_content_gpt_summary":"The user is facing a challenge of deploying multiple Comprehend Custom Classifiers in multi-label mode while needing to classify documents in near real-time with limited resources.",
        "Question_preprocessed_content":"Title: deploying multiple comprehend custom classifiers; Content: i want to train and deploy multiple comprehend custom classifiers . i want to be able to classify my documents in near real time the problem is that deploying one end point for each classifier is very expensive, especially that one or two iu would be enough for all my models combined . is there a way where i can deploy multiple models behind the same endpoint ? or maybe do an asynchronous approach and somewho make sure i get the response within seconds?",
        "Answer_original_content":"no , comprehend don't support hosting multiple models with the same endpoint right now. thanks for your suggestions . we will take them into consideration .",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"no , comprehend don't support hosting multiple models with the same endpoint right now. thanks for your suggestions . we will take them into consideration ."
    },
    {
        "Question_id":70244103.0,
        "Question_title":"Snowflake Connection to Sagemaker - CloudFormation error on StorageIntegrationStack when creating",
        "Question_body":"<p>I'm trying to associate Sagemaker with snowflake using AWS CloudFormation, by following this tutoriel : <a href=\"https:\/\/quickstarts.snowflake.com\/guide\/vhol_snowflake_data_wrangler\/index.html?index=..%2F..index#2\" rel=\"nofollow noreferrer\">https:\/\/quickstarts.snowflake.com\/guide\/vhol_snowflake_data_wrangler\/index.html?index=..%2F..index#2<\/a>\nbut while creating the stack I got this error :<\/p>\n<p><em>StorageIntegrationStack<br \/>\nCREATE_FAILED\nEmbedded stack ... was not successfully created: The following resource(s) failed to create: [KMSKey, SnowflakeSecret].<\/em><\/p>\n<p>What's KMSKey and SnowflakeSecret ? How can I fix this error ?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":6.0,
        "Question_creation_time":1638786842860,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":289.0,
        "Owner_creation_time":1558705489563,
        "Owner_last_access_time":1664046833223,
        "Owner_reputation":43.0,
        "Owner_up_votes":2.0,
        "Owner_down_votes":0.0,
        "Owner_views":11.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70244103",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: snowflake connection to - cloudformation error on storageintegrationstack when creating; Content: i'm trying to associate with snowflake using aws cloudformation, by following this tutoriel : https:\/\/quickstarts.snowflake.com\/guide\/vhol_snowflake_data_wrangler\/index.html?index=..%2f..index#2 but while creating the stack i got this error : storageintegrationstack create_failed embedded stack ... was not successfully created: the following resource(s) failed to create: [kmskey, snowflakesecret]. what's kmskey and snowflakesecret ? how can i fix this error ?",
        "Question_original_content_gpt_summary":"The user encountered an error while attempting to associate with Snowflake using AWS Cloudformation, resulting in the failure of the creation of resources 'kmskey' and 'snowflakesecret'.",
        "Question_preprocessed_content":"Title: snowflake connection to cloudformation error on storageintegrationstack when creating; Content: i'm trying to associate with snowflake using aws cloudformation, by following this tutoriel but while creating the stack i got this error storageintegrationstack embedded stack was not successfully created the following resource failed to create . what's kmskey and snowflakesecret ? how can i fix this error ?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Cloud Vision API in Vertex AI?",
        "Question_body":"  Hi,I am a newbie in Google Cloud and i have an elementary conceptual question about the dependency between Cloud Vision API and Vertex AI or the recently launched Vertex Vision AI.I have an app that makes predictions on images using Google Vision AI API ImageAnnotatorClient() Is this API going to be part of  Vertex AI  or Vertex Vision AI?Or in other words, should I modify the below code to make it part of Vertex AI\/Vertex Vision AI?          ",
        "Question_answer_count":0,
        "Question_comment_count":null,
        "Question_creation_time":1669604340000,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":22.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Vision-API-in-Vertex-AI\/td-p\/493648\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-28T02:59:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi,\n\nI am a newbie in Google Cloud and i have an elementary conceptual question about the dependency between\u00a0Cloud Vision API and Vertex AI or the recently launched Vertex Vision AI.\n\nI have an app that makes predictions on images using Google Vision AI API\u00a0ImageAnnotatorClient()\u00a0\n\nIs this API going to be part of\u00a0 Vertex AI\u00a0 or Vertex Vision AI?\n\nOr in other words, should I modify the below code to make it part of Vertex AI\/Vertex Vision AI?\n\n\u00a0\n\nfrom google.cloud import vision\n\ndef detect_labels_uri(uri):\n    client = vision.ImageAnnotatorClient()\n    image = vision.Image()\n    image.source.image_uri = uri\n\n    response = client.label_detection(image=image)\n    labels = response.label_annotations\n    return(labels)"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: cloud vision api in ?; Content: hi,i am a newbie in google cloud and i have an elementary conceptual question about the dependency between cloud vision api and or the recently launched vertex vision ai.i have an app that makes predictions on images using google vision ai api imageannotatorclient() is this api going to be part of or vertex vision ai?or in other words, should i modify the below code to make it part of \/vertex vision ai?",
        "Question_original_content_gpt_summary":"The user is inquiring about the dependency between the Cloud Vision API and the recently launched Vertex Vision AI, and whether they need to modify their code to make it part of Vertex Vision AI.",
        "Question_preprocessed_content":"Title: cloud vision api in ?; Content: hi,i am a newbie in google cloud and i have an elementary conceptual question about the dependency between cloud vision api and or the recently launched vertex vision have an app that makes predictions on images using google vision ai api imageannotatorclient is this api going to be part of or vertex vision ai?or in other words, should i modify the below code to make it part of vertex vision ai?",
        "Answer_original_content":"hi, i am a newbie in google cloud and i have an elementary conceptual question about the dependency betweencloud vision api and or the recently launched vertex vision ai. i have an app that makes predictions on images using google vision ai apiimageannotatorclient() is this api going to be part of or vertex vision ai? or in other words, should i modify the below code to make it part of \/vertex vision ai? from google.cloud import vision def detect_labels_uri(uri): client = vision.imageannotatorclient() image = vision.image() image.source.image_uri = uri response = client.label_detection(image=image) labels = response.label_annotations return(labels)",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"hi, i am a newbie in google cloud and i have an elementary conceptual question about the dependency betweencloud vision api and or the recently launched vertex vision ai. i have an app that makes predictions on images using google vision ai apiimageannotatorclient is this api going to be part of or vertex vision ai? or in other words, should i modify the below code to make it part of vertex vision ai? from import vision def client image uri response labels return"
    },
    {
        "Question_id":71797449.0,
        "Question_title":"Python Kedro PySpark : py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext",
        "Question_body":"<p>it's my first project using kedro with Pyspark and I have an issue. I work with the new Mac (M1). When I do <code> spark-shell<\/code> in the terminal, spark is successfully installed and I have the right output (welcome to spark version 3.2.1 with the picture). However, I tried to run spark using Kedro project, I have a trouble. I tried to find solutions thanks to stack overflow discussion but nothing linked with this.<\/p>\n<p>Version:<\/p>\n<ul>\n<li>Python : 3.8<\/li>\n<li>Java : openjdk version &quot;18&quot; 2022-03-22<\/li>\n<li>PySpark : 3.2.1<\/li>\n<\/ul>\n<p>Spark conf :<\/p>\n<pre><code>spark.driver.maxResultSize: 3g\nspark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\nspark.sql.execution.arrow.pyspark.enabled: true\n<\/code><\/pre>\n<p>And in my project context of Kedro :<\/p>\n<pre><code>class ProjectContext(KedroContext):\n    &quot;&quot;&quot;A subclass of KedroContext to add Spark initialisation for the pipeline.&quot;&quot;&quot;\n\n    def __init__(\n        self,\n        package_name: str,\n        project_path: Union[Path, str],\n        env: str = None,\n        extra_params: Dict[str, Any] = None,\n    ):\n        super().__init__(package_name, project_path, env, extra_params)\n        if not os.getenv('DISABLE_SPARK'):\n            self.init_spark_session()\n\n    def init_spark_session(self) -&gt; None:\n        &quot;&quot;&quot;Initialises a SparkSession using the config\n        defined in project's conf folder.\n        &quot;&quot;&quot;\n\n        parameters = self.config_loader.get(&quot;spark*&quot;, &quot;spark*\/**&quot;)\n        spark_conf = SparkConf().setAll(parameters.items())\n\n        # Initialise the spark session\n        spark_session_conf = (\n            SparkSession.builder.appName(self.package_name)\n            .enableHiveSupport()\n            .config(conf=spark_conf)\n            .master(&quot;local[*]&quot;)\n        )\n        _spark_session = spark_session_conf.getOrCreate()\n\n<\/code><\/pre>\n<p>When I run it, I have this error :<\/p>\n<pre><code>py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c60b7e7) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x3c60b7e7\n    at org.apache.spark.storage.StorageUtils$.&lt;init&gt;(StorageUtils.scala:213)\n    at org.apache.spark.storage.StorageUtils$.&lt;clinit&gt;(StorageUtils.scala)\n    at org.apache.spark.storage.BlockManagerMasterEndpoint.&lt;init&gt;(BlockManagerMasterEndpoint.scala:110)\n    at org.apache.spark.SparkEnv$.$anonfun$create$9(SparkEnv.scala:348)\n    at org.apache.spark.SparkEnv$.registerOrLookupEndpoint$1(SparkEnv.scala:287)\n    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:336)\n    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:191)\n    at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:277)\n    at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:460)\n    at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:58)\n    at java.base\/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at java.base\/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n    at java.base\/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.base\/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n    at java.base\/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n    at py4j.Gateway.invoke(Gateway.java:238)\n    at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n    at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.base\/java.lang.Thread.run(Thread.java:833)\n\n<\/code><\/pre>\n<p>In my terminal, I adapted the commands to match my Python path :<\/p>\n<pre><code>export HOMEBREW_OPT=&quot;\/opt\/homebrew\/opt&quot;\nexport JAVA_HOME=&quot;$HOMEBREW_OPT\/openjdk\/&quot;\nexport SPARK_HOME=&quot;$HOMEBREW_OPT\/apache-spark\/libexec&quot;\nexport PATH=&quot;$JAVA_HOME:$SPARK_HOME:$PATH&quot;\nexport SPARK_LOCAL_IP=localhost\n<\/code><\/pre>\n<p>Thanks you for your help<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0.0,
        "Question_creation_time":1649422676853,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":3356.0,
        "Owner_creation_time":1638457293672,
        "Owner_last_access_time":1663922846652,
        "Owner_reputation":21.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":3.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71797449",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: python pyspark : py4j.protocol.py4jjavaerror: an error occurred while calling none.org.apache.spark.api.java.javasparkcontext; Content: it's my first project using with pyspark and i have an issue. i work with the new mac (m1). when i do spark-shell in the terminal, spark is successfully installed and i have the right output (welcome to spark version 3.2.1 with the picture). however, i tried to run spark using project, i have a trouble. i tried to find solutions thanks to stack overflow discussion but nothing linked with this. version: python : 3.8 java : openjdk version \"18\" 2022-03-22 pyspark : 3.2.1 spark conf : spark.driver.maxresultsize: 3g spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.s3afilesystem spark.sql.execution.arrow.pyspark.enabled: true and in my project context of : class projectcontext(context): \"\"\"a subclass of context to add spark initialisation for the pipeline.\"\"\" def __init__( self, package_name: str, project_path: union[path, str], env: str = none, extra_params: dict[str, any] = none, ): super().__init__(package_name, project_path, env, extra_params) if not os.getenv('disable_spark'): self.init_spark_session() def init_spark_session(self) -> none: \"\"\"initialises a sparksession using the config defined in project's conf folder. \"\"\" parameters = self.config_loader.get(\"spark*\", \"spark*\/**\") spark_conf = sparkconf().setall(parameters.items()) # initialise the spark session spark_session_conf = ( sparksession.builder.appname(self.package_name) .enablehivesupport() .config(conf=spark_conf) .master(\"local[*]\") ) _spark_session = spark_session_conf.getorcreate() when i run it, i have this error : py4j.protocol.py4jjavaerror: an error occurred while calling none.org.apache.spark.api.java.javasparkcontext. : java.lang.illegalaccesserror: class org.apache.spark.storage.storageutils$ (in unnamed module @0x3c60b7e7) cannot access class sun.nio.ch.directbuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x3c60b7e7 at org.apache.spark.storage.storageutils$.<init>(storageutils.scala:213) at org.apache.spark.storage.storageutils$.<clinit>(storageutils.scala) at org.apache.spark.storage.blockmanagermasterendpoint.<init>(blockmanagermasterendpoint.scala:110) at org.apache.spark.sparkenv$.$anonfun$create$9(sparkenv.scala:348) at org.apache.spark.sparkenv$.registerorlookupendpoint$1(sparkenv.scala:287) at org.apache.spark.sparkenv$.create(sparkenv.scala:336) at org.apache.spark.sparkenv$.createdriverenv(sparkenv.scala:191) at org.apache.spark.sparkcontext.createsparkenv(sparkcontext.scala:277) at org.apache.spark.sparkcontext.<init>(sparkcontext.scala:460) at org.apache.spark.api.java.javasparkcontext.<init>(javasparkcontext.scala:58) at java.base\/jdk.internal.reflect.nativeconstructoraccessorimpl.newinstance0(native method) at java.base\/jdk.internal.reflect.nativeconstructoraccessorimpl.newinstance(nativeconstructoraccessorimpl.java:77) at java.base\/jdk.internal.reflect.delegatingconstructoraccessorimpl.newinstance(delegatingconstructoraccessorimpl.java:45) at java.base\/java.lang.reflect.constructor.newinstancewithcaller(constructor.java:499) at java.base\/java.lang.reflect.constructor.newinstance(constructor.java:480) at py4j.reflection.methodinvoker.invoke(methodinvoker.java:247) at py4j.reflection.reflectionengine.invoke(reflectionengine.java:357) at py4j.gateway.invoke(gateway.java:238) at py4j.commands.constructorcommand.invokeconstructor(constructorcommand.java:80) at py4j.commands.constructorcommand.execute(constructorcommand.java:69) at py4j.clientserverconnection.waitforcommands(clientserverconnection.java:182) at py4j.clientserverconnection.run(clientserverconnection.java:106) at java.base\/java.lang.thread.run(thread.java:833) in my terminal, i adapted the commands to match my python path : export homebrew_opt=\"\/opt\/homebrew\/opt\" export java_home=\"$homebrew_opt\/openjdk\/\" export spark_home=\"$homebrew_opt\/apache-spark\/libexec\" export path=\"$java_home:$spark_home:$path\" export spark_local_ip=localhost thanks you for your help",
        "Question_original_content_gpt_summary":"The user is encountering an issue with their first project using Python and PySpark, where they are receiving a \"py4j.protocol.py4jjavaerror: an error occurred while calling none.org.apache.spark.api.java.javasparkcontext\" error when attempting to run the project.",
        "Question_preprocessed_content":"Title: python pyspark an error occurred while calling; Content: it's my first project using with pyspark and i have an issue. i work with the new mac . when i do in the terminal, spark is successfully installed and i have the right output . however, i tried to run spark using project, i have a trouble. i tried to find solutions thanks to stack overflow discussion but nothing linked with this. version python java openjdk version pyspark spark conf and in my project context of when i run it, i have this error in my terminal, i adapted the commands to match my python path thanks you for your help",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":null,
        "Question_title":"Torch.multiprocessing.spawn fails",
        "Question_body":"<p>I have some code that works standalone, but fails when run from guild.  The offending line is:<\/p>\n<pre><code>torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args))\n<\/code><\/pre>\n<p>and the complaint is:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">  [...] \n  File \"\/usr\/lib\/python3.10\/multiprocessing\/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/context.py\", line 288, in _Popen\n    return Popen(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/spawn.py\", line 183, in get_preparation_data\n    main_mod_name = getattr(main_module.__spec__, \"name\", None)\nAttributeError: 'dict' object has no attribute '__spec__'\n<\/code><\/pre>\n<p>Does anyone have any tips?  I\u2019m not sure I really understand what\u2019s failing in the spawn call\u2026  Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":null,
        "Question_creation_time":1665950939224,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":159.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/torch-multiprocessing-spawn-fails\/929",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-17T17:38:14.680Z",
                "Answer_body":"<p>I\u2019m sorry you\u2019re running into this! I created an <a href=\"https:\/\/github.com\/guildai\/issue-resolution\/tree\/master\/my.guild.ai-929-torch-multiprocessing-spawn-fails\">issue resolution doc<\/a> that easily reproduces this. I\u2019ll spend some time looking into it.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-17T17:58:48.903Z",
                "Answer_body":"<p>Still investigating but there is a work-around - if you your script using Python using Guild\u2019s <code>exec<\/code> spec this way:<\/p>\n<aside class=\"onebox githubblob\" data-onebox-src=\"https:\/\/github.com\/guildai\/issue-resolution\/blob\/93de41a7741148e838e8c65e9b74b6854d11ad49\/my.guild.ai-929-torch-multiprocessing-spawn-fails\/guild.yml#L1-L2\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/guildai\/issue-resolution\/blob\/93de41a7741148e838e8c65e9b74b6854d11ad49\/my.guild.ai-929-torch-multiprocessing-spawn-fails\/guild.yml#L1-L2\" target=\"_blank\" rel=\"noopener\">github.com<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https:\/\/github.com\/guildai\/issue-resolution\/blob\/93de41a7741148e838e8c65e9b74b6854d11ad49\/my.guild.ai-929-torch-multiprocessing-spawn-fails\/guild.yml#L1-L2\" target=\"_blank\" rel=\"noopener\">guildai\/issue-resolution\/blob\/93de41a7741148e838e8c65e9b74b6854d11ad49\/my.guild.ai-929-torch-multiprocessing-spawn-fails\/guild.yml#L1-L2<\/a><\/h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-yml\">\n      <ol class=\"start lines\" start=\"1\" style=\"counter-reset: li-counter 0 ;\">\n          <li>test-exec:<\/li>\n          <li>  exec: python .guild\/sourcecode\/test.py<\/li>\n      <\/ol>\n    <\/code><\/pre>\n\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Note that this form doesn\u2019t support Python global variable based flags - you\u2019d need to either pass command line arguments along or use config files.<\/p>\n<p>Still looking into the underlying issue but I wanted to get you a workaround sooner than later.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: torch.multiprocessing.spawn fails; Content: i have some code that works standalone, but fails when run from guild. the offending line is: torch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, args)) and the complaint is: [...] file \"\/usr\/lib\/python3.10\/multiprocessing\/process.py\", line 121, in start self._popen = self._popen(self) file \"\/usr\/lib\/python3.10\/multiprocessing\/context.py\", line 288, in _popen return popen(process_obj) file \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 32, in __init__ super().__init__(process_obj) file \"\/usr\/lib\/python3.10\/multiprocessing\/popen_fork.py\", line 19, in __init__ self._launch(process_obj) file \"\/usr\/lib\/python3.10\/multiprocessing\/popen_spawn_posix.py\", line 42, in _launch prep_data = spawn.get_preparation_data(process_obj._name) file \"\/usr\/lib\/python3.10\/multiprocessing\/spawn.py\", line 183, in get_preparation_data main_mod_name = getattr(main_module.__spec__, \"name\", none) attributeerror: 'dict' object has no attribute '__spec__' does anyone have any tips? i\u2019m not sure i really understand what\u2019s failing in the spawn call\u2026 thanks!",
        "Question_original_content_gpt_summary":"The user is encountering an AttributeError when attempting to use torch.multiprocessing.spawn, and is seeking tips on how to resolve the issue.",
        "Question_preprocessed_content":"Title: fails; Content: i have some code that works standalone, but fails when run from guild. the offending line is and the complaint is does anyone have any tips? im not sure i really understand whats failing in the spawn call thanks!",
        "Answer_original_content":"im sorry youre running into this! i created an issue resolution doc that easily reproduces this. ill spend some time looking into it. still investigating but there is a work-around - if you your script using python using guilds exec spec this way: github.com guildai\/issue-resolution\/blob\/93de41a7741148e838e8c65e9b74b6854d11ad49\/my.guild.ai-929-torch-multiprocessing-spawn-fails\/guild.yml#l1-l2 test-exec: exec: python .guild\/sourcecode\/test.py note that this form doesnt support python global variable based flags - youd need to either pass command line arguments along or use config files. still looking into the underlying issue but i wanted to get you a workaround sooner than later.",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"im sorry youre running into this! i created an issue resolution doc that easily reproduces this. ill spend some time looking into it. still investigating but there is a work around if you your script using python using guilds spec this way note that this form doesnt support python global variable based flags youd need to either pass command line arguments along or use config files. still looking into the underlying issue but i wanted to get you a workaround sooner than later."
    },
    {
        "Question_id":null,
        "Question_title":"Pull data from Gdrive in GH actions?",
        "Question_body":"<p>Hi, I was wondering how I have to setup the github actions yaml file, so that it pulls the data and the model from google drive? Maybe I need to make the Gdrive folder public, or add my Gdrive credentials? Right now, the sanity-check fails using the workflow file from the video.<\/p>\n<p>test.yaml:<\/p>\n<pre><code class=\"lang-auto\">name: auto-testing\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    container: docker:\/\/dvcorg\/cml-py3:latest\n    steps:\n      - uses: actions\/checkout@v2\n      - name: sanity-check\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          # Your ML workflow goes here\n          pip install -r requirements.txt\n          python test.py\n<\/code><\/pre>",
        "Question_answer_count":10,
        "Question_comment_count":null,
        "Question_creation_time":1657891978346,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":228.0,
        "Owner_creation_time":null,
        "Owner_last_access_time":null,
        "Owner_reputation":null,
        "Owner_up_votes":null,
        "Owner_down_votes":null,
        "Owner_views":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pull-data-from-gdrive-in-gh-actions\/1246",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Question_has_accepted_answer":0.0,
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-16T03:55:07.806Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"rmbzmb\" data-post=\"1\" data-topic=\"1246\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/ed8c4c\/40.png\" class=\"avatar\"> rmbzmb:<\/div>\n<blockquote>\n<p>Hi, I was wondering how I have to setup the github actions yaml file, so that it pulls the data and the model from google drive? Maybe I need to make the Gdrive folder public, or add my Gdrive credentials? Right now, the sanity-check fails using the workflow file from the video.<\/p>\n<\/blockquote>\n<\/aside>\n<p>If you want to combine DVC with Github actions to achieve some CI automation, maybe you should take a look at our another product <a href=\"https:\/\/cml.dev\/\" rel=\"noopener nofollow ugc\">CML<\/a>. In its <a href=\"https:\/\/github.com\/iterative\/cml#using-cml-with-dvc\" rel=\"noopener nofollow ugc\">documents<\/a> it provides some info on how to setup this.<\/p>\n<blockquote>\n<p>env:<br>\nGDRIVE_CREDENTIALS_DATA: ${{ secrets.GDRIVE_CREDENTIALS_DATA }}<\/p>\n<\/blockquote>",
                "Answer_score":11.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-16T08:23:30.675Z",
                "Answer_body":"<p><a href=\"https:\/\/help.talend.com\/r\/en-US\/7.2\/google-drive\/how-to-access-google-drive-using-client-secret-json-file-the\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https:\/\/help.talend.com\/r\/en-US\/7.2\/google-drive\/how-to-access-google-drive-using-client-secret-json-file-the<\/a><\/p>\n<ul>\n<li>First, you have to enable the Google Drive API.<\/li>\n<li>Then add a service account and create credentials as a JSON file.<\/li>\n<li>That file can then be placed in the <code>.dvc\/tmp\/<\/code> folder.<\/li>\n<li>Then you need to modify the storage:<\/li>\n<\/ul>\n<pre><code class=\"lang-auto\">dvc remote modify storage --local gdrive_user_credentials_file .dvc\/tmp\/gdrive-credentials.json\n<\/code><\/pre>\n<p>However, in my case, that led to the following error:<\/p>\n<pre><code class=\"lang-auto\">dvc pull\nERROR: configuration error - GDrive remote auth failed with credentials in '...\/.dvc\/tmp\/gdrive-credentials.json'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\n\nDetails:: '_module'\nERROR: GDrive remote auth failed with credentials in '...\/.dvc\/tmp\/gdrive-credentials.json'.\nBackup first, remove or fix them, and run again.\nIt should do auth again and refresh the credentials.\n\nDetails:\nLearn more about configuration settings at &lt;https:\/\/man.dvc.org\/remote\/modify&gt;.\n<\/code><\/pre>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-17T02:45:37.868Z",
                "Answer_body":"<p>Excuse me, does this credentials file works on a local computer?<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T04:58:29.997Z",
                "Answer_body":"<p>Hi, Could you please try to test it locally if it fails because Github Action or Because of credential problem?<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T05:04:18.014Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/rmbzmb\">@rmbzmb<\/a> if you are using a service account you need to follow the instruction here:<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon-32x32.png?v=dfbc4a93a926127fc4495e9d640409f8\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\" target=\"_blank\" rel=\"noopener\">Data Version Control \u00b7 DVC<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/388;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"388\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/user-guide\/setup-google-drive-remote#using-service-accounts\" target=\"_blank\" rel=\"noopener\">Setup Google Drive Remote<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Namely, you would not use the <code>gdrive_user_credentials_file<\/code>, instead you should specify:<\/p>\n<pre><code class=\"lang-auto\">dvc remote modify myremote gdrive_use_service_account true\ndvc remote modify myremote --local \\\n              gdrive_service_account_json_file_path path\/to\/file.json\n<\/code><\/pre>\n<p>On CI you can then set the <code>GDRIVE_CREDENTIALS_DATA<\/code> to the content of the JSON file with the service account credentials.<\/p>\n<p>Please let us know if this still doesn\u2019t work.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T08:42:14.661Z",
                "Answer_body":"<p>That means, I have to use the json file (or the content) on both the local version and the GitHub version? I cannot not use the json file locally and <code>GDRIVE_CREDENTIALS_DATA<\/code> on GitHub at the same time?<\/p>\n<p>I tried it now. Locally it the json file works, on GitHub, I get the same error message as before.<br>\nIs there an example yaml file available somewhere?<\/p>\n<pre><code class=\"lang-auto\">name: auto-testing\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    container: docker:\/\/dvcorg\/cml-py3:latest\n    steps:\n      - uses: actions\/checkout@v2\n      - name: sanity-check\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n          GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}          \n\n        run: |\n          # Your ML workflow goes here\n          pip install -r requirements.txt\n          dvc pull data\n          dvc repro\n<\/code><\/pre>\n<p>The data pull still fails with the same error message.<\/p>\n<pre><code class=\"lang-auto\">WARNING: You are using pip version 21.1; however, version 21.3.1 is available.\nYou should consider upgrading via the '\/usr\/bin\/python3 -m pip install --upgrade pip' command.\n\/usr\/local\/lib\/python3.6\/dist-packages\/pycaret\/loggers\/mlflow_logger.py:14: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\nPycaret: 2.3.10\n  import mlflow\nTraceback (most recent call last):\n  File \"src\/test.py\", line 55, in &lt;module&gt;\n    (x_train, y_train), (x_test, y_test)  = mdl.load_data()\n  File \"src\/test.py\", line 44, in load_data\n    x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n  File \"src\/test.py\", line 22, in read_images_labels\n    with open(labels_filepath, 'rb') as file:\nFileNotFoundError: [Errno 2] No such file or directory: 'data\/MINST\/train\/train-labels-idx1-ubyte'\nError: Process completed with exit code 1.\n\n<\/code><\/pre>\n<p>Is it enough to pull the parent directory of all the data files, or do I need to pull them all individually specifying all the file names?<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T10:58:15.266Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"rmbzmb\" data-post=\"9\" data-topic=\"1246\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/r\/ed8c4c\/40.png\" class=\"avatar\"> rmbzmb:<\/div>\n<blockquote>\n<p>I tried it now. Locally it the json file works, on GitHub, I get the same error message as before.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Hi <a class=\"mention\" href=\"\/u\/rmbzmb\">@rmbzmb<\/a> You don\u2019t need to use a json file, you just need to copy the contents of json file and set it to the <code>GDRIVE_CREDENTIALS_DATA<\/code> on Github.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T11:26:26.514Z",
                "Answer_body":"<p>Yes, I am using the JSON file only on my local computer and on a remote workstation. On GitHub I copy-pasted the content of the JSON file into a GitHub secret named <code>GDRIVE_CREDENTIALS_DATA<\/code>. But it is still not working.<\/p>\n<pre><code class=\"lang-auto\">    ...\n    GDRIVE_CREDENTIALS_DATA : ${{ secrets.GDRIVE_CREDENTIALS_DATA }}\n    ....\n<\/code><\/pre>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-18T13:40:29.695Z",
                "Answer_body":"<p>It seems dvc on GitHub is ignoring <code>GDRIVE_CREDENTIALS_DATA<\/code>, even though it is set.<\/p>\n<blockquote>\n<p><a href=\"https:\/\/github.com\/sorenwacker\/dvc-test-project-01\/runs\/7390516277?check_suite_focus=true#step:4:46\" rel=\"noopener nofollow ugc\">45<\/a>ERROR: failed to pull data from the cloud - To use service account, set <code>gdrive_service_account_json_file_path<\/code>, and optionally<code>gdrive_service_account_user_email<\/code> in DVC config<\/p>\n<\/blockquote>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-19T10:19:12.228Z",
                "Answer_body":"<p>Seems there is an error in dvc currently: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/7949\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">gdrive: raises unexpected error - name: drive version: v2 (again) \u00b7 Issue #7949 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: pull data from gdrive in gh actions?; Content: hi, i was wondering how i have to setup the github actions yaml file, so that it pulls the data and the model from google drive? maybe i need to make the gdrive folder public, or add my gdrive credentials? right now, the sanity-check fails using the workflow file from the video. test.yaml: name: auto-testing on: [push] jobs: run: runs-on: [ubuntu-latest] container: docker:\/\/org\/cml-py3:latest steps: - uses: actions\/checkout@v2 - name: sanity-check env: repo_token: ${{ secrets.github_token }} run: | # your ml workflow goes here pip install -r requirements.txt python test.py",
        "Question_original_content_gpt_summary":"The user is encountering challenges in setting up a GitHub Actions YAML file to pull data and a model from Google Drive, and is unsure if they need to make the GDrive folder public or add GDrive credentials.",
        "Question_preprocessed_content":"Title: pull data from gdrive in gh actions?; Content: hi, i was wondering how i have to setup the github actions yaml file, so that it pulls the data and the model from google drive? maybe i need to make the gdrive folder public, or add my gdrive credentials? right now, the sanity check fails using the workflow file from the video.",
        "Answer_original_content":"rmbzmb: hi, i was wondering how i have to setup the github actions yaml file, so that it pulls the data and the model from google drive? maybe i need to make the gdrive folder public, or add my gdrive credentials? right now, the sanity-check fails using the workflow file from the video. if you want to combine with github actions to achieve some ci automation, maybe you should take a look at our another product cml. in its documents it provides some info on how to setup this. env: gdrive_credentials_data: ${{ secrets.gdrive_credentials_data }} https:\/\/help.talend.com\/r\/en-us\/7.2\/google-drive\/how-to-access-google-drive-using-client-secret-json-file-the first, you have to enable the google drive api. then add a service account and create credentials as a json file. that file can then be placed in the .\/tmp\/ folder. then you need to modify the storage: remote modify storage --local gdrive_user_credentials_file .\/tmp\/gdrive-credentials.json however, in my case, that led to the following error: pull error: configuration error - gdrive remote auth failed with credentials in '...\/.\/tmp\/gdrive-credentials.json'. backup first, remove or fix them, and run again. it should do auth again and refresh the credentials. details:: '_module' error: gdrive remote auth failed with credentials in '...\/.\/tmp\/gdrive-credentials.json'. backup first, remove or fix them, and run again. it should do auth again and refresh the credentials. details: learn more about configuration settings at <https:\/\/man..org\/remote\/modify>. excuse me, does this credentials file works on a local computer? hi, could you please try to test it locally if it fails because github action or because of credential problem? @rmbzmb if you are using a service account you need to follow the instruction here: data version control setup google drive remote open-source version control system for data science and machine learning projects. git-like experience to organize your data, models, and experiments. namely, you would not use the gdrive_user_credentials_file, instead you should specify: remote modify myremote gdrive_use_service_account true remote modify myremote --local \\ gdrive_service_account_json_file_path path\/to\/file.json on ci you can then set the gdrive_credentials_data to the content of the json file with the service account credentials. please let us know if this still doesnt work. that means, i have to use the json file (or the content) on both the local version and the github version? i cannot not use the json file locally and gdrive_credentials_data on github at the same time? i tried it now. locally it the json file works, on github, i get the same error message as before. is there an example yaml file available somewhere? name: auto-testing on: [push] jobs: run: runs-on: [ubuntu-latest] container: docker:\/\/org\/cml-py3:latest steps: - uses: actions\/checkout@v2 - name: sanity-check env: repo_token: ${{ secrets.github_token }} gdrive_credentials_data : ${{ secrets.gdrive_credentials_data }} run: | # your ml workflow goes here pip install -r requirements.txt pull data repro the data pull still fails with the same error message. warning: you are using pip version 21.1; however, version 21.3.1 is available. you should consider upgrading via the '\/usr\/bin\/python3 -m pip install --upgrade pip' command. \/usr\/local\/lib\/python3.6\/dist-packages\/pycaret\/loggers\/mlflow_logger.py:14: futurewarning: mlflow support for python 3.6 is deprecated and will be dropped in an upcoming release. at that point, existing python 3.6 workflows that use mlflow will continue to work without modification, but python 3.6 users will no longer get access to the latest mlflow features and bugfixes. we recommend that you upgrade to python 3.7 or newer. pycaret: 2.3.10 import mlflow traceback (most recent call last): file \"src\/test.py\", line 55, in <module> (x_train, y_train), (x_test, y_test) = mdl.load_data() file \"src\/test.py\", line 44, in load_data x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath) file \"src\/test.py\", line 22, in read_images_labels with open(labels_filepath, 'rb') as file: filenotfounderror: [errno 2] no such file or directory: 'data\/minst\/train\/train-labels-idx1-ubyte' error: process completed with exit code 1. is it enough to pull the parent directory of all the data files, or do i need to pull them all individually specifying all the file names? rmbzmb: i tried it now. locally it the json file works, on github, i get the same error message as before. hi @rmbzmb you dont need to use a json file, you just need to copy the contents of json file and set it to the gdrive_credentials_data on github. yes, i am using the json file only on my local computer and on a remote workstation. on github i copy-pasted the content of the json file into a github secret named gdrive_credentials_data. but it is still not working. ... gdrive_credentials_data : ${{ secrets.gdrive_credentials_data }} .... it seems on github is ignoring gdrive_credentials_data, even though it is set. 45error: failed to pull data from the cloud - to use service account, set gdrive_service_account_json_file_path, and optionallygdrive_service_account_user_email in config seems there is an error in currently: gdrive: raises unexpected error - name: drive version: v2 (again) issue #7949 iterative\/ github",
        "Answer_original_content_gpt_summary":"",
        "Answer_preprocessed_content":"rmbzmb hi, i was wondering how i have to setup the github actions yaml file, so that it pulls the data and the model from google drive? maybe i need to make the gdrive folder public, or add my gdrive credentials? right now, the sanity check fails using the workflow file from the video. if you want to combine with github actions to achieve some ci automation, maybe you should take a look at our another product cml. in its documents it provides some on how to setup this. env $ first, you have to enable the google drive api. then add a service account and create credentials as a json file. that file can then be placed in the folder. then you need to modify the storage however, in my case, that led to the following error excuse me, does this credentials file works on a local computer? hi, could you please try to test it locally if it fails because github action or because of credential problem? if you are using a service account you need to follow the instruction here data version control setup google drive remote open source version control system for data science and machine learning projects. git like experience to organize your data, models, and experiments. namely, you would not use the , instead you should specify on ci you can then set the to the content of the json file with the service account credentials. please let us know if this still doesnt work. that means, i have to use the json file on both the local version and the github version? i cannot not use the json file locally and on github at the same time? i tried it now. locally it the json file works, on github, i get the same error message as before. is there an example yaml file available somewhere? the data pull still fails with the same error message. is it enough to pull the parent directory of all the data files, or do i need to pull them all individually specifying all the file names? rmbzmb i tried it now. locally it the json file works, on github, i get the same error message as before. hi you dont need to use a json file, you just need to copy the contents of json file and set it to the on github. yes, i am using the json file only on my local computer and on a remote workstation. on github i copy pasted the content of the json file into a github secret named . but it is still not working. it seems on github is ignoring , even though it is set. error failed to pull data from the cloud to use service account, set , and optionally in config seems there is an error in currently gdrive raises unexpected error name drive version v issue github"
    },
    {
        "Question_id":62211555.0,
        "Question_title":"How can I import local package dependencies into Kedro notebooks?",
        "Question_body":"<p>I've placed package dependencies (wheels) of a Kedro project into a <code>&lt;project-root&gt;\/deps\/*.whl<\/code> directory. I'm using a venv installed into <code>&lt;project-root&gt;\/.venv<\/code> and manage it using Poetry.<\/p>\n\n<p>Packages are referenced in <code>pyproject.toml<\/code> like this (here e.g. <code>local-package<\/code>):<\/p>\n\n<pre><code>[tool.poetry.dependencies]\nlocal-package = {path = \"deps\/local_package-X.Y.Z-py2.py3-none-any.whl\"}\n<\/code><\/pre>\n\n<p>The package can be imported in a REPL session:<\/p>\n\n<pre><code>$ .venv\/bin\/python\nPython 3.8.2 (default, Apr 27 2020, 15:53:34) \n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import local_package\n&gt;&gt;&gt; \n\nIn VSCode I've activated my venv and am running `kedro jupyter lab` to start jupyter lab. If I try to import the package it cannot be found (running `import local_package` in a notebook cell leads to `ModuleNotFoundError: No module named \"local_package\"`). If I run `!poetry show | grep local-package` in a notebook cell I get `local-package    X.Y.Z deps\/local_package-X.Y.Z-py2.py3-none-any.whl`. The Python interpreter shown in Jupyter Lab is the project one. What do I have to do to get this working?\n<\/code><\/pre>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_creation_time":1591347097613,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":471.0,
        "Owner_creation_time":1441627039648,
        "Owner_last_access_time":1663951876836,
        "Owner_reputation":3635.0,
        "Owner_up_votes":1076.0,
        "Owner_down_votes":3.0,
        "Owner_views":383.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Augsburg, Germany",
        "Question_last_edit_time":1591347720276,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62211555",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: how can i import local package dependencies into notebooks?; Content: i've placed package dependencies (wheels) of a project into a <project-root>\/deps\/*.whl directory. i'm using a venv installed into <project-root>\/.venv and manage it using poetry. packages are referenced in pyproject.toml like this (here e.g. local-package): [tool.poetry.dependencies] local-package = {path = \"deps\/local_package-x.y.z-py2.py3-none-any.whl\"} the package can be imported in a repl session: $ .venv\/bin\/python python 3.8.2 (default, apr 27 2020, 15:53:34) [gcc 9.3.0] on linux type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import local_package >>> in vscode i've activated my venv and am running ` jupyter lab` to start jupyter lab. if i try to import the package it cannot be found (running `import local_package` in a notebook cell leads to `modulenotfounderror: no module named \"local_package\"`). if i run `!poetry show | grep local-package` in a notebook cell i get `local-package x.y.z deps\/local_package-x.y.z-py2.py3-none-any.whl`. the python interpreter shown in jupyter lab is the project one. what do i have to do to get this working?",
        "Question_original_content_gpt_summary":"The user is encountering challenges with importing local package dependencies into notebooks, despite the packages being referenced in pyproject.toml and being able to be imported in a REPL session.",
        "Question_preprocessed_content":"Title: how can i import local package dependencies into notebooks?; Content: i've placed package dependencies of a project into a directory. i'm using a venv installed into and manage it using poetry. packages are referenced in like this the package can be imported in a repl session",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":70693420.0,
        "Question_title":"DVC(Data Version Control) keeps stuck at \"dvc add xxx\" with \"Collecting stages from the workspace\" in the terminal?",
        "Question_body":"<p>I used : <code>dvc[webhdfs]==2.9.3<\/code>, installed by <code>pip install dvc[webhdfs]<\/code><\/p>\n<p>Then the repo is already cloned by git.<\/p>\n<p>I have also typed : <code>dvc remote add -d storage webhdfs:\/\/xxx\/dvc<\/code> and <code>git add .dvc\/config<\/code><\/p>\n<p>But the command <code>dvc add .\/assets\/xxx\/*<\/code> was still stuck...<\/p>\n<p>The command line window keeps showing : <code>Collecting stages from the workspace<\/code><\/p>",
        "Question_answer_count":0,
        "Question_comment_count":2.0,
        "Question_creation_time":1642062276767,
        "Question_favorite_count":null,
        "Question_score":1.0,
        "Question_view_count":115.0,
        "Owner_creation_time":1553399833283,
        "Owner_last_access_time":1659098922787,
        "Owner_reputation":11.0,
        "Owner_up_votes":0.0,
        "Owner_down_votes":0.0,
        "Owner_views":2.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":"Beijing",
        "Question_last_edit_time":1642068714663,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70693420",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: (data version control) keeps stuck at \" add xxx\" with \"collecting stages from the workspace\" in the terminal?; Content: i used : [webhdfs]==2.9.3, installed by pip install [webhdfs] then the repo is already cloned by git. i have also typed : remote add -d storage webhdfs:\/\/xxx\/ and git add .\/config but the command add .\/assets\/xxx\/* was still stuck... the command line window keeps showing : collecting stages from the workspace",
        "Question_original_content_gpt_summary":"The user is encountering challenges with data version control, as they are stuck at \"add xxx\" with \"collecting stages from the workspace\" in the terminal.",
        "Question_preprocessed_content":"Title: keeps stuck at add xxx with collecting stages from the workspace in the terminal?; Content: i used , installed by then the repo is already cloned by git. i have also typed and but the command was still the command line window keeps showing",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    },
    {
        "Question_id":73733223.0,
        "Question_title":"Installing python packages on the workers in Databricks restarts the interpreter",
        "Question_body":"<p>I have a machine learning job in prod that keeps failing because of the reason below:<\/p>\n<pre><code>InvalidConfigurationError: You haven't configured the CLI yet! Please configure by entering `\/databricks\/python_shell\/scripts\/PythonShell.py configure`\n<\/code><\/pre>\n<p>To address it, I then installed the packages on the workers by doing:<\/p>\n<pre><code>%pip install\n<\/code><\/pre>\n<p>as shown in <a href=\"https:\/\/community.databricks.com\/s\/feed\/0D53f00001HKIENCA5\" rel=\"nofollow noreferrer\">https:\/\/community.databricks.com\/s\/feed\/0D53f00001HKIENCA5<\/a>, instead of<\/p>\n<pre><code>%sh\n\npip install\n<\/code><\/pre>\n<p>The new problem that I now have is that '%pip' restarts the python interpreter and I do not want that because I lose any python computing done before the command.<\/p>\n<p>Question: How do I install a package on the workers without restarting the python interpreter?<\/p>",
        "Question_answer_count":0,
        "Question_comment_count":1.0,
        "Question_creation_time":1663253141683,
        "Question_favorite_count":null,
        "Question_score":0.0,
        "Question_view_count":72.0,
        "Owner_creation_time":1351534968768,
        "Owner_last_access_time":1663768904350,
        "Owner_reputation":823.0,
        "Owner_up_votes":16.0,
        "Owner_down_votes":4.0,
        "Owner_views":114.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_creation_time":null,
        "Answer_score":null,
        "Owner_location":null,
        "Question_last_edit_time":null,
        "Answer_last_edit_time":null,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73733223",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_has_accepted_answer":null,
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_original_content":"Title: installing python packages on the workers in databricks restarts the interpreter; Content: i have a machine learning job in prod that keeps failing because of the reason below: invalidconfigurationerror: you haven't configured the cli yet! please configure by entering `\/databricks\/python_shell\/scripts\/pythonshell.py configure` to address it, i then installed the packages on the workers by doing: %pip install as shown in https:\/\/community.databricks.com\/s\/feed\/0d53f00001hkienca5, instead of %sh pip install the new problem that i now have is that '%pip' restarts the python interpreter and i do not want that because i lose any python computing done before the command. question: how do i install a package on the workers without restarting the python interpreter?",
        "Question_original_content_gpt_summary":"The user is encountering a challenge of installing python packages on the workers in Databricks without restarting the python interpreter.",
        "Question_preprocessed_content":"Title: installing python packages on the workers in databricks restarts the interpreter; Content: i have a machine learning job in prod that keeps failing because of the reason below to address it, i then installed the packages on the workers by doing as shown in instead of the new problem that i now have is that '%pip' restarts the python interpreter and i do not want that because i lose any python computing done before the command. question how do i install a package on the workers without restarting the python interpreter?",
        "Answer_original_content":null,
        "Answer_original_content_gpt_summary":null,
        "Answer_preprocessed_content":null
    }
]